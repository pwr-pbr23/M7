[{"number": 19699, "title": "[Intel MKL] Finished support for bad usernames in the CI build scripts.", "body": "ci_build.sh now passes the environment variable to the container, and the with_the_same_user script adds the --force-badname param to addgroup as well.", "comments": ["@gunan We need this in order to build in the cloud where user/group names sometimes have '.' and '_' which adduser and addgroup don't like. The feature was half there; just not complete."]}, {"number": 19698, "title": "ppc64le: //tensorflow/python/kernel_tests:self_adjoint_eig_op_test test fails", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:self_adjoint_eig_op_test\r\n\r\n### Describe the problem\r\n```\r\n======================================================================\r\nFAIL: testMatrixThatFailsWhenFlushingDenormsToZero (__main__.SelfAdjointEigTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py\", line 87, in testMatrixThatFailsWhenFlushingDenormsToZero\r\n    np.matmul(v, v.transpose()), np.eye(32, dtype=np.float32), atol=2e-3)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1368, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1338, in _assertAllCloseRecursive\r\n    path_str))\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/self_adjoint_eig_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1273, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 754, in assert_array_compare\r\n    chk_same_position(x_isnan, y_isnan, hasval='nan')\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 735, in chk_same_position\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=0.002\r\nMismatched value: a is different from b.\r\nx and y nan location mismatch:\r\n x: array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\r\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\r\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],...\r\n y: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\r\n       [ 0.,  0.,  1., ...,  0.,  0.,  0.],...\r\n\r\n----------------------------------------------------------------------\r\nRan 9 tests in 1.543s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([ 0,  0,  0, ..., 31, 31, 31]), array([ 0,  1,  2, ..., 28, 29, 30]))\r\nnot close lhs =  [ nan  nan  nan ...,  nan  nan  nan]\r\nnot close rhs =  [ 1.  0.  0. ...,  0.  0.  0.]\r\nnot close dif =  [ nan  nan  nan ...,  nan  nan  nan]\r\nnot close tol =  [ 0.002001  0.002     0.002    ...,  0.002     0.002     0.002   ]\r\ndtype = float32, shape = (32, 32)\r\n================================================================================\r\n\r\n```\r\n### Source code / logs\r\n[self_adjoint_eig_op_test.log](https://github.com/tensorflow/tensorflow/files/2063585/self_adjoint_eig_op_test.log)\r\n", "comments": ["@sandipmgiri - Can you look at this issue. Is this another case of needing to disable a denormal_test on the ppc64le platform?\r\n\r\nSee: https://github.com/tensorflow/tensorflow/issues/11902, fixed by: https://github.com/tensorflow/tensorflow/pull/11939/files", "This test is passing for me with CPU support on Ubuntu16.04(ppc64le).\r\nCan you please try re-running the test 3-4 times ?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This test was failing on GPU only, but in the most recent master branch code it is working. I don't know what commit fixed it, but the issue is resolved now. Closing."]}, {"number": 19697, "title": "ppc64le: ///tensorflow/python/kernel_tests:matrix_solve_ls_op_test and svd_op_test core dump", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:svd_op_test\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:matrix_solve_ls_op_test\r\n\r\n\r\n\r\n### Describe the problem\r\n```\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/svd_op_test.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x73cb0c)[0x3ffefda9cb0c]\r\n[0x3fffa9fe04d8]\r\n/usr/lib/libopenblas.so.0(dlasd2_+0xb20)[0x3fffa9372820]\r\n[0x0]\r\n/usr/lib/libopenblas.so.0(dlasd1_+0x3bc)[0x3fffa9371bac]\r\n/usr/lib/libopenblas.so.0(dlasd0_+0xb98)[0x3fffa9371618]\r\n/usr/lib/libopenblas.so.0(dbdsdc_+0xda8)[0x3fffa92be6a8]\r\n/usr/lib/libopenblas.so.0(zgesdd_+0x5878)[0x3fffa95cf368]\r\n/usr/local/lib/python2.7/dist-packages/numpy/linalg/_umath_linalg.so(+0x119cc)[0x3fff0ace19cc]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/umath.so(+0xe9340)[0x3fff75389340]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/umath.so(+0xe9c38)[0x3fff75389c38]\r\n/usr/local/lib/python2.7/dist-packages/numpy/core/umath.so(+0xeb1dc)[0x3fff7538b1dc]\r\n/usr/bin/python(PyEval_EvalFrameEx+0x65b0)[0x1009fb80]\r\n/usr/bin/python(PyEval_EvalCodeEx+0x498)[0x10095768]\r\n/usr/bin/python(PyEval_EvalFrameEx+0x6768)[0x1009fd38]\r\n/usr/bin/python(PyEval_EvalCodeEx+0x498)[0x10095768]\r\n/usr/bin/python(PyEval_EvalFrameEx+0x6f74)[0x100a0544]\r\n/usr/bin/python(PyEval_EvalCodeEx+0x498)[0x10095768]\r\n/usr/bin/python[0x100c01d4]\r\n.\r\n.\r\n.\r\n/usr/bin/python(PyRun_FileExFlags+0xc4)[0x100d7934]\r\n/usr/bin/python(PyRun_SimpleFileExFlags+0x1d8)[0x100d58d8]\r\n/usr/bin/python(Py_Main+0x780)[0x10051c60]\r\n/lib/powerpc64le-linux-gnu/libc.so.6(+0x2309c)[0x3fffa9de309c]\r\n/lib/powerpc64le-linux-gnu/libc.so.6(__libc_start_main+0xb8)[0x3fffa9de3298]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n        tensorflow::CurrentStackTrace[abi:cxx11]()\r\n\r\n        __kernel_sigtramp_rt64\r\n        dlasd2_\r\n\r\n        dlasd1_\r\n        dlasd0_\r\n        dbdsdc_\r\n        zgesdd_\r\n\r\n\r\n\r\n\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n        PyObject_Call\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n\r\n        PyObject_Call\r\n\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n        PyObject_Call\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n\r\n        PyObject_Call\r\n\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n        PyObject_Call\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n\r\n        PyObject_Call\r\n\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n\r\n\r\n\r\n\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyEval_EvalFrameEx\r\n        PyEval_EvalCodeEx\r\n        PyRun_FileExFlags\r\n        PyRun_SimpleFileExFlags\r\n        Py_Main\r\n\r\n        __libc_start_main\r\n*** End stack trace ***\r\n/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/svd_op_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 40: 72935 Aborted                 (core dumped) $@\r\n================================================================================\r\n```\r\n\r\n### Source code / logs\r\n[svd_op_test.log](https://github.com/tensorflow/tensorflow/files/2063542/svd_op_test.log)\r\n[matrix_solve_ls_op_test.log](https://github.com/tensorflow/tensorflow/files/2063544/matrix_solve_ls_op_test.log)\r\n", "comments": ["For me as well these tests were failing in TF-master.\r\nAs suggested by @smatzek, I have upgraded numpy from 1.14.3 to 1.14.4 and openblas from 0.2.18 to 0.3.0, and now these 2 tests are passing for me :                \r\n1. //tensorflow/python/kernel_tests:svd_op_test \r\n2. //tensorflow/python/kernel_tests:matrix_solve_ls_op_test \r\n\r\nFYI - I have used the below steps to install numpy and openblas:\r\n```\r\n$ pip install numpy==1.14.4\r\n$ git clone https://github.com/xianyi/OpenBLAS\r\n$ cd OpenBLAS && make FC=gfortran\r\n$ sudo make PREFIX=/opt/OpenBLAS install\r\n$ export LD_LIBRARY_PATH=/opt/OpenBLAS/lib:$LD_LIBRARY_PATH\r\n$ sudo ldconfig \r\n```\r\nPlease try this.", "Nagging Assignee @shivaniag: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "With PR #20357 merged, this test is now passing."]}, {"number": 19696, "title": "Save training using Tensorflow C++ with VS2015", "body": "------------------------\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio 2015\r\n- **CUDA/cuDNN version**: Yes\r\n- **GPU model and memory**:  NVIDIA Quadro P5000 16GB\r\n- **Exact command to reproduce**: N/A\r\n------------------------\r\n------------------------\r\n\r\n### Description of the problem: Save Training Results using Tensorflow C++ VS2015\r\nI have successfully compiled tensorflow to be able to use the C++ with VS2015, and I have successfully run some examples.\r\n\r\nI want to save my training results and restore them to be able to continue training or simply to use the network to perform e.g. classification.\r\n\r\nI was unable until now to perform that, and the C++ interface didn't have many examples or tutorials available.\r\n\r\nCan someone provide me some instructions to perform that?\r\n\r\n------------------------\r\n------------------------", "comments": ["I also wait this api,I can save GraphDef by using WriteBinaryProto,but I don't know how to get GraphDef from session", "@Pessanha24,\r\nAs this [Stack Overflow Answer](https://stackoverflow.com/a/47403738/11530462) states, \r\n\r\n> Model saving is implemented in Python only. There is currently no way to save a model using C++ APIs. C++ APIs allow you to load and use the models, not to train or save them.\r\n\r\nProcedure to load the `Saved Model` using `C++` is present in the [Tensorflow Documentation](https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 19695, "title": "ppc64le: //tensorflow/contrib/image:image_ops_test test fails", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/image:image_ops_test\r\n\r\n\r\n\r\n### Describe the problem\r\n```\r\n======================================================================\r\nFAIL: test_rotate_even (__main__.ImageOpsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/contrib/image/python/kernel_tests/image_ops_test.py\", line 68, in test_rotate_even\r\n    [1, 7, 13, 19, 25, 31], [0, 6, 12, 18, 24, 30]]])\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1466, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b, err_msg=msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 871, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 1.85185185185%)\r\n x: array([[[ 0,  1,  2,  3,  4,  5],\r\n        [ 6,  7,  8,  9, 10, 11],\r\n        [12, 13, 14, 15, 16, 17],...\r\n y: array([[[ 0,  1,  2,  3,  4,  5],\r\n        [ 6,  7,  8,  9, 10, 11],\r\n        [12, 13, 14, 15, 16, 17],...\r\n\r\n----------------------------------------------------------------------\r\nRan 11 tests in 14.343s\r\n\r\nFAILED (failures=1)\r\nnot equal where =  (array([1, 1]), array([3, 4]), array([3, 4]))\r\nnot equal lhs =  [20 32]\r\nnot equal rhs =  [21 33]\r\n================================================================================\r\n```\r\n\r\n### Source code / logs\r\n[image_ops_test.log](https://github.com/tensorflow/tensorflow/files/2063329/image_ops_test.log)\r\n", "comments": ["This has been found working on RHEL 7.5 and Ubuntu 18.04 without any fix. Not sure what exactly is the difference between new OSes and old ones which has fixed this test. ", "From talking with @sandipmgiri , this issue has the same root cause as https://github.com/tensorflow/tensorflow/issues/14017, which was a defect opened by the system Z team. \r\n\r\nI will try and identify what package has been updated in 18.04 to resolve this issue.", "Responding to tensorflowbutler to keep him happy :)\r\n\r\nThis is still an issue for Ubuntu 16.04, but not 18.04, so it is lower priority on among our issues to investigate. I'm not sure when Tensorflow is moving to using 18.04 for unit testing.", "Is it related to issues with NumPy?\r\n", "Closing old issues I opened. This is not an issue with ubuntu 18.04"]}, {"number": 19694, "title": "ppc64le: //tensorflow/contrib/distributions:sinh_arcsinh_bijector_test test fails", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/distributions:sinh_arcsinh_bijector_test\r\n\r\n\r\n\r\n### Describe the problem\r\n```\r\n======================================================================\r\nFAIL: testBijectorOverRange (__main__.SinhArcsinhBijectorTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/sinh_arcsinh_bijector_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/sinh_arcsinh_bijector_test.py\", line 163, in testBijectorOverRange\r\n    atol=0.)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/sinh_arcsinh_bijector_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1368, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/sinh_arcsinh_bijector_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1338, in _assertAllCloseRecursive\r\n    path_str))\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/sinh_arcsinh_bijector_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1273, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 762, in assert_array_compare\r\n    chk_same_position(x == -inf, y == -inf, hasval='-inf')\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 735, in chk_same_position\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.0001, atol=0\r\nMismatched value: a is different from b.\r\nx and y -inf location mismatch:\r\n x: array([[-2.66556, -52.9496],\r\n       [-2.76943, -53.0418],\r\n       [-2.91071, -53.1402],...\r\n y: array([[  -2.665562,  -52.949618],\r\n       [  -2.769431,  -53.041801],\r\n       [  -2.910706,  -53.140182],...\r\n\r\n----------------------------------------------------------------------\r\nRan 12 tests in 1.492s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([473, 474, 475, ..., 998, 999, 999]), array([1, 1, 1, ..., 1, 0, 1]))\r\nnot close lhs =  [-inf -inf -inf ..., -inf -inf -inf]\r\nnot close rhs =  [-327.55577066 -328.16256741 -328.76936416 ..., -646.12406241 -357.13107722\r\n -646.73085916]\r\nnot close dif =  [ inf  inf  inf ...,  inf  inf  inf]\r\nnot close tol =  [ 0.03275558  0.03281626  0.03287694 ...,  0.06461241  0.03571311\r\n  0.06467309]\r\ndtype = float128, shape = (1000, 2)\r\n================================================================================\r\n\r\n```\r\n\r\n\r\n### Source code / logs\r\n[distributions_sinh_arcsinh_bijector_test.log](https://github.com/tensorflow/tensorflow/files/2063320/distributions_sinh_arcsinh_bijector_test.log)\r\n", "comments": ["I've a fix for this. Will raise a PR.", "Raised a PR - https://github.com/tensorflow/tensorflow/pull/19860", "I will be retesting this since 19860 is now merged.", "This issue is resolved now that 19860 is merged. Thank you @npanpaliya.\r\n\r\nI will note at first it was timing out until I changed the testcase size from small to medium, but I think that is an issue related to the system I'm using to test. I'll try another test on a different system. "]}, {"number": 19693, "title": "ppc64le: //tensorflow/python/kernel_tests:conv_ops_test test fails", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:conv_ops_test\r\n\r\n\r\n\r\n### Describe the problem\r\n```\r\n======================================================================\r\nFAIL: testInceptionFwd_46 (__main__.Conv2DTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 717, in decorated\r\n    f(self, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 1842, in Test\r\n    self._CompareFwdValues(input_size, filter_size, [stride, stride], padding)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/conv_ops_test.py\", line 252, in _CompareFwdValues\r\n    self.assertAllClose(values[0], values[i], rtol=1e-5, atol=1e-5)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1368, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1338, in _assertAllCloseRecursive\r\n    path_str))\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/conv_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1273, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-05, atol=1e-05\r\nMismatched value: a is different from b.\r\n(mismatch 26.2006802721%)\r\n x: array([[[[  9.935687,   9.715093,   9.417497,  10.535542,   9.895369,\r\n            8.248978],\r\n         [ 12.098586,  12.769324,  11.302494,  11.481708,  13.940086,...\r\n y: array([[[[  9.933954,   9.714816,   9.417538,  10.534116,   9.897067,\r\n            8.247837],\r\n         [ 12.099165,  12.76971 ,  11.302752,  11.482008,  13.93998 ,...\r\n\r\n----------------------------------------------------------------------\r\nRan 72 tests in 22.143s\r\n\r\nFAILED (failures=1)\r\n\r\n```\r\n\r\n### Source code / logs\r\n[conv_ops_test.log](https://github.com/tensorflow/tensorflow/files/2063282/conv_ops_test.log)", "comments": ["Sorry for the delay. Thanks for taking this on.", "Unable to recreate using the new CI/CD process of calling ci_build.sh using the latest master branch of Tensorflow code.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "I think I meant to close it 22 days ago when it wasn't recreating. Anyways, I just double checked and it is passing now. Will close.\r\n\r\n```\r\nTarget //tensorflow/python/kernel_tests:conv_ops_test up-to-date:\r\n  bazel-bin/tensorflow/python/kernel_tests/conv_ops_test\r\nINFO: Elapsed time: 590.123s, Critical Path: 414.38s\r\nINFO: 4125 processes: 4125 local.\r\nINFO: Build completed successfully, 4345 total actions\r\n//tensorflow/python/kernel_tests:conv_ops_test                           PASSED in 111.8s\r\n  Stats over 4 runs: max = 111.8s, min = 88.9s, avg = 97.6s, dev = 8.6s\r\n  WARNING: //tensorflow/python/kernel_tests:conv_ops_test: Test execution time (88.9s excluding execution overhead) outside of range for LONG tests. Consider setting timeout=\"moderate\" or size=\"medium\".\r\n  WARNING: //tensorflow/python/kernel_tests:conv_ops_test: Test execution time (94.4s excluding execution overhead) outside of range for LONG tests. Consider setting timeout=\"moderate\" or size=\"medium\".\r\n  WARNING: //tensorflow/python/kernel_tests:conv_ops_test: Test execution time (95.2s excluding execution overhead) outside of range for LONG tests. Consider setting timeout=\"moderate\" or size=\"medium\".\r\n  WARNING: //tensorflow/python/kernel_tests:conv_ops_test: Test execution time (111.8s excluding execution overhead) outside of range for LONG tests. Consider setting timeout=\"moderate\" or size=\"medium\".\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 4345 total actions\r\n```\r\n"]}, {"number": 19692, "title": "ppc64le: //tensorflow/python/kernel_tests:atrous_conv2d_test test fails", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python/kernel_tests:atrous_conv2d_test\r\n\r\n\r\n\r\n### Describe the problem\r\n```\r\n----------------------------------------------------------------------\r\nRan 4 tests in 14.613s\r\n\r\nFAILED (failures=1)\r\natrous_conv2d gradient err = 0.000759065\r\natrous_conv2d gradient err = 0.000683963\r\natrous_conv2d gradient err = 0.000596046\r\nnot close where =  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]), array([0, 1, 1, 2, 2, 3, 4, 5, 7, 8, 9, 9, 0, 0, 0, 1, 0, 1]), array([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]))\r\nnot close lhs =  [   3.    3.   13.   10.   30.   30.  113.  113.  221.  275.  310.  410.\r\n   19.   93.  163.  165.  181.  183.]\r\nnot close rhs =  [   2.38105154    2.80899453   12.87652969   10.06372643   29.96479416\r\n   30.14238167  113.12899017  113.32045746  221.52029419  274.54296875\r\n  310.39041138  410.5213623    19.19488144   93.29485321  161.81001282\r\n  164.79013062  181.79176331  183.26371765]\r\nnot close dif =  [ 0.61894846  0.19100547  0.12347031  0.06372643  0.03520584  0.14238167\r\n  0.12899017  0.32045746  0.52029419  0.45703125  0.39041138  0.5213623\r\n  0.19488144  0.29485321  1.18998718  0.20986938  0.79176331  0.26371765]\r\nnot close tol =  [ 0.00338105  0.00380899  0.01387653  0.01106373  0.0309648   0.03114238\r\n  0.114129    0.11432046  0.22252031  0.27554297  0.3113904   0.41152138\r\n  0.02019488  0.09429486  0.16281003  0.16579014  0.18279177  0.18426372]\r\ndtype = float32, shape = (2, 13, 13, 2)\r\n================================================================================\r\nTarget //tensorflow/python/kernel_tests:atrous_conv2d_test up-to-date:\r\n  bazel-bin/tensorflow/python/kernel_tests/atrous_conv2d_test\r\nINFO: Elapsed time: 67.037s, Critical Path: 46.28s\r\nINFO: Build completed, 1 test FAILED, 3 total actions\r\n//tensorflow/python/kernel_tests:atrous_conv2d_test                      FAILED in 1 out of 2 in 16.1s\r\n  Stats over 2 runs: max = 16.1s, min = 4.3s, avg = 10.2s, dev = 5.9s\r\n  WARNING: //tensorflow/python/kernel_tests:atrous_conv2d_test: Test execution time (4.3s excluding execution overhead) outside of range for MODERATE tests. Consider setting timeout=\"short\" or size=\"small\".\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/python/kernel_tests/atrous_conv2d_test/shard_1_of_2/test.log\r\n\r\nExecuted 1 out of 1 test: 1 fails locally.\r\n```\r\n\r\n\r\n### Source code / logs\r\n\r\n[atrous_conv2d_test.log](https://github.com/tensorflow/tensorflow/files/2063256/atrous_conv2d_test.log)\r\n\r\n", "comments": ["@sandipmgiri any idea?", "Currently I'm working on CPU support only for TF-master, and this test is passing successfully without any error on Ubuntu:16.04(ppc64le).  Need to check for GPU support.", "Nagging Assignee @drpngx: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@wdirons looks like we cannot repro. Closing."]}, {"number": 19691, "title": "tf.image.convert_image_dtype(image, tf.float32) does not normalize output properly", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary via pip\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nIn docs: `This op converts between data types, scaling the values appropriately before casting.`\r\nYes, we can not scale uint8 and then cast, we will loose all the information\r\nBut this function is counterintuitive in a way that we can convert from float to uint and get ready-to-display result (e.g. in Tensorboard) but we can't do same to covert when reading images as input to our model\r\nIs that a correct behaviour?\r\n\r\n### Source code / logs\r\n```\r\nimage_gt = tf.image.decode_image(image_parsed[\"image\"], channels=3)\r\nimage_gt = tf.image.convert_image_dtype(image_gt, tf.float32, saturate=False)\r\nimage_gt = tf.Print(image_gt, [tf.reduce_max(image_gt)], \"image_gt_cast\")\r\n```\r\n\r\nResult of execution:\r\n```\r\nimage_gt_cast[250.892319]\r\nimage_gt_cast[244.243454]\r\nimage_gt_cast[245.468872]\r\nimage_gt_cast[258.527466]\r\nimage_gt_cast[270.992615]\r\nimage_gt_cast[227.410767]\r\n```\r\n", "comments": ["Could you please explain what result your expect in your example? Scale `[0, 255]` (unit8) to `[0, 1]` (float) ?", "@facaiy yes I am expecting scaling of values as written in the docs\r\n```\r\nImages that are represented using floating point values are expected to have values in the range [0,1). Image data stored in integer data types are expected to have values in the range [0,MAX], where MAX is the largest positive representable number for the data type.\r\n\r\nThis op converts between data types, scaling the values appropriately before casting.\r\n```\r\nI looked at the source of this function and there is some scaling-related code, but I don't shure that it works at all. Is this intended behavior of this OP? As it looks to me it allows at best only to get rid of `tf.cast(to.clip_by_value(image, 0.0, 1.1), tf.uint8)` when converting to integers with `saturate=True`\r\nBTW converting from float to uint8 gave me black images (0, 1) -> (0,255) without scaling result in all zeros.\r\nAnd there in some inconsistency between tensorboard's image float range and this function (and probably all funcs in tf.image, tensorboard's float images are in (-1,1) range and tf.image works with (0,1).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@drpngx Could you comment or reassign?", "Reopening since there's still active interest.", "@facaiy My apologies for letting this fall through the cracks. \r\n\r\n@drpngx, @jart Can you help here?\r\n", "Sorry for the delay. @Luonic I don't understand what the problem is. Could you elaborate maybe? Give a small numerical example?", "Hi @drpngx! I have presented example in first post. \r\nI will try to explain:\r\n As we read in docs: \r\n> Images that are represented using floating point values are expected to have values in the range [0,1).\r\n\r\nSo when we do \r\n```\r\nimage_gt = tf.image.decode_image(image_parsed[\"image\"], channels=3)\r\nimage_gt = tf.image.convert_image_dtype(image_gt, tf.float32, saturate=False)\r\n```\r\nwe expect result to be in 0..1 range because `tf.image.decode_image` result is tf.uint8 tensor and `tf.image.convert_image_dtype` has a `tf.float32` as target dtype param\r\n\r\nand when we do `image_gt = tf.Print(image_gt, [tf.reduce_max(image_gt)], \"image_gt_cast\")`\r\nwe expect maximum value of converted tensor to be 1.0\r\nBut instead we get unscaled values\r\n```\r\nimage_gt_cast[250.892319]\r\nimage_gt_cast[244.243454]\r\nimage_gt_cast[245.468872]\r\nimage_gt_cast[258.527466]\r\nimage_gt_cast[270.992615]\r\nimage_gt_cast[227.410767]\r\n```\r\nin example above maximum values differ because i were feeding different images\r\nI hope this clarified this problem for you", "Can you print `image_gt.dtype` and `image_gt.dtype.max`?\r\n\r\nHere's what I did:\r\n```\r\nx = tf.constant([1, 2, 255], dtype=tf.uint8)\r\nsess.run(tf.convert_image_dtype(x, dtype=tf.float32))\r\n```\r\nand I'm getting (as expected):\r\n```array([0.00392157, 0.00784314, 1.        ], dtype=float32)```\r\n\r\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19690, "title": "The tensorflow become dead when I use nccl.all_sum", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:ubuntu14.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.3.1\r\n- **Python version**: python3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI try to use nccl to transfer data between gpus, but I can not run the programme although my 'print' tell\r\nme 'Tensor(\"NcclAllReduce_1:0\", shape=(32, 35, 35, 192), dtype=float32, device=/device:GPU:1)\r\n'\r\n### Source code / logs\r\nthis is my code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef network(x):\r\n    with tf.device('/gpu:1'):\r\n        x1 = tf.zeros(shape=[32, 35, 35, 192])\r\n    with tf.device('/gpu:0'):\r\n        x0= tf.reshape(x, [-1, 35, 35, 192])\r\n        #x1 = tf.contrib.nccl.broadcast(x0, ['/gpu:1'])\r\n        #print(type(x1[1][0]))\r\n        #return x1\r\n        #return x1\r\n        (x_temp, x_temp1) = tf.contrib.nccl.all_sum([x0,x1])\r\n        print(x_temp1)\r\n        result = tf.identity(x_temp1)\r\n        return result\r\n\r\ndef main(_):\r\n    x = np.random.ranf(size=[32, 35, 35, 192])\r\n    x = x.astype('float32')\r\n    y_pre = network(x)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(y_pre)\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run(main=main)\r\n```\r\nthis is my log:\r\nTensor(\"NcclAllReduce_1:0\", shape=(32, 35, 35, 192), dtype=float32, device=/device:GPU:1)\r\n2018-06-01 21:16:36.078103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.80GiB\r\n2018-06-01 21:16:36.197439: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x4741160 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-06-01 21:16:36.198248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:02:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.80GiB\r\n2018-06-01 21:16:36.323103: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x47455a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-06-01 21:16:36.323733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:03:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.80GiB\r\n2018-06-01 21:16:36.456854: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x47499e0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2018-06-01 21:16:36.457447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:04:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.80GiB\r\n2018-06-01 21:16:36.461138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3\r\n2018-06-01 21:16:36.461152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y\r\n2018-06-01 21:16:36.461171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y\r\n2018-06-01 21:16:36.461177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y\r\n2018-06-01 21:16:36.461181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y\r\n2018-06-01 21:16:36.461189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\n2018-06-01 21:16:36.461195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)\r\n2018-06-01 21:16:36.461201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\r\n2018-06-01 21:16:36.461205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)\r\n", "comments": ["Hi luanyunteng, what version of CUDA and NCCL are you using?\r\n\r\nHave you tried running other NCCL ops. I'm trying to figure out if only this particular op is affected, or if it's a general problem with you NCCL setup.", "Nagging Assignee @chsigg: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The all-sum documentation mentions that you need to evaluate all outputs, otherwise it will hang. You are not evaluating x_temp.\r\n\r\nAlso, all_sum should not be called under a device context. All (GPU) devices of the inputs automatically participate in this op.\r\n\r\n"]}, {"number": 19689, "title": "fix typo", "body": "fix typo", "comments": []}, {"number": 19688, "title": "why new defined op always runs on PS host, not in worker host?", "body": "i have refactored word2vec code\uff08from models/tutorials/embedding/\uff09, my problem is new defined OP \"train_op = word2vec.neg_train_word2vec \"  runs on PS,  I don't know where I went wrong.  my code :           \r\nwith tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                                                      cluster=cluster, ps_strategy=ps_strategy, merge_devices=False)):\r\n\r\n            # Declare all variables we need.\r\n            embedding = tf.get_variable(name=\"word_embedding\", shape = [71291,FLAGS.embedding_size],\r\n                        initializer = tf.random_uniform_initializer(-0.5 / FLAGS.embedding_size, 0.5 / FLAGS.embedding_size))\r\n\r\n            weights =   tf.get_variable(name=\"word_weights\",shape = [71291, FLAGS.embedding_size],\r\n                        initializer = tf.random_normal_initializer() )\r\n\r\n\r\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n\r\n            # The training data\r\n            ( words,\r\n              counts,\r\n              words_per_epoch,\r\n              current_epoch,\r\n              total_words_processed,examples,\r\n              labels) = word2vec.skipgram_word2vec(filename=FLAGS.train_data,\r\n                                           batch_size=FLAGS.batch_size,\r\n                                           window_size=FLAGS.window_size,\r\n                                           min_count=FLAGS.min_count,\r\n                                           subsample=FLAGS.subsample)\r\n\r\n            # Linear learning rate decay.\r\n            words_to_train = tf.cast(words_per_epoch * FLAGS.epochs_to_train, tf.float32)\r\n            lr = FLAGS.learning_rate * tf.maximum(\r\n                        0.0001,\r\n                        1.0 - tf.cast(total_words_processed, tf.float32) / words_to_train)\r\n\r\n            train_op = word2vec.neg_train_word2vec(embedding,\r\n                                          weights,\r\n                                          examples,\r\n                                          labels,\r\n                                          lr,\r\n                                          #vocab_count=counts.tolist(),\r\n                                           num_negative_samples=FLAGS.num_neg_samples)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution: linux   centos7.1\r\nTensorFlow installed from: pip install tensorflow\r\nTensorflow version: 1.7.0 cpu-version\r\nCUDA: 9.0\r\ncuDNN:7.2\r\nGPU:P40 memory:22G", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19687, "title": "incompatible with eager and graph when output of tf.keras.Model is a dictionary", "body": "env: ubuntu18.04 python2.7 tensorflow1.8\r\n```   \r\n import tensorflow as tf\r\n tf.enable_eager_execution()\r\n class Model(tf.keras.Model):\r\n        def __init__(self):\r\n            super(Model, self).__init__()\r\n\r\n        def call(self, inputs):\r\n            return {\"inputs\": inputs}\r\n m = Model()\r\n m(tf.random_uniform((2, 2)))\r\n```\r\nthe code runs smoothly, but when I comment ` tf.enable_eager_execution()`, it raises error,  the compatibility between eager and graph is quite important, please have a see and fix it", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: yes\r\nOS Platform and Distribution: ubuntu18.04\r\nTensorFlow installed from: conda\r\nTensorFlow version: 1.8.0\r\nBazel version: None\r\nCUDA/cuDNN version: None, cpu version\r\nGPU model and memory: None\r\nExact command to reproduce: see above code", "@fchollet @brainnoise - Mind taking a look?", "Hello @zakizhou, I believe this issue has been fixed by https://github.com/tensorflow/tensorflow/commit/693b339ab2f062ec5bbb29f976c5d1fd94fbffa5 . Can you please try this in tf-nightly?", "@brainnoise Will this feature be released in version 1.9?, if so, when?", "Yes, it will be in 1.9 which will be available in the next few days."]}, {"number": 19686, "title": "feature request: tf.string_split cannot split more than twice, when the maxlengths are not same", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.7\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN7\r\n- **GPU model and memory**:  NVIDIA Quadro K620\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI used Dataset API to process NLP task.\r\nIn input function, I have a string col, which contains three kinds of  delimiter '\\1', '\\2', ' '.\r\nFor example, the string is a ducument, the '\\1' is used to split passages and '\\2' is used to split sentecnes and ' ' is used to split tokens(words).\r\nI want to split it into a 3-D tensor, whose shape may be [None,None,None], and before the data is truly read, these three numbers cannot be determined.\r\n\r\nFirstly, I try the following code to directly call tf.string_split three times:\r\n```python \r\n#parsed_line[4] is the string col, Tensor(\"DecodeCSV:5\", shape=(), dtype=string)\r\nparsed_line[4]=tf.string_split([parsed_line[4]],delimiter='\\2').values\r\nparsed_line[4]=tf.string_split(parsed_line[4],delimiter='\\1').values\r\nparsed_line[4]=tf.string_split(parsed_line[4],delimiter=' ')\r\nparsed_line[4]=tf.sparse_tensor_to_dense(parsed_line[4], default_value = \"\")\r\n#after processing, parsed_line[4] is Tensor(\"SparseToDense_2:0\", shape=(?, ?), dtype=string)\r\n```\r\nHowever, I want to get [None,None,None], but not [None,None], because it loss the information which passage the sencences are belong to.\r\n\r\nSecondly, I try tf.map_fn as follows:\r\n```python\r\nparsed_line[4]=tf.string_split([parsed_line[4]],delimiter='\\2').values\r\nparsed_line[4]=tf.string_split(parsed_line[4],delimiter='\\1')\r\nparsed_line[4]=tf.SparseTensor(parsed_line[4].indices,tf.map_fn(tf.string_split,[parsed_line[4].values]),parsed_line[4].dense_shape)\r\nparsed_line[4]=tf.sparse_tensor_to_dense(parsed_line[4], default_value = \"\")\r\n```\r\nThen it will throw out the following errors:\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 179, in <module>\r\n    first_batch = sess.run(binput.input_fn(r'D:\\Datadump\\LSAT\\debug_data\\test_tsv.tsv','train'))\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 109, in train_input\r\n    .map(lambda x: decode_tsv_indexing(x)))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 838, in map\r\n    return MapDataset(self, map_func)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1826, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 488, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 321, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 338, in _create_definition_if_needed_impl\r\n    outputs = self._func(*inputs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1791, in tf_map_func\r\n    ret = map_func(nested_args)\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 109, in <lambda>\r\n    .map(lambda x: decode_tsv_indexing(x)))\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 54, in decode_tsv_indexing\r\n    parsed_line[4]=tf.SparseTensor(parsed_line[4].indices,tf.map_fn(tf.string_split,[parsed_line[4].values]),parsed_line[4].dense_shape)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 413, in map_fn\r\n    swap_memory=swap_memory)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3202, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2940, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2877, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 404, in compute\r\n    nest.assert_same_structure(dtype or elems, packed_fn_values)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 267, in assert_same_structure\r\n    _recursive_assert_same_structure(nest1, nest2, check_types)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 193, in _recursive_assert_same_structure\r\n    \"First structure: %s\\n\\nSecond structure: %s.\" % (nest1, nest2))\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: [tf.string]\r\n\r\nSecond structure: SparseTensor(indices=Tensor(\"map/while/StringSplit:0\", shape=(?, 2), dtype=int64), values=Tensor(\"map/while/StringSplit:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"map/while/StringSplit:2\", shape=(2,), dtype=int64)).\r\n\r\nProcess finished with exit code 1\r\n```\r\nI think it is caused by the lengths (the last dimensionality) of each sentence are different.\r\n\r\nTo valid my guess, I split this col to 5 cols in the input file, and I run tf.string_split on each of them to get [None,None] tensor, and finally, I try to stack them int one tensor [None,None,None]\r\n```python\r\nfor i in range(4,9):\r\n    parsed_line[i] = tf.string_split([parsed_line[i]], delimiter='\\1').values\r\n    parsed_line[i] = tf.string_split(parsed_line[i])\r\n    parsed_line[i] = tf.sparse_tensor_to_dense(parsed_line[i], default_value=\"\")\r\nparsed_line[4]=tf.stack(parsed_line[4:9])\r\n```\r\nYes, it throws an error:\r\n```\r\n2018-06-01 17:41:31.412421: W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:891 : Invalid argument: Shapes of all inputs must match: values[0].shape = [2,31] != values[1].shape = [2,35]\r\n\t [[Node: stack = Pack[N=5, T=DT_STRING, axis=0](SparseToDense_2, SparseToDense_3, SparseToDense_4, SparseToDense_5, SparseToDense_6)]]\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1312, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1420, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [2,31] != values[1].shape = [2,35]\r\n\t [[Node: stack = Pack[N=5, T=DT_STRING, axis=0](SparseToDense_2, SparseToDense_3, SparseToDense_4, SparseToDense_5, SparseToDense_6)]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?], [?,5,?,?], [?,?], [?,?,?], [?,?], [?]], output_types=[DT_STRING, DT_STRING, DT_STRING, DT_STRING, DT_STRING, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n\r\n```\r\nWhen truly read the input data, two sentences has 31 tokens and 35 tokens, respectively.\r\nFinally, I try to pad them dynamically as follows:\r\n```python\r\nfor i in range(4,9):\r\n    parsed_line[i] = tf.string_split([parsed_line[i]], delimiter='\\1').values\r\n    parsed_line[i] = tf.string_split(parsed_line[i])\r\n    parsed_line[i] = tf.sparse_tensor_to_dense(parsed_line[i], default_value=\"\")\r\n\r\nshape_list=[]\r\nfor i in range(4, 9):\r\n    tmp=tf.convert_to_tensor(parsed_line[i].shape,dtype=None)\r\n    shape_list.append(tmp)\r\nshape_stack=tf.stack(shape_list)\r\nmaxs=tf.reduce_max(shape_stack,0)\r\nfor i in range(4,9):\r\n    paddings=tf.stack([tf.constant([0,0],dtype='int32'),maxs-parsed_line[i].shape])\r\n    parsed_line[i]=tf.pad(parsed_line[i],paddings,\"CONSTANT\")\r\nparsed_line[4]=tf.stack(parsed_line[4:9])\r\n```\r\nThis code throws the following erros:\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 175, in <module>\r\n    first_batch = sess.run(binput.input_fn(r'D:\\Datadump\\LSAT\\debug_data\\test_tsv.tsv','train'))\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 105, in train_input\r\n    .map(lambda x: decode_tsv_indexing(x)))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 838, in map\r\n    return MapDataset(self, map_func)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1826, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 488, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 321, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 338, in _create_definition_if_needed_impl\r\n    outputs = self._func(*inputs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1791, in tf_map_func\r\n    ret = map_func(nested_args)\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 105, in <lambda>\r\n    .map(lambda x: decode_tsv_indexing(x)))\r\n  File \"D:/PythonProjects/PhillyNNFramework/p_data_func/LSATInput.py\", line 71, in decode_tsv_indexing\r\n    tmp=tf.convert_to_tensor(parsed_line[i].shape,dtype=None)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 950, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1040, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 256, in _tensor_shape_tensor_conversion_function\r\n    \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (?, ?)\r\n```\r\n\"Cannot convert a partially known TensorShape to a Tensor: (?, ?)\"\r\nThus, I don't know whether there is other methods to solve my problems.\r\n\r\nThanks.", "comments": ["Solve it by changing x.shape to tf.shape(x), and I close the issue. Sorry for trouble."]}, {"number": 19685, "title": "bazel build libtensorflow_inference.so failed", "body": "I try to put this `TensorFlow` to Unity for Android. I built the aar file in Android Studio. But I got below error.<br>\r\n\r\n> 06-01 16:38:34.341  4137  4154 E AndroidRuntime: Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'All' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime:   <no registered kernels>\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \t [[Node: assert_equal/All = All[Tidx=DT_INT32, keep_dims=false](assert_equal/Equal, assert_equal/Const)]]\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.Session.run(Native Method)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.Session.access$100(Session.java:48)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.Session$Runner.runHelper(Session.java:285)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.Session$Runner.run(Session.java:235)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:142)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.recognizeImage(TensorFlowObjectDetectionAPIModel.java:158)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:751)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:95)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:154)\r\n06-01 16:38:34.341  4137  4154 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\nDo anyone know how to resolve it?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: NO\r\nOS Platform and Distribution: MacOS\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.8\r\nBazel version: Build label: 0.13.1-homebrew\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: `bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a`\r\n\r\nI just know I don't build the `jar` file for Android.\r\nSo I try to follow the instruction to build the `libtensorflow_inference.so`. I got a error when I execute below command.\r\n\r\n> ERROR: /private/var/tmp/_bazel_manlokwong/dc8d33585d6d4874e40cf9ad36bf2389/external/protobuf_archive/BUILD:265:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Exit 126)\r\n/bin/bash: bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/js_embed: cannot execute binary file\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "@andrehentz - any insight on how to build this package?", "I am also seeing similar kind of issues. Can someone help.\r\n $ pip3 list | grep -i protobuf\r\n protobuf 3.5.1\r\n Bazel version is 0.15.2\r\n tf1.8\r\n OS: Ubuntu16.04.4\r\n Python : 3.5.2\r\n\r\nINFO: Found 1 target...\r\n ERROR: /home/taccuser/.cache/bazel/_bazel_taccuser/cbc64142ddf66660b02ff669605f4d7f/external/protobuf_archive/BUILD:265:1: Executing genrule @protobuf_archive//:generate_js_well_known_types_embed failed (Aborted): bash failed: error executing command\r\n (cd /home/taccuser/.cache/bazel/_bazel_taccuser/cbc64142ddf66660b02ff669605f4d7f/execroot/org_tensorflow && \r\n exec env - \r\n LD_LIBRARY_PATH=/usr/local/lib:/usr/local/mpi/lib:/usr/local/lib:/usr/local/mpi/lib: \r\n PATH=/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \r\n /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/protobuf_archive/js_embed external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/host/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc'): bash failed: error executing command\r\n (cd /home/taccuser/.cache/bazel/_bazel_taccuser/cbc64142ddf66660b02ff669605f4d7f/execroot/org_tensorflow && \r\n exec env - \r\n LD_LIBRARY_PATH=/usr/local/lib:/usr/local/mpi/lib:/usr/local/lib:/usr/local/mpi/lib: \r\n PATH=/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/mpi/bin:/home/taccuser/bin:/home/taccuser/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \r\n /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/protobuf_archive/js_embed external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/host/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc')\r\n /bin/bash: line 1: 15045 Aborted (core dumped) bazel-out/host/bin/external/protobuf_archive/js_embed external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/any.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/struct.js external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types/timestamp.js > bazel-out/host/genfiles/external/protobuf_archive/src/google/protobuf/compiler/js/well_known_types_embed.cc\r\n Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n INFO: Elapsed time: 1.329s, Critical Path: 0.65s\r\n INFO: 12 processes: 12 local.\r\n FAILED: Build did NOT complete successfully\r\n", "Please reopen if this is still an issue, I haven't observed this in more recent builds."]}, {"number": 19684, "title": "how can i import tensorflow lite into eclipse java project?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Thanks for trying TF Lite. It should be possible to import TF Lite into Eclipse but that's not a configuration we actively support."]}, {"number": 19683, "title": "Transform_Graph feed value for  bool placeholder", "body": "### System Information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** MacOS 10.13.3\r\n- **TensorFlow installed from (source or binary):** source\r\n- **TensorFlow version (use command below):** r1.8\r\n- **Python version:** 2.7\r\n- **Bazel version (if compiling from source):** 0.13\r\n- **GCC/Compiler version (if compiling from source):** Apple LLVM version 9.1.0 (clang-902.0.39.1)\r\n- **CUDA/cuDNN version:** version 8.0\r\n- **GPU model and memory:** No\r\n- **Exact command to reproduce:** `bazel build tensorflow/tools/graph_transforms:transform_graph`\r\n\r\n\r\n### Describe the problem\r\nFold the batchnorm of [Facenet](https://github.com/davidsandberg/facenet#Pre-trained).\r\n\r\n### Source Code/ Logs\r\n```bash\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \r\n--in_graph=\"/Users/kit/Downloads/20180408-1029002/20180408-102900.pb\"\r\n --out_graph=\"../optimize_graph.pb\" \r\n--inputs=\"input\" \r\n--outputs=\"InceptionResnetV1/Logits/Flatten/flatten/Reshape\"\r\n --transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms'\r\n```\r\n### Error:\r\n```bash\r\n2018-06-01 12:51:11.118630: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying strip_unused_nodes\r\n2018-06-01 12:51:11.344986: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying remove_nodes\r\n2018-06-01 12:52:04.712395: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying fold_constants\r\n2018-06-01 12:52:05.068919: E tensorflow/tools/graph_transforms/transform_graph.cc:279] fold_constants: Ignoring error You must feed a value for placeholder tensor 'phase_train' with dtype bool\r\n\t [[Node: phase_train = Placeholder[dtype=DT_BOOL, shape=<unknown>]()]]\r\n2018-06-01 12:52:05.201658: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying fold_batch_norms\r\n2018-06-01 12:52:05.509836: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying fold_old_batch_norms\r\n```\r\nI use the tensorboard to see the graph and I `phase_train` is the input to every `slim.batchnorm`.\r\n![image](https://user-images.githubusercontent.com/20907377/40823628-9adb611c-65a3-11e8-8b0a-1f6414718460.png)\r\n\r\nMy Question is how can I feed the value for this bool placeholder in bazel.", "comments": ["Nagging Assignee @jart: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "same issue\r\n```\r\n# transform_graph  --in_graph='freezed_graph.pb' --outputs='cond_2/final_score' --out_graph='test.pb' --transforms='fold_constants(ignore_errors=true)'\r\n2018-08-30 15:56:35.647989: I tensorflow/tools/graph_transforms/transform_graph.cc:264] Applying fold_constants\r\n2018-08-30 15:56:36.710932: E tensorflow/tools/graph_transforms/transform_graph.cc:279] fold_constants: Ignoring error You must feed a value for placeholder tensor 'train_flag' with dtype bool\r\n\t [[Node: train_flag = Placeholder[dtype=DT_BOOL, shape=[]]()]]\r\n```\r\n"]}, {"number": 19682, "title": "Unify learning rate as normal Tensor for tf.contrib.layers.optimize_loss", "body": "In ```tf.contrib.layers.optimize_loss```, we handle ```learning_rate``` differently. \r\n* If it's a Tensor, then keep intact.\r\n* If it's a float, then we wrap it as TF Variable, thus be stored in checkpoint.\r\n\r\nIt will throw exception if we train a model with constant learning rate(for example ```0.2```), save to checkpoint, and then continue train with a Tensor learning rate(for example ```tf.constant(0.2)```). This exceptions was reported at (here](https://github.com/tensorflow/tensor2tensor/issues/809)\r\n\r\nMeanwhile, I think we don't need to store ```learning rate``` in checkpoint. If the checkpoint is used for inference only, no learning rate is needed. If the checkpoint is used for further training, we specify constant learning rate, or learning rate decay function(which takes constant ```learning_rate``` and ```global_step``` variable as input).\r\n\r\nFurther more, if we export ```learning rate``` as variable, when continuing training a model with constant ```learning rate```, we can't change it to different value. But there is need to use a new ```learning rate``` to continue training.\r\n\r\nTo sum up, I would suggest to unify ```learning rate``` as normal Tensor regardless of it's Tensor or float input. I'm glad to hear TF core developers' opinion. Thanks.\r\n\r\n", "comments": ["In general, we prefer [`tf.contrib.training.create_train_op`](https://www.tensorflow.org/api_docs/python/tf/contrib/training/create_train_op). There, you simply create the optimizer you need, no separate argument for the learning rate, and you do what you need to do. \r\n\r\nI'm reluctant to change the behavior of this, we should deprecate this function instead."]}, {"number": 19681, "title": "Dockerfiles need to be updated", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- OS: Ubuntu16.04 in Nvidia-Docker container with image `tensorflow/tensorflow:latest-gpu-py3`\r\n\r\n### Describe the problem\r\nThe Nvidia has upgraded their apt-repo sources lists files to `https` but the source-list files depended in tensorflow are still old one(with http). So, when we rebuild the image, the issue occurs like this:\r\n```shell\r\nErr:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg\r\n  The following signatures were invalid: NODATA 1  NODATA 2\r\nGet:10 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [637 kB]\r\nGet:12 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]\r\nGet:13 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]\r\nGet:11 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release.gpg [691 B]\r\nErr:11 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release.gpg\r\n  The following signatures were invalid: NODATA 1  NODATA 2\r\n```\r\nFollowing the lists file in the up-to-date `nvidia/cuda:9.0-base-ubuntu16.04` image, I tried to update the sources lists files in `/etc/apt/sources.list.d/` and it was solved:  \r\nIn `cuda.list`:  \r\n```\r\ndeb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /\r\n```\r\nIn `nvidia-ml.list`:\r\n```\r\ndeb https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 /\r\n```\r\n Probably you need to rebuild and update your docker images.\r\n", "comments": ["Come across this git through googling!!!!! AND IT WORKS! THANK YOU!!!!! \ud83d\ude04 ", "@yifeif Is there any operation to solve this? Here is the [solution](https://github.com/NVIDIA/nvidia-docker/issues/571#issuecomment-357750971)  from nvidia and all you need to do is to rebuild your images with the newest nvidia-docker images.", "Thanks @ArvinSiChuan! We will be updating the latest docker tags once 1.9.0 official is out. For now could you try 1.9.0-rc1-gpu-py3? It should be built after nvidia updated their base image to use https.", "@yifeif Thanks for the solution but I'm afraid that our application may not support 1.9.0 or there may be some unknown issues because we haven't tested it. The problem occurs when we build a image based on yours so if you can update your images ASAP(or at least a schedule for our further updating) then it will be great. ", "NVIDIA has made the non-ssl links work again because this caused a very widespread outage.\r\nAre you still  running into this problem?", "@gunan We were handling this in the docker build process by our own. We may not turn to the non-ssl one and we'd like to wait for next update building from tensorflow. And @yifeif , you will rebuild all the old images(such as 1.7-gpu-py3 etc.) using the updated base image, right\uff1f\r\n\r\n------  \r\nUPDATE  \r\n@gunan I've just tested building without https, the problem seems still there:  \r\n```shell\r\nErr:11 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release.gpg\r\n  The following signatures were invalid: NODATA 1  NODATA 2\r\nErr:12 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release.gpg\r\n  The following signatures were invalid: NODATA 1  NODATA 2\r\nGet:13 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.7 kB]\r\nGet:14 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [450 kB]\r\nGet:15 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3735 B]\r\nGet:16 http://cn.archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]\r\nGet:17 http://cn.archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]\r\nGet:18 http://cn.archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [176 kB]\r\nGet:19 http://cn.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1028 kB]\r\nGet:20 http://cn.archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [13.1 kB]\r\nGet:21 http://cn.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [825 kB]\r\nGet:22 http://cn.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [18.8 kB]\r\nGet:23 http://cn.archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [5157 B]\r\nGet:24 http://cn.archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [8088 B]\r\nReading package lists...\r\nE: GPG error: http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release: The following signatures were invalid: NODATA 1  NODATA 2\r\nE: GPG error: http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release: The following signatures were invalid: NODATA 1  NODATA 2\r\nError executing command, exiting\r\n```", "If you have your own docker build process, you can always update your dockerfiles to remove the non-ssl repos and add the ssl repos manually before you run any apt commands.\r\nOur policy is to only update previous releases in the case of critical bugs.\r\nAs I can clearly see a workaround here, I think we will not update our previous releases."]}, {"number": 19680, "title": "Branch 198811639", "body": "", "comments": ["There were merge conflicts with this pull in...\r\n\r\ntensorflow/tools/api/generator/create_python_api.py  \r\ntensorflow/tools/api/generator/BUILD\r\n\r\nI think I resolved them properly.  ", "Thanks @case540 ", "Really not sure about changes to the layout_optimizer_test.py. Trying to figure out what caused them now.\r\n\r\nHere is potential test fix, but I just changed test to pass. Not sure what caused the difference to the Add op name.\r\nhttps://github.com/tensorflow/tensorflow/pull/19680/commits/1acaca5c2b033f2d51f7d2e97da0511b04420f1d", "+ @zhangyaobit to confirm changes to layout_optimizer_test.py are sensible", "Thanks @case540!"]}, {"number": 19678, "title": "Manual roll back of PR #19443, because it causes the Raspberry Pi build to fail", "body": "", "comments": []}, {"number": 19677, "title": "Inconsistent calculation of complex derivatives", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0 and 1.8.0\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  the problem is in both cpu and gpu tensorflow\r\n- **GPU model and memory**: nvidia 1080 ti\r\n- **Exact command to reproduce**: see the code below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI run a simple code in tensorflow 1.7.0 and then in 1.8.0, which calculates gradients of a function of complex variables and I got a very strange behaviour especially in tensorflow 1.8.0. See the code and the output. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx1 = tf.placeholder(shape=(), dtype=tf.complex64, name='x1')\r\nx2 = tf.placeholder(shape=(), dtype=tf.complex64, name='x2')\r\n\r\nx1_value = np.complex(1.0, 2.0)\r\nx2_value = np.complex(4.0, 1.0)\r\n\r\nsess = tf.Session()\r\n\r\ninit_op = tf.global_variables_initializer()\r\n\r\nsess.run(init_op)\r\n\r\nx1c = tf.conj(x1)\r\nx2c = tf.conj(x2)\r\n\r\ny1 = 2*x1*tf.log(x2c) \r\ny2 = 2*x1c*tf.log(x2)\r\ny = y1 + y2\r\n\r\n# Print the partial derivative of y evaluated at x1_value and x2_value\r\nprint('tf one sess.run', sess.run(tf.conj(tf.gradients(y, x1, tf.constant(1, dtype=tf.complex64))), feed_dict={x1:x1_value, x2:x2_value}))\r\n\r\n# Print the partial derivative of y1 evaluated at x1_value and x2_value + the partial derivative of y2 evaluated at x1_value and x2_value\r\nprint('tf, two sess.run', sess.run(tf.conj(tf.gradients(y1, x1, tf.constant(1, dtype=tf.complex64))), feed_dict={x1:x1_value, x2:x2_value}) + sess.run(tf.conj(tf.gradients(y2, x1, tf.constant(1, dtype=tf.complex64))), feed_dict={x1:x1_value, x2:x2_value}))\r\n\r\nprint('np, hand computed partial derivative', 2*np.log(np.conj(x2_value))  + 2*np.log(x2_value))\r\n\r\n# Output\r\nTensorflow 1.7.0\r\ntf one sess.run [5.6664267-0.97991467j]\r\ntf, two sess.run [5.6664267-0.97991467j]\r\nnp, hand computed partial derivative (5.66642668811+0j)\r\n\r\nTensorflow 1.8.0\r\ntf one sess.run [5.6664267-0.97991467j]\r\ntf, two sess.run [5.6664267+0.j]\r\nnp, hand computed partial derivative (5.66642668811+0j)\r\n```", "comments": ["Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@alextp, do you have any insight into complex derivativres?", "Indeed this looks like a real bug", "Do note though that your hand-computed partial derivative is wrong; you forgot to conjugate the second term because you use x1c in the computation of y2 and not x1. Once you fix it to be the correct one you get the same gradient as tf 1.7 and as tf 1.8 with a single call to session.run. \r\n", "Also I ran on tf 1.9 and did not reproduce your problem (all numbers match). I'm closing this now as, even if there was a bug, it's already been fixed."]}, {"number": 19676, "title": "Error trying to build for macOS with GPU support: \"no toolchain corresponding to 'local_darwin' found for cpu 'darwin' \"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 'High Sierra' Version 10.13.4 (17E202)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: N/A, attempting to compile at e365deab1333005c8aa186632f160c1bfd4485f8 with minimal local changes (see below)\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: 0.13.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)\r\n- **CUDA/cuDNN version**: cuda_9.2.64_mac with cuda_9.2.64.1_mac, cudnn-9.2-osx-x64-v7.1\r\n- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory, CUDA Compute Capability 3.0\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n./configure\r\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nThis is a re-occurrence of #9072, except that the solutions mentioned there (not using clang as the CUDA compiler, using CommandLineTools) do not resolve the problem.\r\n\r\nTo configure, I selected the following:\r\n```\r\nYou have bazel 0.13.1-homebrew installed.\r\nPlease specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:  \r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: \r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: \r\nApache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2\r\n\r\n\r\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]3.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n\r\nTo build, I try:\r\n\r\n```\r\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nHowever, this results in the error:\r\n\r\n```\r\nStarting local Bazel server and connecting to it...\r\n............\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'\r\nINFO: Elapsed time: 0.903s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n```\r\n\r\nThe output of `xcode-select -p` is:\r\n\r\n```\r\n/Library/Developer/CommandLineTools\r\n```\r\n\r\nThe output of `/usr/bin/gcc --version` is:\r\n\r\n```\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\nI am able to build & run the 'deviceQuery' CUDA SDK sample without issue.\r\n\r\n### Source code / logs\r\nThe only local changes from e365deab1333005c8aa186632f160c1bfd4485f8 I have are:\r\n\r\n```diff\r\ndiff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\nindex a561d918bd..46c91b4511 100644\r\n--- a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n@@ -69,7 +69,7 @@ __global__ void concat_variable_kernel(\r\n   IntType num_inputs = input_ptr_data.size;\r\n \r\n   // verbose declaration needed due to template\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char smem[];\r\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\r\n \r\n   if (useSmem) {\r\ndiff --git a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\nindex 5390222b3a..fcbd733614 100644\r\n--- a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\n+++ b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\n@@ -172,7 +172,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNHWCSmall(\r\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\r\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\r\n   // Holds block plus halo and filter data for blockDim.x depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\n@@ -452,7 +452,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNCHWSmall(\r\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\r\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\r\n   // Holds block plus halo and filter data for blockDim.z depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\n@@ -1118,7 +1118,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNHWCSmall(\r\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\r\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.z));\r\n   // Holds block plus halo and filter data for blockDim.x depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\n@@ -1388,7 +1388,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNCHWSmall(\r\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\r\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.x));\r\n   // Holds block plus halo and filter data for blockDim.z depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\ndiff --git a/tensorflow/core/kernels/split_lib_gpu.cu.cc b/tensorflow/core/kernels/split_lib_gpu.cu.cc\r\nindex 393818730b..58a1294005 100644\r\n--- a/tensorflow/core/kernels/split_lib_gpu.cu.cc\r\n+++ b/tensorflow/core/kernels/split_lib_gpu.cu.cc\r\n@@ -121,7 +121,7 @@ __global__ void split_v_kernel(const T* input_ptr,\r\n   int num_outputs = output_ptr_data.size;\r\n \r\n   // verbose declaration needed due to template\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char smem[];\r\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\r\n \r\n   if (useSmem) {\r\ndiff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl\r\nindex 2a37c65bc7..43446dd99b 100644\r\n--- a/third_party/gpus/cuda/BUILD.tpl\r\n+++ b/third_party/gpus/cuda/BUILD.tpl\r\n@@ -110,7 +110,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    #linkopts = [\"-lgomp\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\ndiff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD\r\nindex 4cb8380938..d025c4f3aa 100644\r\n--- a/third_party/toolchains/gpus/cuda/BUILD\r\n+++ b/third_party/toolchains/gpus/cuda/BUILD\r\n@@ -115,7 +115,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    #linkopts = [\"-lgomp\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\n```", "comments": ["Just upgraded to Bazel 0.14.0-homebrew and Command Line Tools macOS 10.13 for Xcode 9.1, whose version information is:\r\n\r\n```\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\nUnfortunately, I still experience the \"Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin' \" error.", "To try to simplify the conditions a little, @dtrebbien , can you try to build an unedited version of TensorFlow, with the minimal non-default configuration? Does that work?", "Okay. I stashed my changes, ran `git pull` to fast-forward to d836210e7d7c8bf54676fd4154f40920310cdb27, and configured as follows:\r\n\r\n```\r\nYou have bazel 0.14.0-homebrew installed.\r\nPlease specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: \r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: \r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: \r\nApache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2\r\n\r\n\r\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]3.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n\r\nI used the build command from https://www.tensorflow.org/install/install_sources#build_the_pip_package namely:\r\n\r\n```sh\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n.. but I see the error.", "In the more recent dump, you seem to have bazel 0.14 installed, and in the previous, 0.13. Can you repost the System Information requested in the issue template with the configuration used for the more recent failed install?", "Sure:\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 'High Sierra' Version 10.13.4 (17E202)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: N/A, attempting to compile at d836210e7d7c8bf54676fd4154f40920310cdb27 with no local changes\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: 0.14.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.38)\r\n- **CUDA/cuDNN version**: cuda_9.2.64_mac with cuda_9.2.64.1_mac, cudnn-9.2-osx-x64-v7.1\r\n- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory, CUDA Compute Capability 3.0\r\n\r\n(the three differences are TensorFlow version, Bazel version, and GCC/Compiler version)\r\n\r\nI also tried compiling against Python 3.6.5, and using Clang (both system and Homebrew-built via the 'llvm' formula) to compile CUDA, but neither resolves the issue.", "@gunan -- seems to be a build compatibility issue on MacOS. Any ideas here?", "GPUs on MacOS are not supported by us anymore, until apple and nvidia start fully supporting this setup.\r\nIn the meantime, here is a duplicate.\r\nhttps://github.com/tensorflow/tensorflow/issues/9072\r\n\r\nPlease feel free to contribute fixes if you can get the build to work, I will be happy to review and merge the changes.", "Got v1.8.0 working with cuda 9.2 and cudnn 7.1, followed steps in this [link](https://gist.github.com/74th/31eacbbac6351649caa417b19231f09e).\r\n\r\nThis is the [wheel](https://drive.google.com/open?id=11ZCmUC-Daxs548O1VD857pUA0r7UpP00) I built. (Note, I did not get nccl working, as I am not using multi-GPU I commented \"import nccl\" out after installing)", "Trying git bisect, I found that the error was introduced with 6e6dcf4ed62324069631cdcd23e7187ae75f6340\r\n\r\nOn macOS `/usr/bin/gcc` is a frontend for Clang. I am not sure if the referenced changes to `-pie` are going to apply.", "Thanks @dtrebbien! Just reverting that commit though I'm getting link errors :(", "Ah, that's because TensorFlow doesn't seem to build with Xcode 7.3 anymore, but it still works fine with Xcode 8.3. For reference, here is my collection of patches and workarounds to build for Mac with CUDA that work with TensorFlow 1.9.0-rc0:\r\nhttps://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/cppbuild.sh#L131", "@saudet I was able to build with Xcode 9.4.1 installed. If you are seeing something like Symbol not found: _ncclAllReduce, try to copy `nccl.h` from Bazel's temp directories to `tensorflow/contrib/nccl/kernels`; see [this comment](https://gist.github.com/74th/31eacbbac6351649caa417b19231f09e#gistcomment-2626681).", "I believe this is relevant to CUDA version somehow, I am observing same problem on Debian: #19840 with CUDA 9.2", "@gunan this is not isolated to Mac OS, it is a regression. I am using 1.8.0 with CUDA 9.2 on Debian, however I can not make it work with 1.9.0.", "apologies but indeed bug #19840 is different and unrelated to this one"]}, {"number": 19675, "title": "Branch 198743117", "body": "", "comments": ["Any idea why all the presubmit tests arent being triggered?", "The xla build link shows that the build passed.\r\nFor pip3, not sure why it is complaining \"TypeError: a bytes-like object is required, not 'str'\". The file_content was read in under binary mode already?", "@case540 do you or @yifeif mind making a new push as this has failed Python3 Pip? Thanks! ", "Is there a fix internally that creating a new push would solve? Otherwise, I'm keeping on trying things to fix this. Also, not sure if there is an easy way to run the pip3 tests locally. Do you know a way @av8ramit ", "I dont think we run this build internally. Quick way to reproduce this locally is probably use virtualenv.", "Ok, closing PR, creating new one to get latest internal CLs. Will add fix."]}, {"number": 19674, "title": "Patch version parsing in cuda_configure in 1.6.", "body": "", "comments": []}, {"number": 19673, "title": "ImportError: cannot import name 'cache'", "body": "when I try to run the inception code for deep learning (pre-learned) on Anaconda3 which have this command line:\r\nfrom cache import cache\r\n\r\nI get this error and I don't know how to solve it.\r\nImportError: cannot import name 'cache'", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "No it is not. Thank you for your concern.\n\nBest Regards,\nAmr Rashed.\n\nOn Mon, Jun 18, 2018 at 3:47 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 16 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19673#issuecomment-397898142>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlpSiuLIyOjOcX2Td9nxUyhdvdRGonyGks5t9qRIgaJpZM4UVSL7>\n> .\n>\n", "How did you solve the error?\r\n"]}, {"number": 19672, "title": "how to train models on different GPUs in parallel?", "body": "hi, all:\r\n    How to train different models on different GPUs in parallel? For example, gpu_0 is responsible for model_0 and gpu_1 is for model_1\r\n    I tried this way:\r\n    with tf.device('/gpu:0'):\r\n         op_0\r\n    with tf.device('/gpu:1'):\r\n         op_1\r\n    but it seems that the models are trained in sequential order, because each time only one GPU-Util info is printed.\r\n    Any advice please?\r\n    Thanks :)", "comments": ["Yeah. but if op_1 is dependent upon op_0 then it's gonna run in sequential. Make sure your model is not dependent upon each to achieve your goal. ", "thanks @achalshah20 I would check the dependencies again", "You can create a staging area. See https://www.tensorflow.org/performance/performance_models#software_pipelining for example\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19671, "title": "Variables may live longer than they suppose to on Eager Execution", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6\r\n\r\n\r\n### Describe the problem\r\n\r\nUsing tensorflow EagerExecution. According to the documentation :\r\n\r\n> During eager execution the lifetime of state objects is determined by the lifetime of their corresponding Python object.\r\n\r\nI have some trouble with how tensorflow handle memory. I would like to remove tensors from my memory after each iteration on this toy example. The results are shown in the chart.\r\n\r\nI have tried with Variables and with simple tensors. tf.assign doesn't do the job. More and more memory is used. It might be normal in order to be able to compute the gradient. But, if I apply some dummy optimizer at the end of each iteration, the memory isn't not released (more precisely, it happens sometimes but the global trend is that the memory use is growing).\r\n\r\nI haven't found any API to deal with that yet.\r\n\r\n\r\n![stackoverflow](https://user-images.githubusercontent.com/38664274/40790405-c17a6b2c-64ec-11e8-8a35-361885acd44a.PNG)\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\nimport time as ti\r\n\r\n\r\ntf.enable_eager_execution()\r\n\r\nfor i in range(150):\r\n    all_subject=tfe.Variable(np.random.rand(200, 500), dtype=tf.float32)\r\n    tf.assign(all_subject, np.random.rand(200,500) )\r\n    ti.sleep(1.0)\r\n    del all_subject\r\n    ti.sleep(0.5)\r\n```", "comments": ["I was looking at a related issue last week and was going to try fixing this one soon. Sorry you ran into it first!\r\n\r\nAt least one reason variable creation in a loop leaks is caching of the resource creation kernels (cached keyed by attributes, which include a unique string). Shouldn't be too hard to fix, either with a no-attribute resource creation op or by ejecting some from the cache.", "Thanks for taking care of the problem. Do you have any news on the topic ?", "If you want a workaround in the meantime, `tf.set_random_seed(1)` will clear the kernel cache.", "Hello @allenlavoie. Was wondering if you had time to look at this by any chance.", "It's still on my list. Happy to explain one approach to fixing it if you (or someone else) wants to pick it up in the meantime (essentially we need to add an \"anonymous\" resource creation op for variables which does not take a shared name or container name as an attribute but generates these itself, like [AnonymousIteratorHandleOp](https://github.com/tensorflow/tensorflow/commit/70674b950ab48f913ed1c99e48c4162287595d46); that kernel will get cached just once across all variable creation).", "@allenlavoie Is this issue fixed? If not, may I work on it?", "It has not been fixed, although I believe @jaingaurav had started on a fix a week or so ago. So you may want to coordinate with him.", "@allenlavoie Thanks for your quick reply! @jaingaurav Please feel free to let me know if there is anything I can do.   ", "Thanks for the help @feihugis. I should have the fix up in the next few days, justing adding tests for it at the moment.", "Just an update for those interested in this issue. Still working on this issue, however upon further investigation, there are an number of additional ops that are demonstrating the same issue. As a result, I'm trying to come up with a more comprehensive fix for all affected ops.", "@jaingaurav Thanks for your updates! I'm very interested in this issue. Could you share more details here? Is it related to https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md?", "@jaingaurav Thanks for your work thus far. I've run into this same issue myself recently on a project. \r\n\r\nIs the fix still in progress?", "Hi, yes this issue was on the back burner for a bit but I'm currently working through an updated change internally.\r\n\r\n@feihugis: There is some relation to the doc you referenced. However, the issue is more specific to eager execution. At https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/resource_variable_ops.py#L409 we generate unique shared names for eager variables. As a result, the kernel cache expands as more and more eager variables are created since we cache based on the attributes. As @allenlavoie mentioned, `tf.set_random_seed(1)` will clear the kernel cache.", "@jaingaurav Thanks for your explanation. The following issues may also be related to the kernel cache: #23882, #23407, and #24084. ", "@feihugis, @julesbayet: Fix has been landed. Thanks for your patience with this fix. Please let me know if you are still seeing the issue after my change.", "Hi, has the update been done on v1.13?\r\nThank you, ", "Hi @araujoalexandre, the fix did not make it in the v1.13 release. It should be in the next release. You can also try the nightlies to verify that it has indeed been fixed.", "Thank you @jaingaurav, I have access to the v1.12 and v1.13, is it possible to fix this issue an other way ? I tried tf.set_random_seed(1) but is not working ", "@araujoalexandre: `tf.set_random_seed(1)` should have addressed this issue. Would you be able to share code? Perhaps there is something else that is causing a memory leak for you.", "@jaingaurav I am try to do gradient descent in a eager py_func, in this function, I need to make several calls to the model. I think every time I make a call a copy of the variables is made.\r\nYou can find a run code here -> https://github.com/araujoalexandre/tf_mem_leak/blob/master/memory_leak.py\r\n\r\nThe memory leak appends in lines 258-290\r\n\r\nYou can run it with :\r\nexport CUDA_VISIBLE_DEVICES='';\r\nmkdir train_folder; python3 memory_leak.py --train_dir ./train_folder/\r\n\r\nI am using tensorflow 1.12\r\nThank you, \r\n", "Just to need to be confirm, `tf.set_random_seed(1)` should be applied within the for-loop, right?", "@XuesongYang: setting `tf.set_random_seed(1)` can be set at anytime to free the memory. I would use the workaround with caution though as it might change the behavior of your program. An alternative would be to use the nightly pip package or wait till the 1.14 release.", "@jaingaurav I'm using tensorflow 2.0.0b1 keras with eager mode on GPU and got similar issue that the memory keep growing till OOM. I can't find tf.set_random_seed() so I tried to use tf.random.set_seed(1) but it didn't work. Any suggestion? Thanks.", "> @jaingaurav I'm using tensorflow 2.0.0b1 keras with eager mode on GPU and got similar issue that the memory keep growing till OOM. I can't find tf.set_random_seed() so I tried to use tf.random.set_seed(1) but it didn't work. Any suggestion? Thanks.\r\n\r\n[`tf.random.set_seed`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/set_seed)", "> > @jaingaurav I'm using tensorflow 2.0.0b1 keras with eager mode on GPU and got similar issue that the memory keep growing till OOM. I can't find tf.set_random_seed() so I tried to use tf.random.set_seed(1) but it didn't work. Any suggestion? Thanks.\r\n> \r\n> [`tf.random.set_seed`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/set_seed)\r\n\r\n@velocirabbit Thanks for the reply. I tried tf.random.set_seed(1) but it didn't work. I use version  \r\n2.0.0b1 with keras, eager mode, GradientTape and GPU. I just tried the same code on the latest tf\r\nversion 2.0.0rc and got the same result where the memory keeps increasing to 10G after 5 epochs. My training data is dataframe and I passed it to each batch by generator. I print the dataframe.info and it is just 200K size. The batch size is 16 and I think it is not big. The model is seq2seq with LSTM. The memory is not released but I don't how to resolve it. ", "@jaingaurav any suggestion about how to free memory on tf2.0 eager mode? I tried different ways these days but have found solution. Many thanks.", "Can confirm tf.random.set_seed(YOUR_NUMBER) works in eager mode for tf2.0. A tedious workaround tho. Have to set seed every iteration. Any updates?"]}, {"number": 19670, "title": "adjust_hsv_in_yiq_op_gpu.cu.pic.o was not created", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.6.5\r\n- **Bazel version (if compiling from source)**:  0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:  7.3.1\r\n- **CUDA/cuDNN version**: 9.1.85/7.1.3\r\n- **GPU model and memory**: GTX 745, 4GB\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n\r\n### Describe the problem\r\nI'm new in programming. I have searched on google but there is no result relating to this.\r\nThanks in advanced\r\n\r\n### Source code / logs\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/type_traits:1544:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-redhat-linux/7/../../../../include/c++/7/tuple:504:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\nERROR: /home/khanh/tensorflow/tensorflow/contrib/image/BUILD:115:1: output 'tensorflow/contrib/image/_objs/python/ops/_distort_image_ops_gpu/tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.pic.o' was not created\r\nERROR: /home/khanh/tensorflow/tensorflow/contrib/image/BUILD:115:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 589.364s, Critical Path: 50.07s\r\nINFO: 2171 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nMore info:\r\nI have compiled Tensorflow 1.6.1 successfully. The difference between 2 versions is Tensorflow 1.7 forwards need Nvidia NCCL that I haven't installed on my PC.", "comments": ["Is there a solution to this yet?\r\n", "@yifeif could you PTAL? ", "@khanh1412 could you try an older version of gcc based on this https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#system-requirements ?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I have encountered a similar problem, any updates to this issue? @khanh1412 ", "For me, the solution was to go back to Ubuntu 16.04, instead of 18.04.", "@Tom-Evers I was using Fedora.", "I'm using GCC 8.3.0 and I have this issue."]}, {"number": 19669, "title": "No module named 'sparse_dot_topn", "body": "Hello all, trying to install the \"sparse_dot_topn\" package in python using pip install sparse_dot_topn in my mac terminal but failing.How else can i go about this?", "comments": ["Try building it. https://github.com/ing-bank/sparse_dot_topn", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Within Mac anvironment you can download the wheel from:\r\nhttps://pypi.org/project/sparse-dot-topn/"]}]