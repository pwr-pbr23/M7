[{"number": 53161, "title": "tf1.14 tensorboard cannot profile multi batch", "body": "My aim: I use tf1.14 keras to train my model, while I find my gpu utilization rate is very lower, so I want to use tensorboard profile to see which stage cost too much. I follow the wiki from tensorflow set profile_batch to 'str' or 'tuple', it show some error.\r\nmy code is below:\r\n`tf.keras.callbacks.TensorBoard(log_dir=FLAGS.get(\"model_path\") + \"logs/\", histogram_freq=1, profile_batch='2, 10')`\r\nwhile it shows error like :\r\n`\r\nFile \"/data/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks_v1.py\", line 365, in on_batch_end\r\n    self._total_batches_seen == self._profile_batch - 1):\r\nTypeError: unsupported operand type(s) for -: 'str' and 'int'\r\n`\r\nuse tuple like \r\n`tf.keras.callbacks.TensorBoard(log_dir=FLAGS.get(\"model_path\") + \"logs/\", histogram_freq=1, profile_batch=(2, 10))`\r\nit show error like :\r\n`TypeError: unsupported operand type(s) for -: 'tuple' and 'int'`\r\n\r\nCan anyone help me?  if I really need to use tf1.14", "comments": ["Hi @arthur-yh! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Could you please try with `tf.keras.callbacks.TensorBoard(log_dir=FLAGS.get(\"model_path\") + \"logs/\", histogram_freq=1, profile_batch=(2, 10))   `   as the Error suggests! Please create an issue in [TF forum ](https://discuss.tensorflow.org/)or Stackoverflow for further assistance !Thanks!", "@mohantym  thanks for helping\uff0cmaybe only higher version support str and tuple form of parameter profile_batch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53161\">No</a>\n"]}, {"number": 53156, "title": "Can't get TF Dataset to work with Keras ImageDataGenerator.flow_from_directory()", "body": "So far I was using a Keras `ImageDataGenerator` with `flow_from_directory()` to train my Keras model with all images from the image class input folders. Now I want to train on multiple GPUs, so it seems I need to use a TensorFlow `Dataset` object.\r\n\r\nThus    I came up with this solution:\r\n\r\n```python\r\nkeras_model = build_model()\r\ntrain_datagen = ImageDataGenerator()\r\ntraining_img_generator = train_datagen.flow_from_directory(\r\n    input_path,\r\n    target_size=(image_size, image_size),\r\n    batch_size=batch_size,\r\n    class_mode=\"categorical\",\r\n)\r\ntrain_dataset = tf.data.Dataset.from_generator(\r\n    lambda: training_img_generator,\r\n    output_types=(tf.float32, tf.float32),\r\n    output_shapes=([None, image_size, image_size, 3], [None, len(image_classes)])\r\n)\r\n# similar for validation_dataset = ...\r\nkeras_model.fit(\r\n    train_dataset,\r\n    steps_per_epoch=train_steps_per_epoch,\r\n    epochs=epoch_count,\r\n    validation_data=validation_dataset,\r\n    validation_steps=validation_steps_per_epoch,\r\n)\r\n```\r\n\r\nNow this seem to work, the model is trained as usual. However, during training I get the following warning message, when using a mirrored strategy:\r\n\r\n> AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset\r\n\r\nSo I added the following lines between creating the data sets and calling `fit()`:\r\n\r\n```python\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\ntrain_dataset.with_options(options)\r\nvalidation_dataset.with_options(options)\r\n```\r\n\r\nHowever, I still get the same warning.  \r\nThis leads me to these two questions:\r\n\r\n1. What do I need to do in order to get rid of this warning?\r\n2. **Even more important**: Why is TF not able to split the dataset with the default `AutoShardPolicy.FILE` policy, since I am using thousands of images per class in the input folder?", "comments": ["@haimat \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "> @haimat Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 Thank you!\r\n\r\nOk thanks, will do."]}, {"number": 53155, "title": "Saving best metrics based on Custom metrics failing (WARNING:tensorflow:Can save best model only with CUSTOM METRICS available, skipping)", "body": "I have defined a callback that runs on the epoch end and calculated the metrics. It is working fine in terms of calculating the desired metrics. Below is the function for reference\r\n\r\n```\r\nclass Metrics(tf.keras.callbacks.Callback):\r\n    def __init__(self, train_tf_data, val_tf_data, model, CLASSES, logs={}, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.train_tf_data = train_tf_data\r\n        self.val_tf_data = val_tf_data\r\n        self.model = model\r\n        self.CLASSES = CLASSES\r\n        # for train data\r\n        self.train_f1_after_epoch = 0\r\n        self.train_prec_after_epoch = 0\r\n        self.train_recall_after_epoch = 0\r\n        # for val data\r\n        self.val_f1_after_epoch = 0\r\n        self.val_prec_after_epoch = 0\r\n        self.val_recall_after_epoch = 0\r\n\r\n    def on_train_begin(self, logs={}):\r\n        self.train_reports = None\r\n        self.val_reports = None\r\n        self.val_f1_after_epoch = 0\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        # for train data\r\n        self.train_reports = test_model(model=self.model, data=self.train_tf_data, \r\n                                        CLASSES=self.CLASSES)\r\n        self.train_f1_after_epoch = self.train_reports['f1_score']\r\n        self.train_recall_after_epoch = self.train_reports['recall']\r\n        self.train_prec_after_epoch = self.train_reports['precision']\r\n\r\n        # for val data\r\n        self.val_reports = test_model(model=self.model, data=self.val_tf_data, \r\n                                      CLASSES=self.CLASSES)\r\n        self.val_f1_after_epoch = self.val_reports['f1_score']\r\n        self.val_recall_after_epoch = self.val_reports['recall']\r\n        self.val_prec_after_epoch = self.val_reports['precision']\r\n\r\n        # saving train results to log dir\r\n        logs[\"train_f1_after_epoch\"]=self.train_f1_after_epoch\r\n        logs['train_precision_after_epoch'] = self.train_prec_after_epoch\r\n        logs['train_recall_after_epoch'] = self.train_recall_after_epoch\r\n        \r\n        # saving val results to log dir\r\n        logs['val_f1_after_epoch'] = self.val_f1_after_epoch\r\n        logs['val_precision_after_epoch'] = self.val_prec_after_epoch\r\n        logs['val_recall_after_epoch'] = self.val_recall_after_epoch\r\n\r\n\r\n        print('train_reports_after_epoch', self.train_reports)\r\n        print('val_reports_after_epoch', self.val_reports)\r\n\r\n```\r\n```\r\n** .....Some model code .....**\r\n```\r\n\r\n## Using this in call back\r\n\r\n```\r\nm1 = tf.keras.metrics.CategoricalAccuracy()\r\nm2 = tf.keras.metrics.Recall()\r\nm3 = tf.keras.metrics.Precision()\r\nm4 = Metrics(train_tf_data=train_data, \r\n             val_tf_data=test_data, model=model, \r\n             CLASSES=CLASS_NAMES)\r\noptimizers = [\r\n        tfa.optimizers.AdamW(learning_rate=lr * .001 , weight_decay=wd),\r\n        tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\r\n\r\n           ]\r\noptimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]\r\n    \r\noptimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\r\n\r\n\r\nmodel.compile(\r\n    optimizer= optimizer,\r\n    loss = 'categorical_crossentropy',\r\n    metrics=[m1, m2, m3],\r\n    )\r\n\r\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \r\n                                                    monitor = 'val_f1_after_epoch',\r\n                                                    save_best_only=True,\r\n                                                    save_weights_only=True,\r\n                                                    mode='max',\r\n                                                    save_freq='epoch',\r\n                                                    verbose=1)\r\n                                                    \r\ncheckpoint_cb._supports_tf_logs = False\r\n```\r\n\r\nThe issue that I am facing is that it is giving me a warning that says\r\n\r\n**WARNING:TensorFlow: Can save best model only with val_f1_after_epoch available, skipping**\r\n\r\nUpon investigating history I found that metrics is available in the history\r\n\r\n```\r\nprint(list(history.history.keys()))\r\n['loss',\r\n'categorical_accuracy',\r\n'recall',\r\n'precision',\r\n'val_loss',\r\n'val_categorical_accuracy',\r\n'val_recall',\r\n'val_precision',\r\n'train_f1_after_epoch',\r\n'train_precision_after_epoch',\r\n'train_recall_after_epoch',\r\n'val_f1_after_epoch', #this is the metrics\r\n'val_precision_after_epoch',\r\n'val_recall_after_epoch']\r\n\r\n```\r\nI think there is a bug in ModelCheckpoint where is it not looking at the custom metrics and not saving the model.\r\n\r\nI am using Tensorflow 2.7 (Also tried this with Tensorflow 2.5)", "comments": ["@ravinderkhatri ,\r\nPlease take a look at this link [1](https://stackoverflow.com/questions/70017229/saving-best-metrics-based-on-custom-metrics-failing-warningtensorflowcan-save/70059354) and [2](https://github.com/tensorflow/tensorflow/issues/33163) with the similar error.It helps.Thanks", "Please post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n", "@tilakrayal  I have tried all the approaches mentioned in the link. Nothing is working. I have created a ticket at the link keras-team/Keras but I was thinking it is related to TensorFlow. Below is the link for reference \r\nhttps://github.com/keras-team/keras/issues/15684", "@ravinderkhatri ,\r\nPlease feel free to close this issue as it has been tracked in Keras repo.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53155\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53155\">No</a>\n"]}, {"number": 53154, "title": "Changed some Grammatical errors in README", "body": null, "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac "]}, {"number": 53153, "title": "tensorflow2.4 can't train 2 model together on 2 different GPU", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10\r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version (use command below): tensorflow-gpu==2.4\r\n- Python version: python3 .8\r\n- CUDA/cuDNN version: cuda11.1 cudnn8.0\r\n- GPU model and memory: RTX2080ti * 2, 12G\r\n\r\n**Describe the current behavior**\r\nmy train code was tensorflow1.13,  use:    \r\n     `import tensorflow.compat.v1 as tf`     \r\n    ` tf.disable_v2_behavior()` \r\nNow it can train in tensorflow2.4.  \r\n\r\nI try to train 2 modeles with different learning_rate and same batchsize on 2 different GPU,   \r\nfirst can run with  ` os.environ['CUDA_VISIBLE_DEVICES'] = '0'`,  \r\nbut secend use `os.environ['CUDA_VISIBLE_DEVICES'] = '1'`, can't run for `out of memory`\r\n\r\n`failed to allocate 9.20G (9874664192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory`", "comments": ["@nistarlwc sir , you can try reducing the batch size of the 2 models in the training loop (until it works) .The reason for this error to pop up is that the the gpu runs out of memory while trying to train the models.If the above method doesn't work try taking a look at this issues : \r\n\r\n- [stack overflow](https://stackoverflow.com/questions/39465503/cuda-error-out-of-memory-in-tensorflow/46021109)\r\n- [datascience stack exchange](https://datascience.stackexchange.com/questions/47073/cuda-error-out-of-memory-out-of-memory-how-to-increase-batch-size)\r\n\r\nIf the error still persists,feel free to notify us.Thanks", "> @nistarlwc sir , you can try reducing the batch size of the 2 models in the training loop (until it works) .The reason for this error to pop up is that the the gpu runs out of memory while trying to train the models.If the above method doesn't work try taking a look at this issues :\r\n> \r\n> * [stack overflow](https://stackoverflow.com/questions/39465503/cuda-error-out-of-memory-in-tensorflow/46021109)\r\n> * [datascience stack exchange](https://datascience.stackexchange.com/questions/47073/cuda-error-out-of-memory-out-of-memory-how-to-increase-batch-size)\r\n> \r\n> If the error still persists,feel free to notify us.Thanks\r\n\r\nI tried it.  \r\ncan run 8 batchsize in GPU0, and can run 8 batchsize in GPU1.\r\nBut when run first model with 8 batchsize in GPU0, secend can't run any batchsize, include 1 batchsize.  \r\nSo I think that this is a bug.\r\n\r\nMay be the problem is:\r\n`2021-11-22 11:46:08.250497: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set`  \r\n`INFO:__main__:GPU was detected...`", "Hi @nistarlwc! Could you please update standalone code in above template to reproduce this issue as  it is bit difficult to find the same from above threads ? Attaching relevant threads for reference.[link1](https://github.com/tensorflow/tensorflow/issues/44683),[link2](https://github.com/tensorflow/tensorflow/issues/44683) . Thanks!", "Problem solved, install cuda11.1.1, and can run it", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53153\">No</a>\n"]}, {"number": 53152, "title": "Fix typo", "body": null, "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac "]}, {"number": 53151, "title": "[MLIR][TF][SCF] {tf.WhileRegion -> scf.while} + optimize region moving", "body": "1. Lowered the `tf.WhileRegion` op to `scf.while` in the\r\n`convert-tf-control-flow-to-scf` pass.\r\n2. Modified the implementation of the moving of single-block regions\r\nsuch that it uses the `mergeBlocks()` function instead of the original\r\n`cloneRegionBefore()` function. This was done because the latter has a\r\nnon-trivial cost and could be dropped due to the currently specific\r\nrequirement of moving (not cloning) single-block regions in the\r\n`convert-tf-control-flow-to-scf` pass.\r\n\r\nSigned-off-by: Srishti Srivastava <srishti.srivastava@polymagelabs.com>", "comments": ["@joker-eph, all your comments are addressed now !", "@joker-eph, is there anything required from my end here?", "Seems like some issues with our infrastructure, can you try to rebase maybe?", "Sure, I'll do that.", "I have rebased and pushed @joker-eph."]}, {"number": 53150, "title": "tf.keras.callbacks.TensorBoard(update_freq=#) doesn't work/is buggy on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n__Yes, but its still very simple__\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n__Windows 11 Home__\r\n- TensorFlow installed from (source or binary):\r\n__just did `pip install tensorflow`__\r\n- TensorFlow version (use command below):\r\n__same behavior on 2.6.1 and 2.7.0__\r\n- Python version:\r\n__3.8__\r\n- CUDA/cuDNN version:\r\n__11.2__\r\n- GPU model and memory:\r\n__NVIDIA GeForce RTX 2060, 6144MiB__\r\n\r\n**Describe the current behavior**\r\nusing `tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=15)` does not correctly log metrics for both training and validation sets every 15 batches, as the docs state it should. Instead, in tensorboard I either see only a plot for the training set and nothing for validation, or most of the time I don't see a plot for either. Additionally, if I do a training run with the `update_freq` argument set, the plots for any previous runs stop appearing in tensorboard when I hit the refresh button. If I then delete the \"bad\" run (the one that had `update_freq` set), my existing plots come back. When I simply remove the `update_freq` argument and just train with `tf.keras.callbacks.TensorBoard(log_dir=log_dir)`, everything works as expected, and I see plots with points at each epoch.\r\n\r\n**Describe the expected behavior**\r\nusing something like `tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=15)` should produce metrics every 15 batches, and I should be able to view a plot of this in tensorboard, just as I can without specifying to log metrics every `update_freq` batches.\r\n\r\nMy code:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport glob\r\nimport datetime\r\nimport pandas as pd\r\n\r\nnum_features = 4000\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(num_features, 1)),\r\n    tf.keras.layers.Conv1D(filters=7, kernel_size=(256,), activation='swish'),\r\n    tf.keras.layers.Conv1D(filters=1, kernel_size=(2048,), activation=None),\r\n    tf.keras.layers.Flatten()\r\n])\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(0.001),\r\n    loss=tf.keras.losses.MeanSquaredError(),\r\n    metrics=[tf.metrics.MeanAbsoluteError()]\r\n)\r\n\r\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ncheckpoint_path = './tf_checkpoints/naive_cnn/{epoch:02d}_{batch:02d}.ckpt'\r\n\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False, update_freq=15)\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_freq=100, verbose=1)\r\ntrain_X = None\r\ntrain_Y = None\r\n\r\nwith tf.device('/device:CPU:0'):\r\n    train_X = tf.signal.frame(allair1, frame_length=num_features, frame_step=1000)\r\n    train_Y = tf.signal.frame(label_data[(num_features - 1698):], frame_length=1698, frame_step=1000)\r\n    \r\ntrain_X = np.expand_dims(train_X, axis=-1)\r\n    \r\nval_X = train_X[:500]\r\nval_Y = train_Y[:500]\r\ntrain_X = train_X[500:5000]\r\ntrain_Y = train_Y[500:5000]\r\n\r\nhistory = model.fit(\r\n    x=train_X,\r\n    y=train_Y,\r\n    validation_data=(train_X, train_Y),\r\n    batch_size=100,\r\n    epochs=10,\r\n    callbacks=[cp_callback, tensorboard_callback],\r\n    verbose=1\r\n)\r\n\r\nhist = pd.DataFrame(history.history)\r\nhist['epoch'] = history.epoch\r\nhist.tail()\r\n```\r\n", "comments": ["@roballsopp This issue is more related to Tensorboard .Could you please open the issue in [Tensorboard repo](https://github.com/tensorflow/tensorboard/issues) ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53150\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53150\">No</a>\n"]}, {"number": 53148, "title": "Implementation of  fuzzy inference systems.", "body": "Im working on tensorflow v2.7.0, I am willing to contribute to the same.\r\n\r\nCurrently there is no implementation of fuzzy inference systems like Sugeno and Tsukamoto networks\r\n\r\nThis feature request will ideally not change the current api. In fact certain existing tensorflow functionality could be reused.\r\n\r\nAll Machine learning engineers familiar with Fuzzy Inference Systems and Soft computing will be able to blend in their ideas based on the same to solve a wider range of problems that involves fuzzy logic. This will in fact help in increasing the evaluation and prediction metrics to higher values for domains involving soft computing.\r\n\r\nThis idea is inspired from the book:  Neuro-fuzzy and soft computing : a computational approach to learning and machine intelligence  (Chuen-Tsai Sun, Eiji Mizutani, and Jyh-Shing Roger Jang)\r\n", "comments": ["Hi @dylan7rdgz ! Could you post this issue in [keras-team/keras repo.](https://github.com/keras-team/keras/issues) , As I  see this feature can be  added to improve prediction metrics and other parameters during model training.\r\n\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53147, "title": "Naman23 coder", "body": null, "comments": []}, {"number": 53145, "title": "Bazel command to get dependency graph fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.7\r\n- Python version:  3.7\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): MSVS 2019\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: NVid\r\n[error.log](https://github.com/tensorflow/tensorflow/files/7575460/error.log)\r\nia Geforce 940MX \r\n\r\n\r\n\r\n**Describe the problem**\r\nCannot generate dependency graph text with:\r\nbazel query --notool_deps --noimplicit_deps \"deps(tensorflow:tensorflow_cc)\" --output graph\r\nThe following errors might be the problem:\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone https://github.com/tensorflow/tensorflow.git .\r\ngit checkout r2.7\r\nAdded startup --output_user_root=C:/tmp   to bazelrc file \r\nbazel query --notool_deps --noimplicit_deps \"deps(tensorflow:tensorflow_cc)\" --output graph\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n=========\r\nF:\\workdir\\units\\unit_1_asa\\ai\\project\\c++\\ml\\dl\\software\\tensorflow\\tf>bazel query --notool_deps --noimplicit_deps \"deps(tensorflow:tensorflow_cc)\" --output graph\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/43d6991c2a4cc2ac374e68c029634f2b59ffdfdf.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Repository llvm-project instantiated at:\r\n  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/tensorflow/tf/WORKSPACE:15:14: in <toplevel>\r\n  F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/llvm-raw/utils/bazel/overlay_directories.py --src F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/llvm-raw --overlay F:/workdir/units/unit_1_asa/ai/project/c++/ml/dl/software/dl-tool/bazel/.bazel/6iwvpnw5/external/llvm-raw/utils/bazel/llvm-project-overlay --target .'\r\nExited with code 1\r\nstdout:\r\n\r\n\r\n[error.log](https://github.com/tensorflow/tensorflow/files/7575461/error.log)\r\n", "comments": ["@dzorwulu ,\r\nLooks like this is duplicate of issue #53109.Can you please close this issue, since it is already being tracked there? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53145\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53145\">No</a>\n", "Similar to error outlined in issue  #53109.\r\n\r\n"]}, {"number": 53142, "title": "Cannot install tensorflow-gpu==2.3.0rc0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04.5 LTS` (Google Colab)\r\n- TensorFlow version: `tensorflow-gpu 2.3.0rc0`\r\n- Python version: `Python 3.7.12`\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n- CUDA/cuDNN version: CUDA Version: `11.2`. Cuda compilation tools, release `11.1`, `V11.1.105`\r\n- GPU model and memory: `NVIDIA Tesla K80`\r\n\r\n**Describe the problem**\r\n\r\nI get the following error when trying to install tensorflow-gpu in version `2.3.0rc0`:\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.3.0rc0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)\r\nERROR: No matching distribution found for tensorflow-gpu==2.3.0rc0\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`pip install tensorflow-gpu==2.3.0rc0`", "comments": ["@jmudy sir, you might have to make sure the tensorflow version you are trying to install is a valid version. The  reason for the above error is that the distribution you are  tryingto install does not exist. I would suggest you take a look at the current gpu distributions that are available [here](https://www.tensorflow.org/install/source#gpu) and install the version that you feel is appropriate for you. Also I would suggest taking a look at the tensorflow [installation section](https://www.tensorflow.org/install) for a smoother and easier installation experience.Feel free to let the community know about the progress on your tensorflow  issue here.Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53142\">No</a>\n"]}, {"number": 53141, "title": "Whether the padding property in the convolution is valid affects network speed.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution :  Android 11\r\n- TensorFlow installation (pip package or built from source): pip installed\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.2.2\r\n- Inference device: SnapDragon 855 GPU and DSP \r\n### 2.  After conversion\r\nAfter post quantized the model, the quantized model runs slower on DSP than fp32 model on GPU. My model's convolution weight is int8, padding property is valid, does valid affect the computational speed of convolution?\r\nI downloaded the mobileNet model from the tensorflow hub and compared it to find that the convolutional property of mobileNet is same.\r\n\r\n", "comments": ["@yahuuu ,\r\nCould you please update TensorFlow to the latest stable version v.2.6 or v2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53141\">No</a>\n"]}, {"number": 53139, "title": "Tensorflow datasets - Eurosat data installation issue - ConnectionError: HTTPConnectionPool(host='madm.dfki.de', port=80): Max retries exceeded with url: /files/sentinel/EuroSATallBands.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out'))", "body": "Hi,\r\n\r\nI have been facing this issue while trying to download Eurosat dataset from Tensorflow datasets API. Is anyone experiencing the same issue since 11/19/2021?\r\n\r\n**Code used in Jupyter notebook:**\r\n\r\nbuilder = tfds.builder(\"eurosat/all\")\r\n\r\n**Error message:**\r\n\r\nDownloading and preparing dataset eurosat (1.93 GiB) to /home/jovyan/tensorflow_datasets/eurosat/all/2.0.0...\r\nDl Completed...: 0 url [00:00, ? url/s]\r\nDl Size...: 0 MiB [00:00, ? MiB/s]\r\nExtraction completed...: 0 file [00:00, ? file/s]\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)\r\n    174             conn = connection.create_connection(\r\n--> 175                 (self._dns_host, self.port), self.timeout, **extra_kw\r\n    176             )\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     95     if err is not None:\r\n---> 96         raise err\r\n     97 \r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)\r\n     85                 sock.bind(source_address)\r\n---> 86             sock.connect(sa)\r\n     87             return sock\r\n\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNewConnectionError                        Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    705                 headers=headers,\r\n--> 706                 chunked=chunked,\r\n    707             )\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)\r\n    393             else:\r\n--> 394                 conn.request(method, url, **httplib_request_kw)\r\n    395 \r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in request(self, method, url, body, headers)\r\n    238             headers[\"User-Agent\"] = _get_default_user_agent()\r\n--> 239         super(HTTPConnection, self).request(method, url, body=body, headers=headers)\r\n    240 \r\n\r\n/opt/conda/lib/python3.7/http/client.py in request(self, method, url, body, headers, encode_chunked)\r\n   1280         \"\"\"Send a complete request to the server.\"\"\"\r\n-> 1281         self._send_request(method, url, body, headers, encode_chunked)\r\n   1282 \r\n\r\n/opt/conda/lib/python3.7/http/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\n   1326             body = _encode(body, 'body')\r\n-> 1327         self.endheaders(body, encode_chunked=encode_chunked)\r\n   1328 \r\n\r\n/opt/conda/lib/python3.7/http/client.py in endheaders(self, message_body, encode_chunked)\r\n   1275             raise CannotSendHeader()\r\n-> 1276         self._send_output(message_body, encode_chunked=encode_chunked)\r\n   1277 \r\n\r\n/opt/conda/lib/python3.7/http/client.py in _send_output(self, message_body, encode_chunked)\r\n   1035         del self._buffer[:]\r\n-> 1036         self.send(msg)\r\n   1037 \r\n\r\n/opt/conda/lib/python3.7/http/client.py in send(self, data)\r\n    975             if self.auto_open:\r\n--> 976                 self.connect()\r\n    977             else:\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in connect(self)\r\n    204     def connect(self):\r\n--> 205         conn = self._new_conn()\r\n    206         self._prepare_conn(conn)\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connection.py in _new_conn(self)\r\n    186             raise NewConnectionError(\r\n--> 187                 self, \"Failed to establish a new connection: %s\" % e\r\n    188             )\r\n\r\nNewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMaxRetryError                             Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    448                     retries=self.max_retries,\r\n--> 449                     timeout=timeout\r\n    450                 )\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    755             retries = retries.increment(\r\n--> 756                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n    757             )\r\n\r\n/opt/conda/lib/python3.7/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\r\n    573         if new_retry.is_exhausted():\r\n--> 574             raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n    575 \r\n\r\nMaxRetryError: HTTPConnectionPool(host='madm.dfki.de', port=80): Max retries exceeded with url: /files/sentinel/EuroSATallBands.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectionError                           Traceback (most recent call last)\r\n<ipython-input-4-d5e677d3f673> in <module>\r\n     13 print(builder)\r\n     14 # Prepare the data.\r\n---> 15 builder.download_and_prepare()\r\n     16 \r\n     17 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in download_and_prepare(self, download_dir, download_config)\r\n    332           self._download_and_prepare(\r\n    333               dl_manager=dl_manager,\r\n--> 334               download_config=download_config)\r\n    335 \r\n    336           # NOTE: If modifying the lines below to put additional information in\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in _download_and_prepare(self, dl_manager, download_config)\r\n   1028     super(GeneratorBasedBuilder, self)._download_and_prepare(\r\n   1029         dl_manager=dl_manager,\r\n-> 1030         max_examples_per_split=download_config.max_examples_per_split,\r\n   1031     )\r\n   1032 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py in _download_and_prepare(self, dl_manager, **prepare_split_kwargs)\r\n    869         prepare_split_kwargs)\r\n    870     for split_generator in self._split_generators(\r\n--> 871         dl_manager, **split_generators_kwargs):\r\n    872       if splits_lib.Split.ALL == split_generator.split_info.name:\r\n    873         raise ValueError(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/image/eurosat.py in _split_generators(self, dl_manager)\r\n    129   def _split_generators(self, dl_manager):\r\n    130     \"\"\"Returns SplitGenerators.\"\"\"\r\n--> 131     path = dl_manager.download_and_extract(self.builder_config.download_url)\r\n    132     path = os.path.join(path, self.builder_config.subdir)\r\n    133     return [\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py in download_and_extract(self, url_or_urls)\r\n    372     with self._downloader.tqdm():\r\n    373       with self._extractor.tqdm():\r\n--> 374         return _map_promise(self._download_extract, url_or_urls)\r\n    375 \r\n    376   @property\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py in _map_promise(map_fn, all_inputs)\r\n    413   \"\"\"Map the function into each element and resolve the promise.\"\"\"\r\n    414   all_promises = utils.map_nested(map_fn, all_inputs)  # Apply the function\r\n--> 415   res = utils.map_nested(_wait_on_promise, all_promises)\r\n    416   return res\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)\r\n    157         return tuple(mapped)\r\n    158   # Singleton\r\n--> 159   return function(data_struct)\r\n    160 \r\n    161 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py in _wait_on_promise(p)\r\n    397 \r\n    398   def _wait_on_promise(p):\r\n--> 399     return p.get()\r\n    400 \r\n    401 else:\r\n\r\n/opt/conda/lib/python3.7/site-packages/promise/promise.py in get(self, timeout)\r\n    510         target = self._target()\r\n    511         self._wait(timeout or DEFAULT_TIMEOUT)\r\n--> 512         return self._target_settled_value(_raise=True)\r\n    513 \r\n    514     def _target_settled_value(self, _raise=False):\r\n\r\n/opt/conda/lib/python3.7/site-packages/promise/promise.py in _target_settled_value(self, _raise)\r\n    514     def _target_settled_value(self, _raise=False):\r\n    515         # type: (bool) -> Any\r\n--> 516         return self._target()._settled_value(_raise)\r\n    517 \r\n    518     _value = _reason = _target_settled_value\r\n\r\n/opt/conda/lib/python3.7/site-packages/promise/promise.py in _settled_value(self, _raise)\r\n    224             if _raise:\r\n    225                 raise_val = self._fulfillment_handler0\r\n--> 226                 reraise(type(raise_val), raise_val, self._traceback)\r\n    227             return self._fulfillment_handler0\r\n    228 \r\n\r\n/opt/conda/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)\r\n    717             if value.__traceback__ is not tb:\r\n    718                 raise value.with_traceback(tb)\r\n--> 719             raise value\r\n    720         finally:\r\n    721             value = None\r\n\r\n/opt/conda/lib/python3.7/site-packages/promise/promise.py in handle_future_result(future)\r\n    842         # type: (Any) -> None\r\n    843         try:\r\n--> 844             resolve(future.result())\r\n    845         except Exception as e:\r\n    846             tb = exc_info()[2]\r\n\r\n/opt/conda/lib/python3.7/concurrent/futures/_base.py in result(self, timeout)\r\n    426                 raise CancelledError()\r\n    427             elif self._state == FINISHED:\r\n--> 428                 return self.__get_result()\r\n    429 \r\n    430             self._condition.wait(timeout)\r\n\r\n/opt/conda/lib/python3.7/concurrent/futures/_base.py in __get_result(self)\r\n    382     def __get_result(self):\r\n    383         if self._exception:\r\n--> 384             raise self._exception\r\n    385         else:\r\n    386             return self._result\r\n\r\n/opt/conda/lib/python3.7/concurrent/futures/thread.py in run(self)\r\n     55 \r\n     56         try:\r\n---> 57             result = self.fn(*self.args, **self.kwargs)\r\n     58         except BaseException as exc:\r\n     59             self.future.set_exception(exc)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow_datasets/core/download/downloader.py in _sync_download(self, url, destination_path)\r\n    229         response = urllib.request.urlopen(request, context=ca_verify['urllib'])\r\n    230     else:\r\n--> 231       response = session.get(url, stream=True)\r\n    232       if response.status_code != 200:\r\n    233         raise DownloadError('Failed to get url %s. HTTP code: %d.' %\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/sessions.py in get(self, url, **kwargs)\r\n    553 \r\n    554         kwargs.setdefault('allow_redirects', True)\r\n--> 555         return self.request('GET', url, **kwargs)\r\n    556 \r\n    557     def options(self, url, **kwargs):\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\r\n    540         }\r\n    541         send_kwargs.update(settings)\r\n--> 542         resp = self.send(prep, **send_kwargs)\r\n    543 \r\n    544         return resp\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/sessions.py in send(self, request, **kwargs)\r\n    653 \r\n    654         # Send the request\r\n--> 655         r = adapter.send(request, **kwargs)\r\n    656 \r\n    657         # Total elapsed time of the request (approximately)\r\n\r\n/opt/conda/lib/python3.7/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    514                 raise SSLError(e, request=request)\r\n    515 \r\n--> 516             raise ConnectionError(e, request=request)\r\n    517 \r\n    518         except ClosedPoolError as e:\r\n\r\nConnectionError: HTTPConnectionPool(host='madm.dfki.de', port=80): Max retries exceeded with url: /files/sentinel/EuroSATallBands.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44a745ec50>: Failed to establish a new connection: [Errno 110] Connection timed out'))", "comments": ["@aashika-varma \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53139\">No</a>\n"]}, {"number": 53138, "title": "Benchmark Operator Profiling with GPU", "body": "Thanks for the great work.\r\n\r\nThe benchmark program on CPU and hexagon delegate shows detailed information on neural net operators.\r\nHowever, GPU delegate shows only `TfLiteGpuDelegateV2` node.\r\n(I believe this is a squashed operator that runs on the GPU.)\r\n\r\nIt is hard to see what's going on inside GPU here.\r\nWould it be possible to see the internal of this operator like CPU and Hexagon? \r\nOr, am I missing anything?\r\n\r\n# CPU\r\n\r\n```adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_xnnpack=false &> 01_cpu_noxnn.txt```\r\n\r\n```\r\n...\r\nNumber of nodes executed: 64\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t                 CONV_2D\t       34\t   266.475\t    75.118%\t    75.118%\t     0.000\t       34\r\n\t       DEPTHWISE_CONV_2D\t       13\t    83.740\t    23.606%\t    98.724%\t     0.000\t       13\r\n\tTFLite_Detection_PostProcess\t        1\t     3.579\t     1.009%\t    99.733%\t     0.000\t        1\r\n\t           CONCATENATION\t        2\t     0.483\t     0.136%\t    99.869%\t     0.000\t        2\r\n\t                LOGISTIC\t        1\t     0.233\t     0.066%\t    99.935%\t     0.000\t        1\r\n\t                 RESHAPE\t       13\t     0.232\t     0.065%\t   100.000%\t     0.000\t       13\r\n```\r\n\r\n# GPU\r\n```adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_gpu=true --use_xnnpack=false &> 03_gpu_noxnn.txt```\r\n\r\n```\r\n...\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     TfLiteGpuDelegateV2\t        1\t    44.928\t    90.775%\t    90.775%\t     0.000\t        1\r\n\tTFLite_Detection_PostProcess\t        1\t     4.566\t     9.225%\t   100.000%\t     0.000\t        1\r\n```\r\n\r\n# Hexagon Delegate\r\n```adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_xnnpack=false --use_hexagon=true --hexagon_profiling=true &> 05_hexagon.txt```\r\n```\r\n...\r\nNumber of nodes executed: 65\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     Supernode_8x8p32to8\t       34\t  4690.936\t    71.401%\t    71.401%\t     0.000\t       42\r\n\tDepthwiseSupernode_8x8p32to8\t       13\t  1551.444\t    23.615%\t    95.016%\t     0.000\t       13\r\n\t         Requantize_8to8\t        1\t   113.790\t     1.732%\t    96.748%\t     0.000\t        2\r\n\t       QuantizedConcat_8\t        1\t    65.657\t     0.999%\t    97.747%\t     0.000\t        1\r\n\t   TfLiteHexagonDelegate\t        1\t    56.703\t     0.863%\t    98.610%\t     0.000\t        1\r\n\t                 Reshape\t       13\t    56.144\t     0.855%\t    99.465%\t     0.000\t       13\r\n\t      QuantizedSigmoid_8\t        1\t    30.509\t     0.464%\t    99.929%\t     0.000\t        1\r\n\tTFLite_Detection_PostProcess\t        1\t     4.651\t     0.071%\t   100.000%\t     0.000\t        1\r\n```", "comments": ["@heeh ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.\r\n", "@tilakrayal Thanks for the response. Here you go!\r\n\r\n- TensorFlow version: 2.8.0\r\n- Ubuntu 20.04\r\n- Bazel version: 3.7.2\r\n- Android version:\r\n  - SDK: 27\r\n  - NDK: r21e\r\n  - Android Build Tools: 28.0.3\r\n\r\nComplete code: \r\n- Build and push benchmark tool\r\n```bash\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\n- Connect your android device. Push the binary to your phone with adb push (make the directory if required):\r\n```bash\r\nadb push bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp\r\n```\r\n\r\n- Make the binary executable.\r\n```bash\r\nadb shell chmod +x /data/local/tmp/benchmark_model\r\n```\r\n\r\n- Execution on CPU\r\n```bash\r\nadb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_xnnpack=false &> 01_cpu_noxnn.txt\r\n```\r\n\r\n- Execution on GPU\r\n```bash\r\nadb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_gpu=true --use_xnnpack=false &> 03_gpu_noxnn.txt\r\n```\r\n\r\n- Execution on Hexagon Delegate\r\n```bash\r\nadb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/detect.tflite --num_threads=4 --enable_op_profiling=true --use_xnnpack=false --use_hexagon=true --hexagon_profiling=true &> 05_hexagon.txt\r\n```\r\n\r\nThe result is described on the original post.\r\nI did not include hexagon setup since my question is about GPU Delegate.\r\nLet me know if you need anything.\r\nThanks!", "@tilakrayal @jvishnuvardhan \r\nAny updates on this issue? \r\nThank you!", "@impjdi, could you help with this issue? Thx!", "The GPU delegate hides away whether you can use OpenGL or OpenCL and does that selection under the hood.  In OpenCL, the APIs allow you to have op-level profiling numbers, but OpenGL doesn't offer this.  Thus, it's not possible to have this op-level profiling data.\r\n\r\nIf you're using OpenCL, you can play around with `//tflite/delgates/gpu/cl/testing/run_performance_profiling.sh` to get some op-level stats."]}, {"number": 53137, "title": "replace private grpc with tensorflow's grpc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos-7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary(pip download & conda install)\r\n- TensorFlow version: 2.3.0\r\n- Python version:3.8.11\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am an undergraduate research from the Ohio State University. My group is focusing on optimizing grpc(v1.38.0). Right now, we want to fills our optimized grpc into tensorflow to increase the performance of distributed trainning. However, the problem is we don't know how to modify the configuration to let tensorflow using our optimized grpc. I would very appreciate your help and please let me know if you need further information. Thank you!\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @TheGreatRandall ! Could you please post this query on [TF Forum](https://discuss.tensorflow.org/) /Stackoverflow as this is not a bug or feature request. Attaching Relevant threads for your reference though.[ link1](https://discuss.tensorflow.org/t/about-how-grpc-works-on-tensorflow/4723/3),[link2](https://rubikscode.net/2020/02/24/deploying-machine-learning-models-pt-3-grpc-and-tensorflow-serving/). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53137\">No</a>\n"]}, {"number": 53135, "title": "AutoGraph could not transform <function my_loss at 0x16bb96f70> and will run it as-is.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip3 installed\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0 (virtualenv)\r\n- Python version: 3.9.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: CPU version\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nPrompt with bug report appears:\r\n\r\n<WARNING:tensorflow:AutoGraph could not transform <function my_loss at 0x16bb96f70> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function my_loss at 0x16bb96f70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: lineno is out of bounds\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function my_loss at 0x16bb96f70> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function my_loss at 0x16bb96f70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: lineno is out of bounds\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n2021-11-19 23:56:35.400148: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)>\r\n\r\n**Describe the expected behavior**\r\nNo bug prompt\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@christopherclarkeSUTD Could you please try with the latest version of TF ` 2.7.0` and let us know the outcome ? Thanks!", "Thanks for the reply. I tried it with 2.7.0 and still got this report request. I'm running a late 2013 iMac by the way.\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function my_loss at 0x16c5b55e0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function my_loss at 0x16c5b55e0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: lineno is out of bounds\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function my_loss at 0x16c5b55e0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function my_loss at 0x16c5b55e0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: lineno is out of bounds\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n", "@christopherclarkeSUTD \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I will see if I can reproduce this issue with some small code snippet, as this is tied to multiple different files, some of which I might not have the permissions for", "@christopherclarkeSUTD \r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53135\">No</a>\n"]}, {"number": 53134, "title": "Fixing version mismatch of Keras and  TF in  2.6", "body": "Editing 2.6.0 as default version for Keras  in TF 2.6 Branch , As it is still throwing \r\n\r\n> AlreadyExistsError: Another metric with the same name already exists.\r\n\r\n error for 2.6 users.[Reference1](https://github.com/tensorflow/tensorflow/issues/53077),[Reference2](https://github.com/tensorflow/tensorflow/issues/52922)", "comments": []}, {"number": 53133, "title": "undefined reference to `tensorflow::str_util::EndsWith\u2019", "body": "On  centos 8\r\nTensorFlow 2.6  installed from source.\r\nPython 3.8.2\r\ngcc version 8.4.1 20200928\r\n\r\n\r\nFirst:  I build libtensorflow  use cmd: bazel build //tensorflow:libtensorflow_cc.so\r\nsecond: I build example: tensorflow/tensorflow/examples/label_image/main.cc \r\n\r\ng++ -g  -std=c++14  -DLINUX -fpermissive  -fPIC -DHAVE_INTTYPES_H -DHAVE_NETINET_IN_H   -I/usr/local/include/ -I/opensource/tf/  -I/usr/local/include/eigen3   -I/opensource/tf/third_party -I /opensource/tf/third_party/eigen3  -c src/main.cc -o src/main.o\r\n\r\nThis is wrong message\uff1a\r\nundefined reference to `tensorflow::str_util::EndsWith(absl::string_view, absl::string_view)'\r\n\r\nundefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'\r\n\r\nundefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::string_view>)'\r\n\r\nhowever, I run cmd:\r\nnm -Ca libtensorflow_cc.so |grep EndsWith\r\nnm -Ca libtensorflow_framework.so\r\n\r\nThe function is in the lib. \r\nI use std=c++11, c++17 ,It doesn't work.\r\nAny help?\r\nThanks.\r\n\r\n \r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53133\">No</a>\n"]}, {"number": 53132, "title": "Non-OK-status: GpuLaunchKernel", "body": "I trained a simple autoencoder for high resolution images.\r\nA part of the code:\r\n\r\nimage_size_x = 1416\r\nimage_size_y = 1440\r\ncolor_channel = 1\r\n\r\nbatch_size=10\r\nepochs = 50\r\ninitial_epoch = 0\r\ndata_augmentation = True\r\n\r\nverbose = 1\r\n\r\nsubtract_pixel_mean = True\r\n\r\nnumber_of_workers = 1\r\n\r\ninput_shape = [image_size_x, image_size_y, 1]\r\n\r\nimage = Input(shape=input_shape)\r\n\r\ndef autoenc(input_img):\r\n  #encoder\r\n  x = Conv2D(kernel_size=(1,1), strides=2, filters =16)(input_img)\r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2D(kernel_size=(2,2), strides=2, filters =32)(x)  \r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2D(kernel_size=(2,2), strides=2, filters =64)(x)\r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2D(kernel_size=(3,3), strides=3, filters =128)(x)\r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n\r\n  #decoder\r\n  x = Conv2DTranspose(kernel_size=(3,3), strides=3, filters =128)(x)\r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2DTranspose(kernel_size=(2,2), strides=2, filters =64)(x)\r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2DTranspose(kernel_size=(2,2), strides=2, filters =32)(x)\r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2DTranspose(kernel_size=(1,1), strides=2, filters =16)(x)    \r\n  x = BatchNormalization()(x)\r\n  x = LeakyReLU(alpha=0.2)(x)\r\n  x = Conv2DTranspose(kernel_size=(1,1), strides=1, filters =1,activation='sigmoid')(x)\r\n  return x\r\n\r\n# Autoencoder\r\nautoencoder = Model(inputs=image, outputs=autoenc(image))\r\nautoencoder.compile(loss='mse', optimizer = 'adam')\r\nautoencoder.summary()\r\n\r\ndatagen = ImageDataGenerator(\r\n    # set input mean to 0 over the dataset\r\n    featurewise_center=False,\r\n    # set each sample mean to 0\r\n    samplewise_center=False,\r\n    # divide inputs by std of dataset\r\n    featurewise_std_normalization=False,\r\n    # divide each input by its std\r\n    samplewise_std_normalization=False,\r\n    # apply ZCA whitening\r\n    zca_whitening=False,\r\n    # epsilon for ZCA whitening\r\n    zca_epsilon=1e-06,\r\n    # randomly rotate images in the range (deg 0 to 180)\r\n    rotation_range=0,\r\n    # randomly shift images horizontally\r\n    width_shift_range=0,\r\n    # randomly shift images vertically\r\n    height_shift_range=0,\r\n    # set range for random shear\r\n    shear_range=0,\r\n    # set range for random zoom\r\n    zoom_range=0,\r\n    # set range for random channel shifts\r\n    channel_shift_range=0,\r\n    # set mode for filling points outside the input boundaries\r\n    fill_mode='nearest',\r\n    # value used for fill_mode = \"constant\"\r\n    cval=0,\r\n    # randomly flip images\r\n    horizontal_flip=False,\r\n    # randomly flip images\r\n    vertical_flip=False,\r\n    # set rescaling factor (applied before any other transformation)\r\n    rescale=1./255,\r\n    # set function that will be applied on each input\r\n    preprocessing_function=None,\r\n    data_format=None,\r\n    # fraction of images reserved for validation (strictly between 0 and 1)\r\n    validation_split=0.2)\r\n\r\nimage_and_labels=pd.read_csv(csv_path)\r\n\r\ntrain_generator=datagen.flow_from_dataframe(\r\n    dataframe=image_and_labels,\r\n    x_col='images1',\r\n    y_col='images1',\r\n    class_mode='input',\r\n    color_mode='grayscale',\r\n    target_size=(image_size_x, image_size_y),\r\n    batch_size=batch_size)\r\n\r\nautoencoder.fit(train_generator, epochs=epochs, initial_epoch=initial_epoch, verbose=verbose, workers=number_of_workers, callbacks=callbacks)\r\n\r\n\r\nTrained on 1800 images, everything works smoothly. I tried to test it on the SAME train dataset, same batch size.. \r\npredict = model.predict(train_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\r\n\r\nand it crushes here\r\n\r\n2021-11-19 11:47:54.978459: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\r\n180/180 [==============================] - 61s 318ms/step\r\n2021-11-19 11:48:54.493204: F tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc:151] Non-OK-status: GpuLaunchKernel( concat_fixed_kernel<T, IntType>, config.block_count, config.thread_per_block, 0, gpu_device.stream(), input_ptrs, split_size, static_cast<int>(output->dimension(0)), static_cast<int>(output->dimension(1)), output->data()) status: Internal: invalid configuration argument\r\n\r\nCan someone please help me?\r\n\r\nUbuntu 20.04.3 LTS\r\ntensorflow 2.6.0-dev20210614\r\npython 3.8.8\r\nCUDA Version: 11.0\r\nGPU A100-PCIE-40GB\r\n", "comments": ["Hi @apres91! Could you try again with CUDA 11.2 and CuDNN 8.1 or Python 3.7. Attaching Relevant threads for reference.[ link1](https://github.com/tensorflow/tensorflow/issues/36310),[link2](https://stackoverflow.com/questions/63258022/non-ok-status-gpulaunchkernel-status-internal-no-kernel-image-is-availab). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53132\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53132\">No</a>\n"]}, {"number": 53131, "title": "Fix fail to build on AARCH64 for mkl_matmul_op_benchmark", "body": null, "comments": ["Fixes https://github.com/tensorflow/tensorflow/issues/53130"]}, {"number": 53130, "title": "//tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark fails to build on AARCH64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n//tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark fails to build\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --cxxopt=-ffp-contract=off --copt=-Og --copt=-ggdb --cxxopt=-Og --cxxopt=-ggdb --verbose_failures -- //tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=161\r\nINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:\r\n  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3\r\nINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc:\r\n  Inherited 'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Reading rc options for 'test' from /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium\r\nINFO: Found applicable config definition build:short_logs in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file /home/builder/1/tensorflow_build/tensorflow-git/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nINFO: Found applicable config definition build:nonccl in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:linux in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/builder/1/tensorflow_build/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/274f12a44c606ecd20152f3e63c4f186793d9a8c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/0e24cef9fbfb6ac93dd326d458b6bae5183aaa4b.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark (3 packages loaded, 1583 targets configured).\r\nINFO: Found 1 test target...\r\nERROR: /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/external/mkl_dnn_v1/BUILD.bazel:166:11: C++ compilation of rule '@mkl_dnn_v1//:mkl_dnn' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/builder/.cache/bazel/_bazel_builder/9dc2dbd69dc3512cedb530e1521082e7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/builder/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-arm64/bin:/home/builder/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/_objs/mkl_dnn/0/gemm.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/_objs/mkl_dnn/0/gemm.o' -iquoteexternal/mkl_dnn_v1 -iquotebazel-out/aarch64-opt/bin/external/mkl_dnn_v1 -isystem external/mkl_dnn_v1/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -w -DAUTOLOAD_DYNAMIC_KERNELS '-ffp-contract=off' -Og -ggdb '-std=c++14' '-ffp-contract=off' -Og -ggdb -fexceptions -UUSE_MKL -UUSE_CBLAS -DDNNL_ENABLE_MAX_CPU_ISA -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/mkl_dnn_v1/src/common/gemm.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn_v1/_objs/mkl_dnn/0/gemm.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_sgemm(char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const float*, dnnl::impl::dim_t, const float*, dnnl::impl::dim_t, float, float*, dnnl::impl::dim_t, void*)':\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:97:5: error: 'threadpool_utils' has not been declared\r\n   97 |     threadpool_utils::activate_threadpool(\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:101:5: error: 'threadpool_utils' has not been declared\r\n  101 |     threadpool_utils::deactivate_threadpool();\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_gemm_u8s8s32(char, char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const uint8_t*, dnnl::impl::dim_t, uint8_t, const int8_t*, dnnl::impl::dim_t, int8_t, float, int32_t*, dnnl::impl::dim_t, const int32_t*, void*)':\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:109:5: error: 'threadpool_utils' has not been declared\r\n  109 |     threadpool_utils::activate_threadpool(\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:113:5: error: 'threadpool_utils' has not been declared\r\n  113 |     threadpool_utils::deactivate_threadpool();\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_gemm_s8s8s32(char, char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const int8_t*, dnnl::impl::dim_t, int8_t, const int8_t*, dnnl::impl::dim_t, int8_t, float, int32_t*, dnnl::impl::dim_t, const int32_t*, void*)':\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:121:5: error: 'threadpool_utils' has not been declared\r\n  121 |     threadpool_utils::activate_threadpool(\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:126:5: error: 'threadpool_utils' has not been declared\r\n  126 |     threadpool_utils::deactivate_threadpool();\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp: In function 'dnnl_status_t dnnl_threadpool_interop_gemm_bf16bf16f32(char, char, dnnl::impl::dim_t, dnnl::impl::dim_t, dnnl::impl::dim_t, float, const dnnl::impl::bfloat16_t*, dnnl::impl::dim_t, const dnnl::impl::bfloat16_t*, dnnl::impl::dim_t, float, float*, dnnl::impl::dim_t, void*)':\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:134:5: error: 'threadpool_utils' has not been declared\r\n  134 |     threadpool_utils::activate_threadpool(\r\n      |     ^~~~~~~~~~~~~~~~\r\nexternal/mkl_dnn_v1/src/common/gemm.cpp:138:5: error: 'threadpool_utils' has not been declared\r\n  138 |     threadpool_utils::deactivate_threadpool();\r\n      |     ^~~~~~~~~~~~~~~~\r\nTarget //tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark failed to build\r\nINFO: Elapsed time: 2.945s, Critical Path: 2.14s\r\nINFO: 71 processes: 57 internal, 14 local.\r\nFAILED: Build did NOT complete successfully\r\n//tensorflow/core/kernels/mkl:mkl_matmul_op_benchmark           FAILED TO BUILD\r\n\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["@cfRod @nSircombe ", "@elfringham This issue will be closed once the PR is merged .Thanks!", "thanks @elfringham !", "@elfringham Could you please let us know if we can close this issue as the PR is merged ?Thanks!", "All good now thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53130\">No</a>\n"]}, {"number": 53129, "title": "How can I modify the layer parameters of Bert model at trainabel_variables?", "body": "tensorflow\uff1a2.2\r\nos\uff1alinux\r\nI want to try to train a 12-layer Robert model and a 6-layer Robert model. In the process of training, I will calculate the average value of some layer parameters of the 12 layer Robert model and all layer parameters of the 6-layer Robert model, so as to see whether this method can distill knowledge\u3002\r\nThis is the code for modifying layer parameters\uff1a\r\n`bert6 = TFBertModel.from_pretrained('uer/chinese_roberta_L-6_H-768', output_hidden_states=True)\r\nbert12 = TFBertModel.from_pretrained('uer/chinese_roberta_L-12_H-768', output_hidden_states=True)\r\n\r\nfor var in bert6.trainable_variables:\r\n    if 'attention' in var.name:\r\n        name = ''.join(var.name.split('/')[3:])\r\n        for var2 in bert12.trainable_variables:\r\n            if 'attention' in var2.name:\r\n                name2 = ''.join(var.name.split('/')[3:])\r\n                if name2 == name:\r\n                    print(\"start var: \", var)\r\n                    var.numpy = (var + var2) / 2\r\n                    print(\"end var: \", var)\r\n                    print(\"(var + var2) / 2: \", (var + var2) / 2)`\r\n\r\n![1637313249(1)](https://user-images.githubusercontent.com/34124260/142597068-858ecb12-2975-49ee-b3bc-0eb6b3d4afae.png)\r\n\r\n\r\nHowever, after checking the status before and after **var** average calculation, I found that it did not update successfully\u3002\r\nSo I want to know if there is any way to solve this problem in tensorflow 2.2?\r\nThank you\u300b\r\n", "comments": ["@1148330040 ,\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This is not a bug. I just want to find a way to manually change the model layer parameters.\r\n\r\n---Original---\r\nFrom: ***@***.***&gt;\r\nDate: Fri, Nov 19, 2021 22:49 PM\r\nTo: ***@***.***&gt;;\r\nCc: ***@***.******@***.***&gt;;\r\nSubject: Re: [tensorflow/tensorflow] How can I modify the layer parameters ofBert model at trainabel_variables? (Issue #53129)\r\n\r\n\r\n\r\n\r\n \r\n@1148330040 ,\r\n Could you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nTriage notifications on the go with GitHub Mobile for iOS or Android.", "@1148330040 ,\r\nKindly open a tf discussion forum issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53128, "title": "@org_tensorflow//tensorflow/lite/toco:toco  build error", "body": "**Hi,\r\nI am trying to build the tensorflow code, When I execute this command\uff1a**\r\n\r\n> bazel build -s --cxxopt=\"-std=c++14\" -c dbg --cxxopt=-msse4 @org_tensorflow//tensorflow/lite/toco:toco  --experimental_repo_remote_exec\r\n\r\n**\uff0c it gives this error:**\r\ntensorflow/tensorflow/core/platform/default/BUILD:147:11: C++ compilation of rule '//tensorflow/core/platform/default:env_time' failed (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 38 argument(s) skipped)\r\ntensorflow/core/platform/default/env_time.cc:26:17: error: use of undeclared identifier 'CLOCK_REALTIME'\r\n  clock_gettime(CLOCK_REALTIME, &ts);\r\n                ^\r\n1 error generated.\r\nTarget //tensorflow/lite/toco:toco failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /Users/xm20200921/project/tf260_tflite/kikaime-engine-model-15-so-tf260/dict_maker/tensorflow/tensorflow/core/framework/BUILD:1279:31 C++ compilation of rule '//tensorflow/core/platform/default:env_time' failed (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 38 argument(s) skipped)\uff0c\r\n\r\nThank you!\r\n", "comments": ["Hi @sanatmpa1! Could you look at this issue?", "**I use this command to solve the problem, but I have a new error.**\r\n\r\n> sudo rm -rf /Library/Developer/CommandLineTools\r\n> xcode-select --install\r\n**the new error as bellow:**\r\n\r\nIn file included from external/eigen_archive/Eigen/Core:15:\r\nexternal/eigen_archive/Eigen/src/Core/util/DisableStupidWarnings.h:48:38: warning: unknown warning group '-Wimplicit-int-float-conversion', ignored [-Wunknown-warning-option]\r\n    #pragma clang diagnostic ignored \"-Wimplicit-int-float-conversion\"\r\n                                     ^\r\nexternal/org_tensorflow/tensorflow/core/platform/status.cc:234:35: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?\r\n  if (state_ == nullptr) state_ = std::make_unique<State>();\r\n                                  ^~~~~~~~~~~~~~~~\r\n                                  absl::make_unique\r\nexternal/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here\r\ntypename memory_internal::MakeUniqueResult<T>::scalar make_unique(\r\n                                                      ^\r\nexternal/org_tensorflow/tensorflow/core/platform/status.cc:234:35: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?\r\n  if (state_ == nullptr) state_ = std::make_unique<State>();\r\n                                  ^~~~~~~~~~~~~~~~\r\n                                  absl::make_unique\r\nexternal/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here\r\ntypename memory_internal::MakeUniqueResult<T>::scalar make_unique(\r\n", "**To deal the error above, I replace 'std::make_unique' with 'absl::make_unique', but when I build,   I have a new error:**\r\nERROR: /private/var/tmp/_bazel_xm20200921/0cbb1a73cb22dbd58b43fa1332cd18ae/external/org_tensorflow/tensorflow/core/platform/default/BUILD:273:11: C++ compilation of rule '@org_tensorflow//tensorflow/core/platform/default:platform_port' failed (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 57 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 57 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/default/port.cc:18:\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/logging.h:20:\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/types.h:21:\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/bfloat16.h:21:\r\nIn file included from external/org_tensorflow/third_party/eigen3/Eigen/Core:1:\r\nIn file included from external/eigen_archive/Eigen/Core:15:\r\nexternal/eigen_archive/Eigen/src/Core/util/DisableStupidWarnings.h:48:38: warning: unknown warning group '-Wimplicit-int-float-conversion', ignored [-Wunknown-warning-option]\r\n    #pragma clang diagnostic ignored \"-Wimplicit-int-float-conversion\"\r\n                                     ^\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc:360:14: error: no matching constructor for initialization of 'tensorflow::port::MemoryInfo'\r\n  MemoryInfo mem_info = {INT64_MAX, INT64_MAX};\r\n             ^          ~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/org_tensorflow/tensorflow/core/platform/mem.h:62:8: note: candidate constructor (the implicit copy constructor) not viable: requires 1 argument, but 2 were provided\r\nstruct MemoryInfo {\r\n       ^\r\nexternal/org_tensorflow/tensorflow/core/platform/mem.h:62:8: note: candidate constructor (the implicit move constructor) not viable: requires 1 argument, but 2 were provided\r\nexternal/org_tensorflow/tensorflow/core/platform/mem.h:62:8: note: candidate constructor (the implicit default constructor) not viable: requires 0 arguments, but 2 were provided\r\nexternal/org_tensorflow/tensorflow/core/platform/default/port.cc:373:23: error: no matching constructor for initialization of 'tensorflow::port::MemoryBandwidthInfo'\r\n  MemoryBandwidthInfo membw_info = {INT64_MAX};\r\n                      ^            ~~~~~~~~~~~\r\nexternal/org_tensorflow/tensorflow/core/platform/mem.h:67:8: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'long long' to 'const tensorflow::port::MemoryBandwidthInfo' for 1st argument\r\nstruct MemoryBandwidthInfo {\r\n       ^\r\nexternal/org_tensorflow/tensorflow/core/platform/mem.h:67:8: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'long long' to 'tensorflow::port::MemoryBandwidthInfo' for 1st argument\r\nexternal/org_tensorflow/tensorflow/core/platform/mem.h:67:8: note: candidate constructor (the implicit default constructor) not viable: requires 0 arguments, but 1 was provided\r\n1 warning and 2 errors generated.\r\nERROR: /private/var/tmp/_bazel_xm20200921/0cbb1a73cb22dbd58b43fa1332cd18ae/external/org_tensorflow/tensorflow/lite/toco/BUILD:406:13 C++ compilation of rule '@org_tensorflow//tensorflow/core/platform/default:platform_port' failed (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 57 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 57 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nINFO: Elapsed time: 11.038s, Critical Path: 8.78s\r\nINFO: 34 processes: 5 internal, 29 darwin-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53128\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53128\">No</a>\n"]}, {"number": 53126, "title": "[oneDNN] Fusing Mul(x , Sigmoid(x)) into Swish(x)", "body": "This PR implements following fusion : \r\nMul ( x , Sigmoid(x))  ---> Swish (x)\r\nCurrently added support for float and bfloat16.\r\n\r\nOneDNN supports swish eltwise kernel.\r\nWith oneDNN backend, this fusion improves performance of some models by 75%.", "comments": ["@penpornk  Thanks for the review comments , I have addressed all the comments in latest commits and can you re-review ?", "@sachinmuradi Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned There were merge conflicts with this PR, I resolved them, can you check now ?", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 53125, "title": "update TensorShape::CheckDimsAtLeast", "body": " The declaration of `CheckDimsAtLeast` states that \r\n> REQUIRES: dims() >= NDIMS", "comments": ["I'm no longer working on TensorFlow; adding Rohan to triage.", "Yes, flipping the condition would make all callers `abort`. However, it seems that  the tensorflow codebase does not call this function, nor did it have a test case about `CheckDimsAtLeast`. Flipping the condition makes the function semantically correct.", "There is at least [one caller](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/framework/tensor_shape.cc;l=50;bpv=1;bpt=1?q=CheckDimsAtLeast&ss=tensorflow%2Ftensorflow:tensorflow%2F) with [one occurence](https://cs.opensource.google/search?q=AsEigenDSizesWithPadding&sq=&ss=tensorflow%2Ftensorflow:tensorflow%2F)", "Sorry, I missed one caller `AsEigenDSizesWithPadding`. The other is the implementation of `CheckDimsAtLeast`\r\n```c++\r\ntemplate <int NDIMS, typename IndexType>\r\nEigen::DSizes<IndexType, NDIMS> TensorShape::AsEigenDSizesWithPadding() const {\r\n  CheckDimsAtLeast(NDIMS);\r\n  return AsEigenDSizesCopyAndPad<NDIMS, IndexType>();\r\n}\r\n````\r\n`AsEigenDSizesWithPadding` requires that `NDIMS` greater than or equal to the tensor's `dims()`. Maybe `CheckDimsAtLeast` could be renamed to `CheckDimsAtMost`, or  the original comments \r\n```c++\r\n // REQUIRES: dims() >= NDIMS\r\n  void CheckDimsAtLeast(int NDIMS) const;\r\n```\r\ncould be revised to\r\n```c++\r\n // REQUIRES: dims() <= NDIMS\r\n  void CheckDimsAtLeast(int NDIMS) const;\r\n```\r\naccording to the current `CheckDimsAtLeast` implementation. \r\nWhich do you think is better? IMHO, function name should reflect its purpose.", "Let's change the name. I looked internally and there are no other users besides the ones in OSS.", "Just renamed `CheckDimsAtLeast`  to `CheckDimsAtMost` and local build passes."]}, {"number": 53124, "title": "tf.cast issue in 2.7.0", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.6.1810\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: Binary\r\n-   **TensorFlow version (use command below)**: 2.7.0\r\n-   **Python version**: 3.9.7\r\n-   **Bazel version (if compiling from source)**: N/A\r\n-   **GCC/Compiler version (if compiling from source)**: N/A\r\n-   **CUDA/cuDNN version**: 11.4.120\r\n-   **GPU model and memory**: NVIDIA Tesla V100 PCIe 32GB\r\n-   **Exact command to reproduce**: tf.cast(tf.ones(1)+65535, tf.uint16)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\ntf.cast with dtype=tf.uint16 seems to be treating values as dtype=tf.int16. This was not the case with TensorFlow 2.6.0 - I updated to 2.7.0 yesterday.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\ntf.cast(tf.ones(1)+65535, tf.uint16)\r\n<tf.Tensor: shape=(1,), dtype=uint16, numpy=array([32767], dtype=uint16)>\r\n```", "comments": ["@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a3d461de9fe0831b009415e434036ec0/untitled121.ipynb).", "I'm able to see the same results in both Tensorflow 2.7 and Tensorflow 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/0adcba1831b73f4d1c112985183b778a/53124.ipynb), Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53124\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53124\">No</a>\n", "This should be reopened if not fixed!"]}, {"number": 53123, "title": "XLA Compilation fails: Non-root tuple types are not handled", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Yes\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11.4.2 / 8.2.4\r\n- GPU model and memory: RTX 2080 Ti 10GB\r\n\r\nYou can collect some of this information using our environment capture\r\n```\r\npython version: 3.8.10\r\npython branch: \r\npython build version: ('default', 'Sep 28 2021 16:10:42')\r\npython compiler version: GCC 9.3.0\r\npython implementation: CPython\r\n\r\ntf.version.VERSION = 2.7.0\r\ntf.version.GIT_VERSION = v2.7.0-0-gc256c071\r\ntf.version.COMPILER_VERSION = 9.3.0\r\n```\r\n\r\n**Describe the current behavior**\r\nWhen running our custom training loop with `TF_XLA_FLAGS=--tf_xla_auto_jit=2`, the following error is obtained\r\n\r\n```\r\n2021-11-19 01:57:34.543515: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:432 : INTERNAL: Non-root tuple types are not handled.\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 140, in <module>\r\n    main(sys.argv[1:])\r\n  File \"train.py\", line 130, in main\r\n    trainer.train(\r\n  File \"/home/bje2lr/venv/cu114/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/bje2lr/venv/cu114/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError:  Non-root tuple types are not handled.\r\n         [[{{node cluster_3_1/xla_compile}}]] [Op:__inference_distributed_train_step_56538]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf_xla_auto_jit=2` should only jit functions that are supported by XLA.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): -\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nUnfortunatelly, uploading the xla log files did not work.\t", "comments": ["Ok, the issue was probably fixed in 1d77ab0bbf030462d15afd2de435676c46b04c7c or a related commit.", "Hi @janbernloehr ! Could you attach sample standalone code to reproduce this issue too? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is what I dig out from my project. Just a standalone code for reproducing, but it maybe my own case.\r\n- This one will throw error `Non-root tuple types are not handled` in `TF 2.7.0` with `TF_XLA_FLAGS=\"--tf_xla_auto_jit=2`\r\n  ```py\r\n  tf.__version__\r\n  # 2.7.0\r\n  xx = tf.random.uniform((1000, 32, 32, 3))\r\n  yy = tf.one_hot(tf.cast(tf.random.uniform((1000,)) * 10, 'int32'), depth=32)\r\n  mm = keras.models.Sequential([keras.layers.Input([32, 32, 3]), keras.layers.Flatten(), keras.layers.Dense(32)])\r\n  mm.compile(optimizer='SGD', loss=\"categorical_crossentropy\", metrics='acc')\r\n  mm.fit(xx, yy)\r\n  # InternalError:  Non-root tuple types are not handled.\r\n  #          [[{{node cluster_0_1/xla_compile}}]] [Op:__inference_train_function_498]\r\n  ```\r\n- Just use output Dense `31` instead of `32` will work normally\r\n  ```py\r\n  tf.__version__\r\n  # 2.7.0\r\n  xx = tf.random.uniform((1000, 32, 32, 3))\r\n  yy = tf.one_hot(tf.cast(tf.random.uniform((1000,)) * 10, 'int32'), depth=31)\r\n  mm = keras.models.Sequential([keras.layers.Input([32, 32, 3]), keras.layers.Flatten(), keras.layers.Dense(31)])\r\n  mm.compile(optimizer='SGD', loss=\"categorical_crossentropy\", metrics='acc')\r\n  mm.fit(xx, yy)\r\n  ```\r\n- Use output Dense `32` without `metrics='acc'` also works\r\n  ```py\r\n  tf.__version__\r\n  # 2.7.0\r\n  xx = tf.random.uniform((1000, 32, 32, 3))\r\n  yy = tf.one_hot(tf.cast(tf.random.uniform((1000,)) * 10, 'int32'), depth=32)\r\n  mm = keras.models.Sequential([keras.layers.Input([32, 32, 3]), keras.layers.Flatten(), keras.layers.Dense(32)])\r\n  mm.compile(optimizer='SGD', loss=\"categorical_crossentropy\")\r\n  mm.fit(xx, yy)\r\n  ```\r\nYes, theses test cases all pass using latest `tf-nightly 2.8.0-dev20211201` and `TF 2.6.2`. I think maybe it's really fixed.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53123\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53123\">No</a>\n", "> Ok, the issue was probably fixed in [1d77ab0](https://github.com/tensorflow/tensorflow/commit/1d77ab0bbf030462d15afd2de435676c46b04c7c) or a related commit.\r\n\r\nI run into the same issue today on TF2.7 and after upgrading to 2.8 it is indeed fixed."]}, {"number": 53122, "title": "Give a more actionable error message when libtpu is already in use.", "body": null, "comments": ["cc: @skye could you please review this?", "This failing check looks unrelated to the changes, how could we pass it?"]}, {"number": 53121, "title": "changing the error message when libtpu is in use", "body": null, "comments": []}]