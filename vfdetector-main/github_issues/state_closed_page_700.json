[{"number": 32587, "title": "tf.contrib.summary.scalar does not record anything outside of always_record_summaries in eager execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Colab builtin\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- GCC/Compiler version (if compiling from source): [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\r\n\r\n**Describe the current behavior**\r\n\r\nhttps://colab.research.google.com/drive/1c1SUEet_jCxvW8D_ZlmOCPJvutB3PhCD\r\nThis code does not record anything in eager execution mode.\r\n\r\n```\r\nwriter = tf.contrib.summary.create_file_writer(os.path.join(logdir, 'contrib'))\r\nwith writer.as_default():\r\n  for i in range(1000):\r\n    if i % 10 == 0:\r\n      tf.contrib.summary.scalar('sin', math.sin(2*math.pi/500), step=i+1)\r\nwriter.close()\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.contrib.summary.scalar` should save records even out side of `always_record_summaries()`  by default.\r\n\r\nIt's very unnatural that we need to call `always_record_summaries` or `record_summaries_every_n_global_steps` to save records in eager execution mode.\r\n\r\nhttps://www.tensorflow.org/guide/eager#summaries_and_tensorboard\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1c1SUEet_jCxvW8D_ZlmOCPJvutB3PhCD", "comments": ["We're no longer making changes to the contrib summary API, but if you try the TF 2.0 `tf.summary` API, it should look similar to contrib, but doesn't require calling `always_record_summaries()` - summaries are on by default.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32587\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32587\">No</a>\n"]}, {"number": 32586, "title": "RNN does not forward the training flag to StackedRNNCells", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0rc0\r\n- Python version: 3.6.6\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `tf.keras.layers.StackedRNNCells` with `tf.keras.layers.RNN`, the RNN layer does not forward the `training` flag to the cell. This is because the RNN code checks that cell explictly defines the `training` flag as argument, which `tf.keras.layers.StackedRNNCells` does not.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py#L709-L710\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `training` flag should be passed to `tf.keras.layers.StackedRNNCells`, and to each stacked cell.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe code below should not raise the `AssertionError`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass CellWrapper(tf.keras.layers.AbstractRNNCell):\r\n\r\n    def __init__(self, cell):\r\n        super(CellWrapper, self).__init__()\r\n        self.cell = cell\r\n\r\n    @property\r\n    def state_size(self):\r\n        return self.cell.state_size\r\n\r\n    @property\r\n    def output_size(self):\r\n        return self.cell.output_size\r\n\r\n    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\r\n        return self.cell.get_initial_state(\r\n            inputs=inputs, batch_size=batch_size, dtype=dtype)\r\n\r\n    def call(self, inputs, states, training=None, **kwargs):\r\n        assert training is not None\r\n\r\n\r\ncell = tf.keras.layers.LSTMCell(32)\r\ncell = CellWrapper(cell)\r\ncell = tf.keras.layers.StackedRNNCells([cell])\r\n\r\nrnn = tf.keras.layers.RNN(cell)\r\ninputs = tf.random.uniform([4, 7, 16])\r\nrnn(inputs, training=True)\r\n```", "comments": ["Was able to reproduce this issue. Please find the attachment of github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/6e1f7a532de56ec863606e7f1657f1c3/untitled144.ipynb). ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32586\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32586\">No</a>\n"]}, {"number": 32585, "title": "Horrible Energy Impact when opening tensorflow.org with Safari", "body": "<img width=\"801\" alt=\"image\" src=\"https://user-images.githubusercontent.com/22335780/65037849-25d16800-d981-11e9-8167-e17cbdae72d0.png\">\r\n\r\nNot sure it appropriate issue this kind problem here, but got nothing with google. \r\nEvery time opening https://www.tensorflow.org, the battery consuming speed is unreasonably high, why?", "comments": ["This question is better suited on Apple Support forums.\r\nSee https://discussions.apple.com/welcome\r\nI was not able to repro the energy consumption as reported. Energy impact was 66.2 for me using Google Chrome browser on macOS Mojave.\r\nPerhaps you can try using other browsers and reach out to the support forums to seek additional help.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32585\">No</a>\n"]}, {"number": 32584, "title": "[Lite] Support Int8 Unpack Operator", "body": "Added support for Unpack Operator\r\nAdded relevant tests.\r\n\r\nSolves issue [#31902](https://github.com/tensorflow/tensorflow/issues/31902)\r\n\r\nChange-Id: I10cfa4901209c3023d1b5e46258d6c280ecf560e", "comments": ["Any updates on this?", "@MohamedNourArm Can you please resolve conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32584) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "@gbaned @suharshs Having resolved the conflicts and 'merge-committed,' I get the CLA check failure above, although I used the same email and username to commit and resolve the merge conflict. Any idea what could be causing this?", "Closing/abandoning PR in favor of https://github.com/tensorflow/tensorflow/pull/33971"]}, {"number": 32583, "title": "tensorflow2 Custom training fine tuning issue", "body": "**I used  ResNet101V2 pretrained model training my dataset.**\r\nBelow is my code:\r\n\r\n```\r\n    batch_size = 16\r\n    buffer_size = 5000\r\n    image_size = 224\r\n    classes = 9\r\n    num_epochs = 200\r\n    learning_rate = 0.000\r\n    train_image_paths, train_image_labels = util.get_image_paths_and_labels(train_dir)\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_image_labels))\r\n    train_dataset = train_dataset.map(\r\n        lambda path, label: weather.load_and_preprocess_image_and_label(path, label, image_size))\r\n    train_dataset = train_dataset.shuffle(buffer_size=buffer_size)\r\n    train_dataset = train_dataset.batch(batch_size=batch_size)\r\n    val_image_paths, val_image_labels = util.get_image_paths_and_labels(val_dir)\r\n    val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_image_labels))\r\n    # val_dataset = weather.load_data(val_tfrecords_path)\r\n    val_dataset = val_dataset.map(\r\n        lambda path, label: weather.load_and_preprocess_image_and_label(path, label, image_size))\r\n    val_dataset = val_dataset.batch(batch_size=batch_size)\r\n    base_model = ResNet101V2(input_shape=(image_size, image_size, 3),\r\n                             include_top=False,\r\n                             weights='imagenet')  # 'imagenet'\r\n    base_model.trainable = True\r\n    \r\n    model = keras.Sequential([\r\n        base_model,\r\n        keras.layers.GlobalAveragePooling2D(),\r\n        keras.layers.Dense(classes, activation='softmax', name='predictions')\r\n    ])\r\n    model.summary()\r\n    \r\n    loss_fn = keras.losses.CategoricalCrossentropy()\r\n    # optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\r\n    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\r\n    \r\n    train_loss = keras.metrics.Mean()\r\n    train_accuracy = keras.metrics.CategoricalAccuracy()\r\n    \r\n    val_loss = keras.metrics.Mean()\r\n    val_accuracy = keras.metrics.CategoricalAccuracy()\r\n\r\n    for epoch in range(num_epochs):\r\n        for step, (images, labels) in enumerate(train_dataset):\r\n            labels = tf.one_hot(labels, depth=classes)\r\n            with tf.GradientTape() as tape:\r\n                y_ = model(images)\r\n                loss = loss_fn(labels, y_)\r\n            grads = tape.gradient(loss, model.trainable_variables)\r\n\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n            train_loss(loss)\r\n            train_accuracy(labels, y_)  # model(images)\r\n            if step % 50 == 0:\r\n                print(\"Step {:03d}: TrainLoss: {:.3f}, TrainAccuracy: {:.3%}\".format(step,\r\n                                                                                     train_loss.result(),\r\n                                                                                     train_accuracy.result()))\r\n        # Display metrics at the end of each epoch\r\n        print(\"Epoch {:03d}: TrainLoss: {:.3f}, TrainAccuracy: {:.3%}\".format(epoch,\r\n                                                                              train_loss.result(),\r\n                                                                              train_accuracy.result()))\r\n        train_loss.reset_states()\r\n        train_accuracy.reset_states()\r\n\r\n        for images, labels in val_dataset:\r\n            labels = tf.one_hot(labels, depth=classes)\r\n            y_ = model(images)\r\n            loss = loss_fn(labels, y_)\r\n            val_loss(loss)\r\n            val_accuracy(labels, y_)\r\n        print(\"Epoch {:03d}: ValLoss: {:.3f}, ValAccuracy: {:.3%}\".format(epoch,\r\n                                                                          val_loss.result(),\r\n                                                                          val_accuracy.result()))\r\n        val_loss.reset_states()\r\n        val_accuracy.reset_states()\r\n```\r\n\r\n\r\nI set base_model.trainable = True, the training result is bad. \r\n\r\n```\r\n    Step 000: TrainLoss: 2.219, TrainAccuracy: 18.750%\r\n    Step 050: TrainLoss: 2.122, TrainAccuracy: 21.446%\r\n    Step 100: TrainLoss: 2.134, TrainAccuracy: 20.854%\r\n    Step 150: TrainLoss: 2.139, TrainAccuracy: 19.826%\r\n    Step 200: TrainLoss: 2.140, TrainAccuracy: 19.652%\r\n    Step 250: TrainLoss: 2.137, TrainAccuracy: 20.045%\r\n    Step 300: TrainLoss: 2.133, TrainAccuracy: 20.328%\r\n    Step 350: TrainLoss: 2.133, TrainAccuracy: 20.495%\r\n    Epoch 005: TrainLoss: 2.134, TrainAccuracy: 20.396%\r\n    Epoch 005: ValLoss: 2.137, ValAccuracy: 19.844%\r\n    2019-09-17 10:38:11.732802: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:143] Filling \r\n    up shuffle buffer (this may take a while): 1707 of 5000\r\n    2019-09-17 10:38:21.722296: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:143] Filling \r\n    up shuffle buffer (this may take a while): 3409 of 5000\r\n    2019-09-17 10:38:31.856501: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:143] Filling \r\n    up shuffle buffer (this may take a while): 4311 of 5000\r\n    2019-09-17 10:38:41.166725: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:193] \r\n    Shuffle buffer filled.\r\n    Step 000: TrainLoss: 2.057, TrainAccuracy: 18.750%\r\n    Step 050: TrainLoss: 2.102, TrainAccuracy: 21.324%\r\n    Step 100: TrainLoss: 2.123, TrainAccuracy: 20.050%\r\n    Step 150: TrainLoss: 2.124, TrainAccuracy: 20.530%\r\n    Step 200: TrainLoss: 2.127, TrainAccuracy: 19.963%\r\n    Step 250: TrainLoss: 2.130, TrainAccuracy: 20.568%\r\n    Step 300: TrainLoss: 2.132, TrainAccuracy: 20.619%\r\n    Step 350: TrainLoss: 2.132, TrainAccuracy: 20.388%\r\n```\r\n\r\nHowever\uff0cI use keras training way\r\n\r\n```\r\n        model.compile(loss='sparse_categorical_crossentropy',\r\n                      optimizer=tf.keras.optimizers.RMSprop(lr=learning_rate),\r\n                      metrics=['accuracy'])\r\n\r\n        model.fit(train_dataset,\r\n                  epochs=num_epochs,\r\n                  validation_data=val_dataset)\r\n```\r\n\r\nthe training result is good. \r\n\r\nI am very confused\uff0canyone help me\uff1f\r\nThanks\uff01", "comments": ["> the training result is good.\r\n> \r\n> I am very confused\uff0canyone help me\uff1f\r\n\r\nPlease let us know what exactly the issue with training. Thanks!\r\n", "> > the training result is good.\r\n> > I am very confused\uff0canyone help me\uff1f\r\n> \r\n> Please let us know what exactly the issue with training. Thanks!\r\n\r\nI fine tuning the model in a custom training way and trained around 30 epochs, the train accuracy always around 20% . But I fine tuning the model in a tf.keras way (compile and fit), the train accuracy is right.", "@lovejing0306,\r\nDid yo go through the custom training in [Tensorflow website](https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough). Thanks! ", "> @lovejing0306,\r\n> Did yo go through the custom training in [Tensorflow website](https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough). Thanks!\r\n\r\nI copy the program from Tensorflow2 website, but it is not work.\r\nCan you help me ? Thanks!", "@lovejing0306,\r\nCan you please provide us the link from where you have copied the code. Thanks!", "> @lovejing0306,\r\n> Can you please provide us the link from where you have copied the code. Thanks!\r\n\r\nI have solved this problem. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32583\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32583\">No</a>\n"]}, {"number": 32582, "title": "[TFLite] Fix for minimum/maximum quantization problem", "body": "Fix for minimum/maximum quantization problem\r\n\r\nProblem description:\r\nMINIMUM and MAXIMUM operations were not quantized properly. Specifically, the problem was that only one of the inputs was quantized while another one was left in the original data type. Because of this reason, TFLite interpreter was failing to prepare since quantization params of ops did not match.\r\n\r\nProblem cause:\r\nMINIMUM and MAXIMUM operators were created in the way that they only had one input, not two. Hence when looping through inputs while quantizing them, only one of two inputs was quantized. \r\n\r\nProblem fix:\r\nChange the definition of the aforementioned properties.\r\n\r\nThis patch contains fixes for the problem described above. \r\n\r\n1. Properties of MINIMUM and MAXIMUM operators were altered to have\r\ntwo inputs rather than one. This is safe since both 1.* and 2.* branches\r\nhave only two inputs for these ops\r\n2. Test suite for testing Minimum and Maximum ops quantization is added\r\n3. Two small binaries have been added for testing purposes", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32582) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32582) for more info**.\n\n<!-- ok -->", "@suharshs, can you please review this PR?", "@konstantinARM Can you please resolve conflicts? Thanks!", "Hi sorry for the delay, this change actual requires a different solution similar to Concat. I have submitted the fix here: https://github.com/tensorflow/tensorflow/commit/481366eab297011fed94ccc599e27825c905a18c", "Hi @suharshs. Thanks for checking this PR. Can you please review the PR #34484 we raised to replace this one? - It extends your fix to cover for the case of inputs with different quantization parameters, and includes the C++ tests to cover both aspects of the problem.", "Sure will reply on that pr, thanks."]}, {"number": 32581, "title": "Need to save tf model in .h5 format", "body": "I have a trained model in ONNX and i have ported that to TF using below code programmaticaly.\r\n(TensorflowRep). (onnx_tf.backend.prepare ).\r\nA TensorflowRep class object representing the ONNX model\r\nonnx_tf.backend_rep.TensorflowRep.export_graph\r\nHow can I save the model to disk?\r\nCan I save this in .h5 format.\r\n\r\n", "comments": ["@Harikrishnah007 Please take a look at this [issue](https://github.com/onnx/onnx-tensorflow/issues/490)  and let me know if it helps. Also create an issue in the [onnx/onnx-tensorflow](https://github.com/onnx/onnx-tensorflow/issues) repo as this issue is more suitable to that repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32581\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32581\">No</a>\n"]}, {"number": 32580, "title": "tf.test.is_gpu_available() and tf.Session() blocking indefinitely", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Nope\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not tried\r\n- TensorFlow installed from (source or binary): don't remember\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1 and driver version 418.56\r\n- GPU model and memory: 2x Nvidia 1080 and 1x Nvidia Titan \r\n\r\n**Describe the current behavior**\r\nI found similar issue #[29273](https://github.com/tensorflow/tensorflow/issues/22730) but for me it never unfreezes the system. Strange that yesterday my docker container was working perfectly fine, and today something strange happened( I guess something happened after reboot?) . This two commands tf.test.is_gpu_available() and tf.Session() are blocking my machine so that I need to restart it. \r\nThe last thing I see in the console is:\r\n```\r\nYour CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\nCPU Frequency: 3499975000 Hz\r\nXLA service 0x559960dc29d0 executing computations on platform Host.Devices:\r\nStreamExecutor device (0):<undefined>, <undefined>\r\nSuccessfully opened dynamic library libcuda.so.1\r\n``` \r\nAlso I tried to make a conda env with tf via `conda create -n tensorflow_env tensorflow` and the problem remains. According to the https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html the drivers are matching, but the weirdest thing is that it was working for quite a while! Any ideas how to debug it?\r\n\r\n**Describe the expected behavior**\r\nNot to block.\r\n\r\n", "comments": ["Wow looks like simple drivers update worked like a magic"]}, {"number": 32579, "title": "ONNX to Keras(.h5 format)", "body": "I have to export trained model from ONNX to Keras. How can i do this?or  How we can convert it to tensorflow & then convert it to .h5 format.", "comments": ["@Harikrishnah007 Please create an issue in the [onnx/onnx-tensorflow](https://github.com/onnx/onnx-tensorflow/issues) repo as this issue is more suitable to that repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32579\">No</a>\n"]}, {"number": 32578, "title": "sparse_softmax_cross_entropy_with_logits fails for symbolic tensors in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n- Python version: 3.6\r\n\r\nThis code:\r\n```\r\ntf.nn.sparse_softmax_cross_entropy_with_logits(\r\n    tf.keras.layers.Input((None,), dtype=tf.int32),\r\n    tf.keras.layers.Input((None,None)))\r\n```\r\n\r\nFails with this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 3477, in sparse_softmax_cross_entropy_with_logits_v2\r\n    labels=labels, logits=logits, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\", line 3410, in sparse_softmax_cross_entropy_with_logits\r\n    array_ops.shape(logits)[:-1]))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/check_ops.py\", line 506, in assert_equal\r\n    if not condition:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 765, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 534, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 523, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n\r\nBut this works:\r\n```\r\ntf.compat.v1.disable_eager_execution()\r\ntf.nn.sparse_softmax_cross_entropy_with_logits(\r\n    tf.keras.layers.Input((None,), dtype=tf.int32),\r\n    tf.keras.layers.Input((None,None)))\r\n```\r\n\r\nSeems similar to #31848, although the underlying issue here is `assert_equal` not faring well with symbolic tensors in eager mode. The issue seems to be from here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d5efc0e9fd431586e4ebd63a9716577f6a59ba02/tensorflow/python/ops/check_ops.py#L330-L334\r\n\r\nHere a tensor is cast to a boolean in eager mode, but that is not possible in the case of a symbolic keras tensor - even though tensorflow is executing eagerly.", "comments": ["I have tried on colab with TF version 2.0.0-rc0, 2.0.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/662cb67709e65d8df22e61fe73232033/untitled198.ipynb).Thanks!", "@noctune Can you try this way and let me know what you think. Thanks!\r\n\r\n```\r\nimport tensorflow as tf \r\ntf.__version__\r\na = tf.keras.layers.Input((None,), dtype=tf.int32)\r\nb = tf.keras.layers.Input((None,None))\r\n@tf.function\r\ndef crossEntropy_estimate(a,b):\r\n  return tf.nn.sparse_softmax_cross_entropy_with_logits(a, b)\r\n\r\ntf.keras.layers.Lambda(crossEntropy_estimate,arguments={'a':a, 'b':b})\r\n```", "What I ended up using is slightly different:\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.keras.layers.Input((None,), dtype=tf.int32)\r\nb = tf.keras.layers.Input((None,None))\r\ntf.keras.layers.Lambda(lambda args: tf.nn.sparse_softmax_cross_entropy_with_logits(*args))((a,b))\r\n```\r\n\r\nIt's a weird workaround, but it works. Thanks!", "I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32578\">No</a>\n", "> I am closing the issue as it was resolved.\r\n\r\nA workaround was presented, but it seems to me like the underlying issue is still not resolved. Shouldn't `sparse_softmax_cross_entropy_with_logits` work with symbolic tensors?", "@mpdn how are you using this? You can combine tf ops with Keras layers in a model created in the Keras functional style. ", "I was trying to create a custom loss that had a dependency on the label, similar to this stackoverflow answer: https://stackoverflow.com/a/57919819/798414\r\n\r\nI don't quite get what you mean by the \"you can combine tf ops with Keras layers...\", as this issue is a bug when trying to do exactly that.", "Thank you for the response. You can create a custom loss that depends on the label by using `add_loss` API in a custom layer. Please see the add_loss examples here. The VAE example is close to what you are looking for: https://www.tensorflow.org/guide/keras/custom_layers_and_models\r\n\r\nHere is another small [colab](https://colab.research.google.com/drive/1mz1vtC-iVAkgMQLRZGNBSDTUqTowgSNK) example.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32578\">No</a>\n"]}, {"number": 32577, "title": "Unable to find pip package", "body": "I have been using bazel build for my im2txt model from TensorFlow and it shows me\r\n\r\n`ModuleNotFoundError: No module named 'nltk'`\r\n\r\nI have installed the package nltk and tried to even create an environment and run the bazel script\r\n\r\nIs there anyway i need to link my python to bazel for the python modules to be installed separately on an environment ?\r\n\r\nRepo i am trying to run : im2txt\r\n\r\nCode sample which iam running :\r\n```\r\n# Location to save the MSCOCO data.\r\nMSCOCO_DIR=\"${HOME}/im2txt/data/mscoco\"\r\n\r\n# Build the preprocessing script.\r\ncd research/im2txt\r\nbazel build //im2txt:download_and_preprocess_mscoco\r\n\r\n# Run the preprocessing script.\r\nbazel-bin/im2txt/download_and_preprocess_mscoco \"${MSCOCO_DIR}\"   ##Error here <----\r\n```", "comments": ["Are `nltk` and `tensorflow` installed in the same environment?", "You mean the bazel environment ? If so please let me know how can I do that, I tried by creating a python virtual environment and tried running but I had no luck.", "@SundeepPidugu ,\r\nJust verifying, have you installed `bazel` `nltk` and `tensorflow` in the same environment ? try running the repo from the environment where all three are installed. Thanks!", "@oanush i created the virtual env in python and installed nltk and tensorflow, but bazel is installed on my machine and not in the environment, I never used bazel and i have no idea on that please help me on what exactly i need to do.", "@SundeepPidugu ,\r\nCan you please refer the [link](https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03) and let us know if it helps.Thanks!", "Thanks @oanush will try and update you ASAP.", "Unable to run `bash ./compile.sh` it throws me an error despite having `1.8.0_22`.", "@SundeepPidugu ,\r\nCan you please provide us the error logs which is faced?Thanks!", "@SundeepPidugu ,\r\nHi,Any update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32577\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32577\">No</a>\n"]}, {"number": 32576, "title": "Add gradient accumulation support for distribution strategy", "body": "PR for issue #32176 .\r\n\r\nThere are still some issues to be fixed or further discussed:\r\n(1\uff09Gradient accumulation will break the default behavior of `basic_session_run_hooks`, e.g.:\r\n```\r\nI0917 17:51:10.444422 139809108260672 basic_session_run_hooks.py:262] loss = 0.435526, step = 0\r\nI0917 17:51:10.495073 139809108260672 basic_session_run_hooks.py:692] global_step/sec: 19.6194\r\nW0917 17:51:10.495340 139809108260672 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\nI0917 17:51:10.495481 139809108260672 basic_session_run_hooks.py:260] loss = 0.435526, step = 0 (0.051 sec)\r\nI0917 17:51:10.496573 139809108260672 basic_session_run_hooks.py:260] loss = 0.017421033, step = 1 (0.001 sec)\r\nI0917 17:51:10.497781 139809108260672 basic_session_run_hooks.py:692] global_step/sec: 362.328\r\nW0917 17:51:10.498049 139809108260672 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1 vs previous value: 1. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\nI0917 17:51:10.498194 139809108260672 basic_session_run_hooks.py:260] loss = 0.017421033, step = 1 (0.002 sec)\r\nI0917 17:51:10.499253 139809108260672 basic_session_run_hooks.py:260] loss = 0.00069683883, step = 2 (0.001 sec)\r\nI0917 17:51:10.506485 139809108260672 basic_session_run_hooks.py:606] Saving checkpoints for 2 into /tmp/tmpYRa9ZY/model.ckpt.\r\n```\r\n`config.log_step_count_steps` should be scaled `iter_size` times larger.\r\n\r\n(2) Currently gradients are *accumulated* locally, which implies that learning rate is adjusted `iter_size` times larger.\r\n\r\n(3) This PR only support gradient accumulation for distribution strategy currently. Support for non-distribution case is natural as this GA realization is independent of optimizer and can be reused easily.\r\n\r\n(4) GA add one new parameter `iter_size` to base class `Optimizer`, whether this will break some default behavior of some optimizer wrapper?\r\n\r\n(5) Higher test coverage.", "comments": ["@yuefengz \r\nCould you please help take a look at this PR? \r\n\r\nThanks", "Like to hear why \" it may not worth the time to add new features to Estimator now.\"", "Thank you for your PR! I believe this is an important feature to have as well.\r\n\r\nI have a couple suggestions out of the concern that we would like to make our API stable. It looks to me that the logic is not deeply coupled with an optimizer. Is it possible for you to put GA in a separate wrapper optimizer class? Also, since we are promoting TF 2.0, it would be more future-proof to make the wrapper a subclass of [optimizer v2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py) and make it eager-compatible. ", "@yuefengz Okay, I will try to put GA in a separate wrapper optimizer class.", "Thanks! I also believe this looks perfect as an [addon](https://github.com/tensorflow/addons/blob/master/CONTRIBUTING.md).  cc @facaiy ", "@yuefengz Thanks for ping me, Yuefeng. My first thought: I agree with @ppwwyyxx that we'd better to implement GA as an optimizer wrapper. Please refer to:\r\n+ https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/moving_average.py\r\n+ https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/weight_decay_optimizers.py\r\n\r\nGA looks like a useful feature for community, what do you think, @seanpmorgan @windqaq @squadrick?", "> @yuefengz Thanks for ping me, Yuefeng. My first thought: I agree with @ppwwyyxx that we'd better to implement GA as an optimizer wrapper. Please refer to:\r\n> \r\n> * https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/moving_average.py\r\n> * https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/weight_decay_optimizers.py\r\n> \r\n> GA looks like a useful feature for community, what do you think, @seanpmorgan @WindQAQ @Squadrick?\r\n\r\nAgree and would be happy to have it as part of Addons. As just another example of optimizer wrapper that we currently have there is lookahead:\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lookahead.py", "@fanshiqing Shiqing, would you like to close this and open a PR to [addons](https://github.com/tensorflow/addons)?", "@facaiy Okay, I will close this one and open a new one to tf add-ons soon.", "I'm quite interested in it. Do you have already a working implementation ?", "Similar idea is implemented here (https://github.com/run-ai/runai/blob/master/runai/ga/README.md) but unfortunately does not work with tensorflow.keras", "@fanshiqing, any update about it? I looked at tensorflow-addons and there's nothing there yet."]}, {"number": 32575, "title": "TF2.0RC1/RC0  the output shape of  custom layer is None. The same code works in TF2.0 Beta1", "body": "**System information**\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): TF2.0.0beta1/rc0/rc1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: TitanV\r\n\r\n\r\n", "comments": ["In this layer, we use a custom c++ op writen by ourself.  Is there any tutorial for defining a custom c++ op for tf2.0 rc1\uff08our c++ op works in tf2.0 beta1\uff09?", "@KANGRuipeng If you have developed custom c++ op that works for `TF2.0b1`, then you can update the same for `TF2.0rc1`. \r\n\r\n[Here](https://github.com/tensorflow/custom-op) is a seperate repository under tensorflow for developing custom op. Thanks!\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32575\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32575\">No</a>\n", "@jvishnuvardhan I'll add an example. I think this is a bug of TF. ", "@jvishnuvardhan From TF2.0.0RC0\uff0cTensorFlow 2.0.0 is built using GCC7, however, you didn't say this in TF2.0.0RC0's  release notes.(You metioned this in TF2.0.0RC1's release notes) . ", "@KANGRuipeng Please post custom_ops related issue in that repo. When you post a new issue, please add a small standalone example to reproduce the issue. If the issue is more related to TF Core then post it in this repo. Thanks again.", "The problem is caused by GCC Version. Thks!"]}, {"number": 32574, "title": "tensorflow2rc1 import bug  ", "body": "Hi,\r\nI was trying to convert one of my training templates to TensorFlow2 but encountered several issues. I can not reach a point where my code is even executed because many imports already terminate the application at start. \r\n```python\r\nfrom tensorflow_core.python.client.session import InteractiveSession\r\n\r\ndef main():\r\n    print('hello world')\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nThis code results instantly in an error and the print is never reached.\r\n```\r\n2019-09-17 09:18:51.362815: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/python/session_create_counter\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/dev/tf2xxx/bug.py\", line 1, in <module>\r\n    from tensorflow_core.python.client.session import InteractiveSession\r\n  File \"/home/xxx/.virtualenv/tf2/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 49, in <module>\r\n    'Counter for number of sessions created in Python.')\r\n  File \"/home/xxx/.virtualenv/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/monitoring.py\", line 183, in __init__\r\n    name, description, *labels)\r\n  File \"/home/xxx/.virtualenv/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/monitoring.py\", line 121, in __init__\r\n    self._metric = self._metric_methods[self._label_length].create(*args)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.\r\n```\r\nNext to _InteractiveSession_ there are multiple other imports that cause problems with monitoring.py. Changing to tf2's eager mode and not worry about the session did not do the trick.\r\n\r\nKeras however runs fine and does not cause any problems (except the conv2d layer + CUDA what causes problems for a while already). \r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: pip3\r\n- TensorFlow version: 2.0.0rc1\r\n- Python version: 3.6.8", "comments": ["I have tried on colab with TF version 2.0.0rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3a294d09dddd2f1fc218bd4bc30cc47c/untitled197.ipynb).Thanks!", "Don't import directly from `tensorflow_core`, that is private API and will change under you and break you.\r\n\r\nThis import is what's causing your problem", "@schoettner I can confirm that @mihaimaruseac suggestion is working without any issues in `TF1.15rc1` and `2.0rc1`.  Just change first line to \r\n`from tensorflow.python.client.session import InteractiveSession`\r\n\r\nI am closing this issue as it was resolved. Please feel free to open the issue if the suggested approach is not working for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32574\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32574\">No</a>\n", "Note that both importing from `tensorflow_core` and directly importing from `.python` are not supported and will break without any rollback.\r\n\r\nThe recommended way to use TensorFlow is `import tensorflow as tf` and accessing the API via the `tf` symbol and it's attributes.", "Hi, thanks for your feedback. However, I can not confirm your solution. The imports are actually resolved by my IDE (IntelliJ Ultimate) because I could not use the imports I used in earlier versions.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n    with tf.Session() as sess:\r\n        a = tf.constant(1, tf.float32)\r\n        b = tf.constant(2, tf.float32)\r\n        c = a + b\r\n        sess.run([c])\r\n    return\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThis code runs fine with tf14.0 while it does not run in tf2.0.0rc1\r\n```\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\n```\r\nI tried the code in your colab code too. Same result.", "Oh, this is because TF 2.0 is eager by default, no longer has sessions.\r\n\r\nTry this instead:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n    a = tf.constant(1, tf.float32)\r\n    b = tf.constant(2, tf.float32)\r\n    c = a + b\r\n    tf.print(c)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nNote that I also removed the not needed `return`", "Hi Mihai,\r\nremoving the session is not a solution. I am aware that tf2 executes in eager mode. Session is only an example. I am not able to import a bunch of dependencies with tf.xxx", "@schoettner Can you describe with little more details on which of the tf.xxx dependencies you are not able to import? Thanks!", "If you really need TF1.x symbols, look into `tf.compat.v1` but note that symbols from there will disappear at one point when we assume that most of our users have switched to using 2.x APIs.", "@jvishnuvardhan i am working on it at the moment. Since most of the code broke after the migration (i am not used to eager mode) it takes some time to fix it. I am not sure why, but my IDE also does not support tf imports as it used to. When I worked through all the issues I am happy to provide the full list.  ", "There is a converter to automatically change your code to TF2.0.", "@mihaimaruseac To get a better understanding of changes from TF1 to TF2 I did not want to use the converter but do it myself. However, after several hours I resign. Many (but not all) imports could be fixed with adding \r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n```\r\nBut I failed to get non-eager mode working without **tf.compact.v1**. So my current situation is:\r\n- I can not access private library calls because they break my code before startup. Also not sure if they would help solve my problems anyway.\r\n- For session based training I would have to add legacy code from **tf.compact.v1**,  what I want to avoid.\r\n- Big chunks of my code are deprecated, because I am using tf instead of keras layers.\r\n\r\nDue to those points I decided to drop my current templates and start from scratch. I follow the suggestion to switch to Keras and hope the issues I encountered some time ago are fixed (e.g. CUDA errors with the Conv2D layer or wrong mean updating of BatchNorm during inference). Thank you for your support!", "I am having a similar problem, which I posted [here](https://stackoverflow.com/questions/58012741/error-importing-tensorflow-alreadyexistserror-another-metric-with-the-same-nam). If I import tensorflow 2.0 twice in the same IPython console, I get the same error posted here.", "@rodrigovimieiro I answered there but summarizing here: TF imports have side effects which should be only executed once. Doing the same import twice in the same app results in that error. Please always use the only supported method of `import tensorflow as tf` (once) and then accessing the API via `tf`.\r\n\r\nNote that this should go in a different issue, too."]}, {"number": 32573, "title": "TFTRT: CUDA_ERROR_ILLEGAL_ADDRESS", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.14\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:RTX2080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nSeeing the following error:\r\n```\r\n2019-09-16 23:41:12.835022: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:749] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2019-09-16 23:41:12.835047: I tensorflow/stream_executor/stream.cc:4813] [stream=0x5591077fbc60,impl=0x5590ffc9bd90] did not memcpy host-to-device; source: 0x7f7e8425bc80\r\n2019-09-16 23:41:12.835058: E tensorflow/stream_executor/stream.cc:331] Error recording event in stream: error recording CUDA event on stream 0x5590ffc9b740: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2019-09-16 23:41:12.835076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1ailed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n```\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Since TF 1.14 prebuilt binaries support cuda 10.0, can you try by switching to cuda 10.0  instead. Thanks!", "I see.  Using the nvcr container with TF 1.14 .  Which has Cuda 10.1.  ", "@sgambient, Please provide the minimal standalone code to reproduce the reported issue. Thanks!", "That is an easy ask, but very hard to do. I do not think that is the right way to handle large software.  My recommendation is to build in more detailed logs and metrics so as not to have to rely on largess of random developers. ", "Is this still an issue after switching to cuda 10.0?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32573\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32573\">No</a>\n"]}, {"number": 32572, "title": "Use arg 'axis' when calling 'tensorflow.python.ops.nn_impl.l2_normalize'", "body": "See #31990 \r\n\r\n-----\r\n\r\nUse arg 'axis' when calling 'tensorflow.python.ops.nn_impl.l2_normalize' instead of the deprecated arg `dim`.\r\n\r\nWhenever I use k-means, I get this warning:\r\n```\r\nW0826 17:01:44.808238 27644 deprecation.py:506] From C:\\Users\\rpherbig\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\contrib\\factorization\\python\\ops\\clustering_ops.py:740: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ndim is deprecated, use axis instead\r\n```\r\n\r\nDue to the deprecation of the `dim` arg - see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L592", "comments": ["@mihaimaruseac This is the new PR we discussed. I am unable to assign it to you."]}, {"number": 32571, "title": "Expose core/kernels/linalg_ops_common to user_ops", "body": "**System information**\r\n- TensorFlow version r1.14:\r\n- Are you willing to contribute it (Yes/No): No.\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nExpose core/kernels/linalg_ops_common to user applications in core/user_ops.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will allow user ops to access [`class LinearAlgebraOp : public OpKernel`](https://github.com/tensorflow/tensorflow/blob/512a3c3a6c0b36f6750394103b7eb316fe4ce853/tensorflow/core/kernels/linalg_ops_common.h#L40)\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers writing their own broadcasting ops.\r\n\r\n**Any Other info.**\r\n\r\n[here's an explanation of the whole thing](https://stackoverflow.com/questions/57912754/linking-into-the-tensorflow-framework-libraries/57966486#57966486)\r\n", "comments": ["It might also make sense to move that whole file into `/core/framework` somewhere or another.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32570, "title": "Assertion error when using mask with unrolled stacked LSTM", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: no GPU, 32 GB ram\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI receive an assertion error when creating a forward pass for an unrolled multi-layer LSTM while using a mask.\r\n\r\n**Describe the expected behavior**\r\nNo assertion error, or at least a better explanation as to the cause.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.placeholder(tf.float32, (3,4,5))\r\nmask = tf.placeholder(tf.bool, (3,4))\r\n\r\nsingle_cells = [tf.keras.layer.LSTMCell(10) for _ in range(3)]\r\nmulti_cell = tf.keras.layers.StackedRNNCell(cells=single_cells)\r\nlstm = tf.keras.layers.RNN(cell=multi_cell, unroll=True)\r\noutput, state = lstm(inputs=inputs, mask=mask)  # <- assertion error occurs here\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@neonrights \r\nI tried in colab with TF version 1.14 and 1.15.0-rc0 and i am getting the below error`.AttributeError: module 'tensorflow.python.keras.api._v1.keras' has no attribute 'layer'` . I am attaching the [gist ](https://colab.sandbox.google.com/gist/ravikyram/9603c7f3d9814aa73513ef4db56f94b0/untitled193.ipynb)for your reference. Thanks!", "Sorry I have a typo in my code, try this instead\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.placeholder(tf.float32, (3,4,5))\r\nmask = tf.placeholder(tf.bool, (3,4))\r\n\r\nsingle_cells = [tf.keras.layers.LSTMCell(10) for _ in range(3)]\r\nmulti_cell = tf.keras.layers.StackedRNNCells(cells=single_cells)\r\nlstm = tf.keras.layers.RNN(cell=multi_cell, unroll=True)\r\noutput, state = lstm(inputs=inputs, mask=mask)  # <- assertion error occurs here\r\n```", "I have tried on colab with TF version 1.14 ,1.15.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e202ba52a64cfd2a3fb1f51bd50f9b67/untitled200.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32570\">No</a>\n"]}, {"number": 32569, "title": "TF GPU doesn't recognise 2nd GPUs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14 GPU\r\n- Python version: 3.7 64 bit\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 1070 (8GB) and GTX 1050 (2GB) , 32 GB Ram, Core i7 9700 k\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTF GPu wont identify two GPUs, only GPU 0 appears\r\n\r\n**Describe the expected behavior**\r\nI want to see GPU0 and GPU 1 when run the device list command\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAfter I install all, The TF GPU does not recognise my 2nd GPU (Nvidia GTX 1050) as GPU:01\r\n![1](https://user-images.githubusercontent.com/45702408/65003983-57204880-d92d-11e9-8706-45c0f98179b1.JPG)\r\nI can switch to that GPU, using Cuda visible devices, however can not use two GPUs  (for multi GPU calculations), any recommendations? \r\n", "comments": ["@cyrus2018,\r\nPlease try the following code snippet to see the gpu divices:\r\n```\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\r\n```\r\nThanks!", "![image](https://user-images.githubusercontent.com/45702408/65123520-03455a80-da26-11e9-9584-ffb985e766d5.png)\r\n", "@cyrus2018, \r\nPlease provide the result of `nvidia-smi`. Thanks!", "Also, attached is the smi query\r\n\r\n![1](https://user-images.githubusercontent.com/45702408/65138605-cdac6b80-da3d-11e9-86f8-f795825b4662.JPG)\r\n[Qyarry.docx](https://github.com/tensorflow/tensorflow/files/3625833/Qyarry.docx)\r\n", "I suspect you have multiple cuda versions installed on your system. TF 1.14 prebuilt binary supports cuda 10.0 that can be one plausible explanation.\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32569\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32569\">No</a>\n", "The issue has not been resolves, any suggestions?", "Mutli GPU usage on TF works in Windows platform right? (Windows 10 pro to be more specific)\r\n\r\nAnother question: RTX 1070, has separate power input. GTX 1050 draws the power from the motherboard, but i haven't connected the auxiliary power input into the mother board. is there any known hardware configurations that may cause this issue? (to prevent usage of two gpus at the same time?\r\n\r\n", "Added below to top of the codes, issue is resolved\r\nNOTE:: TF_MIN_GPU_MULTIPROCESSOR_COUNT defauls value is 8\r\n\r\n\r\nimport os\r\nos.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"]=\"2\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\r\n\r\n\r\n", "Closing this issue as it has been resolved. Thanks!"]}, {"number": 32568, "title": "Nms stability", "body": "Current CPU implementation has an unstable sort which can lead to significant difference the results. This PR adds ordering criteria for same scored boxes, stabilizing algorithm. Also GPU selection criteria has been changed from **greater** than to **greater or equal** to match CPU implementation.", "comments": ["This seems depending on #32407, so I'll review and merge that first.", "@aaroey I force pushed fix to the CPU op bug that is blocking these GPU ops. "]}, {"number": 32567, "title": "Add CPU BatchNormalization for NCHW format", "body": "Previously, CPU doesn't support NCHW. This makes other operations based on NCHW batch norm hard to use. For example, the layer norm can be transformed to use NCHW batch norm on GPU. However, since we don't have CPU NCHW batch norm, we have to either avoid using NCHW batch norm or add more parameters to control the execution path.\r\n\r\nThis PR enables NCHW batch norm on CPU.\r\n\r\nfyi. @nluehr ", "comments": ["This PR is originally served for https://github.com/tensorflow/tensorflow/pull/31557.\r\n\r\n@reedwm ", "Done. Please check."]}, {"number": 32566, "title": "[r1.15-CherryPick]: Autograph: Remove tf.autograph.experimental.set_loop_options doc", "body": "It's not implemented yet. Having a doc\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/experimental/set_loop_options\r\nis confusing and misleading as it's not working. This PR removes the doc.\r\n\r\nPiperOrigin-RevId: 269387821", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32566) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32566) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 32565, "title": "[r2.0-CherryPick]: Autograph: Remove tf.autograph.experimental.set_loop_options doc", "body": "It's not implemented yet.  Having a doc\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/experimental/set_loop_options\r\nis confusing and misleading as it's not working. This PR removes the doc.\r\n\r\nPiperOrigin-RevId: 269387821", "comments": []}, {"number": 32564, "title": "model.add_loss no longer works for tf.image.ssim_multiscale in tf 2.1", "body": "This tf.keras code was valid in tf 1.13.1:\r\n\r\n`model.add_loss(tf.image.ssim_multiscale(reconstruction.inputs[0], reconstruction.outputs[0], 1.0))`\r\nwith types of reconstruction.inputs[0], reconstruction.outputs[0] being:\r\n```\r\n(<tf.Tensor 'input_1:0' shape=(?, 256, 256, 1) dtype=float32>,\r\n <tf.Tensor 'reconstruction_/truediv:0' shape=(?, 256, 256, 1) dtype=float32>)\r\n```\r\n\r\nbut is no longer valid in tf 1.14.0 or 1.15.0rc0:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n(0) Invalid argument: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,1]\r\n[[{{node input_1}}]]\r\n[[MS-SSIM_1/Scale1/cond/Pad/paddings/_3891]]\r\n(1) Invalid argument: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,1]\r\n[[{{node input_1}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nI can not find any documentation on this change or how to fix it in the newer tf versions.", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "@ravikyram https://colab.research.google.com/gist/isaacgerg/5963501ac71cd7459a8b9e986386373d/tf_ssim_bug.ipynb\r\n\r\nToggle the pip install between tf version 1.13.1 and 1.14.0.\r\n\r\nWith 1.14.0, i get the error: \r\n```\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,1]\r\n\t [[{{node input_1}}]]\r\n\r\n```\r\nNo error with 1.13.1.", "I have tried on colab with TF version 1.14.0 , 1.15.0rc0 and was able to reproduce the issue.However i am not seeing any issue with TF version 1.13.1.Thanks!", "Any update on this?", "Any update? Does this work in tf 2.0?", "@ravikyram @jvishnuvardhan Any update on this?", "This is still broken in tf 2.1", "Any update?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "This problem is fixed as of tensorflow 2.4.  Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32564\">No</a>\n"]}, {"number": 32563, "title": "can't interpret primitive type objects correctly", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0-dev20190915\r\n- Python version:3.6.7\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nwhen i use primitive type objects during building model with tf.keras functional api. tensorflow mistake primtive type objects as tensors.\r\n\r\n**Describe the expected behavior**\r\n\r\ninterpret primitive type as tensor at the right time.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf;\r\nimport numpy as np;\r\n\r\ndef Model(shape):\r\n  input_shapes = [np.array(shape,dtype=np.int32), np.array(shape,dtype=np.int32) // 2, np.array(shape,dtype=np.int32) // 4];\r\n  print(input_shapes)\r\n  inputs = [tf.keras.Input(input_shape) for input_shape in input_shapes];\r\n  outputs = list();\r\n  for input in inputs:\r\n    output = tf.keras.layers.Conv2D(filters=10,kernel_size=(3,3), padding='same')(input);\r\n    outputs.append(output);\r\n  return tf.keras.Model(inputs = inputs, outputs = outputs);\r\n  \r\nmodel = Model([32,32,32]);\r\nmodel.save('model.h5');\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n>Traceback (most recent call last):            File \"test.py\", line 14, in <module>\r\n    model = Model([32,32,32]);\r\n  File \"test.py\", line 7, in Model\r\n    inputs = [tf.keras.Input(input_shape) for input_shape in input_shapes];               File \"test.py\", line 7, in <listcomp>\r\n    inputs = [tf.keras.Input(input_shape) for input_shape in input_shapes];\r\n  File \"/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/input_layer.py\", line 251, in Input\r\n    if shape and batch_input_shape:\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()", "comments": ["Hi @breadbread1984 It's because you try to use `np.array` as shape, which results in errors.\r\n```python\r\n# work\r\nif [1, 2, 3]:\r\n    print(123)\r\n# not work\r\n# ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\nif np.array([1, 2, 3]):\r\n    print(123)\r\n```\r\n\r\nSo the workable approach is to convert `np.array` back to list:\r\n```python\r\nimport tensorflow as tf;\r\nimport numpy as np;\r\n\r\ndef Model(shape):\r\n  input_shapes = [np.array(shape,dtype=np.int32), np.array(shape,dtype=np.int32) // 2, np.array(shape,dtype=np.int32) // 4];\r\n  print(input_shapes)\r\n  inputs = [tf.keras.Input(list(input_shape)) for input_shape in input_shapes];\r\n  outputs = list();\r\n  for input in inputs:\r\n    output = tf.keras.layers.Conv2D(filters=10,kernel_size=(3,3), padding='same')(input);\r\n    outputs.append(output);\r\n  return tf.keras.Model(inputs = inputs, outputs = outputs);\r\n  \r\nmodel = Model([32,32,32]);\r\nmodel.save('model.h5');\r\n```"]}, {"number": 32562, "title": "Ungooglable error: dataset map over dictionaries with multiple input paths and float outputs", "body": "The error in TF2b: \"ValueError: elems must be a 1+ dimensional Tensor, not a scalar\"\r\n\r\nA StackOverflow bounty is a few days from expiring; docs and code.  At my wits' end.\r\n\r\n[SO 100+ bounty and gunga galunga](https://stackoverflow.com/questions/57894869/tf-dataset-multiple-path-inputs-and-mapping-per-batch-to-load-images)", "comments": ["Hi,\r\nI posted a suggested fix on the SO issue (which works as far as your `load_image` function is concerned.\r\nLong story short: as indicated in the exception message, you were trying to feed float values as argument to a function expected tensor(s).\r\nI hope this helps :-)", "@caseybasichis ,\r\nCan you please refer the suggestion provided by @pandrey-fr ?Thanks!", "@caseybasichis ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32561, "title": "[TF2.0] Cannot place the graph after loading weights trained with multiple GPUs and tf.distribute.MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: CentOS Linux 7.2.1511\r\n- TensorFlow installed from: binary (pip)\r\n- TensorFlow version: 2.0.0.dev20190915\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA 10.0.130 / cuDNN 7.6.0\r\n- GPU model and memory: 2x Nvidia Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nI am using tensorflow 2 on a cluster which limits the GPU hours per job. Therefore, I want to be able to load model weights and resume training the model from a checkpoint. The model in question is a `tf.keras` model compiled with `tf.keras.optimizers.SGD` as optimizer, with momentum. When attempting to resume training after loading weights that were trained with multiple GPUs and `tf.distribute.MirroredStrategy`, tensorflow crashes with the following error:\r\n```text\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:GPU:1. The edge src node is sgd_sgd_update_update_0_resourceapplykerasmomentum_accum , and the dst node is SGD/SGD/update/update_1/ResourceApplyKerasMomentum [Op:__inference_distributed_function_1355]\r\n```\r\nThe error does not occur when the model has been pre-trained on CPU, or with only one GPU.\r\n\r\n**Describe the expected behavior**\r\nThe model should be able to resume training after loading weights trained on multiple GPUs.\r\n\r\n**Code to reproduce the issue**\r\nA minimal reproducible example is below. With access to at least 2 GPUs, begin training the model with `initial_run=True`. Interrupt training after a few epochs have completed, then re-run the script with `initial_run=False`. The model will attempt to load the previously trained weights and resume training from the last completed epoch, leading to the error.\r\n```python\r\nimport tensorflow as tf\r\n\r\ninitial_run = True\r\n\r\nbatch_size = 1000\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\r\ntrain_images = train_images / 255.0\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\ntrain_dataset = train_dataset.batch(batch_size).repeat()\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n    model.compile(\r\n            optimizer=tf.keras.optimizers.SGD(momentum=0.9),\r\n            loss='sparse_categorical_crossentropy',\r\n            metrics=['accuracy'])\r\n\r\nif not initial_run:\r\n    model.load_weights(\"latest_weights\")\r\n\r\nmodel.fit(\r\n        train_dataset,\r\n        steps_per_epoch=len(train_images) / batch_size,\r\n        epochs=1000,\r\n        initial_epoch=int(model.optimizer.iterations.numpy() // (len(train_images) / batch_size)),\r\n        callbacks=[\r\n            tf.keras.callbacks.ModelCheckpoint(\r\n                filepath=\"latest_weights\",\r\n                save_weights_only=True)])\r\n```\r\n**Other info / logs**\r\nA log of the output from attempting to load weights and resume training is available here: https://pastebin.com/raw/Dmn2Jt0i", "comments": ["So, I figured out that it works if `model.load_weights` is called within the scope of `strategy.scope()`. If this is not done, the weights of the optimizer (`model.optimizer.weights`) are loaded as `tf.Variable` instead of `MirroredVariable`, leading to the error. I am not sure if this is the intended behavior, as the layer weights of the model get loaded as `MirroredVariable` even when `load_weights` is called outside of `strategy.scope()`. If this is intended, feel free to close this issue.", "This behaviour is also same in case of TPU strategy.", "Additionally, if using less than two GPUs, then optimizer weights are only loaded if `load_weights` is called _outside_ of `strategy.scope()`. This seems undesired, since the structure of the code needed to load identical model weights differs based on hardware configuration.", "Thank you for filing the issue. Your analysis in https://github.com/tensorflow/tensorflow/issues/32561#issuecomment-532441133 is correct - load_weights should happen inside the strategy.scope because it ends up creating optimizer variables at that point. \r\n\r\nWe have recently addressed this in https://github.com/tensorflow/tensorflow/commit/e7fb9e52029a0711b3bd9799ce70428cef918e86#diff-39c47d5e727df3ac37c8c60f072a1e68. With this change, now if you try to create optimizer variables in a scope different than the model's variables, it will give a more informative error like:\r\n\r\n\"Trying to create optimizer slot variable under the scope for \"\r\n            \"tf.distribute.Strategy ({}), which is different from the scope \"\r\n            \"used for the original variable ({}). Make sure the slot \"\r\n            \"variables are created under the same strategy scope. This may \"\r\n            \"happen if you're restoring from a checkpoint outside the scope\"\r\n\r\nI have tested it with your code above using TPUStrategy, and I believe the same thing should apply to MirroredStrategy as well. \r\nWe evaluated different alternatives - whether we should make it work for load weights outside the scope, but it seemed it would be better to give a clearer error message and ask the weights loading to be moved inside the scope. \r\n\r\nI am not sure I understand what you mentioned in https://github.com/tensorflow/tensorflow/issues/32561#issuecomment-533228611 though. Can you elaborate? \r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Thanks for the response.\r\n\r\nThe issue I ran into: I trained a model using MirroredStrategy with 2 GPUs, and saved the model weights. Then, with access to only a single GPU (or only CPU), I tried to load the saved weights using `load_weights` inside `strategy.scope()` as discussed above. However, the weights could only be loaded when `load_weights` was called outside of `strategy.scope()`.", "At load time, if you only have a single GPU or CPU, what strategy are you using at that time? Also can you file another issue for this, along with code / instructions to repro? thank you. \r\nI will close this particular issue in the meantime. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32561\">No</a>\n", "@guptapriya Thanks for the really nice explanation\r\nI think the error message could greatly be improved if it mentioned clearly that the slot variables are created when calling `load_weights`.\r\nI have this error message when loading the optimizer's weights (which was done using `mirrored_strategy.run`), so I was a bit surprised by the error but now I understand it.\r\nWhat do you think of this suggestion? I could submit a PR if needed.\r\n\r\nJust for context I am using `MultiWorkerMirroredStrategy` in a SLURM cluster."]}, {"number": 32560, "title": "Stop caching inf/nan floats", "body": "This fixes a memory leak in tf.clip_by_global_norm in eager mode. ", "comments": []}, {"number": 32559, "title": "[r2.0-CherryPick]:Split segment_reduction_ops.cc to reduce compile time.", "body": "PiperOrigin-RevId: 266052197\r\n\r\nIn order to see if this speeds up windows build time.", "comments": []}, {"number": 32558, "title": "Loading weights from checkpoint file fails [python/c api]", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock example and costume code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Model created with pip package, training performed with C api from compiled libtensorflow\r\n- TensorFlow version (use command below): 2.0rc\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n**My environment**\r\n[tf_env.txt](https://gist.github.com/7PintsOfCherryGarcia/a04c7fe26ed2b546fe6bfc3ebc6b1d2f)\r\n\r\n\r\n**Describe the current behavior**\r\nI am using tensorflow's c API to train models created with tf.keras in python. After training in my C program a checkpoint file is created from which a model's weights can be restored for more training or prediction within my C program.\r\nWhen trying to restore my model in python, trained weights can not be loaded neither by:\r\n```python\r\nmodel.load_weights(\"path_to_checkpoint\")\r\n```\r\nnor with\r\n```python\r\ncheckpoint = tf.train.Checkpoint(model=model)\r\ncheckpoint.restore(\"path_to_checkpoint\")\r\n```\r\nThe model loads successfully with:\r\n```python\r\nmodel = keras.experimental.load_from_saved_model(\"path_to_model_folder\")\r\n```\r\nBut this is an untrained model. To be precise, this model has the same weights as when the model was first defined.\r\n\r\n**Describe the expected behavior**\r\nIn my C program I can load/save/restore my model with no problem, I am expecting to be able to load trained model and make predictions in python as well from the model trained in my C program.\r\n\r\n**Code to reproduce the issue**\r\nThere are several steps that I will describe to accurately reproduce my problem. \r\n\r\n**1. Create model with:\r\n[keras_model.py](https://gist.github.com/7PintsOfCherryGarcia/f0483137a952c3d5d2907d49d723ccf7)\r\n```bash\r\npython keras_model.py\r\n``` \r\nThis will create a keras model and export it in keras_model, this folder name **is hard coded in the C program**\r\n\r\n**2. Get data with:**\r\n[getData.py](https://gist.github.com/7PintsOfCherryGarcia/455370c9550a6c5c10f9135155a8b282)\r\n```bash\r\npython getData.py\r\n```\r\nThis will download the fashion MNIST dataset and stored the training examples and labels as text files in:\r\ndata.txt and labels.txt **These names are hardcoded in the C program**\r\n\r\n**3. Compile C program**\r\nThe only requirement for my C program is the tensorflow library shared object files libtensorflow.so and libtensorflow_framework.so and the c API header file c_api.h. These were compiled with bazel via:\r\n```bash\r\n./configure\r\nbazel build -c opt //tensorflow/tools/lib_package:libtensorflow\r\n```  \r\nExtracting the shared object files from libtensorflow.tar.gz\r\n\r\nMy [C program](https://gist.github.com/7PintsOfCherryGarcia/d6d3df8e7282f2e635bf9362c81f9292) can then be compiled with:\r\n```bash\r\ngcc -Wall -I path_to_libtensorflow/include -L path_to_libtensorflow/lib -o BellyTF BellyTF.c -ltensorflow\r\n```\r\nAssuming data.txt, labels.txt and keras_model are in the same directory as the executing command run:\r\n```bash\r\nLD_LIBRARY_PATH=path_to_libtensorflow_lib ./BellyTF\r\n```\r\nThis will load saved model, and training data, print predictions from untrained model, train for a number of epochs, print predictions from trained model and save weights as a checkpoint file in the variables folder inside keras_model\r\n\r\n**4. Load trained mdel in python from checkpoints file**\r\nThis is where I encounter problems\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n#Get data\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\n#scale and reshape\r\ntrain_images = train_images/255\r\ntrain_images = np.reshape(train_images,(60000,784))\r\n\r\n#Load model\r\nmodel = keras.experimental.load_from_saved_model(\"keras_model\")\r\n#Predictions are wrong\r\nmodel.predict([[train_images[0]]])\r\n\r\n#Try to load weights This produces AssertError (traceback at end of issue)\r\nmodel.load_weights(\"keras_model/variables/belly\")\r\nAssertionError: Some objects had attributes which were not restored:\r\n\r\n#Try load model via tf.train.Checkpoint\r\ncheckpoint = tf.train.Checkpoint(model=model)\r\nstatus =  checkpoint.restore(\"keras_model/variables/belly\")\r\n\r\n#Prediction is wrong\r\nmodel.predict([[train_images[0]]])\r\n ```\r\n\r\n**Other info / logs**\r\nTraceback from\r\n```python\r\nmodel.load_weights(\"keras_model/variables/belly\")\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-39-db6e544ddcae> in <module>\r\n----> 1 model.load_weights(\"keras_model/variables/belly\")\r\n\r\n~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in load_weights(self, filepath, by_name)\r\n    160         raise ValueError('Load weights is not yet supported with TPUStrategy '\r\n    161                          'with steps_per_run greater than 1.')\r\n--> 162     return super(Model, self).load_weights(filepath, by_name)\r\n    163\r\n    164   @trackable.no_automatic_dependency_tracking\r\n\r\n~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in load_weights(self, filepath, by_name)\r\n   1396         # streaming restore for any variables created in the future.\r\n   1397         trackable_utils.streaming_restore(status=status, session=session)\r\n-> 1398       status.assert_nontrivial_match()\r\n   1399       return status\r\n   1400     if h5py is None:\r\n\r\n~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py in assert_nontrivial_match(self)\r\n    915     # assert_nontrivial_match and assert_consumed (and both are less\r\n    916     # useful since we don't touch Python objects or Python state).\r\n--> 917     return self.assert_consumed()\r\n    918\r\n    919   def _gather_saveable_objects(self):\r\n\r\n~/Projects/tensorflow/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py in assert_consumed(self)\r\n    892       raise AssertionError(\r\n    893           \"Some objects had attributes which were not restored: %s\" %\r\n--> 894           (unused_attributes,))\r\n    895     for trackable in self._graph_view.list_objects():\r\n    896       # pylint: disable=protected-access\r\n\r\nAssertionError: Some objects had attributes which were not restored: {<tf.Variable 'dense_2/kernel:0' shape=(784, 128) dtype=float32, numpy=\r\narray([[ 0.01530321, -0.07596322, -0.01604562, ...,  0.03402978,\r\n         0.07713103, -0.05052958],\r\n       [ 0.00116051,  0.04680964, -0.03951511, ..., -0.03624759,\r\n        -0.04315009,  0.06628483],\r\n       [ 0.0188334 , -0.06348058, -0.04110793, ..., -0.02473556,\r\n        -0.04467853, -0.00360004],\r\n       ...,\r\n       [-0.01828651, -0.07760178, -0.06535166, ..., -0.04309935,\r\n        -0.08069813,  0.00902149],\r\n       [ 0.04599006,  0.02525605,  0.06842359, ..., -0.03162104,\r\n        -0.02693111,  0.06349993],\r\n       [-0.02491769,  0.0611873 , -0.00206578, ..., -0.01271369,\r\n         0.00109962, -0.07020341]], dtype=float32)>: ['dense_2/kernel'], <tf.Variable 'dense_2/bias:0' shape=(128,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['dense_2/bias'], <tf.Variable 'dense_1_1/kernel:0' shape=(128, 10) dtype=float32, numpy=\r\narray([[ 0.16446821, -0.17563525,  0.18357025, ...,  0.08780123,\r\n         0.07823227,  0.10018779],\r\n       [ 0.0915112 , -0.17932694,  0.12889476, ...,  0.20635314,\r\n        -0.09271443,  0.11488943],\r\n       [-0.08094949, -0.14578271, -0.01433699, ..., -0.11150974,\r\n        -0.19056591, -0.01099543],\r\n       ...,\r\n       [-0.08208692,  0.06081717,  0.0688145 , ..., -0.08729573,\r\n         0.20750098, -0.08333559],\r\n       [-0.15593694,  0.14401685, -0.20091254, ..., -0.15566093,\r\n        -0.06369022, -0.12938659],\r\n       [-0.18293834,  0.08427195, -0.02965383, ...,  0.02141935,\r\n        -0.05810072, -0.04387315]], dtype=float32)>: ['dense_1_1/kernel'], <tf.Variable 'dense_1_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>: ['dense_1_1/bias']}\r\n```\r\n", "comments": ["If the C program needs more commenting please let me know", "It is challenging to debug large scripts a minimal reproducible code snippet is appreciated.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32558\">No</a>\n", "Ive tried [stackoverflow](www.stackoverflow.com) without success. The C api does not receives a lot of attention so I'm not sure where to go from here. Documentation is very poor in this sense  "]}]