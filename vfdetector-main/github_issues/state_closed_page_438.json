[{"number": 40720, "title": "TF Unit test flaky in haswell/broadwell machines", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version:  3.7\r\n- Bazel version (if compiling from source):3.1\r\n- GCC/Compiler version (if compiling from source):7.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n-CPUModel : Haswell/Broadwell\r\n\r\n\r\n**Describe the current behavior**\r\n   _**//tensorflow/core/kernels:sparse_matmul_op_test**_ \r\n     Is flaky on haswell/broadwell machines (-march=haswell -mtue=broadwell) and fails often (not always). The failure is in vectorization when setting the parameters for the op.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\n      //tensorflow/core/kernels:sparse_matmul_op_test\r\n  build and run on broadwell platform. \r\n\r\n\r\n**Other info / logs** I\r\n", "comments": ["@jojivk73  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40720\">No</a>\n"]}, {"number": 40719, "title": "\"Cannot find bazel\" return 0 in configure.py", "body": "Hello!\r\nIn file configure.py inside a function check_bazel_version when bazel is not exists this function returns 0.\r\n\r\nHere is a part of code:\r\n  if which('bazel') is None:\r\n    print('Cannot find bazel. Please install bazel.')\r\n    sys.exit(0)\r\n\r\nShould be sys.exit(1)? It's an error. ", "comments": ["Sounds good. Can you send a PR please?", "Yes", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40719\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40719\">No</a>\n"]}, {"number": 40718, "title": "Prefer generator expressions over list comprehensions", "body": "Same as #39887\r\n\r\nThis PR replaces list comprehensions that are only used as input to `any()`, `all()` or `zip()` with generator expressions. This removes the need to instantiate unnecessary lists if the expresion is only consumed as an iterator and in the case of any/all allows the loop to potentially exit early which can improve performance for long iterations.\r\n\r\nMost of the changes are not in a hot code path so this won't noticably improve performance but since the change don't hurt readability I think they are still useful to include.", "comments": []}, {"number": 40717, "title": "Writing custom py_function inside custom layer , gradient backpropagation", "body": "Hi , \r\n\r\nI'm writing a custom py_function inside a custom layer. But I get error during gradient backpropagation. I'm using tensorflow 1.1.4 with eager execution enabled. While debugging I reduced the complexity of code, and now 'm just trying 1-d DCT of input. I still get the error. \r\n\r\nHere is my code : \r\n\r\n\r\nfrom scipy.special import expit\r\nfrom scipy.fftpack import dct, idct\r\nimport time\r\n\r\ndef fdct (a): \r\n    return dct(a.T, norm='ortho').T\r\n\r\ndef fdct_top (inp): \r\n    return tf.convert_to_tensor(np.array([fdct(in_) for in_ in list(inp.numpy())]))\r\n    \r\n\r\ndef fidct (a): \r\n    return idct(a.T, norm='ortho').T\r\n\r\ndef fidct_top (inp): \r\n    return tf.convert_to_tensor(np.array([fidct(in_) for in_ in list(inp.numpy())]))\r\n\r\n\r\n    \r\nclass MyDenseLayer(tf.keras.layers.Layer):\r\n  def __init__(self, y_mat):\r\n    super(MyDenseLayer, self).__init__()\r\n    self.w_mat = y_mat\r\n    \r\n  def build(self, input_shape):\r\n    self.w = tf.Variable (initial_value = self.w_mat, trainable = True)\r\n  \r\n  def call(self, input):\r\n    dct_i = tf.py_function (fdct_top, (input,), 'float32')\r\n    dct_i.set_shape(tf.TensorShape((None, 10)))\r\n    \r\n    dcts_q = tf.multiply(dct_i, 1/self.w)    \r\n    print(dcts_q)\r\n    z_floored_NOT_differentiable = tf.floor(dcts_q)\r\n    z_floored_differentiable = (dcts_q - (tf.stop_gradient(dcts_q) - z_floored_NOT_differentiable))\r\n    print(z_floored_differentiable)\r\n    dcts_deg = tf.multiply(z_floored_differentiable, tf.cast(self.w, 'float32'))\r\n    \r\n    dcts_deg = tf.py_function (fidct_top, (dcts_deg,), 'float32')\r\n    dcts_deg.set_shape(tf.TensorShape((None, 10)))\r\n    \r\n    return dcts_deg\r\n\r\nimg_inputs1 = keras.Input(shape=(10,))\r\nq_mat = 5*np.ones((10)).astype(np.float32)\r\nmul_layer = MyDenseLayer (q_mat)\r\noutputs = mul_layer (img_inputs1)\r\nmodel = keras.Model(inputs=[img_inputs1], outputs=outputs)\r\nmodel.summary()\r\n\r\nx = 10*np.random.rand(100,10).astype(np.float32)\r\ny = x \r\n\r\noptimizer = tf.keras.optimizers.RMSprop(0.1)\r\n\r\nmodel.compile(loss=tf.keras.losses.MSE,\r\n                optimizer=optimizer,\r\n                 )\r\n\r\nhistory = model.fit(x, y, epochs = 50 )\r\n\r\n\r\n\r\nInvalidArgumentError: Input to reshape is a tensor with 10 values, but the requested shape has 320\r\n\t [[{{node training_1/RMSprop/gradients/Mul_3_grad/Reshape}}]] [Op:StatefulPartitionedCall]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@nikhil1008 \r\nI ran your code on tf 1.14 and 2.2 and do not face any error as reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a80dfe73ebd37b6c1cf63e7ae7f43164/untitled238.ipynb). ", "Hi, \r\nI think the Indentation in your code was not same as mine , I corrected it in your notebook. \r\n\r\nHere is my full code : \r\n\r\nhttps://colab.research.google.com/drive/1ZEc9cGJ6uNT6ZBxwxIX08J4-RR9_rEzU#scrollTo=lMYfv3ChvZRE\r\n\r\n\r\n\r\npasting here as well : \r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom PIL import Image\r\nCUDA_VISIBLE_DEVICES=0\r\ntf.compat.v1.enable_eager_execution()\r\ntf.compat.v1.enable_v2_behavior\r\n\r\nfrom scipy.special import expit\r\nfrom scipy.fftpack import dct, idct\r\nimport time\r\n\r\ndef fdct (a): \r\n    return dct(a.T, norm='ortho').T\r\n\r\ndef fdct_top (inp): \r\n    return tf.convert_to_tensor(np.array([fdct(in_) for in_ in list(inp.numpy())]))\r\n    \r\n\r\ndef fidct (a): \r\n    return idct(a.T, norm='ortho').T\r\n\r\ndef fidct_top (inp): \r\n    return tf.convert_to_tensor(np.array([fidct(in_) for in_ in list(inp.numpy())]))\r\n\r\n    \r\nclass MyDenseLayer(tf.keras.layers.Layer):\r\n  def __init__(self, y_mat):\r\n    super(MyDenseLayer, self).__init__()\r\n    self.w_mat = y_mat\r\n    \r\n  def build(self, input_shape):\r\n    self.w = tf.Variable (initial_value = self.w_mat, trainable = True)\r\n  \r\n  def call(self, input):\r\n    dct_i = tf.py_function (fdct_top, (input,), 'float32')\r\n    dct_i.set_shape(tf.TensorShape((None, 10)))\r\n    \r\n    dcts_q = tf.multiply(dct_i, 1/self.w)    \r\n    print(dcts_q)\r\n    z_floored_NOT_differentiable = tf.floor(dcts_q)\r\n    z_floored_differentiable = (dcts_q - (tf.stop_gradient(dcts_q) - z_floored_NOT_differentiable))\r\n    print(z_floored_differentiable)\r\n    \r\n    dcts_deg = tf.multiply(z_floored_differentiable, tf.cast(self.w, 'float32'))\r\n\r\n    dcts_deg = tf.py_function (fidct_top, (dcts_deg,), 'float32')\r\n    dcts_deg.set_shape(tf.TensorShape((None, 10)))\r\n\r\n\r\n    return dcts_deg\r\n\r\nimg_inputs1 = keras.Input(shape=(10,))\r\nq_mat = 5*np.ones((10)).astype(np.float32)\r\nmul_layer = MyDenseLayer (q_mat)\r\noutputs = mul_layer (img_inputs1)\r\nmodel = keras.Model(inputs=[img_inputs1], outputs=outputs)\r\nmodel.summary()\r\n\r\nx = 10*np.random.rand(100,10).astype(np.float32)\r\ny = x \r\n\r\noptimizer = tf.keras.optimizers.RMSprop(0.1)\r\n\r\nmodel.compile(loss=tf.keras.losses.MSE,\r\n                optimizer=optimizer,\r\n                 )\r\n\r\nhistory = model.fit(x, y, epochs = 50, batch_size  = 1 ) ", "@nikhil1008 \r\nI had to correct the indentation in your code else i am facing indentation error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/68a3926dc13fcd6f4f29daeedcc24117/untitled238.ipynb).\r\nPlease share a  colab gist with your code where you face an error. the above link is not accessible.\r\nIs there any particular reason for using older version on tf when later versions are available, you could try using later versions and let us know if that helps.", "[@Saduf2019] \r\n\r\nI edited **your** notebook, Please take a look. The issue is reproducible now. \r\n\r\nhttps://colab.research.google.com/gist/Saduf2019/68a3926dc13fcd6f4f29daeedcc24117/untitled238.ipynb#scrollTo=bR0mK9eG0CHF\r\n\r\nI have dependency over cuda version , therefore I need to use TF=1.14. Btw, I tried with 2.2.0 as well , I get same error. \r\n![image](https://user-images.githubusercontent.com/66033489/85427535-ba7c7e00-b530-11ea-8a62-97c654b83d31.png)\r\n", "Please use following link in case you can't see changes I made to your notebook. \r\n\r\nhttps://colab.research.google.com/drive/1s9QPYIclIMcrcFPkrhcOhOHemB-LSuKl?usp=sharing\r\n", "There is a lot that has changed between 1.14 and 2.x, especially for eager execution. Can you retry this with a pure 2.x install, and open a new issue if you run into trouble?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40717\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40717\">No</a>\n"]}, {"number": 40716, "title": "TFLite generation issue with lrn", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n**Infos**\r\n\r\nTflite return a type issue when i use the code below but it doesn't when just switching the lrn and multiply declaration order in the code\r\n\r\n**Custom code (test_tflite.py)**\r\n\r\n```python\r\nimport os\r\nimport numpy\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\ntf.disable_v2_behavior()\r\n\r\nif __name__ == '__main__':\r\n    data = tf.placeholder(dtype=tf.float32, shape=[2,2,16,16], name=\"input\")\r\n    #output size: h,w,d,b = 2,16,16,2\r\n    random_front_0 = tf.nn.local_response_normalization(\r\n        data,\r\n        depth_radius=2,\r\n        bias=1.0,\r\n        alpha=1.0,\r\n        beta=0.5,\r\n        name='random_front_0'\r\n    )\r\n    #output size: h,w,d,b = 2,16,16,2\r\n    random_back_0 = tf.multiply(\r\n        data,\r\n        tf.Variable(numpy.random.rand(1).astype(dtype=numpy.float32)),\r\n        name='random_back_0'\r\n    )\r\n    output = tf.add(\r\n        random_front_0,\r\n        random_back_0,\r\n        name='output'\r\n    )\r\n\r\n    #output size: h,w,d,b = 2,16,16,2\r\n    global_init = tf.global_variables_initializer()\r\n    if len(tf.global_variables()) > 0:\r\n        saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(global_init)\r\n        tf.train.write_graph(sess.graph, \"./\", 'model.pbtxt', as_text=True)\r\n        save_path = saver.save(sess, './model.ckpt')\r\n        print('Model saved in file: {}'.format(save_path))\r\n        # Look here for more details https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph_test.py\r\n        freeze_graph.freeze_graph(\r\n           os.path.join(\"./\", 'model.pbtxt'), # GraphDef\r\n           '',\r\n           False, # is the GraphDef in binary format\r\n           os.path.join(\"./\", 'model.ckpt'), # checkpoint name\r\n           'output', # output node name\r\n           '', '',\r\n           os.path.join(\"./\", 'model.frozen.pb'), # output frozen path graph\r\n           True, # clear devices info from meta-graph\r\n           '', '', '')\r\n        input_arrays = [\"input\"]\r\n        output_arrays = [\"output\"]\r\n        converter = tf.lite.TFLiteConverter.from_frozen_graph('./model.frozen.pb', input_arrays, output_arrays)\r\n        def representative_dataset_gen():\r\n            for _ in range(1000):\r\n                yield [numpy.array(numpy.random.randint(0, 255, size=(2, 2, 16, 16)),dtype=numpy.float32)]\r\n        converter.representative_dataset = representative_dataset_gen\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        tflite_model = converter.convert()\r\n        tflite_model_path = './model.tflite'\r\n        open(tflite_model_path, 'wb').write(tflite_model)\r\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\r\n    assert(interpreter is not None)\r\n    interpreter.allocate_tensors()\r\n```\r\n\r\n**The output**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tflite.py\", line 64, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter.py\", line 247, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"lib/python2.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/local_response_norm.cc:47 input->type != output->type (9 != 1)Node number 1 (LOCAL_RESPONSE_NORMALIZATION) failed to prepare.\r\n```\r\n\r\n**Model.tflite graph**\r\n\r\n![model tflite](https://user-images.githubusercontent.com/29231663/85408924-735cb000-b565-11ea-8a92-6a5e6cee8e20.png)\r\n\r\n**Custom code (with switched layers)**\r\n```python\r\nimport os\r\nimport numpy\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\ntf.disable_v2_behavior()\r\n\r\nif __name__ == '__main__':\r\n    data = tf.placeholder(dtype=tf.float32, shape=[2,2,16,16], name=\"input\")\r\n    #output size: h,w,d,b = 2,16,16,2\r\n    random_back_0 = tf.multiply(\r\n        data,\r\n        tf.Variable(numpy.random.rand(1).astype(dtype=numpy.float32)),\r\n        name='random_back_0'\r\n    )\r\n    #output size: h,w,d,b = 2,16,16,2\r\n    random_front_0 = tf.nn.local_response_normalization(\r\n        data,\r\n        depth_radius=2,\r\n        bias=1.0,\r\n        alpha=1.0,\r\n        beta=0.5,\r\n        name='random_front_0'\r\n    )\r\n    output = tf.add(\r\n        random_front_0,\r\n        random_back_0,\r\n        name='output'\r\n    )\r\n\r\n    #output size: h,w,d,b = 2,16,16,2\r\n    global_init = tf.global_variables_initializer()\r\n    if len(tf.global_variables()) > 0:\r\n        saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(global_init)\r\n        tf.train.write_graph(sess.graph, \"./\", 'model.pbtxt', as_text=True)\r\n        save_path = saver.save(sess, './model.ckpt')\r\n        print('Model saved in file: {}'.format(save_path))\r\n        # Look here for more details https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph_test.py\r\n        freeze_graph.freeze_graph(\r\n           os.path.join(\"./\", 'model.pbtxt'), # GraphDef\r\n           '',\r\n           False, # is the GraphDef in binary format\r\n           os.path.join(\"./\", 'model.ckpt'), # checkpoint name\r\n           'output', # output node name\r\n           '', '',\r\n           os.path.join(\"./\", 'model.frozen.pb'), # output frozen path graph\r\n           True, # clear devices info from meta-graph\r\n           '', '', '')\r\n        input_arrays = [\"input\"]\r\n        output_arrays = [\"output\"]\r\n        converter = tf.lite.TFLiteConverter.from_frozen_graph('./model.frozen.pb', input_arrays, output_arrays)\r\n        def representative_dataset_gen():\r\n            for _ in range(1000):\r\n                yield [numpy.array(numpy.random.randint(0, 255, size=(2, 2, 16, 16)),dtype=numpy.float32)]\r\n        converter.representative_dataset = representative_dataset_gen\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        tflite_model = converter.convert()\r\n        tflite_model_path = './model.tflite'\r\n        open(tflite_model_path, 'wb').write(tflite_model)\r\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\r\n    assert(interpreter is not None)\r\n    interpreter.allocate_tensors()\r\n```\r\n\r\n**Output**\r\n\r\nNo output, work fine\r\n\r\n**Model.tflite graph**\r\n\r\n![model tflite(1)](https://user-images.githubusercontent.com/29231663/85409979-d0a53100-b566-11ea-8940-04b6367b01cc.png)\r\n", "comments": ["Could you try the conversion with tf-nightly?", "I could run your above code without any problems on the tf-nightly version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "You may also try with TF version 2.2 \r\nThe code executes successfully see [gist](https://colab.research.google.com/gist/ymodak/7e23d4816c0859fa5b80d6a888a58074/github-issue_40716.ipynb).\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40716\">No</a>\n"]}, {"number": 40715, "title": "BERT has a invaild variable", "body": "tensorflow 2.2.0\r\ntensorflow_hub 0.8.0\r\n\r\n\r\nmy code is `bert_url = r\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2\"\r\n\r\n bert = hub.KerasLayer(bert_url, trainable=True)\r\n\r\n print(bert.variables[-1])`\r\nand the result is '**<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>**'\r\nmy another task is limited by the variable, how to remove it?", "comments": ["@transformerzhou,\r\nIssues related to Hub modules are tracked in the TensorFlow Hub repo. \r\n\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/hub/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40714, "title": "ValueError: return statements are not supported within a TensorFlow loop.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using return statements within for loops TensorFlow raises the error aforementioned in the issue title. \r\n\r\n**Describe the expected behavior**\r\n\r\nA workaround. As this behavior is unknown to me. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[Colab Notebook](https://colab.research.google.com/gist/sayakpaul/a069acdb369b25f48dd3844b4f701855/scratchpad.ipynb)\r\n\r\n**Other info / logs** \r\n\r\n**Update**\r\nWhen retrieving the dataset just register here: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database. It won't take more than 40 seconds. Once registered, one can update username and the password in the `wget` fields. \r\n", "comments": ["As per the [docs]( https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) the map function doesn't support arbitrary python code because it converts everything into graph. In order to run something more complex you need to use [tf.py_function](https://www.tensorflow.org/api_docs/python/tf/py_function).\r\n\r\nHowever, I think that this kind of task is better suited for different approach entirely. It would be much simple to process labels before you create the Dataset. You could, for example, use numpy or pandas.\r\n\r\nSame goes for IO operations, unless you are limited by memory.  ", "> As per the docs the map function doesn't support arbitrary python code\r\n\r\nThis section (https://www.tensorflow.org/tutorials/load_data/images#load_using_tfdata) showcases otherwise, though. Sorry if my understanding is wrong. \r\n\r\n\r\n> However, I think that this kind of task is better suited for different approach entirely. It would be much simple to process labels before you create the Dataset. You could, for example, use numpy or pandas.\r\n\r\nAny supporting minimal code snippet?", "In some cases Tensorflow can handle the code yes, but only if it can find appropriate graph representation for it. The problem stems from question of what can you define as computational graph. Think about it as if you were compiling a code for a virtual machine.\r\n\r\nYour code, in this case label processing, is taken by AutoGraph, which acts as a compiler, and it tries it's best to figure out how to make it work. But it's not perfect and sometimes has just no idea how to do it. So it fails. That being said, it can work and does work most of the time. But you still should consider alternatives. Speaking of alternatives...\r\n\r\nBoth numpy and pandas have a map (and map like) functions, and both libraries have lots of examples. The great thing is that they use same data structures as tensorflow, multidimensional arrays (Pandas builds on numpy). The only question is, which one is more to your liking. \r\n\r\nPandas works better with more complex data. It can read JSON or csv, and work with it straight away. Data processing and even basic plotting included. \r\n\r\nNumpy is for low level stuff, obviously the numerical data, but you can work with other things too. Although instead of map function it has vectorize which works in similar way, while offering better performance. \r\n\r\nIn general you want the right tool for the right job. Tensorflow offers lot of functionality but it isn't always consistent in quality. \r\n\r\n[Pandas map](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html?highlight=map)\r\n\r\n[Numpy vectorize](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html)\r\n\r\n[There's also a handy article about their performance, again with examples.](https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array)", "Fair points. \r\n\r\nHere's a bit more context as to why I would like to stick to `tf.data`. With a `tf.data.Dataset`abstraction I can avail some performance specific utilities such as [`prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch). ", "@sayakpaul \r\nPlease share all dependencies for us to rpelicate the issue faced, please find [the gist](https://colab.research.google.com/gist/Saduf2019/bc75fa9f584a62f522d67f6f3eecde4d/untitled243.ipynb) of the code shared.", "@Saduf2019 please take a look at the updated description. You did not pass the right username and password. I mentioned it in the updated issue, could you check it once and retry? ", "The third party dataset changed. \r\nAlso I think that we need to find a small test gist to reproduce this. Having a registration dependency and a quite large datasets as input, it doesn't help to monitor this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40714\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40714\">No</a>\n"]}, {"number": 40713, "title": "graph.o: In function `entry':", "body": "Hi,\r\n\r\nI am running into this strange issue. So I am able to compile using a specific TensorFlow library which is the open baselines and whenever I create a model with stable-baselines I keep getting this error when I go to compile. \r\n\r\nI am not a C++ or C wizard so this has proven to be very difficult to figure out for me. Any insight would be much appreciated.\r\n\r\nBeen trying to resolve this for 14 days now.\r\n\r\n```\r\ngraph/graph.o: In function `entry':\r\n__compute_module:(.text+0xafe): undefined reference to `__multi3'\r\n__compute_module:(.text+0xcfa): undefined reference to `__multi3'\r\n__compute_module:(.text+0xe90): undefined reference to `__multi3'\r\n__compute_module:(.text+0x1000): undefined reference to `__multi3'\r\n```\r\n", "comments": ["@DroneMesh \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "This looks like a linking issue but without knowing the issue template and the steps you are taking to build the code we cannot provide guidance. It is very likely that this is actually not a TF coding issues but a question for Stack Overflow (though we need to see the issue template and the steps you are taking to compile to be sure)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40712, "title": "How to run TF lite model on Nvidia GPU (NNAPI or GPU delegate)?", "body": "I found that TF lite targeted major mobile GPUs including Adreno, Mali, and PowerVR from closed issue (https://github.com/tensorflow/tensorflow/issues/34536).\r\n\r\n1. I wonder if... I can run TF lite model on the gpu, Nvidia T4.\r\n\r\n2. I wonder if... \"TF lite model is optimized to mobile gpus\" means the case of using NNAPI.\r\nI want to check if the sentence above means that ... the gpus that I can run TF lite model using NNAPI is limited to mobile GPUs. But, it's possible to infer TF lite model on other gpus like Nvidia gpu using GPU delegate.\r\n\r\nThank you!", "comments": ["1. I think with some plumbing, I think it should work for Nvidia GPUs.  For OpenGL backend, you will need Mesa.  For OpenCL backend, you will need OpenCL drivers for your Nvidia GPUs.  Same with Vulkan.\r\n\r\n2. I don't think the models are optimized with a particular backend in mind.  The runtime incl. GPU parameters such as the workgroup sizes however are optimized for mobile.  You might achieve better performance with different numbers."]}, {"number": 40711, "title": "Question: About Intra-threadpool core affinity", "body": "When we use the intra-threadpool, which I believe is based on pthread,it seems we didn't find some where the code would try to set the CPU hard affinity\r\nhttps://github.com/tensorflow/tensorflow/blob/e5023a1738cce7efcdf9d87863b85c80ab2f8c9e/tensorflow/core/common_runtime/local_device.cc#L114-L159\r\nby calling the functions like `pthread_setaffinity_np` and `pthread_getaffinity_np` \r\nIs anyone knowing the reason behind?\r\n", "comments": ["@Leslie-Fang \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "@Saduf2019 Thanks, I will open it in StackOverflow."]}, {"number": 40710, "title": "Add random access file and test", "body": "@mihaimaruseac \r\nThis PR add the first version of `tf_random_access_file` for `gcs` (without caching). This PR contain much boilerplate code so it is bigger than other PR.", "comments": ["@mihaimaruseac \r\nI add a header to expose symbol. However, this PR contains full of boilerplate code ( absl::string_view for example ). I will make a seperate PR to cleanup the test after this PR is merge\r\n\r\nEdit: Sorry, I misunderstood your review. I will make another PR to add the header right now", "@mihaimaruseac \r\nThis PR #40726 add the header to expose function. I think the best is I only keep the code for `tf_random_access_file` so that we could merge 2 PR independently. I will add test on the next PR. If we keep the test in this PR, we will have to wait for #40726 to be merged, after that, we rebase and we have to make some changes as well. What do you think ?", "Yes, let's wait for #40726 ", "I find that makes senses to use `std::string` instead of `char*`. After #40726 is merged, I will make another PR to change `ParseGCSPath` and `tf_writable_file` to use `std::string`. Finally, I will rebase and make this PR use `std::string` as well", "@mihaimaruseac \r\nThis PR use `std::string` instead of `char*` #40776 . I dont use `absl::string_view` because we need `substr` return `std::string` instead of `absl::string_view`", "@mihaimaruseac Thank you for the fix with `gcs_header`. I rebased and edited.", "@mihaimaruseac PR adds all missing function to `tf_writable_file` #40835 . I dont add test since the PR has a lot of code already. That PR will cause a conflict with this PR because I change `Init` of filesystem. Once one of these PRs is merge, I will rebase and edit the other PR", "@mihaimaruseac Rebase again"]}, {"number": 40709, "title": "What exactly does `tf.signal.fft` compute?", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/fft\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/fft\r\n\r\n## Description of issue (what needs changing):\r\nThe description is simply \"Fast Fourier transform.\" which isn't fully-specified. What is the exact function computed? Is there a normalization term of 1/sqrt(N) or 1/N? Or is the normalization constant entirely in the inverse FFT (which has equally underspecified documentation)?\r\n\r\n### Clear description\r\nA mathematical formula that specifies what `tf.signal.fft` implements would be nice. Likewise with the other FFT methods, the inverse FFT methods, and STFT methods.", "comments": ["https://pytorch.org/docs/master/generated/torch.fft.html can serve as a template for the kind of information that a developer would find useful.", "This should be the file to edit the docstring: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_FFT.pbtxt\r\nPull requests welcome \ud83d\ude00", "@sharvil Are you interested in raising a PR to update the docs? Thanks!", "@jvishnuvardhan, no, I won't be submitting a PR to update these docs.", "I would like to work on this issue", "@Patryk999 Please feel free to open a PR to update the doc and mention this issue number in that PR. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", ".", "Here's  quick test:\r\n\r\nhttps://colab.research.google.com/drive/1pm-CJXaeuti1OT-O1vloH_BtjoM3by8H?authuser=1#scrollTo=9hW4DhoNJjcy\r\n\r\nThe result is pretty clearly N-times the amplitude (rfft only returns the first half of the amplitudes).\r\n\r\nI'll see if I can patch those docs quick."]}, {"number": 40707, "title": "TF 2 Keras | DenseFeatures layer not referring to data set fields by names", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Colab default\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**:\r\nWhile using **tf.keras.layers.DenseFeatures** as input layer, TF doesn't give any error in case the input dataset is missing any field with the required field names as given in DenseFeatures. However this works absolutely fine when directly using the **tf.keras.Input()** - gives error **\"KeyError: fieldname\"**\r\n\r\n**Describe the expected behavior**\r\n**tf.keras.layers.DenseFeatures** should work absolutely like **tf.keras.Input()**\r\n\r\n**Standalone code to reproduce the issue**\r\nUse colab notebook - https://colab.research.google.com/drive/1luzdzGEdJqaUjmYGbGGPIx7DFSAHebnL?usp=sharing\r\n\r\n1. **dummy_model_1( Correct behavior )** in the code uses only tf.keras.input( ) as inputs to TF Keras model and gives error if dataset misses required fields:\r\n\r\nHere **reviews** was not passed as a field in the input dataset. -> So TF raised error\r\n\r\n```\r\ndef dummy_model_1(params):\r\n    METRICS = [\r\n            keras.metrics.RootMeanSquaredError(name='RMSE')\r\n    ]\r\n\r\n    B = tf.keras.Input((), dtype = tf.string, name = 'condition')\r\n    C = tf.keras.Input((), dtype = tf.string, name = 'reviews')\r\n\r\n    model = tf.keras.Model([B, C], [B, C])\r\n\r\n    #Set optimizer\r\n    opt = tf.keras.optimizers.Adam(lr= params['lr'], beta_1=params['beta_1'], \r\n                                        beta_2=params['beta_2'], epsilon=params['epsilon'])\r\n\r\n    #Compile model\r\n    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\r\n\r\n    #Print Summary\r\n    print(model.summary())\r\n    return model\r\n\r\nKeyError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\r\n        return self(x, training=False)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:719 call\r\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:826 _run_internal_graph\r\n        inputs = self._flatten_to_reference_inputs(inputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:926 _flatten_to_reference_inputs\r\n        return [tensors[inp._keras_history.layer.name] for inp in ref_inputs]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:926 <listcomp>\r\n        return [tensors[inp._keras_history.layer.name] for inp in ref_inputs]\r\n\r\n    KeyError: 'reviews'\r\n```\r\n\r\n2. **dummy_model_2( Wrong behavior )** in the code uses tf.keras.DenseFeatures( ) and tf.keras.Input( ) as inputs to TF Keras model and **DOESNT** gives error if dataset misses required fields. In fact I am not sure how it maps data from dataset to the model inputs:\r\n\r\nHere **reviews** & **drugsName** both were passed as incorrect fieldnames and don't exist in the input dataset. -> **TF did not give any error**. It should have given error for both the fields.\r\n\r\nIt seems that in this case, TF just picks up values from dataset as per the index and not actually from the fieldnames.\r\n\r\n```\r\ndef dummy_model_2(params):\r\n    METRICS = [\r\n            # keras.metrics.BinaryAccuracy(name='accuracy'),\r\n            keras.metrics.RootMeanSquaredError(name='RMSE')\r\n    ]\r\n\r\n    A = tf.keras.layers.DenseFeatures(feature_columns=f)({'drugsName' : tf.keras.Input(name='drugsName', shape=(1,), dtype=tf.string)})\r\n    B = tf.keras.Input((), dtype = tf.string, name = 'condition')\r\n    C = tf.keras.Input((), dtype = tf.string, name = 'reviews')\r\n\r\n    model = tf.keras.Model([{'drugsName' : tf.keras.Input(name='drugsName', shape=(1,), dtype=tf.string)}, B, C], [B, C])\r\n\r\n    #Set optimizer\r\n    opt = tf.keras.optimizers.Adam(lr= params['lr'], beta_1=params['beta_1'], \r\n                                        beta_2=params['beta_2'], epsilon=params['epsilon'])\r\n\r\n    #Compile model\r\n    model.compile(loss='mean_squared_error',  optimizer=opt, metrics = METRICS)\r\n\r\n    #Print Summary\r\n    print(model.summary())\r\n    return model\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/98a10b26666e5117229216072d4a7957/untitled241.ipynb).", "Thanks a lot, I hope this gets resolved quickly because it completely defies the purpose of having a **densefeature** layer in TF Keras if this is not fixed.", "Any response ? Issue seems to be also in 2.3 rc", "`DenseFeature` will be deprecated (in plan) in 2.4. Please use 2.3 experimental layers as in this [RFC](https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40707\">No</a>\n"]}, {"number": 40706, "title": "How to use Tensorflow Lite GPU support for python code", "body": "<em>I want to run tflite model on GPU using python code. But it seems that the code does not use GPU (There's no increase in GPU resource usage.). Is it possible to give an GPU-related option in \"tf.lite.Interpreter(model_path, option)\"? </em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Python version:  3.6\r\n- CUDA/cuDNN version: 10.1 \r\n- GPU model and memory: 2080 ti\r\n\r\n**Describe the current behavior**\r\ntflite model with python code runs on cpu.\r\n\r\n**Describe the expected behavior**\r\ntflite model with python code runs on gpu.\r\n\r\n", "comments": ["@swim77 \r\n\r\nPlease, let us know which tensorflow version you are using?.Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I downloaded the tflite model from https://www.tensorflow.org/lite/models/image_classification/overview.\r\n\r\nThe tensorflow version is 2.2.0.\r\n\r\nAnd used the simple code as below:\r\n\r\n> import cv2\r\n> import pathlib\r\n> import tensorflow as tf\r\n> \r\n> folder_path = './data'\r\n> \r\n\r\n> interpreter = tf.lite.Interpreter(model_path=\"test.tflite\")\r\n> input_details = interpreter.get_input_details()\r\n> output_details = interpreter.get_output_details()\r\n> interpreter.allocate_tensors()\r\n> \r\n> images = []\r\n> \r\n> for file in pathlib.Path(folder_path).iterdir():\r\n> \r\n>     img = cv2.imread(r\"{}\".format(file.resolve()))\r\n>     new_img = cv2.resize(img, (224, 224))\r\n> \r\n>     images.append(new_img)\r\n> \r\n>     interpreter.set_tensor(input_details[0]['index'], images)\r\n>     interpreter.invoke()\r\n>     output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\r\n", "TF Lite does not support Nvidia gpu. See https://github.com/tensorflow/tensorflow/issues/34536#issuecomment-565632906", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 40705, "title": "[INTEL MKL] Update sqlite version to 3.32.1 to fix the CVE-2020-13630 and CVE-202\u2026", "body": "\u20260-11656", "comments": ["Also CVE-2020-9327, CVE-2020-13631, CVE-2020-13435, CVE-2020-13434 and CVE-2020-11655\r\n\r\nNote that sqlite 3.32.2 has CVE-2020-13871.\r\n\r\nLet's update to 3.32.3 instead. If this lands before that I'll send an update immediately after."]}, {"number": 40704, "title": "Didn't find op for builtin opcode 'LOGISTIC' version '1'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Mac OS 10.16\r\n- TensorFlow installed from  source at  2020-06-22  ( git clone.. )\r\n- Target platform :  Arm Mbed OS\r\n\r\n**Describe the problem**\r\n\r\nRun-time error on FRDM-K66F board:\r\n```\r\nDidn't find op for builtin opcode 'LOGISTIC' version '1'\r\nFailed to get registration from op code LOGISTIC\r\n```\r\nI looked at the TFLite source code, at it uses LOGISTIC version=0 :\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/logistic.cc#L124\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nCompiled code as described here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-nxp-frdm-k66f\r\n\r\nInstead original model.cc I use my own model.cc which uses LOGISTIC op code.\r\nCompiled, deployed to board and got the run-time error on FRDM-K66F board:\r\n```\r\nDidn't find op for builtin opcode 'LOGISTIC' version '1'\r\nFailed to get registration from op code LOGISTIC\r\n```\r\n\r\n", "comments": ["Hi Michael,\r\n\r\nversion 0 is the default value passed in and it will be overwritten when the builtin operator is added. \r\n\r\nThe error message seem to suggest that you didn't register this operator in the resolver.\r\n\r\nCould you share which OpResovler are you using?\r\n\r\nAlso add @advaitjain who has been working on the versioning etc.\r\n\r\nThanks,", "Adding operation to resolver solves the issue.\r\nI expected what TFLite may figure out from the model file all  ops and register it automatically   ", "@mlubinsky-arm Could you please let us know if the issue still persists and feel free to close the issue if it is resolved for you ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40704\">No</a>\n"]}, {"number": 40703, "title": "[Regression] on_train_batch_begin callbacks with no batch number and size", "body": "Yep this should work in TF2.1 as well, here's a full example (had to fix the metric code a bit):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Count(tf.keras.metrics.Metric):\r\n  def __init__(self, name=None, dtype=None, **kwargs):\r\n    super(Count, self).__init__(name, dtype, **kwargs)\r\n    self.count = tf.Variable(0)\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    first_tensor = tf.nest.flatten(y_true)[0]\r\n    batch_size = tf.shape(first_tensor)[0]\r\n    self.count.assign_add(batch_size)\r\n\r\n  def result(self):\r\n    return tf.identity(self.count)\r\n\r\n\r\nclass PrintInfo(tf.keras.callbacks.Callback):\r\n  def on_train_batch_end(self, batch, logs):\r\n    print('Batch number: {}'.format(batch))\r\n    print('Samples seen this epoch: {}'.format(logs['counter']))\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\nmodel.compile(optimizer='sgd', loss='mse', metrics=[Count(name='counter')])\r\nx, y = tf.ones((10, 10)), tf.ones((10, 1))\r\nmodel.fit(x, y, batch_size=2, callbacks=[PrintInfo()], verbose=2)\r\n```\r\n\r\n_Originally posted by @omalleyt12 in https://github.com/tensorflow/tensorflow/issues/39314#issuecomment-630325421_\r\n\r\nI would like to use the same method, but with `on_train_batch_begin`, and it doesn't work. Actually the logs remain empty even after using the new metric. How can I use the batch size in a callback, with `on_train_batch_begin`?", "comments": ["@KTTrev \r\nPlease confirm if [this gist](https://colab.research.google.com/gist/Saduf2019/91c05885731c39ce65dbc80e2ae4416c/untitled237.ipynb) replicates the issue reported.", "The gist is working well. The issue I'm facing is when I change `on_train_batch_end` with `on_train_batch_begin`. I want to use the batch size at the beginning of a training, however the logs of `on_train_batch_begin` is empty. One solution that I found to solve this problem, is using a metric as quoted in  the code I mentioned above. However the code only works for `on_train_batch_end`. I'm looking for something similar, or another way to use the batch size in `on_train_batch_begin`.", "@KTTrev Based on the [Keras docs](https://keras.io/guides/writing_your_own_callbacks/), I think the dictionary of `logs` is empty when you use `on_train_batch_begin`. On that page, there is an example also which list the `keys` available to access.\r\n\r\n```\r\nStart epoch 0 of training; got log keys: []\r\n...Training: start of batch 0; got log keys: []\r\n...Training: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\r\n...Training: start of batch 1; got log keys: []\r\n...Training: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\r\n...Training: start of batch 2; got log keys: []\r\n...Training: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\r\n...Training: start of batch 3; got log keys: []\r\n...Training: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\r\n```", "Exactly, but do you have any solution on how I can use the batch size within 'on_train_batch_begin' (which was suppose to be in the logs)?", "@KTTrev Yes. This is regression issue. In TF2.1, you can do this \r\n\r\n```\r\n  def on_train_batch_begin(self, batch, logs=None):\r\n    keys = list(logs.keys()) # In TF2.2, this list is empty\r\n    print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\r\n    print('Batch number: {}'.format(batch))\r\n    print('Samples seen this epoch: {}'.format(logs['size']))\r\n```\r\n", "@KTTrev Thanks for the issue!\r\n\r\n`on_train_batch_begin` no longer receives any keys in the `logs`, could you explain your use case? I can advise on workarounds", "![image](https://user-images.githubusercontent.com/47731576/85640784-02b0b300-b68d-11ea-8e37-75b0fd282a17.png)\r\nHere is a screenshot of the model that I'm trying to build. You can notice that I'm using a function `MyFunction` at the output layer, which contain some parameters `param_1`, `param_2`, `param_3`, `param_4` . However these parameters are not static; each image going through the network uses diferent parameters. That's why, I want to update them during training, validation or prediction phases, especially when using mini batches.", "@KTTrev  As per the above comment, it is the expected behavior.  Closing this issue as of now. Please feel free to re open the issue if necessary. Thanks!.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40703\">No</a>\n"]}, {"number": 40702, "title": "Session crashes when I use TFLiteConverter with tf.lite.OpsSet.TFLITE_BUILTINS_INT8 option", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Colaboratory (Ubuntu 18.04.2 LTS)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary, nightly\r\n- TensorFlow version (use command below): '2.3.0-dev20200622'\r\n- Python version: Python 3.6.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA v10.0.130, cuDNN 7.6.0\r\n- GPU model and memory: K80\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n1. AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\n2. v1.12.1-34793-g072cf7ee4b 2.3.0-dev20200622\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to convert and quantize a mask_rcnn_inceptionv2 model from the model zoo for use on a Coral USB Accelerator and the Colab runtime crashes. \r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessful conversion w/o any crashes.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tarfile\r\nimport pathlib\r\n\r\nmodel_name = \"mask_rcnn_inception_v2_coco_2018_01_28\"\r\nbase_url = 'http://download.tensorflow.org/models/object_detection/'\r\nmodel_file = model_name + '.tar.gz'\r\nmodel_dir = tf.keras.utils.get_file(fname=model_name, origin=base_url + model_file, untar=True)\r\nmodel_dir = pathlib.Path(model_dir)/\"saved_model\"\r\nprint('Saved to {}'.format(model_dir))\r\n\r\nexport_dir = \"/root/.keras/datasets/mask_rcnn_inception_v2_coco_2018_01_28/saved_model\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\r\nconverter.experimental_new_converter=True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    yield [input]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_output_type = tf.int8 \r\ntflite_quant_model = converter.convert() # crashes on this line\r\nopen(\"converted_model_quant.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n\r\nJun 23, 2020, 7:09:41 AM | INFO | Discarding 6 buffered messages for 5081aeef-6fb9-46a7-a45d-3af457c9a931:aed26b5d8fdf4925f295f4d907ca617f\r\n-- | -- | --\r\nJun 23, 2020, 7:09:41 AM | INFO | Adapting to protocol v5.1 for kernel 5081aeef-6fb9-46a7-a45d-3af457c9a931\r\nJun 23, 2020, 7:09:39 AM | INFO | Starting buffering for 5081aeef-6fb9-46a7-a45d-3af457c9a931:aed26b5d8fdf4925f295f4d907ca617f\r\nJun 23, 2020, 7:09:39 AM | WARNING | zmq message arrived on closed channel\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayWriteV3 {device = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = i32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = true, tensor_array_name = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = f32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = true, tensor_array_name = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArraySizeV3 {device = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayScatterV3 {device = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayReadV3 {device = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<?x?x3>}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<>}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<3>}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100x4>}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100>}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.Size {device = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.NonMaxSuppressionV2 {T = f32, T_threshold = f32, device = \"\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | tf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\nJun 23, 2020, 7:09:11 AM | WARNING | error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\nJun 23, 2020, 7:09:10 AM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_2/TensorArrayWriteV3@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\nJun 23, 2020, 7:09:10 AM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_1/TensorArrayWriteV3@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\nJun 23, 2020, 7:09:10 AM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_4/TensorArrayWriteV3@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\nJun 23, 2020, 7:09:10 AM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite/TensorArrayWriteV3@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\nJun 23, 2020, 7:09:10 AM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/MultiClassNonMaxSuppression/SortByField/Size@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.Size' op is neither a custom op nor a flex op\r\n...\r\n...\r\n...\r\nloc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/MultiClassNonMaxSuppression/non_max_suppression_70/NonMaxSuppressionV2@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.NonMaxSuppressionV2' op is neither a custom op nor a flex op\r\nJun 23, 2020, 7:09:10 AM | WARNING | : error: 'tf.NonMaxSuppressionV2' op is neither a custom op nor a flex op\r\nJun 23, 2020, 7:09:10 AM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/MultiClassNonMaxSuppression/non_max_suppression_7/NonMaxSuppressionV2@_functionalize_body_0\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\"))\r\n...\r\n...\r\n...\r\n```\r\n", "comments": ["I think you need to enable control flow v2 ops.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2", "Tested with calling `tf.compat.v1.enable_v2_behavior()` before initializing the converter.\r\n\r\nIt now gives me: `error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag)`.\r\n\r\nAdding `tf.lite.OpsSet.SELECT_TF_OPS` by appending it tbo `converter.target_spec.supported_ops` gives me the same error.\r\n\r\n\r\nLog:\r\n```\r\nTimestamp | Level | Message\r\n-- | -- | --\r\nJun 25, 2020, 8:38:39 PM | INFO | Discarding 6 buffered messages for 715bab8b-b522-45ce-9411-e0e24104d8b6:14f1ce01a4e747a4f2ee7e3124a41b41\r\nJun 25, 2020, 8:38:39 PM | WARNING | stream.close()\r\nJun 25, 2020, 8:38:39 PM | WARNING | /usr/local/lib/python2.7/dist-packages/notebook/services/kernels/kernelmanager.py:246: UserWarning: Unregistering FD 35 after closing socket. This could result in unregistering handlers for the wrong socket. Please use stream.close() instead of closing the socket directly.\r\nJun 25, 2020, 8:38:39 PM | WARNING | stream.close()\r\nJun 25, 2020, 8:38:39 PM | WARNING | /usr/local/lib/python2.7/dist-packages/notebook/services/kernels/kernelmanager.py:246: UserWarning: Unregistering FD 32 after closing socket. This could result in unregistering handlers for the wrong socket. Please use stream.close() instead of closing the socket directly.\r\nJun 25, 2020, 8:38:39 PM | WARNING | stream.close()\r\nJun 25, 2020, 8:38:39 PM | WARNING | /usr/local/lib/python2.7/dist-packages/notebook/services/kernels/kernelmanager.py:246: UserWarning: Unregistering FD 29 after closing socket. This could result in unregistering handlers for the wrong socket. Please use stream.close() instead of closing the socket directly.\r\nJun 25, 2020, 8:38:39 PM | INFO | Adapting to protocol v5.1 for kernel 715bab8b-b522-45ce-9411-e0e24104d8b6\r\nJun 25, 2020, 8:38:36 PM | INFO | Starting buffering for 715bab8b-b522-45ce-9411-e0e24104d8b6:14f1ce01a4e747a4f2ee7e3124a41b41\r\nJun 25, 2020, 8:38:36 PM | WARNING | zmq message arrived on closed channel\r\nJun 25, 2020, 8:38:34 PM | WARNING | tcmalloc: large alloc 1527062528 bytes == 0x5625392e8000 @ 0x7fca003181e7 0x56246cc683ee 0x56246cce7930 0x56246cd128cd 0x56246cdad14e 0x56246cd05bba 0x56246cd032aa 0x56246cd1ef29 0x56246cceee6e 0x56246cdb6661 0x56246cd05bba 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd0a926 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa\r\nJun 25, 2020, 8:38:28 PM | WARNING | tcmalloc: large alloc 1527062528 bytes == 0x5625ebb40000 @ 0x7fca003181e7 0x56246cce95c2 0x56246cd5009e 0x56246cd786d5 0x56246cceee6e 0x56246cd965b6 0x56246cd05bba 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd0a926 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e\r\nJun 25, 2020, 8:38:26 PM | WARNING | tcmalloc: large alloc 1527062528 bytes == 0x562590aee000 @ 0x7fca003181e7 0x56246cc683ee 0x56246cce7930 0x56246cd128cd 0x56246cdad14e 0x56246cd05bba 0x56246cd032aa 0x56246cd1ef29 0x56246cceee6e 0x56246cdb6661 0x56246cd05bba 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd032aa 0x56246cd0adce 0x56246cd0a926 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd1f1cc 0x56246cceee6e 0x56246cd08092 0x56246cd032aa 0x56246cd0b39e 0x56246cd032aa\r\nJun 25, 2020, 8:38:08 PM | WARNING | tcmalloc: large alloc 1145217024 bytes == 0xa61b4000 @ 0x7f79204241e7 0x5ab685 0x578518 0x587f87 0x5a6bfb 0x536cf8 0x50a35c 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayWriteV3 {device = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = i32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = true, tensor_array_name = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayV3 {clear_after_read = true, device = \"\", dtype = f32, dynamic_size = false, element_shape = #tf.shape<*>, identical_element_shapes = true, tensor_array_name = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArraySizeV3 {device = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayScatterV3 {device = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayReadV3 {device = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<?x?x3>}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<>}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<3>}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100x4>}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.TensorArrayGatherV3 {device = \"\", element_shape = #tf.shape<100>}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.Size {device = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.NonMaxSuppressionV2 {T = f32, T_threshold = f32, device = \"\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | tf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\nJun 25, 2020, 8:38:08 PM | WARNING | error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\nJun 25, 2020, 8:38:07 PM | WARNING | loc(callsite(\"BatchMultiClassNonMaxSuppression_1/map/while/TensorArrayWrite_2/TensorArrayWriteV3@_functionalize_body_3\" at \"BatchMultiClassNonMaxSuppression_1/map/while/LoopCond\")): error: 'tf.TensorArrayWriteV3' op is neither a custom op nor a flex op\r\n...\r\n...\r\n...\r\n```", "Sorry. You need to enable that option before creating a TF graph, the saved model.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Ok, did that. Final code looks like so:\r\n\r\n```py\r\nimport tensorflow as tf\r\nimport tarfile\r\nimport pathlib\r\n\r\ntf.compat.v1.enable_v2_behavior()\r\n\r\nmodel_name = \"mask_rcnn_inception_v2_coco_2018_01_28\"\r\nbase_url = 'http://download.tensorflow.org/models/object_detection/'\r\nmodel_file = model_name + '.tar.gz'\r\nmodel_dir = tf.keras.utils.get_file(fname=model_name, origin=base_url + model_file, untar=True)\r\nmodel_dir = pathlib.Path(model_dir)/\"saved_model\"\r\nprint('Saved to {}'.format(model_dir))\r\n\r\nsaved_model_dir = \"/root/.keras/datasets/mask_rcnn_inception_v2_coco_2018_01_28/saved_model\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\nopen(\"converted_model_quant.tflite\", \"wb+\").write(tflite_quant_model)\r\n```\r\n\r\nAnd it gives out this error:\r\n```\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n```\r\n\r\nI also tried the code again w/o `enable_v2_behavior` and it worked. Might have been a problem with Colab itself?\r\nTechnically my issue is fixed and now looking at #22564 \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40702\">No</a>\n"]}, {"number": 40701, "title": "Update release notes for TensorFlow 2.3.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.3.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 40700, "title": "Add arm source file into aws-checksums", "body": "According to https://github.com/tensorflow/tensorflow/issues/40463#issuecomment-647640030 , seem the aws libs need to add the arm related libs during build tensorflow package.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40700) for more info**.\n\n<!-- need_sender_cla -->", "recheck", "@bzhaoopenstack Thank you for your contribution. Can you please sign CLA? Thanks!", "@gbaned Hi, I had signed the Individual CLA. But I'm not sure whether the cla:no label can not be removed.", "@bzhaoopenstack Can you use please make sure to use same GitHub username and email-id associated with it.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40700) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 40699, "title": "Quuantization.keras.quantize_model failed for subclasses model", "body": "We are using a subclassed mode:\r\nclass MyNet(tf.keras.Model):\r\n\r\nWhen trying to do \r\nmodel = MyNet()\r\nmodel = tfmot.quantization.keras.quantize_model(model)\r\n\r\nWe get:\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py\", line 134, in quantize_model\r\n    '`to_quantize` can only either be a tf.keras Sequential or '\r\n**ValueError: `to_quantize` can only either be a tf.keras Sequential or Functional model**\r\n\r\nSo does quantization does not works on subclasses network ?\r\nIf so how can I get it to run on quantization only edge devices ?", "comments": ["@ek9852,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!\r\n", "@amahendrakar \r\n# Tensorflow 2.2.0\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\nclass MyNet(tf.keras.Model):\r\n\r\n    def __init__(self, filter_nbr=8):\r\n        \"\"\"Init MyNet fields.\"\"\"\r\n        super(MyNet, self).__init__()\r\n\r\n        self.conv11 = tf.keras.layers.Conv2D(filter_nbr, kernel_size=3, padding='SAME')\r\n\r\n    def call(self, inputs):\r\n        \"\"\"Forward method.\"\"\"\r\n        # Stage 1\r\n        x11 = (tf.nn.relu(self.bn11(self.conv11(inputs))))\r\n\r\n        return x11\r\n\r\nmodel = MyNet()\r\nmodel = tfmot.quantization.keras.quantize_model(model)\r\n\r\n# Got:\r\n# ValueError: `to_quantize` can only either be a tf.keras Sequential or Functional model.\r\n\r\n# expected no error\r\n```\r\n", "Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9303ee19320d88829887a61245a678ca/40699.ipynb). Thanks!", "As the error message clearly suggests and looking into the [code](https://github.com/tensorflow/model-optimization/blob/570444d4b9afb56e91992e8f5ae61abb12f4384f/tensorflow_model_optimization/python/core/quantization/keras/quantize.py#L133), it only works for sequential or functional model, not subclassed  models.", "Closing this issue as its not a bug/performance issue. Please post this question in stackoverflow as there is a wider community to respond. Thanks!", "@ek9852 Hi, do you found a solution to this problem?"]}, {"number": 40698, "title": "Can i change channel count in tf.keras.preprocessing.image.ImageDataGenerator or tf.keras.preprocessing.image.DirectoryIterator?", "body": "In Xception Pretrained model, i'm trying to use 6 channel as an input(299, 299, 6).\r\n\r\nI'm using ImageDataGenerator and DirectoryIterator to load and preprocess data like this.\r\n\r\n```\r\n image_data_generator_train = ImageDataGenerator(\r\n                preprocessing_function = myFunc        \r\n                validation_split=val_ratio,\r\n    )\r\n```\r\n\r\n```\r\ntrain_data = DirectoryIterator(\r\n                directory=imagenet_train,\r\n                image_data_generator=image_data_generator_train,          \r\n                target_size=input_shape,\r\n                classes=train_classes,\r\n                class_mode='sparse',\r\n                batch_size=train_batch_size,\r\n                subset = 'training',\r\n                shuffle=True,\r\n                seed=2030,\r\n                interpolation='bilinear'\r\n)\r\n```\r\n\r\nand in myFunc, i'm trying to return 6 channel(299, 299, 6) but it occurs error because preprocessing_function's input and output should be same shape.\r\nso i want to make input 6 channels, like concatenate input with channel like \r\ntf.concat([img, img], axis = 2) to make shape (299, 299,6)\r\ncan i do this in directoryiterator or imagedatagenerator?\r\n", "comments": ["@DJLee68 \r\nPlease share complete indented code such that we could replicate the error faced, can you share the error or if possible share a colab gist with the error for us to analyse."]}, {"number": 40697, "title": "[-Wsign-compare] warning fixes batch 3", "body": "@mihaimaruseac \r\n\r\nassociated_warning_ids: [\r\n62, 64, 66, 67, \r\n68, 69, 71, 73,\r\n74, 75, 76, 77,\r\n78 \r\n]", "comments": ["A little strange @mihaimaruseac ,  I deleted it the first time. Nothing changed. I delete it now, and it the changes reflect. No problem though, the file is gone. \ud83d\udc4d "]}, {"number": 40696, "title": "Type annotations for tf dtypes", "body": "These changes allow TensorFlow dtypes to have their own types by defining Python dtype classes for all tf dtypes. This will allow us to type annotate Tensors and dtypes provided as arguments to functions.\r\n### Examples:\r\nWe can use this to annotate function arguments:\r\n```python\r\ndef foo(dt: dtypes.Float32) # dt is expected to be tf.float32\r\n```\r\n```python\r\ndef foo(dt: tf.Tensor[dtypes.Float32]) # dt is expected to be a Tensor with dtype=tf.float32\r\n```\r\nBefore: The type of tf.bool, tf.int16, tf.string, etc would be \u201cDType\u201d which meant it would not be possible to differentiate dtypes based on their types.\r\n\r\n### Changes:\r\n- 3b1c93b - create classes for each TF dtype to extend DType base class\r\n- e2931cc - add stub file to give each TF dtype their respective type\r\n- 0c1aed9 - annotations for dtype parameter for tf.saturate_cast\r\n- 2292cee & d7f5e05 - type decorators\r\n- cd36697 - annotations for dtype parameter for tf.constant", "comments": ["@rahul-kamat Can you please address Ubuntu Sanity errors? Thanks!", "@yashk2810 FYI", "@gbaned could you help get the import/copybara bit unstuck?", "This should be ready to merge.", "@rahul-kamat Can you please resolve conflicts? Thanks!", "The PR is still valid, but we've been delayed on the integration.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40696) for more info**.\n\n<!-- need_sender_cla -->", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 40695, "title": "Prefer Tensor._shape_tuple over Tensor._shape_as_list ", "body": "This PR removes the `Tensor._shape_as_list()` method. `Tensor._shape_tuple()` provides the same functionality and its use is more prevelent throughout the codebase. Since it is a private method this removal should not be a breaking change.\r\n\r\nThis change also improves performance of `Tensor._shape_tuple()` and `ResourceVariable._shape_tuple()` by about 20-25% as it avoids one list creation.", "comments": ["Thanks Lucas! I agree `_shape_as_list` has to go away, but unfortunately there are multiple users of this method in the internal codebase, so this change is best done internally.\r\n\r\nCC @jaingaurav.", "> I agree `_shape_as_list` has to go away, but unfortunately there are multiple users of this method in the internal codebase, so this change is best done internally.\r\n\r\n@superbobry I changed the PR to only remove the users of `_shape_as_list` in 0911ccb so that it won't break code that might be relying on `_shape_as_list`."]}, {"number": 40694, "title": "Call Context.ensure_initialized() method directly", "body": "`ensure_initialized()` does `context().ensure_initialized()`, which is unnecessary since we can just call `self.ensure_initialized()` directly.", "comments": []}, {"number": 40693, "title": "Update to have a better error message for tf.math.segment_[*]", "body": "This PR tries to address the issue in #40653 where the error message\r\nof `tf.math.segment_[*]` does not match the error.\r\n\r\nThis PR fixes #40653.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@jaingaurav Can you please review this PR ? Thanks!", "@yongtang  Here are the internal errors, can you please verify ?\r\n\r\nTraceback (most recent call last):\r\n  File \"/third_party/tensorflow/python/framework/ops.py\", line 1840, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\nthird_party.tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be at least rank 1 but is rank 0 for '{{node SegmentMean}} = SegmentMean[T=DT_UINT16, Tindices=DT_INT64](SegmentMean/data, SegmentMean/segment_ids)' with input shapes: [], [0].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/third_party/tensorflow/python/framework/test_util.py\", line 2963, in assertRaisesWithPredicateMatch\r\n    yield\r\n  File \"/third_party/tensorflow/python/kernel_tests/segment_reduction_ops_test.py\", line 264, in testDataInvalid\r\n    data=np.uint16(10), segment_ids=np.array([]).astype(\"int64\"))\r\n  File \"/third_party/tensorflow/python/ops/gen_math_ops.py\", line 8403, in segment_mean\r\n    \"SegmentMean\", data=data, segment_ids=segment_ids, name=name)\r\n  File \"/third_party/tensorflow/python/framework/op_def_library.py\", line 776, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/third_party/tensorflow/python/framework/ops.py\", line 3519, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/third_party/tensorflow/python/framework/ops.py\", line 2003, in __init__\r\n    control_input_ops, op_def)\r\n  File \"/third_party/tensorflow/python/framework/ops.py\", line 1843, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shape must be at least rank 1 but is rank 0 for '{{node SegmentMean}} = SegmentMean[T=DT_UINT16, Tindices=DT_INT64](SegmentMean/data, SegmentMean/segment_ids)' with input shapes: [], [0].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"/third_party/tensorflow/python/kernel_tests/segment_reduction_ops_test.py\", line 265, in testDataInvalid\r\n    self.evaluate(s)\r\n  File \"<embedded stdlib>/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/third_party/tensorflow/python/framework/test_util.py\", line 2968, in assertRaisesWithPredicateMatch\r\n    (str(type(e)), str(e)))\r\nAssertionError: Exception of type <class 'ValueError'>: Shape must be at least rank 1 but is rank 0 for '{{node SegmentMean}} = SegmentMean[T=DT_UINT16, Tindices=DT_INT64](SegmentMean/data, SegmentMean/segment_ids)' with input shapes: [], [0].", "Thanks @gbaned, the PR has been updated to match the error string in graph and eager mode. Think this will resolve the issue in internal tests. Can you give it a try and let me know if the issue still persists?", "Retriggering tests now."]}, {"number": 40692, "title": "Conv1DTranspose documentation inconsistent with code and unclear", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose)\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation claims that padding options {\"valid\" and \"same\"} are supported, but when following the code path to deconv_output_length at line 140 [here](https://github.com/tensorflow/tensorflow/blob/0c227aed65e62f741a88c9915923d262710fc8c9/tensorflow/python/keras/utils/conv_utils.py#L140) there is the option for {\"full\"} as well.\r\n\r\nAdditionally, the equation provided for calculating output shape merely says \"padding\" for a variable which is represented as a string in the API. This makes for a guessing game of how to achieve the desired output shape.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L16](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L16l)\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@EnderWiggin14 \r\n\r\n> The documentation claims that padding options {\"valid\" and \"same\"} are supported, but when following the code path to deconv_output_length at line 140 here there is the option for {\"full\"} as well.\r\n\r\nFull padding is just an extreme case of zero padding. When applied it makes output tensor larger than the input tensor. Unlike Tensorflow implementation, stand alone Keras doesn't allow 'full' padding. Instead it passes value of padding variable to  `_preprocess_padding`  helper function.\r\n\r\n```\r\ndef _preprocess_padding(padding):\r\n    \"\"\"Convert keras' padding to tensorflow's padding.\r\n\r\n    # Arguments\r\n        padding: string, `\"same\"` or `\"valid\"`.\r\n\r\n    # Returns\r\n        a string, `\"SAME\"` or `\"VALID\"`.\r\n\r\n    # Raises\r\n        ValueError: if `padding` is invalid.\r\n    \"\"\"\r\n    if padding == 'same':\r\n        padding = 'SAME'\r\n    elif padding == 'valid':\r\n        padding = 'VALID'\r\n    else:\r\n        raise ValueError('Invalid padding: ' + str(padding))\r\n    return padding\r\n\r\n```\r\n> Additionally, the equation provided for calculating output shape merely says \"padding\" for a variable which is represented as a string in the API. This makes for a guessing game of how to achieve the desired output shape.\r\n\r\nThe padding in this case means number of symbols, for example 0s, used to pad the tensor. Ideally it should be something like `padding_size`. \r\n\r\nThe docs are, just like the code, derived from stand alone Keras, which defines output shape for [2D Transposed convolution](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/) as:\r\n```\r\nnew_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\r\noutput_padding[0])\r\nnew_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\r\noutput_padding[1])\r\n```\r\n\r\n", "I think the documentation is correct. Following example fails;\r\n```python\r\n#TF 2.4.0-dev20200719\r\ntf.keras.layers.Conv1DTranspose(filters = 2, kernel_size = 3, strides=1, padding='full', output_padding=None)\r\n#Output -\r\nValueError: The `padding` argument must be a list/tuple or one of \"valid\", \"same\" \r\n(or \"causal\", only for `Conv1D). Received: full\r\n```", "@ymodak is correct about \"full not being an option. \r\n\r\n> Additionally, the equation provided for calculating output shape merely says \"padding\" for a variable which is represented as a string in the API. This makes for a guessing game of how to achieve the desired output shape.\r\n\r\nYes, it could help if that part were more clar about how to convert from a padding string to a padding-length. `padding=\"vaild\"`  == `padding=0` but `padding=\"same\"` is more complicated.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40691, "title": "ZeroPadding3D  shape bug", "body": "If the padding is 3 tuple of 2 ints: \r\n`((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))` When calculate the compute_output_shape, dim should add `padding[D][0] + padding[D][1] + input_shape[D]` instead of adding `2 * self.padding[D][0]`.\r\n```\r\ninput = np.ones((2, 4, 5, 3, 2)) # channels_last\r\npaddings = ((1, 2), (3, 4), (0, 2))\r\noriginal compute_output_shape: return TensorShape([2, 8, 13, 7, 2])\r\nRight size: TensorShape([2, 7, 12, 5, 2])\r\n```\r\nAnd `call` function holds the right shape.\r\n", "comments": ["> Thanks for the PR. Please add necessary unit tests to check & maintain this fix.\r\n\r\nUpdate test case and supports for different configurations, ie. `channels_first`, `channels_last`."]}, {"number": 40690, "title": "PaddingZero", "body": "", "comments": []}]