[{"number": 8814, "title": "Fixed python 3 compatibility issue for DeepDream example notebook", "body": "* Changed byte cast to compat.as_bytes() to ensure python 3.x compatibility", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8813, "title": "Express cherry-pick", "body": "Cherrypicking a CL that's blocking doc progress.", "comments": ["Failing test is a flake, but will rerun.\r\nJenkins, test this please."]}, {"number": 8812, "title": "Replay of #8637: Add the corresponding --action_env to .bazelrc from \u2026", "body": "\u2026./configure\r\n\r\nThe original PR got overriden by bc456e361d49d1d89a74b80060c70efb51fd7d87\r\n\r\nThis will make the fetch phase reproducible so the refetch does\r\nnot happens unless people re-run configure (the environment variable\r\nare stored in the .bazelrc file).\r\n\r\nFixes #8619.", "comments": ["What I don't see is how bc456e3 overwrite the original -- bc456e3's diff doesn't have any deleted lines in configure which `echo >> bazel.rc`", "Humm maybe I got the commit wrong...\n\nOn Wed, Mar 29, 2017, 9:58 PM Martin Wicke <notifications@github.com> wrote:\n\n> Merged #8812 <https://github.com/tensorflow/tensorflow/pull/8812>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8812#event-1021112538>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf2N0nzxdn32KGVyi0rrgUTsus39Iks5rqrfQgaJpZM4MtS5J>\n> .\n>\n", "Anyway, we're looking into this squashing problem. There's probably bad UX\nin the merging process which encourages this type of problem.\u200b\n"]}, {"number": 8811, "title": "Tensorflow Variables are Not Initialized using Between-graph Replication", "body": "I am running an issue for Distributed Tensorflow code with \"Between-Graph Replication\", which does not initialize Tensorflow Variables for some worker nodes. I posted the issue on StackOverflow: http://stackoverflow.com/questions/43084960/tensorflow-variables-are-not-initialized-using-between-graph-replication. My concern is if this is an issue/bug for `MonitoredTrainingSession` which uses `SessionManager` in Tensorflow. Thanks!\r\n", "comments": ["[Answered on Stack Overflow](http://stackoverflow.com/a/43100415/3574081)."]}, {"number": 8810, "title": "Cannot create a tensor of shape (?), but can create a tensor of shape (?, 1)", "body": "I tried to create a tensor of dynamic shape.\r\n\r\n```\r\nshape = tf.shape(boxes) # boxes' shape is unknown \r\nbatch_inds = tf.zeros((shape[0]), dtype=tf.int32, name='batch_inds')     # fails\r\nbatch_inds = tf.zeros((shape[0], 1), dtype=tf.int32, name='batch_inds')  # works\r\nbatch_inds = tf.reshape(batch_inds, [-1])                     \r\ntf.image.crop_and_resize(images, boxes, batch_inds,\r\n                                     [pooled_height, pooled_width],\r\n                                     method='bilinear',\r\n                                     name='Crop')\r\n```\r\nhmmm.. it seems a little bit clumsy to write code this way.", "comments": ["I think the failure is due to a type error. `(shape[0])` is a scalar `tf.Tensor`, whereas the `shape` argument to `tf.zeros()` must be a vector (1-D `tf.Tensor`). Note that `(shape[0])` and `(shape[0],)` have different types in Python, and the comma is necessary to turn the expression into a `tuple`. One of the following should work:\r\n\r\n```python\r\nbatch_inds = tf.zeros(shape[0:1], dtype=tf.int32, name='batch_inds') \r\n\r\n# N.B. The extra comma here turns the scalar tensor into a `tuple` containing on scalar tensor.\r\n# TensorFlow will convert that to a 1-D tensor.\r\nbatch_inds = tf.zeros((shape[0],), dtype=tf.int32, name='batch_inds') \r\n```", "@mrry Thank you very much!!! \r\nReally learned a lot"]}, {"number": 8809, "title": "an update for the tf.contrib.learn Quickstart example is needed", "body": "Hi all,\r\n\r\njust tried to start the script from here: https://www.tensorflow.org/get_started/tflearn\r\n\r\nfound one issue for python3 users:\r\n\r\n\r\n```python\r\nimport urllib\r\nraw = urllib.urlopen(IRIS_TRAINING_URL).read()\r\n\r\n```\r\n this returns:\r\n\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-32-2005abd64a8b> in <module>()\r\n----> 1 raw = urllib.urlopen(IRIS_TRAINING_URL).read()\r\n\r\nAttributeError: module 'urllib' has no attribute 'urlopen'\r\n\r\n```\r\n\r\nthe solution:\r\n\r\n```python\r\nimport urllib.request as ur\r\nraw = ur.urlopen(IRIS_TRAINING_URL).read()\r\n```\r\n\r\nBut, the main reason for this issue-ticket is **warning messages** like:\r\n\r\n```\r\nWARNING:tensorflow:From /Users/vadimborisov/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported. \r\n```\r\n\r\nor this\r\n\r\n```\r\nWARNING:tensorflow:From <ipython-input-28-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From <ipython-input-28-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\n```\r\n\r\nor like this:\r\n\r\n```\r\nWARNING:tensorflow:From /Users/vadimborisov/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nINFO:tensorflow:Starting evaluation at 2017-03-29-15:28:31\r\nINFO:tensorflow:Evaluation [1/1]\r\nINFO:tensorflow:Finished evaluation at 2017-03-29-15:28:32\r\nINFO:tensorflow:Saving dict for global step 4000: accuracy = 0.966667, auc = 0.998333, global_step = 4000, loss = 0.0793921\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n```\r\n\r\nSo, I believe, the introduction tutorials should be free from warnings. \r\n\r\nI can try to fix the warnings, if someone is interesting. \r\n\r\nI'm using Python 3.5.2 |Anaconda 4.3.1 (x86_64)| (default, Jul  2 2016, 17:52:12) .\r\n", "comments": ["Also please add DynamicRnnEstimator example. Looks like it a have different interface. Not like LinearRegressor and that is very confusing.", "@martinwicke, @wolfffg, are we planning an update here? Could you redirect to the new author?", "@wolffg\u200b\r\n"]}, {"number": 8808, "title": "[Cherry-pick] Contrib build changes", "body": "This change, already applied to master, adds support for more tf.contrib libraries on Windows in r1.1.", "comments": []}, {"number": 8807, "title": "error running tensorflow trained model c++", "body": "Hello,\r\n\r\nI am working on Tensorflow on c++ with other network. I trained facenet on MS-Celeb-1M then i created my graph.pb. I modified the example provided here : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image in order to test my network.\r\nIn main.cpp:    \r\n    `string graph = \"data/graph1.pb\";`\r\n    `string output_layer = \"InceptionResnetV1/Repeat/block35_5/Relu\";`\r\n\r\nI get this error if I test : \r\n    \r\n\r\n> Running model failed: Invalid argument: You must feed a value for placeholder tensor 'phase_train'     with dtype bool [[Node: phase_train = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0    /cpu:0\"]()]]\r\n", "comments": ["Are you feeding this value \"phase_train\" like it asks you to?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8806, "title": "Installation: Permission error", "body": "`(tensorflow) wermarter@Werma:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl\r\nCollecting tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl\r\n  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl\r\nCollecting protobuf>=3.1.0 (from tensorflow-gpu==1.0.1)\r\n  Using cached protobuf-3.2.0-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting numpy>=1.11.0 (from tensorflow-gpu==1.0.1)\r\n  Using cached numpy-1.12.1-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting six>=1.10.0 (from tensorflow-gpu==1.0.1)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting wheel>=0.26 (from tensorflow-gpu==1.0.1)\r\n  Using cached wheel-0.29.0-py2.py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached setuptools-34.3.3-py2.py3-none-any.whl\r\nCollecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached appdirs-1.4.3-py2.py3-none-any.whl\r\nCollecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached packaging-16.8-py2.py3-none-any.whl\r\nCollecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached pyparsing-2.2.0-py2.py3-none-any.whl\r\nInstalling collected packages: six, appdirs, pyparsing, packaging, setuptools, protobuf, numpy, wheel, tensorflow-gpu\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/home/wermarter/anaconda3/lib/python3.5/site-packages/pip/wheel.py\", line 329, in clobber\r\n    os.utime(destfile, (st.st_atime, st.st_mtime))\r\nPermissionError: [Errno 1] Operation not permitted\r\n`\r\nI've tried sudo -H but it said `tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.`. ", "comments": ["If you ran any sudo command in your virtualenv, the owner of some of the files will be root.\r\nYou will probably chmod all of your virtual environment, or create a new one.\r\n\r\nClosing as this is an issue with user setup, not a TF bug.", "I got the same error using anaconda on ubuntu 14.04. I used chmod 777 on the tensorflow env dir and it looked like I had full permissions? Is there some other dirs that I need to chmod? Thx", "I used the python 3.6 URL as that was the version of python in my tensorflow env in anaconda. Do I need to create an env for py 2.7 and install the 2.7 version of tf instead? TIA ", "I met a same trouble, and I hava fixed it by\r\n`sudo chown -R user_name:user_name file_directory`\r\nGood luck!", "@Viscoo  I encounter this problem today and your tip works. Thank you very much!"]}, {"number": 8805, "title": "valgrind helloworld.py throws 7805 errors", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nSeveral issues report memory leaks, but only for specific uses of Tensorflow:\r\nhttps://raw.githubusercontent.com/aymericdamien/TensorFlow-Examples/master/examples/1_Introduction/helloworld.py\r\nhttps://github.com/tensorflow/tensorflow/issues/700\r\nhttps://github.com/tensorflow/tensorflow/issues/4151\r\nhttp://stackoverflow.com/questions/35695183/tensorflow-memory-leak-even-while-closing-session\r\n\r\n### Environment info\r\nOperating System:\r\nLinux, Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\n\u276f ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root   556000 Jan 26 18:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan 26 18:51 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan 26 18:51 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   415432 Jan 26 18:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Jan 26 18:48 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 79337624 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Mar  9 14:14 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n`pip install tensorflow`\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n(env) \u276f python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n1.0.1\r\n```\r\n\r\nIf installed from source, provide \r\n**I'm listing both because I encountered the problem using both source and pip package.**\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n```\r\n~/tensorflow r1.0*\r\n\u276f git rev-parse HEAD\r\ne895d5ca395c2362df4f5c8f08b68501b41f8a98\r\n\r\n```\r\n\r\n2. The output of `bazel version`\r\n```\r\n\u276f bazel version\r\n............\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nvalgrind python helloworld.py\r\n```\r\nHere, `helloworld.py` refers to https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/helloworld.py.\r\n\r\nWhile there are certain cases in which memory violations are not a problem, I am trying to track down a segfault from using Tensorflow with Ros and Gazebo. It's very difficult to know whether one of the memory issues already present in Tensorflow is  responsible.\r\n\r\nFinal summary is as follows:\r\n```\r\n==18112== HEAP SUMMARY:\r\n==18112==     in use at exit: 8,356,021 bytes in 99,634 blocks\r\n==18112==   total heap usage: 775,798 allocs, 676,164 frees, 352,616,777 bytes allocated\r\n==18112== \r\n==18112== LEAK SUMMARY:\r\n==18112==    definitely lost: 154,618 bytes in 82 blocks\r\n==18112==    indirectly lost: 0 bytes in 0 blocks\r\n==18112==      possibly lost: 1,745,007 bytes in 32,420 blocks\r\n==18112==    still reachable: 6,456,396 bytes in 67,132 blocks\r\n==18112==         suppressed: 0 bytes in 0 blocks\r\n==18112== Rerun with --leak-check=full to see details of leaked memory\r\n==18112== \r\n==18112== For counts of detected and suppressed errors, rerun with: -v\r\n==18112== Use --track-origins=yes to see where uninitialised values come from\r\n==18112== ERROR SUMMARY: 7713 errors from 159 contexts (suppressed: 0 from 0)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nNone.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nThis [gist](https://gist.github.com/lobachevzky/0a7319f9cb5df23e32c1cc173210e768) contains the full output of `valgrind python helloworld.py`.\r\n", "comments": ["http://stackoverflow.com/questions/20593450/valgrind-and-cuda-are-reported-leaks-real\r\n\r\nLooks like valgrind can misbehave with CUDA linked in?\r\nCould you try the same thing on CPU-only implementation?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Ok. Sorry about that. For a while now, all the computers I've had access to either don't have valgrind (os x) or have GPUs."]}, {"number": 8804, "title": "why use unknown batch_size in BasicDecoder class?", "body": "I found that in the source code:[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py](basic_decoder.py)\r\n```python\r\n  @property\r\n  def batch_size(self):\r\n    return self._helper.batch_size\r\n\r\n  def _rnn_output_size(self):\r\n    size = self._cell.output_size\r\n    if self._output_layer is None:\r\n      return size\r\n    else:\r\n      # To use layer's compute_output_shape, we need to convert the\r\n      # RNNCell's output_size entries into shapes with an unknown\r\n      # batch size.  We then pass this through the layer's\r\n      # compute_output_shape and read off all but the first (batch)\r\n      # dimensions to get the output size of the rnn with the layer\r\n      # applied to the top.\r\n      output_shape_with_unknown_batch = nest.map_structure(\r\n          lambda s: tensor_shape.TensorShape([None]).concatenate(s),\r\n          size)\r\n      layer_output_shape = self._output_layer._compute_output_shape(  # pylint: disable=protected-access\r\n          output_shape_with_unknown_batch)\r\nreturn nest.map_structure(lambda s: s[1:], layer_output_shape)\r\n```\r\n\r\nAs above, since we can get the batch size by calling self._helper.batch_size, why it is set to be None in\r\n`lambda s: tensor_shape.TensorShape([None]).concatenate(s)`?  Why the batch size is still unknown? Couldn't we set to be `lambda s: tensor_shape.TensorShape([self._helper.batch_size]).concatenate(s)`?", "comments": ["@ebrevdo, could you take a look?", "Helpers batch size is a Tensor. Can't put it in a TensorShape.  Still we can probably do something here. Leave this issue open.", "@ebrevdo Is future activity likely?", "Yes, it's just lower priority but hope to fix by tf 1.3.\n\nOn Jun 16, 2017 2:06 PM, \"Geoffrey Irving\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Is future activity likely?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8804#issuecomment-309134067>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzZFgxdJ_0BjHdCMyTzfGVxRQzwfks5sEu5WgaJpZM4MtDkp>\n> .\n>\n", "@ebrevdo @girving ", "sorry; yes.  PRs are welcome!  i think we would need to use `tensor_util.constant_value(self.batch_size)` to get an integer or None; can't use a tensor there.", "or `tensor_util.constant_value_as_shape(self.batch_size).concatenate(s)` may be cleanest.", "@ebrevdo Ok!", "@ebrevdo ", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=8804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=8804\">No</a>\n"]}, {"number": 8803, "title": "how to read more than one tfrecord files once?", "body": "I want to do validation during training, but how to read two tfrecord files once, one is for training and annother one is for validation", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "that is easy, using tf.train.string_input_producer(filename_list)\r\nlook at tensorflow official document about read data"]}, {"number": 8802, "title": "Wheels on PyPI violate manylinux1 specification / PEP513", "body": "Follow-up issue to #5033.\r\n\r\nAs already pointed out by @jjhelmus, the Linux wheels on PyPI do not conform to the manylinux1 specification provided in [PEP513](https://www.python.org/dev/peps/pep-0513/). To quote @jjhelmus's comment from #5033:\r\n\r\n> Specifically, they contain shared libraries which reference versioned symbols which do not meet the GLIBC <= 2.5 and GLIBCXX <= 3.4.9 requirements specified in the PEP.\r\n\r\nIt would be great if the wheels could be fixed to match the specification.", "comments": ["@yifeif From what I understand on the issue, it was decided to just leave a note on the website?\r\nMaybe we should just remove manylinux1 wheel files, as they are leading to confusion.\r\nAs we only officially support ubuntu officially, is there a way to mark the wheel files as such, instead of the broader manylinux label?", "I don't understand the motivation to label the wheel \"manylinux\" combined with a note that it actually isn't a manylinux wheel. If it does not follow the specification, it's just a regular wheel and calling it manylinux is confusing.\r\n\r\nTo maximise compatibility, it would obviously be much preferred to follow the PEP and build the wheels against the official manylinux docker images (unless there is a true incompatibility).", "@bluenote10 thanks for raising the concern. We are not building against the official manylinux docker for many reasons such as we need nvidia docker to build GPU binaries, we don't have support for centOS etc. And manylinux1 is the only linux ABI Pypi supports now. We'd like to provide the convenience of downloading linux wheel through pip install for now until better solution comes up.", "Just chiming in here. It's counter-productive to claim your wheels are manylinux1-compliant when they aren't. This is producing annoyances for other projects in the ecosystem who have to deal with bug reports such as \"pyarrow crashes when I try to use it alongside tensorflow\".", "\"PEP 571 -- The manylinux2010 Platform Tag\"\r\nhttps://www.python.org/dev/peps/pep-0571/\r\n\r\n\"Tracking issue for manylinux2010 rollout\"\r\nhttps://github.com/pypa/manylinux/issues/179\r\n\r\n\"[Distutils] Opinions on requiring younger glibc in manylinux1 wheel?\"\r\nhttps://mail.python.org/mm3/archives/list/distutils-sig@python.org/message/M4MSVY5MPAPXFWHH4PBLE6PEBPOBIA44/", "The problem I have with is if I build and publish my own wheel internally, and use an extra-index-url to my pypi repo, the public pypi 'manylinux1' will be selected by pip over my internal 'linux' wheel.  If the published wheels were manylinux1, I wouldnt need to build my own.\r\n\r\n@yifeif can this be reopened please?"]}, {"number": 8801, "title": "[Tensorflow]RuntimeError: Graph is finalized and cannot be modified.", "body": "### Environment info\r\nOperating System:\r\nUbuntu 14.04  \r\ncuda 8.0 + cudnn 5.1\r\n\r\nI'm running a Cifar-10 tutorial from [https://www.tensorflow.org/tutorials/deep_cnn]\r\nAnd I modified the code a lot.\r\nSpecifically, I want the model can evaluate simultaneously while training, thus I can get a curve of precision. And I don't have ideal how the cifar10_eval.py and checkpoints work out. So I added an evaluation function to train( ) in cifar10_train.py, and I tried to run the evaluation function every 100 global_step. And then I got this:\r\n\r\n        RuntimeError: Graph is finalized and cannot be modified.\r\n\r\nHere is my evaluation function in cifar10.py:\r\n\r\n      def evaluation():\r\n        images_e, labels_e = inputs(eval_data=True)\r\n        logits_e = inference(images_e)\r\n        correct_pred = tf.equal(tf.argmax(logits_e, 1), tf.argmax(labels_e, 1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n        print('%s: accuracy @ 1 = %.3f' % (datetime.now(), accuracy))\r\n        tf.summary.scalar('accuracy', accuracy)\r\n\r\nAnd here is the call (the last few lines) :\r\n\r\n       def after_run(self, run_context, run_values):\r\n        if self._step % FLAGS.log_frequency == 0:\r\n          current_time = time.time()\r\n          duration = current_time - self._start_time\r\n          self._start_time = current_time\r\n\r\n          loss_value = run_values.results\r\n          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\r\n          sec_per_batch = float(duration / FLAGS.log_frequency)\r\n\r\n          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\r\n                        'sec/batch)')\r\n          print (format_str % (datetime.now(), self._step, loss_value,\r\n                               examples_per_sec, sec_per_batch))\r\n        if self._step % EVAL_STEP == 0: \r\n          print('evaluation\\n')\r\n          cifar10.evaluation()\r\n\r\nI don't know how to solve it. I do searched the Internet before but there is no similar situation. Is this caused by I called inference() twice in one global_step or else? Any suggestions would be helpful. Thank you!\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8800, "title": "Facing error while building APK ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### Currently trying to run tensorflow android code downloaded from Github facing following error\r\nError:Execution failed for task ':buildNativeMake'.\r\nA problem occurred starting process 'command 'tensorflow/contrib/makefile/build_all_android.sh''\r\n\r\n\r\n### Environment info\r\nOperating System: 64 bit\r\nAndroid Studio : 2.3\r\nBazel: 0.3.0\r\npython: 3.5\r\nJDK: 8\r\n\r\nPlease get back to us today ASAP\r\nThanking you in advance.\r\n\r\n[build_gradle.txt](https://github.com/tensorflow/tensorflow/files/878702/build_gradle.txt)\r\n\r\n\r\n\r\n", "comments": ["Closing as duplicate of #8797. Also please add your OS there, since 64 bit isn't very descriptive these days :)", "Hi Andrew Harp,\r\n\r\nI have referred your link and trying to run bash.exe on windows 7. I get error while executing following commands,\r\nStep 1:\r\nCommand: unzip android-ndk-r12b-linux-x86_64.zip -d ~/android\r\nError: unzip command is not present\r\n\r\nStep 2:\r\nCommand: tensorflow/contrib/makefile/build_all_android.sh \\\r\n          -s tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in \\\r\n          -t \"libtensorflow_inference.so libtensorflow_demo.so\"\r\nError: build_all_android.sh no such directory found\r\n\r\n\r\n\r\nI also tried another way which mention on Github, Following are the steps\r\n\r\n1.      Download bazel 0.3.0\r\n\r\n2.      Download Android NDK and unzip it somewhere\r\n\r\n3.      Clone Repo from github\r\n\r\n4.      Install Assets for Demo\r\n\r\n5.      Build native library\r\n\r\nCPU = armeabi \u2013v7a\r\n\r\nBazel build //tensorflow/examples/android:tensorflow_native_Libs \u2013crosstool_top = // external : android/crosstool \u2013cpu = $CPU \u2013 host_crosstool_top = @bazel_tools//tools/cpp:toolchain\r\n\r\n\r\n\r\nNATIVE_FOLDER = tensorflow/examples/android/libs/$CPU\r\n\r\nMkdir \u2013p $ NATIVE_FOLDER\r\n\r\ncp bazel-bin/ tensorflow/examples/android/libtensorflow_demo.so $ NATIVE_FOLDER\r\n\r\n\r\n\r\n6.      Set up the android project (opening project in android studio)\r\n\r\n1.      Open an existing android studio project\r\n\r\n2.      Configure AndroidManifest.xml\r\n\r\nI have stuck in highlighted steps. Can you please help me to solve it?\r\n\r\n\r\n\r\n\r\n\r\nFrom: Andrew Harp [mailto:notifications@github.com]\r\nSent: Thursday, March 30, 2017 12:26 AM\r\nTo: tensorflow/tensorflow\r\nCc: Patil, ttSamruddhi (IMMUC); Author\r\nSubject: Re: [tensorflow/tensorflow] Facing error while building APK (#8800)\r\n\r\n\r\nClosing as duplicate of #8797<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_8797&d=DwMCaQ&c=zdK58V2JKULZdB8nuBRpog&r=sT8ds7OoxLsENbOmz-CwDwfwt8Xi1Uvm-Nu-jp7EjT0&m=uZSl20XxGRdAUNf24n7W6wib1jM0rm2E6GljY8UfECE&s=W_H3UUnO5an60Rc4ymUUP50UHSFe8ofQTIMZO60_2qc&e=>. Also please add your OS there, since 64 bit isn't very descriptive these days :)\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_8800-23issuecomment-2D290189841&d=DwMCaQ&c=zdK58V2JKULZdB8nuBRpog&r=sT8ds7OoxLsENbOmz-CwDwfwt8Xi1Uvm-Nu-jp7EjT0&m=uZSl20XxGRdAUNf24n7W6wib1jM0rm2E6GljY8UfECE&s=YZJlTrn4Kv-geOFh5zmLFqUWV9YB2_KCuRXifOfbb1g&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AYHxmJr6sxNmmYnvlSRFtxwOw0LKjmOdks5rqqk4gaJpZM4MszAh&d=DwMCaQ&c=zdK58V2JKULZdB8nuBRpog&r=sT8ds7OoxLsENbOmz-CwDwfwt8Xi1Uvm-Nu-jp7EjT0&m=uZSl20XxGRdAUNf24n7W6wib1jM0rm2E6GljY8UfECE&s=__D3pTqxn4mtAA1VBZnRHWQGEDeoLgN_pz9ornvbD2U&e=>.\r\n"]}, {"number": 8799, "title": "Ensuring positive definite covariance matrix in tensorflow", "body": "Hi all,\r\n\r\nI have the problem how to ensure psd property of covariance matrix in tensorflow. so I map the rnn outputs to the $\\mu$ and $\\Sigma$ of a MVN. So here I have to make the covariance psd, I tried to use matrix exponential, but it is not implemented in tensorflow, and I tried SVD, but then my loss function is not differentiable. So I tried to use $LL^{T} + \\alpha |$, but still the program has bugs that the Sigma matrix is not invertible. Any suggestions for this problem? Thanks in advance!\r\n\r\n\r\n```\r\ndef get_lossfunc(mu, Sigma,input_data):\r\n    loss = 0\r\n    for i in range(len(mu)):\r\n        muC = tf.squeeze(mu[i])\r\n        SigmaC = Sigma[i]\r\n        inputC = input_data[i]\r\n        SigmaC = tf.matmul(SigmaC, tf.transpose(SigmaC)) + tf.eye(10)\r\n#         s,u,v = tf.svd(Sigma[0])\r\n#         SigmaC = tf.matmul(tf.matmul(u,tf.diag(tf.exp(s))),v)\r\n        dist = tf.contrib.distributions.MultivariateNormalFull(muC, SigmaC)\r\n        loss += -tf.log(dist.pdf(inputC))\r\n#         print loss\r\n    return loss\r\n```", "comments": ["This might be better suitable for stackoverflow, there are tricks to make sure your matrices are invertible (adding C*I to matrix)", "@yaroslavvb , I already used \"tf.matmul(SigmaC, tf.transpose(SigmaC)) + tf.eye(10)\", but there is still bug that the matrix is inconvertible.", "Try adding a multiple of TF.eye, if it's big enough, the inverse exists\n\nOn Mar 29, 2017 3:45 PM, \"KKKL\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> , I already used\n> \"tf.matmul(SigmaC, tf.transpose(SigmaC)) + tf.eye(10)\", but there is still\n> bug that the matrix is inconvertible.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8799#issuecomment-290248881>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHAcusTNKDo33hbO3P_OxSNKAktOrks5rqt7xgaJpZM4Msykq>\n> .\n>\n"]}, {"number": 8798, "title": "Not found error : Key tower/fully_connected/weights not found in checkpoint", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/6263\r\n\r\n### Environment info\r\nOperating System: Google Cloud Platform\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\npython eval.py --eval_data_pattern='gs://youtube8m-ml-us-east1/1/frame_level/validate/validate*.tfrecord' --train_dir=$BUCKET_NAME/LstmModel --run_once=True\r\n\r\n-----------------------------------------------------------------------------------------------------------\r\n\r\nCaused by op u'save/RestoreV2_2', defined at:\r\n  File \"eval.py\", line 332, in <module>\r\n    app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"eval.py\", line 328, in main\r\n    evaluate()\r\n  File \"eval.py\", line 309, in evaluate\r\n    saver = tf.train.Saver(tf.global_variables())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1040, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1070, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 675, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\nNotFoundError (see above for traceback): Key tower/fully_connected/weights not found in checkpoint\r\n         [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_2/tensor_names, sav\r\ne/RestoreV2_2/shape_and_slices)]]\r\n", "comments": ["Please provide information? What is eval.py? What command line are you running? ", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 8797, "title": "Currently trying to run tensorflow android code downloaded from Github facing following error as Error:Execution failed for task ':buildNativeMake'. A problem occurred starting process 'command 'tensorflow/contrib/makefile/build_all_android.sh'' Please reply asap", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Hi @patilsamruddhi, thank you for filling the issue as I oriented in response to your email. Could you please specify the code you tried to run, your OS and other information you find can be relevant? Thanks\r\n\r\ncc @andrewharp ", "@patilsamruddhi Are you trying to build on Windows? If so, see https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600 for instructions on using Bash for Windows to build Android TensorFlow.", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Dear Andrew Selle,\r\n\r\nFollowing are Version we use to build the project.\r\n\r\nOS Details: 64 bit operating system\r\nWindows: 7 Professional\r\nAndroid Studio: 2.3\r\ntensorflow: 1.0\r\nPython: 3.5.2\r\nbazel : 0.3.0\r\njdk: 8(windows)\r\nVisual studio: VS-2015\r\nNDK: android-ndk-r14b\r\n\r\nFrom: Andrew Selle [mailto:notifications@github.com]\r\nSent: Friday, March 31, 2017 1:01 AM\r\nTo: tensorflow/tensorflow\r\nCc: Patil, ttSamruddhi (IMMUC); Mention\r\nSubject: Re: [tensorflow/tensorflow] Currently trying to run tensorflow android code downloaded from Github facing following error as Error:Execution failed for task ':buildNativeMake'. A problem occurred starting process 'command 'tensorflow/contrib/makefile/b...\r\n\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_8797-23issuecomment-2D290519353&d=DwMFaQ&c=zdK58V2JKULZdB8nuBRpog&r=sT8ds7OoxLsENbOmz-CwDwfwt8Xi1Uvm-Nu-jp7EjT0&m=cCNrkh5b2ZWccDx_bTIovOAokrM_MF-mNLW2PSrQO0E&s=PniGCqRpudq5vc7TGrtya6i5ML37kb441RgQPloYFz0&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AYHxmHFc0Cbenf-5Fjn6zCPY1DSJIf3EXAks5rrALogaJpZM4MsxXw&d=DwMFaQ&c=zdK58V2JKULZdB8nuBRpog&r=sT8ds7OoxLsENbOmz-CwDwfwt8Xi1Uvm-Nu-jp7EjT0&m=cCNrkh5b2ZWccDx_bTIovOAokrM_MF-mNLW2PSrQO0E&s=F1BHsZrii6p42VUJb_TL_TDRyc9UZ2gcLnxQ283R2jE&e=>.\r\n", "@petewarden, it appears that this build method is not compatible with windows because it is trying to run a shell script. Is this expected? ", "That's correct. There is a workaround for building Android TF on Windows here: https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I am facing the error below. I tried running TF detect and it worked perfectly fine but I am not sure which change effected the project I am getting this error:\r\n\r\nError:Execution failed for task ':TensorFlow-Android-Inference:buildTensorflow'.\r\n> Process 'command 'tensorflow/contrib/makefile/build_all_android.sh'' finished with non-zero exit value 127\r\n", "Is there any activity on that? I have the same error using Mac OS 10.12. I followed the tutorial here: https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc "]}, {"number": 8796, "title": "Distributed mode hangs on in local mode", "body": "Looks like the workers do not start their server when using the estimator API and running in local-distributed mode. The minimalist code down helps reproduce: runs fine with only one process, but when launched with `TF_CONFIG={\"cluster\": {\"ps\":[\"localhost:5040\"], \"worker\":[\"localhost:5041\"]}, \"task\":{\"type\":\"ps\",\"index\":0}}` (resp. `worker`), it hangs on. After investigating, looks like in\r\n`tensorflow/contrib/learn/python/learn/experiment.py l.250-258`:\r\n\r\n    # Start the server, if needed. It's important to start the server before\r\n    # we (optionally) sleep for the case where no device_filters are set.\r\n    # Otherwise, the servers will wait to connect to each other before starting\r\n    # to train. We might as well start as soon as we can.\r\n    config = self._estimator.config\r\n    if (config.environment != run_config.Environment.LOCAL and\r\n        config.environment != run_config.Environment.GOOGLE and\r\n        config.cluster_spec and config.master):\r\n      self._start_server()\r\nthe server is not started when the environment is local, no matter distributed or not. When I force the _start_server() to be executed though, everything works just fine.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI wrote a stackoverflow thread here: http://stackoverflow.com/questions/43076035/tensorflow-minimalist-program-fails-on-distributed-mode\r\n\r\n### Environment info\r\nOperating System: Windows-64 bit\r\n\r\nInstalled version of CUDA and cuDNN: none, only on CPU\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: from install in the website (`pip install --upgrade tensorflow`)\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.contrib.learn.python.learn import learn_runner\r\n    from tensorflow.contrib import layers\r\n\r\n    DATA_SIZE=10\r\n    DIMENSION=5\r\n\r\n    def generate_input_fn():\r\n        def _input_fn():\r\n            mid = int(DATA_SIZE/2)\r\n            data = np.array([np.ones(DIMENSION) if x < mid else -np.ones(DIMENSION) for x in range(DATA_SIZE)])\r\n            labels = ['0' if x < mid else '1' for x in range(DATA_SIZE)]        \r\n            table = tf.contrib.lookup.string_to_index_table_from_tensor(tf.constant(['0', '1']))\r\n            label_tensor = table.lookup(tf.convert_to_tensor(labels, dtype=tf.string))\r\n            return dict(zip(['features'], [tf.convert_to_tensor(data, dtype=tf.float32)])), label_tensor\r\n        return _input_fn\r\n\r\n    def build_estimator(model_dir):\r\n        features = layers.real_valued_column('features', dimension=DIMENSION)\r\n        return tf.contrib.learn.LinearClassifier(\r\n            feature_columns=[features],\r\n            model_dir=model_dir)\r\n\r\n    def generate_exp_fun():\r\n        def _exp_fun(output_dir):\r\n            return tf.contrib.learn.Experiment(\r\n                build_estimator(output_dir),\r\n                train_input_fn=generate_input_fn(),\r\n                eval_input_fn=generate_input_fn(),\r\n                train_steps=1000)\r\n        return _exp_fun\r\n\r\n    if __name__ == '__main__':\r\n        tf.logging.set_verbosity(tf.logging.DEBUG)\r\n        learn_runner.run(generate_exp_fun(), 'job_dir')\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nIf server is not started, hangs on with log: `INFO:tensorflow:Create CheckpointSaverHook.`", "comments": ["@mrry, is distributed mode expected to work on Windows?", "The distributed-mode tests pass on Windows, so it is expected to work.", "having the same issue trying to use learn.Experiment in a distributed fashion, creating separate processes on a local Ubuntu 16.04, tensorflow 1.1 gpu install.  any progress on this issue?", "@lw394 haven't seen any progress but I did not have time yet to investigate more thoroughly though - wanted to have a glance at the unit tests and see if something makes a difference from my script and their tests (might be something wrong that we do... ). If you have access to sources, you can patch just like I proposed to make it work while waiting for a proper fix and a better understanding.", "@vbod thanks, I commented out `config.environment != run_config.Environment.LOCAL and`, and that appears to work.  I wonder why starting the server is prevented when using the Experiment class with a cluster definition locally.  ", "@mrry What's the status of this issue?", "It sounds like there might be a bug in the  environment-sensing part of the `tf.contrib.learn.Experiment` code. I believe it's in a state of flux, but @xiejw might know if there are any expected issues.\r\n\r\n", "This is intended. For environment in run_config,  'local' means not distributed. We probably should document this better. \r\n\r\nFor distributed training, environment should be set as 'cloud' in the TF_CONFIG. This covers the cases that all nodes in the cluster are localhost process. From implementation perspective, it is hard to define \"local distributed\" precisely. \r\n\r\nHope this clarifies. ", "That took care of it, thank you.  ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing this on the assumption that @xiejw's answer solves the original problem. @vbod, please feel free to reopen if you're still experiencing problems."]}, {"number": 8795, "title": "[CPU Performance Issue] Why don't hugepages, prefetch.", "body": "\r\nHugepages and Prefetch are considered as two main techniques for performance improvement.\r\n\r\nhttps://wiki.debian.org/Hugepages\r\nIntel\u00ae 64 and IA-32 Architectures Optimization Reference Manual (http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html)\r\n\r\nJust don't understand why TF does not use hugepages and prefetch techniques for CPU version.\r\n\r\n\r\nThanks for great TF.\r\n\r\n", "comments": ["Every processor feature takes time to use and implement, and as far as I know nobody has had time to implement it. Putting these in every individual kernel is time consuming. We're hoping that using the XLA compiler framework would be a great place to put these optimizations. On the other hand, we'd definitely welcome any help on these types of optimizations as community contributions!", "@pbar, @benoitsteiner, did either of you experiment with prefetching/hugepages.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8794, "title": " 'module' object has no attribute 'learn'", "body": "I'm trying tensorflow with some examples, when I run https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py, it raise the exception below:\r\n\r\n--------------------------------------------------------------------------------------------------\r\nFile \"/Users/cxm/cifar10-train.py\", line 124, in <module>\r\n  tf.app.run()\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n  _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\nFile \"/Users/cxm/cifar10-train.py\", line 120, in main\r\n  train()\r\nFile \"/Users/cxm/cifar10-train.py\", line 62, in train\r\n  global_step = tf.contrib.framework.get_or_create_global_step()\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 35, in __getattr__\r\n  contrib = importlib.import_module('tensorflow.contrib')\r\nFile \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n  __import__(name)\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 29, in <module>\r\n  from tensorflow.contrib import factorization\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/factorization/__init__.py\", line 24, in <module>\r\n  from tensorflow.contrib.factorization.python.ops.gmm import *\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py\", line 32, in <module>\r\n  from tensorflow.contrib.learn.python.learn import graph_actions\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/__init__.py\", line 83, in <module>\r\n  from tensorflow.contrib.learn.python.learn import *\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 23, in <module>\r\n  from tensorflow.contrib.learn.python.learn import *\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 24, in <module>\r\n  from tensorflow.contrib.learn.python.learn import datasets\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\", line 27, in <module>\r\n  from tensorflow.contrib.learn.python.learn.datasets import base\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 28, in <module>\r\n  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\nFile \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 36, in __getattr__\r\n  return getattr(contrib, item)\r\n\r\nAttributeError: 'module' object has no attribute 'learn'\r\n\r\n----------------------------------------------------------------------------------------------\r\nI run tensorflow on maOS Serria 10.12.3, python 2.7 I install tensorflow by pip install,\r\nbesides, when I run some other example of tensorflow.contrib, I, met the same exception, Any help? Thanks!\r\n", "comments": ["This issue disapears after  \"I pip install --upgrade tensorflow\" \r\nSuccessfully installed setuptools-34.3.3 tensorflow-1.0.1.\r\n", "Hmm... I'm having the same issue and when I run pip install it doesn't fix anything. I'm not sure why it isn't upgrading to the current version.\r\n\r\n## Here's the console log:\r\n\r\n![Console Log](https://i.gyazo.com/c6654d8ac03ca34e8d2d21918bfe94c2.jpg)\r\n\r\nAnyone know how I can fix this? - I also tried pip3 install, sudo install, etc."]}, {"number": 8793, "title": "python StagingArea documentation missing", "body": "StagingArea (and perhaps other structures in [data_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py)) are not present in the python documentation. They are however mentioned in the 1.0.0 and 1.1.0 [release notes](https://github.com/tensorflow/tensorflow/blob/346bd59b3c99055c3fdcbc0d0b0710418f48c5c6/RELEASE.md)\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nA search of the docs for [\"StagingArea\"](https://www.tensorflow.org/s/results/?q=StagingArea) doesn't reveal any of the docstrings present in [data_flow_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py).\r\n\r\n[Stage](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/stage) and Unstage are present in the C++ documentation.", "comments": ["StagingArea was added under tf.contrib.staging.StagingArea with this commit 8 days ago:   https://github.com/tensorflow/tensorflow/commit/90733a8024532f6993d11f0a89deaecb2bba59fe\r\n\r\nThis will allow the documentation to be generated for it.  The new docs should go up this coming weekend."]}, {"number": 8792, "title": "Queue item passed directly instead of via event", "body": "The event object created here was never being cleared till python termination when the worker was killed. This was causing memory leak as described in issue #8265  Resolving this by bypassing the object creation.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I've signed CLA as part of corporation InFoCusp. It has been approved by Google CLA Admin.", "@tensorflow-jenkins test this please", "Test failure due to dependency download. Rerunning @tensorflow-jenkins test this please.", "@tensorflower-gardener  @google-admin @googlebot I signed it! Corporation: Infocusp, Google group email id: tensorflow@infocusp.in", "@falaktheoptimist could you pull rebase and update?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Pulling from tensorflow/master, I saw that this has already been resolved by #8981. So, closing this one now."]}, {"number": 8791, "title": "[tensorboard] make projector plugin request data path relative", "body": "this PR solves #8342 \r\nAs in version 1.1rc0, the changes are made:\r\n>TensorBoard uses a relative data directory, for easier embedding.\r\n\r\n\r\nbut the request of project plugin data in Embeddings panel, e.g. `/data/plugin/projector/runs` still requests in a absolute path.  the code \r\n```html\r\n<vz-projector-dashboard\r\n            id=\"projector\"\r\n            route-prefix=\"/data/plugin/projector\">\r\n```\r\nwith the leading slash leads the  absolutely path request.\r\nthis PR remove the leading slash.\r\n\r\n\r\nUPDATED:\r\nnew version of code is:\r\n```new TF.Dashboard.VzProjectorDashboard('/data/plugin/projector'),```\r\nstill has a leading slash.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR @lspvic. @dandelionmane could you take a look?", "@dandelionmane how does that look?", "@tensorflow-jenkins Test this please"]}, {"number": 8790, "title": " crosstool_wrapper_driver_is_not_gcc failed: error executing command", "body": "I am installing tensorflow 1.0.0-rc0 with GPU, the environment is:\r\nOperating System: Ubuntu 14.04.4 LTS\r\ngcc version: 4.7\r\nbazel:0.4.5\r\npython 3.6\r\nCUDA 8.0\r\n cuDNN:5.1.5\r\ncuDNN is installed at /home/scs4450/CaffeInstall/cuda ranther than /usr/local/cuda-8.0\r\nwhen i ./configure my tensorflow, i get the error like this:\r\nscs4450@scs4450:~/tensorflow-1.0.0-rc0$ bazel build --copt=-march=native -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nINFO: Found 1 target...\r\nERROR: /home/scs4450/.cache/bazel/_bazel_scs4450/a3c5204dbd511f84e92c40ad5244a5e1/external/protobuf/BUILD:334:1: C++ compilation of rule '@protobuf//:protoc' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/scs4450/.cache/bazel/_bazel_scs4450/a3c5204dbd511f84e92c40ad5244a5e1/execroot/tensorflow-1.0.0-rc0 && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/cuda/lib64 \\\r\n    PATH=/home/scs4450/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.d '-frandom-seed=bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/host/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/main.cc -o bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: unrecognized command line option '-fno-canonical-system-headers'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.692s, Critical Path: 0.16s", "comments": ["I have a similar error. I cloned the current master directory, Ubuntu 16.04 LTS, Cuda 8.0, cudNN 5.1.5, gcc 4.9.3, python 3.5\r\n\r\n`\r\nERROR: /home/nils/Downloads/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 121 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::GetBlasGemmAlgorithms(std::vector<long long int>*)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:9: error: 'CUBLAS_GEMM_ALGO5' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:28: error: 'CUBLAS_GEMM_ALGO6' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                            ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:47: error: 'CUBLAS_GEMM_ALGO7' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                                               ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:64: error: unable to deduce 'std::initializer_list<_Tp>&&' from '{CUBLAS_GEMM_DFALT, CUBLAS_GEMM_ALGO0, CUBLAS_GEMM_ALGO1, CUBLAS_GEMM_ALGO2, CUBLAS_GEMM_ALGO3, CUBLAS_GEMM_ALGO4, <expression error>, <expression error>, <expression error>}'\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                                                                ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasOperation_t perftools::gputools::cuda::{anonymous}::CUDABlasTranspose(perftools::gputools::blas::Transpose)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:410:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasFillMode_t perftools::gputools::cuda::{anonymous}::CUDABlasUpperLower(perftools::gputools::blas::UpperLower)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:421:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasDiagType_t perftools::gputools::cuda::{anonymous}::CUDABlasDiagonal(perftools::gputools::blas::Diagonal)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:432:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t perftools::gputools::cuda::{anonymous}::CUDAComputationType(perftools::gputools::blas::ComputationType)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:519:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasSideMode_t perftools::gputools::cuda::{anonymous}::CUDABlasSide(perftools::gputools::blas::Side)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:443:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 156.966s, Critical Path: 118.40s\r\n`\r\n", "so far i get no idea, have you solved your problem?", "There was a bug due to a bad merge.\r\nCould you resync to master, and try again with bazel 0.4.5?", "@gunan I still get the same error message (still using bazel 0.4.5).\r\n\r\nEdit:\r\nI found another thread which might be related. #8462 ", "The root cause of the problem reported by @VersionHX is this\r\n```\r\ngcc: error: unrecognized command line option '-fno-canonical-system-headers'\r\n```\r\n\r\nIt is a duplicate of #8709 \r\nI will close this issue as a duplicate.\r\n\r\n@nmoenning your problem looks different. Could you file a new issue with all the information requested by the template?\r\n", "Why is everyone in a rush to close this issue when it's not fixed even with latest merge in the master? "]}, {"number": 8789, "title": "tensorflow can not be imported in python", "body": "Tensorflow  (CPU) can not be imported in python 2.7 \r\n\r\nOperating System: Linux; output of uname -a\r\nLinux laptop 4.4.14 #2 SMP Fri Jun 24 13:38:27 CDT 2016 x86_64 Intel(R) Core(TM) i3-4005U CPU @ 1.70GHz GenuineIntel GNU/Linux\r\n\r\nInstalled version of CUDA and cuDNN: NONE \r\n\r\nInstalled within virtualenv, via pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\r\n\r\nOutput from python while attempting to import tensorflow:\r\n-------------------------------------------------------------------------\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: /opt/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyUnicodeUCS4_AsUTF8String\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\n\r\n", "comments": ["Which linux distribution are you using?\r\nI think you are running into this: https://github.com/tensorflow/tensorflow/issues/5336", "Thanks gunan.\r\nI am using slackware 14.2 (63-bit).\r\nOn checking various issues (including the one that Gunan pointed out), I decided to compile TensorFlow from source. That involved first installing Bazel on my system, but it was successful and now I am able to import tensorflow in python and run various example programs."]}, {"number": 8788, "title": "Branch 151524146", "body": "", "comments": ["Waiting for an internal fix for the dependency failure here. Closing and re-push later."]}, {"number": 8787, "title": "TFRecord integration with Keras API", "body": "Since the Keras-2 API is now directly in TensorFlow, I think it would be very useful if there were a mechanism directly in TensorFlow to supply TFRecords to Keras, such as in a call to `model.fit()`, or with equivalent functionality to `flow_from_directory`.\r\n\r\nOne key implementation detail is with the way `model.compile()` works as [detailed in a comment on this topic in keras](https://github.com/fchollet/keras/issues/5358#issuecomment-282117525). As discussed in the same comment, numpy arrays have been determined to be the primary way of interacting with Keras, so this should not be an upstream Keras request because TFRecords currently require TensorFlow, so other backends couldn't be supported easily.\r\n\r\nNew Pull Request (includes examples):\r\nhttps://github.com/fchollet/keras/pull/6928\r\n\r\n(outdated) Starter code:\r\nhttps://github.com/farizrahman4u/keras-contrib/pull/27\r\n\r\nRelated Keras issues:\r\nhttps://github.com/fchollet/keras/issues/5356\r\nhttps://github.com/fchollet/keras/issues/5368\r\nhttps://github.com/fchollet/keras/issues/5325", "comments": ["Since Keras will work with Estimator API, we can put this integration in `tf.estimator.inputs`? I think you can actually just ingest your data from TFRecords source inside `input_fn` in Estimator API. Similarly, this will work with `Experiment` as well. ", "@terrytangyuan that sounds good, is what you describe forward looking or already integrated?", "I don't think it's integrated yet since Keras literally just landed in contrib. I am not in the internal team but @martinwicke knows the plan of things. ", "Keras just landed in contrib and will undergo some internal refactoring for a bit. The plan is to provide simple ways to:\r\n\r\n- use Keras inside a model_fn (already the case, Keras layers work fine inside Estimators)\r\n- use a compiled Keras Model to create an Estimator\r\n- add the ability for a Keras Model to accept an input_fn for its fit() method.\r\n\r\nThis will take a while but it's on the roadmap (@fchollet you may correct me on the last bullet).", "Thanks I saw it was just added, just wasn't sure of the status.\r\n\r\nUsing `Experiment` with Keras models does sound good. The other way around may also be useful as an option, where if you already have training code for `Model.fit()` and an existing TFRecord dataset.", "Yes, that would be the \"allowing input_fn for Model\".\n", "Sounds great! Thanks for clarifying, would it be good to leave this open to follow the progress?", "Yes, let's leave this open.", "I'm learning as I go but I've been looking into this a bit and here are my first thoughts.\r\n\r\nThese are from upstream keras, but these two commits/prs take care of a couple of the key parameterizations necessary for TFRecords to work:\r\n- [tensors feeding models](https://github.com/fchollet/keras/commit/e90b0713f2e86f9f540b57b4306a77e3f696213b) - numpy arrays no longer required, merged some time ago\r\n- [kwargs passed to session.run](https://github.com/fchollet/keras/pull/6693) - in progress at time of writing\r\n\r\nOne possible design is to create a `TFRecordInput()` Keras layer that wraps `data_flow_ops.RecordInput` which might be able to fit well with the [tf_cnn_benchmarks suggested high performance design](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/preprocessing.py#L360). Then calls like `model.fit()` could get their data from those input tensors.\r\n\r\n**Two possible cases I've considered for loading tensors**\r\n\r\n`Input()` or `TFRecordInput()` case:\r\nSomething like [these checks in the densenet model for the variable input_tensor](https://github.com/farizrahman4u/keras-contrib/blob/master/keras_contrib/applications/densenet.py#L126) might be possible to fold into the Input, or to put on a backend. \r\n\r\n`model.fit()` case:\r\n`model.fit` is after compile which instantiates the tf graph so this might be untenable. Nonetheless, in an ideal world one could create `data_flow_ops.RecordInput()`, loading/preprocessing ops, then those tensors could be fed to `model.fit()`, which could check the input type, and then the tensors could be passed through the keras backend to the TensorFlow backend.\r\n\r\n**Preprocessing Layers**\r\n\r\nTensorFlow has preprocessing layers and it might be possible to simply use them to create [Keras Preprocessing/Augmentation Layers](https://github.com/fchollet/keras/issues/6655). However there would ideally be some mechanism by which they can be separated from the Keras model when desired.\r\n\r\nHopefully none of this is too far behind tf's ongoing changes, any suggestions/feedback?", "I'm also very interested in using TF Record-files with Keras. I read several issues, that are related to this feature, read the stack-overflow question (https://stackoverflow.com/questions/42184863/how-do-you-make-tensorflow-keras-fast-with-a-tfrecord-dataset) and saw the pull request https://github.com/farizrahman4u/keras-contrib/pull/27 \r\n\r\nWhat is the current state of this Feature? \r\nCan I merge the pull request into my local repository, or will the final implementation very different to this?", "It will be very different. Implementation has not yet started, but it is good to show that there is interest in this feature. :-)", "Oh, too bad. When reading the stackoverflow answer, I really thought, that this feature is almost done :-) \r\n\r\nI was thinking about using Keras (the first time) for my next project. But since I have over 100GB TFRecord-Files as training data, this seems not to be an good option currently. \r\n\r\nHowever thank you very much for driving this topic. If there was a prototype implementation, I would have tested it with pleasure. But for a full implementation, I think I do not have enough knowledge about Keras at the moment. So I cannot help much. Thus I will check again in some month, if your ideas are implemented. Keep going \ud83d\udc4d ", "I'm in a similar situation to you, @Netzeband  \r\n\r\nI'm sure Andrew will correct me if I'm wrong, however, the code in the PR looks good as a single example on 28x28 single channel images. It doesn't appear that it will work with other datasets unless you change the code in the PR to suit your own dataset. For the code to be pulled into TensorFlow master, it'd need to be genericised and also be ported into Keras for use with Theano, so as not to fork the original Keras code base. \r\n\r\nI'm looking at using some of this PR code and modifying it to work with my data, not as part of TensorFlow, but as part of my own project. Unless there's some other way?", "@normanheckscher is correct", "here to show interest for this feature. thanks @ahundt for fighting the good fight!\r\n\r\nwould also like to train Keras models on TFRecord files.", "Notable progress in upstream Keras https://github.com/fchollet/keras/pull/6928 for `model.fit()`. Right now there are a few bugs caught by unit tests so I'd appreciate comments/suggestions.", "https://github.com/fchollet/keras/pull/6928 unit tests pass", "So, what should we do with this issue? Is this relevant or is it covered by recent pull from Keras? @fchollet?", "@martinwicke fchollet/keras#6928 integrates TFRecords via yield op support (i.e. [RecordInput](https://github.com/tensorflow/tensorflow/blob/833252af72af56661aefb0541163109132f9d4a6/tensorflow/python/ops/data_flow_ops.py#L2137)). The fifo queue way to get records and the new Dataset class may or may not work with Keras after those changes, but if they do an example is probably a good idea.\r\n\r\nI'd suggest not closing this until the changes are merged to TensorFlow from upstream Keras. If the other kinds of data input should also be supported this issue might still apply, but a new issue may also be appropriate.", "Based on the discussion in [Keras Input Tensor API Design Proposal](https://docs.google.com/document/d/1tf2Nl7wor8rmWPUoxfClLuPLQGqvZryegD7K7-1tTe8/edit?usp=sharing) and https://github.com/fchollet/keras/pull/6928#issuecomment-312088200 implementation of https://github.com/tensorflow/tensorflow/issues/10837 will be required for the ideal API design that supports TFRecords (and any data input op) to work.", "https://github.com/fchollet/keras/issues/7503 has the latest action plan, pieces of this functionality are being merged into keras upstream", "Will there be also an interoperability also with `tf.contrib.data.Dataset`?", "Yes, you can use the Dataset API to train Keras models. The standard\ntf.keras workflow would likely involve managing data via the Dataset API,\nbuilding your model via the Keras API, and training it via the Estimator\nAPI.\n\nOn 26 August 2017 at 09:17, bhack <notifications@github.com> wrote:\n\n> Will be also an interoperability also with tf.contrib.data.Dataset?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8787#issuecomment-325142623>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb1Cwm7tcXVGhPzsPmOmCG97I-zbqks5scEUcgaJpZM4MsVNC>\n> .\n>\n", "Interested also just found this when trying to move a tf.estimator based model to Keras and realizing the data prep and\r\n\r\n`training_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_data, y=training_label, batch_size=64, shuffle=True, num_epochs=None)`\r\n\r\nWould not work to feed Keras, looking forward to it landing, the ability to use the estimator and experiment stuff with Keras is interesting\r\n", "Checking in to announce my interest as well!", "The `estimator = model.get_estimator()` feature is now available for `tf.keras` (in TF 1.4).\r\n\r\nTo train a Keras model from TFRecords, please see this example: https://github.com/fchollet/keras/blob/master/examples/mnist_tfrecord.py", "Are there any disadvantages in converting input_tensor from tfrecords into a generator this way:\r\n\r\n```\r\ndef` tensors2gen(*input_tensors):\r\n    while True:\r\n        data = sess.run(input_tensors)\r\n        if len(input_tensors) == 1:\r\n            data = data[0]\r\n        yield data\r\n```\r\n\r\nand then train a model using `fit_generator` that also supports validation data parameter?", "What would happen here is that you process the inputs in the TensorFlow\ngraph, evaluate them individually (note that every Session.run call has\nsome non-trivial overhead, especially if you transfer data, such as res),\nand then do a bunch of work to get them back into the graph (which is what\nfit_generator does).\n\nIt would work, but it would be very inefficient.\n\nTry using model_to_estimator, or, especially if you're writing a model from\nscratch, put your model directly into a model_fn.\n\nMartin\n\nOn Fri, Jan 12, 2018 at 9:23 AM, \u041c\u0438\u0445\u0430\u0438\u043b \u041e\u0441\u044c\u043a\u0438\u043d <notifications@github.com>\nwrote:\n\n> Are there any disadvantages in converting input_tensor from tfrecords into\n> a generator this way:\n>\n> def` tensors2gen(*input_tensors):\n>     while True:\n>         res = sess.run(input_tensors)\n>         if len(input_tensors) == 1:\n>             res = res[0]\n>         yield res\n>\n> and then train a model using fit_generator that also supports validation\n> data parameter?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8787#issuecomment-357289279>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_QBThSQuVoklfCI-5GaomoDEHhvTks5tJ5UGgaJpZM4MsVNC>\n> .\n>\n", "Thank you for the clarification!\r\n\r\nI played wit tf estimators a bit. I have to train a stacked denoising autoencoder model - so i have to stack layers on top of the existed models and seems like there is no easy way to do this with estimators comparing to the simplicity of keras. And i also didn't find a good guide how to do validation during training phase with estimators (there are some complaints about weights reinitializations on test/validation switches). So i think i will stick with generators for now because i don't care much about performance at this point.", "Has anyone tried using tf-records to train a keras model with multiple inputs?\r\nI'm trying to train a keras model with multiple inputs using tfrecords.\r\nThe number of samples in both the inputs are the same but the 4th dimension is different. One input is an optical flow(along x and y axes (so two dimension)) and the other is a frame(with three color channels).\r\n\r\nValueError: Dimension 3 in both shapes must be equal, but are 2 and 3. Shapes are [10,240,320,2] and [10,240,320,3].\r\nFrom merging shape 0 with other shapes. for 'stack_1' (op: 'Pack') with input shapes: [10,240,320,2], [10,240,320,3].\r\n\r\nAny ideas on how i can proceed?"]}, {"number": 8786, "title": "Problem with processing/displaying RGB images", "body": "I just switched to a build of the r1.1 branch of tensorflow (due to a fixed bug in tfdbg present in 1.0) and without any changes to my model, RGB images saved with tf.summary.image are showing up weird in tensorboard, whereas they showed up fine in all previous versions of tensorflow/tensorboard.\r\n\r\nHere's an example of what I mean: http://i.imgur.com/xFoEYzO.png\r\n\r\nThose should be regular MSCOCO color images, but now the colors are all over the place.\r\n\r\nBefore being displayed, the images are adjusted as follows:\r\n\r\n        distorted_image = tf.cast(result.image_raw, tf.float32)\r\n        distorted_image = tf.reshape(distorted_image,\r\n                                     [self.width, self.height, 3])\r\n        distorted_image = tf.image.random_brightness(distorted_image,\r\n                                                     max_delta=35)\r\n        distorted_image = tf.image.random_contrast(distorted_image,\r\n                                                   lower=0.4, upper=1.4)\r\n        distorted_image = tf.image.random_hue(distorted_image, max_delta=0.01)\r\n        float_image = tf.image.per_image_standardization(distorted_image)\r\n\r\n\r\nand then the bounding boxes are drawn with: `tf.image.draw_bounding_boxes`\r\n\r\nthe images are logged using `tf.summary.image('images', preview_images, max_outputs=16)`\r\n\r\nI don't know if the issue is exclusive to tensorboard (conv layer activations seem to be displayed just fine, though the first layer conv activations look like they are working on the 'broken' image data as well, so it looks like it's not a tensorboard bug).\r\n\r\nThe images are read from a TFRecord file (which wasn't changed) and supplied with a shuffle_batch\r\n\r\nHave any of the image summary/image adjustment ops changed in a way that I'm not aware of, or is this a bug? It looks like it could have something to do with normalization of the image.\r\n", "comments": ["Looking at recent committed changes to image ops and after some testing, it is due to the change in 60e7360dfcf8951c4a269cfddd2a9cf2a05d7f91 with clipping float images to [0,1)\r\n\r\nFrom the commit message, it seems like this was an intended change, though it seems a bit weird to me that float images are clipped to 0,1 by this op to be be consistent with uint8 images, since floats can have a much larger range than uint8. And the documentation of the operation should be updated to reflect that it expects images in the 0,1 range. I'm wondering if this clipping actually makes sense, since usually, standardization/normalization is made as the last step of preprocessing, but if one has a float image with numbers 0.0-255.0, one might need to standardize twice now (before and after image adjustments), since the image adjustment ops could alter the statistics of the image, and I'm not entirely sure how normalizing twice could change the data.\r\n\r\nIf this change is working as intended and here to stay, then the [Cifar 10 tutorial example](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_input.py) should be changed, since it also exhibits this problem (and is what I based my code on).", "@shlens, what do you think here. This seems like a backward incompatible change (even if the different data types were consistent), on the other hand the adjust_brightness function is a pretty strange definition of brightness, because it seems to reduce the contrast at the same time as changing the brightness. I would have implemented it as using hsl or multiplying by a constant. Do we have any notion of high dynamic rnage image that we want to support?", "We should roll this back. The change in 60e7360 was backwards incompatible and shouldn't have gotten in. I'll send a PR. @gunan @av8ramit FYI, this should go into RC1 if we still have the option.\r\n\r\nIt's unsurprising that functions behave differently when operating on different data types, so I don't agree with the reasoning behind the change. The difference comes from the range of uint8 being limited, and it's unreasonable to force float32 to be limited to [0,1) as well. If you want to, the clip_by_value call can be made explicitly.", "@martinwicke Did that rollback happen?", "I believe so."]}, {"number": 8785, "title": "uninstall tensorflow from windows", "body": "Hello,\r\nI installed python and tensorflow in windows 10. How can I uninstall it?\r\n\r\nThanks in advance", "comments": ["Hi @ymakkapa, `pip uninstall tensorflow` will do it if you installed through `pip`.", "Hi @Carmezim , I did as you said. It processed for a while and I got the following errors\r\n![uninstall](https://cloud.githubusercontent.com/assets/20130992/24431763/f609b332-13eb-11e7-8559-32330a3fbae5.PNG)\r\n", "Can you show what you did before please? I need all the steps you took to install and now as well to uninstall TF and also if is TensorFlow with or without GPU support.\r\nAlso, upgrade pip as the warning message suggests.", "I cannot show as I closed that window. I installed it with pip and without GPU. ", "Can you reproduce it and share the steps?", "Hi, running the same line again gives the following output. I think tensorflow is uninstalled\r\nC:\\WINDOWS\\system32>pip uninstall tensorflow\r\nCannot uninstall requirement tensorflow, not installed\r\nYou are using pip version 8.1.1, however version 9.0.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\n\r\n", "It is uninstalled. If you don't have any other questions, the issue can be closed now otherwise feel free to ask. \r\nThank you for opening the issue and bearing with me.", "I have a similar issue. I have installed tensorflow GPU version successfully.\r\nBut  when I try to import tensorflow in a python program, I get a similar error.\r\nI then uninstalled the GPU version, keeping only the CPU version.\r\n\r\nThen, import tensorflow, works fine for me.\r\nBut I need to work on the GPU version. Help me solve the problem. Please!!", "I too have same issue, as I install tensflow-gpu 1.8.0 using pip but when I uninstall it and it does not.\r\n\r\n![tensor issue](https://user-images.githubusercontent.com/36807429/39409312-2288920c-4bfe-11e8-9242-19306835d079.PNG)\r\n", "I did \" pip uninstall tensorflow-gpu \" and \" pip uninstall tensorflow-gpu==1.8.0 \", this happens\r\n\r\n![tensor issue 2](https://user-images.githubusercontent.com/36807429/39409327-9dafc25c-4bfe-11e8-9c3e-13f9cd173151.PNG)\r\n\r\n\r\n\r\n", "Same thing happened to me too!\r\n![capture](https://user-images.githubusercontent.com/32133971/41425574-233d9338-6fcf-11e8-96bd-5c7148719ed4.PNG)\r\n", "@fzr2009 @mangozy @anyoneelse I ran into this issue recently. I was able to fix it by allowing [longer filenames in Python](https://docs.python.org/3/using/windows.html#removing-the-max-path-limitation).", "Had same issue today. Python 3.6.  Tensorflow GPU 1.9.0.  Cant uninstall, same \r\n\r\n`FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\xxx\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-tmnfhchs\\\\users\\\\xxx\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h'`", "Why was this closed? Is it fixed?\r\n\r\nI followed the instructions at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake to compile tensorflow with GPU from source on Windows 10 using CMake. The I install the generated `.whl` file using `pip`.\r\n\r\nThen I try to uninstall the fresh installed TF with `pip uninstall tensorflow-gpu`. Error was:\r\n\r\n> Exception:\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\shutil.py\", line 544, in move\r\n>     os.rename(src, real_dst)\r\n> FileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\users\\\\t-zifeng\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tf1.8-src\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\tensor_forest\\\\hybrid\\\\python\\\\models\\\\__pycache__\\\\stochastic_hard_decisions_to_data_then_nn.cpython-35.pyc' -> 'C:\\\\Users\\\\t-zifeng\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-jfv6m05f\\\\users\\\\t-zifeng\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tf1.8-src\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\tensor_forest\\\\hybrid\\\\python\\\\models\\\\__pycache__\\\\stochastic_hard_decisions_to_data_then_nn.cpython-35.pyc'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\site-packages\\pip\\_internal\\basecommand.py\", line 228, in main\r\n>     status = self.run(options, args)\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\site-packages\\pip\\_internal\\commands\\uninstall.py\", line 68, in run\r\n>     auto_confirm=options.yes, verbose=self.verbosity > 0,\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 661, in uninstall\r\n>     uninstalled_pathset.remove(auto_confirm, verbose)\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 219, in remove\r\n>     renames(path, new_path)\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 273, in renames\r\n>     shutil.move(old, new)\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\shutil.py\", line 558, in move\r\n>     copy_function(src, real_dst)\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\shutil.py\", line 257, in copy2\r\n>     copyfile(src, dst, follow_symlinks=follow_symlinks)\r\n>   File \"C:\\Users\\t-zifeng\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf1.8-src\\lib\\shutil.py\", line 121, in copyfile\r\n>     with open(dst, 'wb') as fdst:\r\n> FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\t-zifeng\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-jfv6m05f\\\\users\\\\t-zifeng\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\tf1.8-src\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\tensor_forest\\\\hybrid\\\\python\\\\models\\\\__pycache__\\\\stochastic_hard_decisions_to_data_then_nn.cpython-35.pyc'\r\n\r\nI tried @maxamante 's suggestion about enabling longer filenames but it didn't help.\r\n", "pip uninstall tensorflow on Windows fails with the same error on my machines. There is no reason to close this issue.", "@maxamante 's solution works well. The install directory of tensorflow does have a path which is too long.", "This issue should not be closed. I have the same error and I tried @maxamante  solution on two different Windows systems and it doesn't change anything for me. Below is the traceback I got. Please help:\r\n\r\nException:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\shutil.py\", line 544, in move\r\n    os.rename(src, real_dst)\r\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\users\\\\ua12\\\\appdata\\\\local\\\\conda\\\\conda\\\\envs\\\\dlcdependencies\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h' -> 'C:\\\\Users\\\\ua12\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-n6zjpcds\\\\users\\\\ua12\\\\appdata\\\\local\\\\conda\\\\conda\\\\envs\\\\dlcdependencies\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\pip\\_internal\\basecommand.py\", line 228, in main\r\n    status = self.run(options, args)\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\pip\\_internal\\commands\\uninstall.py\", line 68, in run\r\n    auto_confirm=options.yes, verbose=self.verbosity > 0,\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 661, in uninstall\r\n    uninstalled_pathset.remove(auto_confirm, verbose)\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 219, in remove\r\n    renames(path, new_path)\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 273, in renames\r\n    shutil.move(old, new)\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\shutil.py\", line 558, in move\r\n    copy_function(src, real_dst)\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\shutil.py\", line 257, in copy2\r\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\r\n  File \"C:\\Users\\ua12\\AppData\\Local\\conda\\conda\\envs\\DLCdependencies\\lib\\shutil.py\", line 121, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ua12\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-n6zjpcds\\\\users\\\\ua12\\\\appdata\\\\local\\\\conda\\\\conda\\\\envs\\\\dlcdependencies\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h'\r\nYou are using pip version 10.0.1, however version 18.0 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.", "Please reopen this issue because @maxamante 's approach doesn't resolve the issue,\r\nand I have the same error as @Umar-Ayub does on TensorFlow GPU version 1.10.0\r\nThank you.", "Does the below mean tensorflow is uninstalled?\r\n\r\n>>conda remove -n tensorflow pip python=3.6\r\nSolving environment: done\r\n\r\n\r\n==> WARNING: A newer version of conda exists. <==\r\n  current version: 4.5.4\r\n  latest version: 4.5.11\r\n\r\nPlease update conda by running\r\n\r\n    $ conda update -n base conda\r\n\r\n\r\n\r\n## Package Plan ##\r\n\r\n  environment location: C:\\Users\\Chand\\Anaconda3\\envs\\tensorflow\r\n\r\n  removed specs:\r\n    - pip\r\n    - python=3.6\r\n\r\n\r\nThe following packages will be REMOVED:\r\n\r\n    certifi:      2018.8.24-py36_1\r\n    pip:          10.0.1-py36_0\r\n    python:       3.6.6-hea74fb7_0\r\n    setuptools:   40.2.0-py36_0\r\n    wheel:        0.31.1-py36_0\r\n    wincertstore: 0.2-py36h7fe50ca_0\r\n\r\nProceed ([y]/n)? y\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n", "@Umar-Ayub @willSapgreen @fzqneo  Are you trying to uninstall? Do the files exist even though a FileNotFoundError is raised?\r\n\r\nUPDATE: From the errors @Umar-Ayub and @fzqneo  posted it looks like you're using Conda? Here's an issue that might explain the errors you're seeing https://github.com/conda/conda/issues/7203\r\n\r\n@bhumireddyc does `conda list` show `tensorflow` installed?", "I have the same problem!! I already tried the solution mentioned here of enabling long paths but im still getting the same results. Please Help!\r\n\r\n\r\n\r\nException:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\daniel domit\\appdata\\local\\conda\\conda\\envs\\robond\\lib\\shutil.py\", line 544, in move\r\n    os.rename(src, real_dst)\r\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\users\\\\daniel domit\\\\appdata\\\\local\\\\conda\\\\conda\\\\envs\\\\robond\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorcontractionthreadpool.h' -> 'C:\\\\Users\\\\DANIEL~1\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-lyhm1m_z\\\\users\\\\daniel domit\\\\appdata\\\\local\\\\conda\\\\conda\\\\envs\\\\robond\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorcontractionthreadpool.h'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n", "Why the issue is closed? we are still facing this problem", "I have found a workaround , \r\n`pip show tensorflow-gpu` \r\nOpen the installed directory as shown by the above output and delete all tensorflow folders there.", "> \r\n> \r\n> I have found a workaround ,\r\n> `pip show tensorflow-gpu`\r\n> Open the installed directory as shown by the above output and delete all tensorflow folders there.\r\n\r\nWorked for me.", "just go to C:\\Users\\USERNAME\\AppData\\Local\\Programs\\Python\\Python35\\Lib\\site-packages\r\nand rename the folder of tensorflow to something else. That worked for me!", "> pip show tensorflow-gpu\r\n\r\nWorked for me as well", "Since the issue is really due to long file name, it should be temporarily fixed using the Registry-longfilename setting as mentioned at - https://github.com/conda/conda/issues/7203. This setting can be changed back to 0 after the uninstall to avoid breaking other tools. Worked for me (Conda Py3.7/tf1.13).", "The uninstall worked successfully for me with `conda uninstall tensorflow`. pip was giving me the same error.", "> @fzr2009 @mangozy @anyoneelse I ran into this issue recently. I was able to fix it by allowing [longer filenames in Python](https://docs.python.org/3/using/windows.html#removing-the-max-path-limitation).\r\n\r\nThank you, it's useful.", "ERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-d6579f534729>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n ", "I am also facing the same issue please someone help me on this", "The pip uninstall was taking longer time even 2 hour and nothing happens. I went to the tensorflow location and deleted the folder. e.g. for me the path was- C:\\Users\\Username\\Anaconda3\\Lib\\site-packages. I deleted anything that starts with tensor.\r\n I then install tensorflow using conda and then keras."]}]