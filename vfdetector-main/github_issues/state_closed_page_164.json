[{"number": 49866, "title": "recognise silence 0 ", "body": "esp-idf 4.0\r\ndivice\uff1aesp32 eye\r\n\r\nbuild  code ok and flash \r\n\r\nbut\uff1a\r\nrecognise silence 0 \r\n\r\nwhy\uff1f", "comments": ["@coffice12 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the tensorflow version,the complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49866\">No</a>\n"]}, {"number": 49865, "title": "TF batch gradient calculation", "body": "I am trying to figure out the math model (or data flow) to compute gradient for batch gradient descent algorithms. All the materials I have been reading seems to say that the final gradient of a batch is the average of all gradients evaluated for all training samples of a batch, but when I check the TF loss function of a batch, I found the output of a TF loss function is a scalar value rather than (BATCH_SIZE,) -- a scalar vector of length equals to BATCH_SIZE. So I am wondering how TF calculates the gradient of a batch, does it just use this scalar loss value to calculate the gradient only ONCE for the entire batch on the trainable variables, or this loss value is used by BATCH_SIZE times to calculate the gradients for each training pairs of a batch, and then it does an average. \r\n\r\nAny reference is welcome. Thanks.", "comments": ["@llodds ,\r\n\r\nPlease refer these link for the information on gradient.[link1](https://www.oreilly.com/library/view/getting-started-with/9781786468574/ch02s03.html),[link2](https://www.tensorflow.org/quantum/tutorials/gradients),[link3](https://www.tensorflow.org/guide/advanced_autodiff).\r\n\r\nPlease file an issue in [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow), as it is neither a bug nor a feature request.\r\n\r\nThanks!\r\n\r\n", "Thanks!"]}, {"number": 49864, "title": "polymorphism dtype tensor for custom op Attr?", "body": "Can I ask how we can add polymorphism dtype tensor for custom op Attr? For example, how can we allow the below `attr` dtype to be DT_FLOAT or DT_HALF?\r\n\r\nexample:\r\n```\r\nREGISTER_OP(\"DoStuff\")\r\n    .Attr(\"attr: tensor = { dtype: DT_FLOAT }\")\r\n    .Input(\"in: float\")\r\n    .Output(\"out: float\");\r\n```\r\n\r\n```\r\nREGISTER_OP(\"DoStuff\")\r\n    .Attr(\"attr: tensor = { dtype: DT_HALF }\")\r\n    .Input(\"in: float\")\r\n    .Output(\"out: float\");\r\n```", "comments": ["This question is better asked on [TensorFlow Forum](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49864\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49864\">No</a>\n"]}, {"number": 49863, "title": "Added description about Force Zero Output for Mask when return_sequences=True", "body": "Fixes #49738", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49863) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49862, "title": "Wrong libcudart.so loaded with TF 2.5.0", "body": "**System information**\r\n- OS Platform and Distribution : ubuntu 20.04\r\n- TensorFlow installed from : binary via pip3\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8.5\r\n- Installed using : pip\r\n- CUDA/cuDNN version: NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2  \r\n- GPU model and memory: GeForce GTX 1070\r\n\r\n\r\nAfter installing nvidia drivers, cuda and cudnn with matching versions from:\r\n- https://www.tensorflow.org/install/source#gpu\r\n- https://docs.nvidia.com/deploy/cuda-compatibility/index.html\r\n- https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html\r\n\r\nand installing tensorflow with pip3 I get this message:\r\n```\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15)\r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-05-28 16:34:56.376524**: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory**\r\n2021-05-28 16:34:56.376553: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> tf.__version__\r\n'2.5.0'\r\n```\r\nWhy is tensorflow 2.5.0 loading cuda 11.0 but the tensorflow compatibility page says that it must work with cuda 11.2 ?\r\n\r\n\r\nnvcc output:\r\n```\r\n\u00bb /usr/local/cuda/bin/nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n```\r\nnvidia-smi output:\r\nFri May 28 16:38:20 2021       \r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   32C    P8     9W / 200W |    138MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n", "comments": ["`libcudart.so.11.0` should be the correct version of the cuda runtime library that is installed along with rest of the cuda 11.2 package. If TF is unable to find that binary, then I would double check that your runtime linker bindings have been setup correctly with `ldconfig -p | grep libcudart`. You should get an output like:\r\n\r\n```bash\r\n\tlibcudart.so.11.0 (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so.11.0\r\n\tlibcudart.so (libc6,x86-64) => /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\r\n```\r\n\r\nIf not, I'd verify the cuda 11.2 runtime package was properly installed, and that `libcudart.so.11.0` exists somewhere in the install path, and finally that the ldconfig cache is properly setup in `/etc/ld.so.conf.d/` to point to that path.\r\n\r\nBtw, it's a bit confusing but the `CUDA Version` reported by `nvidia-smi` is the **cuda driver** version that probably came with your GPU drivers. The **cuda runtime** libraries that are installed when you `apt-get install cuda-11-2` for example is a completely different thing. IE, it's possible to have cuda driver version 11.2 and have multiple cuda 10.1/10.2/11.0/11.3 runtimes installed.\r\n \r\n", "Nothing was listed with `ldconfig -p | grep libcudart`.\r\nSo I checked and found the lib in `/usr/local/cuda-11.2/targets/x86_64-linux/lib`\r\nI added this path to `LD_LIBRARY_PATH`\r\nThen it was loading but reporting a problem: `MemoryError in TensorFlow; and \u201csuccessful NUMA node read from SysFS had negative value (-1)\u201d `\r\nSo I `echo 0 | sudo tee -a /sys/bus/pci/devices/MyGPU/numa_node` and now eveything seem to work properly.\r\n(Might be useful for someone)\r\n\r\nThank you for your help!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49862\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49862\">No</a>\n"]}, {"number": 49861, "title": "Update image_dataset.py", "body": "Updated return values for \"tf.keras.preprocessing.image_dataset_from_directory\"\r\n\r\nFixes #49829", "comments": ["For these doc changes we should also have a doctest example to accompany them.\r\n\r\nSince this seems `tf.data`, adding Jiri to review.", "This is Keras. Adding @fchollet for triage.", "Thanks for the PR. We no longer develop Keras via the TensorFlow GitHub, so please reopen the PR at https://github.com/keras-team/keras"]}, {"number": 49860, "title": "[Feature] Adding layer_range for plot_model and model_to_dot", "body": "Fixes #48641 \r\n\r\nPlease take a look at [gist](https://colab.research.google.com/drive/1ynkoCxW9RPh8s0HgUAo_N0pNDrUXOqKk?usp=sharing) to see that the code changes in this PR actually works. (To see sub-graph plot on passing `layer_range`)\r\n\r\ncc @mihaimaruseac @bhack ", "comments": ["@ashutosh1919 \r\nIt would be nice also to have the **layer id** based subplot of the model along with this `layer_range` (that is **layer index** based). As each layer has its own unique name (**layer id**), so I think, it would be convenient.", "@innat , I am not sure how will it be done because `layer_id` doesn't have any specific pattern to start from and how should we track it? If you see, below are the first few layers of `EfficientNetB0`, but the first layer doesn't have `layer_id` 0. That is why I implemented it based on index. I am still unclear on what you mentioned in previous comment. Can you please explain little more briefly?\r\n\r\n`EfficientNetB0` some initial layers (printed `layer_id` and `class_name`)\r\n```\r\nlayer_id 139674164915472 layer_class InputLayer\r\nlayer_id 139674140900368 layer_class Rescaling\r\nlayer_id 139674140505296 layer_class Normalization\r\nlayer_id 139674140421200 layer_class ZeroPadding2D\r\nlayer_id 139674139973840 layer_class Conv2D\r\nlayer_id 139674090404368 layer_class BatchNormalization\r\nlayer_id 139674139974416 layer_class Activation\r\nlayer_id 139674089795792 layer_class DepthwiseConv2D\r\nlayer_id 139674140506448 layer_class BatchNormalization\r\nlayer_id 139674089798608 layer_class Activation\r\nlayer_id 139674089869008 layer_class GlobalAveragePooling2D\r\nlayer_id 139674089923920 layer_class Reshape\r\nlayer_id 139674089952912 layer_class Conv2D\r\nlayer_id 139674089454288 layer_class Conv2D\r\nlayer_id 139674089506640 layer_class Multiply\r\nlayer_id 139674089528784 layer_class Conv2D\r\nlayer_id 139674089535632 layer_class BatchNormalization\r\nlayer_id 139674089569040 layer_class Conv2D\r\nlayer_id 139674089594000 layer_class BatchNormalization\r\nlayer_id 139674089614480 layer_class Activation\r\nlayer_id 139674089629392 layer_class ZeroPadding2D\r\nlayer_id 139674089175824 layer_class DepthwiseConv2D\r\nlayer_id 139674089670864 layer_class BatchNormalization\r\nlayer_id 139674089203856 layer_class Activation\r\nlayer_id 139674089191312 layer_class GlobalAveragePooling2D\r\n```", "@ashutosh1919 \r\nWhat I meant is to enable also the `layer_name` same as their index value to plot the sub-graph of the model. For example\r\n\r\n```\r\n# currently \r\nplot_model(model, layer_range=[9, 15])\r\n# wish to have \r\nplot_model(model, layer_range=['conv_layer_1', 'conv_laye_2'])\r\n```", "@innat, IMO it wouldn't be possible since the pyDot.Dot generated by model_to_dot is only successful when the subgraph to generate is complete (Means the layers to plot should be continuous as well as no intermediate layer should depend on layer which is not in range. For ex, for efficientnet given in gist above, you can pass [9, 29] and it will work but if you pass [10,29] it will not work because layer 11 depends upon layer 9 which is not in range)\n\nSo, as if you are suggesting if we only want to plot any particular type of layer (ex Conv2D), then this type of layer can be present anywhere discontinuous and model_to_plot will fail.\n\nLet me know your opinion.", "@ashutosh1919  cc: @gbaned \r\n\r\nWhat you're saying is true, but I think you miss something, here we're not trying to plot discontinuous range. When you pass `layer_range = [9, 29]`, these 9th and 29th layers have their own name, which is unique throughout the layers. We simply want to pass those string arguments.  If you see from [here](https://keras.io/api/models/model/), we can get the layer by its name or by its index.\r\n\r\n```\r\nModel.get_layer(name=None, index=None)\r\nRetrieves a layer based on either its name (unique) or index.\r\n```\r\n\r\n---\r\n\r\nAnyway, if you don't mind, I've done this additional option. Please find the gist [here](https://colab.research.google.com/drive/1OcN-qsJFpeVHj-WQyLuzhp0YKQ91ZL65?usp=sharing).\r\n\r\n```python\r\n# full-graph plot \r\nplot_model(model) # VALID \r\n\r\n# Sub-graph plot\r\n# Test: using layer names \r\n\r\n# scenario 1\r\nplot_model(model, layer_range=['block1a_project_conv', \r\n                               'block1a_activation'])  \r\n\r\n# scenario 2\r\nplot_model(model, layer_range=['block1a_project_conv', # 16th indexed layer\r\n                               'block1a_activation',   # 9th indexed layer \r\n                               'block1a_bn'])          # 8th indexed layer \r\n```\r\n\r\nFor the above case, I've added a function named `get_layer_index_by_layer_name`, which will compute the **index value** of those **layer's names**. For a scenario such as passing multiple-layer range (e.g, 16th, 9th, 8th indices), the program will simply compute the lower bound and upper bound. \r\n\r\n\r\nAnd additionally, I've also made some changes to your (@ashutosh1919) `layer_range` shape checkpoint (please check to verify) such as the program can now take multiple indexes (greater than 2) and compute upper and lower bound instead of raising the value error.\r\n\r\n```python\r\nplot_model(model, layer_range=[9, 15])    # VALID \r\nplot_model(model, layer_range=[8, 6, 16]) # VALID\r\nplot_model(model, layer_range=[9, \"15\"])  # VALID \r\n``` \r\n\r\nLastly, what is **not VALID** until now is to pass both e.g **index** and **layer id / name**, which makes no sense of course, and passing such an argument will raise an error.\r\n\r\n```\r\nplot_model(model, layer_range=[9, 'block1a_activation']) # INVALID\r\n```", "@innat, ohh i see now. I was completely understanding this as something else. Yes the solution you described will work and thanks for the gist. Will take a look and make the needed changes to the PR. ", "@innat , I have modified the code as per your comment. Now, we can pass `layer_range` as either layer indexes or layer names. \r\n\r\nFeel free to review.", "@ashutosh1919 It seems ok to me, however one small issue and I think it should be considered. For example, if I check \r\n\r\n```python\r\nmodel.get_layer(index=9).name, model.get_layer(index=16).name\r\n('block1a_activation', 'block1a_project_bn')\r\n\r\nmodel.get_layer(index=0).name, model.get_layer(index=1).name\r\n('input_1', 'rescaling')\r\n\r\nmodel.get_layer(index=235).name, model.get_layer(index=239).name\r\n('top_bn', 'predictions')\r\n```\r\n\r\nAnd now if I want to plot sub-graph by their index value, I probably should do as follows \r\n\r\n```python\r\nplot_model(model, layer_range=[9, 16]) \r\nplot_model(model, layer_range=[0, 1]) \r\nplot_model(model, layer_range=[235, 239]) \r\n```\r\n\r\nAnd it will plot the sub-graph but the issue here is index number **16th**, **1st**, **239th** become excluded and will be plotted until index number **15th**, **0**, and **238th**. The same goes with the layer name approach also. We can verify this by looking at the bottom of the sub-plot, we won't get the last indexed layer output. \r\n\r\nAnd that's why we added `+1` to the max layer index number (mention below), just a quick remedy from this but didn't check for test cases though.\r\n\r\n```python\r\n# for layer name range approach\r\n# changing only by adding + 1 to max index\r\n \r\ndef get_layer_index_by_layer_name(model, layer_names):\r\n    ....\r\n    return [min(index), max(index) + 1] if len(index) > 1 else index\r\n```\r\n```python\r\n# for layer index range approach \r\n# changing only by adding + 1 to max index \r\n\r\n  if all(map(lambda x: str(x).isdigit(), layer_range)):\r\n    layer_range = list(map(int, layer_range))\r\n\r\n    if len(layer_range) >= 2:\r\n      layer_range = [min(layer_range), max(layer_range) + 1]  # adding + 1\r\n    elif len(layer_range) != [2]:\r\n      raise ValueError(\"layer_range must be of shape (2,)\")\r\n    if layer_range[0] < 0 or layer_range[1] > len(model.layers) + 1: # adding + 1 \r\n      raise ValueError(\"Both values in layer_range should be in\",\r\n                       \"range (%d, %d)\"%(0, len(model.layers)))\r\n```\r\n\r\nNow, if we call this function with layer range by index or name, unlike before, all layers will be plotted. ", "@innat, I am aware about this and I have intentionally kept it that way because range() function or any other functions in python related to range works thay way. Shouldn't we create this considering that in mind because most of the users by default will assume this because of common programming paradigm ", "@ashutosh1919 \r\nI see. Actually, when I use this function, I would simply pass the **range of layer index** values to get a sub-plot. If I need to get a subplot from layer index in the range of 235 to 239, currently I had to pass `layer_range = [235, 240])`, otherwise the last layer would be missed. \r\n\r\n```python\r\nmodel.get_layer(index=235).name, model.get_layer(index=239).name\r\n('top_bn', 'predictions')\r\n\r\nplot_model(model, layer_range=[235, 240]) \r\n```\r\n\r\nBut I understand what you mean and I agree with that. However, it's also true that we will get errors if we attempt to do `model.get_layer(index = 240)` here, and picking the layers (by `index` or `name`) for feature extraction is very common and most of the practitioners are aware of this. \r\n\r\nThis probably becomes more visible if we pass the layer name to get the plot. For example: if we do as follows, we would miss the `predictions` layer and end up in the `top_dropout` layer of index number 238. \r\n\r\n```python\r\nplot_model(model, layer_range=['top_bn',       # 235th indexed layer \r\n                               'predictions']) # 239th indexed layer \r\n```\r\n\r\n", "@innat, definitely agree with you. But I observed in your gist that when we pass layer_range as [9, 240], it makes it [9, 241]. Now, as per you we need to pass [9, 239] but most of the users will check len(model.layers) which outputs 240 and they will put 240 as it is without second thought.\r\n\r\nBut i think both implementations have their own pros and cons and i think we should wait for opinions from other contributors as well.\r\n\r\ncc: \r\n@gbaned @mihaimaruseac @bhack ", "@qlzh727 , @innat - Please review the latest changes as per comments. I have also commented on some of the reviews above so please look at them as well.", "@qlzh727 , Below things are done as part of last commit\r\n(1) Implemented regex logic to find indexes to layers (Also added tests for these)\r\n(2) Converted all test cases as parameterized\r\n(3) code and docstring refactored as per your comments.", "@qlzh727 , can you please approve again?", "ok. This change is merged to keras-team/keras as https://github.com/keras-team/keras/commit/d9e7b83c70e2117f40465b79e2e8201cea46d36a. (since we split the keras code to keras-team/keras).\r\n\r\nClosing this PR now."]}, {"number": 49859, "title": "Update doc example for tf.Variable.scatter_nd_update()", "body": "", "comments": ["@maxhgerlach can you please check latest comments.", "Hi @allenlavoie and @rthadur, thanks for having a look at this.\r\n\r\nI've updated the examples for the other two functions as well.\r\n\r\nSince I've been doing these edits via the Github UI, I am not going to introduce any new tests myself."]}, {"number": 49858, "title": "tflite model produces different results if using the CPU on a Ubuntu PC or a NPU+NNAPI on an embedded system", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04 / Linux Yocto\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: 2.4.1\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI trained a classification network and I performed quantization-aware training using Tensorflow Model Optimization. Then I converted the trained and quantized model to tflite format.\r\nThe model is supposed to be used in an embedded system running Yocto. The system is provided with a NPU and the inference is performed with the [tflite_runtime](https://www.tensorflow.org/lite/guide/python) package and the NNAPI.\r\nThe problem is that the same tflite model produces different results if tested on my PC with TF2.4.1 and on the Yocto board. The result are often very similar but there are some cases where they are completely different, resulting in an accuracy drop on the board. I don't know why this happens. Is there a way to prevent it during training? \r\n\r\n\r\n", "comments": ["@FSet89 ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled. \r\nAlso for analysing the issue can you please provide the stand alone code or share a colab gist with the issue reported.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49857, "title": "Fix error when compile asm files", "body": "This PR fix a bug when compile assemble files using function `microlite_test`.\r\n\r\nThis function is called by https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/Makefile.inc and also other examples. So, if we add bsp files to `HELLO_WORLD_SRCS` and it contains assemble files, it will cause error here. The Error message is like below.\r\n```\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu_micro/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/start.S', needed by 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu_micro/bin/hello_world'.  Stop.\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49857) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 49856, "title": "Enable method delegation test in eager distributed `AutoCastVariable`", "body": "This PR enables the method delegation tests of `AutoCastVariable` when run in eager mode within a distribution strategy as the issue appears to be fixed on master.\r\n\r\n/cc @reedwm ", "comments": []}, {"number": 49855, "title": "Update GPU support page to reflect correct libraries required for TF 2.5.0", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/gpu\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation has not been updated to state the correct versions of cuda, cudnn and nvinfer that the current stable Tensorflow version (2.5.0) requires.\r\n\r\nFor example in the Ubuntu 18.04 configuration, instead of:\r\n\r\n```bash\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-11-0 \\\r\n    libcudnn8=8.0.4.30-1+cuda11.0  \\\r\n    libcudnn8-dev=8.0.4.30-1+cuda11.0\r\n```\r\n\r\nIt should be something like:\r\n\r\n```bash\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-11-2 \\\r\n    libcudnn8=8.1.0.77-1-1+cuda11.2  \\\r\n    libcudnn8-dev=8.1.0.77-1+cuda11.2\r\n```\r\n\r\nIt's also particularly unclear which version of the nvinfer libraries are required to enable TF-TRT optimizations since it's also not stated in the [Build from source](https://www.tensorflow.org/install/source) page and it's also not obvious how to check that TRT is enabled.\r\n ", "comments": ["Thanks for your issue. This is tracked with https://github.com/tensorflow/tensorflow/issues/49175"]}, {"number": 49852, "title": "Why is profiler dir deleted for non-chief nodes in MWMS in keras?", "body": "Hi,\r\nBased on the keras tensorboard callback: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2472\r\nThe profiler dir of non-chief node is set to a temp dir when MWMS is being used, which will be deleted later on:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2227\r\nhttps://github.com/tensorflow/tensorflow/blob/da15b34d11432b789527e4a3c3ad4eac5193a5f0/tensorflow/python/keras/callbacks.py#L2256\r\n\r\nI understand checkpoints for non-chief nodes should be cleaned up, but why profiling result? \r\nHappy to contribute a PR if this is a bug. \r\n\r\nCC @jbaiocchi  @omalleyt12\r\n\r\n", "comments": ["@burgerkingeater,\r\nNeither [**`_log_write_dir`**](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2227) nor [**`log_dir`**](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2257) is a temporary directory but they are the actual directories in which **`Tensorboard Events`** are stored. \r\n\r\nPlease let me know if I have understood your question correctly. \r\n\r\nThanks!", "@rmothukuru thanks! But if you trace the value of _log_write_dir, it is a temp dir for MWMS's non-chief node: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2225\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e608ef6eede05b6d7864201b181968bdc962362f/tensorflow/python/keras/distribute/distributed_file_utils.py#L96\r\n\r\n", "@burgerkingeater thanks for the question. With MWMS the workers are expected to have the same metrics with all-reduces. Is there a reason why the logging on chief alone isn't sufficient?", "@rchao thanks for the reply. Trace view in the profiler is not necessarily to be the same across the nodes, right? E.g, two nodes are running with different hardware configuration", "@burgerkingeater I see. Would you mind giving a try to not remove the profiling on non-chiefs (and re-build TensorFlow), and see if it matches what you're expecting?", "CC myself @yisitu ", "@rchao yes, and I submitted a PR to fix this issue: https://github.com/tensorflow/tensorflow/pull/50068/, would you mind taking a look? ", "@rchao also I wonder why not just not writing summary if it's non-chief? Why is the overhead of creating the temp dir, writing summary to temp dir, and deleting the temp dir necessary?", "Thanks @burgerkingeater! With MWMS, any API calls can trigger all-reduces across the workers and if we only perform that on chief, there is a chance that things will just hang (because it keeps waiting to all-reduce with other workers). Therefore in general all workers (including chief) should perform the same actions, but if it involves file writing to a centralized place, we'd need temporary locations for non-chief for later deletion.", "Related PR has been merged , closing this issue. Thank you ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49852\">No</a>\n"]}, {"number": 49850, "title": "Tensorflow build failed with error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus' on s390x", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.1 on s390x platform\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: r2.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source):3.5.0\r\n- GCC/Compiler version (if compiling from source): gcc-toolset-9\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nCompilation of source code failed with the following error message\r\n\r\n```\r\nERROR: /tmp/source/tensorflow/tensorflow/core/kernels/BUILD:5530:18: C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1)\r\nIn file included from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/framework/op_kernel.h:24,\r\n                 from tensorflow/core/kernels/sparse_split_op.cc:19:\r\n./tensorflow/core/framework/numeric_types.h: In function 'tensorflow::bfloat16 FloatToBFloat16(float)':\r\n./tensorflow/core/framework/numeric_types.h:51:13: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *reinterpret_cast<tensorflow::bfloat16*>(\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n         reinterpret_cast<uint16_t*>(&float_val));\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/core/framework/numeric_types.h:51:13: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:35,\r\n                 from tensorflow/core/kernels/sparse_split_op.cc:19:\r\ntensorflow/core/kernels/sparse_split_op.cc: In member function 'void tensorflow::SparseSplitOp<T>::Compute(tensorflow::OpKernelContext*)':\r\ntensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'\r\n                      dense_shape.AddDimWithStatus(input_shape_flat(i)));\r\n                                  ^~~~~~~~~~~~~~~~\r\n./tensorflow/core/framework/op_requires.h:52:29: note: in definition of macro 'OP_REQUIRES_OK'\r\n     ::tensorflow::Status _s(__VA_ARGS__);                    \\\r\n                             ^~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tmp/source/tensorflow/tensorflow/python/tools/BUILD:82:10 C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1)\r\nINFO: Elapsed time: 5244.642s, Critical Path: 263.52s\r\nINFO: 5985 processes: 5985 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nyum -y install atlas-devel gcc-toolset-9 python3-scipy python36-devel python3-scipy python3-pip pkgconf-pkg-config zip unzip libaec-deve libtool wget git hdf5-dev openssl-devel \r\n\r\npip3 install cython wheel numpy scipy virtualenv future h5py portpicker wheel protobuf keras_preprocessing \r\nexport GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=True\r\npip3 install grpcio\r\n\r\n```\r\nBuild bazel 3.5.0\r\n```\r\nmkdir -p $SOURCE_ROOT \\\r\n    && cd $SOURCE_ROOT \\\r\n    && mkdir bazel \\\r\n    && cd bazel \\\r\n    && wget https://github.com/bazelbuild/bazel/releases/download/3.5.0/bazel-3.5.0-dist.zip \\\r\n    && unzip bazel-3.5.0-dist.zip\r\nRUN cd $SOURCE_ROOT/bazel \\\r\n    && chmod -R +w . \\\r\n# Add patch to resolve java oom issue\r\n    && sed -i \"147s/-classpath/-J-Xms16g -J-Xmx16g -classpath/\" scripts/bootstrap/compile.sh\r\n\r\ncd $SOURCE_ROOT/bazel \\\r\n    && env EXTRA_BAZEL_ARGS=\"--host_javabase=@local_jdk//:jdk\" bash ./compile.sh\r\n```\r\n\r\nBuild Tensorflow\r\n```\r\ncd $SOURCE_ROOT \\\r\n    && git clone https://github.com/tensorflow/tensorflow.git \\\r\n    && cd tensorflow \\\r\n    && git checkout r2.3 \\\r\n# Configure\r\n    && yes \"\" | ./configure\r\n# Build TensorFlow\r\n cd $SOURCE_ROOT/tensorflow \\\r\n    && bazel --host_jvm_args=\"-Xms16g\" --host_jvm_args=\"-Xmx16g\" build \\\r\n       --define=tensorflow_mkldnn_contraction_kernel=0 --copt=-Wno-maybe-uninitialized --copt=-mzvector --copt=-funroll-loops --copt=-march=z14 --color=yes --curses=yes \\\r\n       //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@chandrureddy ,\r\n\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the tested build [configurations](https://www.tensorflow.org/install/source#gpu). Can you please try installing as mentioned and check if you are facing the same error. \r\n\r\nThanks!\r\n\r\n", "@tilakrayal thank you for the update. Let me have a look into into and provide an update.", "@tilakrayal I have check out r2.3 to below commit level and build succeeded\r\n\r\n`git checkout -b r2.3 8de24d1c0eb93068d657be68d982e9dba526a6cc`\r\n\r\nIt looks to me some new changes broken ? I do see other issues reporting similar issue https://github.com/tensorflow/tensorflow/issues/49377\r\n\r\nSo do you really see a concern ?", "@chandrureddy Can you please try with latest release 2.5?", "@ymodak right now I am looking for 2.3.2 version", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@chandrureddy I will give it try with 2.5 and also do you have any intention to fix for branch 2.3.2 ? Thank you !", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49850\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49850\">No</a>\n"]}, {"number": 49849, "title": "[MLIR][DISC] pattern conversion from tf2mhlo: ConvertSplitOpDynamic, ConvertSliceOpDynamic", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\nThis is the third PR about tf2mhlo pattern conversion, which including ConvertSplitOpDynamic and ConvertSliceOpDynamic.\r\nThe rest pattern conversions we will add:\r\n- ConvertSqueezeOpxxx\r\n- ConvertStridedSliceOpxxx\r\n- ConvertTileOpxxx\r\n- ConvertTopKV2Opxxx\r\n- ConvertUnpackOpxxx\r\n- ConvertPrintOp\r\n- ConvertSigmoidGradOpxxx\r\n- ConvertSignOpxxx", "comments": ["Can you add test coverage for this?", "> Can you add test coverage for this?\r\n\r\nadded", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49849) for more info**.\n\n<!-- need_author_cla -->", "@azazhu  Can you please resolve conflicts? Thanks!", "> @azazhu Can you please resolve conflicts? Thanks!\r\n\r\nDone"]}, {"number": 49848, "title": "Fix an incorrect axis in the Dense layer documentation", "body": "The text should either read \"axis 0 of the kernel\" or \"the kernel operates along axis 3 of the input\" to be consistent.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49848) for more info**.\n\n<!-- need_sender_cla -->", " @googlebot I signed it!"]}, {"number": 49847, "title": "tf.image.resize behavior", "body": "not working code\r\n`\r\n@tf.function\r\ndef read_dicom(sample_dicom_path):\r\n\r\n\timage_file_bytes=tf.io.read_file(sample_dicom_path)\r\n\timage=tfio.image.decode_dicom_image(image_file_bytes, dtype=tf.float32)\r\n\t\r\n\timage = tf.squeeze(image)\r\n\timage = tf.stack([image, image, image], axis=-1)\r\n\r\n\tsize=tf.cast(tf.constant([224,224]), tf.int32)\r\n\timage = tf.image.resize(image, size)\r\n\r\n\treturn image`\r\n\r\nimage=read_dicom('sample.dcm')\r\n\r\nRunning this the code yields the following error message:ValueError: \r\n![Screen Shot 2021-05-27 at 5 50 45 PM](https://user-images.githubusercontent.com/58089750/119913957-10f9f580-bf14-11eb-994d-570386f63fc3.png)\r\n\r\n\r\n\r\n", "comments": ["@Saduf2019 \r\n\r\nI was able to replicate the issue reported here,Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/f32817ccef2db11884f4b65881a1725b/untitled74.ipynb#scrollTo=jDhOr1KwjVNG).  Thanks", "@slala2121 \r\nPlease try with out @tf function, we tried the same and see error \"\"kittycat; No such file or directory [Op:ReadFile]\" which clearly shows there is an issue with the path you have provided, please fix that.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49846, "title": "Fix tf.raw_ops.SparseCross  failing CHECK.", "body": null, "comments": []}, {"number": 49845, "title": "Fix duration computation in MicroProfiler.", "body": "Prior to this change, if the BeginEvent occurred at a tick that was larger than EndEvent, we would incorrectly compute the duration for that event.\r\n\r\nThe only requirement that we have is that no more than 2^32-1 ticks should have passed between a call to BeginEvent and EndEvent\r\n\r\nThe discussion on https://github.com/tensorflow/tensorflow/pull/49144 alerted us to this bug.\r\n\r\nNote that we do not have any unit tests for MicroProfiler at this time.\r\n\r\nManually confirmed that the profiling info for this command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 OPTIMIZED_KERNEL_DIR=xtensa XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n```\r\n\r\nis identical with and without the current change.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@njeffrie confirmed that this PR is unnecessary. If GetCurrentTimeTicks overflows as a two's complement then end-start will always give a positive number within the int32_t range."]}, {"number": 49844, "title": "Fix `tf.raw_ops.RaggedTensorToVariant` invalid resize.", "body": "PiperOrigin-RevId: 368299574\nChange-Id: I751c186325aa0bab397928845e790e60c2d90918", "comments": []}, {"number": 49843, "title": "Fix `tf.raw_ops.RaggedTensorToVariant` invalid resize.", "body": "PiperOrigin-RevId: 368299574\nChange-Id: I751c186325aa0bab397928845e790e60c2d90918", "comments": []}, {"number": 49842, "title": "Fix `tf.raw_ops.RaggedTensorToVariant` invalid resize.", "body": "PiperOrigin-RevId: 368299574\nChange-Id: I751c186325aa0bab397928845e790e60c2d90918", "comments": []}, {"number": 49841, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows version 20h2\r\n- Python version:  3.7.7\r\n- Installed using conda\r\n\r\nTrying to run python in terminal, already done:\r\n\r\n`(base) C:\\Users\\Yinqi\\trRosetta2\\trRosetta>conda create -n tf tensorflow`\r\n\r\nBut ran into this error:\r\n\r\n`ImportError: DLL load failed: The specified module could not be found. Failed to load the native TensorFlow runtime.`\r\n\r\nFull traceback:\r\n\r\n```\r\nFile \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n        from tensorflow.python._pywrap_tensorflow_internal import *\r\n    ImportError: DLL load failed: The specified module could not be found.\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"predict.py\", line 6, in <module>\r\n        import tensorflow as tf\r\n      File \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n        from tensorflow.python.tools import module_util as _module_util\r\n      File \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n        from tensorflow.python.eager import context\r\n      File \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n        from tensorflow.python import pywrap_tfe\r\n      File \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n        from tensorflow.python import pywrap_tensorflow\r\n      File \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n        raise ImportError(msg)\r\n    ImportError: Traceback (most recent call last):\r\n      File \"C:\\Users\\Yinqi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n        from tensorflow.python._pywrap_tensorflow_internal import *\r\n    ImportError: DLL load failed: The specified module could not be found.\r\n```", "comments": ["@yinqihuang ,\r\n\r\n We can see you are using conda env to install TF, You may want to raise this issue on [anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) for better support.\r\n Also please take a look at this issues with similar error log.[Link1](https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-573211235),[link2](https://github.com/tensorflow/tensorflow/issues/42394),[link3](https://github.com/tensorflow/tensorflow/issues/36138). It helps.\r\n\r\nThanks!\r\n", "Thanks, it seems that my cpu doesn't support AVX.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49841\">No</a>\n"]}, {"number": 49840, "title": "[MLIR][TOSA] Legalize TFLite tanh/sigmoid/hardswish with int8[256] TOSA table.", "body": "- rewrite 8-bit TOSA const table generation utility function with int8[256]\r\n- remove int16 hardswish since TFLite doesn't support this.\r\n- rewrite int16 tanh/sigmoid. Generate int16[513] using the same gen_lut() as 16-bit tfl.softmax() use.\r\n\r\nSigned-off-by: Kevin Cheng <kevin.cheng@arm.com>\r\nChange-Id: I01380ec5b9f2bfc4678302c983a529eaef3d02a8", "comments": ["Sorry forgot to tagging @stellaraccident and @rsuderman for review this time."]}, {"number": 49839, "title": "Update the TFLM readme to reflect the new repo location.", "body": "Bug: http://b/182914089\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49838, "title": "[CherryPick]Fix 2 issues with .", "body": "PiperOrigin-RevId: 369242785\r\nChange-Id: Ie94067b2d41f58699af99ebb5af335ad9defd931", "comments": []}, {"number": 49837, "title": "[CherryPick]Fix 2 issues with .", "body": "PiperOrigin-RevId: 369242785\r\nChange-Id: Ie94067b2d41f58699af99ebb5af335ad9defd931", "comments": []}, {"number": 49836, "title": "Faild Build 2.5.0 Windows", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: Conda x64\r\n- Bazel version (if compiling from source): 3.7.2\r\n- CUDA/cuDNN version: 11.3/8.2.0\r\n- GPU model and memory: 3060\r\n\r\nProblems arise only when building 2.5.0. Build version 2.4.1 is not problems\r\n\r\nError\r\n```\r\nERROR: F:/tensorflow/tensorflow/python/util/BUILD:609:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/X/_bazel_X/2qttxlm7/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\X\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=8.6\r\n    SET TMP=C:\\Users\\X\\AppData\\Local\\Temp\r\n  C:/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc\r\n```", "comments": ["As you have cuda 11.3 check if it is related to https://github.com/tensorflow/tensorflow/pull/48803", "Also since you are using conda env to install TF, You may want to raise this issue on [anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) for better support.", "I`m used #48803 and new error \r\n```\r\nERROR: F:/tensorflow/tensorflow/python/util/BUILD:329:27: C++ compilation of rule '//tensorflow/python/util:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command\r\n```", "We don't support Conda build directly here. You need to ask third party support in the anaconda repo.\r\n\r\nWe could support you here if you use the official build guide:\r\nhttps://www.tensorflow.org/install/source_windows", "I've tried with pure pytho too. The error is the same\r\n```\r\nERROR: F:/tensorflow/tensorflow/python/util/BUILD:329:27: C++ compilation of rule '//tensorflow/python/util:_tf_stack.so' failed (Exit 2): python.exe failed: error executing command\r\n\r\n```", "!up", "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n    TensorFlow installed from (source or binary): source\r\n    TensorFlow version: 2.5.0\r\n    Python version: 3.7.9\r\n    Installed using virtualenv? pip? conda?: pip\r\n    Bazel version (if compiling from source): 3.7.2\r\n    CUDA/cuDNN version: None\r\n    GPU model and memory: None\r\n\r\nSame error for me. Except I'm building CPU version only, and my CPU doesn't support avx instructions, but since I'm building it, it shouldn't make any difference.\r\n[https://pastebin.com/x8UbCzjc](url)\r\n\r\nDidn't test 2.4.0 version, but successfully build tf 2.5.0 on RPi 4, Ubuntu server 20.04.2, using python 3.8 and pip packages.", "@sonfire86 ,\r\nCan you please try the latest tf v2.6 and let us know if the issue still persists.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49836\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49836\">No</a>\n"]}, {"number": 49834, "title": "Use image orientation tag in exif data", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15, 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently tensorflow will use images without rotating them per the orientation flag in the exif data. This can result in bounding boxes in annotation files being placed in the wrong locations when Tensorflow loads the image. This can be _very_ difficult to find and diagnose, even if using a utility like Tensorboard to view eval images and training statistics as you need to luck out and have it show an affected image where the groundtruth boxes are in the wrong place. Further compounding this issue is that most annotation and image viewing tools _will_ use the orientation flag to rotate images accordingly, making finding and correcting affected images an extremely painful and time consuming activity.\r\n\r\nCurrently my workaround is to strip all exif data from each image as a quick-and-dirty way of ensuring that I'm annotating the image exactly as tensorflow will load it. I lucked out that I found out about this while training on a fairly small custom dataset because correcting this _after_ annotating images involves finding any images with an orientation tag and resaving a rotated image without the exif data before rebuilding the tfrecords file with the new images. While building the tools necessary to do this isn't that difficult it could be entirely unneeded if tensorflow simply rotated the images correctly when loading, the way labelImg, labelme, and all other annotation tools do.\r\n \r\n**Will this change the current api? How?**\r\n\r\nUsage of the api will be unaffected, images will simply be loaded with the correct orientation without the need for extra input from the user\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who uses images that contain an orientation flag other than 1 in the exif data\r\n\r\n**Any Other info.**\r\n\r\n-\r\n", "comments": ["@dhalnon,\r\nYou have stated:\r\n\r\n> Currently tensorflow will use images without rotating them per the orientation flag in the exif data. This can result in bounding boxes in annotation files being placed in the wrong locations when Tensorflow loads the image. \r\n\r\nCan you please share a reproducible code that supports your statement so that the issue can be easily understood? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49832, "title": "MLIR-based TFLite converter - 16bit fake quantization", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5\r\n- TensorFlow installation (pip package or built from source): 2.4.0\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): pip package\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\nhttps://colab.research.google.com/drive/1Adz4A66PjnsvPigUi3cRL21C2aEe_dMI?usp=sharing\r\n\r\nError during tflite conversion:\r\nFailed to convert element type '!quant.uniform<u16:f32, 6.1036087586785687E-5>': 'isSigned' can only be set for 8-bits integer type\r\n\r\n\r\nHi,\r\nThe model conversion works fine when using TOCO converter (which is deprecated) but fails when using MLIR-based TFLite converter.\r\nIt seems that fake-quant can be converted only when it quantizes using 8 bits...\r\nAny ideas?\r\n\r\nThanks!\r\n\r\n\r\n", "comments": ["Hi, any news? ", "Hi @reuvenperetz ! This issue is not replicating in TF [2.8](https://colab.sandbox.google.com/gist/mohantym/87e87f40265c25dc06c498b07149663c/copy-of-16bit-fake-quant-mlir-based-tflite-conversion.ipynb#scrollTo=kOyTcnb_0L3w) version . Can we move this to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49832\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49832\">No</a>\n", "Thanks!\r\nI see I still have to use the MLIR converter flag of optimizations:\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nOtherwise, I get the same error.... is is possible to convert 16bits fake-quant layer without using the converter optimizations?\r\n\r\nThanks again.\r\n", "converter optimization isn't faulty by comparison. its the lack of improper devices and components that need adjustments and updating to better produce fast and reliable results without lagging or booting data. ", "Currently in the process of being in the conversion of getting help with better components and malware protection software for the issues currently being produced towards my keyboard system."]}]