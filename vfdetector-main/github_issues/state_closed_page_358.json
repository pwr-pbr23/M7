[{"number": 43325, "title": "Fix rename of gpu pips for single pip package on Windows GPU", "body": "", "comments": []}, {"number": 43323, "title": "AttributeError: '_TfDeviceCaptureOp' object has no attribute 'node_def'", "body": "Hi, i'm sorry to bother you, but i was meet a error when I use keras. my envs as follow: tensorflow-gpu==1.12, keras==2.2.4 , tensorpack==0.9.0.1\r\nthe error is:\r\n![1](https://user-images.githubusercontent.com/56585970/93546332-26864b00-f995-11ea-9391-93b2373ec165.png)\r\nand i use functions of keras by follow way:\r\n![2](https://user-images.githubusercontent.com/56585970/93546361-30a84980-f995-11ea-908a-aa1242ceb4eb.png)\r\nof course, it has the same error when i use ConvLSTM2D have get while i use the function \"BatchNormalization()\", which belong to keras. but the function 'Reshape()' and 'concatenate()' are run well . \r\n\r\nBy the way, I used the keras function directly instead of modeling with keras.\r\n\r\nCould you help me slove this error? I'd appreciate it if you could help me with this error.\r\n", "comments": ["@haiqinzhong,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue?\r\n\r\nAlso, in order to expedite the trouble-shooting process, could you please provide the complete code/script you are running to reproduce the issue reported here. Thanks!", "Thanks for your reply. But I'm sorry, in order to run another function well, I have delete these functions and change to others function which I use. and the main purpose I use the function ConvLSTM2D( ), which I can't find in TensorFlow1.x , even some functions with similar functions.\r\nAnd for your first proposed, maybe its OK, but the codes I use is using TensorFlow1.12, and as a rookie, I'm not able to change all code into TensorFlow2.3, and it use tensorpack to get the model and train which is base your TensorFlow.\r\nAll right, if TensorFlow 1.x is not actively supported, I'll not try the function. \r\nLast, could you tell me if TensorFlow1.x have the similar function like ConvLSTM2D( )?", "Also, I can write it again if nescessly.", "@haiqinzhong,\r\nSorry for the delayed response. \r\n\r\nConvLSTM2D does exist for TensorFlow 1.x. Please take a look at the [documentation](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/ConvLSTM2D) for more information regarding this. Thanks!", "Thank you, I'll try again. And I'll tell you what happen soon", "Hi, @amahendrakar ,I have tried it by use tf.keras.layers.ConvLSTM2D, but it told a new error about \r\nAttributeError: '_TfDeviceCaptureOp' object has no attribute 'type'\r\nthe Traceback as follow:\r\n![1](https://user-images.githubusercontent.com/56585970/94634333-596bff80-0302-11eb-8403-53d77f8ebc25.png)\r\n![2](https://user-images.githubusercontent.com/56585970/94634339-5e30b380-0302-11eb-8e9e-34b3c69cb8b4.png)\r\n\r\nand my code is \r\n![3](https://user-images.githubusercontent.com/56585970/94634413-8fa97f00-0302-11eb-8ae4-acadc0ac3eb8.png)\r\nI only use it once\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@haiqinzhong,\r\nCould you please provide the Python script/notebook you are running along with the dataset you are using, so that we can reproduce the issue on our end? Thanks!", "Yeah, but how can i give you? Can you give me a email so that i can give it to you solely. @amahendrakar ", "@haiqinzhong,\r\nYou can send the code to amahendrakar@google.com. However please note that to resolve the issue, I will have to share the code with other members of the TensorFlow repo.\r\n\r\nAlternatively you can also send a minimal working example with a dummy dataset, so that users from the TensorFlow community can also take a look at the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43322, "title": "vectorized_map fails if function contains an abs of a complex128", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`vectorized_map` throws an `AssertionError` if the body contains a `tf.abs` of a `complex128` tensor.\r\n\r\n**Describe the expected behavior**\r\nNo exception should be thrown.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.vectorized_map(tf.abs, tf.cast([0, 1], dtype=tf.complex128))\r\n```\r\n(also see https://colab.research.google.com/drive/1ExIw0N92bHpgWxwRONmeP24BWCzWG3QL?usp=sharing)\r\n\r\n**Other info / logs**\r\nI think the issue is that the `ComplexAbs` gets converted to a straight call to `math_ops.complex_abs` (https://github.com/tensorflow/tensorflow/blame/master/tensorflow/python/ops/parallel_for/pfor.py#L2731), which by default returns a `float32` (instead of deciding the dtype based on the dtype of the argument). I believe it could be fixed just by changing that call to `math_ops.abs`.", "comments": ["@charmasaur \r\nI ran the code shared on tf nightly and do not face any issues, please refer to [gist here](https://colab.research.google.com/gist/Saduf2019/7cfac81f581cfde17c1feaf3c0629a9c/untitled416.ipynb) and let us know.", "Sorry, I didn't think to check nightly.\r\n\r\nIn any case, while the original code I gave does indeed work on nightly, the issue is still there: changing `[0]` to `[0, 1]` causes the same failure as before.\r\n\r\nI think https://github.com/tensorflow/tensorflow/commit/c2e594440e1d9839546b93a93d8646b06891d7de#diff-a99b0652aa0a4be5b8fd65a0c02bea64 fixed the `[0]` case as a side-effect, since after that change the parameter gets treated as a loop invariant, so isn't converted by pfor.", "@charmasaur \r\nI ran the code on tf 2.5, please verify the [gist here](https://colab.research.google.com/gist/Saduf2019/26078dc9d4b45cd2e471e971b7bdb858/untitled597.ipynb) and confirm if we may move this to closed status.", "Hi @Saduf2019 , unfortunately it's still not fixed in the case where the tensor has more than one element (see my comment https://github.com/tensorflow/tensorflow/issues/43322#issuecomment-695864353). Changing `[0]` to `[0, 1]` in that gist still causes a failure.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43322\">No</a>\n"]}, {"number": 43321, "title": "Disable a flaky test on mac py38", "body": "", "comments": []}, {"number": 43320, "title": "Revert \"Revert \"Add symmetric int16 support to tflu softmax reference kernel\"\"", "body": "Reverts tensorflow/tensorflow#43127\r\n\r\n#43180 removes std:vector from the reference implementation of int16 softmax so we should be all set to use it from TFLM. The continuous integration coverage has been increased with #43160 so having that pass will give us higher confidence that things are good to go.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@freddan80 @sicong-li-arm @yair-ehrenwald, let's try this again.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43320) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43320) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 43319, "title": "Undocumented NotFoundError occurs when using tf.io.gfile.glob() to retrieve files in a non-existent directory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Chrome OS Linux 5.4.40-04224-g891a6cce2d44\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a\r\n- TensorFlow installed from (source or binary): Installed from source (pip3 install tensoflow)\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/a\r\n- GCC/Compiler version (if compiling from source): N/a\r\n- CUDA/cuDNN version: N/a\r\n- GPU model and memory: N/a\r\n\r\n**Describe the current behavior**\r\nWhen globbing files in a directory that does not exist, a non-documented tf.errors.NotFoundError is thrown if we try to wildcard glob files in a non-existent directory. This is not currently documented [here](https://www.tensorflow.org/api_docs/python/tf/io/gfile/glob).\r\n\r\n**Describe the expected behavior**\r\nWe expect that either a ValueError should be thrown, or the an empty list returned (since no files were matched).\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nfiles = tf.io.gfile.glob(\"non_existent_dir/*\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIssue was first discovered through [this PR](https://github.com/tensorflow/tfx/pull/2339#discussion_r490576170) for TFX, where we are expecting a ValueError instead of a NotFoundError.\r\n", "comments": ["I think that you can open this in https://github.com/tensorflow/io", "@bhack seems like this is the correct place to put this, as previous issues about tf.io.gfile.glob were also opened here\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/37758", "Yes sorry the code is in this repo. I have quickly read tfio instead of tf.io", "specifically, looking at stack traces, the tf.errors.NotFoundError is thrown at this line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4f3a8d84cf74f4f276bfd378e60efe5a18a6c30a/tensorflow/python/lib/io/file_io.py#L409", "I have tried in colab with TF version [2.3](https://colab.research.google.com/gist/ravikyram/16fd1051c7c8131f7f859fd67f02d036/untitled376.ipynb),[nightly ](https://colab.research.google.com/gist/ravikyram/6cd3e6a5984c26da393079210fb8ef88/untitled378.ipynb) versions and was able to reproduce the issue.Thanks!", "Opened a PR with a fix for this ([PR#43333](https://github.com/tensorflow/tensorflow/pull/43333)), not sure if this is an error we could be fixing at a deeper level, but proposed changes are linked.", "I would say that the error should be returned instead of silently failing\r\n\r\n```\r\n[test] \u03bb mkdir a b c\r\n[test] \u03bb ls\r\na  b  c\r\n[test] \u03bb ls d/*\r\nls: cannot access 'd/*': No such file or directory\r\n```", "PR to fix this issue has been merged, thanks again!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43319\">No</a>\n"]}, {"number": 43318, "title": "Pin numpy on windows too", "body": "", "comments": []}, {"number": 43317, "title": "Disable more tests on mac", "body": "", "comments": []}, {"number": 43316, "title": "Rename horizontal_fusion to horizontal_loop_fusion.", "body": "This is just to rename horizontal_fusion to horizontal_loop_fusion for better distinguishing between horizontal_loop_fusion and horizontal_input_fusion.", "comments": ["@thomasjoerg this is just the renaming per our previous discussion ([here](https://github.com/tensorflow/tensorflow/pull/43051/files#r485541826)).\r\n\r\nPlease help to take a look. Thanks!", "\r\nThanks Thomas for taking a look of the PR.\r\n\r\nI checked the CI results. The failures should not be related to the changes in this PR. Many of them are MLIR-related somehow.\r\n\r\nLet me know if you need my further assistance in merging this PR. Thanks!\r\n"]}, {"number": 43315, "title": "Pin estimator to the right version", "body": "We pinned to a version that was too new", "comments": []}, {"number": 43314, "title": "Add -D<tag> for every tag specified with TAGS=<tag_list>", "body": "This allows for ifdefs in the code based on the tag and is groundwork for kernel refactoring.\r\n\r\nManually tested that:\r\n```\r\n  make -f ... TAGS=\"cmsis-nn abcd-efgh hello_world\"\r\n```\r\nhas these as part of the cflags:\r\n```\r\n  -DCMSIS_NN -DABCD_EFGH -DHELLO_WORLD\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43313, "title": "Add rel/ pool", "body": "", "comments": ["From the `release/` version in `r2.3` branch, matching the same names as in `r2.2` branch. I only manually changed the macos libtensorflow since the full version was the one for nightly but we don't need the nightly flags here."]}, {"number": 43312, "title": "TensorFlow Custom loop gives different value of loss", "body": "I am using following versions:\r\n\r\n```\r\ntf version:  2.3.0\r\nkeras version:  2.4.0\r\n```\r\nmy customized error is\r\n\r\n```\r\ndef mse_fn(y_true, y_pred):\r\n  error = y_true - y_pred\r\n  mserror = tf.reduce_mean(tf.square(error))\r\n  return mserror\r\n```\r\ndata processing for my code is\r\n\r\n```\r\n(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\r\nY_train = np.array(Y_train).astype(np.float32)\r\nY_test = np.array(Y_test).astype(np.float32)\r\nX_train = X_train/255.\r\nX_test = X_test/255.\r\n```\r\nmy customized DNN layer is\r\n\r\n```\r\nclass MyDense(keras.layers.Layer):\r\n  def __init__(self, units, activation=None, **kwargs):\r\n    super().__init__(**kwargs)\r\n    self.units = units\r\n    self.activation = keras.activations.get(activation)\r\n  \r\n  def build(self, batch_input_shape):\r\n    self.kernel = self.add_weight(name = \"kernel\", shape=[batch_input_shape[-1], self.units], initializer=\"glorot_normal\")\r\n    self.bias = self.add_weight(name = \"bias\", shape=[self.units], initializer = 'zeros')\r\n    super().build(batch_input_shape)\r\n\r\n  def call(self, X):\r\n    return self.activation(X @ self.kernel + self.bias)\r\n  \r\n  def compute_output_shape(self, batch_input_shape):\r\n    return tf.TensorShape(batch_input_shape.as_list()[:-1]+[self.units])\r\n  \r\n  def get_config(self):\r\n    base_config = super().get_config()\r\n    return {**base_config, \"units\":self.units, \"activation\": keras.activations.serialize(self.activation)}\r\n\r\n```\r\nthe model is\r\n\r\n```\r\nclass DNN_model(keras.Model):\r\n  def __init__(self, output_dim, **kwargs):\r\n    super().__init__(**kwargs)\r\n    self.hidden1 = keras.layers.Flatten(input_shape = [28, 28])\r\n    self.hidden2 = MyDense(32, activation='relu')\r\n    self.hidden3 = MyDense(16, activation='relu')\r\n    self.hidden4 = MyDense(8, activation='relu')\r\n    self.hidden5 = MyDense(output_dim, activation='relu')\r\n  \r\n  def call(self, inputs):\r\n    Z = self.hidden1(inputs)\r\n    Z = self.hidden2(Z)\r\n    Z = self.hidden3(Z)\r\n    Z = self.hidden4(Z)\r\n    Z = self.hidden5(Z)\r\n    return Z\r\n\r\n```\r\nwhen I compile and fit using model.fit I get following loss\r\n\r\n```\r\nmodel_cust_loss = DNN_model(1)\r\nmodel_cust_loss.compile(optimizer='adam', loss = mse_fn, metrics=['accuracy'])\r\nmodel_cust_loss.fit(X_train, Y_train, epochs=20 , batch_size=32)\r\n```\r\nthe output\r\n\r\n```\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.5784 - accuracy: 0.1690\r\nEpoch 2/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 1.0625 - accuracy: 0.1910\r\nEpoch 3/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.8146 - accuracy: 0.1947\r\nEpoch 4/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.7062 - accuracy: 0.1981\r\nEpoch 5/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6342 - accuracy: 0.1999\r\nEpoch 6/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5785 - accuracy: 0.2014\r\nEpoch 7/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5263 - accuracy: 0.2015\r\nEpoch 8/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4914 - accuracy: 0.2036\r\nEpoch 9/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4480 - accuracy: 0.2066\r\nEpoch 10/20\r\n...\r\n```\r\nand then I try customized loop\r\n\r\n```\r\ndef random_batch(X, y, batch_size=32):\r\n  idx = np.random.randint(len(X), size=batch_size)\r\n  return X[idx], y[idx]\r\n\r\nn_epochs = 5\r\nbatch_size = 32\r\nn_steps = len(X_train)//batch_size\r\noptimizer = keras.optimizers.Adam() \r\nprint(\"n_steps: \", n_steps)\r\n\r\nepoch_losses = []\r\nfor epoch in range (1,n_epochs+1):\r\n  print(\"Epoch {}/{}\".format(epoch, n_epochs))\r\n  batch_losses = []\r\n  for step in range(1, n_steps+1):\r\n    #print(\"step:\",step)\r\n    X_batch, Y_batch = random_batch(X_train, Y_train, batch_size = 32)\r\n    with tf.GradientTape() as tape:\r\n      Y_pred = model_cust_loss(X_batch, training=True)\r\n      current_loss = mse_fn(Y_batch, Y_pred) \r\n    batch_losses.append(current_loss)\r\n    gradients = tape.gradient(current_loss, model_cust_loss.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients,model_cust_loss.trainable_variables))\r\n  epoch_losses.append(np.mean(batch_losses))\r\n  print(\"loss per epoch:\", np.mean(batch_losses))\r\n```\r\nThe output gives a different value of loss than the model.fit method\r\n\r\n```\r\nEpoch 1/5\r\nloss per epoch: 8.3357525\r\nEpoch 2/5\r\nloss per epoch: 8.387993\r\nEpoch 3/5\r\nloss per epoch: 8.356688\r\nEpoch 4/5\r\nloss per epoch: 8.362662\r\nEpoch 5/5\r\nloss per epoch: 8.374953\r\n```\r\ncould you help?\r\n\r\nThanks, Debottam", "comments": ["Please, can you clean up a little bit your example so that we can quickly copy, paste, run your code.\r\nThanks", "Hi @bhack , @amahendrakar \r\nI have created an example in github you should be able to copy and run as it is https://github.com/Debottam/tensorFlowCustomized/blob/master/customizedTensorFlow.ipynb", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/fb8218a6ea60aaabea14c8e304b600f3/43312-tf-nightly.ipynb). Thanks!", "Hi @amahendrakar , \r\nCould you provide a solution to this issue?\r\nThanks,\r\nDebottam.", "Hi @jvishnuvardhan ,\r\nCould you help me on this?\r\n\r\nThanks,\r\nDebottam", "Dear tensorflow developers,\r\nI need help in this matter.\r\nThanks,\r\nDebottam", "@Debottam I've not seen your model and data in detail but it seems to me very similar to the working example at ~~https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/customization/custom_training_walkthrough.ipynb~~ \r\nCan you give a  run to this tutorial?\r\n\r\nEdit:\r\nthe correct link is:\r\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/writing_a_training_loop_from_scratch.ipynb", "Hi @bhack \r\nThe loss of https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/writing_a_training_loop_from_scratch.ipynb matches with the \r\n\r\n```\r\nmodel_cust_loss.fit(X_train, Y_train, epochs=20 , batch_size=32)\r\n```\r\npart of my code which is \r\n\r\n```\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.5784 - accuracy: 0.1690\r\nEpoch 2/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 1.0625 - accuracy: 0.1910\r\nEpoch 3/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.8146 - accuracy: 0.1947\r\nEpoch 4/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.7062 - accuracy: 0.1981\r\nEpoch 5/20\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6342 - accuracy: 0.1999\r\n```\r\nbut the loss I get from my customized loop is way bigger. That is my question and would like to know why its happening?\r\n\r\nThanks,\r\nDebottam.", "It doesn't seems to me a bug. But if you want to have a specific support on your model/data generation I suggest you to post your question on our channel at:\r\nhttps://stackoverflow.com/questions/tagged/tensorflow.\r\n\r\nTickets are  mainly for Bug reports and Feature requests.", "@bhack ,\r\n\r\nDid that too https://stackoverflow.com/questions/63941144/tensorflow-custom-loop-gives-different-value-of-loss didn't get any response.\r\n\r\nThanks,\r\nDebottam.", "@Debottam I quickly checked and looks like there was a shape mismatch between `y_pred` and `y_true` depending on whether `y_pred` is from model.fit or custom training. I added a `reshape` command as shown below and the error are similar from both the approaches.\r\n\r\n`Y_pred = tf.reshape(Y_pred,(1,batch_size))`\r\n\r\nPlease take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ea88b6046ba08b12f0d0ecb0314d8ca7/43312-tf-nightly.ipynb). Thanks!\r\n\r\nOuput from the custom training\r\n\r\n```\r\nn_steps:  1875\r\nEpoch 1/20\r\nloss per epoch: 2.3010604\r\nEpoch 2/20\r\nloss per epoch: 1.0047752\r\nEpoch 3/20\r\nloss per epoch: 0.79963183\r\nEpoch 4/20\r\nloss per epoch: 0.68781537\r\nEpoch 5/20\r\nloss per epoch: 0.5867094\r\nEpoch 6/20\r\nloss per epoch: 0.527706\r\nEpoch 7/20\r\nloss per epoch: 0.4982657\r\nEpoch 8/20\r\nloss per epoch: 0.45960277\r\nEpoch 9/20\r\nloss per epoch: 0.43102258\r\nEpoch 10/20\r\nloss per epoch: 0.39782792\r\nEpoch 11/20\r\nloss per epoch: 0.3768253\r\nEpoch 12/20\r\nloss per epoch: 0.37661105\r\nEpoch 13/20\r\nloss per epoch: 0.34664243\r\nEpoch 14/20\r\nloss per epoch: 0.35457188\r\nEpoch 15/20\r\nloss per epoch: 0.3290785\r\nEpoch 16/20\r\nloss per epoch: 0.31267858\r\nEpoch 17/20\r\nloss per epoch: 0.30636093\r\nEpoch 18/20\r\nloss per epoch: 0.29752013\r\nEpoch 19/20\r\nloss per epoch: 0.27903932\r\nEpoch 20/20\r\nloss per epoch: 0.2706806\r\n```\r\n", "Hi @jvishnuvardhan ,\r\nMy apologies for the delayed reply. Thanks a lot for debugging it. \r\n\r\nThanks,\r\nDebottam."]}, {"number": 43311, "title": "Building Tensorflow using non-default GCC fails with /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found and further errors", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**SLURM cluster node: Linux gnode028 3.10.0-1062.12.1.el7.x86_64 #1 SMP x86_64 x86_64 x86_64 GNU/Linux**\r\n\r\n- TensorFlow version:\r\n**TensorFlow v2.3.0**\r\n\r\n- Python version:\r\n**Python 3.8.0 complied from source**\r\n\r\n- Bazel version (if compiling from source):\r\n**bazel 3.1.0 compiled from source**\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n**gcc (GCC) 7.3.0 compiled from source**\r\n\r\n- CUDA/cuDNN version:\r\n**CUDA 10.2**\r\n\r\n- GPU model and memory:\r\n**Tesla K20m**\r\n\r\n**Describe the problem**\r\nI'm trying to compile Tensorflow on a SLURM cluster machine where the default version of GCC is outdated (`4.8.5`) and cannot be upgraded. The only solution is to use newer version of GCC (in my case `7.3.0`)  installed as user. However, when using non-default GCC the compilation procedure ends up with the following error:\r\n\r\n`bazel-out/host/bin/external/flatbuffers/flatc: /lib64/libstdc++.so.6: version GLIBCXX_3.4.20 not found (required by bazel-out/host/bin/external/flatbuffers/flatc)`\r\n\r\nFrom what I understand the root of the problem is the fact that `Bazel` links to the default libraries from hardcoded `/lib64/libstdc++` instead of using the libraries from non-default gcc. The problem has been reported frequently by many different users. I tried all the solutions provided in multiple issues (https://github.com/bazelbuild/bazel/issues/4510, https://github.com/tensorflow/tensorflow/issues/38718, https://github.com/tensorflow/tensorflow/issues/26826), implemented many of them but eventually failed to finish compilation successfully. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nMy goal is to create a comprehensive instructions for all the people facing this issue. I created a gist file listing all the steps I followed (including compiling `Bazel` from source): https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445\r\n\r\nAfter all the steps my setup consists of `Python 3.8`, `GCC 7.3.0`, `Bazel 3.1.0` and `Binutils 2.34`. All these packages are installed locally as user.\r\n\r\n**Any other info / logs**\r\n`Bazel 3.1.0` is compiled from source using the following command:\r\n```\r\nexport GCC_HOST_COMPILER_PATH=$HOME/gcc/bin/gcc\r\nexport GCC_HOST_COMPILER_PREFIX=$HOME/gcc/bin\r\nexport CXX=$HOME/gcc/bin/gcc\r\nexport CC=$HOME/gcc/bin/gcc\r\nexport LD_LIBRARY_PATH=$HOME/gcc/lib64\r\nexport LDFLAGS=\"-L$HOME/gcc/lib -L$HOME/gcc/lib64\"\r\nexport CXXFLAGS=\"-L$HOME/gcc/lib -L$HOME/gcc/lib64\"\r\n\r\nenv BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm bash ./compile.sh\r\nmkdir $HOME/bin && cp output/bazel $HOME/bin\r\n```\r\n\r\nThe TensorFlow build process initiated by:\r\n```\r\nexport PATH=~/bin:$PATH\r\nexport GCC_HOST_COMPILER_PATH=$HOME/gcc/bin/gcc\r\nexport GCC_HOST_COMPILER_PREFIX=$HOME/gcc/bin\r\nexport CXX=$HOME/gcc/bin/gcc\r\nexport CC=$HOME/gcc/bin/gcc\r\nexport LD_LIBRARY_PATH=$HOME/gcc/lib64\r\nexport LDFLAGS=\"-L$HOME/gcc/lib -L$HOME/gcc/lib64\"\r\nexport CXXFLAGS=\"-L$HOME/gcc/lib -L$HOME/gcc/lib64\"\r\n\r\nenv BAZEL_LINKOPTS=-static-libstdc++:-static-libgcc BAZEL_LINKLIBS=-l%:libstdc++.a:-lm BAZEL_CXXOPTS=-std=gnu++0x bazel build //tensorflow/tools/pip_package:build_pip_package --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" --verbose_failures\r\n```\r\n\r\nfails with:\r\n\r\n```\r\nERROR: /home/users/user/tensorflow/tensorflow/lite/toco/BUILD:439:1: Linking of rule '//tensorflow/lite/toco:toco' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/users/user/.cache/bazel/_bazel_user/77a387926d2bc2a010179601b7d63fc2/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/home/users/user/gcc/lib64 \\\r\n    PATH=/home/users/user/bin:/home/users/user/tensorflow/venv/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/slurm/bin:/bin:/opt/slurm/bin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/host/bin/tensorflow/lite/toco/toco-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<long long>::allocator()'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 10.872s, Critical Path: 1.07s\r\nINFO: 5 processes: 5 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI allow myself to cc @krafczyk and @soporteCluster as they were reporting similar issues and may provide some insight.\r\n\r\nI'm ready to try different solutions and provide more info. I really want this issue to be solved once and for all. I understand that the root cause may be related to how Bazel operates. Anyway, the issue affects many TensorFlow users. Let's provide some workaround for them. Thanks in advance for all suggestions. I promise to test them all.\r\n", "comments": ["I don't think that modern TF versions (like your 2.3.0) could be built with gcc<5.\r\nThis is the official list of the tested configurations:\r\nhttps://www.tensorflow.org/install/source#tested_build_configurations", "@bhack thanks for your comment. I updated the issue to clarify that I use `GCC 7.3.0` installed as user and that causes the problem.", "Just to be sure:\r\nHave you already  run `bazel clean --expunge` before you final configuration build?", "@bhack Yes. I do `bazel clean --expunge` before testing any new build settings", "Hi,\r\nI compiled it following the steps that i have commented below:\r\nhttps://github.com/tensorflow/tensorflow/issues/38718#issuecomment-624681019\r\n\r\nI had to install bazel again with that configuration and then build Tensorflow.\r\nI made the compilation using the following environment variables:\r\nPATH=/share/apps/curl/bin/curl:$PATH\r\nLD_LIBRARY_PATH=/share/apps/curl/lib/:$LD_LIBRARY_PATH \r\nPATH=/share/apps/git/bin/:$PATH\r\nPATH=/share/apps/binutils/binutils-2.34/bin/:$PATH\r\nLD_LIBRARY_PATH=/share/apps/binutils/binutils-2.34/lib/:$LD_LIBRARY_PATH\r\nI have all the programs installed in /share/apps because it is a Cluster HPC.", "@soporteCluster Your comment (https://github.com/tensorflow/tensorflow/issues/38718#issuecomment-624681019) was very helpful for me. I followed all your steps. Unfortunately, they don't work for me. What TF and CUDA version did you use?", "I finally solved this issue. The detailed build instruction is available at: https://gist.github.com/jakublipinski/40ba68994fe0092600a05b0060e7d445", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43311\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43311\">No</a>\n"]}, {"number": 43310, "title": "Pin Estimator nightly to latest version used during release", "body": "", "comments": []}, {"number": 43309, "title": "tf.nn.softmax_cross_entropy_with_logits output on multichannel continuous targets", "body": "Hi,\r\n\r\nHow does the tf.nn.softmax_cross_entropy_with_logits work for a target that is continuous with C channels,\r\neg - HxWxC, they are not soft targets (values across channels dont add up to 1) with values ranging between 0 and 1.\r\n\r\nThanks in advance\r\n", "comments": ["This is a support question for https://stackoverflow.com/questions/tagged/tensorflow\r\nPlease close this ticket and submit your question there.\r\n\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43308, "title": "Pin Estimator-nightly to latest version used for 2.2 release", "body": "", "comments": []}, {"number": 43307, "title": "g++/gcc cannot find header files in/or libtensorflow_cc.so", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 on docker (image: `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04`)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version: **r2.3**\r\n- Python version: **3.7.6**\r\n- Installed using virtualenv? pip? conda?: **No**\r\n- Bazel version (if compiling from source): **3.1.0**\r\n- GCC/Compiler version (if compiling from source): **7.5.0**\r\n- CUDA/cuDNN version: **10.1/7.6**\r\n- GPU model and memory: **RTX 2080 Super - 8GB**\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm building tensor flow from source for a C++ project using the guide here: https://www.tensorflow.org/install/source\r\nI can generate the `.so` files using the bazel build however, when i try to link the lib to my program, i get\r\n\r\n`fatal error: tensorflow/cc/saved_model/loader.h: No such file or directory`\r\n\r\nLocation of build files:\r\n\r\n```\r\nroot@167c90dbc0fe:~# ls -l /tensorflow/bazel-bin/tensorflow/\r\ntotal 427692\r\ndrwxr-xr-x  5 root root      4096 Sep 17 19:17 c\r\ndrwxr-xr-x  6 root root      4096 Sep 17 19:16 cc\r\ndrwxr-xr-x  6 root root      4096 Sep 17 18:28 compiler\r\ndrwxr-xr-x 19 root root     12288 Sep 17 19:19 core\r\nlrwxrwxrwx  1 root root        21 Sep 17 19:19 libtensorflow_cc.so -> libtensorflow_cc.so.2\r\nlrwxrwxrwx  1 root root        25 Sep 17 19:19 libtensorflow_cc.so.2 -> libtensorflow_cc.so.2.3.0\r\n-r-xr-xr-x  1 root root 399797080 Sep 17 19:19 libtensorflow_cc.so.2.3.0\r\n-r-xr-xr-x  1 root root    178414 Sep 17 18:39 libtensorflow_cc.so.2.3.0-2.params\r\nlrwxrwxrwx  1 root root        32 Sep 17 18:30 libtensorflow_framework.so.2 -> libtensorflow_framework.so.2.3.0\r\n-r-xr-xr-x  1 root root  37893936 Sep 17 18:30 libtensorflow_framework.so.2.3.0\r\n-r-xr-xr-x  1 root root     47188 Sep 17 18:20 libtensorflow_framework.so.2.3.0-2.params\r\ndrwxr-xr-x 10 root root      4096 Sep 17 19:06 stream_executor\r\n```\r\n\r\nSource file: `load_saved_model.cpp`\r\n\r\n```\r\n#include <stdlib.h>\r\n#include <stdio.h>\r\n#include <string>\r\n#include \"tensorflow/cc/saved_model/loader.h\"\r\n\r\nusing namespace std;\r\n\r\n\r\n\r\nint  main(int argc, char* argv[]){\r\n    printf(\"Will load and serve a saved model...\")\r\n}\r\n```\r\n\r\nCompiling the source file using the following:\r\n\r\n```g++ -L/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc -ltensorflow_framework  -o /object_detection/load_saved_model -c /object_detection/load_saved_model.cpp```\r\n\r\nresults in:\r\n\r\n```\r\nroot@167c90dbc0fe:/object_detection# g++ -L/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc -ltensorflow_framework -I/tensorflow/bazel-bin/  -o /object_detection/load_saved_model -c /object_detection/load_saved_model.cpp \r\n/object_detection/load_saved_model.cpp:4:10: fatal error: tensorflow/cc/saved_model/loader.h: No such file or directory\r\n #include \"tensorflow/cc/saved_model/loader.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. `git clone https://github.com/tensorflow/tensorflow.git `\r\n2. `git checkout r2.3`\r\n3. `bazel clean`\r\n4. `root@167c90dbc0fe:/tensorflow# ./configure`\r\n\r\n```\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.6/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]\r\n/usr/local/lib/python3.6/dist-packages\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda-10.1/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.1/targets/x86_64-linux/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5, 7.0, 7.5\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\n5. `bazel build --jobs=8 --verbose_failures --config=cuda -c opt //tensorflow:libtensorflow_cc.so`\r\n\r\n```\r\nTarget //tensorflow:libtensorflow_cc.so up-to-date:\r\n  bazel-bin/tensorflow/libtensorflow_cc.so\r\nINFO: Elapsed time: 3582.131s, Critical Path: 175.91s\r\nINFO: 11806 processes: 11806 local.\r\nINFO: Build completed successfully, 16884 total actions\r\n```\r\n\r\n7. Run the `load_saved_model.cpp`\r\n\r\n```\r\nroot@167c90dbc0fe:/object_detection# g++ -L/tensorflow/bazel-bin/tensorflow/ -ltensorflow_cc -ltensorflow_framework -I/tensorflow/bazel-bin/  -o /object_detection/load_saved_model -c /object_detection/load_saved_model.cpp \r\n\r\n/object_detection/load_saved_model.cpp:4:10: fatal error: tensorflow/cc/saved_model/loader.h: No such file or directory\r\n #include \"tensorflow/cc/saved_model/loader.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\n**Additional info**\r\n\r\nHow can i link the generated `.so` files to my cpp program?\r\n\r\nI also tried \r\n\r\n1. moving the `.so` files to `/usr/lib/x86_64-linux-gnu/` followed by `ldconfig` \r\n2. Adding the `.so` location to a `.conf` file and then adding the conf file to `./etc/ld.so.conf.d` which didn't help either.\r\n\r\nThanks", "comments": ["I think that you need to include (`-I`) `tensorflow/cc/saved_model/` from the source code dir. ", "It was an issue with the build targets which I could figure out from [40538](https://github.com/tensorflow/tensorflow/issues/40538) . Sad that the official [build from source](https://www.tensorflow.org/install/source) leaves much to desired\r\n\r\nChanging my bazel build command to \r\n\r\n```\r\nbazel build --jobs=8 --config=v2 --copt=-O3 --copt=-m64 --copt=-march=native --verbose_failures //tensorflow:install_headers //tensorflow:tensorflow //tensorflow:tensorflow_cc //tensorflow:tensorflow_framework //tensorflow/tools/lib_package:libtensorflow\r\n```\r\ngenerates the header files. (I was missing esp ` //tensorflow:install_headers` earlier)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43307\">No</a>\n", "@borarak Yes `//tensorflow:install_headers` is not documented. \r\nCan you open a new Doc type issue specific for this (or a PR if you can) Thanks.", "@bhack Sure, i will raise an issue and try to do a PR this weekend, thanks", "I'm yet to raise a PR but if it helps anybody I have documented all information for installation and setup of the C++ API here: https://github.com/borarak/tensorflow2_cpp\r\n\r\n", "This is my gist explaining how to compile TF .so and also create headers step-by-step: https://gist.github.com/rangsimanketkaew/f372eca5ded50ca9a0cc61d3021a1c3a"]}, {"number": 43306, "title": "Pin estimator nightly to the latest 2.1 version", "body": "", "comments": []}, {"number": 43305, "title": "Disable test which fails on mac pip", "body": "", "comments": []}, {"number": 43304, "title": "Model and layers set to non-trainable, but weights still adjusted", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version (use command below): Google Colab 2.3.0\r\n- Python version: 3.6.9 (default, Jul 17 2020, 12:50:27) \\n[GCC 8.4.0]\r\n- GPU model and memory: Google Colab GPU\r\n\r\nI want to freeze a model. That means, set it to non-trainable. I do so by setting model.trainable and the individual layers to non-trainable. However, when I then fit this model (with just 1 epoch) I can see afterwards that the model weights changed. I can see this by checking the model.evaluate output and by comparing the model weights with print(model.trainable_variables) prior and afterwards the model fitting.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport numpy as np\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\n(train_x, train_labels), (test_x, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\r\n\r\nx_train_padded = pad_sequences(train_x, maxlen=500)\r\nx_test_padded = pad_sequences(test_x, maxlen=500)\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Embedding(10000, 128, input_length=500),\r\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\r\n    tf.keras.layers.GlobalAveragePooling1D(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),optimizer='adam', metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\r\n\r\nhistory = model.fit(x=x_train_padded,\r\n                      y=train_labels,\r\n                      validation_data=(x_test_padded , test_labels),\r\n                      epochs=4, batch_size=128)\r\n```\r\n\r\nThis gives the output:\r\n\r\n> Epoch 4/4\r\n> 196/196 [==============================] - 9s 48ms/step - loss: 0.1202 - accuracy: 0.9578 - val_loss: 0.3663 - val_accuracy: 0.8669\r\n\r\nI can confirm this with:\r\n\r\n`model.evaluate(x_test_padded , test_labels)`\r\n\r\n> 782/782 [==============================] - 3s 4ms/step - loss: 0.3663 - accuracy: 0.8669\r\n> \r\n> [0.36633849143981934, 0.866919994354248]\r\n\r\nNow I set the model and individual layers to non-trainable:\r\n\r\n```\r\nmodel.trainable=False\r\n\r\nfor layer in model.layers:\r\n  layer.trainable=False\r\n```\r\n\r\nI check with `model.summary()` that\r\n\r\n>  Trainable params: 0\r\n\r\nAnd fit the model again:\r\n```\r\nhistory = model.fit(x=x_train_padded,\r\n                      y=train_labels,\r\n                      validation_data=(x_test_padded , test_labels),\r\n                      epochs=1, batch_size=128)\r\n```\r\n\r\nThis gives:\r\n\r\n\r\n\r\n> 196/196 [==============================] - 10s 52ms/step - loss: 0.0911 - accuracy: 0.9677 - val_loss: 0.4298 - val_accuracy: 0.8648\r\n\r\nI can again check this with\r\n\r\n`model.evaluate(x_test_padded , test_labels)`\r\n\r\n\r\n\r\n> 782/782 [==============================] - 4s 5ms/step - loss: 0.4298 - accuracy: 0.8648\r\n> \r\n> [0.4298333525657654, 0.8648399710655212]\r\n\r\n\r\nWhich clearly shows that the model was adjusted. The val_loss and val_accuracy is not equal to loss: 0.3663 - accuracy: 0.8669. Moreover, when I check the model weights with \r\n`print(model.trainable_variables)` before and afterwards I can see that the model weights got adjusted. However, actually all parameters should be set to non-trainable.\r\n\r\n", "comments": ["It is not a bug you need to `model.compile` again after you set to `False`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43304\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43304\">No</a>\n"]}, {"number": 43303, "title": "How about we stop deprecating everything every other release....", "body": "I can't believe how terrible the experience is using these tools.\r\n\r\nIt seems the whole process is playing jigsaw puzzle games trying to line up all the versions so that something, anything works.\r\n\r\nWe are sorry but your original working tensorflow code for training is being moved to generic tf.slim for generic training.\r\nWe are sorry but your changed code using generic tf.slim was not specific enough, make these changes.\r\nWe are sorry but after having you do all this, tf.slim is moving to contrib.\r\nOkay, now contrib is going away and here is a basic script that will just tell you cannot change anything for you, code it all again yourself. All 3rd party guides are now worthless.\r\n(Roughly after about a year of this, I realized what a waste of time and left tensorflow)\r\n\r\nNow I'm back only to try to get other frameworks (which work) models converted to tflite since it's the only decent solution for performance on mobile.\r\nOf course the tools to convert to tf-lite require a bunch of tensorflow things to be installed again.\r\nOops, I'm using Cuda 10.1, watch pip packages try opening libcudart-9.\r\nInstall tensorflow 2.x from source for cuda 10.1, whoops it wants newer python now.\r\nInstall tensorflow addons, BOOM it doesn't support tensorflow 2.4.0.\r\nBackdate to tensorflow 2.1, rebuild everything.\r\nInstall tensorflow addons, BOOM it doesn't support tensorflow <2.2.0.\r\n\r\nI know this will just get closed and someone will mention that I'm wrong for being a Gentoo user who expects to build from source and not limp off of a packaging system to fix a project that cannot properly figure out how to handle backwards compatibility. Nor will I use some \"docker\" container....\r\n\r\nDo you need some help coding? Is this intentional lock-in related movement to keep everyone rewriting things? Can you stop deprecating things long enough for someone to actually build something longer than a year and have it work when they are done? Thanks.", "comments": ["Comment previously posted on Tensorflow/addons, but I deprecated it to get it over here ;)", "I think it is a duplicate of https://github.com/tensorflow/tensorflow/issues/43302", "Here's some more...\r\n\r\nConverting a yolov3 model to TFLITE to run on a phone.\r\n\r\nTried 4 different projects.\r\n\r\n1st: All code worked until needing slim from tf.contrib. Didn't want to rewrite their code.\r\n2nd: needed onnx and tensorflow_addons but it doesn't work with tensorflow 1.x. Older pip package for tensorflow_addons with 1.x support is now removed so it cannot be done.\r\n3rd: Go all the way through, it spit out tflite files that I then mapped into my android project. Crashes with error that I don't have a ZIP with metadata.\r\n\r\nYou'd think that my tflite means it would be the right format.... but TFLITE has various \"metadata\" and looks like mine is missing it.\r\n\r\njava.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n\r\nStuck again. My tflite in a binary editor looks similar but has \"TOCO Converted\" where the example tflite for android shows \"TFLITE_METADATA\".\r\n\r\nJust documenting the various ways I get stuck here, since I need a big collection to get enough ammo to make the case to management  for using something else.\r\n\r\n", "@acidtonic Thank you for your feedback and apologies for the inconvenience.\r\n`tf.contrib` module was a collection of modules contributed by external users as well as internal resources and it was a big challenge to maintain all of it along with TF 2 upgrades.\r\n\r\nSo we wanted to apply a divide and conquer approach where part of it is consumed in TF Core, another part where we have designated owners and some portions deprecated which were relatively less popular and no one claimed ownership.\r\nAs a result we have [TF Addons](https://www.tensorflow.org/addons) (works with TF 2.X) where we have revived popular modules of `tf.contrib` and also provides a place where users can request to add new functionality as well.\r\nWhen using TF Addons it can be a good idea to familiarize yourself with python op compatibility with various TF versions. See https://github.com/tensorflow/addons#python-op-compatility\r\n\r\nLot of our developers prefer having support for the latest CUDA versions and sadly, CUDA is not backward compatible so some of the moves are breaking. As you can imagine it would create too many branches/forks to support several versions at once, which can be very resource intensive.\r\n\r\nWe appreciate your feedback and are committed to enhance TensorFlow significantly and provide a better user experience.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 43302, "title": "changes its APIs quite often", "body": "There are a lot of very good projects by top international experts (think Google) that are dropping Tensorflow because it changes its APIs quite often we shouldn't expect users to have always a GPU. Please consider this failure.", "comments": ["Thanks for your feedback. For users interested in using TF with gpu, tpu can try using [Google Colab](https://colab.sandbox.google.com/notebooks/intro.ipynb#recent=true). The google colab also hosts latest TensorFlow version thus saves the user from the hassle of setting up the environment, All you need is a good internet connection. ", "Closing as there is no real issue to solve.\r\n\r\nOur API keeps evolving based on user studies and always improving. If there is actionable feedback, please open new issue."]}, {"number": 43301, "title": "Should we remove deprecated and ignored field 'version' in graph.proto", "body": "We noticed that the `version` field in `graph.proto` has been deprecated (since 5 years ago). When we tried to generate `GraphDef` scala code by `sbt-protoc` from this proto file, it produced annoying \"deprecation\" warnings during the compilation stage. \r\n\r\nShould we remove it from tensorflow's codebase?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto#L24-L27\r\n\r\nAccording to the description in the codebase:\r\n>  // Deprecated single version field; use versions above instead.  Since all\r\n    // GraphDef changes before \"versions\" was introduced were forward\r\n    // compatible, this field is entirely ignored.\r\n\r\nThis field seems to be completely not used, no matter old or new versions. \r\n\r\nWe appreciate any feedbacks. Thanks!", "comments": ["Hey, @ymodak thanks for assigning correct tags on this issue. Is there any update? \r\n\r\nI think we just need to remove the deprecated `version` field since it is not used at all.  ", "I don't think we can remove it until we are sure there is no client that expects to see it in the proto. Otherwise, said client will get a bad parse, segfault or exhibit other unwanted behavior.", "Closing this issue for now. Feel free to revisit if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43301\">No</a>\n"]}, {"number": 43300, "title": "Crash when attempting to use xla.conv with complex inputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-41557-gae0a324182 2.4.0-dev20200915\r\n- Python version: Python 3.8.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nTF throws an exception when trying to use `tensorflow.compiler.tf2xla.python.xla.conv` with complex inputs, and the stack trace indicates it may be a bug in the HLO -> LLVM IR lowering.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.compiler.tf2xla.python import xla as tfxla\r\nfrom tensorflow.compiler.xla import xla_data_pb2\r\n\r\nproto = xla_data_pb2.ConvolutionDimensionNumbers()\r\nproto.input_batch_dimension = 0\r\nproto.input_feature_dimension = 1\r\nproto.output_batch_dimension = 0\r\nproto.output_feature_dimension = 1\r\nproto.kernel_output_feature_dimension = 0\r\nproto.kernel_input_feature_dimension = 1\r\nproto.input_spatial_dimensions.extend([2, 3])\r\nproto.kernel_spatial_dimensions.extend([2, 3])\r\nproto.output_spatial_dimensions.extend([2, 3])\r\n\r\nlhs = np.ones((2,3,9,10), dtype=np.complex64)\r\nrhs = np.ones((3,3,4,5), dtype=np.complex64)\r\n\r\npadding = ((0, 0), (0, 0))\r\nwindow_strides, lhs_dilation, rhs_dilation = (1, 1), (1, 1), (1, 1)\r\nfeature_group_count = 1\r\n\r\ntfxla.conv(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation,\r\n           proto, feature_group_count=feature_group_count, precision_config=None)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nStack trace:\r\n\r\n```\r\n2020-09-17 16:13:20.385677: E tensorflow/compiler/xla/status_macros.cc:56] Internal: RET_CHECK failure (tensorflow/compiler/xla/service/cpu/cpu_compiler.cc:501) !llvm::verifyModule(llvm_module, &err_stream) Invalid LLVM IR before optimizations:\r\nFloating-point arithmetic operators only work with floating-point types!\r\n  %60 = fmul reassoc nsz contract %complex64 %57, %59\r\nFloating-point arithmetic operators only work with floating-point types!\r\n  %62 = fadd reassoc nsz contract %complex64 %61, %60\r\n\r\nThis probably indicates a bug in the HLO -> LLVM IR lowering. Rerun with --xla_dump_to to get the IR. \r\n*** Begin stack trace ***\r\n\r\n        xla::status_macros::MakeErrorStream::Impl::GetStatus()\r\n\r\n        xla::cpu::CpuCompiler::RunBackend(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*)\r\n        xla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*)\r\n        xla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_2020_02_25::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\r\n        xla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_2020_02_25::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\r\n        tensorflow::XlaCompilationCache::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&, std::unique_ptr<xla::LocalExecutable, std::default_delete<xla::LocalExecutable> >*)\r\n        tensorflow::XlaCompilationCache::CompileImpl(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, absl::lts_2020_02_25::Span<tensorflow::XlaArgument const>, std::function<tensorflow::Status (tensorflow::XlaCompiler*, tensorflow::XlaCompilationResult*)> const&, absl::lts_2020_02_25::optional<long long>, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\r\n        tensorflow::XlaCompilationCache::CompileSingleOp(tensorflow::XlaCompiler::Options const&, absl::lts_2020_02_25::Span<tensorflow::XlaArgument const>, tensorflow::OpKernelContext*, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\r\n        tensorflow::XlaCompileOnDemandOp::Compile(tensorflow::OpKernelContext*, tensorflow::XlaCompilationResult const**, tensorflow::XlaCompilationCache**, absl::lts_2020_02_25::flat_hash_map<int, absl::lts_2020_02_25::optional<tensorflow::Tensor>, absl::lts_2020_02_25::hash_internal::Hash<int>, std::equal_to<int>, std::allocator<std::pair<int const, absl::lts_2020_02_25::optional<tensorflow::Tensor> > > >*, xla::LocalExecutable**)\r\n        tensorflow::XlaCompileOnDemandOp::Compute(tensorflow::OpKernelContext*)\r\n        tensorflow::KernelAndDeviceOp::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<absl::lts_2020_02_25::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<absl::lts_2020_02_25::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tensorflow::CancellationManager*, absl::lts_2020_02_25::optional<tensorflow::EagerRemoteFunctionParams> const&)\r\n        tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_2020_02_25::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, absl::lts_2020_02_25::optional<tensorflow::EagerRemoteFunctionParams> const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::lts_2020_02_25::Span<tensorflow::TensorHandle*>)\r\n        tensorflow::ExecuteNode::Run()\r\n        tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*)\r\n\r\n        tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*)\r\n        tensorflow::EagerOperation::Execute(absl::lts_2020_02_25::Span<tensorflow::AbstractTensorHandle*>, int*)\r\n        TFE_Execute\r\n        TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::lts_2020_02_25::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::lts_2020_02_25::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*)\r\n\r\n\r\n\r\n\r\n        _PyObject_MakeTpCall\r\n        _PyEval_EvalFrameDefault\r\n        _PyEval_EvalCodeWithName\r\n        _PyFunction_Vectorcall\r\n        _PyEval_EvalFrameDefault\r\n        _PyEval_EvalCodeWithName\r\n        _PyFunction_Vectorcall\r\n        _PyEval_EvalFrameDefault\r\n        _PyEval_EvalCodeWithName\r\n        _PyFunction_Vectorcall\r\n        _PyEval_EvalFrameDefault\r\n        _PyEval_EvalCodeWithName\r\n        _PyFunction_Vectorcall\r\n        _PyEval_EvalFrameDefault\r\n        _PyEval_EvalCodeWithName\r\n        PyEval_EvalCode\r\n\r\n        PyRun_FileExFlags\r\n        PyRun_SimpleFileExFlags\r\n\r\n        Py_BytesMain\r\n        __libc_start_main\r\n        _start\r\n*** End stack trace ***\r\n```", "comments": ["If I remember correctly `xla.conv` doesn't support complex type inputs. \r\nSee e.g. the recent Jax patch:\r\nhttps://github.com/google/jax/pull/3735", "I have tried in colab with TF versions 2.3, nightly versions(`2.4.0-dev20200917`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0ea6e759643cd4078d5caf742db8c1b8/untitled379.ipynb). Thanks!", "@bchetioui \r\nThis has been fixed on tf 2.5, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/37b3b34cf9b4539c628a6f164c2cb018/untitled597.ipynb), and move this to closed status.", "Thank you for the update! I have run it on my machine as well and confirmed it now works.\r\nI am thus closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43300\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43300\">No</a>\n"]}, {"number": 43299, "title": "Add GPU kernels for ApplyProximalAdagrad and SparseApplyFtrl", "body": "These were observed to improve performance in some models when run on the GPU. Kernels for additional ops will be added in subsequent PRs.\r\n\r\nI'm not sure how to make the relevant tests run on both the CPU and GPU (currently they only seem to run on the CPU and I had to manually force them with `use_gpu=True` during development). Let me know if there's an easy way to do that.\r\n\r\ncc @nluehr @reedwm ", "comments": ["Closing this in favor of multiple smaller PRs starting with https://github.com/tensorflow/tensorflow/pull/43814."]}, {"number": 43298, "title": "There is a bug about tf/keras/Sequential#fit()", "body": "For \"tf.keras.Sequential.fit\" API, in TF2.1, documentation said that \"fit_generator\" is deprecated and \"fit\" support generator. But actually, in TF2.0, I find that \"fit\" also support generator. So it means that the deprecated information mislead to users that TF2.0 \"fit\" API cannot support the function of \"fit_generator\" API.", "comments": ["Yes but the meaning is the `fit_generator` is deprecated starting from TF >=2.1. \r\nIt is just a warning to port your code cause at some point it will be removed.", "The warnings are removed from the nightly version of documentation.\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=nightly#fit_generator", "@ymodak But still deprecated right? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L1795-L1798", "@bhack That's correct. \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Sequential#deprecated_5"]}, {"number": 43297, "title": "Kernel crash with complex matrices on Windows", "body": "Hello, I'd like to raise this issue again as I am encountering the same problem on TF 2.3.0.\r\n\r\nThe original issue at https://github.com/tensorflow/tensorflow/issues/40414 attempts to replicate using Colab, which isn't running Windows.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10.0.18362**\r\n- TensorFlow installed from (source or binary): **Binary (pip)**\r\n- TensorFlow version (use command below): **2.3.0 (v2.3.0-rc2-23-gb36436b087)**\r\n- Python version: **3.8.5**\r\n- CUDA/cuDNN version: **10.1.120/7.6.5 (also tried 10.1.243/7.6.5)**\r\n- GPU model and memory: **GeForce RTX 2080 Ti, 11265 MB**\r\n\r\n**Describe the current behavior**\r\n\r\nMultiplying two matrices of complex type (values themselves can be real) with either `adjoint_a=True` or `adjoint_b=True` results in the following crash. It also seems to crash when running `eig`. This was reproduced on both TF 2.3.0 and 2.2.0, but works on 2.1.0. It also seems to be working on Linux.\r\n\r\nI tried doing a fresh install of CUDA with CUDA 10.1.120 and 10.1.243. In both cases the [MNIST tutorial example](https://www.tensorflow.org/tutorials/quickstart/beginner) runs with no problems.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\na = tf.constant([[1, 0], [0, 1]], dtype=tf.complex64)\r\ntf.linalg.matmul(a, a)  # No issues\r\ntf.linalg.matmul(a, a, adjoint_a=True)  # Crash\r\n```\r\n\r\n**Other info / logs**:\r\n\r\n```\r\n>>> tf.linalg.matmul(a, a)\r\n2020-08-25 18:30:03.799456: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n<tf.Tensor: shape=(2, 2), dtype=complex64, numpy=\r\narray([[1.+0.j, 0.+0.j],\r\n       [0.+0.j, 1.+0.j]], dtype=complex64)>\r\n>>> tf.linalg.matmul(a, a, adjoint_a=True)\r\n2020-08-25 18:30:07.982835: E tensorflow/stream_executor/cuda/cuda_driver.cc:951] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FF991B9B115     tensorflow::CurrentStackTrace\r\n0x00007FF9918E989E      tensorflow::CostGraphDef_Node::set_is_final\r\n0x00007FF991A91D7E      stream_executor::StreamExecutor::SetDeviceSharedMemoryConfig\r\n0x00007FF98F622C16      tensorflow::StepStats::internal_default_instance\r\n0x00007FF98F634444      google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add\r\n0x00007FF9774EF867      std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=\r\n0x00007FF9774CA7AB      absl::lts_2020_02_25::Span<tensorflow::Tensor const >::end\r\n0x00007FF9774431BF      TFE_TensorHandleResolve\r\n0x00007FF9773E0A33      TFE_Py_TensorShapeSlice\r\n0x00007FF9773DE29A      std::_Tree<std::_Tmap_traits<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,0>,tensorflow::monitoring::CounterCell,std::less<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char>\r\n0x00007FF9A51CA3D6      PyList_New\r\n0x00007FF9A51F5626      Py_CheckFunctionResult\r\n0x00007FF9A51F7954      PyEval_EvalFrameDefault\r\n0x00007FF9A51F596E      Py_CheckFunctionResult\r\n0x00007FF9A51F7954      PyEval_EvalFrameDefault\r\n0x00007FF9A51F2EF8      PyEval_EvalCodeWithName\r\n0x00007FF9A51F5C66      Py_CheckFunctionResult\r\n0x00007FF9A51F801E      PyEval_EvalFrameDefault\r\n0x00007FF9A51F3D7D      PyFunction_Vectorcall\r\n0x00007FF9A525FAF9      PyEval_GetFuncDesc\r\n0x00007FF9A525F8DD      PyEval_GetFuncDesc\r\n0x00007FF9A51E7DE4      PyObject_Repr\r\n0x00007FF9A5181E2F      PyFile_WriteObject\r\n0x00007FF9A5181B1B      PyFile_WriteString\r\n0x00007FF9A51EB200      PyFloat_AsDouble\r\n0x00007FF9A51E401E      PyObject_CallFunctionObjArgs\r\n0x00007FF9A51F9E70      PyEval_EvalFrameDefault\r\n0x00007FF9A51F2EF8      PyEval_EvalCodeWithName\r\n0x00007FF9A52038DF      PyEval_EvalCodeEx\r\n0x00007FF9A520383D      PyEval_EvalCode\r\n0x00007FF9A5203526      PyArena_New\r\n0x00007FF9A52034B5      PyArena_New\r\n0x00007FF9A5365760      PyRun_InteractiveOneObject\r\n0x00007FF9A5365373      PyRun_InteractiveLoopFlags\r\n0x00007FF9A53651B1      PyRun_AnyFileExFlags\r\n0x00007FF9A52F917D      Py_FatalError\r\n0x00007FF9A526428F      Py_RunMain\r\n0x00007FF9A5264141      Py_RunMain\r\n0x00007FF9A5264126      Py_Main\r\n0x00007FF9A52640DD      Py_Main\r\n0x00007FF7C5F41268      (unknown)\r\n0x00007FF9F1B57BD4      BaseThreadInitThunk\r\n0x00007FF9F324CE51      RtlUserThreadStart\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\qulab\\Anaconda3\\envs\\tf2-3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1009, in __repr__\r\n    self.shape, self.dtype.name, numpy_text(self, is_repr=True))\r\n  File \"C:\\Users\\qulab\\Anaconda3\\envs\\tf2-3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 225, in numpy_text\r\n    text = repr(tensor._numpy()) if is_repr else str(tensor._numpy())\r\n  File \"C:\\Users\\qulab\\Anaconda3\\envs\\tf2-3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1031, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed\r\n```", "comments": ["I can reproduce and getting the same error with RTX 2070 SUPER, Windows 10, CUDA 10.1, python 3.8.5, TF 2.3.0.\r\n\r\nAlso confirming that it works with Ubuntu env (same python and TF version and CUDA 10.1).\r\n\r\nWhat's interesting is that it's crashing when trying to print the variable (like the python console does by default).\r\n\r\nThe following does not crash/prints \"DONE\" (but still hangs, does not exit...):\r\n```\r\npython -c \"import tensorflow as tf; print('00000'); a = tf.constant([[1, 0], [0, 1]], dtype=tf.complex64); print('AAAAA'); tf.linalg.matmul(a, a); print('BBBBB'); tf.linalg.matmul(a, a, adjoint_a=True); print('DONE')\"\r\n``` \r\nbut this crashes with `GPU sync failed`:\r\n```\r\npython -c \"import tensorflow as tf; print('00000'); a = tf.constant([[1, 0], [0, 1]], dtype=tf.complex64); print('AAAAA'); print(tf.linalg.matmul(a, a)); print('BBBBB'); print(tf.linalg.matmul(a, a, adjoint_a=True)); print('DONE')\"\r\n```", "I tried now with tf_nightly-2.4.0.dev20200915 and there it works fine. Requires CUDA 11.0 though. With Windows multiple CUDA installations is fine, just have to adjust the env path variable to point to the required CUDA version dir. GPU drivers can be the latest, they are also backwards compatible to CUDA 10.x.\r\n\r\nFor a temporary setup:\r\n```\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\r\nSET CUDA_TOOLKIT_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\libnvvp;%PATH%\r\n```", "@liuhenry \r\nI ran your code on tf 2.3 and nightly and do not face the error reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/68d0865435a9a2a101f70594094c814b/untitled416.ipynb).\r\nPlease refer to below issues for the error log shared above:\r\n[link](https://stackoverflow.com/questions/51112126/gpu-sync-failed-while-using-tensorflow), [link2](https://github.com/tensorflow/tensorflow/issues/1450) [try to reduce gpu memory], #37942", "@ahtik Thanks for the note! I will install CUDA 11 later and try that out.\r\n\r\n@Saduf2019 Thanks for your response as well! Do you mean that you tested this on Colab or did it also work on a Windows machine? The issue will not reproduce in Colab since it seems to specifically be related to the interaction of TF and CUDA on windows, but is working fine in linux. RE: memory limitations, I've tried with enabling memory growth, but I don't think this is relevant given that the problem shows up on a single 2x2 matrix.", "@liuhenry Is this still an issue? I don't have access to GPU on Windows. But, i tested on cpu and kernel didn't crash. Please test it on GPU and let us know whether this still an issue for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43296, "title": "Document possible values of the nnapi_execution_preference option", "body": "", "comments": []}, {"number": 43295, "title": "Arguments to max_pool are missing correspondance", "body": "https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/ops/nn_ops.py#L3820\r\n\r\nI have a tensor with shape (N,W,C). When I want to do max_pool1d with stride 2 for W, and then the strides with shape [1,2,1] throws an error and it indicates that the W dimension in the strides parameter is on the first position, so the strides parameter in this case must be [2,1,1]. This should be either changed so that it reflects the shape (N,W,C), or at  least described in the documentation.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport unittest\r\n\r\nclass TestMethods(unittest.TestCase):\r\n\r\n    def test_max_pool(self):\r\n        num_classes = 10\r\n        print('num_classes: ', num_classes)\r\n        batch_size = 2\r\n        y_output = tf.constant(value=[[.1, .3, .2, .0, .8, .4, .1, .1, .1, .2],\r\n                                      [.1, .9, .2, .0, .1, .4, .1, .1, .1, .2]])\r\n        input = tf.reshape(tensor=y_output, shape=[batch_size, num_classes, 1])\r\n        print('input shape: ', input.shape)\r\n        y_max = tf.nn.max_pool1d(input=input, ksize=[num_classes, 1, 1], strides=[1, 1, 1], padding='VALID', data_format='NWC')\r\n        print('y_max: ', y_max)\r\n        y_max = tf.reshape(tensor=y_max, shape=[batch_size])\r\n        print('y_max: ', y_max)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.compat.v1.enable_eager_execution()\r\n    unittest.main()\r\n```\r\n\r\nResults:\r\n```\r\nTesting started ...\r\n\r\nProcess finished with exit code 0\r\nnum_classes:  10\r\ninput shape:  (2, 10, 1)\r\n\r\nRan 1 test in 0.031s\r\n\r\nOK\r\ny_max:  tf.Tensor(\r\n[[[0.8]]\r\n\r\n [[0.9]]], shape=(2, 1, 1), dtype=float32)\r\ny_max:  tf.Tensor([0.8 0.9], shape=(2,), dtype=float32)\r\n```", "comments": ["@adam-dziedzic \r\n\r\nI tried reproducing the issue in colab with TF version 2.3,nightly version and i am seeing the error (`AttributeError: module '__main__' has no attribute '/root/'`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/4d746b19e567397f62880b731df7b689/untitled380.ipynb).Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]