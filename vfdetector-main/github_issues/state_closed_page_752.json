[{"number": 31005, "title": "\"Not found: Resource does not exist\" exception thrown in runtime", "body": "I am facing a similar error mentioned above. I will try my best to help resolve this issue as it benefits my work as well. It is the same problme with issue #22631, \r\n\r\nOS Platform and Distribution: Linux Ubuntu x86_64 - 4.15.0-52-generic (kernel)\r\nTensorFlow installed from: conda 4.7.5\r\nTensorFlow version: 1.13.1\r\nBazel version: N/A\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: Tesla V100-SXM2-16GB\r\nExact command to reproduce:\r\nMobile device: N/A\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\ndef discriminative_loss(y_true, y_pred):\r\n    \"\"\"Computes loss for a batch of images\r\n    Args:\r\n        y_true: (n, h, w) where each elements contains the ground truth instance id\r\n        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch\r\n    Returns:\r\n        loss\r\n    \"\"\"\r\n    # Compute the loss for each image in the batch\r\n    def compute_loss(input):\r\n        prediction = input[1]\r\n        label = input[0]\r\n\r\n        # Number of clusters in ground truth\r\n        clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n        # Compute cluster means and variances for each cluster\r\n        def compute_mean(c):\r\n            mask = tf.equal(label[:,:,0], c)\r\n            masked_pixels = tf.boolean_mask(prediction, mask)\r\n            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n            return cluster_mean\r\n\r\n        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n        return tf.reduce_mean(cluster_means)\r\n\r\n    # We want to know the loss for each image in the batch\r\n    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))\r\n    return losses\r\n\r\ndef discriminative_loss_working(y_true, y_pred):\r\n    # Compute the loss for only the first image in the batch\r\n\r\n    prediction = y_pred[0]\r\n    label = y_true[0]\r\n\r\n    # Number of clusters in ground truth\r\n    clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n    # Compute cluster means and variances for each cluster\r\n    def compute_mean(c):\r\n        mask = tf.equal(label[:,:,0], c)\r\n        masked_pixels = tf.boolean_mask(prediction, mask)\r\n        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n        return cluster_mean\r\n\r\n    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n    return tf.reduce_mean(cluster_means)\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, input_shape):\r\n        super(MyModel, self).__init__()\r\n        self.conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1,1))\r\n\r\n    def call(self, input):\r\n        return self.conv(input)\r\n\r\ninput_shape = (1,128,128,3)\r\ndef my_gen():\r\n    while True:\r\n        x = np.random.rand(1,input_shape[1], input_shape[2],3)\r\n        y = np.random.randint(11000, 11015, (input_shape[1], input_shape[2],1))\r\n        yield x,y\r\n\r\ntrain_dataset = tf.data.Dataset.from_generator(\r\n                    my_gen,\r\n                    (tf.float32, tf.float32),\r\n                    (tf.TensorShape([1,128,128,3]),\r\n                     tf.TensorShape([128,128,1])))\r\ntrain_dataset = train_dataset.batch(1)\r\ntrain_dataset = train_dataset.repeat()\r\n\r\nmodel = MyModel(input_shape=input_shape)\r\n\r\n# This is a fix to make loading weights possible\r\n# x = tf.zeros((1,) + input_shape)\r\nx = tf.zeros(input_shape)\r\ny = model(x)\r\n\r\nwith tf.Session(config=config):\r\n    optimizer = tf.keras.optimizers.SGD(lr=0.0001)\r\n    model.compile(loss=discriminative_loss,optimizer=optimizer)\r\n    model.fit(train_dataset, epochs=5, steps_per_epoch=2)\r\n```\r\n[tf_error.log](https://github.com/tensorflow/tensorflow/files/3415129/tf_error.log)\r\n", "comments": ["Does this reproduce on tf nightly, or 1.14?", "@lengoanhcat This is not an issue in tf-nighlty version. Would you like to try. Thanks!", "@alextp: it only tried on a stable 1.14. As @gadagashwini said it would not happen with tf-nightly version. I will try and report back the result.\r\n\r\nAfter testing the code on Google Colab with GPU and tf-nightly-gpu, I can confirm that the error does happen with tf-nightly-gpu. It means that elements of graphs inside nested map_fn have not been assigned correctly by Tensorflow Nightly build (v1.15 at the moment of testing)\r\n\r\nhttps://colab.research.google.com/drive/1PS8PwZIlUawv9OPUbNg_lKlfxQhR_Qvn\r\n\r\nPlease have a look at this colab notebook when I run the code with tf-nightly-gpu.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31005\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31005\">No</a>\n"]}, {"number": 31004, "title": "tf.tensordot documentation describes non-existent `a_axes` and `b_axes` parameters", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensordot\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere are no `a_axes` and `b_axes` parameters, but the documentation describes the function as if there are.\r\n", "comments": ["@bashi-bazouk \r\nIn the function tf.tensordot( a,    b,    axes,    name=None)\r\naxes parameter means a-axis , b-axis parameters.\r\nFor eg: `tf.tensordot(a, b, [[0], [2]])` means `a-axis param =[0]` and `b-axis param=[2].`\r\n\r\nFor more details refer [this link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensordot#args).\r\n\r\nThanks!", "Maybe you should make the documentation say that instead of the github issue.", "@bashi-bazouk \r\n\r\nThe above information is available in the [documentation.](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensordot#args).Please, let me know if you have any questions.Thanks!", "@ravikyram There is no sentence in the documentation that says that axes parameter decomposes into a_axes and b_axes. The documentation simply starts referring to these arguments, as if they are self-evident. However, they're not, and the lack of that documentation is the reason that I created this issue.\r\n\r\nThe link you provided points to the exact page that I said is lacking this documentation.\r\n\r\n"]}, {"number": 31003, "title": " Tensor2tensor acceleration ops", "body": "This pull request:\r\n\r\n- Instantiates a number of previously missed CPU bfloat16 ops\r\n\r\n-  Creates a new contrib project t2t with two CUDA-optimized ops for use by tensor2tensor. These ops accelerate GPU training of tensor2tensor transformer models by ~10% using FP32 and up to 20% using FP16.\r\n", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac", "@ekuznetsov139 Please make this PR against master and if you also want the changes in the 1.14 branch you can make a cherry-pick against it. The window for cherry-picks for 1.14.1 patch release is just until Wednesday.", "So I did (PR #31381). Not that it did much good."]}, {"number": 31002, "title": "Add support for TensorRT precision mode in lowercase", "body": "- The precision mode in lowercase was supported in TF1.13 and was dropped in TF1.14.\r\n- This PR adds the support back.\r\n- The plan is to cherry-pick this into TF1.14.1", "comments": []}, {"number": 31001, "title": "[ROCm] Adding support to rnn ops", "body": "Add ROCm support to core/kernels/rnn/*. The code change is trivial, please review and merge. @whchung @chsigg \r\n\r\n------\r\nTest performed:\r\n\r\n//tensorflow/contrib/rnn:lstm_ops_test\r\n//tensorflow/contrib/rnn:gru_ops_test\r\n//tensorflow/contrib/rnn:fused_rnn_cell_test\r\n//tensorflow/contrib/rnn:rnn_test", "comments": ["@chsigg In the most recent commit, I manually updated the bazel build file's formatting so that buildifier does not complain. This is to address the `Ubuntu Sanity` build's complaint in `//tensorflow/tools/ci_build:gen_ci_sanity_out` target.\r\n\r\nI wonder if there's a way to build this target locally? Since there doesn't seem to be a Build file that specifies `gen_ci_sanity_out`, I have to pull, build and run buildifier all manually on my changed files.", "@chsigg A gentle ping"]}, {"number": 31000, "title": "Tensor2tensor acceleration ops", "body": "This pull request:\r\n* Instantiates a number of previously missed CPU bfloat16 ops \r\n* Creates a new contrib project t2t with two CUDA-optimized ops for use by tensor2tensor. These ops accelerate GPU training of tensor2tensor transformer models by ~10% using FP32 and up to 20% using FP16.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31000) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 30999, "title": "r2.0 windows build ProtoCompile issue", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: source \r\n- TensorFlow version: 2.0\r\n- Python version: 3.6.5\r\n- Installed using: Visual studio\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2017\r\n- CUDA/cuDNN version: 10, 7.1\r\n- GPU model and memory: NVIDIA 1080\r\n\r\nBuilding under windows (following https://www.tensorflow.org/install/source_windows) I have an issue (or possibly 2) with compiling XXX.pb.h and XXX.pb.cc files. \r\n\r\n1) Get errors about certain ones not being build (see below)\r\n2) Bazel just sits there for hours with lines like :\r\n    ProtoCompile tensorflow/core/protobuf/autotuning.pb.h\r\n\r\nIt seems like different files each time but always these two issues. Killing, re-booting machine and starting build again sometimes gets you a bit further in terms of bazel actions completed.\r\n\r\nc:\\Software\\tensorflow>bazel build --config=opt --config=cuda --config=monolithic --config=v2 --define=no_tensorflow_py_deps=true //tensorflow:tensorflow.dll\r\nStarting local Bazel server and connecting to it...\r\n... still trying to connect to local Bazel server after 10 seconds ...\r\n... still trying to connect to local Bazel server after 20 seconds ...\r\n... still trying to connect to local Bazel server after 30 seconds ...\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda, monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe\r\nINFO: Reading rc options for 'build' from c:\\software\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from c:\\software\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/lib/site-packages --python_path=C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:cuda in file c:\\software\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\software\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:monolithic in file c:\\software\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:opt in file c:\\software\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\software\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\software\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:monolithic in file c:\\software\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:v2 in file c:\\software\\tensorflow\\.tf_configure.bazelrc: --define=tf_api_version=2\r\nINFO: Analyzed target //tensorflow:tensorflow.dll (122 packages loaded, 9152 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/derek/_bazel_derek/fesmau5k/sandbox\r\nSlow read: a 67402971-byte read from C:/users/derek/_bazel_derek/fesmau5k/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/sobol_direction_vectors.h took 13946 ms.\r\nINFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/output.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nINFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/extension.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nINFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/float_conversion.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nINFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/arg.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nINFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/bind.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nINFO: From Compiling external/com_google_absl/absl/strings/internal/str_format/parser.cc:\r\ncl : Command line warning D9025 : overriding '/w' with '/W3'\r\nSlow read: a 67402971-byte read from C:/users/derek/_bazel_derek/fesmau5k/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/sobol_direction_vectors.h took 23140 ms.\r\nSlow read: a 50122658-byte read from C:/users/derek/_bazel_derek/fesmau5k/execroot/org_tensorflow/bazel-out/x64_windows-opt/bin/external/protobuf_archive/libprotoc_lib.a took 8412 ms.\r\nINFO: From Linking external/grpc/libgrpc++_base.a:\r\nserver_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nrpc_method.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\ncreate_channel_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_properties.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nERROR: C:/software/tensorflow/tensorflow/core/BUILD:2743:1: output 'tensorflow/core/protobuf/queue_runner.pb.h' was not created\r\nERROR: C:/software/tensorflow/tensorflow/core/BUILD:2743:1: output 'tensorflow/core/protobuf/queue_runner.pb.cc' was not created\r\nERROR: C:/software/tensorflow/tensorflow/core/BUILD:2743:1: not all outputs were created or valid\r\n[3,342 / 5,505] 3 actions running\r\n    ProtoCompile tensorflow/core/protobuf/autotuning.pb.h; 11649s local\r\n    ProtoCompile tensorflow/core/profiler/op_profile.pb.h; 11648s local\r\n    ProtoCompile tensorflow/core/protobuf/debug.pb.h; 11648s local\r\n\r\n\r\n", "comments": ["@dmagee Can you tell us, Which protobuf version are you using? ", "I didn't install protobuf (https://www.tensorflow.org/install/source_windows doesn't mention installing protobuf, so assumed it came with the distribution. I fetched r2.0). Maybe that's the issue?", "Update: I installed all the python modules from the linked :\r\n\r\n\"The dependencies are listed in the setup.py file under REQUIRED_PACKAGES.\" \r\n[https://www.tensorflow.org/install/source_windows]\r\n\r\nPython now has protobuf 3.9.0. I'm still getting similar errors:\r\n\r\n....\r\nINFO: From Linking external/grpc/libgrpc++_base.a:\r\nserver_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nrpc_method.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\ncreate_channel_posix.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From Linking external/protobuf_archive/libprotobuf.a:\r\nerror_listener.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From ProtoCompile tensorflow/core/debug/debugger_event_metadata.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nERROR: C:/software/tensorflow/tensorflow/core/debug/BUILD:64:1: output 'tensorflow/core/debug/debugger_event_metadata.pb.h' was not created\r\nERROR: C:/software/tensorflow/tensorflow/core/debug/BUILD:64:1: output 'tensorflow/core/debug/debugger_event_metadata.pb.cc' was not created\r\nERROR: C:/software/tensorflow/tensorflow/core/debug/BUILD:64:1: not all outputs were created or valid\r\n[3,710 / 6,123] ProtoCompile tensorflow/core/profiler/tfprof_log.pb.h; 562s local\r\n\r\n\r\n", "Are you trying to build the 2.0 branch? What configuration options did you use?\r\nWhat exact hash are you building?\r\n\r\n@goldiegadde do we have a prebuilt binary for windows for 2.0 beta?", ">Are you trying to build the 2.0 branch? \r\nIt did \"git checkout r2.0\"\r\n\r\n>What configuration options did you use?\r\nBasically default, except specifying to use CUDA 10 (see below)\r\n\r\nc:\\Software\\tensorflow>python configure.py\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.0 installed.\r\nPlease specify the location of python. [Default is C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nNOTE: Many c/cpp files are built fine, it's just the ProtoCompile and XX.pb.h/XXX.pb.cc files that it is having issues with. \r\n\r\n> do we have a prebuilt binary for windows for 2.0 beta?\r\nI wasn't aware of prebuilt libraries/DLLs for use with the C++ API. I'm sure I read you had to build from source for this. I'd be very happy to hear otherwise!\r\n\r\nThanks \r\n", "I just tried this with the r1.14 version and am encountering the same issue(s).\r\n\r\nINFO: From Linking external/protobuf_archive/libprotobuf.a:\r\nerror_listener.o : warning LNK4221: This object file does not define any previously undefined public symbols, so it will not be used by any link operation that consumes this library\r\nINFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/conv_autotuning.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/worker.pb.h:\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/src: warning: directory does not exist.\r\nERROR: C:/software/tensorflow/tensorflow/core/BUILD:2280:1: output 'tensorflow/core/protobuf/worker.pb.h' was not created\r\nERROR: C:/software/tensorflow/tensorflow/core/BUILD:2280:1: output 'tensorflow/core/protobuf/worker.pb.cc' was not created\r\nERROR: C:/software/tensorflow/tensorflow/core/BUILD:2280:1: not all outputs were created or valid\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 866.684s, Critical Path: 88.69s\r\nINFO: 1136 processes: 1136 local.\r\nFAILED: Build did NOT complete successfully ", "Closing as there has been no activity on this issue and the 2.0 branch has been fast forwarded past the time the comments are.\r\n\r\nPlease open a new issue if the problem persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30999\">No</a>\n", "@dmagee did u find any solutions to this problem ? it still occurs while building r2.5 from source", "Please open a new issue, filling in the bug template as most likely the environment has changed in the last 2 years.", "@mihaimaruseac i have opened a new issue , thanks"]}, {"number": 30998, "title": "Can't install tensorflow-text", "body": "\r\n", "comments": ["I can't install tensorflow-text via pip as it is shown in the tf2.0 tutorial pages [https://www.tensorflow.org/beta/tutorials/tensorflow_text/intro](url). I tried using -q or -U flag (as it is specified in tensorflow-text github page) but without any success. I made an environment in anaconda and installed tf2.0. The evironment has python 3.7.3 but I tried with other versions too. I checked pip version to be higher than 19.1 as the tutorial mentions it. Bellow is the error I get.\r\n\r\n![image](https://user-images.githubusercontent.com/44173617/61817722-db87ab00-ae57-11e9-979e-7d84be100cce.png)\r\n\r\nOn google colab I can install it (checking that there's nothing related to the package availability).", "I've done further researches and it appears that tensorflow-text is not supported on windows. I will close the issue as it has been resolved.", "@georgealexandruvlad Has the issues been solved ? Have they found a way to support `tensorflow-text` on windows?"]}, {"number": 30997, "title": "Do not force NHWC->NCHW conversion in backward pooling kernels ", "body": "We have efficient NHWC cuDNN backward pooling implementations after 7.3.0. So we don't need to force the NHWC->NCHW conversion before calling these kernels.\r\n\r\nfyi @nluehr ", "comments": []}, {"number": 30996, "title": "Propagate intra_op_parallelism_threads from SessionOptions to xla::LocalClientOptions", "body": "With CPU Tensorflow, when we set intra_op_parallelism_threads to 1 in the SessionOptions config, the XLA CPU backend still spawns a huge number of threads because xla::LocalClientOptions defaults this setting to -1, which causes the CPU backend to spawn one thread per core.\r\n\r\nFix is to propagate this setting from the SessionOptions into xla::LocalClientOptions when creating the xla::LocalClient.\r\n", "comments": []}, {"number": 30995, "title": "LSTM prediction is numerically inconsistent for the last few instances.", "body": "The predictions you get may differ slightly depending on input length and position within it.  E.g., if you have 11 instances of input, you get one answer for the first 8, and a different answer for the last 3.\r\nI write \"may\" as it happens to me with probability around 0.4. \"Slightly\" means in the order of the least significant bits of the float32 mantissa.\r\n\r\n\r\n\r\n**System information**\r\n- Yes, I have written custom code, supplied below as a reprex in R using keras.\r\n- Tried on two platforms, with identical results.\r\nPlatform A:\r\n- Linux Ubuntu Ubuntu 16.04.5 LTS\r\n- TensorFlow version:\r\n      VERSION                        \"1.7.0\"               \r\n      GIT_VERSION                    \"v1.7.0-3-g024aecf414\"\r\n      COMPILER_VERSION               \"4.8.4\"               \r\n- Python version: 2.7.12\r\n\r\nPlatform B:\r\n- Linux Ubuntu 14.04.6 LTS\r\n- TensorFlow version: tried both\r\n      VERSION                        \"1.12.0\"\r\n      GIT_VERSION                    \"v1.12.0-0-ga6d8ffae09\"\r\n      COMPILER_VERSION               \"4.8.5\"                \r\n- Python version: 2.7.6\r\n\r\nboth:\r\n- TensorFlow installed from binary.\r\n- Not a mobile device.\r\n- CUDA/cuDNN version: Not used.\r\n- GPU model and memory:  Not used\r\n\r\n**Describe the current behavior**\r\nIf the first dimension of `x` is `n`, \"row\" `i` will get one value if `0 <= i < (n&-4)`, but a possibly different value for `(n&-4) <= i < n`. (These C++/Python style 0-based indices.  For R, 1-based, it's `0 < i <= bitwAnd(n, -4)` versus `bitwAnd(n, -4) < i <= n`.)\r\n\r\n**Describe the expected behavior**\r\nReproducible prediction from same input instance, independent of row number or input length.  I use generalized \"row\" for a slice of a tensor with a given fixed first index, e.g., `x[i,,]` or `pred[i,]`.\r\n\r\n**Code to reproduce the issue**\r\nThis is a reprex written in R.  I'd be happy to port to other languages if that's preferable.\r\n``` r\r\noptions(digits=8)\r\nfake <- function(shape_) {                        # arbitrary but reproducible\r\n   array(seq_len(prod(shape_)) %% 2.71 - 1.04, shape_)\r\n}\r\n\r\nlibrary(keras)\r\nshape <- c(30,5)\r\nmodel <- keras_model_sequential() %>%\r\n   layer_lstm(units=2, input_shape=shape) %>%\r\n   set_weights(list(fake(c(5, 8)), fake(c(2, 8)), fake(8)))\r\n\r\nn <- 11                                           # not a multiple of 4\r\nx <- array(rep(fake(shape), each=n), c(n, shape)) # n copies of identical input\r\np <- model %>% predict(x)                         # all predictions should match\r\np                                                 # but last n%%4 rows differ\r\n#>             [,1]        [,2]\r\n#>  [1,] 0.46561426 -0.22865930\r\n#>  [2,] 0.46561426 -0.22865930\r\n#>  [3,] 0.46561426 -0.22865930\r\n#>  [4,] 0.46561426 -0.22865930\r\n#>  [5,] 0.46561426 -0.22865930\r\n#>  [6,] 0.46561426 -0.22865930\r\n#>  [7,] 0.46561426 -0.22865930\r\n#>  [8,] 0.46561426 -0.22865930\r\n#>  [9,] 0.46561423 -0.22865926\r\n#> [10,] 0.46561423 -0.22865926\r\n#> [11,] 0.46561423 -0.22865926\r\n(t(p)-p[1,]) * 2**26                              # the difference is low bits\r\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\r\n#> [1,]    0    0    0    0    0    0    0    0   -2    -2    -2\r\n#> [2,]    0    0    0    0    0    0    0    0    3     3     3\r\nstopifnot(t(p)==p[1,])                            # all *should* be equal\r\n#> Error in eval(expr, envir, enclos): t(p) == p[1, ] are not all TRUE\r\n\r\n##in contrast...\r\nx12 <- array(rep(fake(shape), each=12), c(12, shape))\r\np12 <- model %>% predict(x12)\r\nstopifnot(t(p12)==p12[1,])                         # ...all is well for n == 12\r\n```\r\n\r\n<sup>Created on 2019-07-25 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1.9000)</sup>\r\n\r\n**Other info / logs**", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "This is a **bug** report.  (It is not about build, installation, or performance.  It is about correctness.)\r\n\r\nI provided a small reproducible example illustrating a bug.  (In our real code, we trained a bigger model on a training set of thousands of instances, but then found that the trained model behaved oddly.)  The same input should give the same response.  And it does, except if the length of the input isn't divisible by 4; then the remaining 1 to 3 instances differ.  To make this obvious, I repeated the same input 11 times in my reprex.\r\n\r\nThis bug in keras or tensorflow also manifests like this when applying to time series:   If predicting n days, the first n-1 predictions should match what you get if you had 1 less data of data and were generating just n-1 predictions.  But the prediction on the historical data does change!", "@Quiigi Can you provide a standalone code in python to reproduce the issue? Thanks! ", "After looking at python basics, I rewrote my R example above in python:\r\n``` python\r\nimport numpy as np\r\nfrom keras.models import Sequential\r\nfrom keras.layers import LSTM\r\n\r\ndef fake(shape_):                                # arbitrary but reproducible\r\n    f = np.reshape(range(np.prod(shape_)), shape_, order=\"F\") + 1\r\n    return f % 2.71 - 1.04\r\n\r\nshape = (30,5)\r\nmodel = Sequential()\r\nmodel.add(LSTM(units=2, input_shape=shape))\r\nmodel.set_weights([fake((5, 8)), fake((2, 8)), fake(8)])\r\n\r\nfor n in [8, 7]:\r\n    print(\"\\nn = \" + str(n))\r\n    x= np.broadcast_to(fake(shape), (n,)+shape) # n copies of identical input\r\n    p = model.predict(x)                        # all predictions should match\r\n    p == p[1]                                   # but last n%4 rows differ\r\n    (p-p[1]) * 2**26                            # the difference is low bits\r\n    assert (p == p[1]).all()                    # fails iff n%4 > 0\r\n```\r\n\r\nI also do two experiments.  For n=8, a multiple of 4, my check passes; for 7 it fails.  Here's my output:\r\n```\r\nn == 8\r\narray([[ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [ True,  True]])\r\narray([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]])\r\n\r\nn == 7\r\narray([[ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [ True,  True],\r\n       [False, False],\r\n       [False, False],\r\n       [False, False]])\r\narray([[ 0.,  0.],\r\n       [ 0.,  0.],\r\n       [ 0.,  0.],\r\n       [ 0.,  0.],\r\n       [-2.,  3.],\r\n       [-2.,  3.],\r\n       [-2.,  3.]])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 7, in <module>\r\nAssertionError\r\n```", "Thanks for reporting the issue. Let me take a look.", "@qlzh727 What did you find?", "Sorry for the late reply. \r\n\r\nI was able to reproduce the issue, and I think it is somehow happening when batch_size that is not a perfect 2^n number. I can see the value difference between batch 0-3 and 4-6. If I change the batch size to 9, then the difference is between 0-7 and 8, same for batch size 17.\r\n\r\nThe cause of this might be numerical instability for numerical libraries. Also, given the fact that the diff is so small, this is usually ignored in the unit test (the default atol and rtol for numpy assert allclose() is 1e-6). In fact if I change assert in the code to np.allclose(), the issue goes away.\r\n\r\nCould you give more details about why this issue is concerning you in your application, and what's the specific problem it causes?", "We are predicting financial time series.  In this application, snooping future \r\ndata is an insidious problem: critical to avoid, but at times subtle and hard \r\nto notice.  It's even possible that it is avoided in research, only to creep in\r\nin production use.  For example, you need to use info on FX rates to get\r\ninformation into a common currency.  The live data source for FX rates\r\nmight \"snap\" the prices at a different time each day than your research\r\ndata source and inadvertently \"cheat\".\r\n\r\nWe have found a \"snoop test\" to be a useful tool: every day, we generate\r\npredictions not just for \"tomorrow\", but also for the past few days.  Then\r\na live check can compare predictions on the overlapped days (all but the\r\nlast day).  Specifically, when predicting n days, the first n-1 predictions\r\nshould match what you get if you had 1 less day of data and were generating\r\njust n-1 predictions.  This property is useful in testing and checking to\r\nverify it catch bugs where a backtest accidentally snoops into future.  If\r\non a new day your prediction changes for an old day, regardless of how\r\nsmall the change, the cause might be snooped data, that was not available\r\nback at the original date.  But keras/tensorflow has a bug causing the final \r\n1, 2or 3 predictions to change as you stack on additional data, even though \r\nno change is made to the historical data!\r\n\r\nThe numerical difference is very small, on the order of floating point\r\nnoise, and ordinarily completely insignificant.  But it might be very\r\nsignificant in terms of tending to falsely inflating your (small) edge in\r\naccurately predicting financial time series. Also: if you snoop the future\r\ndata, the error you get is also very small.  Finally, this issue makes it\r\nhard to to use our \"snoop test\" idea because it introduces many false\r\nalarms, that are generally pretty indistinguishable from the real thing.\r\n\r\nIdeally, the fix should be in the library.  The way \"multiple of 4\" gets\r\ninto it makes us think it is some sort of batch optimization that is flawed\r\nin some way.  If there was a clean fix at that level, it would be ideal.\r\n(It doesn't have to be a perfect 2^n number; any multiple of 4 is immune,\r\ne.g., 12 in my original test case.  We use the default batch size, 32.)\r\n\r\nWe have a workaround: we tack on additional 0, 1, 2, or 3 irrelevant data\r\nelements when predicting, to force the length to always be multiple of 4.\r\nThen we discard as many from the prediction.  Possibly this sort of fix\r\ncould be pushed into keras, wrapped around the predict code, but hopefully\r\nthere is a more elegant solution.\r\n", "Thanks for the detailed explanation.\r\n\r\nAfter some debug, the lowest op I can track that cause the difference was the \"recurrent_activation\" function (sigmoid), where the inputs with same value will produce slight different result. The underlying implementation of sigmoid for CPU goes to Eigen, which I don't have any knowledge.\r\n\r\nAdding @rmlarsen who is the expert for Eigen in TF team for this issue.", "Thank you for the update!\r\n\r\nTrying to replicate your debugging, I wasn't able to find a node called \"recurrent_activation\" or \"sigmoid\" in my tensorflow graph.  The closest I see is \"Tanh\", and its output (Tanh:0) shows the tiny discrepancies at the end.  I see the issue the nodes feeding it directly, \"add_5\", and idirectly, MatMul_6, BiasAdd_2, MatMul_2.  Notably, the inputs to the latter (Enter and TensorArrayReadV3) are clean.  So I would guess the difference starts around there.", "The recurrent_activation I am talking about is at https://github.com/tensorflow/tensorflow/blob/3a9580e2c7ed4473eb63438c6f408a49e87f417d/tensorflow/python/keras/layers/recurrent_v2.py#L109. I guess for any of the non-linear function, they might have tiny discrepancy, and really depends on the implementation.", "I have `keras.__version__` '2.2.4', and there's no \"recurrent_v2.py\".\r\nI see recurrent activation at https://github.com/tensorflow/tensorflow/blob/3a9580e2c7ed4473eb63438c6f408a49e87f417d/tensorflow/python/keras/layers/recurrent.py#L2103\r\n\r\n(And activation is 'tanh', resulting presumably in the node from where I was able to trace back the mod 4 issue to **matrix multiplication**).\r\n\r\n", "In my case, the inconsistency originates in MatMul.  It's possible that activation functions 'sigmoid' or 'hard_sigmoid' have a similar issue.  This reduced example demonstrates the issue:\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf\r\na = np.broadcast_to(np.float32([.6, -.8, -.3, 0]), (5,4))\r\nb = np.float32([[8, 1], [6, 4], [-9, 1], [0, 0]])\r\ntf.matmul(a,b).eval(session=tf.Session())\r\n```\r\nMy output:\r\n```\r\narray([[ 2.6999998, -2.9      ],\r\n       [ 2.6999998, -2.9      ],\r\n       [ 2.6999998, -2.9      ],\r\n       [ 2.6999998, -2.9      ],\r\n       [ 2.7      , -2.8999999]], dtype=float32)\r\n```\r\nThis is an improvement because LSTM generates a graph with 268 nodes, including While loops.  The above reproducible example has just 1 simple node.  I'm not sure where exactly the discrepancy creeps in; maybe in the function `evalGemm`, i.e.,\r\n```\r\n  void Eigen::TensorContractionEvaluatorBase<\r\n    Eigen::TensorEvaluator<\r\n      Eigen::TensorContractionOp<\r\n        Eigen::array<Eigen::IndexPair<long>, 1ul> const,\r\n        Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const,\r\n        Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const\r\n        > const,\r\n      Eigen::ThreadPoolDevice> >\r\n  ::evalGemm<true, true, false, 0>(float*) const\r\n```\r\nbut that's just a guess.\r\n\r\n", "Due the numerical instability, I don't think there is anything we can address here (the diff is smaller than the normal limit when we do the tests), I am going to close this bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30995\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30995\">No</a>\n", "I had hoped to learn where exactly the instability arises.\r\n\r\nI assume we see an artifact of optimization, trading accuracy for speed.  And as the error is within your normal tolerance, the result we see must be deemed \"correct\".  Therefore, closing this issue is appropriate."]}, {"number": 30994, "title": "[TF 2.0 API Docs] tf.math.sqrt", "body": "Documentation contributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\nTensorFlow.v.2.0\r\nLink to doc : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/sqrt\r\n\r\n## Doc issue description:\r\n\r\n**1.** The generated file in which this symbol is defined .i.e **python/ops/gen_math_ops.py** is just plain text rather than a link to the source file. It would be great if it's rightly linked to the source file to enable change proposals.\r\n\r\n**2.** There isn't a clear distinction between the choice of usage of either **tf.sqrt** or **tf.math.sqrt** and the implications (may be performance wise) of choosing one over the other.\r\n\r\n**3.** The usage example isn't a complete code sample but largely syntax-like. One of the parameters the function takes is represented by a placeholder symbol rather than a real valued tensor.\r\n\r\n**4.** There should also be a mention of the at-least the common errors that may arise as a result of incorrect usage of this function.\r\n", "comments": ["If needed I could work on this if is ok with you @dynamicwebpaige . Reading now this https://www.tensorflow.org/community/contribute/docs.\r\n\r\nCan you point me to a guideline on how to update such files? Am I correct that I need to update the source file docstring? Thanks in advance.", "@michelucci have you achieved any luck on this? I don't seem to locate the ```gen_math_ops.py``` file. I only see ```math_ops``` in the ```ops``` directory and in the ```math_ops```, imports from ```gen_math_ops.py``` are made !!", "Let me check what the status is. I am in London for a conference until Sunday, so I will have limited time. Will update this issue as soon as I can.", "I got the file where its defined..no more queries."]}, {"number": 30993, "title": "Missing shuffle argument on validation call during training", "body": "On TensorFlow 1.14 (OS Ubuntu 16.04), when I call fit() of a tf.Keras model using HDF5Matrix on both training and validation data, I set the argument shuffle='batch' and it works for training batches, however it fails when starting validation batches:\r\n\r\nTypeError: TypeError while preparing batch. If using HDF5 input data, pass shuffle=\"batch\".\r\n\r\n  The problem seems to be the shuffle argument missing on the validation call during training:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_arrays.py#L424\r\n\r\n  It seems the recursive call of model_iteration() does not set the argument from parent training call into the validation call.  Simply \"shuffle=shuffle\" in argument list should fix the issue.\r\n\r\nBest,\r\n  Andre.\r\n", "comments": ["@andmax Will it be possible to provide the minimal code to reproduce the reported issue. Thanks!", "Yes, based on keras / cifar10 example:\r\n\r\n```\r\nimport h5py\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.utils import to_categorical, HDF5Matrix\r\n\r\n(Xtr, Ytr), (Xva, Yva) = tf.keras.datasets.cifar10.load_data()\r\nXtr, Ytr, Xva, Yva, nc = Xtr[:1000], Ytr[:1000], Xva[:100], Yva[:100], 10\r\nXtr, Xva = Xtr.astype('float32') / 255, Xva.astype('float32') / 255\r\nYtr, Yva, ins = to_categorical(Ytr, nc), to_categorical(Yva, nc), Xtr.shape[1:]\r\nwith h5py.File('cifar10.hdf5', 'w') as h5f:\r\n    h5f['Xtr'], h5f['Ytr'], h5f['Xva'], h5f['Yva'] = Xtr, Ytr, Xva, Yva\r\ndel Xtr, Ytr, Xva, Yva\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3), input_shape=ins, activation='relu'))\r\nmodel.add(tf.keras.layers.Conv2D(8, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(tf.keras.layers.Dropout(0.25))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(32, activation='relu'))\r\nmodel.add(tf.keras.layers.Dropout(0.5))\r\nmodel.add(tf.keras.layers.Dense(nc, activation='softmax'))\r\nopt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\r\n\r\nXtr, Ytr = HDF5Matrix('cifar10.hdf5', 'Xtr'), HDF5Matrix('cifar10.hdf5', 'Ytr')\r\nXva, Yva = HDF5Matrix('cifar10.hdf5', 'Xva'), HDF5Matrix('cifar10.hdf5', 'Yva')\r\n\r\nmodel.fit(x=Xtr, y=Ytr, batch_size=32, epochs=100,\r\n          validation_data=(Xva, Yva), shuffle='batch')\r\n```", "I am bale to reproduce the reported issue with TF version 1.14.0 on Colab, find the gist [here](https://colab.research.google.com/drive/1UuloCJ4vLmILAVws7QxSI4N6IWxYJHCx). Thanks!", "This issue has *not* been fixed in TF 1.15 and 2.0.", "This issue is simple to resolve, just place shuffle=shuffle in model_iteration call for val_results in:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_arrays.py#L428\r\n\r\nIn this way, shuffle will not be default to True in validation call raising the TypeError about not being shuffle=batch, here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/engine/training_arrays.py#L379\r\n\r\n", "This is fixed with TF 2.1.0  Thanks!", "Close this issue as it's fixed in TF 2.1.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30993\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30993\">No</a>\n"]}, {"number": 30992, "title": "INT8 TensorRT Quantization Fails to Calibrate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, [nvcr.io/nvidia/tensorrt:19.02-py3](https://docs.nvidia.com/deeplearning/sdk/tensorrt-container-release-notes/rel_19-02.html#rel_19-02)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-6532-g9aaf74d733 1.15.0-dev20190718\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.2 as per the above linked container\r\n- GPU model and memory: Tesla V100 32GB\r\n\r\nRelevant output from `pip freeze`:\r\n```\r\ntf-estimator-nightly==1.14.0.dev2019072201\r\ntf-nightly-gpu==1.15.0.dev20190718\r\n```\r\n\r\nI am trying to quantize a Tensorflow frozen model to FP32, FP16, and INT8 using [this](https://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py) script and a few helper functions from [here](https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/graph_utils.py). \r\n\r\n**Describe the current behavior**\r\nThe current behavior is that the graph first gets converted to a TRT graph with TRTEngineOps, but then when it gets calibrated for the INT8 quantization, an error occurs. Logs follow:\r\n\r\n```\r\n2019-07-24 14:47:33.099857: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 9 ops of 6 different types in the graph that are not converted to TensorRT: ResizeNearestNeighbor, SplitV, $onv2D, ConcatV2, Placeholder, NoOp, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n2019-07-24 14:47:33.104169: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:731] Number of TensorRT candidate segments: 6\r\n2019-07-24 14:47:33.165775: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 82 nodes succeeded.\r\n2019-07-24 14:47:33.166041: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node TRTEngineOp_1 added for segment 1 consisting of 4 nodes succeeded.\r\n2019-07-24 14:47:33.166100: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node TRTEngineOp_2 added for segment 2 consisting of 4 nodes succeeded.\r\n2019-07-24 14:47:33.172398: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node detector/tiny-yolo/TRTEngineOp_3 added for segment 3 consisting of 54 nodes succeeded.\r\n2019-07-24 14:47:33.174729: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node detector/tiny-yolo/TRTEngineOp_4 added for segment 4 consisting of 25 nodes succeeded.\r\n2019-07-24 14:47:33.174886: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:832] TensorRT node detector/tiny-yolo/TRTEngineOp_5 added for segment 5 consisting of 29 nodes succeeded.\r\n2019-07-24 14:47:33.253907: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: tf_graph\r\n2019-07-24 14:47:33.253947: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 171 nodes (-129), 186 edges (-130), time = 146.881ms.\r\n2019-07-24 14:47:33.253954: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   layout: Graph size after: 223 nodes (52), 238 edges (52), time = 53.403ms.\r\n2019-07-24 14:47:33.253960: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 223 nodes (0), 238 edges (0), time = 40.922ms.\r\n2019-07-24 14:47:33.253966: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   TensorRTOptimizer: Graph size after: 31 nodes (-192), 40 edges (-198), time = 171.468ms.\r\n2019-07-24 14:47:33.253986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 23 nodes (-8), 36 edges (-4), time = 58.458ms.\r\n...\r\n2019-07-24 14:47:45.089947: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f1924019480\r\n2019-07-24 14:47:45.090050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5\r\n2019-07-24 14:47:45.091103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5\r\n2019-07-24 14:47:45.098144: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:45.098663: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:45.098767: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:45.098842: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:45.099041: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:45.099740: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:45.108558: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:49.380842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-24 14:47:50.417645: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f1970006de0\r\n2019-07-24 14:47:50.426935: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:50.427977: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:50.438033: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:50.438830: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:50.439010: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:50.981266: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f1970061730\r\n2019-07-24 14:47:50.997300: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:50.997816: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.592716: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f196400b0b0\r\n2019-07-24 14:47:51.593407: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.593434: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.593455: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.593495: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.593507: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.593523: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.694882: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f19c4007080\r\n2019-07-24 14:47:51.695388: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:47:51.747525: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:740] Starting calibration thread on device 0, Calibration Resource @ 0x7f19c4014cf0\r\n2019-07-24 14:47:51.748014: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-07-24 14:51:46.016234: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource \r\n Calibrator = 0\r\n Builder    = 0\r\n Engine     = 0\r\n Logger     = 0x7f19240194f8\r\n Thread     = 0x7f192400cf80\r\n\r\n2019-07-24 14:51:53.253125: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource \r\n Calibrator = 0\r\n Builder    = 0\r\n Engine     = 0\r\n Logger     = 0x7f1970006e58\r\n Thread     = 0x7f197000a000\r\n\r\n2019-07-24 14:51:57.064220: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource \r\n Calibrator = 0\r\n Builder    = 0\r\n Engine     = 0\r\n Logger     = 0x7f19700617a8\r\n Thread     = 0x7f1970006f20\r\n\r\n2019-07-24 14:52:05.905501: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource \r\n Calibrator = 0\r\n Builder    = 0\r\n Engine     = 0\r\n Logger     = 0x7f196400b128\r\n Thread     = 0x7f1964007350\r\n\r\n2019-07-24 14:52:06.831720: I tensorflow/compiler/tf2tensorrt/utils/calibration_resource.cc:27] Destroying Calibration Resource \r\n Calibrator = 0\r\n Builder    = 0\r\n Engine     = 0\r\n Logger     = 0x7f19c4014d68\r\n Thread     = 0x7f19c4014c60\r\n\r\npure virtual method called\r\nterminate called without an active exception\r\nAborted (core dumped)\r\n```\r\n**Describe the expected behavior**\r\nThe expected behavior is for this error to not occur, and for the INT8 calibrated and quantized graph to be produced correctly.\r\n\r\n**Code to reproduce the issue**\r\nCommand executed:\r\n```\r\npython do.py \\\r\n--frozen_graph=tiny-yolov3_frozen.pb \\\r\n--image_file=/path/to/any/image/file \\\r\n--int8 \\\r\n--output_dir=/workspace \\\r\n--input_node=inputs --output_node=output_boxes\r\n```\r\n\r\nYou can find the `do.py` script and the `utilities.py` script referenced from it attached, and the `tiny-yolov3_frozen.pb` frozen model [here](https://drive.google.com/file/d/1Bgj9h6TJLwedtrhnritRs9eYm_iczG4v/view).\r\n\r\n[do.py.txt](https://github.com/tensorflow/tensorflow/files/3427406/do.py.txt)\r\n[utilities.py.txt](https://github.com/tensorflow/tensorflow/files/3427407/utilities.py.txt)\r\n", "comments": ["(issues with quantized model file sizes and inference speeds here) #30717", "@nseidl \r\n Did you check whether the issue persists with latest versions (TF1.14, or latest nightly builds) of TF? Thanks!", "Can confirm that simply upgrading to `tf-nightly-gpu==1.15.0.dev20190725` solved the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30992\">No</a>\n"]}, {"number": 30990, "title": "LSTM is not working with ModelCheckpoint callback", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhile training a model with `tf.keras.layers.LSTM` and having `tf.keras.callbacks.ModelCheckpoint` in callbacks, `model.fit` stops with an error message at end of last epoch, and no model weights is saved as `ModelCheckpoint` should do.\r\n\r\n**Describe the expected behavior**\r\n\r\n`model.fit` should train the model, and model weights should be saved in desired files.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere is an example which reproduces this error:\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.keras import layers\r\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.LSTM(units=64, input_shape=(28, 28), return_sequences=False))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nsample, sample_label = x_train[0], y_train[0]\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer='sgd',\r\n              metrics=[])\r\n\r\ncallback = ModelCheckpoint(filepath='saved/',\r\n                           monitor='val_loss',\r\n                           save_weights_only=False,\r\n                           mode='min', save_freq='epoch')\r\n\r\nmodel.fit(x_train, y_train,\r\n          validation_data=(x_test, y_test),\r\n          batch_size=64, epochs=2, callbacks=[callback])\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is error message after `model.fit` with the precedent code snippet:\r\n```\r\nW0724 14:56:18.298580 4508739008 deprecation.py:323] From /Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/2\r\n59904/60000 [============================>.] - ETA: 0s - loss: 2.2443W0724 14:57:10.469849 4508739008 saved_model.py:733] Skipping full serialization of object <tensorflow.python.keras.layers.recurrent.LSTM object at 0xb286c80b8>, because an error occurred while tracing layer functions. Error message: in converted code:\r\n    relative to /Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras:\r\n    saving/saved_model.py:1143 call_and_return_conditional_losses  *\r\n        return layer_call(inputs, training=training), layer.get_losses_for(inputs)\r\n    layers/recurrent.py:2533 call\r\n        inputs, mask=mask, training=training, initial_state=initial_state)\r\n    layers/recurrent.py:743 call\r\n        zero_output_for_mask=self.zero_output_for_mask)\r\n    backend.py:3806 rnn\r\n        input_time_zero, tuple(initial_states) + tuple(constants))\r\n    layers/recurrent.py:728 step\r\n        output, new_states = self.cell.call(inputs, states, **kwargs)\r\n    TypeError: wrapped_call() takes 1 positional argument but 2 were given\r\nW0724 14:57:10.520410 4508739008 saved_model.py:733] Skipping full serialization of object <tensorflow.python.keras.engine.sequential.Sequential object at 0x10f940518>, because an error occurred while tracing layer functions. Error message: in converted code:\r\n    relative to /Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras:\r\n    saving/saved_model.py:1143 call_and_return_conditional_losses  *\r\n        return layer_call(inputs, training=training), layer.get_losses_for(inputs)\r\n    engine/sequential.py:248 call\r\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n    engine/network.py:753 call\r\n        return self._run_internal_graph(inputs, training=training, mask=mask)\r\n    engine/network.py:895 _run_internal_graph\r\n        output_tensors = layer(computed_tensors, **kwargs)\r\n    layers/recurrent.py:619 __call__\r\n        return super(RNN, self).__call__(inputs, **kwargs)\r\n    engine/base_layer.py:667 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    layers/recurrent.py:2533 call\r\n        inputs, mask=mask, training=training, initial_state=initial_state)\r\n    layers/recurrent.py:743 call\r\n        zero_output_for_mask=self.zero_output_for_mask)\r\n    backend.py:3806 rnn\r\n        input_time_zero, tuple(initial_states) + tuple(constants))\r\n    layers/recurrent.py:728 step\r\n        output, new_states = self.cell.call(inputs, states, **kwargs)\r\n    TypeError: wrapped_call() takes 1 positional argument but 2 were given\r\n2019-07-24 14:57:10.531729: W tensorflow/python/util/util.cc:280] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n60000/60000 [==============================] - 51s 854us/sample - loss: 2.2442 - val_loss: 2.1145\r\nEpoch 2/2\r\n59968/60000 [============================>.] - ETA: 0s - loss: 1.9488Traceback (most recent call last):\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-d29c3be2bf34>\", line 1, in <module>\r\n    runfile('/Users/user/Desktop/tf2-rnn-callback-bugcheck/main.py', wdir='/Users/user/Desktop/tf2-rnn-callback-bugcheck')\r\n  File \"/Users/user/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/Users/user/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/user/Desktop/tf2-rnn-callback-bugcheck/main.py\", line 37, in <module>\r\n    batch_size=64, epochs=2, callbacks=[callback])\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 664, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 439, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 295, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 961, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1008, in _save_model\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1213, in save\r\n    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 106, in save_model\r\n    saved_model.save(model, filepath, overwrite, include_optimizer)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 1492, in save\r\n    save_lib.save(model, filepath)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 812, in save\r\n    checkpoint_graph_view)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_serialization.py\", line 65, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 139, in list_functions\r\n    self._serialization_cache)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2249, in _list_functions_for_serialization\r\n    fns = (saved_model.serialize_all_attributes(self, serialization_cache)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 723, in serialize_all_attributes\r\n    function_dict['_default_save_signature'] = _default_save_signature(layer)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 881, in _default_save_signature\r\n    fn.get_concrete_function()\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 681, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1360, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1648, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1541, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 716, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py\", line 139, in _wrapped_model\r\n    outputs_list = nest.flatten(model(inputs=inputs))\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 667, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 248, in call\r\n    return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 753, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 895, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 619, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 667, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2533, in call\r\n    inputs, mask=mask, training=training, initial_state=initial_state)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 743, in call\r\n    zero_output_for_mask=self.zero_output_for_mask)\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3806, in rnn\r\n    input_time_zero, tuple(initial_states) + tuple(constants))\r\n  File \"/Users/user/anaconda3/envs/tf2beta/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 728, in step\r\n    output, new_states = self.cell.call(inputs, states, **kwargs)\r\nTypeError: wrapped_call() takes 1 positional argument but 2 were given\r\n```\r\n\r\nThe same error occurs on a Linux machine with tf-nightly (`2.0.0-dev20190723`).\r\n\r\nThanks for help! ", "comments": ["I am experiencing a similar issue. \r\n--------------------------\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION) > 2.2.4-tf\r\n-----------------------------\r\nOS: Ubuntu\r\n Distributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.2 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n-------------------------------------\r\nrunning on CPUs\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              20\r\nOn-line CPU(s) list: 0-19\r\nThread(s) per core:  2\r\nIntel(R) Xeon(R) W-2155 CPU @ 3.30GHz\r\n------------------------------------\r\nData Generated using a class which inherits from Sequence\r\nfrom tensorflow.python.keras.utils import Sequence\r\n-----------------------------------------------------------------\r\nthis works fine:\r\nmodel.fit_generator(generator=gent, validation_data=genv, epochs=10, callbacks=[logger]) \r\n---------------------------------------------------------------\r\nthis causes an error:\r\ncallback = ModelCheckpoint(checkpoint_file, verbose=1)\r\nmodel.fit_generator(generator=gent, validation_data=genv, epochs=10, callbacks=[callback])\r\n--------------------------------------------------------\r\nError:\r\n Epoch 1/10\r\n7/8 [=========================>....] - ETA: 1s - loss: 1.5245 - acc: 0.2857\r\nEpoch 00001: saving model to /.../checkpoints/weights-improvement-01.hdf5\r\nTraceback (most recent call last):\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/model_base_baseline.py\", line 380, in <module>\r\n    model.fit_generator(generator=gent, validation_data=genv, epochs=10, callbacks=[callback]) #, use_multiprocessing=True, workers=20)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1433, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 331, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 311, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 969, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1018, in _save_model\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1211, in save\r\n    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 113, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 99, in save_model_to_hdf5\r\n    'config': model.get_config()\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 940, in get_config\r\n    layer_config = layer.get_config()\r\n  File \"/media/iwona/Optane/Project_BugLoc/DeepTracePy/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 919, in get_config\r\n    raise NotImplementedError\r\nNotImplementedError\r\n\r\nProcess finished with exit code 1\r\n\r\n----------------------------------------------------\r\nPlease help.\r\n[model_base_baseline.txt](https://github.com/tensorflow/tensorflow/files/3429426/model_base_baseline.txt)\r\n[nn_data_generator.txt](https://github.com/tensorflow/tensorflow/files/3429429/nn_data_generator.txt)\r\n\r\n\r\n\r\n", "I am able to reproduce the issue on Colab with Tensorflow version 2.0.0.beta1. Please take a look at [gist](https://colab.research.google.com/drive/1-WNfJK6G7ahlrTyouiStGNOhCFxVuK-R) of Colab. Thanks!", "I could reproduce the issue with `!pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190724`. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/5ee731bc2b38fc03604f66e964b9aff3/untitled323.ipynb). Thanks", "Thanks for reporting the issue, let me take a look.", "I wasn't be able to reproduce issue with latest nightly in tf-nightly-gpu-2.0-preview==2.0.0.dev20190731, somehow the issue was fixed recently. Can u have a try again? \r\n\r\nThanks.", "I am closing the issue. I can confirm that the issue wasn't reproducible with `tf-nightly-gpu-2.0-preview==2.0.0.dev20190731`. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/ba9fbfba12a51072cb59b0ee4b1e5962/untitled323.ipynb) for your reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30990\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30990\">No</a>\n"]}, {"number": 30989, "title": "tf.compat.v2.summary.scalar  has no attribute 'experimental'", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 2.7 \r\n- Bazel version (if compiling from source): Build label: 0.24.1\r\n- GPU model and memory:  CPU \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow.compat.v2.summary as b\r\nb.scalar\r\n```\r\n<function scalar at 0x7fbe6ed09050>\r\n```\r\nb.experimental\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'experimental'\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` \r\ntrain_log_dir = \"logs/scalars/\"\r\nself._train_summary_writer = tf.compat.v2.summary.create_file_writer(train_log_dir)\r\n def get_metrics(self,step):\r\n    self._step= step\r\n    loss=self.metrics.get(\"loss\", None)\r\n    accuracy = self.metrics.get(\"accuracy\", None)\r\n    with self._train_summary_writer.as_default():\r\n      tf.compat.v2.summary.scalar('loss', loss,step=self._step)\r\n      tf.compat.v2.summary.scalar('loss', accuracy,step=self._step )\r\n      self._train_summary_writer.flush()\r\n    return self.metrics\r\n```\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"/home/sendate/Desktop/TeamTrung/Bintu/Tensor/Agent.py\", line 152, in <module>\r\n    print('history dict:', agent.get_metrics(episode))\r\n  File \"/home/sendate/Desktop/TeamTrung/Bintu/Tensor/Agent.py\", line 83, in get_metrics\r\n    tf.compat.v2.summary.scalar('loss', loss,step=self._step)\r\n  File \"/home/sendate/Desktop/TeamTrung/Bintu/Tensor/venv/local/lib/python2.7/site-packages/tensorboard/plugins/scalar/summary_v2.py\", line 57, in scalar\r\n    getattr(tf.summary.experimental, 'summary_scope', None) or\r\n  File \"/home/sendate/Desktop/TeamTrung/Bintu/Tensor/venv/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation_wrapper.py\", line 104, in __getattr__\r\n    attr = getattr(self._dw_wrapped_module, name)\r\nAttributeError: 'module' object has no attribute 'experimental'\r\n", "comments": ["I am able to execute the code without any issues in 2.0.0-beta1. Please, find the screenshot.\r\n![experimental](https://user-images.githubusercontent.com/51902062/61863096-995c7900-aeec-11e9-8517-9f73fc71696d.png)\r\nIf that was not the expected output ,please share the full code snippet to reproduce the issue.\r\nThanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 30988, "title": "Relations/differences between Container and Scopes in Context Manager are not Clear", "body": "From respective docs\r\nhttps://www.tensorflow.org/api_docs/python/tf/container\r\nhttps://www.tensorflow.org/api_docs/python/tf/VariableScope\r\n\r\nThe relation between these two context manager is not clear (although variable scope and name scope inter-relation are fine). \r\n\r\nBut from the source code ones can see that they are clearly two things that inter-relates but not the same thing. \r\n\r\nShould i use Container only in the eager mode ? why not use only variable scope or name scope instead? also, maybe other questions will come as people uses low-level API to specialize in tensorflow dev.\r\n\r\nSo i think its an Issue.\r\n\r\nThankyou \r\n", "comments": ["The documentation for [tf.compat.v1.variable_scope](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) has been updated.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 30987, "title": "Delete the parameter allow_soft_placement in function AssignDevice", "body": "The parameter allow_soft_placement is useless in function AssignDevice", "comments": ["@leike666666 thanks for your contribution , can you please add some more description why  'allow_soft_placement' is not necessary ?", "> @leike666666 thanks for your contribution , can you please add some more description why 'allow_soft_placement' is not necessary ?\r\n\r\n@rthadur This parameter 'allow_soft_placement' is not used in function 'AssignDevice' implementation.", "I'm sorry but I'm not the right reviewer for this.  Try @mrry ?"]}, {"number": 30986, "title": "[XLA] Make ReplaceInstruction preserve the sharding info.", "body": "Right now ReplaceInstruction preserves the metadata if the user hasn't specified any on the new instruction. If we don't do this for the sharding information as well optimizations will drop the\r\nsharding information.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30986) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30986) for more info**.\n\n<!-- ok -->", "> I'm not sure if this is correct. IIUC sharding is semantically meaningful information that cannot be \"transparently\" copied in this way.\r\n> \r\n> @hyouklee for second opinion.\r\n\r\nThanks for getting back @sanjoy \r\n\r\nI'd agree that sharding is semantically meaningful. The issue we have currently is fairly critical because the sharding information as specified by user python code is being dropped by HLO optimization passes (specifically in this case algebraic simplifier). In other words the semantic meaning is lost altogether. The only other option I can see is for HLO passes (at least in algebraic simplifier) to be rewritten to preserve sharding information.\r\n\r\nI understand that we may not want this to add semantic meaning, however if an optimization writer wishes to add their own sharding information they can and this diff won't overwrite it. This will only add it in the case it has been left empty (either removing the information from the old instruction or adding the default sharding information to the new one would avoid this). \r\n\r\nI don't mind if this diff specifically isn't accepted but we will need some way of addressing this issue.\r\n", "We currently don't have a well defined policy whether the sharding is a guaranteed property of the operation or just a user hint. If what you want is the latter, it would probably be okay to propagate the sharding as long as shapes match between the old and new instruction. But this PR still wouldn't be enough to guarantee preserving the sharding annotation since optimization passes don't always use ReplaceInstruction() API.\r\n\r\nWe use kDomain instructions (with HloDomainIsolator and HloDomainRemover) to preserve sharding annotations on some backends, although it doesn't always work well (and could potentially prevent quite some compiler optimizations). You can see \"tensorflow/compiler/xla/service/hlo_domain_test.cc\" how it can be used.", "@hyouklee  Ok if it is not XLA policy to preserve sharding we may need to revisit a proper solution later. In the meantime could this PR still be approved? ReplaceInstruction guarantees the shapes are the same and even if it isn't enough to catch all cases it at least fixes the regressions we have reproducible cases for right now.", "For the TPU backend we do guarantee sharding using the domain mechanism I commented earlier. I was just saying that we don't have a global policy in XLA about sharding that all backends agreed on. Is it a not an option to use domains? If not, I would be fine to approve the PR.\r\n\r\n@sanjoy Do you have other concerns?", "No other concern, but please add a comment on `ReplaceInstruction` that makes this new behavior obvious.", "Thanks guys! @sanjoy, added a comment now. @hyouklee it's not that it is not an option to use domains but that would be a larger fix. We could revisit that later if more sharding issues crop up but for now this feels like a less intrusive fix for just the issues we have now."]}, {"number": 30985, "title": "Delete the parameter allow_soft_placement in function AssignDevice", "body": "This parameter is useless in the function", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30985) for more info**.\n\n<!-- need_author_cla -->", "@leike666666 Please sign CLA in order to proceed with next steps. Thank you!"]}, {"number": 30984, "title": "[tf2.0] tf.function and dataset perf issue", "body": "Hi everyone,\r\n\r\nI'm having a look at datasets and `tf.function` and It feels like I've stumble upon a strange perf issue.\r\nIs it a normal behaviour or am I doing something wrong?\r\n\r\n**System information**\r\n- Have I written custom code: `yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OSX`\r\n- TensorFlow installed from (source or binary): `binary - 2.0.0-beta1`\r\n- TensorFlow version (use command below): `v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1`\r\n- Python version: `3.6`\r\n\r\n**Describe the current behavior**\r\nThe tf.function decoration is said to improve eprformance over eager codes. Yet when using datasets the opposite happens on 3 different machines\r\n\r\n**Describe the expected behavior**\r\nThe tf.function decoration optimize my code even when using dataset\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport time\r\nimport multiprocessing\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nNB_LOOP = 100\r\nBATCH_SIZE = 256\r\nNB_EPOCHS = 100\r\nNB_SAMPLES = NB_EPOCHS * BATCH_SIZE\r\nDIM_SIZE = 256\r\n\r\n\r\nnp.random.seed(1)\r\ntf.random.set_seed(1)\r\n\r\n\r\ndef to_float32(x, y=None):\r\n    \"\"\"\r\n    Cast TF inputs as float32\r\n    \"\"\"\r\n    if y is None:\r\n        return tf.cast(x, tf.float32)\r\n\r\n    return tf.cast(x, tf.float32), tf.cast(y, tf.float32)\r\n\r\n\r\ndef bench(w, b, optim):\r\n    for i in tf.range(NB_EPOCHS):\r\n        a = tf.random.normal([BATCH_SIZE, DIM_SIZE])\r\n        with tf.GradientTape() as t:\r\n            out = tf.matmul(a, w) + b\r\n            out = tf.square(out)\r\n\r\n        grads = t.gradient(out, [w, b])\r\n        optim.apply_gradients(zip(grads, [w, b]))\r\n\r\n\r\n@tf.function\r\ndef bench_f(w, b, optim):\r\n    for i in tf.range(NB_EPOCHS):\r\n        a = tf.random.normal([BATCH_SIZE, DIM_SIZE])\r\n        with tf.GradientTape() as t:\r\n            out = tf.matmul(a, w) + b\r\n            out = tf.square(out)\r\n\r\n        grads = t.gradient(out, [w, b])\r\n        optim.apply_gradients(zip(grads, [w, b]))\r\n\r\n\r\ndef dataset_bench(p_data, w, b, optim):\r\n    for a in p_data:\r\n        with tf.GradientTape() as t:\r\n            out = tf.matmul(a, w) + b\r\n            out = tf.square(out)\r\n\r\n        grads = t.gradient(out, [w, b])\r\n        optim.apply_gradients(zip(grads, [w, b]))\r\n\r\n\r\n@tf.function\r\ndef dataset_bench_f(p_data, w, b, optim):\r\n    for a in p_data:\r\n        with tf.GradientTape() as t:\r\n            out = tf.matmul(a, w) + b\r\n            out = tf.square(out)\r\n\r\n        grads = t.gradient(out, [w, b])\r\n        optim.apply_gradients(zip(grads, [w, b]))\r\n\r\n\r\ndata = np.random.normal(size=[NB_SAMPLES, DIM_SIZE])\r\np_data = tf.data.Dataset.from_tensor_slices(data) \\\r\n           .shuffle(buffer_size=BATCH_SIZE * 10) \\\r\n           .map(to_float32, num_parallel_calls=max(multiprocessing.cpu_count() // 2, 1)) \\\r\n           .batch(BATCH_SIZE)\r\n\r\nw = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)\r\nb = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)\r\noptim = tf.keras.optimizers.SGD(1e-3)\r\nt1 = time.time()\r\nfor i in range(NB_LOOP):\r\n    bench(w, b, optim)\r\nt2 = time.time()\r\nprint(\"bench, total time: {}\".format((t2 - t1) / NB_LOOP))\r\n\r\nw = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)\r\nb = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)\r\noptim = tf.keras.optimizers.SGD(1e-3)\r\nt1 = time.time()\r\n# Tracing\r\nbench_f(w, b, optim)\r\nfor i in range(NB_LOOP):\r\n    bench_f(w, b, optim)\r\nt2 = time.time()\r\nprint(\"bench_f, total time: {}\".format((t2 - t1) / NB_LOOP))\r\n\r\nw = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)\r\nb = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)\r\noptim = tf.keras.optimizers.SGD(1e-3)\r\nt1 = time.time()\r\nfor i in range(NB_LOOP):\r\n    dataset_bench(p_data, w, b, optim)\r\nt2 = time.time()\r\nprint(\"dataset_bench, total time: {}\".format((t2 - t1) / NB_LOOP))\r\n\r\nw = tf.Variable(np.random.uniform(size=[DIM_SIZE, DIM_SIZE]), dtype=tf.float32)\r\nb = tf.Variable([0.] * DIM_SIZE, dtype=tf.float32)\r\noptim = tf.keras.optimizers.SGD(1e-3)\r\n# Tracing\r\ndataset_bench_f(p_data, w, b, optim)\r\nt1 = time.time()\r\nfor i in range(NB_LOOP):\r\n    dataset_bench_f(p_data, w, b, optim)\r\nt2 = time.time()\r\nprint(\"dataset_bench_f, total time tf.func: {}\".format((t2 - t1) / NB_LOOP))\r\n```\r\n\r\n**Results**\r\nOSX:\r\n```\r\nbench, total time: 0.760720341205597\r\nbench_f, total time: 0.1514965009689331  # <-- nice improvement\r\ndataset_bench, total time: 0.904261839389801\r\ndataset_bench_f, total time tf.func: 0.9035982608795166 # <-- no improvement\r\n```\r\nLinux:\r\n```\r\nbench, total time: 0.14520598888397218\r\nbench_f, total time: 0.0331043267250061 # <-- nice improvement\r\ndataset_bench, total time: 0.3545421266555786\r\ndataset_bench_f, total time tf.func: 0.4910250568389893 # <-- worst result\r\n```\r\n", "comments": ["Tried executing the code on Linux platform, I am able to reproduce the issue with TF 2.0.0.beta1 and it results \r\n```\r\nbench, total time: 0.5007321739196777\r\nbench_f, total time: 0.06961017847061157\r\ndataset_bench, total time: 0.4967630958557129\r\ndataset_bench_f, total time tf.func: 0.34414920330047605\r\n```\r\n", "@jsimsa @rohan100jain can you take a look at this?", "You are not comparing \"apples to apples\". Your tf.data input pipeline ends up performing more data movement compared to your non-tf.data input pipeline. To mimic the behavior of your non tf.data program, your input pipeline should look like this:\r\n\r\n```\r\ndata = np.random.normal(size=[NB_EPOCHS, BATCH_SIZE, DIM_SIZE])\r\np_data = tf.data.Dataset.from_tensor_slices(data)\r\n           .shuffle(buffer_size=10)\r\n           .map(to_float32, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n```\r\n\r\nWhen I run the benchmark with the above modification, I get the following result:\r\n\r\n```\r\nbench, total time: 0.535095500946045\r\nbench_f, total time: 0.10285426378250122\r\ndataset_bench, total time: 0.4540321445465088\r\ndataset_bench_f, total time tf.func: 0.1117027997970581\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30984\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30984\">No</a>\n", "Sorry for the late response here.\r\n\r\nIf I'm not mistaken your solution doesn't provide the same behaviour. In one case I generate `NB_EPOCHS` more data and I don't loop anymore on potential fixed given datasets.\r\n\r\nGiven a standard dataset, how do I ensure that I keep the speed up and loop over it?"]}, {"number": 30983, "title": "Sample weights being expanded to match y_true/y_pred before they're sliced using a class_id in keras metrics.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 7.5\r\n\r\n**Describe the current behavior**\r\nWhen passing sample weights and a class id to a metric class `sample_weights` is expanded to match the shape of y_true in [line 306 of this file](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/utils/metrics_utils.py#L306), then the slicing using `class_id` is done in [line 312](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/keras/utils/metrics_utils.py#L312) which means the broadcast weights op in [line 339-340](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/tensorflow/python/keras/utils/metrics_utils.py#L339) fails as the shapes don't match anymore.\r\n\r\n**Describe the expected behavior**\r\nSample weights should be squeezed or expanded to match the new shape of y_true and y_pred after they're sliced. Or it could be enforced to be the same shape as y_true and y_pred and then sliced as well.\r\n\r\n**Code to reproduce the issue**\r\nThis code should illustrate the problem\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n\r\n    class MaskedRecall(tf.keras.metrics.Metric):\r\n\r\n        def __init__(self, class_id=None, name=None, dtype=None):\r\n            super().__init__(name, dtype)\r\n            self.inner = tf.keras.metrics.Recall(class_id=class_id)\r\n\r\n        def result(self):\r\n            return self.inner.result()\r\n\r\n        def update_state(self, y_true, y_pred, sample_weight=None):\r\n            sample_weight = tf.where(tf.equal(y_true[..., self.inner.class_id], -1),\r\n                                    tf.zeros_like(y_true[..., self.inner.class_id]),\r\n                                    tf.ones_like(y_true[..., self.inner.class_id]))\r\n            self.inner.update_state(y_true, y_pred, sample_weight)\r\n\r\n        def reset_states(self):\r\n            self.inner.reset_states()\r\n\r\n\r\n    if __name__ == \"__main__\":\r\n        # metric = tf.keras.metrics.Recall()\r\n        metric = MaskedRecall(class_id=0)  # 0)\r\n        y_true = np.array([[[1], [0], [-1]], [[0], [1], [0]]])\r\n        y_pred = np.array([[[1], [0], [0]], [[0], [1], [0]]])\r\n\r\n        tf.config.experimental_run_functions_eagerly(True)\r\n        metric.reset_states()\r\n        metric.update_state(y_true, y_pred)\r\n        print(\"Metric result: \", metric.result())\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback from the above:\r\nTraceback (most recent call last):\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/Downloads/metric.py\", line 34, in <module>\r\n~                                                                                                                                                             \u2502    metric.update_state(y_true, y_pred)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py\", line 75, in decorated\r\n~                                                                                                                                                             \u2502    update_op = update_state_fn(*args, **kwargs)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 400, in __call__\r\n~                                                                                                                                                             \u2502    return self._python_function(*args, **kwds)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/Downloads/metric.py\", line 20, in update_state\r\n~                                                                                                                                                             \u2502    self.inner.update_state(y_true, y_pred, sample_weight)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py\", line 75, in decorated\r\n~                                                                                                                                                             \u2502    update_op = update_state_fn(*args, **kwargs)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\", line 1318, in update_state\r\n~                                                                                                                                                             \u2502    sample_weight=sample_weight)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py\", line 340, in update_confusion_matrix_vari\r\n~                                                                                                                                                             \u2502ables\r\n~                                                                                                                                                             \u2502    math_ops.cast(sample_weight, dtype=dtypes.float32), y_pred)\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 167, in broadcast_weights\r\n~                                                                                                                                                             \u2502    with ops.control_dependencies((assert_broadcastable(weights, values),)):\r\n~                                                                                                                                                             \u2502  File \"/home/tbradley/.virtualenvs/anim/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 103, in assert_broadcastable\r\n~                                                                                                                                                             \u2502    weights_rank_static, values.shape, weights.shape))\r\n~                                                                                                                                                             \u2502ValueError: weights can not be broadcast to values. values.rank=1. weights.rank=2. values.shape=(2,). weights.shape=(2, 1).\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Please, find the [gist ](https://colab.research.google.com/drive/1yoHN-3k50q3KHCg6U-vJ2v8U_yqXafI2)here.Thanks!", "I patched my local copy yesterday, it's a very small tweak. I'll make a pull request this afternoon.", "@simian-typist Thanks for submitting PR to resolve the issue. Thanks!", "This is still throwing an error with `tf-nightly`. Error trace is as follows. [Here](https://colab.research.google.com/gist/jvishnuvardhan/3dfc9a40e3d200687830c3860b51c92c/untitled47.ipynb) is the gist for our reference.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-3658c987367a> in <module>()\r\n     30     tf.config.experimental_run_functions_eagerly(True)\r\n     31     metric.reset_states()\r\n---> 32     metric.update_state(y_true, y_pred)\r\n     33     print(\"Metric result: \", metric.result())\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/weights_broadcast_ops.py in assert_broadcastable(weights, values)\r\n    101             \" values.shape=%s. weights.shape=%s.\" % (\r\n    102                 _ASSERT_BROADCASTABLE_ERROR_PREFIX, values_rank_static,\r\n--> 103                 weights_rank_static, values.shape, weights.shape))\r\n    104       weights_shape_static = tensor_util.constant_value(weights_shape)\r\n    105       values_shape_static = tensor_util.constant_value(values_shape)\r\n\r\nValueError: weights can not be broadcast to values. values.rank=2. weights.rank=3. values.shape=(2, 3). weights.shape=(2, 3, 1).\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: Current TensorFlow version is 2.2.0-dev20200331. To use TF 1.x instead,\r\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\r\nyou run \"import tensorflow\".\r\n---------------------------------------------------------------------------\r\n```", "Sorry I missed this, I'll have a look over the easter weekend.", "I could reproduce the issue with TF 2.2-rc4.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c01ba4f312fede29904b496712f3fbbb/untitled860.ipynb).Thanks!", "Was able to reproduce the issue with TF v2.3 and TF-nightly i.e. v2.4.0-dev20200909. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/448971baf40e42a8ee759ebbef1e97a2/30983.ipynb). Thanks!", "Was able to reproduce the issue in TF 2.6.0-dev20210527,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/14c84a989f5e0d7aa99fe36629dd3c98/untitled17.ipynb)..Thanks !", "I could reproduce the issue with TF 2.6 . Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/17efd18642e06b85391f5519e8516b3b/untitled17.ipynb#scrollTo=pUdyzYRG4X2T). Thanks!", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30983\">No</a>\n"]}, {"number": 30982, "title": "Delete the parameters allow_soft_placement in function AssignDevice", "body": "This paramenters 'allow_soft_placement' is useless in function AssignDevice.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30982) for more info**.\n\n<!-- need_author_cla -->", "@fortheD Please sign CLA in order to proceed with next steps. Thank you!"]}, {"number": 30981, "title": "does pip wheels support hdfs on default?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary?\r\n- TensorFlow version: 1.12\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nUncertain if pip tensorflow_gpu 1.12 for python 3.5 supports hdfs by default\r\n\r\n**Any other info / logs**\r\nutil.NativeCodeLoader: Unable to load native-hadoop library for your platform\r\ninfo retry.RetryInvocationHandler org.apache.hadoop.net.ConnectTimeoutException: Call From ... failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=...] while invoking ClientNameNodeProtocolTranslatorPB.getFileinfo over ... after 1 failover attempts. Trying to failover after sleeping for 781ms\r\n\r\n", "comments": ["Yes. you should set hadoop env in advance:\r\n```\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${JAVA_HOME}/jre/lib/amd64/server:${HADOOP_HDFS_HOME}/lib/native\r\nexport CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath --glob)\r\npython your_script.py\r\n```", "much thanks!", "@sharkdtu sorry for bothering, I am running it on a paas, therefore instead of shell export, i did instead\r\n\"\"\" os.environ[\"LD_LIBRARY_PATH\"]+=\":\"+os.environ[\"JAVA_HOME\"]+\"/jre/lib/amd64/server:\"+os.environ[\"HADOOP_HDFS_HOME\"]+\"/lib/native\"\r\n  f = os.popen(\"/opt/hadoop/bin/hadoop classpath --glob\")\r\n  classpath=f.read()\r\n  os.environ[\"CLASSPATH\"]=classpath.strip('\\n') \r\n\"\"\"\r\nthe unable to load error goes away but the socket timeout exception persists and the program is unable to run.", "@colmantse ,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "@anush-o sorry for the ambiguity , I have narrowed down it to this function:\r\n\r\ndata_files = sorted(tf.contrib.slim.parallel_reader.get_data_files(\r\n        data_filepattern))\r\nwhere datafilepattern is the hdfs://path/pattern\r\n", "> _Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template_\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n> * TensorFlow installed from (source or binary): binary?\r\n> * TensorFlow version: 1.12\r\n> * Python version: 3.5\r\n> * Installed using virtualenv? pip? conda?: pip\r\n> * Bazel version (if compiling from source):\r\n> * GCC/Compiler version (if compiling from source):\r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory:\r\n> \r\n> **Describe the problem**\r\n> Uncertain if pip tensorflow_gpu 1.12 for python 3.5 supports hdfs by default\r\n> \r\n> **Any other info / logs**\r\n> util.NativeCodeLoader: Unable to load native-hadoop library for your platform\r\n> info retry.RetryInvocationHandler org.apache.hadoop.net.ConnectTimeoutException: Call From ... failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=...] while invoking ClientNameNodeProtocolTranslatorPB.getFileinfo over ... after 1 failover attempts. Trying to failover after sleeping for 781ms\r\n\r\n@colmantse ,\r\nLooks like the issue is from hadoop,can you confirm?Thanks!", "um.... yeah Hadoop error is raised. I am just not sure if it is caused because some of the function in tf 1.12 and Hadoop is incompatible. ", "@jhseu do you know who know about the hdfs support in TF?", "update, apart from tf.contrib.slim.parallel_reader.get_data_files, \r\n\r\ntf.python_io.tf_record_iterator does not support hdfs as well", "@colmantse Is your Hadoop cluster running in secure mode? You may also need to pass the Kerberos ticket cache env var:\r\nhttps://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/hadoop.md", "hi, I would double check and come back after the weekend. thanks.", "@jhseu hi, the cluster is not running in secure mode.\r\nMy current status is that i have located the function below as the cause of the error,\r\ndata_files = sorted(tf.contrib.slim.parallel_reader.get_data_files(\r\ndata_filepattern))\r\nwhere datafilepattern is the hdfs://path/pattern\r\nand i work it around by\r\ndata_filepattern = nfs_data_filepattern\r\ndata_files = sorted(tf.contrib.slim.parallel_reader.get_data_files(\r\ndata_filepattern))\r\ndata_files = [x.replace(nfs_path,hdfs_path) for x in data_files]\r\nafter that the below log happened\r\ngraph was finalized.\r\nrestoring parameters from nfs/data\r\nsaving check point.\r\nwarn retry.retryinvovationhandler: a failover has occured since the start of call #2 clientnamenodeprotocaltranslatorPB.getBlocklocations over /30.12.2.128.9000\r\ninfo retry.RetryInvocationHandler org.apache.hadoop.net.ConnectTimeoutException: Call From ... failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=...] while invoking ClientNameNodeProtocolTranslatorPB.getFileinfo over ... after 1 failover attempts. Trying to failover after sleeping for 781ms", "Can we close this?", "Hi @colmantse !\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30981\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30981\">No</a>\n"]}, {"number": 30980, "title": "Running FaceNet and MTCNN model simutaneously on 2 cameras", "body": "Hello everyone!\r\n\r\nI am trying to run the MTCNN and FaceNet model on 2 cameras simultaneously. So, I am not getting any error while doing this but the code don't give me any results.\r\n\r\nIt just loads both the models and doesn't give me any predictions. Can anyone help me with this? I have created 2 graphs using g=tf.Graph for MTCNN and FaceNet. \r\n\r\nI think this error is coming due to multi-processing with tensorflow as it might trying to load MTCNN input to Facenet graph. *this is my assumption. \r\n\r\nPlease let me know if you have any idea about this. Thanks.\r\n", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "Automatically closing due to lack of recent activity.Thanks!"]}, {"number": 30979, "title": "Converting Tensorflow model to Tensorflowlite model ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Monikasinghjmi Please provide the information asked in Template. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30978, "title": "attribute error", "body": "AttributeError: module 'tensorflow' has no attribute 'get_default_graph'", "comments": ["Hi,\r\nCould you please provide more details as to which version of tensorflow you are running, and in which context this error appears? This is most probably an issue with trying to use a behaviour from TF 1.x while running TF 2.0beta", "Thank you for your response. This error is coming after upgrading tensorflow to 2.0.When i am going to create sequential model,this error is coming.I had to downgrade the version.After that it is working fine.", "Great ; you are welcome, and do not forget to close the issue if your problem is solved.\r\n\r\nTensorflow 2.0 changes the API quite a bit. It is up to you to start using it now or not, but if you do you indeed need to update your code :-)", "OK. Thank you."]}, {"number": 30977, "title": "Online convertor", "body": "hello,\r\ni have too many problem to convert  a custom model, install python and ...\r\nactually i have been tired to make a custom model and have asked from a person to make it.\r\nits kind of hard to ask other people to do these simple things and hard to me cuz i dont know cant understand. can you make a online convertor for us?\r\nwe upload our images and you give us .tflite and .txt\r\ni think its easy for you and it can save our time too. (i mean install python and other things)\r\ntnx\r\n\r\nor if you have it give me the link plz", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "i asked alot but its have to me because i dont know anything about python and im useing sources that i dont know that are they.\r\nAll developers always have same problem if you make an online convertor its going to save our time and your wont want to answer to our questions (which most of them are the same)\r\ni lost too many project that because i cant work with python and got too many problems (and also download too many things from Nvidia website that i just lose my laptop`s space)\r\nif its possible create an online convertor please\r\nthanks", "@tensorflowbutler  @ravikyram so? is it possible ?"]}, {"number": 30976, "title": "AttributeError: 'NoneType' object has no attribute 'keys'", "body": "I am trying to make classification using tensorflow \r\n\r\nI am using anaconda version 1.7.2 on Ubuntu 18.04.2 LTS and python 3.7.3\r\n\r\ni downloaded retrain.py code from [ https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py](url)\r\n\r\n when i run this file i got error \r\n\r\n`File \"retrain.py\", line 1349, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/deepak/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/deepak/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/deepak/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"retrain.py\", line 1006, in main\r\n    class_count = len(image_lists.keys())\r\nAttributeError: 'NoneType' object has no attribute 'keys'`\r\n\r\ni was running this command to run that file \r\n\r\n`python retrain.py --image_dir ~/home/deepak/new_data_EHL/Augmented --learning_rate=0.0001 --testing_percentage=20 --validation_percentage=20 --training_batch_size=32 --validation_batch_size=-1 --eval_step_interval=100 --how_many_training_steps=10 --random_scale=30 --random_brightness=30 --architecture mobilenet_1.0_224 --output_graph=output_graph.pb --output_labels=output_labels.txt`\r\n\r\ni am a beginner in tensorflow and deep learning i searched about the solution but i couldn't get the solution that work.\r\n", "comments": ["Hi,\r\nThis is actually not a tensorflow issue, the exception is apparently raised because the `create_image_lists` function returns a `None` value instead of the expected `OrderedDict`.\r\n\r\nI would advise you to run Python in a terminal, copy/paste the `retrain.py` file's content save for the `if __name__ == '__main__'` final block (or run `from retrain import create_image_lists` if the file is in your working directory) and try to call it with the proper arguments - so, in your case, `create_image_lists('~/home/deepak/new_data_EHL/Augmented', 20, 20)`.\r\n\r\nSee what this returns, whether you get warnings, etc. If you do not get a dict with information on your images, maybe try running the function's code line by line to see where it goes wrong...\r\n\r\nGood luck!", "@dee98 Please provide the correct link for retrain.py. \r\nAnd also, let us know @pandrey-fr's comment resolve your issue. Thanks!", "@gadagashwini link of retrain.py  `https://github.com/tensorflow/hub/tree/master/examples/image_retraining`\r\nyes @pandrey-fr  comment resolve my issue\r\nthis error occured because i was using \"~\" this sign in my image path\r\n\r\nThanks for help me \r\n", "@dee98 You are welcome :)"]}, {"number": 30975, "title": "Bug in TF2.0 & V100: Inconsistent 2d convolution results w.r.t batch size", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10 / cuDNN 7\r\n- GPU model and memory: NVIDIA V100 / 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI set inputs are tensors which contain only ones.\r\nThen, the output of `tf.nn.conv2d` is different when batch_sizes are 64 and 128.\r\n\r\n**Describe the expected behavior**\r\nThe output of `tf.nn.conv2d` should not be different w.r.t batch sizes.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ncode:\r\n```python\r\nimport tensorflow as tf\r\nprint(\"TF_VERSION: %s\" % tf.__version__)\r\n\r\nx = tf.ones((128, 28, 28, 32))\r\nkernel = tf.ones((3, 3, 32, 64))\r\n\r\nya = tf.nn.conv2d(x[:64], kernel, [1, 1], \"SAME\")\r\nyb = tf.nn.conv2d(x, kernel, [1, 1], \"SAME\")[:64]\r\nprint(tf.reduce_sum(tf.math.abs(ya- yb)))\r\n```\r\nprinted outputs:\r\n```\r\nTF_VERSION: 2.0.0-beta1\r\ntf.Tensor(144.375, shape=(), dtype=float32)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFrom the code I'm given, the values of `ya` and `yb` are as below:\r\n`ya`:\r\n```\r\n<tf.Tensor: id=10, shape=(64, 28, 28, 64), dtype=float32, numpy=\r\narray([[[[128., 128., 128., ..., 128., 128., 128.],\r\n         [192., 192., 192., ..., 192., 192., 192.],\r\n         [192., 192., 192., ..., 192., 192., 192.],\r\n         ...,\r\n         [192., 192., 192., ..., 192., 192., 192.],\r\n         [192., 192., 192., ..., 192., 192., 192.],\r\n         [128., 128., 128., ..., 128., 128., 128.]]]], dtype=float32)>\r\n```\r\n`yb`:\r\n```\r\n<tf.Tensor: id=15, shape=(64, 28, 28, 64), dtype=float32, numpy=\r\narray([[[[128.00009, 128.00009, 128.00009, ..., 128.00009, 128.00009,\r\n          128.00009],\r\n         [192.0001 , 192.0001 , 192.0001 , ..., 192.0001 , 192.0001 ,\r\n          192.0001 ],\r\n         [192.00009, 192.00009, 192.00009, ..., 192.00009, 192.00009,\r\n          192.00009],\r\n         ...,\r\n         [192.00012, 192.00012, 192.00012, ..., 192.00012, 192.00012,\r\n          192.00012],\r\n         [192.00012, 192.00012, 192.00012, ..., 192.00012, 192.00012,\r\n          192.00012],\r\n         [128.0001 , 128.0001 , 128.0001 , ..., 128.0001 , 128.0001 ,\r\n          128.0001 ]]]], dtype=float32)>\r\n```", "comments": ["@jaywalnut310, I tried reproducing the reported issue on Colab with TF 2.0.0.beta1 but i got same result for `ya` and `yb`. The result of the above code is \r\n```\r\nTF_VERSION: 2.0.0-beta1\r\ntf.Tensor(0.0, shape=(), dtype=float32)\r\n```\r\nPlease try once and let us know the expected behavior. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]