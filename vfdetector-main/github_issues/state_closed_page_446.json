[{"number": 40467, "title": "TF-Lite Micro: Fix C linkage for stm32f4HAL target", "body": "Fix C linkage for output retargeting on stm32f4HAL target", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40467) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40467) for more info**.\n\n<!-- ok -->"]}, {"number": 40466, "title": "LoadSavedModel very slow in loading the net", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: GeForce GTX 960M 2004MiB\r\n\r\n**Describe the current behavior**\r\nI am trying to load a keras SavedModel in C++ using the LoadSavedModel function and SavedModelBundleLite. \r\nThe net is correctly loaded but it takes a lot of time (in my case around 70-90 seconds, which is too much in my opinion for such net, also because in python the laoding is much faster).\r\n\r\n**Describe the expected behavior**\r\nThe time needed to load the net should be comparable with the python case.\r\n\r\n**Standalone code to reproduce the issue**\r\nPYTHON SIDE:\r\n```\r\ndef create_model_vgg16(input_img_shape):\r\n    base_model = keras.applications.VGG16(include_top=False, weights=\"imagenet\", input_shape=input_img_shape)\r\n    for layer in base_model.layers:\r\n        layer.trainable = False\r\n        \r\n    model = keras.Sequential()\r\n\r\n    model.add(keras.layers.Lambda(preprocess, name='preprocessing', input_shape=input_img_shape, output_shape=input_img_shape))\r\n    for layer in base_model.layers[1:]:\r\n        model.add(layer)\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(600, activation=\"relu\"))\r\n    model.add(keras.layers.Dense(300, activation=\"relu\"))\r\n    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\r\n    \r\n    return model\r\n\r\ninput_img_shape = (100,100,3)\r\n\r\nmodel = create_model_vgg16(input_img_shape)\r\n\r\n# ... training ...\r\nmodel.save('saved_model/vgg_model') \r\n```\r\n\r\nC++ SIDE\r\n\r\n```\r\n// loading the dnn model from file\r\n  std::string modelfile_path = \r\n      std::string(common::kTrainingSamplesDirectory) + \"/../assets/\" + options_.modelfile_name;\r\n      \r\n  LOG(INFO) << \"ready to load the model\";\r\n\r\n  auto status = \r\n      tf::LoadSavedModel(session_options_, run_options_, \r\n         modelfile_path, {tf::kSavedModelTagServe}, &model_);\r\n  \r\n  \r\n  LOG(INFO) << \"after loading the model\";\r\n  // this takes a lot\r\n\r\n  if (!status.ok()) {\r\n    LOG(ERROR) << \"Tracker Model not loaded successfully!\";\r\n    return; \r\n  }\r\n```\r\n\r\nThe output log :\r\n\r\n```\r\n2020-06-15 11:46:53.606488: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /home/hiro-robotics/Workspace/vs4_app_screwing/training_samples/../assets/vgg_model\r\n2020-06-15 11:46:53.610125: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-06-15 11:46:53.610146: I tensorflow/cc/saved_model/loader.cc:295] Reading SavedModel debug info (if present) from: /home/hiro-robotics/Workspace/vs4_app_screwing/training_samples/../assets/vgg_model\r\n2020-06-15 11:46:53.610211: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-15 11:46:53.624335: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2599990000 Hz\r\n2020-06-15 11:46:53.624736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb6f38aca80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-15 11:46:53.624750: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-15 11:46:53.626171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-15 11:46:53.635132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.0975GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-06-15 11:46:53.635491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-15 11:46:53.637397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-15 11:46:53.638597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-15 11:46:53.638994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-15 11:46:53.640652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-15 11:46:53.641880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-15 11:46:53.645200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-15 11:46:53.645773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-06-15 11:48:04.764783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-15 11:48:04.764807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-06-15 11:48:04.764813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-06-15 11:48:04.765615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 903 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-06-15 11:48:04.767379: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb66fabe440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-15 11:48:04.767394: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\n2020-06-15 11:48:04.783707: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\r\n2020-06-15 11:48:04.928174: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /home/hiro-robotics/Workspace/vs4_app_screwing/training_samples/../assets/vgg_model\r\n2020-06-15 11:48:04.941220: I tensorflow/cc/saved_model/loader.cc:364] SavedModel load for tags { serve }; Status: success: OK. Took 71334728 microseconds.\r\n```\r\n\r\nEDIT:\r\nActually I checked again the time needed in python and it is almost as slow as in C++. \r\nThen my problem is: why is it so slow? Should I expect that or there is something I can do to speed up the thing?\r\n\r\nThank you very much every hint is welcome", "comments": ["This seems to be related to the GPU.\r\nI have found out that by disabling the GPU the performances improve dramatically (less than 1 second to load the same network)\r\nThe code I used to disable the GPU: \r\n```\r\n  auto config = &session_options_.config;\r\n  // disabled GPU entirely\r\n  (*config->mutable_device_count())[\"GPU\"] = 0;\r\n  // place nodes somewhere\r\n  config->set_allow_soft_placement(true);\r\n```\r\nWhat could be the reason for such slow down in the GPU version? \r\nI do understand that the GPU version has to do some stuff for initialization but still this looks too slow to me. ", "@jappoz Thanks for reporting this issue. Can you please check with recent `TF2.2`, `tf-nightly` and let us know whether the issue persists? Thanks", "Thanks for the reply.\r\nI noticed that there was a mismatch between the tf python version (2.1) and the c++ version (2.2) in my pc.\r\nI installed tf2.2 by building the pip package from sources and now the python side is fast in loading the model. \r\nFrom the C++ side instead the problem persists (possibly worse than before).\r\nUsing tf-nightly does not help too.", "I finally found out the solution.\r\nThe problem was that the c++ library was compiled with cuda capabilities 3.5, 7.0 while the exact cuda capabilities of my gpu is 5.0.\r\nRecompiling with the flag TF_CUDA_COMPUTE_CAPABILITIES set to 5.0 solved the problem.\r\n", "Same problem for me with RTX 2080Ti(CC 7.5).\r\nAdding `TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"` in front of the `bazel` command did the trick."]}, {"number": 40464, "title": "Fix Keras documentation", "body": "@qlzh727 8ff354d254dc786dea9fc1f9d3d7cc39df25e39f\r\n@omalleyt12 b4db28ca0df3b4938a923ffac1fc2085062dbe9d", "comments": ["@lutzroeder Can you please address Ubuntu Sanity errors? Thanks!", "@lutzroeder Can you please fix build failures ? Thanks!"]}, {"number": 40463, "title": "Add arm64 third-party CI", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0+ and master branch\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, Tensorflow only has the official build CI on X86 and third-party build CIs on x86 and ppc64. There is no CI for arm64. Adding a arm64 CI can help community to discover arm64 problems easily.\r\n\r\nOpenLab supports public CI system for opensource projects[1]. Now it supports arm64 arch and the tensorflow nightly build jobs for 2.0+ and master version have been added there as well[2]. It runs tensorflow build everyday at UTC-18.\r\n\r\nJust like what tensorflow do currently, we can just easily add a new badge in the README.md file to link the Openlab arm64 third-party CI.\r\n\r\nAs you can see in the page[2], tensorflow 2.0, 2.1 and 2.2(master) build well on aarch64. But in some aws libs strongly based on x86 ARCH, so in master branch, I skip that part for build. You can see the build brief in [3], download the build whl packages there, and see the details logs in [4].\r\n\r\nSo adding the arm64 build CI is useful for the community.\r\n\r\n1: https://openlabtesting.org\r\n2: http://status.openlabtesting.org/builds?project=tensorflow%2Ftensorflow\r\n3: http://status.openlabtesting.org/build/c816e5c9d6cc4519b933414fc6044d28\r\n4: https://logs.openlabtesting.org/logs/periodic-18/github.com/tensorflow/tensorflow/master/tensorflow-arm64-build-daily-v2.1.0/c816e5c/\r\n\r\nAdditional context\r\nNow the test is CPU only and is basing on Ubuntu 18.04 and python3.6. More can be added in the future.\r\n\r\nAnd I'm from OpenLab commuinty. I'll keep looking after the tensorflow arm64 CI and try my best to fix the arm64 failure then.\r\n\r\nHere is an example[1] we done in pytorch community. See 'Linux (aarch64) CPU' badge in the README.md. \r\nAlso for another community[2], we done in greenplum-db community. See 'Zuul Regression Test On Arm' badge in the README.md\r\n\r\n1: https://github.com/pytorch/pytorch/blob/master/README.md\r\n2: https://github.com/greenplum-db/gpdb\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nThe arm64 users and developers\r\n\r\n", "comments": ["Hi team, any idea about this? @terryheo Hi, we are welcome any advice/feedback from you guys. Thanks very much", "Hi @bzhaoopenstack, would it be possible to open a separate issue for `AWS lib build for AArch64'. It is easier to discuss in an issue than via emails or other mechanisms.", "@ymodak , could you pass this to TF infra team?", "@all @AshokBhat \r\nHi,\r\n sorry for late. \r\nTest env:\r\nUbuntu 1804, 8vCpus/16G memory on aarch64 platform\r\nTests use the tensorflow master branch.\r\n\r\nI tested with build option like below:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --local_ram_resources=13240 --local_cpu_resources=6 --verbose_failures\r\nAnd I got the first error is\r\n\r\n```\r\nERROR: /home/zuul/tensorflow/tensorflow/python/BUILD:3018:1: Linking of rule '//tensorflow/python:gen_list_ops_py_wrappers_cc' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/zuul/.cache/bazel/_bazel_zuul/ce9de8ddb6d970afbff1ea6d450e3cd7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc @bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/python/gen_list_ops_py_wrappers_cc-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Spython_Cgen_Ulist_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'aws_checksums_crc32c_hw'\r\nbazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Spython_Cgen_Ulist_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'aws_checksums_do_cpu_id'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 642.843s, Critical Path: 77.35s\r\nINFO: 2161 processes: 2161 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\nThis error is from https://github.com/awslabs/aws-checksums . \r\n\r\nWill update in the following tests", "@av8ramit Hi, any idea about this? Thanks", "Sorry I'm not sure, I have not seen this error before. @mihaimaruseac any chance you have?", "It looks like the arm source files are [missing in the glob here](https://github.com/tensorflow/tensorflow/blob/4823ee3dc12edfa45fbc60e60ca4d5c87eced770/third_party/aws/aws-checksums.bazel#L18). Adding  source/arm/*.c to the link ~~below~~ above file should do the trick, but I haven't tested it.\r\n\r\nAlso there is a PR to add HW support for Arm to aws-checksums: https://github.com/awslabs/aws-checksums/pull/29\r\n\r\nIt should build with the existing implementation and aws_checksums_crc32c_hw() calls aws_checksums_crc32c_sw() so it should work without the PR.  We'll also get that PR or something like it merged shortly. \r\n", "I haven't seen it before ~~but it looks to be caused by having all the filesystems included in the framework. tensorflow/community#101 aims to remove filesystems, reducing pip size and the need to compile platforms that are not needed~~\r\n\r\nEdit: @AGSaidi has the better answer", "@AGSaidi @mihaimaruseac Cool. Thanks for your kind advice.\r\n\r\nThat part already pass. But I get the following error.\r\n\r\n```\r\nERROR: /home/zuul/tensorflow/tensorflow/python/BUILD:501:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/zuul/.cache/bazel/_bazel_zuul/ce9de8ddb6d970afbff1ea6d450e3cd7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -fPIC -DSQLITE_OMIT_DEPRECATED -DMLIR_CUDA_CONVERSIONS_ENABLED -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL '-DGRPC_ARES=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/png -iquote bazel-out/host/bin/external/png -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/com_github_grpc_grpc -iquote bazel-out/host/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/host/bin/external/upb -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardPatternsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/png -isystem bazel-out/host/bin/external/png -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Transforms/InstCombine -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/X86 -isystem external/com_github_grpc_grpc/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/host/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -g0 '-march=native' -g0 '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ntensorflow/python/lib/core/bfloat16.cc: In function 'bool tensorflow::{anonymous}::Initialize()':\r\ntensorflow/python/lib/core/bfloat16.cc:664:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:668:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:671:77: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:675:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:679:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:683:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/zuul/tensorflow/tensorflow/python/tools/BUILD:143:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/zuul/.cache/bazel/_bazel_zuul/ce9de8ddb6d970afbff1ea6d450e3cd7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -fPIC -DSQLITE_OMIT_DEPRECATED -DMLIR_CUDA_CONVERSIONS_ENABLED -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL '-DGRPC_ARES=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/png -iquote bazel-out/host/bin/external/png -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/com_github_grpc_grpc -iquote bazel-out/host/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/host/bin/external/upb -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardPatternsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/png -isystem bazel-out/host/bin/external/png -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Transforms/InstCombine -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/X86 -isystem external/com_github_grpc_grpc/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/host/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -g0 '-march=native' -g0 '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 5534.936s, Critical Path: 204.04s\r\nINFO: 9877 processes: 9877 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAlso @av8ramit , hi, could you guys consider add a 3rd party ARM CI for tensorflow community like this issue description mentioned above? Thank you", "For above error, I use the build cmd is:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --local_ram_resources=14240 --local_cpu_resources=7 --verbose_failures\r\n```\r\nWhich is the same with other builds(2.0, 2.1 tensorflow whl build jobs)", "Looks the same issue like https://github.com/tensorflow/tensorflow/issues/40688 , will test on it and let you know the result later. Thanks", "Hi guys @AGSaidi @mihaimaruseac @av8ramit \r\n\r\nAfter merge https://github.com/tensorflow/tensorflow/pull/40700 in my local repo, and downgrade numpy via ```pip3 install numpy==1.18.0```.  Now I can success build the whl package on master branch using \r\n```\r\nbazel clean --expunge ; bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --local_ram_resources=10240 --local_cpu_resources=7 --verbose_failures\r\n```", "That's good news. Seem the issue is solved, right? Can we close if that is the case?", "> That's good news. Seem the issue is solved, right? Can we close if that is the case?\r\n\r\nThe original request was to add Arm64 third party CI, which is still unresolved.", "@mihaimaruseac @av8ramit Yeah, the fix is only about the master branch build, the issue is about to add a arm64 3rd party CI.. I think you guys already consider it. Please give some feedback. Any advice is welcome..Thanks", "Are you a member of [SIG Build](https://groups.google.com/a/tensorflow.org/g/build)? This might be a better conversation to have there. I vaguely remember some discussions about ARM CI, although I myself do not have plans to implement it. ", "I agree with Amit here. The ARM CI should be via SIG Build, as a community maintained CI", "@mihaimaruseac @av8ramit Hi, Thanks. I already raised a ARM CI discussion in SIG Build for a long time. As I think this one looks like a aarch64(ARM) basic CI to add into tensorflow community as a 3rd-party CI. Also the discussion in SIG Build is a totally new build CI system, they can be done in parallel. And for pytorch https://github.com/pytorch/pytorch/blob/master/README.md , my colleage pushed the same into the community as a 3rd-party ARM CI, and it successed. So, do you guys raise this in your team? Because I think the basic ARM CI is necessary and valuable thing for the AI area.\r\n\r\nAnd all things about ARM CI will be maintaned by us, including the code change ,package building, arm issues fix and human resources. The only thing is add a link or badge into tensorflow README.MD to show to the customers and developers, and the way to show is based on community's feedback. How about to consider it? Thanks\r\n\r\nWe're happy to discuss more.", "@bzhaoopenstack As far as I understand, community-supported builds are handled by SIG BUILD team. The whole process is documented in https://github.com/tensorflow/community/blob/master/sigs/build/community-builds.md page. As part of the first step of the process, a tracking bug needs to be raised in GitHub (as per the document). My guess is that this should be raised in https://github.com/tensorflow/build repo rather than TensorFlow/TensorFlow repo. ", "@AshokBhat Thanks for your suggest. Let's raise this there. ", "Can we close this?", "sure"]}, {"number": 40462, "title": "passing labels=None to image_dataset_from_directory doesn't work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installed from (source or binary): tf-nightly\r\n- TensorFlow version (use command below): v1.12.1-34068-g9a70ab8813 2.3.0-dev20200614\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\nGives an error:\r\nValueError: `labels` argument should be a list/tuple of integer labels, of the same size as the number of image files in the target directory. If you wish to infer the labels from the subdirectory names in the target directory, pass `labels=\"inferred\"`. If you wish to get a dataset that only contains images (no labels), pass `labels=None`.\r\n\r\n**Describe the expected behavior**\r\nShould return a dataset hat only contains images (like the error message says)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntrain_images = tf.keras.preprocessing.image_dataset_from_directory(\r\n    'images',\r\n    labels=None,\r\n)\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 5, in <module>\r\n    labels=None,\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/image_dataset.py\", line 145, in image_dataset_from_directory\r\n    '`labels` argument should be a list/tuple of integer labels, of '\r\nValueError: `labels` argument should be a list/tuple of integer labels, of the same size as the number of image files in the target directory. If you wish to infer the labels from the subdirectory names in the target directory, pass `labels=\"inferred\"`. If you wish to get a dataset that only contains images (no labels), pass `labels=None`.\r\n```\r\n", "comments": ["Was able to reproduce the issue, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4bb0f4d2439bbc4efc2e9d53a7ce6e4d/40462.ipynb). Thanks!", "Perhaps you can use `label_mode` arg to achieve this.\r\n```python\r\ntrain_images = tf.keras.preprocessing.image_dataset_from_directory(\r\n                 '/content/cats_and_dogs_filtered/train',\r\n                  label_mode = None)\r\n```", "@ymodak I think that's the right way to do it, but would also be nice if labels=None worked, or at least if the error message that comes up tells you the right way to do it. Right now it says \"If you wish to get a dataset that only contains images (no labels), pass `labels=None`\" which doesn't work.", "@mulka Agreed. As the error message suggests\r\n\r\n> ValueError: `labels` argument should be a list/tuple of integer labels, of the same size as the number of image files in the target directory. If you wish to infer the labels from the subdirectory names in the target directory, pass `labels=\"inferred\"`. If you wish to get a dataset that only contains images (no labels), pass `labels=None`.\r\n\r\nAs the error message suggests, passing `labels=None` as an argument should work but its not in this case. ", "This is fixed with the latest tf-nightly. The error message has been updated to,\r\n```python\r\n. . . If you wish to get a dataset that only contains images (no labels), pass `labels_mode=None`.\r\n```\r\nThanks!", "It looks like label_mode has been misspelled as labels_mode.", "Ahh.. I see.. thanks for pointing that out.\nWill fix it now.\n\nOn Sat, Jun 20, 2020, 1:11 PM Kyle Mulka <notifications@github.com> wrote:\n\n> It looks like label_mode has been misspelled as labels_mode.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40462#issuecomment-647040562>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKGNUTLDHWPBOT4IGJNUV73RXUJXPANCNFSM4N5XILBQ>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40462\">No</a>\n", "The issue is still there. The error is ```ValueError: `labels` argument should be a list/tuple of integer labels, of the same size as the number of image files in the target directory. If you wish to infer the labels from the subdirectory names in the target directory, pass `labels=\"inferred\"`. If you wish to get a dataset that only contains images (no labels), pass `label_mode=None`.``` and the solution does not work.", "> \r\n\r\nI've also noticed this, it's still prevalent in TF2.8. To get around this, I have moved the images into an a subdirectory with the name 1, which will later be interpreted as the label of all images.\r\nThen, alter the function call as follows:\r\n\r\n```\r\ntrain_ds = (\r\n    tf.keras.preprocessing.image_dataset_from_directory(\r\n        directory=\"path/to/dir/\", # <-- note, it does not directly point to the '1' folder, but to its parent\r\n        image_size=image_size,\r\n        validation_split=0.3,\r\n        subset=\"training\",\r\n        seed=1337,\r\n    )\r\n    .map(lambda x, y: x) # manually remove the faulty label\r\n```"]}, {"number": 40461, "title": "Tensorflow 2.2.0 import error - DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro [Version 10.0.19041.264]\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.8 (issue also occurs with 3.7.7 and 3.8.3)\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI get the error that I'm missing some dynamic libraries when I try to import tensorflow after installing it. In the error message it is suggested that I download certain C++ dlls from visual studio. I did that and it still didn't work. I also downloaded visual studio code and the same problem persisted. Please note: the error message attached below also occurred with only tensorflow installed, with only tensorflow and tensorflow-gpu installed and finally, with all 3 (tensorflow, tensorflow-gpu, and tensorflow-cpu) installed.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow\r\npip install tensorflow-gpu\r\npip install tensorflow-cpu\r\npython\r\n>>import tensorflow\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[tensorflow import error log.txt](https://github.com/tensorflow/tensorflow/files/4777264/tensorflow.import.error.log.txt)\r\n\r\n", "comments": ["@saharatech \r\nPlease go through [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) and [this issue](#33348) and let us know if it helps.\r\n[Link](https://github.com/tensorflow/tensorflow/issues/39423#issuecomment-628957625)\r\n\r\n", "@Saduf2019 \r\nYes, that helped, thank you very much. My CPU does not support AVX2. I will try building TensorFlow from source. Please feel free to close this issue.", "@saharatech\r\nGlad you were able to resolve the issue, hence moving it to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40461\">No</a>\n"]}, {"number": 40460, "title": "Can tf.keras.utils.normalize be verified by tf.norm?", "body": "\r\nI tried using `tf.keras.utils.normalize` to normalize the MNIST image dataset and then checking the normalized result with `tf.norm` on an arbitrary image (e.g. the 11th in the following sample) with:\r\n\r\n```\r\n(trainX, trainY), (testX, testY) = mnist.load_data()\r\nprint(trainX.shape)\r\nprint(np.amax(trainX[11]))  \r\ntrainX = tf.keras.utils.normalize(trainX,axis=1)\r\nprint(np.amax(trainX[11]))\r\nprint(tf.norm(trainX[11]))\r\n```\r\n\r\nbut it gave me the following result:\r\n\r\n```\r\n(60000, 28, 28)\r\n255\r\n0.88178858555868\r\ntf.Tensor(4.47213595499958, shape=(), dtype=float64)\r\n```\r\n\r\nI thought after normalization the max value of an arbitrary image should be `1` which is actually `0.88178858555868`. On the other hand, the norm of the normalized image should be `1` while it is `4.47213595499958`. Is my understanding wrong? Can anyone help me please?", "comments": ["@kathy-lee This is not the normalization you are expecting. Please check this [stackoverflow answer](https://stackoverflow.com/questions/52571752/what-is-the-purpose-of-keras-utils-normalize) for more information. \r\n\r\nPlease post this kind of support questions (that are not related to bug/performance/feature) in Stackoverflow as there is a large community to support and depending on requirement we will also answer questions there. Thanks!", "@jvishnuvardhan Thanks for your reply. If it is L2 normalization, then after keras.utils.normalize() and tf.norm() we should get 1.\r\nBut apparently it is not. as I wrote above, I got `tf.Tensor(4.47213595499958, shape=(), dtype=float64)`. So I think it's either a bug in the normalization calculation (because it could not be verified by `tf.norm()`), or an error in its [document](https://www.tensorflow.org/api_docs/python/tf/keras/utils/normalize) because it's not really a L2 norm when `order=2`. ", "Please note that `keras.utils.normalize` and `tf.norm` are different. Please post support related questions in Stackoverflow. GitHub is mainly for bugs and performance related issues. \r\n\r\nI am closing this issue. Please feel free to post it in StackOverflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40460\">No</a>\n"]}, {"number": 40459, "title": "ImportError: No module named '_pywrap_tensorflow_internal'", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): i don't know which one; I'm fairly new to this. i installed via the command `pip install --user --upgrade tensorflow` iirc\r\n- TensorFlow version: I am not sure; but its the version that pip currently installs i guess.\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip, as stated above.\r\n- CUDA/cuDNN version: unsure how to find this.\r\n- GPU model and memory: Nvidia GeForce gtx970, 3.5gb (if I'm not mistaken)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI downloaded this repo and attempted to run \"test.py\" after installing all requirements successfully:\r\nhttps://github.com/ialhashim/DenseDepth\r\n\r\ni get an error message:\r\n`ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'`\r\n\r\n\r\n\r\n**Any other info / logs**\r\nplease see test.py from the above repository:\r\nhttps://github.com/ialhashim/DenseDepth/blob/master/test.py\r\n\r\n***here is my stack trace:***\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Program Files (x86)\\Python38-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 8, in <module>\r\n    from keras.models import load_model\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Program Files (x86)\\Python38-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\aekna\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["@Fennecai,\r\nThe GitHub repo you have linked was tested with TF 1.x. I was able to run the `test.py` file on TF v1.15 without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/612b9aa8e1daeb46b7f99a94f9840ca5/40459.ipynb). Thanks!", "A similar problem is raised for me. If you can help it will be greatful\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-027d7fb732fd> in <module>\r\n----> 1 from keras.layers import Convolution2D\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 from . import losses_utils\r\n      8 from . import metrics_utils\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n----> 1 from .load_backend import epsilon\r\n      2 from .load_backend import set_epsilon\r\n      3 from .load_backend import floatx\r\n      4 from .load_backend import set_floatx\r\n      5 from .load_backend import cast_to_floatx\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\load_backend.py in <module>\r\n     88 elif _BACKEND == 'tensorflow':\r\n     89     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 90     from .tensorflow_backend import *\r\n     91 else:\r\n     92     # Try and load external backend.\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.eager import context\r\n      7 from tensorflow.python.framework import device as tfdev\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help", "@basheerghub,\r\n[This](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) solution from a similar issue should fix the `ImportError`. \r\n\r\nIf you still need further help, please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40458, "title": "d", "body": "d", "comments": ["@Lucysmith8 What does\r\n`How can I use tensorflow as a local module?` mean? Can you please elaborate. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Lucysmith8 I am afraid to say that you cant do that. You definitely have to import tensorflow. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40458\">No</a>\n"]}, {"number": 40457, "title": "TF 2.2.0: Error with model.fit; class_weight is only supported for Models with a single output.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using custom loss function\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1, 7.6\r\n- GPU model and memory:  NVIDIA T4 Tensor Core GPU, AWS g4dn.xlarge 16GB\r\n\r\n**Describe the current behavior**\r\n- Error with `model.fit` function, when using `class_weight` as dictionary mapping class indices (integers) to a weight (float) values, for example {1.0: 0.6, 2.0, 0.4}.\r\n- **Traceback**:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 117, in <module>\r\n    main()\r\n  File \"train.py\", line 112, in main\r\n    use_multiprocessing=True\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 815, in fit\r\n    model=self)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1117, in __init__\r\n    dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1621, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3981, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3221, in __init__\r\n    self._function = wrapper_fn.get_concrete_function()\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2532, in get_concrete_function\r\n    *args, **kwargs)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2496, in _get_concrete_function_garbage_collected\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3214, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3156, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 262, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 492, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 346, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1246, in _class_weights_map_fn\r\n    \"`class_weight` is only supported for Models with a single output.\")\r\nValueError: `class_weight` is only supported for Models with a single output.\r\n```\r\n- **My model input and output:**\r\n    - Input: <tf.Tensor 'input.base_1.t1:0' shape=(None, 16) dtype=int32>\r\n    - Output: <tf.Tensor 'output.base_1.t1/Identity:0' shape=(None, 25) dtype=float32>\r\n\r\n\r\n**Describe the expected behavior**\r\n- This was perfectly working with TF 2.1.0 and other previous versions. After upgrading to the 2.2.0 I'm getting this error and no changes was made to model architecture or any other parts of `model.fit` function.\r\n", "comments": ["@spate141 \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40457\">No</a>\n", "@ravikyram Apologies for late reply, please feel free to run this notebook. I've replicated the issue.\r\n\r\nhttps://colab.research.google.com/drive/1nfwR9Q84mcysKqRgGeGJdAkXXr4ToiS9?usp=sharing\r\n", "@spate141 \r\n\r\nRequest you to share supporting files as well to reproduce the issue the issue in our environment.Thanks!", "@ravikyram I'm not sure how to share files in colab notebook. Can you please download the two files from this? \r\n**Train:** https://send.firefox.com/download/f5336cd0534570c6/#_SL-J4dKlCR80cWosghx6w\r\n**Dev:** https://send.firefox.com/download/5687c582c5eb6853/#FmSKbwDMOxcxfdvfxFN67g", "I have tried in colab with TF -GPU 2.2.0,nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/63a31782e5a6ddae8676d888c3e6e42a/untitled66.ipynb).Thanks!", "@omalleyt12 Just checking in on update. Let me know if you need any other information. Cheers!", "I've encountered the same problem, also on version 2.3 and 2.3.1.", "With reference to this [update](https://blog.mozilla.org/blog/2020/09/17/update-on-firefox-send-and-firefox-notes/), I am unable to download the the two files mentioned [above](https://github.com/tensorflow/tensorflow/issues/40457#issuecomment-652402934).\r\n@spate141 can you please guide?\r\nThanks ", "@geetachavan1 You can download the files from here: https://drive.google.com/drive/folders/1LzqVyq8KZibB7jFho5jhlXardStwINmS?usp=sharing", "@spate141 Thanks. I was able to reproduce the issue in 2.4.0-rc1. Here is the [gist](https://colab.research.google.com/gist/geetachavan1/b155fac322e766ed40002deb336519c4/untitled66.ipynb) ", "It seems that passing an array instead of the (documentation and tutorial-recommended) dictionary lets the code run, but on viewing the [implementations](https://github.com/keras-team/keras/blob/d89afdfd82e6e27b850d910890f4a4059ddea331/keras/engine/training.py#L1392) it looks like if not isinstance(class_weight, dict) (and sample weights aren't used) then the standardise_user_weights function just returns [1s]. This seems like a pretty major silent failure\r\n\r\n\r\ntensorflow-gpu version 2.1 (sadly couldn't get 2.3 working with python 3.6 conda env so don't know if it's fixed).\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as kl\r\n\r\nmodel = tf.keras.Sequential(kl.Dense(4))\r\n\r\noptim = tf.keras.optimizers.Adam(\r\n    learning_rate=0.001,\r\n)\r\n\r\nloss = tf.keras.losses.CategoricalCrossentropy(\r\n    from_logits=True,\r\n)\r\n\r\nclass_weights = {\r\n\t0: 1., 1: 0.5, 2: 0.8, 3: 1.3,\r\n}\r\n\r\nxs = tf.zeros((16, 5, 5, 1))  # img\r\nys = tf.zeros((16, 5, 5, 4))  # one hot label\r\n\r\nmodel.compile(optim, loss)\r\n\r\n# Runs but fails silently\r\nmodel.fit(xs, ys, class_weight=class_weights.values())\r\n# Broken for n_classes> 1 (but is written as-documented)\r\nmodel.fit(xs, ys, class_weight=class_weights)\r\n```", "I've encountered the same problem, also on version 2.5. So, Is there a solution to this problem\uff1f", "I would like to look into whether this needs 'fixing', but I haven't had the chance. I resolved it myself by writing a [custom loss function](https://github.com/j-bernardi/bayesian-label-smoothing/blob/main/losses/custom_loss.py) that applies class weights (and label smoothing, but probably ignore that part).\r\n\r\nUse as a workaround for the time-being at your own risk - I tested it a bit but obviously not preferable to actual TF functions!", "Thanks for you reply.\n\n\nBut I have another question when i writing a custom loss function that applies class weights \n\n\nwhich needs some parameter. for example, my custom loss function defined as follows:\n\n\ndef myLoss(originalLossFunc, weightsList):\ndef lossFunc(y_true, y_pred):\n        axis = -1  # if channels last\nclassSelectors = K.argmax(y_true, axis=axis)\n        classSelectors = tf.cast(classSelectors, tf.int32)\n\n        classSelectors = [K.equal(i, classSelectors) for i in range(len(weightsList))]\n\n        classSelectors = [K.cast(x, K.floatx()) for x in classSelectors]\n\n        weights = [sel * w for sel, w in zip(classSelectors, weightsList)]\n\n        weightMultiplier = weights[0]\nfor i in range(1, len(weights)):\n            weightMultiplier = weightMultiplier + weights[i]\n\n        loss = originalLossFunc(y_true, y_pred)\n        loss = loss * weightMultiplier\nreturn loss\n\nreturn lossFunc\nThe value of attention is my method needs two parameters(originalLossFunc, weightsList), when model is training, the LOSS which i set as follows:\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=myConfig.learning_rate),\nloss=myLoss,)\nand there is no problems during training.\n\n\nWhen testing the model, although I set load_model as follows:\nmodel = tf.keras.models.load_model(predict_model_file_path, custom_objects={\"myLoss\":myLoss})\nan error still occurred, which is that the definition of lossFunc could not be found.\n\n\nSo, what should i do to fix this error?\n\n\nBest wish!\n\n\n| |\nJiangyu1181\n|\n|\njiangyu1181@163.com\n|\n\u7b7e\u540d\u7531\u7f51\u6613\u90ae\u7bb1\u5927\u5e08\u5b9a\u5236\nOn 12/23/2020 20:11\uff0cJames Bernardi<notifications@github.com> wrote\uff1a\n\nI would like to look into whether this needs 'fixing', but I haven't had the chance. I resolved it myself by writing a custom loss function that applies class weights (and label smoothing, but probably ignore that part).\n\nUse at your own risk - I tested it a bit but obviously not preferable to actual TF functions!\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or unsubscribe.", "@1181zuishuai that seems like a separate issue to the one being discussed in this bug thread, so I'd recommend asking on Stack Overflow or something similar to get a good response. I'd also suggest you look into formatting code in markdown language for ease of reading.\r\ne.g.\r\n\\`\\`\\`python\r\nprint(\"Hello\")\r\n\\`\\`\\` ", "Thanks\n\n\n| |\nJiangyu1181\n|\n|\njiangyu1181@163.com\n|\n\u7b7e\u540d\u7531\u7f51\u6613\u90ae\u7bb1\u5927\u5e08\u5b9a\u5236\nOn 12/24/2020 01:18\uff0cJames Bernardi<notifications@github.com> wrote\uff1a\n\n@1181zuishuai that seems like a separate issue to the one being discussed in this bug thread, so I'd recommend asking on Stack Overflow or something similar to get a good response. I'd also suggest you look into formatting code in markdown language for ease of reading.\ne.g.\n```python\nprint(\"Hello\")\n```\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.", "I have raised this pull request (currently draft) - I would be interested in whether this addresses the issue for some people here.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/46189 ", "@j-bernardi Thanks for the update, appreciate it.", "Have moved #46189 to read-for-review, if anyone here is interested in reviewing\r\n\r\nThe change proposes an extension to the Model class that enforces dense (..., 1) shape or one-hot (... n_classes) shape, when using 3+ dimensional labels. The reason the problem existed for 3+d classes is it's ambiguous as to whether the final label dimension is a one-hot encoding or a dimension belonging to the output, so this change enforces that it's one or the other (without impacting users currently not using the flag)", "Is this resolved? A lot of people with such multi-label/multi-output problems seem to have raised this as a big (and unresolved) issue across many recent forums. ", "@niki-j thanks for the poke, good to hear there's still a need for this feature.\r\n\r\nI have finally cracked the rebase + got the tests running locally again, so the patch set #46189 is still open and ready for review. \r\n\r\nI ask if any budding (or previous) contributors are following this thread and have the time, please take a look at the PR and add your review", "CC: @omalleyt12 Can you please take a look at this? It seems like you're assigned to this thread.", "Hello, I solve this problem by adding two lines to the library, \r\ntensorflow==2.4.0\r\npython==3.8.5\r\nfile: venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\r\n\r\nline: 1292\r\n\r\nbefore change: \r\n```\r\n  def _class_weights_map_fn(*data):\r\n    \"\"\"Convert `class_weight` to `sample_weight`.\"\"\"\r\n    x, y, sw = unpack_x_y_sample_weight(data)\r\n\r\n    if nest.is_nested(y):\r\n      raise ValueError(\r\n          \"`class_weight` is only supported for Models with a single output.\")\r\n```\r\n\r\nafter change: \r\n\r\n```\r\n def _class_weights_map_fn(*data):\r\n    \"\"\"Convert `class_weight` to `sample_weight`.\"\"\"\r\n    x, y, sw = unpack_x_y_sample_weight(data)\r\n    if isinstance(y, dict):\r\n      y = y.popitem()[1]\r\n    if nest.is_nested(y):\r\n      raise ValueError(\r\n          \"`class_weight` is only supported for Models with a single output.\")\r\n```\r\n\r\nbut please do solve this problem permanently!\r\n\r\nNote: I prepared a generator function to input for model.fit and the shape of my labels are {\"output\": 0 } so I got rid of dict and use the values just by adding this two lines:\r\n```\r\n    if isinstance(y, dict):\r\n      y = y.popitem()[1]\r\n```\r\nthanks with love for the brilliant TensorFlow .", "@spate141 \r\nCould you please try on tensorflow==2.4.0\r\npython==3.8.5 and let us know if you still face this issue., You may also try on latest tf version and let us know.", "@Saduf2019 I'm getting same error in `python 3.8.5` and `TF 2.5.0` on my machine. You can try running this notebook on your side of env and you can see the same error. \r\n- Colab: https://colab.research.google.com/drive/1MH6C07QaIChcAnCqtTeo62Jv9ACRglr8?usp=sharing \r\n- Train/Dev Files: https://drive.google.com/drive/folders/1LzqVyq8KZibB7jFho5jhlXardStwINmS?usp=sharing\r\n- Here is the pull request which is still open about this issuse: #46189 ", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40457\">No</a>\n"]}, {"number": 40456, "title": "Model trained and saved on GPU fails at model open on CPU ", "body": "Hi, this does not seem right. I have been training some models on TS-877 Ryzen-based NAS with 8 cores and 16 threads, and a GeForce GTX 1060 6GB graphics card, in a gpu-docker container Tensorflow:2.1.1-gpu-notebook (with CUDA version V10.1.243); then I have saved them as usual with `model.save()`. When I open them inside the container, everything works fine:\r\n\r\n```\r\nnew_model.summary()\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_word_ids (InputLayer)  [(None, 192)]             0         \r\n_________________________________________________________________\r\ntf_distil_bert_model (TFDist multiple                  134734080 \r\n_________________________________________________________________\r\ntf_op_layer_strided_slice (T multiple                  0         \r\n_________________________________________________________________\r\ndense (Dense)                multiple                  769       \r\n=================================================================\r\nTotal params: 134,734,849\r\nTrainable params: 134,734,849\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n**Describe the current behavior**\r\nHowever, when I try to open this model in my computer, it fails tremendously, the summary shows all empty:\r\n```\r\n2020-06-14 21:38:07.495770: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-14 21:38:07.509869: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb6ff0417a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-14 21:38:07.509888: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nAnd trying to get any prediction yields all this:\r\n```\r\n2020-06-14 21:38:50.852268: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nTraceback (most recent call last):\r\n  File \"open-model.py\", line 16, in <module>\r\n    model.save(f'models/CPU_distilbert_batch16_epochs3_maxlen192') \r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1052, in save\r\n    signatures, options)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 138, in save_model\r\n    signatures, options)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1027, in _build_meta_graph\r\n    options.namespace_whitelist)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 629, in _fill_meta_graph_def\r\n    signatures = _generate_signatures(signature_functions, resource_map)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 497, in _generate_signatures\r\n    function, mapped_inputs, resource_map)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 449, in _call_function_with_mapped_captures\r\n    resource_map)\r\n  File \"/Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 372, in _map_captures_to_created_tensors\r\n    ).format(interior))\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"26682:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\nWhich is really odd. How can I load and do inference, predictions on a CPU machine with a model trained on GPU on another machine?\r\n\r\n**Describe the expected behavior**\r\nI expected to be able to open the trained model and make predictions.\r\n\r\n**System information where I try to OPEN the model**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, I followed the examples of Tensorflow docs.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.5 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: Python 3.6.4\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: AMD Radeon Pro 580 (but not used)\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\n\r\nprint(tf.version.VERSION)\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\nmodel = tf.keras.models.load_model(f'models/distilbert_batch16_epochs3_maxlen192')\r\nprint(model.summary())\r\n\r\nwith tf.device('cpu:0'):\r\n    model.save(f'models/CPU_distilbert_batch16_epochs3_maxlen192') \r\n\r\nwith tf.device('cpu:0'):\r\n    new_model = tf.keras.models.load_model(f'models/CPU_distilbert_batch16_epochs3_maxlen192')\r\nprint(new_model.summary())\r\n```\r\n\r\nI cannot upload the model itself because its more than 1 GB, but the summary is shown above.\r\n", "comments": ["@margaritageleta,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to generate the model or upload the files to Google Drive and share the link with us. Thanks!", "Yesterday I had a crazy night trying to figure out what was going on. And I found it. @amahendrakar the versions of tensorflow in the machine where I trained the model and where I tried to retrieve the model were diferent, I switched to an older version (2.1.0) and it worked, yet it was really weird. "]}, {"number": 40455, "title": "Fix and Refactor NonAVX512 CPU platform graceful exit for BFloat16 Ops with eigen fallback", "body": "Attn: @penpornk \r\nThe Bug in PR https://github.com/tensorflow/tensorflow/pull/40212 which addresses NONAVX512 CPU Platform graceful exit, by removing the check for (T == DT_BFLOAT16) is fixed and refactored for clear understanding.\r\n\r\n### **Problem:** \r\n( in https://github.com/tensorflow/tensorflow/pull/40212 ) Removed from review comments\r\n`return (T == DT_FLOAT || (T == DT_BFLOAT16 && CheckBfloat16Support(T)));`\r\nto\r\n`return (T == DT_FLOAT || CheckBfloat16Support(T));`\r\ncreated lot of failures internally.\r\n\r\n\r\n### **Fix**\r\n1. Put back the datatype check. `T == DT_BFLOAT16`\r\n2. Refactored the code for clear understanding by making the function not Datatype dependent. `IsBF16SupportedByOneDNNOnThisCPU()`\r\n3. For unsupported mkl bfloat16 ops, we fall back to Eigen otherwise\r\n4. Created separate function for the warning `BF16UnsupportedWarning()`\r\n\r\n", "comments": []}, {"number": 40454, "title": "predict method of tf.estimator is very slow", "body": "May be this is already known thing for others and this can be a duplicate issue. But could not find relevant information about this. Please let me know if there is way to deal with this. \r\n\r\nThanks. \r\n\r\n\r\n\r\n\r\n<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@minesh1291 \r\nPlease share simple stand alone code and version of tf where issue is faced for us to replicate it in pout environment.", "Sorry could not reproduce the issue. Things are working fine now. May be I had problem with input_fn. Thanks. @Saduf2019 "]}, {"number": 40453, "title": "model.fit() is 10% slower over custom training loops when using TPUs", "body": "I've created two training loops for training models on TPUs. One using the model.fit() method and one using a custom training loop based on information I've found in various notebooks.\r\n\r\nThe custom training loop consistently beats model.fit() by 10-15%.\r\n\r\nSome overhead for using model.fit() over custom code is of course acceptable but 10-15% seem excessive. Maybe there's a bug somewhere?\r\n\r\nHere are the relevant parts of my custom training loop:\r\n\r\n```python\r\nclass LossAccObserver:\r\n    def __init__(self):\r\n        self.loss = metrics.SparseCategoricalCrossentropy()\r\n        self.acc = metrics.SparseCategoricalAccuracy()\r\n    def reset(self):\r\n        self.loss.reset_states()\r\n        self.acc.reset_states()\r\n    def update(self, y, y_hat):\r\n        self.loss.update_state(y, y_hat)\r\n        self.acc.update_state(y, y_hat)\r\n\r\ndef compute_and_apply_gradients(model, x, y):\r\n    with tf.GradientTape() as tape:\r\n        y_hat = model(x, training = True)\r\n        loss = model.compiled_loss(y, y_hat,\r\n                                   regularization_losses = model.losses)\r\n    vars = model.trainable_variables\r\n    grads = tape.gradient(loss, vars)\r\n    grads = [tf.clip_by_norm(g, 0.5) for g in grads]\r\n    model.optimizer.apply_gradients(zip(grads, vars))\r\n    return y_hat\r\n\r\n@tf.function\r\ndef train_epoch(model, strategy, batch_size, dataset, obs):\r\n    def step_fn(x, y):\r\n        y_hat = compute_and_apply_gradients(model, x, y)\r\n        obs.update(y, y_hat)\r\n    for x, y in dataset:\r\n        strategy.run(step_fn, args = (x, y))\r\n\r\n@tf.function\r\ndef evaluate_epoch(model, strategy, dataset, obs):\r\n    def step_fn(x, y):\r\n        y_hat = model(x, training = False)\r\n        obs.update(y, y_hat)\r\n    for x, y in dataset:\r\n        strategy.run(step_fn, args = (x, y))\r\n\r\ndef manual_training(model, strategy, train, valid, batch_size, epochs):\r\n    with strategy.scope():\r\n        train_obs = LossAccObserver()\r\n        valid_obs = LossAccObserver()\r\n    ...\r\n    for i in range(epochs):\r\n        ...\r\n        train_epoch(model, strategy, batch_size, train, train_obs)\r\n        evaluate_epoch(model, strategy, valid, valid_obs)\r\n        ...\r\n```\r\nHere is my model.fit() training code:\r\n```python\r\nclass MyModel(Model):\r\n    def train_step(self, data):\r\n        x, y = data\r\n        y_hat = compute_and_apply_gradients(self, x, y)\r\n        self.compiled_metrics.update_state(y, y_hat)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\ndef automatic_training(model, train, valid, batch_size, epochs):\r\n    ...\r\n    model.fit(x = train, validation_data = valid,\r\n              epochs = epochs,\r\n              verbose = 2)\r\n```\r\n", "comments": ["@bjourne \r\n\r\nCan you please share colab link or complete code snippet to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Sure! Here is my code: https://github.com/bjourne/python3-libs/blob/master/programs/char_lm_tf.py ", "@bjourne \r\n\r\nI have tried in colab with TF version 2.2 and  i am seeing the below error message(`ModuleNotFoundError: No module named 'observations'`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5c0d2e9e5b178e3a8151ea015411afa1/untitled24.ipynb).Please, help me in reproducing the issue.Thanks!", "It's a dependency. You need to `pip install observations docopt` to be able to run the code.", "@bjourne \r\n\r\nI tried again in colab with your suggestion and i am seeing new error message.(`DocoptLanguageError: \"usage:\" (case-insensitive) not found.`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/7a7a1493ff7ded74db66c9444328b8ee/untitled975.ipynb).Thanks!", "You have not copied the docstring correctly. You could also modify the main function to hard code the parameters I'm reading from the command line.", "@bjourne Can you please hardcode the parameters and share the working gist with us. Thanks!", "https://gist.github.com/bjourne/486bad6b28320735c2283f03facd4a99", "There are some known inefficiencies in how the model.fit training loop works, in particular around how metrics are calculated.\r\n\r\nThese are being worked on, but for the time being a small performance loss should be expected. ", "HI Bjorn,\r\n\r\nIf you are using `model.fit`, there is an `experimental_steps_per_execution` argument you can tune in `model.compile` https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#compile. Setting to a higher number (like your epoch size) should help the performance.\r\n", "@bjourne were you able to rerun with `experimental_steps_per_execution` as noted above by rxsang@. \r\nPlease let us know if that helped and if we can go ahead and close the issue! Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40452, "title": "Error popping up", "body": "Pl. pin-point the line in the code from which the error is coming?\r\n\r\nclass ReconstructingRegressor(keras.models.Model):\r\n    def __init__(self, output_dim, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.flatten1 = keras.layers.Flatten(input_shape=[109,109])\r\n        self.hidden = [keras.layers.Dense(30, activation=\"relu\",\r\n                                          kernel_initializer=\"lecun_normal\")\r\n                       for _ in range(5)]\r\n        self.out = keras.layers.Dense(output_dim,activation=\"softmax\")\r\n\r\n    def build(self, batch_input_shape):\r\n       \r\n        n_inputs = batch_input_shape[-1]\r\n        print(n_inputs)\r\n        self.reconstruct = keras.layers.Dense(n_inputs)\r\n        super().build(batch_input_shape)\r\n\r\n    def call(self, inputs, training=None):\r\n        Z=self.flatten1(inputs)\r\n        \r\n        for layer in self.hidden:\r\n            Z = layer(Z)\r\n         \r\n        reconstruction = self.reconstruct(Z)\r\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\r\n        self.add_loss(0.05 * recon_loss)\r\n        #if training:\r\n        #    result = self.reconstruction_mean(recon_loss)\r\n        #    self.add_metric(result)\r\n        return self.out(Z)\r\nmodel = ReconstructingRegressor(10)\r\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(), metrics=[\"accuracy\"])\r\nhistory = model.fit(avast, housing_labels, epochs=100,validation_split=0.1) \r\n\r\n\r\n\r\nError- InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=1629497199368); accessed from: FuncGraph(name=train_function, id=1629516591688).", "comments": ["@pushkarkishore,\r\nOn running the code I am facing an error stating `NameError: name 'avast' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f7e3c20519d747ab1a773f8def3c1206/40452.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "> Pl. pin-point the line in the code from which the error is coming?\r\n> \r\n> class ReconstructingRegressor(keras.models.Model):\r\n> def **init**(self, output_dim, **kwargs):\r\n> super().**init**(**kwargs)\r\n> self.flatten1 = keras.layers.Flatten(input_shape=[109,109])\r\n> self.hidden = [keras.layers.Dense(30, activation=\"relu\",\r\n> kernel_initializer=\"lecun_normal\")\r\n> for _ in range(5)]\r\n> self.out = keras.layers.Dense(output_dim,activation=\"softmax\")\r\n> \r\n> ```\r\n> def build(self, batch_input_shape):\r\n>    \r\n>     n_inputs = batch_input_shape[-1]\r\n>     print(n_inputs)\r\n>     self.reconstruct = keras.layers.Dense(n_inputs)\r\n>     super().build(batch_input_shape)\r\n> \r\n> def call(self, inputs, training=None):\r\n>     Z=self.flatten1(inputs)\r\n>     \r\n>     for layer in self.hidden:\r\n>         Z = layer(Z)\r\n>      \r\n>     reconstruction = self.reconstruct(Z)\r\n>     recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\r\n>     self.add_loss(0.05 * recon_loss)\r\n>     #if training:\r\n>     #    result = self.reconstruction_mean(recon_loss)\r\n>     #    self.add_metric(result)\r\n>     return self.out(Z)\r\n> ```\r\n> \r\n> model = ReconstructingRegressor(10)\r\n> model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(), metrics=[\"accuracy\"])\r\n> history = model.fit(avast, housing_labels, epochs=100,validation_split=0.1)\r\n> \r\n> Error- InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=1629497199368); accessed from: FuncGraph(name=train_function, id=1629516591688).\r\n\r\nUnfortunately, the methods of a nested class cannot directly access the instance attributes of the outer class. Here you are getting an error because of tf.reduce_mean and tf.square functions are out of scope.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40451, "title": "Add GCS Path Parser", "body": "@mihaimaruseac \r\nThis is a helper function for GCS. This is a small PR but it has a new dependency so I think it is better that we split it to a seperate PR", "comments": []}, {"number": 40450, "title": "Fix wrong comments", "body": "Reference\r\n[Probot: Stale](https://github.com/probot/stale)", "comments": []}, {"number": 40449, "title": "Document usage of the profiling_output_csv_file option", "body": "Previously undocumented flag which is useful when profiling operations", "comments": []}, {"number": 40448, "title": "tf_ops.cc takes 79s to compile / 4300+ lines long", "body": "This is with TensorFlow at commit 80768cb23a3a4314c52af0b48a6bcf23ca541e19. The file \r\ntensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc is about 4300 lines long and takes nearly 79s by itself to compile on a fast workstation (Intel Skylake-based Core i7 8700K 3.70GHz) with a typical bazel config below. It'll be great to split this file into two. \r\n```\r\nbazel build --linkopt=\"-fuse-ld=lld\" -j 11    //tensorflow/compiler/mlir:tf-opt\r\ngcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)\r\nOn an Fedora Core 31 x86-64 Linux, Intel Core i7 8700K 3.70 GHz, 32 GB DDR4 RAM.\r\n```\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/compiler/mlir:tf-opt (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/compiler/mlir:tf-opt up-to-date:\r\n  bazel-bin/tensorflow/compiler/mlir/tf-opt\r\nINFO: Elapsed time: 79.099s, Critical Path: 78.93s\r\nINFO: 2 processes: 2 local.\r\nINFO: Build completed successfully, 3 total actions\r\n```\r\n\r\nTo reproduce, please change tf_ops.cc and rebuild tf-opt as shown above. The `linkopt` shouldn't make a difference here.\r\n", "comments": ["@bondhugula \r\nPlease share platform details on which this issue is faced.", "> @bondhugula\r\n> Please share platform details on which this issue is faced.\r\n\r\nDone - added to the original post.", "There is b/158483489 already tracking this internally", "@bondhugula could you see if the recent change has improved this?", "> @bondhugula could you see if the recent change has improved this?\r\n\r\nI don't have the exact same machine, but it's taking 48s now on a machine with similar single thread perf; so it's about 60% faster now. Thanks! (But 48s is still too high! Time for multithreaded compilers! :-))\r\n", "We are working on that ;-) But the good thing is I could split it again (we'll hit point of diminishing returns quickly, but splitting is much easier vs the first split) and check.\r\n\r\nI'll close for now as speeding up can be a bit unbounded as a task.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40448\">No</a>\n"]}, {"number": 40447, "title": "[Using tf.function()] ValueError: Input 0 of layer dense is incompatible with the layer", "body": "**Set up:** Using tensorflow 2.2.0 on python 3.7. \r\n\r\n**Background**: I'm just trying to get the **concrete function** of a simple `tf.keras.Model` to save it using SavedModel and use it later, yet there is no example of doing so in the documentation. Trying to perform this _simple task_ following the documentation gives the following problem: \r\n\r\n**Issue (possibly bug)**\r\nI'm trying to run a very simple concrete function [based on the documentation](https://www.tensorflow.org/guide/concrete_function#accessing_concrete_function): \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.Input(shape=(3,))\r\nx = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\r\noutputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nx = tf.constant([[1.0,2.0,3.0], [1.0,2.0,3.0]])\r\nmodel.predict(x) \r\n\r\n# outputs: array([[0.19964378, 0.23859118, 0.17491119, 0.15458305, 0.23227075],\r\n#       [0.19964378, 0.23859118, 0.17491119, 0.15458305, 0.23227075]],\r\n#      dtype=float32)\r\n# OK!\r\n```\r\n\r\nNow using tf.function: \r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)])\r\ndef predict(x):\r\n  return model.predict(x)\r\np = predict.get_concrete_function()\r\n\r\n# TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n```\r\n\r\nWithout `None` : \r\n\r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(3,), dtype=tf.float32)])\r\ndef predict(x):\r\n  return model.predict(x)\r\np = predict.get_concrete_function()\r\n\r\n# ValueError: Input 0 of layer dense is incompatible with the layer: \r\n# expected axis -1 of input shape to have value 3 but received input with shape [None, 1]\r\n```\r\n\r\nFinally, without declaring the input attribute on the `tf.function`:\r\n```python\r\n@tf.function\r\ndef predict(x):\r\n  return model.predict(x)\r\n\r\np = predict.get_concrete_function(x=tf.TensorSpec(shape=(1,3,), dtype=tf.float32))\r\n#  AttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n_(Among many other permutations, none o which works, because either it's a bug, or somehow you've made this thing unusably  complicated, or I'm missing something)_\r\n", "comments": ["I have tried in colab with TF 2.2 and was able to reproduce the issue. But with TF nightly version(`2.3.0-dev20200614`) i am seeing  different error `RuntimeError`.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ab008de764138bbcb0a038c92f7d5f05/untitled22.ipynb).Thanks!", "@ravikyram thanks for the collab file. \r\nSo, my question is if I'm doing it wrong or is it a bug. I tried following as closely as I could the way it is instructed on the docs, but they are quite confusing.\r\n\r\nMy goal is just to get the concrete function of the model's `predict` method, so I can save it using `SavedModel` and use it elsewhere. ", "I see that on nightly the error message is more useful:\r\n\r\n```\r\nRuntimeError: Detected a call to `Model.predict` inside a `tf.function`. `\r\nModel.predict is a high-level endpoint that manages its own `tf.function`. \r\nPlease move the call to `Model.predict` outside of all enclosing `tf.function`s. \r\nNote that you can call a `Model` directly on `Tensor`s inside a `tf.function` like: `model(x)`.\r\n```\r\n\r\nSo, when I did: \r\n\r\n```python\r\n# Using Model instead of Model.predict\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, 3), dtype=tf.float32)])\r\ndef predict(x):\r\n  return model(x)\r\np = predict.get_concrete_function()\r\n\r\n# Test\r\nx = tf.constant([[1.0,2.0,3.0], [1.0,2.0,3.0]])\r\np(x) \r\n\r\n# out -> <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\r\n# array([[0.104079  , 0.3223787 , 0.28109816, 0.18215933, 0.11028478],\r\n#       [0.104079  , 0.3223787 , 0.28109816, 0.18215933, 0.11028478]],\r\n#      dtype=float32)>\r\n```\r\nIt actually worked! For some reason, you can't call `model.predict(x)` inside a `tf.function`, but you can call `model(x)` inside a tf.function (they should be equivalent... oh well)\r\n\r\n\r\n\r\n\r\n", "@dhiegomaga Sorry for the delay in my response. \r\n\r\nThe decorator `tf.function` will automatically transform the code in a function into the graph-style code for execution. As mentioned in the above `RuntimeError`,  under the hoods, `model.predict` manages its own execution graph. decorating with   another `tf.function` results in internal error. \r\n\r\nI understand you already resolved this issue. If this is not an issue, please feel free to close the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40447\">No</a>\n"]}, {"number": 40446, "title": " _load(spec) ImportError: DLL load failed while importing _pywrap_tensorflow_internal", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): calling HMR\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): https://files.pythonhosted.org/packages/af/50/d7da24189d95e2084bb1cc350a8e4acdf1b0c9b3d57def7a348f0d9cb062/tensorflow-2.2.0-cp37-cp37m-win_amd64.whl\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: nvidia quadro fx 570\r\n- CPU: Intel Xeon x5450\r\n\r\n\r\n**Describe the current behavior**\r\n\"import tensorflow as tf\" fails \r\n**Describe the expected behavior**\r\n\"import tensorflow as tf\" doesn't fail \r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n**Other info / logs** \r\n$ python -m export --img_dir C:\\Users\\dell\\Videos\\temp\\images_data --json_dir C:\\Users\\dell\\Videos\\temp\\openpose_data --log_dir C:\\Users\\dell\\Videos --output_path C:\\Users\\dell\\Videos\\temp\\hmr_data.json\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Une routine d\u2592initialisation d\u2592une biblioth\u2592que de liens dynamiques (DLL) a \u2592chou\u2592.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"D:\\DEV\\video2mocap\\3rdparty\\hmr\\export.py\", line 32, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Une routine d\u2592initialisation d\u2592une biblioth\u2592que de liens dynamiques (DLL) a \u2592chou\u2592.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@pndiaye,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it helps. Thanks!", "Hi @amahendrakar I have indeed insalled vc_redist and can find msvc140_1.dll and my cpu does support AVX. my version of python 3.7 is 64 bits too.\r\nI have looked in the \"C:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\tensorflow\\python\" directory and found \"_pywrap_tensorflow_internal.lib\" from the error it seems a \"dll\" is expected?!\r\nthanks for your help", "ps: I can reproduce this issue with tensorflow cpu 1.13.1  and 2.1.0", "TF 1.13 is not supported anymore", "@pndiaye Looks like your cpu make does not support AVX instruction set.\r\nSee https://ark.intel.com/content/www/us/en/ark/products/34446/intel-xeon-processor-x5450-12m-cache-3-00-ghz-1333-mhz-fsb.html\r\n\r\nAlso I recommend step 2. Installing TensorFlow (TF) CPU prebuilt binaries from comments upstream.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40446\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40446\">No</a>\n"]}, {"number": 40445, "title": "Keras Reshae layer doesn't support in tf 2.2 ?", "body": "\r\n", "comments": ["Reshape layer", "@cuge1995 \r\nPlease share simple stand alone code where issue is faced while using the reshape layer for us to analyse the issue faced.\r\nAlso share the tf version on which the issue us faced.", "# input_Transformation_net\r\ninput_points = Input(shape=(num_points, 3))\r\nx = Convolution1D(64, 1, activation='relu',\r\n                  input_shape=(num_points, 3))(input_points)\r\nx = BatchNormalization()(x)\r\nx = Convolution1D(128, 1, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = Convolution1D(1024, 1, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = MaxPooling1D(pool_size=num_points)(x)\r\nx = Dense(512, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = Dense(256, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\r\ninput_T = Reshape((3, 3))(x)\r\n\r\n# forward net\r\ng = Lambda(mat_mul, arguments={'B': input_T})(input_points)\r\ng = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\r\ng = BatchNormalization()(g)\r\ng = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\r\ng = BatchNormalization()(g)\r\n\r\n\r\nsuch keras code not running in tf 2.2", "@cuge1995 \r\nI ran the code shared above and face a different error, please share all dependencies and executable code for us to replicate.\r\nPlease find the [gist here](https://colab.research.google.com/gist/Saduf2019/57204907eb11a520ea77b89b01bdda2a/untitled231.ipynb)", "\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nfrom keras import optimizers\r\nfrom keras.layers import Input\r\nfrom keras.models import Model\r\nfrom keras.layers import Dense, Flatten, Reshape, Dropout\r\nfrom keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\r\nfrom keras.layers import Lambda\r\nfrom keras.utils import np_utils\r\nimport h5py\r\n\r\n\r\ndef mat_mul(A, B):\r\n    return tf.matmul(A, B)\r\n\r\n\r\ndef load_h5(h5_filename):\r\n    f = h5py.File(h5_filename)\r\n    data = f['data'][:]\r\n    label = f['label'][:]\r\n    return (data, label)\r\n\r\n\r\ndef rotate_point_cloud(batch_data):\r\n    \"\"\" Randomly rotate the point clouds to augument the dataset\r\n        rotation is per shape based along up direction\r\n        Input:\r\n          BxNx3 array, original batch of point clouds\r\n        Return:\r\n          BxNx3 array, rotated batch of point clouds\r\n    \"\"\"\r\n    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\r\n    for k in range(batch_data.shape[0]):\r\n        rotation_angle = np.random.uniform() * 2 * np.pi\r\n        cosval = np.cos(rotation_angle)\r\n        sinval = np.sin(rotation_angle)\r\n        rotation_matrix = np.array([[cosval, 0, sinval],\r\n                                    [0, 1, 0],\r\n                                    [-sinval, 0, cosval]])\r\n        shape_pc = batch_data[k, ...]\r\n        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\r\n    return rotated_data\r\n\r\n\r\ndef jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\r\n    \"\"\" Randomly jitter points. jittering is per point.\r\n        Input:\r\n          BxNx3 array, original batch of point clouds\r\n        Return:\r\n          BxNx3 array, jittered batch of point clouds\r\n    \"\"\"\r\n    B, N, C = batch_data.shape\r\n    assert(clip > 0)\r\n    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)\r\n    jittered_data += batch_data\r\n    return jittered_data\r\n\r\n\r\n\r\nnum_points = 2048\r\n\r\n\r\nk = 40\r\n\r\n\r\nadam = optimizers.Adam(lr=0.001, decay=0.7)\r\n\r\n\r\ninput_points = Input(shape=(num_points, 3))\r\nx = Convolution1D(64, 1, activation='relu',\r\n                  input_shape=(num_points, 3))(input_points)\r\nx = BatchNormalization()(x)\r\nx = Convolution1D(128, 1, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = Convolution1D(1024, 1, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = MaxPooling1D(pool_size=num_points)(x)\r\nx = Dense(512, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = Dense(256, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\nx = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\r\ninput_T = Reshape((3, 3))(x)\r\n\r\ng = Lambda(mat_mul, arguments={'B': input_T})(input_points)\r\ng = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\r\ng = BatchNormalization()(g)\r\ng = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\r\ng = BatchNormalization()(g)\r\n\r\n\r\nf = Convolution1D(64, 1, activation='relu')(g)\r\nf = BatchNormalization()(f)\r\nf = Convolution1D(128, 1, activation='relu')(f)\r\nf = BatchNormalization()(f)\r\nf = Convolution1D(1024, 1, activation='relu')(f)\r\nf = BatchNormalization()(f)\r\nf = MaxPooling1D(pool_size=num_points)(f)\r\nf = Dense(512, activation='relu')(f)\r\nf = BatchNormalization()(f)\r\nf = Dense(256, activation='relu')(f)\r\nf = BatchNormalization()(f)\r\nf = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\r\nfeature_T = Reshape((64, 64))(f)\r\n\r\ng = Lambda(mat_mul, arguments={'B': feature_T})(g)\r\ng = Convolution1D(64, 1, activation='relu')(g)\r\ng = BatchNormalization()(g)\r\ng = Convolution1D(128, 1, activation='relu')(g)\r\ng = BatchNormalization()(g)\r\ng = Convolution1D(1024, 1, activation='relu')(g)\r\ng = BatchNormalization()(g)\r\n\r\nglobal_feature = MaxPooling1D(pool_size=num_points)(g)\r\n\r\n\r\nc = Dense(512, activation='relu')(global_feature)\r\nc = BatchNormalization()(c)\r\nc = Dropout(rate=0.7)(c)\r\nc = Dense(256, activation='relu')(c)\r\nc = BatchNormalization()(c)\r\nc = Dropout(rate=0.7)(c)\r\nc = Dense(k, activation='softmax')(c)\r\nprediction = Flatten()(c)\r\n\r\nmodel = Model(inputs=input_points, outputs=prediction)\r\nprint(model.summary())\r\n", "@\r\nI ran the above code and fixed as many indentation and syntax error as i could but there are way to many to fix, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f1f7701ca7f4db0ad099d2aae2bb38b8/untitled231.ipynb).\r\nPlease provide executable code such that it replicates the issue faced or if possible please share a colab gist for us to analyse.", "https://github.com/garyli1019/pointnet-keras", "the train_cls.py file", "I can build the model just using keras, but not work when I using tf.keras", "@cuge1995 Can you please update your code in the [colab gist](https://colab.research.google.com/gist/jvishnuvardhan/c5e3ce3fb15aa0a1bba4efbdf36d44e7/-40445.ipynb) I prepared from your repo. Next time, please try to provide a standalone code so that the resolution will be faster. Thanks!", "thanks, but this model still cannot run\r\n\r\n\r\n_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'reshape_2/Identity:0' shape=(None, 3, 3) dtype=float32>, <tf.Tensor 'reshape_3/Identity:0' shape=(None, 64, 64) dtype=float32>]\r\n", "you can look at this kaggle kernel, which contain the dataset\r\nhttps://www.kaggle.com/cuge1995/tpu-2-transformers\r\n\r\n", "??", "@cuge1995 Can you please update the colab or create a simple standalone code to reproduce the issue? Kaggle's kernel link throws '404' error. But, please provide a standalone code to to reproduce the issue. You could use any public data. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40445\">No</a>\n"]}, {"number": 40444, "title": "Create a function to compute positive negative and neutral tweets. ", "body": "\r\n# Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\r\ndef getAnalysis(score):\r\nif score < 0:\r\n  return 'Negative'\r\nelif score == 0:\r\n  return 'Neutral'\r\nelse:\r\n  return 'Positive'\r\ndf['Analysis'] = df['Polarity'].apply(getAnalysis)\r\n# Show the dataframe\r\ndf\r\n\r\nThis program doesn't creates analysis column.", "comments": ["@AasthaNagpal \r\n\r\nThis issue is not related to Tensorflow.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request related to Tensorflow. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40444\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40444\">No</a>\n"]}, {"number": 40443, "title": "**stop_if_no_decrease_hook** did not work during training?", "body": "```\r\n    early_stopping_hook = tf.estimator.experimental.stop_if_no_decrease_hook(\r\n                                deepfm_model,\r\n                                metric_name='loss',\r\n                                max_steps_without_decrease=13,\r\n                                min_steps=5)\r\n    training_hooks.append(early_stopping_hook)\r\n\r\n    tf.estimator.train_and_evaluate(\r\n        estimator=deepfm_model,\r\n        train_spec=tf.estimator.TrainSpec(train_input_fn, hooks=training_hooks),\r\n        eval_spec=tf.estimator.EvalSpec(eval_input_fn, steps=100)#, exporters=best_exporter)\r\n    )\r\n```\r\n\r\nUsing the code, as the params define, the evaluation should be processed every 13 training steps and the training process should be ended if the loss did not decrease. \r\n\r\nBut it keep training and did not do evaluation every 13 training steps.\r\n\r\n", "comments": ["@alansplaza,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40442, "title": "image normalization preprocess in Tensorflow Lite iOS object detection examples", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iphoneX\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):no\r\n- CUDA/cuDNN version: not related, run on cpu \r\n- GPU model and memory:  not related, run on cpu \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHi, can anyone help to confirm which one is correct for the object detection model used in example(coco_mobilenet_ssd_v1)?\r\n\r\nfrom the Tensorflow Lite IOS object_detection example, it use \"x / 255.0\" to normalize image for preprocess.\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/ios/ObjectDetection/ModelDataHandler/ModelDataHandler.swift#L319\r\n\r\n    // Not quantized, convert to floats\r\n    let bytes = Array<UInt8>(unsafeData: byteData)!\r\n    var floats = [Float]()\r\n    for i in 0..<bytes.count {\r\n      floats.append(Float(bytes[i]) / 255.0)\r\n    }\r\n    return Data(copyingBufferOf: floats)\r\n\r\nwhile in the Tensorflow Lite Android object_detection example, it use \"(x -128.0) / 128.0\" \r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L171\r\n\r\n  // Float model\r\n  private static final float IMAGE_MEAN = 128.0f;\r\n  private static final float IMAGE_STD = 128.0f;\r\n...\r\n          imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n\r\nwhile from the ssd mobilenet v1 feature extractor , it use \"(2.0 / 255.0) * resized_inputs - 1.0\" which is different from the above two \r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L78\r\n\r\n350   def preprocess(self, resized_inputs):\r\n351     \"\"\"SSD preprocessing.\r\n352 \r\n353     Maps pixel values to the range [-1, 1]. The preprocessing assumes an input\r\n354     value range of [0, 255].\r\n355 \r\n356     Args:\r\n357       resized_inputs: a [batch, height, width, channels] float tensor\r\n358         representing a batch of images.\r\n359 \r\n360     Returns:\r\n361       preprocessed_inputs: a [batch, height, width, channels] float tensor\r\n362         representing a batch of images.\r\n363     \"\"\"\r\n364     return (2.0 / 255.0) * resized_inputs - 1.0\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nwe got different accuracy when using a same model(ssd_mobilenet_v1) on PC vs. iOS when using tensorflow lite example code and pre trained model, for ssd_mobilenet_v1 how much accuracy loss on iOS is expected? \r\n\r\nAppreciate for any help, Thanks!\r\n", "comments": ["@Jamesweng \r\nPlease share simple stand alone code so we can replicate the accuracy for both and analyse the issue. or if possible share a colab gist with the issue faced.", "> @Jamesweng\r\n> Please share simple stand alone code so we can replicate the accuracy for both and analyse the issue. or if possible share a colab gist with the issue faced.\r\n\r\nThanks for your reply Saduf, we finally verified that it's the wrong preprocess parameter in iOS causes the accuracy loss, when we change the iOS object detection example code \r\n\r\nFrom  \r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/ios/ObjectDetection/ModelDataHandler/ModelDataHandler.swift#L319\r\n\r\nfor i in 0..<bytes.count {\r\n  floats.append(Float(bytes[i]) / 255.0)\r\n}\r\n\r\nTo \r\n\r\nfor i in 0..<bytes.count {\r\n  floats.append(Float(bytes[i]) / 127.5 - 1.0)\r\n}\r\n\r\nthe accuracy in iOS is the same as PC now. ", "@Jamesweng Thanks for the detailed investigation! I think you're right that the iOS side preprocessing logic is incorrect for the floating-point model.\r\n\r\nWould you be willing to send a PR with a fix?", "Hi @yyoon I created a PR here - https://github.com/tensorflow/examples/pull/226\r\n\r\nThanks", "Hi @Jamesweng. I'm also facing this loss accuracy problem but with quantized models. I opened the issue #40518 to try solving this, we have some possibilities but not very clear right now. Can you please share how do you run the inference in iOS? I get a `UIImage` from camera or gallery, covert it to `CVPixelBuffer` and then call the `runModel(onFrame: myCvPixelBuffer)` from the `modelDataHandler.swift`. Are you also doing this procedure?\r\n\r\nThanks!", "> Hi @Jamesweng. I'm also facing this loss accuracy problem but with quantized models. I opened the issue #40518 to try solving this, we have some possibilities but not very clear right now. Can you please share how do you run the inference in iOS? I get a `UIImage` from camera or gallery, covert it to `CVPixelBuffer` and then call the `runModel(onFrame: myCvPixelBuffer)` from the `modelDataHandler.swift`. Are you also doing this procedure?\r\n> \r\n> Thanks!\r\n\r\nHi @zampnrs , here's something I can share when loading image from gallery - \r\n\r\nMAKE SURE YOUR INPUT IMAGE's pixel format(ARGB channel order) is consistent with the one needed in model preprocess, there's 'rgbDataFromBuffer()' process inside runModel() which is expected to remove the alpha channel from the input pixelBuffer. If you're passing in ARGB and take it as BGRA mistakenly, you may result a bad accuracy. \r\n\r\nThanks,\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40442\">No</a>\n"]}, {"number": 40441, "title": "0-th dimension of output tensor is None. Why not support batch?", "body": "my out tensor has shape = [None, 16]. Why doesn't support batch?\r\n```\r\npreds = tf.concat(  # [bboxes, landms, landms_valid, conf]\r\n            [bbox_regressions[0], landm_regressions[0],\r\n             tf.ones_like(classifications[0, :, 0][..., tf.newaxis]),\r\n             classifications[0, :, 1][..., tf.newaxis]], 1)\r\n        priors = prior_box_tf((tf.shape(inputs)[1], tf.shape(inputs)[2]),\r\n                              cfg['min_sizes'],  cfg['steps'], cfg['clip'])\r\n        decode_preds = decode_tf(preds, priors, cfg['variances'])\r\n\r\n        selected_indices = tf.image.non_max_suppression(\r\n            boxes=decode_preds[:, :4],\r\n            scores=decode_preds[:, -1],\r\n            max_output_size=tf.shape(decode_preds)[0],\r\n            iou_threshold=iou_th,\r\n            score_threshold=score_th)\r\n\r\n        out = tf.gather(decode_preds, selected_indices)\r\n\r\n    return Model(inputs, out, name=name)\r\n```\r\n", "comments": ["@ThiagoMateo \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40440, "title": "add None 0-dimension to output tensor (tf2)", "body": "to get dynamic batch, i need to add None to below ouput tensor. \r\n```\r\nout = tf.gather(decode_preds, selected_indices)\r\n```\r\nCan someone guide me?", "comments": ["@HoangTienDuc,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40439, "title": "tensorflow-gpu==1.13.1", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.9\r\n- Installed using: Miniconda3, pip3\r\n- GPU model and memory: Google Colab\r\n\r\n**Description of the problem**\r\n In order to run this python script, TensorFlow-GPU and TensorBoard both version 1.13.1 are required. According to the log, the second code block does run successfully - however, in reality, there seems to be an installation issue since a ModuleNotFoundError is prompted.\r\n\r\nSome general assistance and explanation would be extremely helpful - perhaps there exists a direct link that can be used for these installations instead?\r\n\r\n**Sequence of commands executed before running into the problem**\r\n```bash\r\n!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\r\n!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\r\n!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\r\n!conda install -q -y --prefix /usr/local python=3.6 ujson\r\nimport sys\r\nsys.path.append('/usr/local/lib/python3.6/site-packages')\r\n# test it\r\nimport ujson\r\n```\r\n```bash\r\n!pip3 install tensorboard==1.13.1\r\n!pip3 install tensorflow-gpu==1.13.1\r\n```\r\n**Unsuccessful command and traceback**\r\n```bash\r\n!conda create -y -n grover python=3.6 && source activate grover && pip3 install -r /content/drive/My\\ Drive/CS/GPT-2/grover/requirements-gpu.txt\r\n!python /content/drive/My\\ Drive/CS/GPT-2/grover/download_model.py base\r\n!PYTHONPATH=$(pwd) python /content/drive/My\\ Drive/CS/GPT-2/grover/sample/contextual_generate.py -model_config_fn lm/configs/base.json -model_ckpt models/base/model.ckpt -metadata_fn sample/april2019_set_mini.jsonl -out_fn april2019_set_mini_out.jsonl\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/content/drive/My Drive/CS/GPT-2/grover/sample/contextual_generate.py\", line 1, in <module>\r\n    import tensorflow as tf\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n", "comments": ["@alex-wey \r\nIs there any specific reason you are trying to install TensorFlow 1.12?\r\nTensorFlow 1.x is not actively supported, instead could you please install the latest version of TensorFlow i.e. v2.2 and let us know if you are facing the same issue.\r\n\r\nPlease refer to this [comment](https://github.com/tensorflow/tensorflow/issues/39937#issuecomment-635397756) and let us know if it helps.\r\nFor more information, please check this [installation guide.](https://www.tensorflow.org/install/pip#system-requirements) Thanks!\r\n\r\n\r\n", "Please switch to 1.15, 2.0, 2.1 or 2.2 (or nightly) as these are the only versions supported at this time.", "> @alex-wey\r\n> Is there any specific reason you are trying to install TensorFlow 1.12?\r\n> TensorFlow 1.x is not actively supported, instead could you please install the latest version of TensorFlow i.e. v2.2 and let us know if you are facing the same issue.\r\n> \r\n> Please refer to this [comment](https://github.com/tensorflow/tensorflow/issues/39937#issuecomment-635397756) and let us know if it helps.\r\n> For more information, please check this [installation guide.](https://www.tensorflow.org/install/pip#system-requirements) Thanks!\r\n\r\nHi @Saduf2019,\r\n\r\nI have tried using TensorFlow v2.2,", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40439\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40439\">No</a>\n", "> @alex-wey Is there any specific reason you are trying to install TensorFlow 1.12? TensorFlow 1.x is not actively supported, instead could you please install the latest version of TensorFlow i.e. v2.2 and let us know if you are facing the same issue.\r\n> \r\n> Please refer to this [comment](https://github.com/tensorflow/tensorflow/issues/39937#issuecomment-635397756) and let us know if it helps. For more information, please check this [installation guide.](https://www.tensorflow.org/install/pip#system-requirements) Thanks!\r\n\r\nHi, I also need to use tensorflow 1.12 to rerun one project that our group did few years ago. When using the latest version of TensorFlow, the result we get is different from what we reported before. Therefore, we would like to install tensorflow 1.12 to do debugging."]}, {"number": 40438, "title": "Building TF Lite C++ shared library for macOS, linux, iOS and Android", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave (10.14)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.2\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.17)\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\nI am trying to develop a C++ library on macOS and ubuntu to use in Android and iOS. Therefore, I am trying to build TFLite C++ shared library to link it my own C++ shared library using CMake. I have tried to build that using command `bazel build -c opt //tensorflow/lite:tensorflowlite` and got `libtensorflowlite.dylib` file as bazel-output. However CMake fails to link it and gives the following error:\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n  \"tflite::FlatBufferModel::~FlatBufferModel()\", referenced from:\r\n      std::__1::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const in my_class.cpp.o\r\n  \"tflite::DefaultErrorReporter()\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n  \"tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n  \"tflite::impl::Interpreter::AllocateTensors()\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n      MyClass::run_model(std::__1::vector<std::__1::vector<float, std::__1::allocator<float> >, std::__1::allocator<std::__1::vector<float, std::__1::allocator<float> > > > const&) in my_class.cpp.o\r\n  \"tflite::impl::Interpreter::ResizeInputTensor(int, std::__1::vector<int, std::__1::allocator<int> > const&)\", referenced from:\r\n      MyClass::run_model(std::__1::vector<std::__1::vector<float, std::__1::allocator<float> >, std::__1::allocator<std::__1::vector<float, std::__1::allocator<float> > > > const&) in my_class.cpp.o\r\n  \"tflite::impl::Interpreter::Invoke()\", referenced from:\r\n      MyClass::run_model(std::__1::vector<std::__1::vector<float, std::__1::allocator<float> >, std::__1::allocator<std::__1::vector<float, std::__1::allocator<float> > > > const&) in my_class.cpp.o\r\n  \"tflite::impl::Interpreter::~Interpreter()\", referenced from:\r\n      std::__1::default_delete<tflite::impl::Interpreter>::operator()(tflite::impl::Interpreter*) const in my_class.cpp.o\r\n  \"tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n  \"tflite::impl::InterpreterBuilder::~InterpreterBuilder()\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n  \"tflite::impl::InterpreterBuilder::operator()(std::__1::unique_ptr<tflite::impl::Interpreter, std::__1::default_delete<tflite::impl::Interpreter> >*)\", referenced from:\r\n      MyClass::MyClass(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, int) in my_class.cpp.o\r\n  \"vtable for tflite::MutableOpResolver\", referenced from:\r\n      tflite::MutableOpResolver::~MutableOpResolver() in my_class.cpp.o\r\n```\r\nBut, If I try to link shared tflite library(`libtensorflowlite.dylib`) with my static library, it builds without error.\r\n\r\nI am fairly new to `bazel` and CMake, however I thought it is caused by building the `libtensorflowlite.dylib` with missing x86_64 architecture and tried below commands and got following errors.:\r\n```\r\nbazel build -c opt --config=ios_fat  //tensorflow/lite:tensorflowlite\r\nERROR: /Users/username/tensorflow/tensorflow/lite/kernels/internal/BUILD:687:1: C++ compilation of rule '//tensorflow/lite/kernels/internal:kernel_utils' failed (Exit 1)\r\nclang: error: invalid version number in '-mmacosx-version-min=13.2'\r\n```\r\nand \r\n```\r\nbazel build -c opt --cpu=x86_64  //tensorflow/lite:tensorflowlite\r\nERROR: /private/var/tmp/_bazel_username/ec3ee607500668f8e05b1977bf58c2f5/external/local_config_cc/BUILD:41:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86_64'\r\nERROR: Analysis of target '//tensorflow/lite:tensorflowlite' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n```\r\n\r\nFinally, I thought it is related to #36178 , #39876 and #35386, but couldn't resolve it.", "comments": ["As I said, I was (and still am) new to CMake. And CMakeList.txt of my project was missing some link parameters. I added them and it is working. Therefore, there was not an issue at all. Sorry to bother.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40438\">No</a>\n", "> And CMakeList.txt of my project was missing some link parameters. I added them and it is working. Therefore, there was not an issue at all. Sorry to bother.\r\n\r\n@ebraraktas Hi there, could u list all the details how you solve it plz? Definitely need your help on this:)", "@wisonye \r\n```\r\nADD_LIBRARY(tensorflowlite SHARED IMPORTED)\r\nset_property(TARGET tensorflowlite PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/lib/libtensorflowlite.dylib)\r\n```\r\nThe lines I have added to solve the issue are given above. As you may guess, first you need to build `tensorflowlite` shared library with bazel and copy it to `${CMAKE_CURRENT_SOURCE_DIR}/lib`. Then you can link your cmake target with the line below:\r\n```\r\ntarget_link_libraries(my_awesome_target tensorflowlite)\r\n```", "@ebraraktas could you show me the whole CMakeLists.txt? I tried your code above but I am still getting \"Undefined symbols for architecture x86_64:\r\n  \"_TfLiteInterpreterAllocateTensors\"\"\r\n\r\nThis is my CMakeLists.txt\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.17)\r\nproject(veryfi_cpp)\r\nset(CMAKE_CXX_STANDARD 11)\r\n#include directories\r\ninclude_directories(includes/tensorflow)\r\ninclude_directories(includes/flatbuffers)\r\n\r\n# OPENCV\r\nfind_package(OpenCV REQUIRED)\r\ninclude_directories(${OpenCV_INCLUDE_DIRS})\r\nmessage(\"OpenCV_INCLUDE_DIRS = ${OpenCV_INCLUDE_DIRS}\")\r\nmessage(\"OpenCV_LIBS = ${OpenCV_LIBS}\")\r\n\r\n# TensorFlowLite\r\nadd_library(tensorflowlite SHARED IMPORTED)\r\nset_property(TARGET tensorflowlite PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/lib/libtensorflowlite.dylib)\r\nadd_executable(veryfi_cpp main.cpp)\r\n\r\n# linking libraries\r\ntarget_link_libraries(veryfi_cpp ${OpenCV_LIBS} tensorflowlite)\r\n```\r\n\r\nHow do you create the shared library, I am using `bazel build -c opt  //tensorflow/lite:libtensorflowlite.dylib `what command line are you using? ", "@alejouribesanchez As I wrote in my first comment, I was using `bazel build -c opt //tensorflow/lite:tensorflowlite` (You may add some optimization parameters for your target, I recommend it, use `--config=opt` at least). I have added my `CMakeLists.txt` below.\r\n\r\n```CMake\r\ncmake_minimum_required(VERSION 3.15)\r\nproject(TFLiteExample)\r\n\r\nset(CMAKE_CXX_STANDARD 14)\r\n\r\n# include has 2 subdirectories: tensorflow and flatbuffers\r\nINCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/include)\r\n\r\n# lib has 1 file: libtensorflowlite.dylib\r\nADD_LIBRARY(tensorflowlite SHARED IMPORTED)\r\nset_property(TARGET tensorflowlite PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/lib/libtensorflowlite.dylib)\r\n\r\nadd_executable(TFLiteExample src/main.cpp)\r\ntarget_link_libraries(TFLiteExample tensorflowlite)\r\n```\r\n\r\nAnd tree structure of directory:\r\n```\r\n\u251c\u2500\u2500 CMakeLists.txt\r\n\u251c\u2500\u2500 README.md\r\n\u251c\u2500\u2500 include\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 flatbuffers\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tensorflow\r\n\u251c\u2500\u2500 lib\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 libtensorflowlite.dylib\r\n\u2514\u2500\u2500 src\r\n    \u2514\u2500\u2500 main.cpp\r\n```\r\n\r\n__The important thing is that you should use header files from the branch you build the tensorflow.__ I'm sorry that I was late, but I have discovered this fact while testing my commands before writing this message. __You cannot mix headers and binary from different branches__. I have tested the build command and the `CMakeLists.txt` above with `r2.4` and `r2.3` branches just before I wrote this. ~~Unfortunately, there is no easy way to find and copy necessary headers to your include directory. I had a sloppy Python script at that time but I don't have it right now.~~\r\n\r\n--- \r\n\r\n__Update:__\r\n\r\nJust after I have submitted the comment I have found a way to list necessary headers for the header you will be including in your source code. Let's say you will `#include \"tensorflow/lite/interpreter.h\"`, you may find necessary headers with the command below (Assuming you have `flattbuffers` in the upper directory and you are in `tensorflow`):\r\n```bash\r\ng++ --std=c++11 -I ../flatbuffers/include/ -I . -MM tensorflow/lite/interpreter.h\r\n``` \r\nReference : https://stackoverflow.com/a/42513\r\n\r\nSee my gist for simple python script: https://gist.github.com/ebraraktas/0770c745c1cf5560397ffd412eebcea7\r\n\r\n--- \r\n\r\nHope it helps, please don't hesitate to ask further if you need.", "Thanks, @ebraraktas for your detailed response, I was able to run your example using C++ and flatbuffers 1.12.0 and it worked for me but it seems that the C support is still not working, I mean the C API code is not working, try this main.cpp\r\n\r\n```\r\n#include <iostream>\r\n#include \"include/tensorflow/lite/c/c_api.h\"\r\n\r\nint main() {\r\n    const char* model_path = \"../models/model.tflite\";\r\n    TfLiteModel* model = TfLiteModelCreateFromFile(model_path);\r\n    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n    TfLiteInterpreterOptionsSetNumThreads(options, 3);\r\n    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n\r\n// Allocate tensors and populate the input tensor data.\r\n    TfLiteInterpreterAllocateTensors(interpreter);\r\n    TfLiteTensor* inputTensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n    int inputImageHeight = inputTensor->dims->data[1];\r\n    int inputImageWidth = inputTensor->dims->data[2];\r\n    std::cout << \"inputImageHeight: \" << inputImageHeight << std::endl;\r\n    std::cout << \"inputImageWidth: \" << inputImageWidth << std::endl;\r\n}\r\n```\r\n\r\nAnd then you will get my first issue reported\r\n\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"_TfLiteInterpreterAllocateTensors\", referenced from:\r\n      _main in main.cpp.o\r\n  \"_TfLiteInterpreterCreate\", referenced from:\r\n      _main in main.cpp.o\r\n  \"_TfLiteInterpreterGetInputTensor\", referenced from:\r\n      _main in main.cpp.o\r\n  \"_TfLiteInterpreterOptionsCreate\", referenced from:\r\n      _main in main.cpp.o\r\n  \"_TfLiteInterpreterOptionsSetNumThreads\", referenced from:\r\n      _main in main.cpp.o\r\n  \"_TfLiteModelCreateFromFile\", referenced from:\r\n      _main in main.cpp.o\r\nld: symbol(s) not found for architecture x86_64\r\n```\r\nSo let me know if you find a way to run the C code instead of the C++ code.\r\n", "@alejouribesanchez I have built your code with no issue. Are you sure you are linking it with the correct library, to use C API you should build with command below:\r\n\r\n```bash\r\nbazel build -c opt --config=opt  //tensorflow/lite/c:tensorflowlite_c\r\n```\r\nAnd link your target with `libtensorflowlite_c.dylib`, see lines below:\r\n```CMake\r\nADD_LIBRARY(tensorflowlite_c SHARED IMPORTED)\r\nset_property(TARGET tensorflowlite_c PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/lib/libtensorflowlite_c.dylib)\r\n\r\nadd_executable(TFLiteC main.cpp)\r\ntarget_link_libraries(TFLiteC tensorflowlite_c)\r\n```", "@ebraraktas thank you so much!, I was using the previous `build -c opt --config=opt  //tensorflow/lite:tensorflowlite` so I was not linking the correct library this `bazel build -c opt --config=opt  //tensorflow/lite/c:tensorflowlite_c` worked with my C code\r\n", "> @alejouribesanchez I have built your code with no issue. Are you sure you are linking it with the correct library, to use C API you should build with command below:\r\n> \r\n> ```shell\r\n> bazel build -c opt --config=opt  //tensorflow/lite/c:tensorflowlite_c\r\n> ```\r\n> \r\n> And link your target with `libtensorflowlite_c.dylib`, see lines below:\r\n> \r\n> ```cmake\r\n> ADD_LIBRARY(tensorflowlite_c SHARED IMPORTED)\r\n> set_property(TARGET tensorflowlite_c PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/lib/libtensorflowlite_c.dylib)\r\n> \r\n> add_executable(TFLiteC main.cpp)\r\n> target_link_libraries(TFLiteC tensorflowlite_c)\r\n> ```\r\n\r\nhow to build tflite to work with iOS and Android in this way?\r\nusing `bazel build -c opt --config=opt //tensorflow/lite:tensorflowlite` I got `Mach-O 64-bit dynamically linked shared library x86_64`\r\nhow to get iOS and android arm64?", "@k1ngcyk I didn't continue my C++ project for cross platform library. Therefore, I cannot suggest a tested solution to your problem. But I think it would be easiest to build TensorFlowLiteC shared library for different platforms and using API of it. Note that this may lack platform specific accelerators like NNAPI, Metal etc. Another solution may be using experimental CMake integration, but it seems not supporting iOS right now. https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite  ", "@k1ngcyk @ebraraktas Hi I also need to run tflite with C++ on android+ios+macos+linux. Have you succeeded in doing it - could you please share a bit of hints? (From your latest comments seems that there is no easy way?) Thanks!", "@fzyzcjy see this issue too: https://github.com/tensorflow/tensorflow/issues/43955#issuecomment-757103893", "Hi @fzyzcjy , \r\nFirst of all, I am sorry for late response.\r\n\r\nAs I stated above I had abandoned the C++ project, and using Rust currently. I have been using [this crate](https://crates.io/crates/tflitec) (disclaimer: I am the developer of it). \r\n\r\nOn the other hand, I have tried [this official guide](https://www.tensorflow.org/lite/guide/build_cmake) about CMake build and succeeded to build a toy project for all of the platforms you mentioned. Note that I have used toolchain in [this repository](https://github.com/leetal/ios-cmake) and [this command](https://www.tensorflow.org/lite/guide/build_cmake#cross-compilation) given in the official guide to build it for iOS. Similarly, you need to use the toolchain in the NDK to build it for Android.\r\n\r\nThe issue about the CMake for me was that I couldn't build stand-alone `TensorFlowLite` shared library. Therefore, the Rust library that I have mentioned about builds `TensorFlowLiteC` shared library, as well. However, you can build your project by adding TensorFlowLite as a subdirectory to your `CMakeLists.txt`, see [this section](https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite) please.", "@ebraraktas Thanks all the same for the late reply.\r\n\r\nIndeed I have also used Rust and wrap the tflite-C API a few days ago. After looking at your code and compare with mine, a small suggestion: Your `Tensor` struct has no lifetime, so `Tensor.tensor_ptr` can live longer than it should. For example, a user can keep hold of a Tensor for a longer time than a Model/Interpreter. Then when he wants to use it, that pointer is not valid anymore. So maybe add a lifetime for Tensor struct ;)\r\n\r\nI have compiled on Android and MacOS by using [this readme](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/README.md), and ios by [using this](https://www.tensorflow.org/lite/guide/build_ios#build_tensorflowlitec_dynamic_framework_recommended).", "Hi @ebraraktas, sorry for replying to this old thread, but I'm also trying to build the TFLite c++ library on iOS using CMake. \r\n\r\n\r\n> On the other hand, I have tried [this official guide](https://www.tensorflow.org/lite/guide/build_cmake) about CMake build and succeeded to build a toy project for all of the platforms you mentioned. Note that I have used toolchain in [this repository](https://github.com/leetal/ios-cmake) and [this command](https://www.tensorflow.org/lite/guide/build_cmake#cross-compilation) given in the official guide to build it for iOS. Similarly, you need to use the toolchain in the NDK to build it for Android.\r\n\r\nI have tried building in CMake using the above toolchain and this command (inside a subfolder in my local TensorFlow repo which contains the toolchain file):\r\n`cmake -G Xcode -DCMAKE_TOOLCHAIN_FILE=ios.toolchain.cmake -DPLATFORM=OS64 ../tensorflow/lite`\r\n\r\nHowever I got the following errors:\r\n```\r\nCMake Error at /$(PATH_TO_TENFORFLOW)/tensorflow/ios_cmake_build/flatbuffers/CMakeLists.txt:554 (install):\r\n  install TARGETS given no BUNDLE DESTINATION for MACOSX_BUNDLE executable\r\n  target \"flatc\".\r\n```\r\nand\r\n```\r\nCMake Error at /$(PATH_TO_TENFORFLOW)/tensorflow/ios_cmake_build/xnnpack/CMakeLists.txt:87 (MESSAGE):\r\n  Unrecognized CMAKE_SYSTEM_NAME = iOS\r\n```\r\n\r\nSince you mentioned you've succeeded to build the project on all platforms, have you run into similar errors when building for iOS, and if so, how did you resolve it? Thanks!\r\n\r\n", "@zhitaop Sorry for late response. First of all, I don't remember specific options I used at that time when I posted the message. However, I have tried to build with current `master` branch, and managed to build it with the external options below:\r\n```shell\r\nmkdir tf_build && cd tf_build\r\ncmake ../tensorflow_src/tensorflow/lite/ -G Xcode \\\r\n  -DCMAKE_TOOLCHAIN_FILE=../ios-cmake/ios.toolchain.cmake \\\r\n  -DPLATFORM=OS64 \\\r\n  -DFLATBUFFERS_BUILD_FLATC=OFF \\\r\n  -DTFLITE_ENABLE_XNNPACK=OFF\r\ncmake --build .\r\n```\r\n\r\nNote that disabling XNNPACK may reduce performance, and I don't remember if I could build it with `cmake` for iOS. I added `-DFLATBUFFERS_BUILD_FLATC=OFF` to get rid off `BUNDLE_DESTINATION` error, and could build tflite successfully, but there may exist more proper and elegant solution."]}, {"number": 40437, "title": "Issue with tensorflow not being able to use my gpu after installing cuda and cuDNN", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): used sudo pip3 install tensorflow-gpu and sudo pip3 install tensorflow to install both versions.\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.2\r\n- CUDA: release 10.1\r\n- cuDNN version: 7.6.5 \r\n- GPU model and memory: Geoforce GTX - 1050\r\n\r\n\r\n\r\n**Current behavior**\r\n\r\nrunning the following code (to verify if I have completed all the steps correctly of installing cuda and cuDNN) - \r\n`from tensorflow.python.client import device_lib`\r\n`print(device_lib.list_local_devices())`\r\n\r\ncurrently the following error occurs - \r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n\r\n**Expected behavior**\r\nDue to the above error, I am not able to let tensorflow use my gpu for machine learning purposes. I would like to rectify this error\r\n\r\nKindly help me solve this problem, and let me know if any further details are required from my side.\r\n", "comments": ["hey, I resently installed cuda an cudnn in my machine (ubuntu 18.04) so maybe I can help you. Clearly there is some issue with your cudnn install. How did you install it?", "Installed cuda toolkit with the following command - \r\n`sudo apt install nvidia-cuda-toolkit`\r\nThen I followed the steps on the nvidia website for cuDNN to install cuDNN 7.6.5 for cuda 10.1 from a tar file from the following link - \r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html", "what does your terminal display if you run `nvidia-smi` ?\r\nedit: `nvcc --version` could be usefull too.", "nvidia - smi shows:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   40C    P3    N/A /  N/A |   1050MiB /  4040MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1073      G   /usr/lib/xorg/Xorg                            45MiB |\r\n|    0      1674      G   /usr/lib/xorg/Xorg                           363MiB |\r\n|    0      1930      G   /usr/bin/gnome-shell                         224MiB |\r\n|    0      2301      C   /usr/lib/libreoffice/program/soffice.bin      45MiB |\r\n|    0      3048      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files   179MiB |\r\n|    0      4099      G   ...uest-channel-token=16056379025461602208   127MiB |\r\n|    0     14258      G   ...quest-channel-token=7332220839678530975    49MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nwhile nvcc --version shows - \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\n", "if you go to `/usr/local/` what `cuda-x.x` folder(s) do you see? ( you can do this with `cd /usr/local` and then 'ls')", "I don't have a /usr/local/cuda directory.\r\nRather I have a /usr/lib/cuda directory\r\ndoes this mean something is wrong ?", "I don't think so, but you moved the cudnn files to `/usr/lib/cuda` then?", "yes", "Then you did that just fine, not really sure what the issue could be. The discrepancy between `nvidia-smi` and `nvcc --version` is odd though, but from what I read what `nvidia-smi` displays isn't necesarily true. \r\n\r\nMaybe you can try the deb alternative of cudnn installation, don't forget to erase the previous files you moved to cuda. Also as a sanity check, that this is no tensorflow issue but rather a cudnn installation issue, you can do the verification process outlined in the link you provided above.", "I think the debian files are only for the ubuntu 18.04 and below. I do not want to risk ruining something on this version of ubuntu 20.04, but I will look at other methods. Thank you for your quick responses!", "No problem, good luck! I recommend you try to install cuda toolkit from some official nvidia source as `nvidia-cuda-toolkit` isn't made by nvidia and maybe it's incompatible with that cudnn installation method.", "@ZiZizou,\r\nPlease take a look at [this](https://github.com/tensorflow/tensorflow/issues/20271) similar issue and let us know if it helps. Thanks!", "\r\n\r\n\r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n> * TensorFlow installed from (source or binary): used sudo pip3 install tensorflow-gpu and sudo pip3 install tensorflow to install both versions.\r\n> * TensorFlow version (use command below): 2.2.0\r\n> * Python version: 3.8.2\r\n> * CUDA: release 10.1\r\n> * cuDNN version: 7.6.5\r\n> * GPU model and memory: Geoforce GTX - 1050\r\n> \r\n> **Current behavior**\r\n> \r\n> running the following code (to verify if I have completed all the steps correctly of installing cuda and cuDNN) -\r\n> `from tensorflow.python.client import device_lib`\r\n> `print(device_lib.list_local_devices())`\r\n> \r\n> currently the following error occurs -\r\n> W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n> \r\n> **Expected behavior**\r\n> Due to the above error, I am not able to let tensorflow use my gpu for machine learning purposes. I would like to rectify this error\r\n> \r\n> Kindly help me solve this problem, and let me know if any further details are required from my side.\r\n\r\nTo avoid complications, I would recommend you using anaconda.\r\nYou need to run this one line and it will create a tensorflow-gpu environment for you,\r\n`conda create --name tf_gpu tensorflow-gpu `", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]