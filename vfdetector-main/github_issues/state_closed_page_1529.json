[{"number": 7056, "title": "Build is failing on Linux GPU", "body": "Hi,\r\n\r\nI'm trying with no luck to build TF for OpenCL from source. I configured a new installed machine as is described in your guide but the build is failing.\r\n\r\nI'm launching the build through this command:\r\n`bazel build -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAnd below you can find all the requested informations from my OS. In attach you'll find the [Errors](https://github.com/tensorflow/tensorflow/files/729409/Errors.TF.txt) the compiler shows me.\r\nI've tryed also to build with clang against GCC/G++ with no luck.\r\n[Errors TF.txt](https://github.com/tensorflow/tensorflow/files/729409/Errors.TF.txt)\r\n\r\nThank you\r\n\r\n------------------\r\n\r\nOperating System: Ubuntu 16.04, Kernel 4.4\r\n\r\nBuilding TF for OpenCL as descripted in ./configure below:\r\n`$ ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] y\r\nOpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is ]: /usr/bin/g++\r\nPlease specify which C compiler should be used as the host C compiler. [Default is ]: /usr/bin/gcc\r\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: `\r\n\r\n`$ bazel version\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885`\r\n\r\n`$ git rev-parse HEAD\r\na12c7dc3d83049e10c1dca8903d73cc71d3cb7b2`", "comments": ["@gunan we don't support opencl, right?", "[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md)\r\n\r\nIt is your guide that tells me to do so.", "We should update our guide, sorry for the confusion.\r\nOpenCL support is still in its early stages, and it is experimental:\r\nYou can follow this issue for more information:\r\nhttps://github.com/tensorflow/tensorflow/issues/22", "@lorenzo93 Does it compile without `--config=sycl`? It seems like the compilation error comes from GCC6 headers. ", "I'll test it tomorrow night and reply. I'm out for work\n\n2017-01-26 12:27 GMT+01:00 Luke Iwanski <notifications@github.com>:\n\n> @lorenzo93 <https://github.com/lorenzo93> Does it compile without\n> --config=sycl? It seems like the compilation error comes from GCC6\n> headers.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7056#issuecomment-275366718>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALtNaqaGWdf4xA3dc2hDqB1sbIfru5W1ks5rWIMZgaJpZM4LtYNV>\n> .\n>\n", "I have the same issue, so I thought that I'll drop it here instead of creating a new one. Compiling TF with `--config=syscl` throws an error:\r\n```INFO: Found 1 target...\r\nERROR: /home/danielius/.cache/bazel/_bazel_danielius/c3436072caa9f5957dc8575117a2d1e5/external/llvm/BUILD:1661:1: C++ compilation of rule '@llvm//:support' failed: computecpp failed: error executing command external/local_config_sycl/crosstool/computecpp -Wall -msse3 -g0 -O2 -DNDEBUG '-std=c++11' -MD -MF ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/llvm/lib/Support/RandomNumberGenerator.cpp:34:8: error: reference to 'cl' is ambiguous\r\n static cl::opt<unsigned long long>\r\n        ^\r\nIn file included from ./bazel-out/local_linux-opt/bin/external/llvm/_objs/support/external/llvm/lib/Support/RandomNumberGenerator.pic.sycl:13:0,\r\n                 from <command-line>:0:\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/predefines.h:88:14: note: candidates are: namespace cl { }\r\n namespace cl {\r\n              ^\r\nIn file included from external/llvm/lib/Support/RandomNumberGenerator.cpp:17:0:\r\nexternal/llvm/include/llvm/Support/CommandLine.h:48:14: note:                 namespace llvm::cl { }\r\n namespace cl {\r\n              ^\r\nexternal/llvm/lib/Support/RandomNumberGenerator.cpp:34:8: error: 'cl' does not name a type\r\n static cl::opt<unsigned long long>\r\n        ^\r\nexternal/llvm/lib/Support/RandomNumberGenerator.cpp: In constructor 'llvm::RandomNumberGenerator::RandomNumberGenerator(llvm::StringRef)':\r\nexternal/llvm/lib/Support/RandomNumberGenerator.cpp:51:13: error: 'Seed' was not declared in this scope\r\n   Data[0] = Seed;\r\n             ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 73.376s, Critical Path: 68.71s\r\n```\r\n\r\nCompiling without `--config=syscl` works fine.\r\n\r\nSystem information:\r\n- Ubuntu 14.04 64bit\r\n- Intel Haswell Mobile graphics / Radeon 8800M\r\n\r\nMaybe two graphics cards might be an issue?", "@lorenzo93 The GCC 6 STL headers exposed a bug in clang 3.6, which has been fixed upstream and will soon be merged into computecpp. Until then, it is possible to work around this by either using the GCC5 headers or libc++ (https://llvm.org/bugs/show_bug.cgi?id=24770) \r\n\r\n@dvisockas the issue that you are seeing is fixed in https://github.com/benoitsteiner/tensorflow-opencl/pull/31/commits/3a0391a796ed77f7fbd3fdd3c073bbe271f9bedd. This is effectively a namespace clash.   ", "thanks @lukeiwanski -- very helpful!"]}, {"number": 7055, "title": "TensorFlow Bazel build is failing on Windows", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/339/console\r\n\r\n```\r\n18:24:27 ERROR: C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/core/kernels/BUILD:3802:1: C++ compilation of rule '//tensorflow/core/kernels:quantized_ops' failed: msvc_cl.bat failed: error executing command \r\n...\r\n18:24:27 c:\\tmp\\_bazel_system\\rrc05caq\\execroot\\tf-master-win-bzl\\external\\gemmlowp\\internal\\common.h(21): fatal error C1083: Cannot open include file: 'pthread.h': No such file or directory\r\n```\r\n\r\nCulprit: f736991fd3a7987665a6f9fcd26d464ea7f68e2b\r\nThis changes introduced the dependency on `//tensorflow/core/kernels:quantized_ops` through `//tensorflow/tools/graph_transforms:transform_graph_lib`, which doesn't build on Windows yet.\r\n", "comments": ["If simply exclude `//tensorflow/tools/graph_transforms:transform_graph_lib`, we'll get another error:\r\n```\r\nERROR: C:/tools/msys64/home/pcloudy/workspace/tensorflow/tensorflow/python/BUILD:2274:1: undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_tensorflow.so':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/python/pywrap_tensorflow.cc':\r\n  'C:/tools/msys64/home/pcloudy/workspace/tensorflow/tensorflow/tools/graph_transforms/transform_graph.h'\r\n  'C:/tools/msys64/home/pcloudy/workspace/tensorflow/tensorflow/tools/graph_transforms/transform_utils.h'\r\n```", "@petewarden FYI", "We fixed this in cmake, and I am attempting to fix it in https://github.com/tensorflow/tensorflow/pull/7082\r\nBut not been successful so far.", "Thanks, @gunan !  The pip package build is back to normal, only two python tests are failing. Are they supposed to pass on Windows, or should we just skip them?\r\n```\r\n23:48:48 //py_test_dir/tensorflow/python:bigquery_reader_ops_test                 FAILED in 1 out of 2 in 6.6s\r\n23:48:48   C:/tmp/_bazel_SYSTEM/rrc05Caq/execroot/tf-master-win-bzl/bazel-out/vc_14_0_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/bigquery_reader_ops_test/test.log\r\n23:48:48 //py_test_dir/tensorflow/python/kernel_tests:denormal_test               FAILED in 1 out of 2 in 4.5s\r\n23:48:48   C:/tmp/_bazel_SYSTEM/rrc05Caq/execroot/tf-master-win-bzl/bazel-out/vc_14_0_x64-py3-opt/testlogs/py_test_dir/tensorflow/python/kernel_tests/denormal_test/test.log\r\n```", "Two PRs out for that. #7107 and #7112 ", "This one now should be fixed."]}, {"number": 7054, "title": "Slicing", "body": "I wish to implement the following line of code in tensorflow. Given below is the numpy version\r\ncorrect_scores=scores[range(N), np.array(yTrain,dtype='int32')]\r\n\r\nnow in tensorflow, scores and yTrain are all variables, calculated on each minibatch. Is there a way to do this\r\n", "comments": ["This is a better suited question for StackOverflow. They also monitor there under the TensorFlow tag."]}, {"number": 7053, "title": "TensorFlow- Recurrent Neural networks.", "body": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Sounds like you are missing input files (your /tmp/simple-examples/data directory doesn't have the train files). These kinds of questions are best asked on stackoverflow, this list is for bugs in TensorFlow itself + feature requests"]}, {"number": 7052, "title": "Please create a new version tag.", "body": "There's currently no version tag that has all the WORKSPACE links working correctly. Can we please create one that I can point to to have a stable TF version? If I point to HEAD using git submodule we often update the TF version by mistake.\r\n\r\nThanks!", "comments": ["The latest release is [0.12.1](https://github.com/tensorflow/tensorflow/releases).", "That release is broken as it points to a non-existing zlib url:\r\nhttps://github.com/tensorflow/tensorflow/blob/0.12.1/tensorflow/workspace.bzl", "@jart I thought we had cherry-picked into this?", "@yifeif I thought we had cherry-picked into all release points?", "Tags were not moved after cherrypicks.\r\nOne option for you is to use the tip of the release branches.\r\nHowever, v1.0.0-rc0 is just created. So you may also use that one.\r\nhttps://github.com/tensorflow/tensorflow/tree/v1.0.0-rc0", "Tensorflow 1.0 does not have the fix for #6706. We still don't have any tags or branches that work when Tensorflow is used as an external dependency, like tensorflow_serving uses it. Only the HEAD version is usable.", "I think I'm going to reopen that bug.", "That bug is about a broken reference to a bzl file. It's fixed in HEAD but in this bug I'm asking for a stable tag that has the fix. There's no such tag today. Tensorflow 1.0 does not have the fix for #6706.\r\n\r\nRunning ./configure is a separate issue that we can live with for now, but it'd be nice to get that fixed too.", "Which commit did fix the problem for you? I can maybe cherrypick that into 1.0 branch.", "I'm not sure the exact commit. See your comment here: https://github.com/tensorflow/tensorflow/issues/6706#issuecomment-273939158", "Looks like this commit? https://github.com/tensorflow/tensorflow/pull/6958/commits/0ee1301dd2eb7cc3671f88f5e25bb1795faf87fe\r\n", "there should be a separate one for jpeg.\n\nOn Wed, Feb 1, 2017 at 4:16 PM, Yifei Feng <notifications@github.com> wrote:\n\n> Looks like this commit? 0ee1301\n> <https://github.com/tensorflow/tensorflow/pull/6958/commits/0ee1301dd2eb7cc3671f88f5e25bb1795faf87fe>\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7052#issuecomment-276826251>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOditkg_GSmAJDT4bgRkiNxnLwkBCks5rYSBVgaJpZM4LtCVt>\n> .\n>\n", "Also saw this one https://github.com/tensorflow/tensorflow/commit/4f6e6eb285e0e925271d632cc688811ca4e8c5a2 but it should already be in v1.0.0-rc0."]}, {"number": 7051, "title": "Bug - tfdbg + multi-gpu gives ValueError: Duplicate node name: 'n/_0'", "body": "Hello tensorflow team,\r\n\r\nI have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi-gpu model I get `ValueError: Duplicate node name: 'n/_0'`.\r\n\r\nInspecting things closer, I saw that the issue originated from the run_metadata, whose partition graphs have many _Send and _HostRecv ops with names like 'n/_0'.  These ops are replicated with identical names across my towers which is what is causing the issue.\r\n\r\nLooking through the tensorflow code, I believe I tracked where this name is set down to [graph_partition.cc:195](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/graph_partition.cc#L195) where the edge's source name is used as the prefix 'n'.  Unfortunately, I have not been able to figure out why the source's name is only 'n', but that seems to be the root of the issue here.\r\n\r\nI should add that I never set any tensor name to 'n' anywhere in my own code.  Plus, I see certain tests in your codebase rely on names such as 'n/_0' which indicates to me the name is being set somewhere internally in the tensorflow code.\r\n\r\nAny help you can provide would be much appreciated!\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI didn't find any related issues.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5 LTS (running in a [singularity](http://singularity.lbl.gov) container on a CentOS 6.7 host). \r\n\r\nInstalled version of CUDA and cuDNN: \r\nI am using CUDA 8.0 with NVIDIA driver 367.48, and cuDNN v5.1 . \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nlibOpenCL.so\r\nlibOpenCL.so.1\r\nlibOpenCL.so.1.0\r\nlibOpenCL.so.1.0.0\r\nlibcublas.so\r\nlibcublas.so.8.0\r\nlibcublas.so.8.0.45\r\nlibcublas_device.a\r\nlibcublas_static.a\r\nlibcudadevrt.a\r\nlibcudart.so\r\nlibcudart.so.8.0\r\nlibcudart.so.8.0.44\r\nlibcudart_static.a\r\nlibcudnn.so\r\nlibcudnn.so.5\r\nlibcudnn.so.5.1.5\r\nlibcudnn_static.a\r\nlibcufft.so\r\nlibcufft.so.8.0\r\nlibcufft.so.8.0.44\r\nlibcufft_static.a\r\nlibcufftw.so\r\nlibcufftw.so.8.0\r\nlibcufftw.so.8.0.44\r\nlibcufftw_static.a\r\nlibcuinj64.so\r\nlibcuinj64.so.8.0\r\nlibcuinj64.so.8.0.44\r\nlibculibos.a\r\nlibcurand.so\r\nlibcurand.so.8.0\r\nlibcurand.so.8.0.44\r\nlibcurand_static.a\r\nlibcusolver.so\r\nlibcusolver.so.8.0\r\nlibcusolver.so.8.0.44\r\nlibcusolver_static.a\r\nlibcusparse.so\r\nlibcusparse.so.8.0\r\nlibcusparse.so.8.0.44\r\nlibcusparse_static.a\r\nlibnppc.so\r\nlibnppc.so.8.0\r\nlibnppc.so.8.0.44\r\nlibnppc_static.a\r\nlibnppi.so\r\nlibnppi.so.8.0\r\nlibnppi.so.8.0.44\r\nlibnppi_static.a\r\nlibnppial.so\r\nlibnppial.so.8.0\r\nlibnppial.so.8.0.44\r\nlibnppicc.so\r\nlibnppicc.so.8.0\r\nlibnppicc.so.8.0.44\r\nlibnppicom.so\r\nlibnppicom.so.8.0\r\nlibnppicom.so.8.0.44\r\nlibnppidei.so\r\nlibnppidei.so.8.0\r\nlibnppidei.so.8.0.44\r\nlibnppif.so\r\nlibnppif.so.8.0\r\nlibnppif.so.8.0.44\r\nlibnppig.so\r\nlibnppig.so.8.0\r\nlibnppig.so.8.0.44\r\nlibnppim.so\r\nlibnppim.so.8.0\r\nlibnppim.so.8.0.44\r\nlibnppist.so\r\nlibnppist.so.8.0\r\nlibnppist.so.8.0.44\r\nlibnppisu.so\r\nlibnppisu.so.8.0\r\nlibnppisu.so.8.0.44\r\nlibnppitc.so\r\nlibnppitc.so.8.0\r\nlibnppitc.so.8.0.44\r\nlibnpps.so\r\nlibnpps.so.8.0\r\nlibnpps.so.8.0.44\r\nlibnpps_static.a\r\nlibnvToolsExt.so\r\nlibnvToolsExt.so.1\r\nlibnvToolsExt.so.1.0.0\r\nlibnvblas.so\r\nlibnvblas.so.8.0\r\nlibnvblas.so.8.0.44\r\nlibnvgraph.so\r\nlibnvgraph.so.8.0\r\nlibnvgraph.so.8.0.44\r\nlibnvgraph_static.a\r\nlibnvrtc-builtins.so\r\nlibnvrtc-builtins.so.8.0\r\nlibnvrtc-builtins.so.8.0.44\r\nlibnvrtc.so\r\nlibnvrtc.so.8.0\r\nlibnvrtc.so.8.0.44\r\nstubs\r\n```\r\n\r\n\r\n\r\n1. A link to the pip package you installed:\r\nI installed tensorflow using `pip install tensorflow-gpu==0.12.1`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.1 \r\n```                                                                                     \r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nThe single GPU case works fine.\r\n\r\n### Logs or other output that would be helpful\r\n\r\nHere is the dump of some of the problematic nodes.\r\n\r\n```\r\nnode {\r\n  name: \"n/_0\"\r\n  op: \"_Send\"\r\n  input: \"__copy_TOWER0/Const_0\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_545___copy_TOWER0/Const_0\"\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"n/_1\"\r\n  op: \"_HostRecv\"\r\n  input: \"^n/_0\"\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_545___copy_TOWER0/Const_0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_type\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"n/_2\"\r\n  op: \"_Send\"\r\n  input: \"__copy_TOWER0/Sub_0\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_551___copy_TOWER0/Sub_0\"\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\nEnd of backtrace at crash point\r\n```\r\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py(419)run()                 \r\n-> run_end_resp = self.on_run_end(run_end_req)                                                                              \r\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py(262)on_run_end()  \r\n-> self._dump_root, partition_graphs=partition_graphs)                                                                      \r\n  /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(407)__init__()                    \r\n-> self._load_partition_graphs(partition_graphs)                                                                            \r\n> /home/raphtown/.local/lib/python2.7/site-packages/tensorflow/python/debug/debug_data.py(493)_load_partition_graphs()      \r\n-> raise ValueError(\"Duplicate node name: '%s'\" % node.name)                                                                \r\n```", "comments": ["@raphtown a small code repro would be nice if you have one.\r\n\r\n@caisq any idea?", "Hello @drpngx, and thank you for your swift response.  I can try and make a minimal code reproduction if you think it necessary, but it may take a bit for me to do so.  My hunch is that to fix this bug we just need to figure out where these nodes with names 'n' are coming from in the library code.", "@raphtown Thanks for reporting this bug. As @drpngx said, a short reproduction code will be very helpful. Even if the code can't show the error without a multiple-GPU setup, we may be able to suggest a quick workaround.", "FYI, this bug about incompatibility between tfdbg and multiple GPUs should have been fixed by the commit above. Please let me know if you see any remaining issues.", "@caisq @aselle \r\nIf you run `transformer_encoder` model from `tensor2tensor` repo: https://github.com/tensorflow/tensor2tensor#sentiment-analysis\r\nwith tfdbg enabled, it will report:\r\n\r\n`multiple devices with nodes named 'n/_78' but device_name is not specified`\r\n\r\nMy node has 4 v100 gpus. The script I use attached here:\r\n\r\n``` bash\r\n# t2t hparams\r\nPROBLEM=sentiment_imdb\r\nMODEL=transformer_encoder\r\nHPARAMS=transformer_tiny\r\n\r\n# t2t dirs\r\n# MODEL-HPARAMS format is mandatory\r\nPROJECT=/project/chli\r\nTRAIN_DIR=$PROJECT/$MODEL/t2t_train/$PROBLEM/$MODEL-$HPARAMS\r\nTMP_DIR=/tmp/t2t_datagen\r\nDATA_DIR=$PROJECT/$MODEL/t2t_data\r\n\r\nt2t-trainer \\\r\n  --data_dir=$DATA_DIR \\\r\n  --tmp_dir=$TMP_DIR \\\r\n  --output_dir=$TRAIN_DIR \\\r\n  --t2t_usr_dir=$USER_DIR \\\r\n  --problems=$PROBLEM \\\r\n  --model=$MODEL \\\r\n  --hparams_set=$HPARAMS \\\r\n  --generate_data=True \\\r\n  --train_steps=2000 \\\r\n  --worker_gpu=4 \\\r\n  --tfdbg\r\n```", "Hello @raphtown ,did you solve this problem? I meet with the same problem as you, and I didn't find any useful information about this after looking through this page, I wonder did you solve this problem? and what did you do?", "Hello @spacegoing  ,did you solve this problem? I meet with the same problem as you, and I didn't find any useful information about this after looking through this page, I wonder did you solve this problem? and what did you do?", "@Vamix  Sorry I didn't. I will let you know if I did:D"]}, {"number": 7050, "title": "Update release note.", "body": "", "comments": []}, {"number": 7049, "title": "NotFoundError: /tf_files/retrained_graph.pb", "body": "When I tried to follow the Tensorflow Poets tutorial steps, I got this error when try to run the label_image.py file with classifier.\r\n\r\npython /tf_files/label_image.py /tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg \r\nTraceback (most recent call last):\r\n  File \"/tf_files/label_image.py\", line 15, in <module>\r\n    graph_def.ParseFromString(f.read())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 106, in read\r\n    self._preread_check()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 73, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /tf_files/retrained_graph.pb\r\nroot@858f20e4c756:/tensorflow# ls /tf_files/retrained_graph.pd \r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ngit rev-parse HEAD              \r\n45ab528211c962b19e12f6b77165848310271624\r\n2. The output of `bazel version`\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["The log got truncated. What's the result of ls `/tf_files`?", "You can close this now. I found out is was some weird permission issue on the file. I run in sudo mode and it fixed it."]}, {"number": 7048, "title": "XLA on MacOS: ./configure gives java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@llvm' (requested by nodes 'REPOSITORY:@llvm')", "body": "Not sure if this is WAI since XLA is experimental, but this is the error on MacOS that happens when you try to run ./configure with XLA=true\r\n\r\nSame version configures fine on Linux (master as of now)\r\n\r\n```\r\nbash-3.2$ ./configure\r\n./configure\r\nPlease specify the location of python. [Default is /Users/yaroslav/anaconda/bin/python]: \r\n\r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\ny\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /Users/yaroslav/anaconda/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/yaroslav/anaconda/lib/python3.5/site-packages]\r\n\r\n\r\nUsing python library path: /Users/yaroslav/anaconda/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\ny\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\n\r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\n\r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\nlibcudnn.dylib resolves to libcudnn.dylib\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\n\r\n____Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.........\r\n____Loading package: tensorflow/contrib/image\r\n____Loading package: tensorflow/core/debug\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 65,536 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,676,424 bytes\r\n____Loading package: @bazel_tools//tools/jdk\r\n____Loading package: tools/defaults\r\n____Loading package: third_party/py/numpy\r\n____Loading package: util/python\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@llvm' (requested by nodes 'REPOSITORY:@llvm')\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\r\n\tat java.util.concurrent.Semaphore.acquire(Semaphore.java:312)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:196)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:594)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:316)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:344)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\t... 4 more\r\njava.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@llvm' (requested by nodes 'REPOSITORY:@llvm')\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:429)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.InterruptedException\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\r\n\tat java.util.concurrent.Semaphore.acquire(Semaphore.java:312)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:196)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:594)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:316)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.callMethod(FuncallExpression.java:344)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:732)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:784)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:770)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:48)\r\n\tat com.google.devtools.build.lib.syntax.ExpressionStatement.doExec(ExpressionStatement.java:46)\r\n\tat com.google.devtools.build.lib.syntax.Statement.exec(Statement.java:37)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:136)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:439)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:106)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:155)\r\n\tat com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n\t... 4 more\r\n\r\n```", "comments": ["@hawkinsp in case he knows status of XLA on MacOS", "That looks like a Bazel bug to me. Are you using the latest version of Bazel? If so, we should file a bug for Bazel.\r\n\r\nI have previously built XLA on Mac, although there is currently no continuous build for XLA on Mac so it may have bitrotted.", "I'm also encountering this same error message. I'm trying to [install Tensor Serving from the docker image](https://tensorflow.github.io/serving/docker) using the Docker Toolbox on Windows 7. I also tried installing it within another Ubuntu docker container.", "@hawkinsp Hi, I upgraded Bazel to latest (0.4.3) but still get the same error message as the OP.", "same on ubuntu using master, with bazel 0.4.3, downgraded to 0.4.2 and same problem, tried 0.3.2 but ran into https://github.com/tensorflow/tensorflow/issues/6436 \r\n\r\ndead in the water at this point, can't get past configure\r\n\r\nupdate: get this with or without XLA", "same error ", "Could you try, before configure:\r\n`sudo update-ca-certificates -f`", "Either running the update certificates command above or upgrading to 0.4.4 fixed this for me.", "There's a high chance 0.4.4 fixes it, so if nobody complains this issue can be closed in 7 days", "Bazel 0.4.4 was just released:\r\nhttps://github.com/bazelbuild/bazel/releases/tag/0.4.4", "I came with the same problem. I upgraded Bazel to latest (0.4.4).", "It sounds to me like this issue is fixed by bazel 0.4.4. Closing; feel free to reopen if not."]}, {"number": 7047, "title": "More cherry-picks", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 7046, "title": "Remove the files that are moved to be http_util*.", "body": "During my push, the files were not correctly deleted. removing them now to fix windows cmake and pip tests.", "comments": ["@drpngx this should fix the test failures.", "Makefile build had an infra flake.\r\nHere is a rerun:\r\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/7196", "2nd android test passed.\r\nMerging."]}, {"number": 7045, "title": "Add support for dict generator input_fn in learn_io", "body": "This issue if related to #5546, where it is claimed that `Datafeeders` will become deprecated.\r\n\r\nAs such, this branch has following changes \r\n- [ ] Added `_GeneratorFeedFn` class\r\n- [ ] `generator_input_fn` in `generator_io.py`\r\n- [ ] support for `generator_input_fn` in `enqueue_data()`\r\n- [ ] added unitest in `generator_io_test.py`\r\n- [ ] refactored code keeping indent spacing to 2.\r\n", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I consent to contribute to this project with my commits", "Cc @martinwicke ", "@ilblackdragon @martinwicke - this is what you guys suggested in #5546 . please review.", "@ilblackdragon this is ready for review since @agistrueai  can given his consent to the PR.", "@martinwicke , @ilblackdragon , @rmlarsen - it's been a while, can one of the reviewers please take a look. As already mentioned, @agistrueai has already provided his consent.", "Friendly ping for @abhitopia ", "Apologies, I used my other github account @outcastofmusic  to reply to the comments. Hence it's me replying. (Though I should make this clear otherwise the replies seem completely nonsensical)", "thanks @vrv ", "@martinwicke PTAL", "@abhitopia Please resolve the conflict", "@martinwicke  fixed conflicts with master and made relevant indentation changes. Generator_io is still in learn_io  in the contrib section while the other input functions have been moved to estimator.inputs is this ok?", "All of the code should go into contrib, including the private backing class. Can you move it? I'll add comments.", "@martinwicke I can move the the _GeneratorFeedFn inside tensorflow.contrib.learn.python.learn.dataframe.queues.feeding_functions.py, but what do I do about the changes in  _enqueue_data (which is located inside tensorflow.python.estimator.inputs.queues.feeding_functions.py", "I think the change to _enqueue_data is fine where it is\u200b, it's generally\nuseful.\n", "@martinwicke  ok I 'll need to ad an import for the _GeneratorFeedFn though", "Argh. Darn, you're right. Having the object over with _enqueue was better. I'm sorry about that. Can you revert that change?", "reverted :)", "@martinwicke @ilblackdragon changes were done as requested please have a look at the review.", "Ah, the question about feeding_functions.py:323 is real: should it be FunctionType or GeneratorType?", "FunctionType that returns a GeneratorType. the feeder needs to be able to call it again if/when the generator gets consumed (for the next epoch)", "Jenkins, test this please?", "Wonderful. We will merge assuming the tests are happy. Thank you!", "I think I over did it a bit this morning on the security. The split was the better option. Need input functions for many different classification types. I am concerned that I did not get separate the files properly. The n% security file this morning was to much and will cause soft Max to not work and will cause this test to fail. Also if we do not allow for some pyramid pooling then this always fails because we set Max never> . I didn't know where these notes went as I was being key logged on my phone. ", "@Realtimedeployment commenting for the wrong PR?\r\n\r\n@agistrueai can you fix the indentation? See the sanity test failure for a full list. ", "Sorry wrong pr. ", "@martinwicke  sanity issues fixed. please retest", "Jenkins, test this please.", "@martinwicke  All tests passed. Just to reconfirm I verify consent to merge.", "Wonderful, thank you!"]}, {"number": 7044, "title": "tf.gradients like functionality in the C and C++ API", "body": "Currently gradients can only be computed manually in C++ or automatically in Python with tf.gradients. Work on registering C++ implemented gradients for all the OPs has started. However, there is currently no easy way to automatically use those gradients to generate a graph. The C-api also does not have gradient creation calls yet.\r\n\r\nThis supercedes the gradient part of #476, since the rest of #476's issue was addressed.", "comments": ["Closing this as duplicate of #6268"]}, {"number": 7043, "title": "Branch 145436800", "body": "", "comments": ["Tests failed due to renamed files still residing in workspaces with old filenames. Merging."]}, {"number": 7042, "title": "replicated function tf.where()?", "body": "Hi all,\r\n\r\nI have a problem adding an Embedding layer to a `Sequential` model.\r\n\r\n```\r\nfrom keras.layers import Embedding\r\nfrom keras.layers import Dense, Input, Flatten\r\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\r\nfrom keras.models import Model\r\nfrom keras.models import Sequential\r\nfrom keras.layers import LSTM,Bidirectional\r\n\r\nembedding_layer = Embedding(len(word_index) + 1,\r\n                            EMBEDDING_DIM,\r\n                            weights=[embedding_matrix],\r\n                            input_length=MAX_SEQUENCE_LENGTH*2,\r\n                             dropout=0.2,\r\n                            trainable=False)\r\n\r\nmodel = Sequential()\r\nmodel.add(embedding_layer)\r\nmodel.add(Bidirectional(LSTM(100, dropout_W=0.2, dropout_U=0.2)))\r\nmodel.add(Dense(200))\r\nmodel.add(Dense(6, activation='softmax'))\r\n```\r\n\r\nI have the next error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"recurrent_ltsm.py\", line 201, in <module>\r\n    model.add(embedding_layer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 299, in add\r\n    layer.create_input_layer(batch_input_shape, input_dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 401, in create_input_layer\r\n    self(x)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 572, in __call__\r\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 635, in add_inbound_node\r\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 166, in create_node\r\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/embeddings.py\", line 123, in call\r\n    B = K.random_binomial((self.input_dim,), p=retain_p) * (1. / retain_p)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2895, in random_binomial\r\n    tf.zeros(shape, dtype=dtype))\r\nTypeError: where() takes at most 2 arguments (3 given)\r\n```\r\nI have seen two versions of `tf.where()` in the documentation:\r\n\r\n[https://www.tensorflow.org/versions/r0.10/api_docs/python/control_flow_ops/comparison_operators#where](url)\r\nand \r\n[https://www.tensorflow.org/api_docs/python/math_ops/sequence_comparison_and_indexing#where](url)\r\n\r\nAny idea why the program confuses them?\r\n\r\n```\r\npython -c 'import tensorflow as tf; print tf.__version__'\r\n0.10.0\r\n```\r\n\r\n\r\nThank you very much.", "comments": ["Could you try with a more recent version of tensorflow?", "Hi @drpngx I tried by installing version 0.12, but a new error raises \r\n\r\n```\r\nUsing TensorFlow backend.\r\nRuntimeError: module compiled against API version 0xa but this version of numpy is 0x9\r\nTraceback (most recent call last):\r\n  File \"/almac/ignacio/nlp-pipeline/recurrent_ltsm.py\", line 126, in <module>\r\n    from keras.preprocessing.text import Tokenizer\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/__init__.py\", line 2, in <module>\r\n    from . import backend\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.py\", line 67, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: numpy.core.multiarray failed to import\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```", "Are you installing from source? If so, you need to `cd ..` or somewhere else.\r\n\r\nIf not, then it looks like a problem with numpy. stackoverflow says that `sudo pip install -U numpy` might improve things (upgrade numpy).", "Can you try the steps above?", "Hi @drpngx I tried what you kindly suggested, but the error is the same.\r\nGiven I'm in Kears, Theano works well at the moment.", "OK, I'm not sure what it is. This is a keras issue, perhaps @fchollet can help you. Best is to ask on stackoverflow, where we monitor all questions with tag `tensorflow`."]}, {"number": 7041, "title": "Tensorboard: Undefined import in serialize_tensorboard.py", "body": "It looks like this change https://github.com/tensorflow/tensorflow/commit/e121667dc609de978a223c56ee906368d2c4ceef modified the module imports in `serialize_tensorboard.py` to remove the import of `tf` but did not update all the usages of `tf`. As such, `tf` is not defined error is thrown when trying to run the script.", "comments": ["@jart looks like we missed some `tf.foo` in tests?", "Assigning to owner of script.", "I'm going to remove the script.", "The script was removed."]}, {"number": 7040, "title": "Branch 145423129", "body": "", "comments": ["Pushing again with more fixes."]}, {"number": 7039, "title": "Tensorboard stopped showing runs names", "body": "Hi,\r\n\r\nTensorboard stopped showing runs names.\r\nI can still filter (blindly) and see the curves, but there are no names of the runs to filter.\r\n\r\nAny idea on how to fix that?\r\nTF is installed under anaconda.\r\n\r\nIt worked with no issues until 2 weeks ago. I got back to the project today and this issue occurred.\r\nI tried reinstalling 0.11, or even installing the latest 0.12.1 (under conda-forge), but the issue hasn't solved.\r\n\r\nI also tried (with no luck)\r\n```\r\ncd /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/\r\npython tensorboard.py --logdir=path/to/log\r\n```\r\n\r\nAttached is a screenshot, and below (the technical details) is the TB console output. Note that under TF12.1 there are no errors on the console output, but the issue still happens.\r\n\r\n![screenshot from 2017-01-24 17 58 30](https://cloud.githubusercontent.com/assets/5911675/22257868/016e4ac2-e268-11e6-83a9-7e3b33ce5eae.png)\r\n\r\n\r\nThanks\r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/1421\r\nhttps://github.com/tensorflow/tensorflow/issues/5341\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally                                    \u2502\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally                                     \u2502\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally                                     \u2502\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally                                    \u2502\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally                                    \u2502\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\r\n0.11.0rc0 \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\n\r\n$ tensorboard --port=8008 --logdir=\"./\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nStarting TensorBoard 29 on port 8008\r\n(You can navigate to http://XXXXXXXXXXXXXX:8008)\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET / HTTP/1.1\" 200 -\r\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js\r\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/dist/bazel-html-imports.html' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/dist/bazel-html-imports.html\r\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/external/dist/bazel-html-imports.html' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/external/dist/bazel-html-imports.html\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] code 404, message Not Found\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 404 -\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET /lib/css/global.css HTTP/1.1\" 200 -\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\r\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/polymer/polymer.html' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/polymer/polymer.html\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET /polymer/polymer.html HTTP/1.1\" 200 -\r\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/d3/d3.js' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/d3/d3.js\r\n127.0.0.1 - - [24/Jan/2017 18:51:19] \"GET /d3/d3.js HTTP/1.1\" 200 -\r\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/plottable/plottable.js' on path /home/gamir/yuval/anaconda2/envs/tf_011/lib/python2.7/site-packages/tensorflow/tensorboard/plottable/plottable.js\r\n127.0.0.1 - - [24/Jan/2017 18:51:20] \"GET /plottable/plottable.js HTTP/1.1\" 200 -\r\n127.0.0.1 - - [24/Jan/2017 18:51:20] \"GET /data/runs HTTP/1.1\" 200 -\r\n127.0.0.1 - - [24/Jan/2017 18:51:20] \"GET /data/runs HTTP/1.1\" 200 -\r\nWARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.\r\nWARNING:tensorflow:Detected out of order event.step likely caused by a TensorFlow restart. Purging expired events from Tensorboard display between the previous step: -1 (timestamp: -1) and current step: 20084 (timestamp: 1482950339.25). Removing 12 scalars, 23 histograms, 23 compressed histograms, 0 images, and 0 audio.\r\n\r\n```\r\n", "comments": ["@dandelionmane is that intentional?", "I suspect this is a screen resolution / zoom issue. The UI elements on the top left are fixed size, and the remainder goes to the run names. \r\n\r\nCan you try pressing (ctrl + -) or decreasing the zoom level in the browser, and report if that fixes it?", "Embarrassingly (for me) you were right :)\r\nIt happens with a 100% zoom, reducing the zoom to 75%, shows the list of runs. Any idea how to still show it with 100% zoom?", "@yuvval Try changing the resolution on the display. Or you can submit a pull request that reduces the space needed by the UI on the left :) ", "@dandelionmane Thanks. I don't have permissions to change the resolution on that machine, nor knowledge of UI syntax to make a PR :)"]}, {"number": 7038, "title": "multiple dequeue ops are optimized away in latest TF", "body": "Multiple ops Dequeue ops from the same queue started getting optimized away in latest TensorFlow:\r\n\r\nThe following executes dequeue once in Jan17 head, but 3 times in 12.1 \r\n`sess.run([q.dequeue(), q.dequeue(), q.dequeue()])\r\n`\r\n\r\nOne gets expected behavior (3 dequeues) when graph optimization is turned off\r\n\r\n```\r\ntf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess = tf.Session(config = config)\r\n``` \r\n\r\nSelf-contained repro: https://github.com/yaroslavvb/stuff/blob/master/parallel_dequeue_test.py\r\n\r\nCame up in http://stackoverflow.com/questions/41830206/how-to-share-a-queue-containing-variable-length-sequences-batches-between-multip", "comments": ["cc @mrry the queue expert", "Can you print the GraphDef for that program? In particular, I'd like to know if the `FIFOQueueV2` (or some other `...QueueV2`) op is being used.", "Yes, it's using V2 ops for queue. The 12.1 version is using regular\r\n\r\n\r\nhttp://pastebin.com/gsu7xrU7", "@alextp: It looks like the switch to resource-typed queues has caused a subtle bug in some of the queue ops. As far as I can tell, multiple instances of `queue.dequeue()` in the same subgraph are now being treated as common subexpressions, whereas before the fact that they had an incoming ref-typed edge meant they were [never candidates for CSE](https://github.com/tensorflow/tensorflow/blob/3570b5de07c8079d1005c7877e235bd8aa0e0ccc/tensorflow/core/graph/optimizer_cse.cc#L162).\r\n\r\nI can see at least two solutions:\r\n\r\n* Modify the CSE optimizer to reject anything with a resource-typed input as a candidate for elimination.\r\n* Modify all of the `Queue*V2` accessor ops to be \"stateful\", which [also inhibits the optimization](https://github.com/tensorflow/tensorflow/blob/3570b5de07c8079d1005c7877e235bd8aa0e0ccc/tensorflow/core/graph/optimizer_cse.cc#L158).\r\n", "https://github.com/tensorflow/tensorflow/commit/06e3aa655506773e49ce9a855f81ba3eae2a6b88#diff-333845d139890198962551b3a1c2a33b should have fixed this issue (it adds a test, even)", "I just tried in tf1.0 and this behavior seems to still be here back. To reproduce, try this\r\n\r\n```\r\nimport tensorflow as tf\r\ninput1 = tf.train.range_input_producer(10, shuffle=False)\r\ninput1_batch = [input1.dequeue(),input1.dequeue()]\r\n\r\nconfig = tf.ConfigProto()\r\n#config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\ninput2 = tf.train.batch([input1_batch], batch_size=4, enqueue_many=True)\r\n\r\nsess = tf.InteractiveSession(config=config)\r\ntf.train.start_queue_runners()\r\n\r\nfor i in range(5):\r\n    print(sess.run(input2))\r\n```\r\nResult is\r\n\r\n```\r\n[0 0 1 1]\r\n[2 2 3 3]\r\n[4 4 5 5]\r\n[6 6 7 7]\r\n[8 8 9 9]\r\n```\r\n\r\nIf you uncomment \"#config\" line to remove rewriting optimization, then result is as expected\r\n\r\n```\r\n[0 1 3 2]\r\n[5 4 6 7]\r\n[8 9 1 0]\r\n[2 3 4 5]\r\n[7 6 9 8]\r\n```\r\n\r\nVersion info:\r\n```\r\n>>> tf.__version__\r\n'1.0.0'\r\n\r\n>>> sys.version\r\n'3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 20:51:01) \\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]'\r\n\r\n```\r\n", "It seems like the commit which fixed this bug didn't make the cut into the 1.0 release. master is fixed, and so will 1.1.", "If you need the fix by itself cherry-pick https://github.com/tensorflow/tensorflow/commit/06e3aa655506773e49ce9a855f81ba3eae2a6b88#diff-333845d139890198962551b3a1c2a33b", "Got it, I forgot that releases are non-linear (it worked in version I built before TF 1.0 was released)"]}, {"number": 7037, "title": "Replace tf.mul by tf.multiply in docs.", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 7036, "title": "tf slim, default initializers for relu", "body": "The defaults in layers such as conv and conv_transpose have the default activations as relu and the default initializer as xavier. From the paper https://arxiv.org/pdf/1502.01852v1.pdf, would this be the time to change the default initializer to use tf.contrib.layers.variance_scaling_initializer to account for the variance of the distributions being halved at each layer of activation?\r\n\r\nAndrej Karpathy has a lecture which shows the effects of this https://youtu.be/gYpoJMlgyXA?t=47m6s\r\n\r\nedit:forgot to add time in the video\r\n", "comments": ["@nathansilberman @sguada is that something we'd want to do? (change the default)", "@martinwicke could you comment?\r\n", "Due to backwards compatibility guarantees, we will not be able to change anything under core.\r\nTherefore, we are not going to be able to change this for most of TF.\r\n"]}, {"number": 7035, "title": "Build with --config==cuda just runs fine, with --config== fails over and over", "body": "### Environment info\r\nOperating System: Ubuntu 14.04 64 bit\r\n\r\nInstalled version of CUDA and cuDNN: Cuda7.5, CuDNN 7.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nspu@TOBCAT:~$ ls -l /usr/local/cuda-7.5/lib/libcud*\r\n-rw-r--r-- 1 root root 189170 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Aug  8 16:03 /usr/local/cuda-7.5/lib/libcudart_static.a\r\n```\r\n\r\nI tried following the `build from sources` directives from `https://www.tensorflow.org/get_started/os_setup#installing_from_sources` but as soon as I get to the command `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` i get the following issues\r\n\r\n```\r\nspu@TOBCAT:~/Frameworks/deeplearning/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Found 1 target...\r\nERROR: /home/spu/.cache/bazel/_bazel_spu/d4f375c9af64c0d3baa31a5949dd9b54/external/protobuf/BUILD:113:1: C++ compilation of rule '@protobuf//:protobuf' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 43 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3.875s, Critical Path: 1.50s\r\n```\r\n\r\nWent googling for possible related issues, but none of the suggestions seem to work out. Using Tensorflow without CUDA seems to kill the reason why I am willing to use Tensorflow in the first place ...", "comments": ["It looks like the host compiler is improperly configured. `cc1plus` is the compiler that's called after preprocessing is done.\r\n\r\nCan you try gcc? It would seem that gcc doesn't work. Try re-installing it perhaps.", "Also, another possibility.\r\nWhen running configure, did you enable GPU support?", "Sorry for the delay, but was not at work until now to try this out. It seems the following steps enabled me to get everything fixed and building\r\n\r\n- Reinstalled `sudo apt-get install build-essentials gcc g++`\r\n- Based on that, reinstalled CUDA7.5, to ensure CUDA was built using the correct compiler\r\n- Then re-ran `./configure`\r\n\r\nNow the command `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` seems to be doing building, just like the non-gpu version. I will close this, and reopen if something goes wrong.\r\n\r\nThank you for the help!", "I will re-open this issue, because I still have some problems. So compiling went fine, then run the command  `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` and then ran `sudo pip install /tmp/tensorflow_pkg/tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl` which should be the last step of the installation... \r\n\r\nHowever I get the following error\r\n\r\n```\r\ntensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\nStoring debug log for failure in /home/spu/.pip/pip.log\r\n```\r\n\r\nThe log says\r\n\r\n```\r\n------------------------------------------------------------\r\n/usr/bin/pip run on Fri Jan 27 10:11:46 2017\r\ntensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 257, in run\r\n    InstallRequirement.from_line(name, None))\r\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 168, in from_line\r\n    raise UnsupportedWheel(\"%s is not a supported wheel on this platform.\" % wheel.filename)\r\nUnsupportedWheel: tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\n```", "I guess it has to do I told the installation to use python3 but pip seems to use python 2.7, if anyone knows how to handle this, do let me know.", "Found it! Changed command to `sudo pip3 install /tmp/tensorflow_pkg/tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl`\r\n\r\nI suggest to update the tutorial to mention this!\r\n"]}, {"number": 7034, "title": "QueueRunner hanging", "body": "I find the queuerunner is hanging in the code below unless I uncomment the line \r\n\r\n`#threads = tf.train.start_queue_runners(sess=sess, coord=coord)`\r\n\r\nThis isn't mentioned on the documentation \r\n[https://www.tensorflow.org/versions/r0.10/how_tos/threading_and_queues/]\r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport string\r\n\r\ndef run_training():\r\n  # Tell TensorFlow that the model will be built into the default Graph.\r\n  with tf.Graph().as_default():\r\n    with tf.name_scope('input'):\r\n      alph_initializer = tf.placeholder(\r\n          dtype=tf.string,\r\n          shape=[26,1])\r\n      input_alph = tf.Variable(\r\n          alph_initializer, trainable=False, collections=[])\r\n\r\n      alph = tf.train.slice_input_producer(\r\n          [input_alph], shuffle=False, capacity=1,num_epochs=1)\r\n      alphs = tf.PaddingFIFOQueue(26,\r\n                              tf.string,\r\n                              [[1]])\r\n\r\n      alphs_qr = tf.train.QueueRunner(queue=alphs,enqueue_ops=[alphs.enqueue(alph)] )\r\n\r\n    my_list_val = np.array(list(string.ascii_lowercase)).reshape(26,1)\r\n\r\n\r\n    # Create the op for initializing variables.\r\n    init_op = tf.initialize_all_variables()\r\n\r\n    # Create a session for running Ops on the Graph.\r\n    sess = tf.Session()\r\n\r\n    # the Op to initialize the variables.\r\n    sess.run(init_op)\r\n\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n    # Start input enqueue threads.\r\n\r\n    coord = tf.train.Coordinator()\r\n\r\n    sess.run(input_alph.initializer,\r\n             feed_dict={alph_initializer: my_list_val})\r\n    collection = []\r\n    #If I uncomment the below line then \r\n    #threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    enqueue_threads = alphs_qr.create_threads(sess, coord=coord, start=True)\r\n    try:\r\n        while not coord.should_stop():\r\n            char =  sess.run(alphs.dequeue())\r\n            collection.append(char[0])\r\n            print(\"String val\", char)\r\n    except tf.errors.OutOfRangeError:\r\n        print('here')\r\n    finally:\r\n        coord.request_stop()\r\n    coord.join(enqueue_threads)\r\n\r\n    sess.close()\r\n\r\n\r\ndef main(_):\r\n  run_training()\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n```", "comments": ["Yes, you always have to start the queue runners by hand. If the documentation was incomplete, please send a PR to amend it. Thanks!", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen."]}, {"number": 7033, "title": "Branch 145375395", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Fixed email on the merge commit to fix CLA.\r\n", "Well, this has a bad change for android build scripts. So we also abandon\nthis one...\n\nOn Jan 23, 2017 11:27 PM, \"drpngx\" <notifications@github.com> wrote:\n\n> *@drpngx* approved this pull request.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7033#pullrequestreview-18097042>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOVZphbH-lUUIWYlVe4zA4NgvZ3G3ks5rVafSgaJpZM4Lr613>\n> .\n>\n"]}, {"number": 7032, "title": "Have tf.sub  and tf.mul been removed from latest TF build ? ", "body": "I am running TF 0.12.head and ran into this issue where tf.sub was not found. I am writing this since the [docs](https://www.tensorflow.org/api_docs/python/math_ops/) don't seem to indicate any such thing.", "comments": ["Yes, it's been renamed to `tf.subtract` (and `tf.multiply` respectively), to align with numpy.", "Thanks!", "Thanks! just ran into same issue when tensorflow update to version 1.0", "Thanks. just found it. updated my code.", "Thanks. ", "I learned from Google search. Thank you.", "Thanks!"]}, {"number": 7031, "title": "TypeError: Expected int32, got list containing Tensors of type '_Message' instead.", "body": "Hi,\r\nI am using TF ver 0.12.head on Ubuntu 16.04 Python 2.7 and following [this](https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb) for learning inception model in TF Slim\r\n```\r\n\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nimport urllib2\r\n\r\nfrom datasets import imagenet\r\nfrom nets import inception\r\nfrom preprocessing import inception_preprocessing\r\n\r\nslim = tf.contrib.slim\r\n\r\nbatch_size = 3\r\nimage_size = inception.inception_v1.default_image_size\r\ncheckpoints_dir = '/tmp/checkpoints/'\r\nwith tf.Graph().as_default():\r\n    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\r\n    image_string = urllib2.urlopen(url).read()\r\n    image = tf.image.decode_jpeg(image_string, channels=3)\r\n    processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\r\n    processed_images  = tf.expand_dims(processed_image, 0)\r\n    \r\n    # Create the model, use the default arg scope to configure the batch norm parameters.\r\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\r\n        logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)\r\n    probabilities = tf.nn.softmax(logits)\r\n    \r\n    init_fn = slim.assign_from_checkpoint_fn(\r\n        os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\r\n        slim.get_model_variables('InceptionV1'))\r\n    \r\n    with tf.Session() as sess:\r\n        init_fn(sess)\r\n        np_image, probabilities = sess.run([image, probabilities])\r\n        probabilities = probabilities[0, 0:]\r\n        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]\r\n        \r\n    plt.figure()\r\n    plt.imshow(np_image.astype(np.uint8))\r\n    plt.axis('off')\r\n    plt.show()\r\n\r\n    names = imagenet.create_readable_names_for_imagenet_labels()\r\n    for i in range(5):\r\n        index = sorted_inds[i]\r\n        print('Probability %0.2f%% => [%s]' % (probabilities[index], names[index]))\r\n```\r\nHowever I am getting the following issue when running this:\r\n```\r\n    logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)\r\n  File \"/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py\", line 290, in inception_v1\r\n    net, end_points = inception_v1_base(inputs, scope=scope)\r\n  File \"/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py\", line 96, in inception_v1_base\r\n    net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1053, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 651, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n```", "comments": ["Thanks for contacting us! This is better asked on stackoverflow, where we monitor all questions with the `tensorflow` tag.", "I did actually ask this on StackOverflow  [here](http://stackoverflow.com/questions/41813665/tensorflow-slim-typeerror-expected-int32-got-list-containing-tensors-of-type). Please feel free to respond.", "Oh, I didn't realize it was working on 0.11. Our tensorflow/models are woefully out of date and we don't really have the bandwidth to support them.", "For now, I have reverted to TF 0.11 to get this to work until this is resolved in TF0.12.", "Looks similar to a problem when the parameter order was swapped. Maybe @sguada knows something about that.", "I am seeing this error when running this example for the udacity course: [/examples/udacity/6_lstm.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb)", "@mlucool `tf.concat` has changed the order of parameters. You can check the doc to see the difference.\r\n\r\n", "Ah that solved it! Thanks! I should have downloaded from the 0.12.1 tag to be sure", "I have the same problem.", "I should be told to use the slim instead of inception", "Hi, \r\nI am using TF 1.0.0 on ubuntu 14.04 64bit os , I also meets this error, can you help to resolve it, thank you.\r\n\r\nTraceback (most recent call last):\r\n  File \"JP_train.py\", line 409, in <module>\r\n    initial_weights=initial_weights)\r\n  File \"JP_train.py\", line 277, in train\r\n    x, y, y_t = JP_model.get_googlenet_min_model(batch_size,0.5)\r\n  File \"/mnt/tolly/work/git/10.5.253.119/tools/2D/AP/Deep_ANPR/JP/JP_model.py\", line 195, in get_googlenet_min_model\r\n    incept3a = _inception(pool3,    192, 64, 96, 128, 16, 32, 3, 32)\r\n  File \"/mnt/tolly/work/git/10.5.253.119/tools/2D/AP/Deep_ANPR/JP/JP_model.py\", line 113, in _inception\r\n    incept = array_ops.concat(3, [conv1, conv3, conv5, pool])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1047, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 651, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n", "I also get this error while executing \"ptb_word_lm.py\" . Any help appreciated", "hi all the issue is due to Keras version. I tried above all without any success. Uninstall Keras and install via pip. It worked for me.\r\n\r\nI was facing this error with Keras 1.0.2 & resolved with Keras 1.2.0\r\n\r\nHope this will help. Thank you", "Unfortunately I seem to be having the same issue with the latest keras-1.2.2. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/daniel/Documents/git/serving/tf_models/bazel-bin/textsum/seq2seq_attention.runfiles/__main__/textsum/seq2seq_attention.py\", line 213, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/daniel/Documents/git/serving/tf_models/bazel-bin/textsum/seq2seq_attention.runfiles/__main__/textsum/seq2seq_attention.py\", line 208, in main\r\n    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)\r\n  File \"/home/daniel/Documents/git/serving/tf_models/textsum/seq2seq_attention_decode.py\", line 92, in __init__\r\n    self._model.build_graph()\r\n  File \"/home/daniel/Documents/git/serving/tf_models/textsum/seq2seq_attention_model.py\", line 298, in build_graph\r\n    self._add_seq2seq()\r\n  File \"/home/daniel/Documents/git/serving/tf_models/textsum/seq2seq_attention_model.py\", line 201, in _add_seq2seq\r\n    self._enc_top_states = tf.concat(1, encoder_outputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1029, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 637, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 702, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 110, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 99, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n\r\n```", "I am getting the same error on Keras 2.0.1", "@tollytao \r\n\r\n> incept = array_ops.concat(3, [conv1, conv3, conv5, pool])\r\n\r\nit need to change to\r\n\r\n> incept = array_ops.concat( [conv1, conv3, conv5, pool], 3)\r\n\r\nor\r\n\r\n> incept = tf.concat( [conv1, conv3, conv5, pool], 3)", "@samfk6  \r\nI will try it later, thank you very much :) ", "Just to contextualize, these changes have been placed to have a consistent format with Numpy. Ex: numpy.concatenate((a1, a2, ...), axis=0)", "Closing as WAI: the order of `tf.concat`'s arguments changed with 1.0.", "i have the same problem \uff1f", "why change the order of tf.concat's arguments???", "@zzks It was changed to match numpy.", "i am also having the same issue,\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\noutputs_forward,- shape,(1, 2, 100)\r\noutputs_backward-shape (1, 2, 100)\r\noutput = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\r\n\r\ntf version -tensorflow-gpu==0.12.0", "from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport time\r\nimport logging\r\n\r\nimport numpy as np\r\n# from six.moves import xrange \r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n\r\nclass Encoder(object):\r\n\r\n    def __init__(self,hidden_size,initializer=lambda:None):\r\n        self.hidden_size=hidden_size\r\n        self.initializer=initializer\r\n\r\n    def encode(self,inputs,span):\r\n\r\n        document,answer = inputs\r\n        start_idx,end_idx = span\r\n\r\n\r\n\r\n\r\n        with tf.variable_scope('encode'):\r\n            # weights = tf.Variable(tf.random_normal([2,3], stddev=0.35),name=\"weights\")\r\n            # biases = tf.Variable(tf.zeros([1]), name=\"biases\")\r\n            # label  = tf.Variable(tf.random_normal([2,3],stddev=0.35),name='lable')\r\n            # weights = tf.Variable(shape=[None,None],name=\"weights\")\r\n            # biases = tf.Variable(tf.zeros([None]), name=\"biases\")\r\n            # label  = tf.Variable(shape=[None,None],name='lable')\r\n            print(document.get_shape())\r\n            document_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\r\n            document_vector=document_vector_orig\r\n            for i in range(tf.shape(document_vector)[0]):\r\n                document_vector[i]=document_vector[i][start_idx:end_idx]\r\n            annotation_sequence=tf.concat(document_vector,answer,axis=2, name='annotation_sequence')\r\n            answer_vector=self.bidirectional_LSTM(annotation_sequence,self.hidden_size,self.initializer,output_sequence=False)\r\n\r\n            r=tf.matmul(lable,answer_vector)+(tf.reduce_sum(document_vector_orig,2,keep_dims=True))\r\n            intial_state=tf.tanh(tf.matmul(r,weights)+biases)\r\n\r\n        return document_vector_orig,answer_vector ,intial_state\r\n\r\n            \r\n\r\n\r\ndef bidirectional_LSTM(input, hidden_state_dimension, initializer, sequence_length=None, output_sequence=True):\r\n\r\n    with tf.variable_scope(\"bidirectional_LSTM\"):\r\n        initializer=None\r\n        print(sequence_length)\r\n        if sequence_length == None:\r\n            batch_size = 1\r\n            sequence_length = tf.shape(input)[1]\r\n            sequence_length = tf.expand_dims(sequence_length, axis=0, name='sequence_length')\r\n        else:\r\n            batch_size = tf.shape(sequence_length)[0]\r\n\r\n        lstm_cell = {}\r\n        initial_state = {}\r\n        for direction in [\"forward\", \"backward\"]:\r\n            with tf.variable_scope(direction):\r\n                # LSTM cell\r\n                lstm_cell[direction] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(hidden_state_dimension, \\\r\n                                        forget_bias=1.0, initializer=initializer, state_is_tuple=True)\r\n                # initial state: http://stackoverflow.com/questions/38441589/tensorflow-rnn-initial-state\r\n                initial_cell_state = tf.get_variable(\"initial_cell_state\", shape=[1, hidden_state_dimension], \\\r\n                                                    dtype=tf.float32, initializer=initializer)\r\n                initial_output_state = tf.get_variable(\"initial_output_state\", shape=[1, hidden_state_dimension],\\\r\n                                                     dtype=tf.float32, initializer=initializer)\r\n                c_states = tf.tile(initial_cell_state, tf.stack([batch_size, 1]))\r\n                h_states = tf.tile(initial_output_state, tf.stack([batch_size, 1]))\r\n                initial_state[direction] = tf.nn.rnn_cell.LSTMStateTuple(c_states, h_states)\r\n\r\n        # sequence_length must be provided for tf.nn.bidirectional_dynamic_rnn due to internal bug\r\n        outputs, final_states = tf.nn.bidirectional_dynamic_rnn(lstm_cell[\"forward\"],\r\n                                                                    lstm_cell[\"backward\"],\r\n                                                                    input,\r\n                                                                    dtype=tf.float32,\r\n                                                                    sequence_length=sequence_length,\r\n                                                                    initial_state_fw=initial_state[\"forward\"],\r\n                                                                    initial_state_bw=initial_state[\"backward\"])\r\n        if output_sequence == True:\r\n            outputs_forward, outputs_backward = outputs\r\n            print(outputs)\r\n            print(outputs_forward.get_shape())\r\n            print(outputs_backward.get_shape())\r\n            # print(outputs_forward[0][1][1])\r\n            output = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\r\n        else:\r\n\r\n            final_states_forward, final_states_backward = final_states\r\n            output = tf.concat([final_states_forward[1], final_states_backward[1]], 1, name='output')\r\n\r\n    return output\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef test():\r\n\r\n    hidden_size=100\r\n    encoder=Encoder(hidden_size)\r\n    document_placeholder=tf.placeholder(tf.float32,(None,2,4),name='Document_placeholder')\r\n    answer_paceholder=tf.placeholder(tf.float32,(None,2,3),name='Answer_paceholder')\r\n    start_id=tf.placeholder(tf.float32,(None),name='start_id')\r\n    end_id=tf.placeholder(tf.float32,(None),name='end_id')\r\n\r\n    paragraph =[[0,1],[1,0],[0,2],[5,3]]\r\n\r\n    question = [[3,3],[5,5],[1,1]]\r\n    func = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\r\n\r\n    init = tf.global_variables_initializer()\r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n        HQ,HP,state = sess.run(func, feed_dict = {document_placeholder :paragraph, answer_paceholder : question,start_id : 1, \\\r\n            end_id : 2}) \r\n    print(HQ.get_shape().as_list())\r\n    print(HP.get_shape().as_list())\r\n\r\n\r\nif __name__ == '__main__':\r\n    test()\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n(?, 2, 4)\r\nNone\r\n(<tf.Tensor 'encode/bidirectional_LSTM/BiRNN/FW/FW/transpose:0' shape=(1, 2, 100) dtype=float32>, <tf.Tensor 'encode/bidirectional_LSTM/ReverseSequence:0' shape=(1, 2, 100) dtype=float32>)\r\n(1, 2, 100)\r\n(1, 2, 100)\r\nTraceback (most recent call last):\r\n  File \"q_gen_model.py\", line 132, in <module>\r\n    test()\r\n  File \"q_gen_model.py\", line 120, in test\r\n    func = encoder.encode([document_placeholder,answer_paceholder],[start_id,end_id])\r\n  File \"q_gen_model.py\", line 38, in encode\r\n    document_vector_orig=bidirectional_LSTM(document,self.hidden_size,self.initializer,sequence_length=None)\r\n  File \"q_gen_model.py\", line 95, in bidirectional_LSTM\r\n    output = tf.concat([outputs_forward, outputs_backward],2,name='output_sequence')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1075, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n", "parameters changed \r\n\r\n`tf.concat(0,[t1, t2])  \r\n`\r\nto \r\n`tf.concat([t1, t2], 0) `\r\n\r\nsolved my issue but why they need to change it ? i dont understand why some parameters is changing always like this way.", "examples from api doc\r\n\r\n https://www.tensorflow.org/api_docs/python/tf/concat\r\n\r\n\r\n      t1 = [[1, 2, 3], [4, 5, 6]]\r\n      t2 = [[7, 8, 9], [10, 11, 12]]\r\n      tf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\r\n      tf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\r\n\r\n      # tensor t3 with shape [2, 3]\r\n      # tensor t4 with shape [2, 3]\r\n       tf.shape(tf.concat([t3, t4], 0))  # [4, 3]\r\n       tf.shape(tf.concat([t3, t4], 1))  # [2, 6]", "Try to your tensorflow version if your tensorflow==0.12.0", "> parameters changed\r\n> \r\n> `tf.concat(0,[t1, t2])`\r\n> to\r\n> `tf.concat([t1, t2], 0)`\r\n> \r\n> solved my issue but why they need to change it ? i dont understand why some parameters is changing always like this way.\r\n\r\nWorked For me Too :P", "Hi, i get the same TypeError message but i don't know why. I do not use `tf.concat()`\r\nI'd appreciate it if you could help me solve this issue. I have keras version 2.2.4 and tensorflow version 1.11.0 but i have tried it with several different versions as well. Furthermore i have python 3.6.3 on a MacOS 10.10.5. \r\nHere's a snippet from the code\r\n`word_model_in = Input(shape=(context_window,))`\r\n`word_model_out = Embedding(output_dim=embedding_matrix.shape[1],\r\n                         input_dim=embedding_matrix.shape[0],\r\n                         input_length=context_window,\r\n                         weights=[embedding_matrix],\r\n                         mask_zero=True)(word_model_in)`\r\n`word_model_out1 = Bidirectional(LSTM(100,input_shape=(None,3,300),return_sequences=False))(word_model_out)`\r\n`word_model = Model(word_model_in, word_model_out1)`\r\n`word_model.summary()`\r\n\r\nAnd here's the error message\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-41-4a0bf7e03efd> in <module>()\r\n      5                          weights=[embedding_matrix],\r\n      6                          mask_zero=True)(word_model_in)\r\n----> 7 word_model_out1 = Bidirectional(LSTM(100,input_shape=(None,3,300),return_sequences=False))(word_model_out)\r\n      8 word_model = Model(word_model_in, word_model_out1)\r\n      9 word_model.summary()\r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py in __call__(self, inputs, **kwargs)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py in call(self, inputs, mask)\r\n    210         kwargs = {}\r\n    211         if has_arg(self.layer.call, 'training'):\r\n--> 212             kwargs['training'] = training\r\n    213         uses_learning_phase = False\r\n    214 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py in call(self, inputs, mask, initial_state, training)\r\n    288             (e.g. via the `input_shape` argument)\r\n    289 \r\n--> 290     # Input shape\r\n    291         3D tensor with shape `(batch_size, timesteps, input_dim)`.\r\n    292 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py in preprocess_input(self, inputs, training)\r\n   1031             warnings.warn('The `implementation` argument '\r\n   1032                           'in `SimpleRNN` has been deprecated. '\r\n-> 1033                           'Please remove it from your layer call.')\r\n   1034         if K.backend() == 'theano' and (dropout or recurrent_dropout):\r\n   1035             warnings.warn(\r\n\r\n~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in concatenate(tensors, axis)\r\n   1525     \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\r\n   1526 \r\n-> 1527     This function is more numerically stable than log(sum(exp(x))).\r\n   1528     It avoids overflows caused by taking the exp of large inputs and\r\n   1529     underflows caused by taking the log of small inputs.\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in concat(concat_dim, values, name)\r\n   1073   ```\r\n   1074 \r\n-> 1075   would produce:\r\n   1076 \r\n   1077   ```python\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n    667       `TypeError`.\r\n    668     \"\"\"\r\n--> 669     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n    670                     \"Use `if t is not None:` instead of `if t:` to test if a \"\r\n    671                     \"tensor is defined, and use TensorFlow ops such as \"\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    174     TypeError: if shape is incorrectly specified or unsupported.\r\n    175   \"\"\"\r\n--> 176   ctx = context.context()\r\n    177   if ctx.executing_eagerly():\r\n    178     t = convert_to_eager_tensor(value, ctx, dtype)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    163 \r\n    164     shape:          Optional dimensions of resulting tensor.\r\n--> 165 \r\n    166     name:           Optional name for the tensor.\r\n    167 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    365 \r\n    366   Returns:\r\n--> 367     A `TensorProto`. Depending on the type, it may contain data in the\r\n    368     \"tensor_content\" attribute, which is not directly useful to Python programs.\r\n    369     To access the values you should convert the proto back to a numpy ndarray\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\r\n    300 \r\n    301 \r\n--> 302 def _FilterStr(v):\r\n    303   if isinstance(v, (list, tuple)):\r\n    304     return _FirstNotNone([_FilterStr(x) for x in v])\r\n\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.`"]}, {"number": 7030, "title": "Feature Request: an op that returns bytes_in_use for its device", "body": "We are trying to optimize some models to fit into TitanX's 12GB of RAM, and it's hard because of lack of transparency in TF available memory.\r\n\r\nWhat would help this situation is an op that returns amount of bytes on its device when executed. Something similar to what's done in [stack_ops](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L222) for memory-aware heuristics\r\n\r\n```\r\nDeviceContext* device_ctxt = ctx->op_device_context();\r\nauto device = static_cast<tensorflow::Device*>(ctx->device());\r\nAllocator* allocator = device->GetAllocator(alloc_attrs);\r\nAllocatorStats stats;\r\nallocator->GetStats(&stats)\r\n//  output stats.bytes_in_use\r\n\r\n```\r\n\r\nThis op can be wedged between other ops using control dependencies and used for memory debugging. This is complementary to request in https://github.com/tensorflow/tensorflow/issues/6716 because it would account for memory from parallel run calls, variables, persistent tensors.\r\n\r\nI can take this issue if this op fits into TF framework", "comments": ["@prb12 @mrry @vrv \r\nThat sounds like a pretty useful idea.", "@yaroslavvb maybe try writing it as a custom op using load_op_library for now so you can try the idea out and see how useful it is for you first -- can always add it to the core later if it's widely useful.", "load_library seems like a great idea. Can custom ops depend on `//tensorflow/core:core_cpu`? I need this for Allocator header `tensorflow/core/common_runtime/device.h` but adding the dependency fails\r\n\r\n```\r\ntensorflow/core:core_cpu cannot depend on tensorflow/core:framework.\r\nERROR: Analysis of target '//tensorflow/core/user_ops:bytes_in_use.so' failed; build aborted.\r\n```\r\n\r\nhttps://github.com/yaroslavvb/tensorflow/blob/custom_op/tensorflow/core/user_ops/BUILD", "`core_cpu` is an internal target, but `core` in that directory has includes everything and has `//visibility:public`.", "Same error when depending on `core:core`, this seems like a circular dependency somewhere\r\n\r\n```\r\ntensorflow/core:core cannot depend on tensorflow/core:framework.\r\nERROR: Analysis of target '//tensorflow/core/user_ops:bytes_in_use.so' failed; build aborted.\r\n\r\n```", "From the error message it looks like \"core\" depends on \":framework\" somehow. That should fail everywhere. Do you need that build file in `core/user_ops`? It looks like it's globbed by `user_ops_libs` already.", "Yes, the error message doesn't make sense. I followed instructions in https://www.tensorflow.org/how_tos/adding_an_op/ which says to put `BUILD` file in `core/user_ops`. It passes analysis if I don't add dependency to `//tensorflow/core:core_cpu` or `//tensorflow/core:core`, but then bazel complains about missing `device.h`\r\n\r\ncc @keveman in case he knows how to properly have user ops depend on `device.h` (part of `//tensorflow/core:core_cpu` target)", "OK, custom op turned out to be relatively straightforward, I put it here: https://github.com/yaroslavvb/memory_probe_ops\r\n\r\nOne snag is that when the op is placed on CPU, the allocator returns 0 for `bytes_in_use`, so this only works or GPUs. I still see LOGMEMORY messages coming from cpu allocations, so it seems this info gets lost somewhere.\r\n\r\nAlso, when graph optimization are turned on, it sometimes returns 0, and I suspect that ctx->device()->GetAllocator() gives me a CPU allocator. This is despite run_metadata and log_device_placement showing the op as existing on GPU. Things go back to normal when setting up session with `tf.OptimizerOptions.L0`", "Maybe you have to run it with `control_dependencies`?", "I got around the optimization issue by adding `.SetIsStateful()` to my op. Regarding CPU allocations, looks like by default byte tracking it is turned off. There's a function `EnableCPUAllocatorStats(true);` which I can call from the kernel, but it doesn't do anything. I suspect the problem is that the underlying variable is defined as `static bool cpu_allocator_collect_stats = false;`, so it can't be modified outside of `pywraptensorflow.so`", "Right. If you're linked against it, it should.\r\n\r\nFor some reason we chose to use a `Mutex`, but I think `std::atomic` would have worked fine.", "So I think for this feature to be complete using `load_library` mechanism, what's missing is the ability to enable CPU allocator stats either from TF client, or from dynamically loaded op kernel. @michaelisard  any suggestions?", "BTW, the issue with using Bazel to build such op is that the such internal dependencies are disallowed. IE there's this [line](https://github.com/tensorflow/tensorflow/blob/a0d784bdd31b27e013a7eac58a86ba62e86db299/tensorflow/tensorflow.bzl#L701) in tf_custom_op\r\n\r\n`             disallowed_deps=[\"//tensorflow/core:framework\",\r\n`\r\n\r\nSince `//tensorflow/core:core` depends on `//tensorflow/core:framework` and `//tensorflow/core:framework` is disallowed, I can't have a custom op depending on `//tensorflow/core:core`", "@keveman, @josh11b  I feel like we should allow the following public headers from libraries as we do in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L22\r\n\r\n@yaroslavvb have you tried not using bazel and trying to build your code using https://www.tensorflow.org/how_tos/adding_an_op/#building_the_op_library and then just loading the .so?  I suspect that will work because we package more headers there.", "@vrv indeed, I was able to compile it successfully using g++ on Mac and Linux, and released final product here -- https://github.com/yaroslavvb/memory_probe_ops\r\n\r\nCuriously, I can build it using Bazel on MacOS without adding that dependency, using same version of bazel (0.4.3), it seems in MacOS the build is not fully hermetic\r\n\r\nThe only remaining issue is that cpu allocator always reports 0 bytes allocated. There's this line in `allocator.cc`\r\n\r\n`static bool cpu_allocator_collect_stats = false;\r\n`\r\n\r\nBut it feels like it should be superceded by this line:\r\n\r\n`  if (cpu_allocator_collect_full_stats || LogMemory::IsEnabled()) {\r\n`\r\nso I'm still tracking that one down", "@josh11b suggested this op is not a good target for user-op in long-term because it would require opening up Allocator API. Made a tracking issue here https://github.com/tensorflow/tensorflow/issues/7581\r\n", "@yaroslavvb Hi, any update on this ? I can close this if there is a solution provided for the op requested or any workaround is done.", "I think it can be closed, there are now ops which return memory + memory is available in timeline, I have examples of extracting memory from timeline in https://github.com/yaroslavvb/memory_util", "Sounds good. Thank you !"]}, {"number": 7029, "title": "tensorflow has no attribute 'parallel_stack'", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I was trying import tf.parallel_stack, but I got following error:\r\n'module' object has no attribute 'parallel_stack'\r\nI am using tensorflow version 0.12, python 2.7 in ubuntu 14.\r\nThanks", "This op was removed in https://github.com/tensorflow/tensorflow/commit/705cc933, the replacement is called `tf.stack`"]}, {"number": 7028, "title": "Update roadmap.md", "body": "Update to the roadmap. I may forget things, please add them.\r\n\r\nFYI: @petewarden @vrv @benoitsteiner @jhseu, please add people or suggest items as needed.", "comments": ["Don't forget to update the year to 2017, too.", "Thanks!"]}, {"number": 7027, "title": "Is there any GPU implementation of tf.stack for int64? ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I am wondering if there is any GPU implementation of tf.stack? It seems that tf.stack only can run on CPU. Is there any equivalent command that I can use it in GPU? \r\nThanks ", "Native op corresponding to `tf.stack` seems to be called Pack and I see a GPU kernel registration for it in [pack_ops.cc](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/pack_op.cc#L142)", "Yes, to second what @yaroslavvb said, there is one and it's calling [`ConcatGPU`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pack_op.cc#L116) if the conditions are right. You shouldn't have to do anything to invoke it.", " @yaroslavvb thanks for response. but it seems that tf.pack also just runs in CPU. I am using tf.device to put it in the gpu, but after looking at log_device_placement, it is still pinned to CPU. DO u have any suggestion? I can upload the code if u want it. Thanks  ", "what if you run with soft_device_placement=False? Also, I'm not sure where\nlog_device_placement takes assignments from, I normally look at\nrun_metadata to see where it gets put\n\nrun_metadata = tf.RunMetadata()\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nsess.run(.., options=run_options, run_metadata=run_metadata)\nprint(run_metadata)\n\nOn Mon, Jan 23, 2017 at 7:43 PM, amortazi <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> thanks for response. but it\n> seems that tf.pack also just runs in CPU. I am using tf.device to put it in\n> the gpu, but after looking at log_device_placement, it is still pinned to\n> CPU. DO u have any suggestion? I can upload the code if u want it. Thanks\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7027#issuecomment-274693375>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHM4MWcR1P7kAq3b_THV-URqCj_qFks5rVXNhgaJpZM4LrvFB>\n> .\n>\n", "@yaroslavvb I got following error with soft_device_placement=False:\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'pack': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: pack = Pack[N=4, T=DT_INT64, axis=0, _device=\"/device:GPU:0\"](mul_1, floordiv, floordiv_1, mul_3)]]\r\n\r\nUnfortunately, it is in the middle of my CNN and it causes data passes to CPU and increases my training time significantly. I appreciate if u have any solution for it. ", "Oh, it's `dtypes.int64`. The [GPU number types](https://github.com/tensorflow/tensorflow/blob/a304537954a865752ad1b18461e6bd67b36082db/tensorflow/core/framework/register_types.h#L171) are float and double.", "Could you try to add `TF_CALL_int64(REGISTER_GPU)` in `pack_ops.cc`?", "I need to install tf from source, i am working on it! ", "@drpngx should I just add TF_CALL_int64(REGISTER_GPU) in pack)ops.cc? May i ask where I should add it in the code? \r\nThanks,\r\nAli", "Just before the `#undef` [here](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/pack_op.cc#L148) should be fine.", "I am using tensorflow 0.12 which tf.pack is not defined for this version. So, I have to use tf.stack here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/stack_ops.cc#L265\r\n\r\nWould you tell me how I should this file to work with int64?", "`tf.stack` just calls the [pack op](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L843). It's confusing just for historical reasons. So the modification in `pack_ops.cc` is correct.", "It didn't work after applying the changes. Still tf.stack is pinned to CPU:\r\n\r\n> stack: (Pack): /job:localhost/replica:0/task:0/cpu:0\r\n2017-01-25 19:16:59: I tensorflow/core/common_runtime/simple_placer.cc:841] stack: (Pack)/job:localhost/replica:0/task:0/cpu:0\r\n\r\nI am using tf.device to pin it to GPU (with allow_soft_placement = True).\r\nAny suggestion?  ", "And this is an error I got with allow_soft_placement = False : \r\n\r\n> InvalidArgumentError (see above for traceback): Cannot assign a device to node 'stack': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: stack = Pack[N=4, T=DT_INT64, axis=0, _device=\"/device:GPU:0\"](mul_1, floordiv, floordiv_1, mul_3)]]\r\n", "It means that the registration did not work, for some reason.\r\n\r\nCould you go to [`NewSession`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc#L130) then call [`LogAllRegisteredKernels`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h#L1118) and see if it's there? It should not be, but you should see the CPU versions.", "Would you provide some hints how I should do it? Is there any example? ", "Just go to the code in NewSession linked above, then add LogAllRegisteredKernels(). In the `#include` section, add `#include \"tensorflow/core/framework/op_kernel.h`, then rebuild etc.", "I reseted my computer and now when I wanna to build tf again I got this error:\r\n\r\n\r\n> ERROR: /home/user/.cache/bazel/_bazel_user/f9f5c94ad789d21bcfdb930889866229/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/user/.cache/bazel/_bazel_user/f9f5c94ad789d21bcfdb930889866229/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/user/.cache/bazel/_bazel_user/f9f5c94ad789d21bcfdb930889866229/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/user/.cache/bazel/_bazel_user/f9f5c94ad789d21bcfdb930889866229/external/local_config_cuda/crosstool/BUILD.\r\n\r\nThere is an issue about that in the following link but it didn't work for me:\r\nhttps://github.com/tensorflow/tensorflow/issues/4841", "That's odd. Did you try to re-run configure as instructed?", "Yes, even I installed bazel again. ", "You did answer \"Y\" at the configure right? It should have run `bazel fetch ...` and gotten what was needed. Could you try bazel fetch ... by hand again to see?", "Yes,I answered Y. It was working before I restarted my computer. I did it, it didn't work. ", "What do you have in this file: `third_party/gpus/crosstool/BUILD` ?", "It is empty! ", "That's the right content. Can you do:\r\n\r\n```\r\nfind tensorflow -type f -name BUILD |xargs grep -n error_gpu_disabled\r\n```\r\n?", "it returned nothing. ", "That's strange.  Could you blow your cache again (`bazel --clean`) and run?", "bazel --clean returned error. but bazel clean worked fine. Is that fine?", "Yes sorry. Could run `find . -type f -name BUILD|xargs grep -n error_gpu_disabled`?", "it returned nothing. Also, I did bazel clean and run it again, but it returned same error.", "How about `git clone` to another directory and try from there?", "I got same error! :(", "I'm slowly running out of ideas. If you have tensorflow installed, please `pip uninstall` it.\r\n\r\nAlso, report the output of\r\n\r\n```\r\ndeclare -x\r\n```\r\n\r\nYou could try to use the docker file and build in a docker environment, roughly speaking:\r\n```\r\ncd tensorflow/tools/docker\r\nsudo docker build -t tfbuild  -f Dockerfile.cpu-devel\r\nsudo docker run -it -v /tmp:/tmp tfbuild bash\r\ncd /tmp\r\ngit clone ....\r\n```\r\nand build from source there.\r\n\r\nI have to ask -- are you sure you are building in the right directory?", "I am not sure I understand the status of this issue. It is pretty clear from the code that by default the GPU kernel is not registered for `scatter_nd`. I make this conclusion by observing that the [following lines for CPU kernels](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/scatter_nd_op.cc#L377):\r\n\r\n```\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_ADD_SUB_CPU);\r\n// TODO(simister): Re-enable all types after binary size is under control.\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_UPDATE_CPU);\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_CPU);\r\n```\r\n\r\nand only two respective lines for GPU kernels \r\n\r\n```\r\nTF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_ADD_SUB_GPU);\r\nTF_CALL_GPU_NUMBER_TYPES_NO_HALF(REGISTER_SCATTER_ND_UPDATE_GPU);\r\n# where is the third one??\r\n``` \r\n\r\n@drpngx is the third kernel left out intentionally? If yes, is enabling a GPU implementation for `scatter_nd` in the timeline?\r\n\r\nI'd like to note that this issue has a nasty side-effect: `scatter_nd` is used in the gradient of `gather_nd`, and because of the lack of the GPU kernel, the `gather_nd` gradient is computed on CPU...", "It looks like @drasmuss looked at this issue in https://github.com/tensorflow/tensorflow/issues/7026#issuecomment-277005409 and found that scatter_nd GPU kernel doesn't exist", "Yeah, we were trying to add it to see what happens but somehow the thread died out. Please try it out if you have cycles.", "Oops, this is a wrong issue, I will continue in #7026. ", "@drpngx What's the status here?", "I think the issue has been resolved through:\r\nhttps://github.com/tensorflow/tensorflow/commit/fdf4ad4c35e925bf3d944dac5b91efcc64b2c8b3#diff-53b0b5ca00738b21110fb55fda34b0c6", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Looks like this has been resolved. Feel free to reopen if I misunderstood."]}]