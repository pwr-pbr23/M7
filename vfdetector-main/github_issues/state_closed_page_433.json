[{"number": 40877, "title": "Check failed: cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type) == CUDNN_STATUS_SUCCESS (3 vs. 0)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): SOURCE\r\n- TensorFlow version (use command below): LATEST FROM GIT\r\n- Python version: 3.8.1\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11 / 8.0.1\r\n- GPU model and memory: GeForce RTX 2070\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nErrors out with this line when i ran a sequential model\r\n\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:1186] Check failed: cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type) == CUDNN_STATUS_SUCCESS (3 vs. 0)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(\r\n  keras.layers.Bidirectional(\r\n    keras.layers.LSTM(\r\n      units=128, \r\n      input_shape=(X_train.shape[1], X_train.shape[2])\r\n    )\r\n  )\r\n)\r\nmodel.add(keras.layers.Dropout(rate=0.2))\r\nmodel.add(keras.layers.Dense(units=1))\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n\r\n**Error happens at this line,**\r\nhistory = model.fit(\r\n    X_train, y_train, \r\n    epochs=30, \r\n    batch_size=32, \r\n    validation_split=0.1,\r\n    shuffle=False\r\n)\r\n\r\n", "comments": ["@summa-code,\r\nOn running the code I am facing an error stating `NameError: name 'X_train' is not defined`. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "Here is the code in GIT, \r\nhttps://github.com/curiousily/Deep-Learning-For-Hackers\r\n\r\nI have used the 12th example for Time series prediction. \r\nI built a code on 22nd, and that works. The codes that i pulled after 22nd seems to throw this issue. Related to LSTM/RNN ?", "@summa-code,\r\nI was able to run the code without any issues with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1480936b3b43c78426a7aae0c6755778/40877-tf-nightly.ipynb).\r\n\r\nCould you please check if you are facing the same issue in a virtual environment? Thanks! ", "Also, please take a look the [tested build configuration](https://www.tensorflow.org/install/source#gpu) and check if you are running the compatible CUDA and cuDNN versions. Thanks!", "> Also, please take a look the [tested build configuration](https://www.tensorflow.org/install/source#gpu) and check if you are running the compatible CUDA and cuDNN versions. Thanks!\r\n\r\n+1, please report your CUDA and cuDNN versions.  We have observed this error internally with CUDA11+cuDNN8, and are currently investigating it.  CC @chsigg ", "I have the same problem. I use LSTM layer. On CPU all working fine.\r\n```bash\r\nroot@RedShark:/var/www# python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2020-07-01 17:53:54.907427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nv1.12.1-35611-gcd0a2e6c1f 2.4.0\r\n```\r\n", "cudnnSetRNNDescriptor_v6 has been deprecated in cuDNN 8, and cudnnSetRNNMatrixMathType returns an error. \r\n\r\n28766652e6db8020881b55b1ebe77b05b2ac994a switched to cudnnSetRNNDescriptor_v8.\r\n\r\nI'm closing this isssue as I think it has been resolved. Please reopen if you still see the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40877\">No</a>\n", "The previously described error no longer occurs. No less, when trying to use the LSTM layer, another error appeared\r\n```\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-24-66b029fffc15> in <module>\r\n      1 # train\r\n----> 2 hist = textClassifier.train(X_train[:10000], y_train[:10000])\r\n      3 hist\r\n\r\n<ipython-input-21-d0b8fb2d36c8> in train(self, X, y, w2v_model, verbose, X2)\r\n    308 \r\n    309     def train(self, X, y, w2v_model=None, verbose=0, X2=None):\r\n--> 310         self.fit(X, y, w2v_model, verbose, X2)\r\n    311         return self.hist\r\n    312 \r\n\r\n<ipython-input-21-d0b8fb2d36c8> in fit(self, X, y, w2v_model, verbose, X2)\r\n    301                                   epochs           = self.epochs,\r\n    302                                   batch_size       = self.batch_size,\r\n--> 303                                   shuffle          = False)\r\n    304 \r\n    305         self.model      = model\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1097                 batch_size=batch_size):\r\n   1098               callbacks.on_train_batch_begin(step)\r\n-> 1099               tmp_logs = train_function(iterator)\r\n   1100               if data_handler.should_sync:\r\n   1101                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    838         # Lifting succeeded, so variables are initialized and we can run the\r\n    839         # stateless function.\r\n--> 840         return self._stateless_fn(*args, **kwds)\r\n    841     else:\r\n    842       canon_args, canon_kwds = \\\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2842     with self._lock:\r\n   2843       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2844     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2845 \r\n   2846   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1845                            resource_variable_ops.BaseResourceVariable))],\r\n   1846         captured_inputs=self.captured_inputs,\r\n-> 1847         cancellation_manager=cancellation_manager)\r\n   1848 \r\n   1849   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1921       # No tape is watching; skip to running the function.\r\n   1922       return self._build_call_outputs(self._inference_function.call(\r\n-> 1923           ctx, args, cancellation_manager=cancellation_manager))\r\n   1924     forward_backward = self._select_forward_and_backward_functions(\r\n   1925         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    548               inputs=args,\r\n    549               attrs=attrs,\r\n--> 550               ctx=ctx)\r\n    551         else:\r\n    552           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnknownError:    CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1130): 'cudnnSetRNNDescriptor_v8( rnn_desc.get(), rnn_algo, rnn_mode, bias_mode, direction_mode, input_mode, data_type, compute_type, math_type, input_size, hidden_size, cell_size, num_layers, dropout_desc.handle(), aux_flags)'\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[functional_3/bidirectional_1/forward_lstm_1/PartitionedCall]] [Op:__inference_train_function_13519]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n\r\n```bash\r\nroot@RedShark:/usr# python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n2020-07-03 00:10:58.059582: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nv1.12.1-35676-g89798077df 2.4.0\r\n```\r\nIf I turn off cuda ( os.environ['CUDA_VISIBLE_DEVICES'] = '' ), everything works fine on cpu.\r\n", "> No less, when trying to use the LSTM layer, another error appeared\r\n\r\n@ApelSYN,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 40876, "title": "TFL Classify keeps stopping on Pixel 3.", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@liwumass \r\nPlease share simple stand alone code for the issue reported.", "Hi there,\n\nFinally, I fixed it! I appreciate your support!\n\nBest,\n\n\nOn Monday, June 29, 2020, Saduf2019 <notifications@github.com> wrote:\n\n> @liwumass <https://github.com/liwumass>\n> Please share simple stand alone code for the issue reported.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40876#issuecomment-651241761>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOL2YCPGG2DDQCQSDOIB2CLRZDBS3ANCNFSM4OKLIJJQ>\n> .\n>\n", "Moving to closed status as issue is resolved."]}, {"number": 40875, "title": "Failed to load the native TensorFlow runtime.", "body": "Traceback (most recent call last):\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\vmathesh\\Vinod\\Task\\Python\\RASA\\venv\\Scripts\\rasa.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\rasa\\__main__.py\", line 82, in main\r\n    set_log_level(log_level)\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\rasa\\utils\\common.py\", line 71, in set_log_level\r\n    update_tensorflow_log_level()\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\rasa\\utils\\common.py\", line 112, in update_tensorflow_log_level\r\n    import tensorflow as tf\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\vmathesh\\vinod\\task\\python\\rasa\\venv\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 243, in load_module\r\n  File \"C:\\Users\\vmathesh\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@mvinodraja \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please close all sessions of Jupyter Notebook and relaunch. Try to import again as a first step and it should work fine.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40875\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40875\">No</a>\n"]}, {"number": 40874, "title": "in docker tensorflow/tensorflow:latest-gpu was unable to find libcuda.so DSO", "body": "sudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu\r\npython -c \"import tensorflow as tf; tf.config.list_physical_devices('GPU')\"\r\n\r\n```\r\n2020-06-27 10:14:59.618701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-27 10:14:59.618726: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2020-06-27 10:14:59.618744: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: fd26039ac692\r\n2020-06-27 10:14:59.618752: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: fd26039ac692\r\n2020-06-27 10:14:59.618821: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2020-06-27 10:14:59.618853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0\r\n```\r\nDocker version 19.03.12, build 48a66213fe\r\n\r\nNVIDIA 440.82\r\n\r\nI install nvidia-container-toolkit\r\n\r\nIn my host it works,but in docker I get \"was unable to find libcuda.so DSO \"", "comments": ["@ighack,\r\nPlease take a look at similar issues [#4078](https://github.com/tensorflow/tensorflow/issues/4078#issuecomment-255129832) and [#4267](https://github.com/tensorflow/tensorflow/issues/4267#issuecomment-256029240), and let us know if it works. Thanks!", "@amahendrakar \r\n\r\nchange /etc/nvidia-container-runtime/config.toml\r\nfrom \"@/sbin/ldconfig\" to \"/sbin/ldconfig\"", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40874\">No</a>\n"]}, {"number": 40873, "title": "Cleanup Keras CallbackList hook delegation", "body": "Handling of `_supports_tf_logs` added some additional complexity to the individual hook functions. This PR removes duplicagtion and simplifies the hook delegation by moving the handling into a shared helper function.", "comments": ["@lgeiger  Can you please resolve conflicts? Thanks!", "> @lgeiger Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned This is a small refactor that has been open for 1.5 months now and would've been easy to merge in a timely manner. I am not interested in resolving the conflicts anymore."]}, {"number": 40872, "title": "failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error", "body": "python 3.6\r\ntensorflow-gpu 1.13.1\r\nCUDA 10.0.130\r\nCudnn 7.6.5\r\n\r\nI used TensorFlow-GPU 1.13.1 to train my faster-rcnn model\uff0cbut I could not use GPU to accelerate the training process\uff0cjust could use CPU to train the model, and I run the codes\uff1aimport tensorflow as tf tf.test.is_gpu_available() .\r\n\r\nAnd the error as show:\r\n2020-06-28 08:13:08.860753: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-06-28 08:13:08.936714: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2020-06-28 08:13:08.953830: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: DESKTOP-PSVDSUR\r\n2020-06-28 08:13:08.959987: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: DESKTOP-PSVDSUR\r\nFalse\r\n\r\nIn addition, the acceleration performance of GPU can be used normally in the past.\r\nSo, I hope you can help me to solve my problem, thanks!", "comments": ["@A12-RUI \r\nPlease refer to [this comment](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u) and let us know if it helps.\r\n[link](https://technofob.com/2019/06/14/how-to-compile-tensorflow-2-0-with-avx2-fma-instructions-on-mac/) [link1](https://github.com/llSourcell/tensorflow_chatbot/issues/79)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40871, "title": "AutoGraph not working ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7.7\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to run a basic lenet5 model and it will not run.  I am copying the model from a book, Hands on Computer Vision with TensorFlow2.  When I run it, I get a warning that says I should report it:\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7ff5121fa440> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n\r\nI also get an error later in the run:\r\n\r\n  File \"/Users/toddwimer/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\", line 180, in assert_input_compatibility\r\n    str(x.shape.as_list()))\r\n\r\nValueError: Input 0 of layer sequential_4 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [32, 28, 28]\r\nI do not get an output.\r\n\r\n**Describe the expected behavior**\r\nI expect the code to run and produce a trained model.\r\n\r\n**Standalone code to reproduce the issue**\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\r\nimport numpy as np\r\nprint(tf.__version__)\r\n\r\nnum_classes = 10\r\nimg_rows, img_cols = 28, 28\r\nnum_channels = 1\r\ninput_shape = (img_rows, img_cols, num_channels)\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nmodel = Sequential()\r\nmodel.add(Conv2D(6, kernel_size=(5,5), padding='same', activation='relu',\r\n                 input_shape=input_shape, name='conv1'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2), name='pool1'))\r\nmodel.add(Conv2D(16, kernel_size=(5,5), activation='relu', name='conv2'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2), name='pool2'))\r\nmodel.add(Flatten(name='flatten'))\r\nmodel.add(Dense(120, activation='relu', name='dense1'))\r\nmodel.add(Dense(84, activation='relu', name='dense2'))\r\nmodel.add(Dense(10, activation='softmax', name='output'))\r\nmodel.compile(optimizer='sgd',\r\n loss='sparse_categorical_crossentropy',\r\n metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=32, epochs=80, verbose=1, validation_data=(x_test,y_test))\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I resolved my errors, but am not sure what output to provide as I only get numbers.  The warning is the only concern and I am only reporting it because the warning tells me to.\r\n\r\nThank you.", "@twimer1 \r\n\r\nGlad to know that you resolved the issue.Please, close this thread as your issue was resolved.Thanks!", "I did not resolve the warning that recommended that I make this post, see below for what was posted originally.  I only resolve the issue with my model running at all.  I can still close it if you like, but want to be sure the warning isn't something that should be resolve since the warning from tensorflow asks me to report it.\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function..train_function at 0x7ff5121fa440> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output.", "@twimer1 \r\n\r\nCan you please share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I wasn't able to attach my python code, but copied it below.  If this doesn't help, I am not sure what else I could provide.  If you have any recommendations for code to help you, I am happy to add it.  If not, maybe I should just close this ticket.  I am a newbie and doing this as a hobby.  So, I am not too adept at debugging, etc.  Although I am happy to learn if you have recommendations.  Thank you.\r\n\r\n#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Sat Jun  6 09:14:04 2020\r\n\r\n@author: toddwimer\r\n\"\"\"\r\n\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\r\n# layers = tf.keras.layers\r\nimport numpy as np\r\nprint(tf.__version__)\r\n\r\nnum_classes = 10\r\nimg_rows, img_cols = 28, 28\r\nnum_channels = 1\r\ninput_shape = (img_rows, img_cols, num_channels)\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nx_train = x_train.reshape(x_train.shape[0], *input_shape)\r\nx_test = x_test.reshape(x_test.shape[0], *input_shape)\r\n\r\n# plt.figure(figsize=(10,10))\r\n\r\n# for i in range(25):\r\n#     plt.subplot(5,5,i+1)\r\n#     plt.xticks([])\r\n#     plt.yticks([])\r\n#     plt.grid(False)\r\n#     plt.imshow(x_train[i], cmap=plt.cm.binary)\r\n#     # plt.xlabel(class_names[y_train[i]])\r\n    \r\n# plt.show()\r\nprint('x_train size: {}'.format(x_train.shape))\r\nmodel = Sequential()\r\nmodel.add(Conv2D(6, kernel_size=(5,5), padding='same', activation='relu',\r\n                 input_shape=input_shape, name='conv1'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2), name='pool1'))\r\nmodel.add(Conv2D(16, kernel_size=(5,5), activation='relu', name='conv2'))\r\nmodel.add(MaxPooling2D(pool_size=(2,2), name='pool2'))\r\nmodel.add(Flatten(name='flatten'))\r\nmodel.add(Dense(120, activation='relu', name='dense1'))\r\nmodel.add(Dense(84, activation='relu', name='dense2'))\r\nmodel.add(Dense(10, activation='softmax', name='output'))\r\nmodel.compile(optimizer='sgd',\r\n loss='sparse_categorical_crossentropy',\r\n metrics=['accuracy'])\r\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')]\r\nmodel.fit(x_train, y_train, batch_size=32, epochs=80, verbose=1, \r\n          validation_data=(x_test,y_test), callbacks=callbacks)\r\n\r\n", "I have tried in colab with TF versions 2.2, nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/175a5b6ea31ea2bae054ff2848feadfd/untitled69.ipynb).Thanks!\r\n", "@twimer1 Shape of the MNIST data is 6000x28x28. As you are adding a dimension in the model by `input_shape = (img_rows, img_cols, num_channels)`, the model expecting data in that format but the provided data is 6000x28x28. So, If we add a dimension to the `x_train` and `x_test`, then everything works as expected. Just add the following two lines.\r\n\r\n```\r\nx_train = tf.expand_dims(x_train,-1)\r\nx_test = tf.expand_dims(x_test,-1)\r\n```\r\n\r\nAlternatively, you could update the `input_shape` from `(img_rows, img_cols, num_channels)` to `(img_rows, img_cols)`.\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/64479380f4ba939b325719430e3d26fe/untitled69.ipynb). Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This is good to know that I can transform the dimension in another way, but I had resloved the error I had with \r\nx_train = x_train.reshape(x_train.shape[0], *input_shape)\r\nx_test = x_test.reshape(x_test.shape[0], *input_shape)\r\n\r\nIf I change these to the tf.expand_dims version, the model still trains, but I also still get the warning below:\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff4d1f20a70> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7ff4d1f20a70> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n\r\nIs there something that I could output using verbose=10?\r\nDoes it mean that the issue with the warning has been fixed in the nightly updates and will be released at some time in the future?\r\n", "@twimer1 `verbose=10` will output more info about the root-cause of the issue that will help the developers to fix the issue faster. I didn't see that warning in my gist. Stable version TF2.3 will be released in near future. \r\n\r\nPlease close the issue as this was resolved. Feel free to open if this persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40871\">No</a>\n"]}, {"number": 40870, "title": "I already has tensorflow installed", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n![image](https://user-images.githubusercontent.com/31926010/85931140-c2953e80-b8df-11ea-8b98-930cc9170a8d.png)\r\nbut getting this error on import tensorflow", "comments": ["Duplicate of #40875 and many many other similar ones. Linking to #40875 as the last comment there points to solutions and other duplicated issues."]}, {"number": 40869, "title": "module 'keras.backend' has no attribute 'get_session'", "body": "\r\n**System information**\r\n- macOS 10.15.5\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.3\r\n\r\nhey guys! just got a trouble, can't find any ansewrs at web, can some one help me?\r\n\r\n**Error:**\r\nTraceback (most recent call last):\r\n  File \"/Users/a/PycharmProjects/untitled/venv/main.py\", line 7, in <module>\r\n    detector = ObjectDetection()\r\n  File \"/Users/a/PycharmProjects/untitled/venv/lib/python3.7/site-packages/imageai/Detection/__init__.py\", line 88, in __init__\r\n    self.sess = K.get_session()\r\nAttributeError: module 'keras.backend' has no attribute 'get_session\r\n\r\n\r\n**code:**\r\nfrom imageai.Detection import ObjectDetection\r\nimport tensorflow.keras.backend\r\nimport os\r\n\r\nexec_path = os.getcwd()\r\ndetector = ObjectDetection()\r\ndetector.setModelTypeAsRetinaNet()\r\ndetector.setModelPath(os.path.join(exec_path, \"resnet50_coco_best_v2.0.1.h5\"))\r\ndetector.loadModel()\r\n\r\nList = detector.detectObjectsFromImage(input_image=os.path.join(exec_path, \"traffic_jam.jpg\"),\r\n                                       output_image_path=os.path.join(exec_path, \"new_traffic_jam.jpg\")\r\n                                       )\r\n\r\n\r\n", "comments": ["I had the same error and fixed by downgrading tensorflow to 1 rather than 2.2. Personally I used 1.14 and the error disappeared. Also the imageai doc shows the tf 2 is coming soon, which means not available currently.", "Thank u so much, i downgraded tensorflow and keras to 2.1.2, works good)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869\">No</a>\n", "> I had the same error and fixed by downgrading tensorflow to 1 rather than 2.2. Personally I used 1.14 and the error disappeared. Also the imageai doc shows the tf 2 is coming soon, which means not available currently.\r\n\r\nHow did you downgrade tensorflow? I get an error while installing tensorflow 1.14 \r\n![image](https://user-images.githubusercontent.com/62503301/88225281-e21b4f00-cc72-11ea-99c5-7093dfeb4cae.png)\r\n\r\n\r\n> Are you satisfied with the resolution of your issue?\r\n> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869)\r\n> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869)\r\n\r\n", "> > I had the same error and fixed by downgrading tensorflow to 1 rather than 2.2. Personally I used 1.14 and the error disappeared. Also the imageai doc shows the tf 2 is coming soon, which means not available currently.\r\n> \r\n> How did you downgrade tensorflow? I get an error while installing tensorflow 1.14\r\n> ![image](https://user-images.githubusercontent.com/62503301/88225281-e21b4f00-cc72-11ea-99c5-7093dfeb4cae.png)\r\n> \r\n> > Are you satisfied with the resolution of your issue?\r\n> > [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869)\r\n> > [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869)\r\n\r\nFirstly, your screenshot showed the version was 1.4. Secondly, if your Python version is 3.8, it only supports tf 2.2 or later. I hope this will help you.", "> > I had the same error and fixed by downgrading tensorflow to 1 rather than 2.2. Personally I used 1.14 and the error disappeared. Also the imageai doc shows the tf 2 is coming soon, which means not available currently.\r\n> \r\n> How did you downgrade tensorflow? I get an error while installing tensorflow 1.14\r\n> ![image](https://user-images.githubusercontent.com/62503301/88225281-e21b4f00-cc72-11ea-99c5-7093dfeb4cae.png)\r\n> \r\n> > Are you satisfied with the resolution of your issue?\r\n> > [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869)\r\n> > [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40869)\r\n\r\nI think, you have typed \"1.4\" instead of \"1.14\" by mistake.", "You first need to uninstall tensorflow by typing `pip uninstall tensorflow`, then you can install the older version with `pip3 install tensorflow==1.4`.", "I had this error, so I downgraded to Tensorflow 1.14, but then I get this error... `ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via 'pip install tensorflow'`", "What version of Keras are you using? https://imageai.readthedocs.io/en/latest/ suggests 2.2.3. The imageai library is not built for TF 2.0 (it says so on their website) and so you need TF 1.x for this to work. Only some versions of keras are compatible with TF 1.x, so you need to make sure you have those versions installed.", "In my case help replacing \r\n```from keras import backend as K```\r\nto\r\n```import tensorflow.python.keras.backend as K```\r\nwithout any downgrading to 1-xx tf. Works fine with tf 2.2"]}, {"number": 40868, "title": "Remove unused TensorBoard._epoch", "body": "Since 7b43b3ce08035b6c502b1aa4caa23ba59e4710f2 `TensorBoard._epoch` isn't used anymore in the callback, so it can be safely removed.", "comments": ["@lgeiger  Can you please resolve conflicts? Thanks!\r\n", "> Can you please resolve conflicts?\r\n\r\n@gbaned Done", "@gbaned This PR seems to be stuck in the pipeline, can you check what's going on?", "> @gbaned This PR seems to be stuck in the pipeline, can you check what's going on?\r\n\r\n@lgeiger Sorry for the delay. Internal test failures are appearing, @fchollet is working on this. Thank you!", "No worries (it's a trivial PR anyway), just pinged you since I thought it might be stuck due to a technical problem :)", "Unfortunately this appears to be blocked due to technical glitches. We're going to have to fix it separately. Nevertheless, thanks for the PR!"]}, {"number": 40867, "title": "using tf.GPUOptions in tf 2.x", "body": "what is the proper call for this in version of tensorflow 2.x ?\r\ntf.GPUOptions(per_process_gpu_memory_fraction=0.3) # i used this with tf.compat.v1 , GPUOptions is not available in 2.x , can someone help me use my gpu cores without using gpuoptions ,coz i dont want to use compat v1", "comments": ["@glennford49 \r\n\r\nCan you please go through the [link](https://www.tensorflow.org/guide/gpu#) and see if it helps you.This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!"]}, {"number": 40866, "title": "TFLu wrong predictions for optimized model", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: macOS 10.14.16 and Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from: source\r\n- Tensorflow version: TF v2.2\r\n- Target platform: ARM Mbed OS (bare-metal) on STM32L4\r\n- MCU Compiler: GCC ARM (GNU Arm Embedded Toolchain 9-2020-q2-update)\r\n\r\n**Describe the problem**\r\n\r\nRunning an optimized model (see code snippet for generation below) on the MCU results in wrong predictions. Those predictions differ depending on the build profile (see more below).\r\nRunning the model locally with the TF Lite Interpreter works flawlessly.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI'm currently benchmarking multiple models on the STM32L4 and want to verify the accuracy on the MCU itself by sending the test data and the results back and forth via UART.\r\nThis works flawlessly for the converted model without any optimizations:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\ntflite_model = converter.convert()\r\nopen('./TFLite-model/LeNet-MNIST.tflite', 'wb').write(tflite_model)\r\n```\r\n\r\nThis also works flawlessly for the model with full int8 quantization:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = yield_representative_dataset\r\ntflite_model = converter.convert()\r\nopen('./TFLite-model/LeNET-MNIST_int8ops.tflite', 'wb').write(tflite_model)\r\n```\r\n\r\nHowever, the predictions are wrong when only using `[tf.lite.Optimize.DEFAULT]` - where only some of the weights are quantized.\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nopen('./TFLite-model/LeNet-MNIST_optimized.tflite', 'wb').write(tflite_model)\r\n```\r\n\r\nEverything works flawlessly on the local TFLite Interpreter, and the problem only occurs with the mentioned model **with and without cmsis-nn** on the MCU.\r\n\r\nBuilding with the `release.json` profile (mainly `-Os`) always results in the following prediction independent from the input:\r\n```\r\n[0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0]\r\n```\r\n\r\nHowever, building with the `debug.json` profile (`-Og`) always results in different wrong outputs but reproducible with the same input e.g.:\r\n```\r\n[0.1012007445, 0.1061911806, 0.0979489163, 0.1032304987, 0.1272625774, 0.0807816312, 0.0818515792, 0.0830543563, 0.0847685188, 0.1337099075]\r\n```\r\n\r\n\r\n**Things I already tried**\r\n- the model works flawlessly on the TF Lite interpreter on my host machines\r\n- the model is correctly ported to the MCU\r\n    - no TFLu runtime errors\r\n    - verified the input and output tensors size and datatypes (as expected float32)\r\n- there is ongoing computation on the MCU and I can even benchmark each layer individually\r\n\r\nI currently don't have a second MCU at hand to test this.\r\n\r\n---\r\n\r\nGiven that all the other models run flawlessly on the MCU and that the output depends on the compiler optimizations, I'm a bit clueless and expect some problem with TFLu.\r\n\r\nThe only difference from the optimized to the non-optimized model are the quantized weights in the fully-connected layers, I expect a problem somewhere there.\r\nFurthermore the fully int8 quantized model runs flawlessly.\r\nSo I would restricte the error to kernels which use the int8 quantized weights, but still do the computation in float32 -- this is the only spot where I do see a difference of the model architectures.\r\n\r\nThankful for any feedback and pointers.\r\n\r\n**Attachements**\r\n[GDrive Folder](https://drive.google.com/drive/folders/1bXUNiN5vrr1n1mrFuKsB0_h_nLqd0DD5?usp=sharing)\r\n- [model architecture](https://drive.google.com/file/d/1UKt578IcToaQJdPrIVAvRexwb6XEyJBd/view?usp=sharing)\r\n- the original keras model\r\n- the converted optimized model which gives wrong predictions\r\n", "comments": ["Thanks for the report, and sorry you're hitting problems!\r\n\r\nThis definitely seems like a bug, but the behavior that I would expect here is that the kernels would report an error because they don't support the combination of float calculations and eight-bit weights. This isn't well documented, but for code size reasons we focus on either pure eight-bit activations and weights, or float activations and weights, and don't support the \"hybrid\" combination of float activations and eight-bit weights in TFL Micro. This combination is supported in the mobile version of TFL, which is why it works in the Python interpreter, since that's using the mobile version.\r\n\r\nWe should report an error here rather than letting the calculations continue, we should figure out which kernel is failing silently and allowing uninitialized data to flow through the graph.\r\n\r\nTo solve your problem, can we help you figure out how to convert your graph to fully quantized weights and activations? Are you seeing accuracy issues with that approach right now?", "Thanks for getting back to me @petewarden.\r\n\r\nThis makes sense and confirms my observations. The *pure* models with int8 or either float32 activations **and** weights do work flawlessly.\r\n\r\nI pasted my code snippet for a graph with fully quantized weights and activations above:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = yield_representative_dataset\r\ntflite_model = converter.convert()\r\nopen('./TFLite-model/LeNET-MNIST_int8ops.tflite', 'wb').write(tflite_model)\r\n```\r\n\r\nThe resulting model works and performs well, especially using cmsis-nn. \ud83d\ude0a \r\n\r\nI'm fine with either closing the issue or, as you mentioned, keep it open to track the work on an error report for the kernel. In my case the layer is probably the `FULLY_CONNECTED` layer.\r\n\r\n", "And for anyone running TF 1.15 the tweaked conversion script below (derived from TinyML by @petewarden) also fully quantizes your h5 model...\r\n\r\n`converter = tf.lite.TFLiteConverter.from_keras_model_file('LeNet-MNIST.h5')`\r\n`converter.inference_input_type = tf.lite.constants.INT8`\r\n`converter.inference_output_type = tf.lite.constants.INT8`\r\n`converter.optimizations = [tf.lite.Optimize.DEFAULT]` \r\n", "@lheim It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "@sushreebarsa I'm not working on this project anymore, so I can't test if the problem still exists. Nonetheless, feel free to close the issue if you think it's fixed now.\r\nCheers!", "Can someone from the community can clarify if this issue is resolved so that we can close it ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40866\">No</a>\n"]}, {"number": 40865, "title": "added return attention parameter", "body": "Added `return_attention` as parameter to `BaseDenseAttention` from which `Attention` and `AdditiveAttention` layers are inherited.  By setting the parameter to True, the Attention layer will return the result along with scores calculated using `query` and `key` dot product followed by softmax. \r\n\r\nThe returned attention distribution can be used to interpret the result while inference. ", "comments": ["Thank you for the PR. This has already been added, as the `return_attention_scores` argument."]}, {"number": 40864, "title": "Colab TPU failing with distribution strategy on a dataset hosted on a public GCS bucket", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have hosted the cats-vs-dogs dataset on a public GCS bucket. I have created the dataset pipeline using `tf.data`. After building the model inside the `TPUStrategy` scope and compiling it, I am calling `.fit`. Currently, it results into:\r\n\r\n```\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:174 run  **\r\n        return self.extended.tpu_run(fn, args, kwargs, options)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:867 tpu_run\r\n        return func(args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:916 tpu_function\r\n        maximum_shape = tensor_shape.TensorShape([None] * rank)\r\n\r\n    TypeError: can't multiply sequence by non-int of type 'NoneType'\r\n```\r\n\r\nNote that I am using Colab TPUs here. The larger log file is attached. In this particular case, we have included image stylization in the data input pipeline as one would notice. Without the stylization, the training seems to be working fine. Here's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/5c897959a8b472311d6b745d494cdd7f/texturecnn.ipynb) that confirms that. \r\n\r\n### Source code / logs\r\nHere's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/90a946d414eea1380d0c814f4cd025ce/shapecnn.ipynb). You can choose to not install `wandb`, in that case just comment out the `wandb` imports and remove `WandbCallback()` from the callback list while calling `model.fit()`. \r\n\r\n[error_trace.txt.zip](https://github.com/tensorflow/tensorflow/files/4840232/error_trace.txt.zip)", "comments": ["Could you provide an update on this?", "Hi @sayakpaul, seems to me like the error is caused because input_tensor rank is None. See source code for line causing the error [here](https://github.com/tensorflow/tensorflow/blob/241a209628ca74059df9f46ca4b316b1b23f9090/tensorflow/python/distribute/tpu_strategy.py#L1154). Although I'm not sure why the rank is None.\r\n\r\nCan you confirm that you are able to train without using TPUs? And also let me know what happens with TF Nightly?\r\n", "@nikitamaia thanks for replying. \r\n\r\n> Can you confirm that you are able to train without using TPUs?\r\n\r\nYes, it works on GPUs. \r\n\r\n> And also let me know what happens with TF Nightly?\r\n\r\nThe following issue gets raised:\r\n```\r\nInvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified; Op<name=_Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>; NodeDef: {{node _Send}}\r\n```\r\n\r\nHere's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/9e3cfab5cbaa9b342125873be2618a44/shapecnn.ipynb). \r\n\r\nCc: @ayulockin", "Any update on this issue?", "Hi @sarim-zafar, apologies for the delay here. I think you're facing a similar issue to #41590. Explicit size is needed for TPUs, and you'll find that after you apply your stylization functions, the datasest has an unknown shape for your images.\r\n`<PrefetchDataset shapes: (<unknown>, (None,)), types: (tf.float32, tf.int32)>`\r\n\r\nTry modifying the return of your stylize_image function to \r\n`return tf.reshape(tf.clip_by_value(stylized_image, 0., 1.),[224, 224, 3])`\r\n\r\nThat should work, as your train_ds_texture will now have a known shape for the images\r\n`<PrefetchDataset shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.int32)>`\r\n", "@nikitamaia not a problem really. We will try this out and we will keep you posted. \r\n\r\nThis is one of those examples where a TF Hub module is being used for replicating a research paper actually. Isn't that cool? :D \r\n\r\nCc: @ayulockin", "@nikitamaia adding this `return tf.reshape(tf.clip_by_value(stylized_image, 0., 1.),[224, 224, 3])` worked. Thank you so much.", "Hi, I have a similar problem (same error message):\r\n\r\n```\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:346 run\r\n        return self.extended.tpu_run(fn, args, kwargs, options)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1095 tpu_run\r\n        return func(args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1144 tpu_function\r\n        maximum_shape = tensor_shape.TensorShape([None] * rank)\r\n\r\n    TypeError: can't multiply sequence by non-int of type 'NoneType'\r\n```\r\n\r\nI tried adding `tf.reshape(...)` to the mappings of `tf.Dataset.map`, but still doesn't work.\r\nAnd here is the stack trace: [https://gist.github.com/hav4ik/ae09fec4df4d99f968c9928e9549975b](https://gist.github.com/hav4ik/ae09fec4df4d99f968c9928e9549975b)\r\n\r\n----------------------------------------------------------------------------------------------------------\r\n\r\n## Update\r\n\r\nI have resolved my issue by adding `tf.reshape` to:\r\n* All endpoints of `tf.Dataset` pipelines by doing `ds.map(lambda x: tf.reshape(x, ...), num_parallel_calls=AUTO)`\r\n* Entry- and end- points to models, loss functions, metrics, etc. are also secured with `tf.reshape`\r\n\r\nI might have overdone it (I am sure it will work with fewer `tf.reshape`s), but it works now. Thanks everyone above for the comments and explanations.", "@hav4ik please open a new issue and provide reproducible code so we can help troubleshoot. Thanks!", "@nikitamaia Thanks for the quick response. I was able to resolve my issue. I have updated my previous comment for those who might encounter a similar issue in the future."]}, {"number": 40863, "title": "Access feature map regions in keras custom layer and assigning values to output layer.", "body": "I am creating a custom layer where I have to access the individual map windows and do operations on them, but because the model sends in none type as the first dimension of the input, I am unable to iterate over or access the region. Some help would be greatly appreciated. Here is the link to the full problem and the code:\r\n[stack_overflow_link](https://stackoverflow.com/questions/62564980/access-image-regions-in-keras-custom-layer-and-assigning-values-to-the-output)", "comments": ["I am trying to implement custom pooling layer. If the pooing region would be (3x3), then I would need to convert it into an array or vector of size 9, and then operations on that including exponential and sorting. Any idea on how I can access such regions or iterate over it?", "@Aviral-Aggarwal \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40862, "title": "Link Broken on tutorials/images/object detection API", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nThis link is broken https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\n![image](https://user-images.githubusercontent.com/6630197/85917788-7c979680-b85d-11ea-8d93-0fae5b8d9bf2.png)\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\nYes, this one [https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb)\r\n\r\n\r\n### Submit a pull request?\r\n\r\nNo.\r\n", "comments": ["@pana1990,\r\nLooks like this is a duplicate of issue [#8698](https://github.com/tensorflow/models/issues/8698).\r\n\r\nCan we close this issue since it is already being tracked in the TensorFlow models repo? Thanks!", "@amahendrakar ups, it's true, thanks a lot for your reply ;)"]}, {"number": 40861, "title": "Change GetMatchingPaths to avoid traversing unnecesscary paths", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis pr updates the function `GetMatchingPaths`.\r\n\r\nThe old code will collect all possible path before using any wildcard characters, which will be really slow and memory demanding when matching patterns like \"/*\". Similar issue was stated in #40553 . The updated function will match the directory while gradually traverse the possible path.\r\n\r\nThank you for your time on reviewing this pr.\r\n", "comments": ["@mihaimaruseac \r\nI've fixed the build failures, which are related to paths with protocol like `file://test`. Could you have another look? Thank you!", "This causes build failures :( Can you check the Ubuntu CPU log please", "@mihaimaruseac \r\nSorry...  I didn't notice that `//tensorflow/tools/api/tests:api_compatibility_test` had different error... The new build failure is caused by paths like `/root/tensorflow/../*` and I've fixed it."]}, {"number": 40860, "title": "protoc not using the right environment - build failure", "body": "The tensorflow build I have with commit 80768cb23a3a4314c52af0b48a6bcf23ca541e19 fails here apparently because `protoc` is being passed an empy environment - as such it doesn't pick up the right libstdc++. A log with the `-s` option is attached below. \r\n\r\nOS: CentOS Linux release 7.7.1908 (Core)\r\ngcc (GCC) 8.2.0\r\nbazel: 3.1.0\r\n\r\n```\r\n$ bazel build -s tensorflow/core/data/service:all\r\n...\r\nERROR: /net/uday-dev/srv/nfs/uday-data/ws/tensorflow-private/tensorflow/core/data/service/BUILD:310:1: Action tensorflow/core/data/service/master.grpc.pb.h failed (Exit 1)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\n```\r\nPassing `-s` reveals:\r\n\r\n```\r\nexec env - \\\r\n  bazel-out/host/bin/external/com_google_protobuf/protoc '--plugin=protoc-gen-PLUGIN=bazel-out/host/bin/external/com_github_grpc_grpc/src/compiler/grpc_cpp_plugin' '--PLUGIN_out=generate_mock_code=true:bazel-out/k8-opt/bin' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/any_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/api_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/type_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto' '--proto_path=bazel-out/k8-opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=.' '--proto_path=bazel-out/k8-opt/bin' tensorflow/core/data/service/master.proto)\r\nERROR: /net/uday-dev/srv/nfs/uday-data/ws/tensorflow-private/tensorflow/core/data/service/BUILD:310:1: Action tensorflow/core/data/service/master.grpc.pb.h failed (Exit 1)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\nbazel-out/host/bin/external/com_google_protobuf/protoc: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by bazel-out/host/bin/external/com_google_protobuf/protoc)\r\n```\r\n\r\nWhy is protoc being executed with an empty environment (`env -`) here instead of using the environment with which everything else is being built? Using an empty environment here will make it pick the wrong libstdc++ (/usr/lib64/libstdc++.so.6) instead of the one available with LD_LIBRARY_PATH where those symbols are available.\r\n\r\nA similar issue was reported in the past (https://github.com/bazelbuild/bazel/issues/1358) but the workaround there of adding `env=ctx.configuration.default_shell_env` to the `ctx.action` call in `third_party/systemlibs/protobuf.bzl` doesn't help here. If this was fixed in a later commit upstream, I'd appreciate a pointer to the commit - I couldn't immediately tell from the log. Thanks.", "comments": ["@bondhugula \r\nCan you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/40388#issuecomment-646029592) and let us know if that helps.", "> @bondhugula\r\n> Can you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/40388#issuecomment-646029592) and let us know if that helps.\r\n\r\nUnfortunately, this issue has nothing to do with `swig` - I don't even have an `bazel-tensorflow/external/swig`. ", "The following workaround however works:\r\n\r\nAdd `linkopts = [\"-Wl,-rpath,/path/to/custom/libstdc/path/\", \"-L/path/to/custom/libstdc/path\" ]` to the target `protoc_lib` in `bazel-tensorflow-<suffix>/external/com_google_protobuf/BUILD` so that the new linkopts looks like:\r\n\r\n```\r\nlinkopts = LINK_OPTS + [\"-Wl,-rpath,/opt/gcc-8.2.0/lib64\", \"-L/opt/gcc-8.2.0/lib64\"] + \r\n...\r\n```", "@bondhugula Please close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please do close this - resolved through a workaround.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40860\">No</a>\n", "why protoc command must add '''exec env -''' ?", "> why protoc command must add '''exec env -''' ?\r\n\r\nBecause otherwise you'd be using a different compile configuration from the rest  and it would lead to the errors in the OP when things are linked with protoc_lib."]}, {"number": 40859, "title": "still windows unavailable for build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 2004\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0-rc.0\r\n- Python version: 3.8.3 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.3.0 x64\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2019\r\n- CUDA/cuDNN version: 10.2 / 7.6.5\r\n- GPU model and memory:  RTX2080Ti GDDR6 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nbuild with gpu error\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n.\\configure\r\nbazel build --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: D:/repo/tensorflow/tensorflow/core/kernels/BUILD:3782:18: C++ compilation of rule '//tensorflow/core/kernels:tridiagonal_matmul_op_gpu' failed (Exit 1): python.exe failed: error executing command\r\n  cd C:/users/alan-workstation/_bazel_alan-workstation/ibqopsat/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.26.28801\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.26.28801\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.26.28801\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.26.28801\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.26.28801\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\ALAN-W~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\ALAN-W~1\\AppData\\Local\\Temp\r\n  C:/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Iexternal/aws /Ibazel-out/x64_windows-opt/bin/external/aws /Iexternal/aws-c-common /Ibazel-out/x64_windows-opt/bin/external/aws-c-common /Iexternal/aws-c-event-stream /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream /Iexternal/aws-checksums /Ibazel-out/x64_windows-opt/bin/external/aws-checksums /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cusolver/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusolver/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /Iexternal/aws/aws-cpp-sdk-core/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-core/include /Iexternal/aws/aws-cpp-sdk-s3/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-s3/include /Iexternal/aws/aws-cpp-sdk-transfer/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-transfer/include /Iexternal/aws-c-common/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-common/include /Iexternal/aws-c-event-stream/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream/include /Iexternal/aws-checksums/include /Ibazel-out/x64_windows-opt/bin/external/aws-checksums/include /Iexternal/local_config_cuda/cuda/cusparse/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusparse/include /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DCURL_STATICLIB /DPLATFORM_WINDOWS /DENABLE_CURL_CLIENT /DOPENSSL_IS_BORINGSSL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI -nvcc_options=disable-warnings /std:c++14 -x cuda -DGOOGLE_CUDA=1 -Xcuda-fatbinary=--compress-all --no-cuda-include-ptx=all --cuda-include-ptx=sm_75 --cuda-gpu-arch=sm_75 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/tridiagonal_matmul_op_gpu/tridiagonal_matmul_op_gpu.cu.o /c tensorflow/core/kernels/tridiagonal_matmul_op_gpu.cu.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9002 : ignoring unknown option '--no-cuda-include-ptx=all'\r\ncl : Command line warning D9002 : ignoring unknown option '--cuda-include-ptx=sm_75'\r\ncl : Command line warning D9002 : ignoring unknown option '--cuda-gpu-arch=sm_75'\r\n.\\tensorflow/core/kernels/cuda_sparse.h(39): error: identifier \"cusparseDnMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(40): error: identifier \"cusparseSpMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(41): error: identifier \"cusparseSpMMAlg_t\" is undefined\r\n\r\n3 errors detected in the compilation of \"C:/Users/ALAN-W~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp8susqlh4/tridiagonal_matmul_op_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2236.805s, Critical Path: 165.63s\r\nINFO: 4808 processes: 4808 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["It's been very long since I succeeded building tensorflow with my windows pc....", "and 2.4.0 also fails to build with windows 10 and cuda 10.2+cudnn7.6.5", "On my machine I cannot reproduce, and build completes successfully.\r\nSince the above is a missing cuda/cusparse symbol, redirecting to @chsigg and @sanjoy ", "Are you building with CUDA 10.2 or CUDA 11?  Otherwise those lines will be `#ifdef`ed out.\r\n\r\n@nluehr Any ideas on what could be going on here?", "@sanjoy \r\nsorry, 10.2, of course", "cusparse.h is included on line 30, and the 10.2 cusparse header definitely defines cusparseDnMatDescr_t. Is it possible that the copy of cusparse.h in the bazel-out sandbox is stale (perhaps left around from a cuda 10.1 build)?", "I am currently facing same issue, i am trying to build tensorflow v2.3.0 with CUDA 10.2 and cuDNN 7.6.5,\r\n\r\n![image](https://user-images.githubusercontent.com/30038478/92100141-fde74880-ee0d-11ea-9f31-4a12041384b2.png)\r\n", "Thanks @nluehr for the clue.\r\n\r\n@ibrahimsoliman97 @alanpurple to test the theory above, could you first run `bazel clean --expunge`\r\nThen rerun configure and rebuild?\r\nIf indeed your workspace ran a build with cuda 10.1 before, unless you run the `bazel clean --expunge` command old configs will stay around and corrupt your build.", "Same problem here with tensorflow 2.3.1, cuda 10.2, python 3.8.6, bazel 3.1.0 and MSVC 2019 16.7.7 on Windows 20H2 64bit. Only cuda 10.2 installed (no other cuda versions installed according to this thread https://github.com/tensorflow/tensorflow/issues/39333#issuecomment-700357960), and `bazel clean --expunge` does not work.\r\n\r\nLogs:\r\n\r\n```\r\nERROR: D:/build/tensorflow-2.3.1/tensorflow/core/kernels/BUILD:3791:1: C++ compilation of rule '//tensorflow/core/kernels:tridiagonal_solve_op_gpu' failed (Exit 1): python.exe failed: error executing command\r\n  cd C:/users/spindensity/_bazel_spindensity/ba7jcydy/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Dev/BasicTools/python/virtualenvs/tf_build/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Dev/BasicTools/python/virtualenvs/tf_build/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\SPINDE~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_ENABLE_XLA=1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\SPINDE~1\\AppData\\Local\\Temp\r\n  C:/Dev/BasicTools/python/virtualenvs/tf_build/Scripts/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/mkl_dnn /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Iexternal/aws /Ibazel-out/x64_windows-opt/bin/external/aws /Iexternal/aws-c-common /Ibazel-out/x64_windows-opt/bin/external/aws-c-common /Iexternal/aws-c-event-stream /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream /Iexternal/aws-checksums /Ibazel-out/x64_windows-opt/bin/external/aws-checksums /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cusolver/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusolver/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /Iexternal/mkl_dnn/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/include /Iexternal/mkl_dnn/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src /Iexternal/mkl_dnn/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/common /Iexternal/mkl_dnn/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu /Iexternal/mkl_dnn/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/gemm /Iexternal/mkl_dnn/src/cpu/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn/src/cpu/xbyak /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /Iexternal/aws/aws-cpp-sdk-core/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-core/include /Iexternal/aws/aws-cpp-sdk-s3/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-s3/include /Iexternal/aws/aws-cpp-sdk-transfer/include /Ibazel-out/x64_windows-opt/bin/external/aws/aws-cpp-sdk-transfer/include /Iexternal/aws-c-common/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-common/include /Iexternal/aws-c-event-stream/include /Ibazel-out/x64_windows-opt/bin/external/aws-c-event-stream/include /Iexternal/aws-checksums/include /Ibazel-out/x64_windows-opt/bin/external/aws-checksums/include /Iexternal/local_config_cuda/cuda/cusparse/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cusparse/include /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DCURL_STATICLIB /DPLATFORM_WINDOWS /DENABLE_CURL_CLIENT /DOPENSSL_IS_BORINGSSL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /std:c++14 -x cuda -DGOOGLE_CUDA=1 -Xcuda-fatbinary=--compress-all --no-cuda-include-ptx=all --cuda-include-ptx=sm_61 --cuda-gpu-arch=sm_61 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_USE_XLA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/tridiagonal_solve_op_gpu/tridiagonal_solve_op_gpu.cu.o /c tensorflow/core/kernels/tridiagonal_solve_op_gpu.cu.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9002 : ignoring unknown option \u201c--no-cuda-include-ptx=all\u201d\r\ncl : Command line warning D9002 : ignoring unknown option \u201c--cuda-include-ptx=sm_61\u201d\r\ncl : Command line warning D9002 : ignoring unknown option \u201c--cuda-gpu-arch=sm_61\u201d\r\n\r\n.\\tensorflow/stream_executor/dnn.h(858): here\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(39): error: identifier \"cusparseDnMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(40): error: identifier \"cusparseSpMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(41): error: identifier \"cusparseSpMMAlg_t\" is undefined\r\n\r\ntensorflow/core/kernels/tridiagonal_solve_op_gpu.cu.cc(98): warning: extra \";\" ignored\r\n\r\n3 errors detected in the compilation of \"C:/Users/SPINDE~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp3nb_ygoq/tridiagonal_solve_op_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6783.098s, Critical Path: 4048.81s\r\nINFO: 5532 processes: 5532 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "> cusparse.h is included on line 30, and the 10.2 cusparse header definitely defines cusparseDnMatDescr_t. Is it possible that the copy of cusparse.h in the bazel-out sandbox is stale (perhaps left around from a cuda 10.1 build)?\r\n\r\n@nluehr @gunan \r\n\r\nAfter some investigation, I found cuda 10.2 does not define `cusparseDnMatDescr_t`, `cusparseSpMatDescr_t`, `cusparseSpMMAlg_t` at all when the compilation target is x64 on windows.\r\n\r\nThese symbols are defined in `cusparse.h` in a `#if !defined(_WIN32) ... #endif` scope:\r\n\r\n```cpp\r\n//##############################################################################\r\n//# SpMM APIs\r\n//##############################################################################\r\n\r\n#if !defined(_WIN32)\r\n\r\ntypedef enum {\r\n    CUSPARSE_MM_ALG_DEFAULT = 0,\r\n    CUSPARSE_COOMM_ALG1 = 1, // non-deterministc results\r\n    CUSPARSE_COOMM_ALG2 = 2, // deterministic results\r\n    CUSPARSE_COOMM_ALG3 = 3, // non-deterministc results, for large matrices\r\n    CUSPARSE_CSRMM_ALG1 = 4\r\n} cusparseSpMMAlg_t;\r\n\r\ntypedef struct cusparseSpMatDescr* cusparseSpMatDescr_t;\r\ntypedef struct cusparseDnMatDescr* cusparseDnMatDescr_t;\r\n\r\n#endif // !defined(_WIN32)\r\n```\r\n\r\nAccording to the [Microsoft documentation](https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros?view=msvc-160):\r\n\r\n```\r\n_WIN32 Defined as 1 when the compilation target is 32-bit ARM, 64-bit ARM, x86, or x64. Otherwise, undefined.\r\n```\r\n\r\nSo these symbols are not defined when the compilation target is x64 and make the build fail.\r\n\r\nAll `cusparse.h` files in `bazel-out` directory have the same SHA256 with the very file in `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\include`, cuda version is `10.2.89_441.22`.", "this is not a problem anymore", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40859\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40859\">No</a>\n"]}, {"number": 40858, "title": "how to improve accuracy for an example code at tensorflow.org ", "body": "I am trying to run some example Deep learning python3 code on databricks/GPU. The code is from https://www.tensorflow.org/tutorials/keras/text_classification_with_hub#evaluate_the_model\r\n\r\nI got the results :\r\n\r\n    training loss: 0.0762 - training accuracy: 0.9929 \r\n     validation_loss: 0.5734 - validation_accuracy: 0.8628\r\n\r\nThe example said\r\n\r\n      \"This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%.\"\r\n\r\nI want to find how to improve the accuracy.\r\n\r\nFrom the results, I think it is overfitting. So, I tried to add l1 and l2 regularizer and dropout.\r\n\r\n     embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\r\n     hub_layer = hub.KerasLayer(embedding, input_shape=[], \r\n                       dtype=tf.string, trainable=True)\r\n  \r\n     tf.keras.regularizers.l1_l2(l1=0.04, l2=0.01)  # L1 + L2 penalties\r\n      model = tf.keras.Sequential()\r\n     model.add(hub_layer)\r\n     model.add(tf.keras.layers.Dense(8, activation='relu'))\r\n     model.add(tf.keras.layers.Dropout(rate=0.3))\r\n     model.add(tf.keras.layers.Dense(1))\r\n\r\nI have tried different dropout (0.2, 0.3, 0.5, 0.7) and l1/l2 regularizers (0.01, 0.02, 0.04).\r\n\r\nI have reduced the units in the first hidden layer from 16 to 8. I have tried Reducing (Versus Delaying) Overfitting in Neural Network and how to reduce overfitting in neural networks?\r\n\r\nBut, no improvement. How can I reduce the overfitting ?\r\n\r\nthanks", "comments": ["@umusa,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "@amahendrakar , I have posted it on stackoverflow, and I have tried all solutions that I can search on stakoverflow, but\r\nno solutions work well. Hope that the experts of TensorFlow here can help me. thanks ", "@umusa Did you find a solution?  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this  issue as it has been awaiting response for more than 2 weeks. Please add additional comments for us to open this issue again."]}, {"number": 40857, "title": "ImportError: cannot import name device_spec", "body": "\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_tf1_test.py\", line 21, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/home/magdalena/models/research/object_detection/builders/model_builder.py\", line 19, in <module>\r\n    from object_detection.builders import anchor_generator_builder\r\n  File \"/home/magdalena/models/research/object_detection/builders/anchor_generator_builder.py\", line 23, in <module>\r\n    from object_detection.anchor_generators import flexible_grid_anchor_generator\r\n  File \"/home/magdalena/models/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py\", line 19, in <module>\r\n    from object_detection.anchor_generators import grid_anchor_generator\r\n  File \"/home/magdalena/models/research/object_detection/anchor_generators/grid_anchor_generator.py\", line 27, in <module>\r\n    from object_detection.utils import ops\r\n  File \"/home/magdalena/models/research/object_detection/utils/ops.py\", line 28, in <module>\r\n    import tf_slim as slim\r\n  File \"/home/magdalena/.local/lib/python2.7/site-packages/tf_slim/__init__.py\", line 25, in <module>\r\n    from tf_slim.layers import *\r\n  File \"/home/magdalena/.local/lib/python2.7/site-packages/tf_slim/layers/__init__.py\", line 25, in <module>\r\n    from tf_slim.layers.layers import *\r\n  File \"/home/magdalena/.local/lib/python2.7/site-packages/tf_slim/layers/layers.py\", line 30, in <module>\r\n    from tf_slim.ops import variables\r\n  File \"/home/magdalena/.local/lib/python2.7/site-packages/tf_slim/ops/variables.py\", line 27, in <module>\r\n    from tensorflow.python.framework import device_spec as tf_device\r\nImportError: cannot import name device_spec\r\n\r\n\r\nPython = 2.7\r\nTensorflow = 1.13.1\r\nUbuntu 18.04.4\r\n\r\nWhat could be the problem and how can i solve this issue?\r\n", "comments": ["@marynaggita \r\n\r\nIs there any particular reason for using older version of tf when there are later versions, can you upgrade to later versions to see if it helps resolve the issue.\r\nAlso refer to similar [issue here](https://github.com/tensorflow/tensorflow/issues/40182#issuecomment-641810207).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@marynaggita I believe you are trying to execute [TensorFlow Object Detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md)\r\nIt's installation guide states that you require TF 1.15 Version as a prerequisite.\r\nFollowing import is successful in TF 1.15 but fails in 1.13.1 with stack trace you shared.\r\n```python\r\nfrom tensorflow.python.framework import device_spec as tf_device\r\n```\r\nPlease upgrade your TF version to 1.15\r\nThanks!\r\n\r\n", "Thank you. It worked out after upgrading", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40857\">No</a>\n"]}, {"number": 40856, "title": "TFTRT combinednms fail in in TRT 6 and TF 1.15.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0 cuDNN 7.6.3\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nTensorRT 6 support is added by https://github.com/tensorflow/tensorflow/pull/32397 which include three changes: header files, fall to fp16, and combined nms WAR last output dim.\r\n\r\nHowever in 1.15 release, only header file change is cherry-picked https://github.com/tensorflow/tensorflow/pull/32828, as a result, two other change is missing. TFTRT CombinedNMS is no workable with TRT 6.\r\n\r\n**Describe the expected behavior**\r\n\r\nShould also cherry-pick other two changes.\r\n", "comments": ["@aaroey ", "@bixia1 ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40856\">No</a>\n"]}, {"number": 40855, "title": "tfrecords on s3 very slow on tf-nightly, not tensorflow 2.2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-35161-gd659eb9c0d 2.3.0-dev20200625\r\n- Python version: 3.7.1\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nLoading tfrecord data from s3 is slow in the nightly version. Following script takes 0.8 seconds on tf2.2 but 5 seconds on tf-nightly. The effects are more pronounced with slow network speeds. \r\n\r\n**Describe the expected behavior**\r\n\r\ntfrecord streaming should be as fast in tf-nightly as in tf2.2. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Q5zsGJYKrogNGqfQ6DXds4AKWEXSzwKx?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Abhipray,\r\nI did not observe much difference on comparing TF v2.2 with the latest TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/145c3807e9e932202ecad1aa9cb89a44/40855-2-2.ipynb). The difference was about ~0.07 seconds.\r\n\r\nCould you please upgrade to the latest nightly version i.e. TF v2.5.0-dev20200629 and check if it works. Thanks!", "@amahendrakar \r\nYou are right. On Google colab with 2.5.0-dev20200629, I am no longer able to replicate this issue. However, I still see this issue on my macbook. tf 2.5.0-dev20200629 is 3x slower than tf 2.2. How can I produce a gist that captures this on my local machine? \r\n\r\n", "@Abhipray,\r\nPlease share the `.ipynb` file or the Jupyter notebook which you have run. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40854, "title": "Added allocate_temp to op kernel methods and refactored tests for kernels", "body": "@annarev @bmzhao ", "comments": ["@dnguyen28061 Can you please check @annarev's comments and keep us posted. Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40854) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it. ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40854) for more info**.\n\n<!-- ok -->"]}, {"number": 40853, "title": "Added allocate_temp to op kernel methods and refactored tests for kernels ", "body": "", "comments": []}, {"number": 40852, "title": "tf.keras.backend.repeat_elements does not support negative indexing on tensors with dynamic shape", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and Google Colab\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: v2.2.0-0-g2b96f3662b 2.2.0\r\n-   **Python version**: 3.7\r\n-   **CUDA/cuDNN version**: 10.1\r\n-   **GPU model and memory**: Colab GPU\r\n\r\n### Describe the problem\r\nThe implementation of repeat_elements behave differently whether the input tensor has static or dynamic shape.\r\nFor tensors with dynamic shape it does not accept negative indexing for the axis parameter.\r\nFor tensors with static shape it accepts negative indexing for the axis parameter.\r\nTensorFlow follow standard python indexing rules. \r\nThere is a workaround using positive indexing.\r\n\r\n**How to reproduce**\r\nRun the repeat_elements with axis=-1 and tf.config.experimental_run_functions_eagerly(False)\r\nNote the resulting array\r\nRun with tf.config.experimental_run_functions_eagerly(True)\r\nNote the resulting array\r\nSet axis=1 and repeat 1-4\r\nNote that input_signature parameter in tf.function is there to reproduce a scenario of a graph with the tensor x with dynamic shape.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n```\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.int32)])\r\ndef f(x):\r\n  x = K.repeat_elements(x, rep=3, axis=-1)\r\n  return x\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\nv = tf.Variable([[0, 1],[2, 3]])\r\nf(v)\r\n```\r\n> <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\r\n> array([[0, 0, 0, 1, 1, 1],\r\n>        [2, 2, 2, 3, 3, 3]], dtype=int32)>\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.int32)])\r\ndef f(x):\r\n  x = K.repeat_elements(x, rep=3, axis=-1)\r\n  return x\r\n\r\ntf.config.experimental_run_functions_eagerly(False)\r\nv = tf.Variable([[0, 1],[2, 3]])\r\nf(v)\r\n```\r\n> <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\r\n> array([[0, 1, 2, 3, 0, 1],\r\n>        [2, 3, 0, 1, 2, 3]], dtype=int32)>", "comments": ["I am able to replicate the reported issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/9a76939c1ff63d1b3fb551e0314720c8/untitled244.ipynb)", "repeat_elements is kept for backwards compatibility right now, but you should prefer the underlying TensorFlow manipulations or [tf.experimental.numpy.repeat](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/repeat) if you need alternate behavior.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40852\">No</a>\n"]}, {"number": 40851, "title": "initial test for matmul with abstract tensors", "body": "@saxenasaurabh ", "comments": ["@amturati Can you please check @saxenasaurabh's comments and keep us posted. Thanks!"]}, {"number": 40850, "title": "Resource Exhausted when re-training Half of Efficientnet b0 on V100 32GB.", "body": "Hi All,\r\n    i am experiencing a scenario where i can train anEfficientNetB0 ([efficientnet](https://github.com/qubvel/efficientnet)) with a batch size of 4 . when i cut the model at some layer and make a new (smaller) model out of it , the same training is throwing Resource Exhausted error . `\r\n\r\n`Working implementation`\r\n\r\n```\r\nmodel_conv = efn.EfficientNetB0(weights='/work/source/pre_trained/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5',include_top=False,input_tensor=input_layer)\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.TimeDistributed(model_conv, input_shape=(3, 1024,1024,3)))\r\nmodel.add(tf.keras.layers.GlobalAveragePooling3D())\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Dense(classes_n - 1))\r\nmodel.add(tf.keras.layers.Activation('sigmoid', dtype='float32', name='predictions'))\r\n```\r\n\r\n`Not Working implementation`\r\n\r\n```\r\ninput_layer = tf.keras.layers.Input(shape=(1024,1024,3))\r\nmodel_conv = efn.EfficientNetB0(weights='/work/source/pre_trained/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5',\r\n                                include_top=False,input_tensor=input_layer)\r\n\r\nmodel_conv = tf.keras.Model(model_conv.input,model_conv.get_layer('block6d_add').output)\r\nmodel_conv = tf.keras.models.load_model('./models_cut/effb0_5block.h5')\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.TimeDistributed(model_conv, input_shape=(3, 1024,1024,3)))\r\nmodel.add(tf.keras.layers.GlobalAveragePooling3D())\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\nmodel.add(tf.keras.layers.Dense(classes_n - 1))\r\nmodel.add(tf.keras.layers.Activation('sigmoid', dtype='float32', name='predictions'))\r\n```\r\n\r\n\r\nError\r\n```\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[15,64,64,672] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\r\n```", "comments": ["@yuvaramsingh94 \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 40849, "title": "changed EarlyStopping so best weights are also loaded at the end of training", "body": "Hi,\r\n\r\nI love using tf.keras.callbacks.EarlyStopping. But I also want the best weights to be loaded automatically at the end of the training, in addition to during training if `restore_best_weights=True`. Checking out the semantics from the documentation:\r\n\r\nrestore_best_weights | Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.\r\n-- | --\r\n\r\nI think this PR more closely aligns with expected behavior vs. the current version. It's easy enough to make a version of EarlyStopping with this behavior, but I was pretty surprised that this wasn't the default so figured I'd make a PR.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40849) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40849) for more info**.\n\n<!-- ok -->", "FWIW --- a summary of the current behavior is:\r\n\r\nif you early stop during training causing the model to exit early, you will load your best weights.\r\n\r\nI think it should be\r\n\r\nif you stop early during training causing the model to exit early, you will load your best weights. And, if your training continues to completion without early stopping, at the end of training, you still load your best weights.", "@omalleyt12 BTW --- I saw some check is not successful; anything I can do to fix it? I can't see the details because it looks to be some google-internal check."]}, {"number": 40848, "title": "[CherryPick:r2.3] Add SaveableObjects to SavedModel.", "body": "When objects are loaded from the SavedModel, they don't retain their `_gather_saveables_for_checkpoint` functions, which can result in values not being loaded from the checkpoint.\n\nThis CL adds a field in the SavedModel proto that stores a save and restore function for each SaveableObject in each node. When loading into Python, the SaveableObjects are restored using the functions.\n\nPiperOrigin-RevId: 318512603\nChange-Id: I9b2b773c263703e9eb8e6114c631160ff4f7d1c1", "comments": ["Just a note, this CL will have to be rolled back for a minor edit (it is causing one of our tests to fail). I'll create a new PR once it is submitted."]}]