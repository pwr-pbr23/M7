[{"number": 15927, "title": "R1.5/verbs w 0 copies", "body": "## Verbs implementation to use direct tensor writes (0 copies)\r\n\r\n### Motivation:\r\n\r\nFollowing HKUST research on the use of GPU direct, and their [GDR implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/README.md), we wish to adopt the 0 copies approach and apply it to the current verbs implementation, while keeping the current implementation advantages, such as configurability and the use of RDMA for control messages.\r\n\r\n### Performance:\r\n\r\nCompared with the current GRPC, verbs and GDR implementation, the result implementation gave the best performance for every model, with any number of nodes. For VGG16 on 8 nodes with 4 P100 GPUs each, the prototype beat the second place by over 15%.\r\n\r\n### Implementation requirements:\r\n\r\n1. Tensor writes need to be done directly from the source Tensor to the destination Tensor, with no memory copies in between. This should be done for all DMAble tensors which are located either on CPU or on a RDMA compatible GPU device (GPU direct). \r\n2. Non DMAble tensors (CanMemCopy == false) will be serialized to proto on the sender side, RDMA written to a registered buffer on the receiver side, and then deserialized by the receiver.\r\n3. Tensors which are located on a non-RDMA-compatible GPU, will be RDMA written to a registered CPU proxy buffer on the receiver side, and then copied to GPU by the receiver.\r\n\r\n### Implementation constrains:\r\n\r\nFor best stability and proof of correctness, we will divide the implementation to two stages:\r\n1. At first stage we will keep changes to the current implementation to the minimum possible. The expense will be that we may have unused or unnecessary code leftovers, which may also affect performance. \r\n2. At second stage, we will re-iterate over the code and remove irrelevant code parts.\r\nThe design of the solution aims that we will achieve both stages with relative ease. \r\n\r\n### Design guidelines:\r\n\r\n1. Since we do not want to do any unnecessary memory copying, we will no longer allocate a fixed CPU buffer as the destination for the RDMA write. Instead we will do the writing directly to the result tensor, or if the result tensor is on a device which does not support RDMA, we will do the writing to a proxy CPU tensor and then copy its content to the result tensor.\r\n2. The address of the destination Tensor needs to be sent to the sender side for writing, meaning that the result/proxy tensor should be pre-allocated on the receiver side, prior to sending the tensor request. In order to do that, we need to know its meta-data, i.e. shape and data-type for DMAble tensors, and proto-size for serialized tensors. Unfortunately, this information is only available on the sender side which complicates manners. In order to avoid sending extra messages for querying the meta-data on each step, we store a local meta-data cache per tensor. Based on the assumption that the meta-data of a tensor rarely changes between steps, we expect that on most times the cache will only be updated once. When the sender receives a request for a tensor, if it is the first time this tensor is requested, or in the rare case that the meta-data did change, the sender will first send a meta-data response, on which the receiver will update the local cache, and reallocate the result/proxy tensors if required. When the receiver sends the tensor request, it will contain also the meta-data currently stored in its local cache, so the sender can compare it to see if there was a change.\r\n3. When the sender writes the tensor content to the result tensor, no additional data is being written with it. That means we need to reside on ibverbs immediate (uint32_t) to indicate which request we are responding to (in order to trigger the receive callback). The easiest and most elegant way is to key the recv callback with a unique request_index (uint32_t), instead of the current key_with_step_id (string). \r\n4. Since the sender no longer writes the tensor from/to fixed buffers, we no longer need to schedule the writes using the local/remote status. In addition we no longer rely on the RmdaTensorBuffer members as the source/destination addresses and rkey/lkey. Instead, each RdmaTensorBuffer will hold multiple \"Response\" objects (one per step-id), from which we derive destination address and rkey. The source address and lkey are always the ones of the source Tensor.\r\n5. With the addition of tensor pre-allocation, we noticed there is a large code similarity between sending the first tensor request and re-sending the request in case of meta-data changes. After implementing a common method for tensor pre-allocation, it turned out that implementation becomes much simpler by encapsulating the process of request sending/re-sending, meta-data response callback and content response callback, all in a single \"Request\" class. The request class holds all the relevant request information, which reduces excessive parameter passing and lambda capturing. This decision is purely for elegance and code simplicity, and we decided to implement it in first stage because it makes the implementation much easier.\r\n\r\n### New types/classes:\r\n\r\n* **enum RdmaImmDataType** - Immediate types to distinguish between different RDMA writes on the remote side. Ack writes and control-message writes have a fixed immediate value. The rest of the writes are tensor writes and the immediate value is the relevant request index.\r\n* **enum  RdmaWriteIDType**    - Types to distinguish between different RDMA write-complete events: Ack, control message, tensor DMA write and tensor proto write.\r\n* **class RdmaWriteID**        - Context for RDMA write complete events. Holds the RdmaWriteIDType and additional data.\r\n* **class RemoteAddressContext** - Remote address information (address + mr). Will be passed as write context for tensor proto writes.\r\n* **class RdmaTensorMetaData** - Meta-data for a tensor (type, shape, is_dead, proto_size).\r\n* **class RdmaMemoryMgr**      - Manages the meta-data cache, and the registered memory regions.\r\n* **class RdmaTensorRequest**  - Holds and manages information for a single tensor request throughout the entire receive cycle. API:\r\n\t* Start() - Start the request.\r\n\t* RecvTensorMetaData() - Receive meta-data from the remote side.\r\n\t* RecvTensorContent() - Receive tensor content from the remote side and invoke the done() callback. \r\n* **class RdmaTensorResponse** - Holds information for a single tensor response, such as destination address and rkey.\r\n\r\n### Protocol changes:\r\n\r\nThe protocol messages themselves will remain mostly unchanged at the first stage, but will be used differently, as described below. The current messages structures already have most of the required fields for the new implementation. The only change is the \"buffer_size\" field which is no longer used since we are no longer sending additional information with the tensor, and thus it is now always equal to the \"tensor_bytes\" field. Instead, we use that field to pass the \"request_index\".\r\n\r\n### Message structure:\r\n\r\n| type | name_size | name | step_id | request_index | remote_addr | rkey | is_dead | data_type | tensor_shape | tensor_bytes |\r\n|------|---------- |------|---------|---------------|-------------|------|---------|-----------|--------------|--------------|\r\n|  1B  |    2B     | 512  |  8B     |      8B       |         8B  |   4B |      1B |     XB    |    XB        |    8B        |\r\n\r\n* **RDMA_MESSAGE_TENSOR_REQUEST**  - (receiver ==> sender) The original tensor request. \r\n\t* type - The message type.\r\n\t* name (name_size) - Name of the requested tensor.\r\n\t* step_id - Step ID.\r\n\t* request_index - Request index.\r\n\t* remote_addr/rkey - Address/rkey of the result/proxy tensor. Irrelevant for first-time request.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The current meta-data as stored in the receiver local cache. The sender will use that information to know if the receiver's cache requires updating.\r\n* **RDMA_MESSAGE_BUFFER_REQUEST**  - (sender ==> receiver) The meta-data update message in case meta-data had changed (or if it is the first time the tensor is requested).\r\n\t* type - The message type.\r\n\t* request_index - Request index.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The up-to-date meta-data.\r\n* **RDMA_MESSAGE_BUFFER_RESPONSE** - (receiver ==> sender) Tensor re-requset after meta-data update and reallocation of result/proxy tensors.\r\n\t* type - The message type.\r\n\t* name (name_size) - Name of the requested tensor.\r\n\t* step_id - Step ID.\r\n\t* request_index - Request index.\r\n\t* remote_addr/rkey - Address/rkey of the reallocated result/proxy tensor.\r\n\t* is_dead/data_type/tensor_shape/tensor_bytes - The new meta-data. Will be removed in the next phase.\r\n* **RDMA_MESSAGE_TENSOR_WRITE**    - (sender ==> receiver) No longer sent. There is only a direct write of the tensor content to the result/proxy tensor. Request index passed as the immediate value of the write.\r\n* **RDMA_MESSAGE_TENSOR_IDLE**     - (receiver ==> sender) No longer sent.\r\n\r\n![alt text](https://raw.githubusercontent.com/Mellanox/tensorflow/eladw_verbs_w_0_copies/tensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.jpg \"Phase 1 message protocol\")\r\n\r\n### Second stage optimizations:\r\n1. Remove unused code leftovers.\r\n2. Remove the ACK buffer completely, since we can rely completely on its immediate value.\r\n\r\n### Future optimizations:\r\n1. Map the tensor names to indexes, to significantly reduce the request message size.\r\n2. Understand the purpose of empty tensors and if we can skip remote fetching for them.\r\n3. Consider concatenating multiple requests and/or using multiple message buffers.\r\n4. Consider a no-request architecture.\r\n  ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "@byronyi @poxvoculi @shamoya ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "The current PR state contains both phase1 and phase2 of the implementation.", "CLAs look good, thanks!\n\n<!-- ok -->", "@junshi15 @yanivbl6 ", "**As requested I will move this PR to master.**", "Closing this one."]}, {"number": 15926, "title": "_Assert3DImage that adds a control dependency for the shape check", "body": "I noticed that the _Check3DImage was always used in exactly the same way and extracted this into its own convenience function.\r\n\r\nThe only remaining use of _Check3DImage was an import in gen_image_ops.py saying\r\n\"# TODO(drpng): remove these once internal use has discontinued.\", so maybe it could be removed entirely.", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution!"]}, {"number": 15925, "title": "cmake compile error C2678: binary '*': no operator found sparse_column_iterable.cc", "body": "`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/ProgramData/chocolatey/bin/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 `\r\n\r\n```\r\nc:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\\_working_dir\\_ml\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc) [D:\\_working_dir\\_ml\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  c:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2100: illegal indirection (compiling source file D:\\_working_dir\\_ml\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc) [D:\\_working_dir\\_ml\\tensorflow\\tensorflow\\con\r\ntrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n```\r\nWindows 8.1 x64\r\ncmake 3.10.1\r\nswig 3.0.9\r\nVisual Studio 2017 Community\r\n\r\nthe same problem asked\r\nhttps://stackoverflow.com/questions/48058113/compiling-tensorflow-1-4-on-windows-10", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@vldbnc I encountered same error while trying to build arch:AVX pip file. I couldn't say for sure but re-installing Visual Studio 2017 might help.\r\n\r\nI was able to build arch:AVX version of pip file only after cleaning all [Visual studio](https://github.com/Microsoft/VisualStudioUninstaller) components and reinstalling [MSVC 2015 update 3](https://www.tensorflow.org/install/install_sources). MSVC 2015 update 3 setup cant compile arch:AVX2 pip because of header file [issue](https://github.com/tensorflow/tensorflow/issues/10199). \r\n\r\nBut you can try it as @aluo-x  has build it successfully using VS2017 15.4\r\nHere is [link](https://github.com/aluo-x/tensorflow_windows/blob/master/working/tensorflow-1.4.0-cp36-cp36m-win_amd64.whl) to .whl file which you are trying to build.  \r\n(I am using this file and its working fine.)\r\n\r\nMaybe @aluo-x can help in this error.\r\n  ", "I'm running into the same issue.\r\nWindows 10 x64, using the 64-bit MSBuild (not the default for some reason)\r\ncmake 3.9.2\r\nswig 3.0.12\r\nVisual Studio 2017 community.\r\n\r\nI'll try @vksbhandary 's suggestion and update", "@vksbhandary I'm using @aluo-x whl file and it's working.\r\nMissing support of AVX2 instruction set in MSCV2015 and some compiling error made me install MSCV2017 based on discussion [10199](https://github.com/tensorflow/tensorflow/issues/10199) but I ran into this issue. Thanks for suggestion, I will try to re-install and build again with VS2017 15.4 toolset.\r\n\r\n", "The problem is that in Visual Studio 2017 (15.5) the std::lower_bound() function internally creates a const object of the iterator for which it then calls operator*(). However, IndicesRowIterator does not define this for const objects, which results in the error the compiler is giving you.\r\nYou could fix this by adding const to\r\n\r\n`reference operator*() { return iter_->ix()(row_idx_, 0); }, `\r\n\r\ni.e.\r\n\r\n`reference operator*() const { return iter_->ix()(row_idx_, 0); }`\r\n\r\nAs the ix() function is defined as const this should be no problem. \r\n  ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@Michael-4 's [comment](#issuecomment-356275963) provides the fix for building pip file on Windows using VS 2017. I was able to built tensorflow 1.5rc with AVX2 as SMID option. So this issue can be closed.", "Closing since it looks like this is fixed on the master branch.", "Could you please reopen the issue?\r\n\r\nThis is _not_ fixed in the master branch: https://github.com/tensorflow/tensorflow/blob/054cf11347dc19a1bd2ec2b09ac4c71dc0f8ee8f/tensorflow/core/framework/op_kernel.h#L455\r\n\r\nAnd the mentioned above fix works (and it's more correct semantically):\r\n```C++\r\nreference operator*() const { return iter_->ix()(row_idx_, 0); }\r\n```"]}, {"number": 15924, "title": "Tensorflow Optimize for Inference KeyError.", "body": "I got the following error when optimizing the graph for inference:\r\nTraceback (most recent call last):\r\n  File \"C:\\Python35\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Python35\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\tools\\optimize_for_\r\ninference.py\", line 146, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", l\r\nine 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\tools\\optimize_for_\r\ninference.py\", line 90, in main\r\n    FLAGS.output_names.split(\",\"), FLAGS.placeholder_type_enum)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\tools\\optimize_for_\r\ninference_lib.py\", line 109, in optimize_for_inference\r\n    placeholder_type_enum)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\tools\\strip_unused_\r\nlib.py\", line 83, in strip_unused\r\n    raise KeyError(\"The following input nodes were not found: %s\\n\" % not_f\r\nound)\r\nKeyError: \"The following input nodes were not found: {'input'}\\n\"\r\n\r\nPlease help me soon!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Ok I got the error and now it is working.\r\nIt was due to wrong argument given while optimizing the model!", "Hi Maurya Ritesh,\r\nI am new to quantization scripts..\r\n\r\nI need you help, I am facing the same issue while running the optimize_for_inference.py script.\r\nMy arguments are as below: \r\n\r\n`python optimize_for_inference.py   --input=res101_faster_rcnn_iter_5600.pb   --output=optimized_graph.pb   --input_names=\"input\"   --output_names=\"final_result\"`\r\n\r\nMy pb file is based on Resnet-101. I am sharing my res101_faster_rcnn_iter_5600.ckpt.custom file below to see it's contents:\r\n\r\n`node {\r\n  name: \"Placeholder\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n        dim {\r\n          size: 1\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"Placeholder_1\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n        dim {\r\n          size: 1\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"Placeholder_2\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 5\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"resnet_v1_101/Pad/paddings\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n          dim {\r\n            size: 4\r\n          }\r\n          dim {\r\n            size: 2\r\n          }\r\n        }\r\n        tensor_content: \"\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000\\000\\000\\003\\000\\000\\000\\003\\000\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"resnet_v1_101/Pad\"\r\n  op: \"Pad\"\r\n  input: \"Placeholder\"\r\n  input: \"resnet_v1_101/Pad/paddings\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"Tpaddings\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"resnet_v1_101/conv1/weights/Initializer/truncated_normal/shape\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"_class\"\r\n    value {\r\n      list {\r\n        s: \"loc:@resnet_v1_101/conv1/weights\"\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n          dim {\r\n            size: 4\r\n          }\r\n        }\r\n        tensor_content: \"\\007\\000\\000\\000\\007\\000\\000\\000\\003\\000\\000\\000@\\000\\000\\000\"\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"resnet_v1_101/conv1/weights/Initializer/truncated_normal/mean\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"_class\"\r\n    value {\r\n      list {\r\n        s: \"loc:@resnet_v1_101/conv1/weights\"\r\n      }\r\n    }\r\n  }\r\n\r\n\r\n\r\nI am getting tyhe below error : \r\n\r\n`_result\"\r\n/home/arun/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n/home/arun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"optimize_for_inference.py\", line 146, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"optimize_for_inference.py\", line 90, in main\r\n    FLAGS.output_names.split(\",\"), FLAGS.placeholder_type_enum)\r\n  File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py\", line 109, in optimize_for_inference\r\n    placeholder_type_enum)\r\n  File \"/home/arun/anaconda3/lib/python3.6/site-packages/tensorflow/python/tools/strip_unused_lib.py\", line 83, in strip_unused\r\n    raise KeyError(\"The following input nodes were not found: %s\\n\" % not_found)\r\nKeyError: \"The following input nodes were not found: {'input'}\\n\"\r\n`\r\nPlease help me identifying what arguments you passed to run the script.\r\n\r\nThank you.\r\n\r\nKind Regards\r\nArun", "python touch.py -m res101_faster_rcnn_iter_5600.ckpt.meta -o res101_faster_rcnn_iter_5600.pb -b\r\n\r\nand my touch.py has following contents, which I took from some repository: \r\n```\r\n#! /usr/bin/python3\r\n#generate a pb file !\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\nimport argparse\r\nimport os\r\n\r\nif __name__ == '__main__':\r\n\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('-m','--model',required=True, type=str, metavar='' ,help=\"model meta graph AKA .ckpt.meta file to start with\")\r\n    parser.add_argument('-o', '--output_file', type=str, required=True, metavar='',help=\"Where to save the output file \")\r\n    parser.add_argument('-b','--bin',dest='binary_file',action='store_true', help=\"saves file as binary instead of default text file\")\r\n    parser.add_argument('-v','--verbose',dest='verbosity',action='store_true',help=\"trigger the verbose output (show tensorflow debug info)\")\r\n    args = parser.parse_args()\r\n    #get output dir from output file name\r\n    output_dir = os.path.dirname(args.output_file)\r\n    if output_dir == \"\":\r\n        output_dir=\".\"\r\n    output_file_name = os.path.basename(args.output_file)\r\n    #setting verbosity    \r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n    if args.verbosity:\r\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\r\n    print (\"Welcome to tensorflow graph saver\")\r\n    print (\"loading \",os.path.basename(args.model))\r\n    #opening a new session\r\n    with tf.Session() as sess:\r\n        new_saver = tf.train.import_meta_graph(args.model)\r\n        new_saver.restore(sess,args.model.replace(\".meta\",\"\") )\r\n        print (\"model\",os.path.basename(args.model),\"loaded correctly... converting it to graph\")\r\n        tf.train.write_graph(sess.graph, output_dir ,output_file_name, as_text=not args.binary_file)\r\n        print(\"conversion done, output file saved as\",args.output_file)\r\n```\r\n", "https://github.com/MauryaRitesh what was the wrong argument you have passed? I have the same error please help me!!! \r\nhttps://github.com/tensorflowbutler", "Note: If you are getting an error KeyError: \u201cThe following input nodes were not found: {\u2018input\u2019}\\n\u201d then change \"input\" to \"Mul\" \r\nIf you have any doubt refer to this tutorial: [TensorFlow on Mobile: Tutorial](https://towardsdatascience.com/tensorflow-on-mobile-tutorial-1-744703297267)"]}, {"number": 15923, "title": "Add clean_dep to copts macro.", "body": "Currently, copts macro has select statement with //tensorflow/...\r\nThis resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "test failures are unrelated. Merging PR."]}, {"number": 15922, "title": "Add clean_dep to a bazel macro.", "body": "Currently, copts macro has select statement with //tensorflow/...\r\nThis resulted in bazel error when any supermodule that uses tensorflow as a submoudle and uses copts macro.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Test failures are unrelated. Mergin PR. Thanks, @si-you "]}, {"number": 15921, "title": "iOS: Op type not registered 'DecodeWav'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am trying to run graph model from Simple Audio Recognition example on iOS.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13\r\n- **TensorFlow installed from (source or binary)**: Branch r1.4\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: Build label: 0.9.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI am trying to run graph model from Simple Audio Recognition example on iOS. When I am calling `session->Create(tensorflow_graph)` with the graph I get the error: \"Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav'...\".\r\n\r\nMy initial thought is that because I am using TensorFlow-experimental (1.1.1) from pods, it's possible that this Op type is not registered. So I tried building it myself, which builds without errors with command: `tensorflow/contrib/makefile/build_all_ios.sh`. I then remove TensorFlow-experimental (1.1.1) from the project and link my own build of tensorflow, but I get the same error. \r\n\r\nI also found the following PR - [[iOS] Add optional Selective Registration of Ops #14421](https://github.com/tensorflow/tensorflow/pull/14421)\r\n\r\nI tried building from master with the above PR merged like so:\r\n \r\nFor iPhone 5:\r\n`tensorflow/contrib/makefile/build_all_ios.sh -a armv7 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`\r\n\r\nFor iPhone SE:\r\n`tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`\r\n\r\nIf I then go and check the file `/tensorflow/tensorflow/core/framework/ops_to_register.h` (auto-generated after above command) I can see that DecodeWav is listed among kernels and operations:\r\n\r\n```\r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"BinaryOp< CPUDevice, functor::add<float>>\",\r\n\"AudioSpectrogramOp\",\r\n\"ConstantOp\",\r\n\"Conv2DOp<CPUDevice, float>\",\r\n\"DecodeWavOp\",\r\n\"IdentityOp\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"MaxPoolingOp<CPUDevice, float>\",\r\n\"MfccOp\",\r\n\"NoOp\",\r\n\"PlaceholderOp\",\r\n\"ReluOp<CPUDevice, float>\",\r\n\"ReshapeOp\",\r\n\"SoftmaxOp<CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"Add\")\r\n     || isequal(op, \"AudioSpectrogram\")\r\n     || isequal(op, \"Const\")\r\n     || isequal(op, \"Conv2D\")\r\n     || isequal(op, \"DecodeWav\")\r\n     || isequal(op, \"Identity\")\r\n     || isequal(op, \"MatMul\")\r\n     || isequal(op, \"MaxPool\")\r\n     || isequal(op, \"Mfcc\")\r\n     || isequal(op, \"NoOp\")\r\n     || isequal(op, \"Placeholder\")\r\n     || isequal(op, \"Relu\")\r\n     || isequal(op, \"Reshape\")\r\n     || isequal(op, \"Softmax\")\r\n     || isequal(op, \"_Recv\")\r\n     || isequal(op, \"_Send\")\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n\r\n```\r\n\r\nBut when I try to run the graph model I still get same error message. I have removed the pod version, and I am 100% sure I am running my own build version of tensorflow on iOS.\r\n\r\nI can't tell if this is a bug or I am doing something wrong during the build process.\r\n\r\nHas anyone tried running any graph that uses DecodeWav on iOS?\r\n\r\nThanks.\r\n\r\n### Source code / logs\r\n\r\nError: Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav' in binary running on Antons-iPhone. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@petewarden can you take a look?", "@anton6 any luck on this? I'm having the same problem, and I tried adding both `-DSELECTIVE_REGISTRATION` and `-DSUPPORT_SELECTIVE_REGISTRATION` in the Makefile, according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py, but to no avail..", "@jeffxtang hi! No luck yet. I'm still trying things out, but same result. I'm hoping that someone at Tensorflow team can have a look.", "If I go to `/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_ARM64/tensorflow/core/kernels/` after running the build it's listed among ops: \r\n\r\n<img width=\"358\" alt=\"screen shot 2018-01-23 at 14 13 10\" src=\"https://user-images.githubusercontent.com/16119505/35280279-99d6dd30-0047-11e8-979e-a5da84b2d583.png\">\r\n\r\nSo it looks like decode_wav gets built. Not sure why it's not working.", "Some progress: having realized the unregistered DecodeWav error is similar to the old familiar DecodeJpeg issue (https://github.com/tensorflow/tensorflow/issues/2883), I ran strip_unused on the pb as follows:\r\n```\r\nbazel-bin/tensorflow/python/tools/strip_unused \\\r\n  --input_graph=/tf_files/speech_commands_graph.pb \\\r\n  --output_graph=/tf_files/stripped_speech_commands_graph.pb \\\r\n  --input_node_names=wav_data,decoded_sample_data \\\r\n  --output_node_names=labels_softmax \\\r\n  --input_binary=true\r\n```\r\nIt does get rid of the DecodeWav op in the resulting graph. But running the new stripped graph on iOS now gives me an `Op type not registered 'AudioSpectrogram'` error. \r\nAlso there's no object file audio*.o generated after build_all_ios.sh is done, although `AudioSpectrogramOp` is specified in `tensorflow/core/framework/ops_to_register.h`:\r\n```\r\nJeffs-MacBook-Pro:tensorflow-1.4.0 zero2one$ find  . -name decode*.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_ARM64/tensorflow/core/kernels/decode_bmp_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_ARM64/tensorflow/core/kernels/decode_wav_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_ARMV7/tensorflow/core/kernels/decode_bmp_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_ARMV7/tensorflow/core/kernels/decode_wav_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_ARMV7S/tensorflow/core/kernels/decode_bmp_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_ARMV7S/tensorflow/core/kernels/decode_wav_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/kernels/decode_bmp_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/kernels/decode_wav_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_X86_64/tensorflow/core/kernels/decode_bmp_op.o\r\n./tensorflow/contrib/makefile/gen/obj/ios_X86_64/tensorflow/core/kernels/decode_wav_op.o\r\nJeffs-MacBook-Pro:tensorflow-1.4.0 zero2one$ find  . -name audio*_op.o\r\nJeffs-MacBook-Pro:tensorflow-1.4.0 zero2one$ \r\n```\r\n\r\n", "Sorry you're hitting problems! The issue here is that the *op* isn't being registered, not the *kernel*. The op is like the function signature of a TensorFlow operation, and in this case is defined in the following file:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/audio_ops.cc\r\n\r\nI think to fix the error, you'll need to add this file to the manual list of objects that are compiled by the makefile:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt#L285\r\n\r\nLet me know if that helps with the issue, and if it does I'd love a PR! ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Thanks @petewarden! That fixed the error. I need to use the original speech_commands_graph.pb. The stripped version fails to load with this error `Could not create TensorFlow Graph: Invalid argument: Node 'Mfcc': Connecting to invalid output 1 of source node decoded_sample_data which has 1 outputs`. \r\n\r\nJust like Pete said, op is different from kernel. The error with DecodeJpeg is `No OpKernel was registered to support Op 'DecodeJpeg'` but the error with DecodeWav is `Op type not registered 'DecodeWav'`, and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/audio_ops.cc has DecodeWav, AudioSpectrogram, etc registered. ", "Hi @petewarden I just created a PR https://github.com/tensorflow/tensorflow/pull/16537 Thanks! Jeff", "Closing as fixed, thanks Jeff!"]}, {"number": 15919, "title": "A bug of tf.layers.batch_normalization when training is not a constant", "body": "I encountered a bug of tf.layers.batch_normalization when the training argument is not a constant. Usually, when the training argument evaluates to False, the moving mean and variance should not be updated. However, in certain cases, the moving mean and variance may become NaN.\r\n\r\nThe bug occurs in the following code in python/layers/normalization.py:\r\n\r\n```\r\n    training_value = utils.constant_value(training)\r\n    if training_value is None:\r\n      one_minus_decay = utils.smart_cond(training,\r\n                                         lambda: self._one_minus_decay,\r\n                                         lambda: 0.)\r\n    else:\r\n      one_minus_decay = ops.convert_to_tensor(self._one_minus_decay)\r\n    if training_value or training_value is None:\r\n      mean_update = self._assign_moving_average(self.moving_mean, mean,\r\n                                                one_minus_decay)\r\n      variance_update = self._assign_moving_average(self.moving_variance,\r\n                                                    variance, one_minus_decay)\r\n```\r\n\r\nWhen training is not a constant but evaluates to False, one_minus_decay is set to 0, and it is expected that _assign_moving_average does not actually change the moving average. However, mean and variance are outputs of FusedBatchNormOp, which are not actually computed when training is False. So, the content of mean and variance are random, and it can contain NaN values in certain cases. The NaN values in mean and variance then lead to NaN values in the moving mean and moving variance, even if the one_minus_decay is 0 (NaN times 0 is still NaN). Once moving mean and variance contain NaN values, the network produces NaN outputs forever.\r\n\r\nI think a way to fix this issue is to modify _assign_moving_average. Just add following one line: \r\n`update_delta = tf.cond(tf.equal(one_minus_decay, 0), 0, update_delta)`\r\n Another way to fix is to let mean and variance output be zero if training is False. This way also helps preventing triggering the inf_or_nan_filter of tfdbg.\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I've run into this issue. Using a python bool for the `training` argument works, but as soon as I replace it with a tf.placeholder I get a `nan` loss. \r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Ubuntu\r\nTensorFlow installed from: https://www.tensorflow.org/install/install_linux\r\nTensorFlow version: 1.6\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce:\r\n```\r\n  training = tf.placeholder(tf.bool, [])\r\n  ...\r\n  node = tf.layers.batch_normalization(node, training=training)\r\n  ...\r\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='mynet')\r\n  with tf.control_dependencies(update_ops):\r\n      loss, train_step = loss_fn(label, prediction)\r\n  ...\r\n  # train\r\n  train_loss, _ = sess.run([loss, train_step], feed_dict={training: True, ...})\r\n  print('Train loss: {}'.format(train_loss))  # <- 'Train loss: 0.9273328\r\n\r\n  # evaluate\r\n  test_loss = sess.run([loss], feed_dict={training: False, ...})\r\n  print('Test loss: {}'.format(test_loss))  # <-  'Test loss: nan'\r\n```", "We're moving to keras in favor of layers. Does this still happen with Keras?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@drpngx I can confirm that this does not happen when using Keras layers (tf 1.13), whereas with TF layers this issue does occur (in my case, cost goes to NaN).", "Thanks @Kwander !"]}, {"number": 15918, "title": "Add pos_weights practical interpretation", "body": "The current  weighted_cross_entropy_with_logits docs don't explain practically the relationship of \r\n`pos_weights > 1`,  `pos_weights < 1` to precision, recall, and class imbalance.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!"]}, {"number": 15917, "title": "Update docs for `concat` in case `axis < 0`", "body": "This fix tries to address the issue raised in #15905 where the documentation does not cover the case of `axis < 0` for `tf.concat`.\r\n\r\nThis fix fixes #15905.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 15916, "title": "Object Tracking Support ", "body": "I have a bug after updating to the latest android studio and building the detection app with it.\r\nFor previous versions of android studio I didn't have this issue before\r\n\r\nwhen I ran the tf_detect app it showed an error for few seconds that says \"Object Tracking Support Not Found...\"\r\nand when I add the line \"dependencies {\r\n    compile 'org.tensorflow:tensorflow-android:+'\r\n}\"\r\nto the gradle build file it shows another error\r\nError:(42, 0) Could not find method compile() for arguments [org.tensorflow:tensorflow-android:+] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler.\r\n<a href=\"openFile:C:\\Users\\mohda\\Desktop\\tensorflow-master_new\\tensorflow-master\\tensorflow\\examples\\android\\build.gradle\">Open File</a>\r\n\r\nany suggestions or fixes to this issue please?\r\nWhat am I doing:\r\nI have created a custom trained detector and it was working fine till android studio was updated. I have even tried with a fresh copy of the original demo and I have the same error. Yet when I downloaded the nightly build apk it didn't show any error. so it must be the android studio / tensorflow compatibility / dependency issue here.\r\nThanks \r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Object tracking support in the demo is provided by libtensorflow_demo.so (as opposed to libtensorflow_inference.so). You can build it with the `bazel`, `cmake`, or `makefile` options in build.gradle (it's not packaged as part of the TensorFlow AAR release, so `none` will not work). You can see what libs are getting packaged in your APK with `unzip -v tensorflow_demo.apk`\r\n\r\nAlternatively you can grab libtensorflow_demo.so from the prebuilt artifacts provided at https://ci.tensorflow.org/view/Nightly/job/nightly-android/ and drop that into your project.", "downloading the .so file and dropping it into my project didn't solve the problem and generated an error while building the project : \r\n\r\n    No implementation found for void package_name_tracking.ObjectTracker.initNative(int, int, boolean)   (tried Java_package_name_tracking_ObjectTracker_initNative and Java_package_name_tracking_ObjectTracker_initNative__IIZ)\r\n\r\nI opened an issue about this providing more informations about the problem please @andrewharp  check it out it's [here](https://github.com/tensorflow/tensorflow/issues/19384) ."]}, {"number": 15915, "title": "Failed to synchronise stop event.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:  5.4.0 \r\n- **CUDA/cuDNN version**: 9.1/7.0.5\r\n- **GPU model and memory**: GT 750M 2GB\r\n- **Exact command to reproduce**: Run the custom program.\r\n\r\n### Describe the problem\r\nTrying to train a simple 5 layer model with 3.7million parameters that are trainable which would occupy around 1.5GB VRAM, the training fails instantly at the very first epoch. People who had similar error claimed that it was fixed in cuDNN 7.0.5 from this thread [#14363](https://github.com/tensorflow/tensorflow/issues/14363) . But this update isn't fixing the crash. I've posted the error log below:\r\n\r\n### Source code / logs\r\n```\r\nTotal params: 3,714,788\r\nTrainable params: 3,714,788\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2018-01-06 21:58:27.567991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-06 21:58:27.568491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GT 750M major: 3 minor: 0 memoryClockRate(GHz): 0.967\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.95GiB freeMemory: 1.60GiB\r\n2018-01-06 21:58:27.568526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\nTrain on 646 samples, validate on 162 samples\r\nEpoch 1/20\r\n2018-01-06 21:58:30.447458: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-01-06 21:58:30.447529: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x5650223aede0: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-01-06 21:58:30.447572: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x5650223aede0: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-01-06 21:58:30.447629: F tensorflow/stream_executor/cuda/cuda_dnn.cc:2964] failed to set stream for cudnn handle: CUDNN_STATUS_MAPPING_ERROR\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n  \r\n  ", "comments": ["@ramanmohan Did you try the steps suggested by @cliffwoolley from the thread #14363. \r\nAlso adding @juliebernauer for any updates. ", "@ramanmohan Please follow steps described in #14363 indeed. On our side it is fixed but happy to have a look into this if it fails in the NGC container. Thanks @shivaniag.", "I tried everything mentioned in the thread [#14363](https://github.com/tensorflow/tensorflow/issues/14363). I have TensorFlow 1.4.1 installed on my Ubuntu 16.04 along with CUDA 9.1 and cuDNN 7.0.5. I am using a Nvidia GeForce GT 750M.\r\n\r\nIt is still throwing the same error. @shivaniag  @juliebernauer \r\n", "I'm having the same issue with the same GPU (GT 750M), using Arch Linux with Tensorflow 1.4.1, CUDA 9.1 and cuDNN 7.0.5, so perhaps it's an issue with this particular hardware.", "@ramanmohan @blzq Can you confirm you are using the container?", "Using the container there's an additional message, which doesn't appear otherwise:\r\n\r\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:1093] Ignoring visible gpu device (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.`\r\n\r\nI've tried recompiling from source with Cuda capability 3.0, and still get the same problem. ", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 15914, "title": "How to know my loss function does not have numerical problems?", "body": "I wrote the following loss function that I post below. How do I know that I don't have any kind of numerical issues with it? (because something tells me that I do)\r\n\r\n```\r\ndef gather_cols(params, indices, name=None):\r\n    \"\"\"Gather columns of a 2D tensor.\r\n\r\n    Args:\r\n        params: A 2D tensor.\r\n        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\r\n        name: A name for the operation (optional).\r\n\r\n    Returns:\r\n        A 2D Tensor. Has the same type as ``params``.\r\n    \"\"\"\r\n    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\r\n        # Check input\r\n        params = tf.convert_to_tensor(params, name=\"params\")\r\n        indices = tf.convert_to_tensor(indices, name=\"indices\")\r\n        try:\r\n            params.get_shape().assert_has_rank(2)\r\n        except ValueError:\r\n            raise ValueError('\\'params\\' must be 2D.')\r\n        try:\r\n            indices.get_shape().assert_has_rank(1)\r\n        except ValueError:\r\n            raise ValueError('\\'params\\' must be 1D.')\r\n\r\n        # Define op\r\n        p_shape = tf.shape(params)\r\n        p_flat = tf.reshape(params, [-1])\r\n        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\r\n                                       [-1, 1]) + indices, [-1])\r\n        return tf.reshape(tf.gather(p_flat, i_flat),\r\n                          [p_shape[0], -1])\r\n\r\n\r\ndef custom_binary_crossentropy(y_true, y_pred):\r\n    # Assumes y_pred are probabilities and that y_true has actually 2 labels inside\r\n    # Calculate: gain(y1, y2) * log(p) + gain(y2, y1) * log(1 - p)\r\n    # gain(x1, x2) = (2 ^ x1 - 1) / ((2 ^ x1 - 1) + (2 ^ x2 - 1))\r\n\r\n    # Gather y1 and y2 first\r\n    y1 = gather_cols(y_true, [0])\r\n    y2 = gather_cols(y_true, [1])\r\n\r\n    # Get 2^y - 1\r\n    y1_g = tf.subtract(tf.pow(tf.fill(tf.shape(y1), 2.0), y1), tf.fill(tf.shape(y1), 1.0))\r\n    y2_g = tf.subtract(tf.pow(tf.fill(tf.shape(y2), 2.0), y1), tf.fill(tf.shape(y2), 1.0))\r\n\r\n    # Get gains\r\n    gain1 = tf.div(y1_g, tf.add(y1_g, y2_g))\r\n    gain2 = tf.div(y2_g, tf.add(y1_g, y2_g))\r\n\r\n    # Get logs\r\n    log1 = tf.log(y_pred)\r\n    log2 = tf.log(tf.subtract(tf.fill(tf.shape(y_pred), 1.0), y_pred))\r\n\r\n    return -K.mean(tf.add(tf.multiply(gain1, log1), tf.multiply(gain2, log2)))\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 15913, "title": "How to build contrib module that depends on tensorflow/core/kernels:linalg?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. Making a contrib module.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.9\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)\r\n- **CUDA/cuDNN version**: Building without GPU support\r\n- **GPU model and memory**: Building without GPU support\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI am writing a contrib module that depends on tensorflow/core/kernels:linalg (specifically the `//tensorflow/core/kernels:linalg_ops_common` target). However, adding this dependency\r\n\r\n\ttf_custom_op_library(\r\n\t    name = \"python/ops/_medical_image_ops.so\",\r\n\t    srcs = [\r\n\t        \"kernels/index_ops.cc\",\r\n\t        \"ops/index_ops.cc\",\r\n\t    ],\r\n\t    deps = [\r\n\t        \"//tensorflow/core/kernels:linalg\"\r\n\t    ],\r\n\t)\r\n\r\nyields\r\n\r\n\tERROR: /Users/kasper/Development/tensorflow3/tensorflow/contrib/medical_image/BUILD:13:1: in check_deps rule //tensorflow/contrib/medical_image:python/ops/_medical_image_ops.so_check_deps:\r\n\tTraceback (most recent call last):\r\n\t\tFile \"/Users/kasper/Development/tensorflow3/tensorflow/contrib/medical_image/BUILD\", line 13\r\n\t\t\tcheck_deps(name = 'python/ops/_medical_image_ops.so_check_deps')\r\n\t\tFile \"/Users/kasper/Development/tensorflow3/tensorflow/tensorflow.bzl\", line 1196, in _check_deps_impl\r\n\t\t\tfail(((_dep_label(input_dep) + \" cann...)))\r\n\ttensorflow/core/kernels:linalg cannot depend on tensorflow/core:framework\r\n\r\nThe `//tensorflow/core/kernels:linalg_ops_common` target is private so I cannot depend on it directly. In an earlier release I believe there was a `//tensorflow/core/kernels:linalg_ops_common_headers_lib` target but this is not there anymore.\r\n\r\nHow can I add one of the linalg targets as a dep without introducing this circular dependency?\r\n\r\n(I have scoured over many of the `cannot depend on tensorflow/core:framework` issues from the past but have not found a solution)\r\n\r\n### Source code / logs\r\nThis is my full BUILD file:\r\n\r\n\t# Description:\r\n\t#   Contains ops for working natively with medical images in TensorFlow.\r\n\r\n\tlicenses([\"notice\"])  # Apache 2.0\r\n\r\n\texports_files([\"LICENSE\"])\r\n\r\n\tpackage(default_visibility = [\"//visibility:public\"])\r\n\r\n\tload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\", \"tf_custom_op_py_library\",\r\n\t     \"tf_kernel_library\", \"tf_gen_op_libs\", \"tf_gen_op_wrapper_py\")\r\n\r\n\ttf_custom_op_library(\r\n\t    name = \"python/ops/_medical_image_ops.so\",\r\n\t    srcs = [\r\n\t        \"kernels/index_ops.cc\",\r\n\t        \"ops/index_ops.cc\",\r\n\t    ],\r\n\t    deps = [\r\n\t        \"//tensorflow/core:lib\"\r\n\t    ],\r\n\t)\r\n\r\n\ttf_gen_op_libs(\r\n\t    op_lib_names = [\"index_ops\"],\r\n\t)\r\n\r\n\ttf_gen_op_wrapper_py(\r\n\t    name = \"medical_image_ops\",\r\n\t    deps = [\":index_ops_op_lib\"],\r\n\t)\r\n\r\n\ttf_custom_op_py_library(\r\n\t    name = \"medical_image_py\",\r\n\t    srcs = [\r\n\t        \"__init__.py\",\r\n\t        \"python/medical_image/medical_image.py\",\r\n\t        \"python/medical_image/transforms.py\",\r\n\t        \"python/ops/index_ops.py\",\r\n\t    ],\r\n\t    dso = [\":python/ops/_medical_image_ops.so\"],\r\n\t    srcs_version = \"PY2AND3\",\r\n\t    deps = [\r\n\t        \":medical_image_ops\",\r\n\t    ],\r\n\t)\r\n\r\n\tfilegroup(\r\n\t    name = \"all_files\",\r\n\t    srcs = glob(\r\n\t        [\"**/*\"],\r\n\t        exclude = [\r\n\t            \"**/METADATA\",\r\n\t            \"**/OWNERS\",\r\n\t        ],\r\n\t    ),\r\n\t    visibility = [\"//tensorflow:__subpackages__\"],\r\n\t)\r\n\r\n  \r\n  ", "comments": ["@rmlarsen Do you have any comments on this?", "@rmlarsen?", "Nagging Assignee @rmlarsen: It has been 165 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 15912, "title": "Eager: error when restore tfe.Network checkpoint by tfe.restore_network_checkpoint", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.6.0dev20170105\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**:pascal\r\n- **Exact command to reproduce**:N/A\r\n### Describe the problem\r\nWhen I want to restore tfe.Network by using tfe.restore_network_checkpoint from a graph-mode checkpoint(such as pretrained slim resnet ckpt, so need name map of tfe.restore_network_checkpoint), I get an error:\r\n```Python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-cbde22383c9e> in <module>()\r\n     20     images = tf.constant(toy_data)\r\n     21     logits = net(images)\r\n---> 22     tf.contrib.eager.restore_network_checkpoint(net, ckpt)\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\network.py in restore_network_checkpoint(network, save_path, map_func)\r\n    946       save_path=save_path,\r\n    947       map_func=map_func,\r\n--> 948       user_map_func=user_map_func)\r\n    949   # Step two is to set a custom getter which restores variables on creation,\r\n    950   # for those variables which have not been added to sub-Layers yet.\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\eager\\python\\network.py in _restore_existing_variables(network, save_path, map_func, user_map_func)\r\n    859       sess = ops.get_default_session()\r\n    860     saver_lib.Saver(var_list=existing_variables_by_checkpoint_name).restore(\r\n--> 861         sess=sess, save_path=save_path)\r\n    862   return existing_variables_by_checkpoint_name\r\n    863 \r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py in restore(self, sess, save_path)\r\n   1686                {self.saver_def.filename_tensor_name: save_path})\r\n   1687     else:\r\n-> 1688       self._build_eager(save_path, build_save=False, build_restore=True)\r\n   1689 \r\n   1690   @staticmethod\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py in _build_eager(self, checkpoint_path, build_save, build_restore)\r\n   1250   def _build_eager(self, checkpoint_path, build_save, build_restore):\r\n   1251     self._build(\r\n-> 1252         checkpoint_path, build_save=build_save, build_restore=build_restore)\r\n   1253 \r\n   1254   def _build(self, checkpoint_path, build_save, build_restore):\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py in _build(self, checkpoint_path, build_save, build_restore)\r\n   1282           restore_sequentially=self._restore_sequentially,\r\n   1283           filename=checkpoint_path,\r\n-> 1284           build_save=build_save, build_restore=build_restore)\r\n   1285     elif self.saver_def and self._name:\r\n   1286       # Since self._name is used as a name_scope by builder(), we are\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py in _build_internal(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\r\n    748                         [saveable.op for saveable in saveables]) as name:\r\n    749       # Add the Constant string tensor for the filename.\r\n--> 750       filename_tensor = constant_op.constant(filename or \"model\")\r\n    751 \r\n    752       # Add the save ops.\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    182   ctx = context.context()\r\n    183   if not ctx.in_graph_mode():\r\n--> 184     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    185     if shape is None:\r\n    186       return t\r\n\r\nc:\\users\\yanyan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    129     return t\r\n    130   else:\r\n--> 131     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\r\n    132 \r\n    133 \r\n\r\nRuntimeError: Error copying tensor to device: /job:localhost/replica:0/task:0/device:GPU:0. Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.\r\n```\r\ncode to reproduce this error:\r\nfirst run graph code to save a ckpt:\r\n```Python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ndef cnn(x):\r\n    with tf.variable_scope('net'):\r\n        x = tf.layers.dense(x, 10, name='fc')\r\n        return x\r\nckpt = '/tmp/graph/test.ckpt'\r\nwith tf.Graph().as_default():\r\n    images = tf.placeholder(tf.float32, [None, 784])\r\n    logits = cnn(images)\r\n    saver = tf.train.Saver(save_relative_paths=True)\r\n    print(tf.global_variables())\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.save(sess, ckpt)\r\n```\r\nThen run eager code to load ckpt, note that any ckpt path can produce same error, so I think ckpt file has no problem.\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ntfe.enable_eager_execution(config=config)\r\nclass CNN(tfe.Network):\r\n    def __init__(self, name):\r\n        super(CNN, self).__init__(name)\r\n        self.fc = self.track_layer(tf.layers.Dense(10, name='fc'))\r\n    def call(self, x):\r\n        x = tf.reshape(x, [-1, 784])\r\n        x = self.fc(x)\r\n        return x\r\ntoy_data = np.ones((100, 784)).astype(np.float32)\r\ndevice = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\r\nnet = CNN('net')\r\nckpt = '/tmp/graph/test.ckpt' # any other paths produce same error\r\nwith tf.device(device):\r\n    images = tf.constant(toy_data)\r\n    logits = net(images)\r\n    tfe.restore_network_checkpoint(net, ckpt)\r\n```\r\n  ", "comments": ["This problem has been solved by add a scope ```with tf.device('cpu:0'):``` before tfe.restore_network_checkpoint."]}, {"number": 15910, "title": "Feature Request: Sparse Cholesky decomposition", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nTensorFlow has a Cholesky decomposition [kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cholesky_op.cc) based on wrappers around the [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) and [cuSOLVER](http://docs.nvidia.com/cuda/cusolver/index.html#cusolver-intro) (for GPUs) libraries.\r\n\r\nFrom experience, a sparse solver can provide huge speedups in the right circumstances. The cuSOLVER library has the feature in their sparse LAPACK library, cuSolverSP, and the Eigen library in the SparseCholesky module.\r\n\r\nAlternatively, there is the [CHOLMOD](https://developer.nvidia.com/cholmod) library which is supported by Eigen in the CholmodSupport module. This CHOLMOD library supports both CPU and GPU sparse Cholesky factorisations.\r\n\r\nWould a Cholesky decomposition for sparse matrices be a feature of interest? \r\n  ", "comments": ["@rmlarsen can you comment or redirect? Thanks!", "Let me revive this issue (and add something to it):\r\n\r\n* At present there is a sparse Cholesky factorization hidden deep in TensorFlow. It's a little bit inconvenient, since it requires you to convert your SparseTensor to a \"CSRSparseMatrix\". It doesn't appear in the documentation.\r\n    - [source code](https://github.com/tensorflow/tensorflow/blob/9664ba19296e58f4437feab4d4b2789cc1e38fd4/tensorflow/core/kernels/sparse/sparse_cholesky_op.cc)\r\n    - [API definition](https://github.com/tensorflow/tensorflow/blob/315ee4a1b9fec30e6e65c025b5b603412baf778a/tensorflow/core/api_def/base_api/api_def_SparseMatrixSparseCholesky.pbtxt)\r\n* I would add another \"welcome contribution\": a **gradient method** for the sparse Cholesky decomposition. \r\n    - Backpropagation in this context would maintain the sparse structure. \r\n    - I.e., if A=LL^T then backprop would only propagate the error in L to the nonzero entries of the lower triangle of A.\r\n\r\nIn the source for SparseMatrixSparseCholesky I see a few contributors. @kiszk @penpornk @bloops I wonder if you have thoughts about the sparse Cholesky decomposition and its gradient? \r\n\r\nI have research interests related to this topic and would like to contribute, but I'm finding it difficult to learn TensorFlow's C++ API. I welcome any advice or suggestions.", "Sure, @dpmerrell, I can help you with the TensorFlow implementation. Do you have the algorithm ready that you want to implement?", "@rwolst,\r\n\r\nPlease check [SparseMatrixSparseCholesky](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/op/linalg/sparse/SparseMatrixSparseCholesky). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 15909, "title": "Python Configuration Error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**:  source \r\n- **TensorFlow version (use command below)**:the latest master branch\r\n- **Python version**: 3.6.3 in anaconda ,python path is :C:/Users/huo_y/Anaconda3/python.exe\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:msvc 14\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**:\r\n bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nwhen I build tensorflow with bazel on windows by msys2 shell. I got this error\r\nPython Configuration Error : --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?\r\n\r\n### Source code / logs\r\nERROR: C:/users/huo_y/tensorflow-master/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl\", line 291\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl\", line 251, in _create_local_python_repository\r\n                _check_python_bin(repository_ctx, python_bin)\r\n        File \"C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl\", line 204, in _check_python_bin\r\n                _fail((\"--define %s='%s' is not execut...)))\r\n        File \"C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl\", line 27, in _fail\r\n                fail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?\r\n and referenced by '//util/python:python_headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 16.677s\r\nFAILED: Build did NOT complete successfully (63 packages loaded)\r\n    currently loading: tensorflow/core/kernels\r\n    Fetching http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz; 32,768b 4s\r\n\r\nIt may cause by python_configure.bzl I think,But I don't know how to correct it.\r\n\r\nhere is on function about the error  in python_config.bzl \r\n```\r\ndef _check_python_bin(repository_ctx, python_bin):\r\n  \"\"\"Checks the python bin path.\"\"\"\r\n  cmd =  '[[ -x \"%s\" ]] && [[ ! -d \"%s\" ]]' % (python_bin, python_bin)\r\n  result = repository_ctx.execute([\"bash\", \"-c\", cmd])\r\n  if result.return_code == 1:\r\n    _fail(\"--define %s='%s' is not executable. Is it the python binary?\" % (\r\n        _PYTHON_BIN_PATH, python_bin))\r\n```\r\nI find that when I run configure  the python path is windows format just like C:/Users/huo_y/Anaconda3/python.exe  but in msys2 the path may show /c/Users/huo_y/Anaconda3/python.exe .  I guess that when using bash -c ,it should need the path just like /c/Users/huo_y/Anaconda3/python.exe can get the correct return code, but it seems that the python_bin parameter is the windows path format .\r\n\r\ncan anyone help check it because I don't be farmiliar with bazel\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15908, "title": "How to see the implementation of softmax_cross_entropy", "body": "In tf.nn_ops file \uff0cit imports the file : tensorflow.python.ops import gen_nn_ops\r\nbut I can't find the file gen_nn_ops in the directory", "comments": ["`tensorflow/python/ops/nn_ops.py`", "@mokii I was looking for this too. Eventually I found the answer [here](https://stackoverflow.com/questions/41147734/looking-for-source-code-of-from-gen-nn-ops-in-tensorflow)."]}, {"number": 15907, "title": "nightly installed TF is the new 1.5 TF?", "body": "today I heard that there are a new version 1.5 TF which is with good support dynamic graph.\r\n\r\nAnd I also find there is a new nightly installed method.\r\n\r\n\r\nSo this nightly installing method is install the new Version TF?\r\n\r\n\r\n```\r\nchg0901@ubuntu:~$ python3\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2018-01-06 16:49:32.242801: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library\r\n>>> print(tf.__version__)\r\n1.6.0-dev20180105\r\n>>> x = [[2.]]\r\n>>> m = tf.matmul(x,x)\r\n>>> print(m)\r\nTensor(\"MatMul:0\", shape=(1, 1), dtype=float32)\r\n>>> print(tf.Session().run(m))\r\n2018-01-06 16:51:25.418750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA\r\n[[ 4.]]\r\n\r\n```\r\n\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I am sorry for that.\r\n\r\nAnd I have got the answer from\r\n[ **Stackoverflow**](https://stackoverflow.com/questions/48125424/nightly-installed-tf-is-the-new-1-5-tf )\r\n"]}, {"number": 15906, "title": "Add additional argument to freeze_graph", "body": "This PR fixes the failed testFreezeGraphV1 test for PR https://github.com/tensorflow/tensorflow/pull/14341", "comments": ["Can one of the admins verify this patch?", "There is the write version and the read version. I think we want to support the read version but not the write version for the time being. If you update this, then you'd probably also want to write a test.", "@drpngx Do you have a specific suggestion for the author in order to make progress on this?", "Sure,\r\n1. Write a test\r\n2. Do not implement the write path. Only the read path.", "@drpngx OK, my reading comprehension is not very impressive, I suppose :-)", "Yeah probably that wasn't very clear. Thanks for asking!", "Hi @drpngx \r\nCould you help me understand \"read version\" and \"read path\"? My understanding of  write_version is from the [Saver constructor](https://github.com/tensorflow/tensorflow/blob/a1b155b6f51c539974e1bf73d0e0d15b388b9219/tensorflow/python/training/saver.py#L1171)", "@tedhtchang I'd like to make it hard for people to create `v1` checkpoints. It's OK to be able to load them, but I'd rather we didn't provide an option to write `v1` checkpoints since they are deprecated. Does that make sense?", "Hi @drpngx I don't think the change will alter the write path (i.e. let people create a v1 checkpoint). The new argument, [checkpoint_version](https://github.com/tedhtchang/tensorflow/blob/4b04b1b89d8571ee36c5cae43079f5dc9e5367b0/tensorflow/python/tools/freeze_graph.py#L254), I introduce is to explicitly tell the [saver.restore](https://github.com/tedhtchang/tensorflow/blob/4b04b1b89d8571ee36c5cae43079f5dc9e5367b0/tensorflow/python/tools/freeze_graph.py#L131) to read the input_checkpoint as v1 format.\r\nThe only write path I see is the [output_graph_def](https://github.com/tedhtchang/tensorflow/blob/4b04b1b89d8571ee36c5cae43079f5dc9e5367b0/tensorflow/python/tools/freeze_graph.py#L160) but that doesn't create checkpoint.", "@drpngx is this OK to merge, given @tedhtchang's explanation?", "Oh OK, got it. You're right. Good to go!", "@drpngx @tedhtchang Thanks!", "Now `main` will raise a\r\n`NameError: name 'checkpoint_version' is not defined`\r\non line [254](https://github.com/tensorflow/tensorflow/blob/73cf824a24e46766a1674c7879d8c48bd0728083/tensorflow/python/tools/freeze_graph.py#L254). Or am I doing something wrong?\r\n@drpngx @tedhtchang "]}, {"number": 15905, "title": "Documentation does not explain the utility of -1 as value for the axis parameter of the tf.concat method", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X, version 10.13.2\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126\r\n- **Python version**: Python 3.5.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:  N/A\r\n- **CUDA/cuDNN version**:  N/A\r\n- **GPU model and memory**:  N/A\r\n- **Exact command to reproduce**: Just run a Python script with the code I am sharing with you\r\n\r\n### Describe the problem\r\n\r\nIt apparently concatenates along the last axis. See the following example:  \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nt1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]\r\nt2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]\r\n\r\nwith tf.Session() as sess:\r\n    result = sess.run(tf.concat([t1, t2], -1))\r\n    print(result)\r\n```\r\n\r\nwhich produces\r\n\r\n```\r\n[[[ 1  2  7  4]\r\n  [ 2  3  8  4]]\r\n\r\n [[ 4  4  2 10]\r\n  [ 5  3 15 11]]]\r\n``` \r\n\r\nThe following documention does not seem to explain this use case:\r\n\r\n- https://www.tensorflow.org/api_docs/python/tf/concat\r\n- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/concat\r\n  \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Ok, updated the text of the issue. Anyway, you should really revise the documentation before putting a new version out of TF. And you should constantly try to improve the documentation, which, IMHO, is often unclear, especially for newbies.\r\n  ", "Added PR #15917 to address the documentation.", "@yongtang IMHO, you can give a clearer explanation than the one you added in the commit of the PR.\r\n\r\nWhat do you mean by \"it is counted backward from the end\"? \"it\" may be ambiguous. Are you referring to what? Give one or two examples (like the one in my explanation above), that's what usually clarifies all doubts!!\r\n  \r\n  ", "@nbro The PR has been updated."]}, {"number": 15904, "title": "\u3010TensorFlow Servering\u3011 bazel build tensorflow_serving/... ImportError: No module named numpy", "body": "fong@ubuntu:~/serving$ bazel build tensorflow_serving/...\r\n..........\r\nDEBUG: /home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow_serving/model_servers:tensorflow_model_server_tar: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\n\r\nERROR: /home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl\", line 291\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl\", line 255, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl\", line 239, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl\", line 54, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\n and referenced by '@org_tensorflow//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow_serving/example:inception_saved_model' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 35.784s\r\nFAILED: Build did NOT complete successfully (149 packages loaded)\r\n    currently loading: @org_tensorflow//tensorflow/contrib/rnn ... (6 packages\\\r\n)\r\n\r\n\r\nThis is the error\uff0cbut  my python paths and python library path is true,python library path have numpy already.\r\n\r\n\r\nfong@ubuntu:~/serving/tensorflow$ ./configure \r\nYou have bazel 0.9.0 installed.\r\nPlease specify the location of python. [Default is /home/fong/anaconda3/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/fong/anaconda3/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/fong/anaconda3/lib/python3.5/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: y\r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: y\r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: y\r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: y\r\nGDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: y\r\nVERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nConfiguration finished", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15903, "title": "Does Staged Variables only support parameter_server mode\uff1f", "body": "Does Staged Variables only support parameter_server mode\uff1fWhen I used it in my multi-gpu like replicated mode\uff0cit works and trainning faster than no Staged Variables\uff1fCan I use it to speedup my trainning\uff1f", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 15902, "title": " W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745519: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745553: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["An ideal title, IMHO, shouldn't be too long, should be readable and as much as possible descriptive of the actual problem! Please, change the title of the issue to one or two lines with readable text which, in your opinion, roughly describes your problem.\r\n  ", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 15901, "title": "Branch 180993147", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins \r\n\"test this please\"", "All tests pass except interleave_op_test, which timesout (same as seen in previous CLs)", "@tensorflow-jenkins test this please"]}, {"number": 15900, "title": "Feature Request: MonitoredTrainingSession should accept checkpoint steps", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **TensorFlow installed from (source or binary)**: n/a\r\n- **TensorFlow version (use command below)**: n/a\r\n- **Python version**: n/a\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:n/a\r\n- **CUDA/cuDNN version**:n/a\r\n- **GPU model and memory**:n/a\r\n- **Exact command to reproduce**:n/a\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nAs of 1.4, MonitoredTrainingSession conveniently offers constructor parameters for checkpoint_secs, summary_secs, and summary_steps. It does not offer a parameter for checkpoint_steps, so if I want to checkpoint by steps and not seconds, I have to register a custom CheckpointSaverHook. It would be nice if MonitoredTrainingSession encapsulated that, as it does for the similar summary_secs, summary_steps, and checkpoint_secs.\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/cddf8d82dec9fff526f5c064add725b7f35f95fa/tensorflow/python/training/monitored_session.py#L272\r\n", "comments": ["@ispirmustafa FYI", "I agree! here's a PR:\r\nhttps://github.com/tensorflow/tensorflow/pull/16154"]}, {"number": 15899, "title": "Addresses S3 timeout configurability discussed in #15868", "body": "This provides the ability to specify S3 timeouts via environment variables, as requested in #15868.\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please\r\n@jhseu Ready to take another look?", "@jhseu What does it mean that the Ubuntu CC is waiting for status to be reported? I'm blocked by this PR currently.", "Will this be merged into release branches, or will this just be taken up in new releases? ", "I believe only for a large security concern would there be back-porting of introduced code - so this functionality will likely exist, release-wise, only starting with the next release.\r\n"]}, {"number": 15898, "title": "DO NOT MERGE YET: Cuda config", "body": "", "comments": []}, {"number": 15897, "title": "Tensor Core support for NVIDIA Volta architecture", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:\r\n5386775e64aac0bb5020974122645da900bc312a\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:0.8.1\r\n- **GCC/Compiler version (if comp6iling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:9.1 / 7.0.5\r\n- **GPU model and memory**: Titan V\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nIt is widely reported that using float16 on Nvidia Volta architecture comes only with x2 improvement instead of the expected x4 x8 improvement using Tensor Cores\r\nhttps://github.com/tensorflow/benchmarks/issues/77\r\nhttps://devblogs.nvidia.com/parallelforall/programming-tensor-cores-cuda-9/\r\n\r\nI checked that Tensorflow master branch used \r\ncudnnGetConvolutionForwardAlgorithm\r\nto get the best possible algorithm for the given GPU.\r\nHowever I think either\r\ncudnnGetConvolutionForwardAlgorithm_v7\r\nor \r\ncudnnFindConvolutionForwardAlgorithmEx\r\nshould be used to fully utilize the Volta architecture.\r\nCould you please check this issue with a Volta architecture GPU?\r\n### Source code / logs\r\n", "comments": ["@reedwm do you have any comments on this?", "@yzhwang can you look into using cudnnGetConvolutionForwardAlgorithm_v7 or cudnnFindConvolutionForwardAlgorithmEx, and see why we don't get a 4x performance improvement on ResNet 50?\r\n\r\nAlso /CC @nluehr, can you comment?", "@zheng-xq @poxvoculi is this request easy or near what you're doing already? \r\n  \r\n[edit] Sorry, I made this request before seeing @reedwm 's updates. Feel free to ignore.", "The TensorFlow head is already including TensorCore support on Volta. ", "Hi @egborbe , as @zheng-xq said, we have already included TensorCore support on Volta in TensorFlow. Here I will explain more the difference between all the cudnn APIs to find/get internal convolution algorithms:\r\n - Any algorithm starts with cudnnGet will use heuristics given a specific input shape as well as the scratch memory limitation to provide an internal algorithm that performs the best. This is light-weighted but not accurate. E.g. We might ended up with an internal algorithm that doesn't perform the best.\r\n - Any algorithm starts with cudnnFind will actually run all the available internal algorithms and returns a list of algorithm names as well as their profiling results, sorted in terms of performance. This is accurate, but first of all, it is as slow as writing our own autotune framework, second, the metrics it uses is not flexible and behind the binary, so users cannot control it.\r\n\r\nFor the above reason, in TensorFlow, we use our own autotune framework, if the autotune is turned off, we use cudnnGet to find a heuristic:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2303\r\nIf at cuda_dnn.cc level, an algorithm is set, that means one of the two things:\r\n 1) We are running autotune and we need to profile this algorithm;\r\n 2) We know this algorithm performs the best.\r\n\r\nSo current autotune framework has already considered using tensor op:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2370\r\nIt will automatically being used with cudnn version >=7000. To disable it, user can set the envvar:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L559\r\n\r\nThe benefit of using our own autotune system is that we can setup arbitrary metrics, like the one we have right now (you can find details in the comments):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/gpu_utils.h#L40\r\n\r\nI hope this answers your question and I think we can close this issue.", "Hi ,\r\n\r\nthank you for your attention.\r\nI now understand the way TF handles Cuda cores\r\nI did some further measurements and it turned out that NCHW - NHWC conversion is now consuming as much time as the convolution itself on Volta.\r\nI tried with NCHW and the performance gain against Pascal GPU shot up immediatelly to 250% from 200%. However this is still far from what Nvidia reported with NVCaffe (370% improvement in inference).\r\nEVen when I use NCHW order for convulotion I realized there were nchw kernel calls as seen from nvprof.\r\nSo I think there is still room for improvement.\r\nThanks and regards,\r\nGabor Bereczki", "Correction\r\nEven when I use NCHW order for convolution I realized there were nhwc kernel calls as seen from nvprof.", "this is the nvprof output\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   34.28%  797.43ms       301  2.6493ms  2.6269ms  2.6636ms  volta_fp16_s884cudnn_fp16_256x64_ldg8_relu_f2f_exp_small_nhwc2nchw_tn_v1\r\n                   30.73%  714.84ms       301  2.3749ms  2.3708ms  2.3818ms  volta_fp16_s884cudnn_fp16_128x128_ldg8_relu_f2f_exp_small_nhwc2nchw_tn_v1\r\n                   21.63%  503.23ms      1204  417.96us  3.0400us  1.2488ms  void nchwToNhwcKernel<__half, __half, float, bool=1>(int, int, int, int, __half const *, __half*, float, float)\r\n \r\n\r\nand this is the code I used\r\nimport tensorflow as tf\r\nimport time\r\n\r\ndf='NCHW'\r\ndef net(dt, filt):\r\n    a=tf.random_uniform((16,1,256,256), dtype=dt)\r\n    f=tf.random_uniform((filt,filt,1,64), dtype=dt)\r\n    a=tf.nn.conv2d(a,f,[1,1,1,1],'SAME',data_format=df)\r\n    f=tf.random_uniform((filt,filt,64,128), dtype=dt)\r\n    f2=tf.random_uniform((filt,filt,128,64), dtype=dt)\r\n    for i in range(100):\r\n        a=tf.nn.conv2d(a,f,[1,1,1,1],padding='SAME',data_format=df)\r\n        a=tf.nn.conv2d(a,f2,[1,1,1,1],padding='SAME',data_format=df)\r\n    a=tf.reduce_sum(a)\r\n    return a\r\nf16=net(tf.float16,3)\r\n\r\ns=tf.Session()\r\ns.run(tf.global_variables_initializer())\r\n#input(\"press key\")\r\nfor i in range(3):\r\n    s1=time.time()\r\n    x=s.run(f16)\r\n    e1=time.time()\r\n    print(\"FP16\",(e1-s1)*1000)\r\n", "Thanks for the investigation. I will do some benchmark and give updates.", "Volta GPU actually has a format conversion by itself. It's from Cudnn, not TensorFlow. For certain shapes, it is actually faster to be in NHWC. ", "Currently TensorFlow assumes NCHW as the better data format to present tensors for convolution on the GPU. For Volta and later generation of GPUs, this may not be true any more. From my benchmark running on Volta machine with cudnn7, NHWC's performance is only shown to be better than NCHW for float32 with SAME padding, and the gap gets larger when number of channel gets larger.\r\nWe will work on adding autotune feature to choose between the better data format when doing convolution.", "Thanks @zheng-xq @egborbe @yzhwang for the helpful info!\r\n\r\nAre Tensor cores are on by default using Tensorflow 1.7 or later? I run resnet50 with float16 and synthetic data (channels_first) on one node of v100 but only get 637 examples/sec, which is only half of what is reported [here](https://devblogs.nvidia.com/tensor-core-ai-performance-milestones/?linkId=100000002357715). In [this](https://github.com/tensorflow/benchmarks/issues/77) issue 675 examples/sec is reported. I wonder why people can get over 1k and whether 637 can be regarded as a reasonable number."]}, {"number": 15896, "title": "Add EvalResultsExporter for writing the results of evaluation to a file", "body": "Previously you could capture the return value of `estimator.train_and_evaluate`,\r\nwhich was a dictionary of the final evaluation results (like accuracy, AUC).\r\n\r\nSince `estimator.train_and_evaluate` no longer returns a value, it looks like\r\nthere's no way to get the `eval_results` dictionary directly. This is the\r\nsolution I wrote for getting and storing that dictionary. Since the solution\r\nis generalizable, I thought it'd be useful to include it for others to use.", "comments": ["Can one of the admins verify this patch?", "train_and_evaluate can return something for local mode. But that behavior does not make sense during distributed mode. This is why it does not return anything.\r\n\r\nJeff, may i ask some information. What's your major use case? local or distributed? And how do you deal with the results saved in json later?\r\n\r\nmartinwicke@, I think several options could be chosen here\r\n1. returns the eval dict for local mode. But this is very confusing behavior for distributed. \r\n2. adds a tooling to grab the results. e.g., this pull request or reading events file directly. \r\n\r\nIf we focus on local mode, option 1 is the easiest solution as it is convenient. ", "Thanks for the review -\r\n\r\n> train_and_evaluate can return something for local mode\r\n\r\nI missed how to accomplish that - it looks like [the function itself has no return value](https://github.com/tensorflow/tensorflow/blob/398699286064bc0821056209e1c62065f0e00f82/tensorflow/python/estimator/training.py#L256)?\r\n\r\nOur use case is:\r\n- We kick off a (local) training job on ML Engine every 24h\r\n- We want to write the evaluation results somewhere\r\n- We have a script that compares the evaluation results between models\r\n- We can use this script manually to verify a trained model is ready for production, and we plan on in the future using this script to automatically promote models as long as their evaluation results are good", "@xiejw can you respond to @jeffcarp's question?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is a fine Exporter. Note that you also get the evaluation output in the event files used by TensorBoard (as a protobuf, not as JSON, but otherwise the same). \r\n\r\nI don't think we would want to maintain this as part of TensorFlow though. ", "@martinwicke got it, thanks for the review!", "I am using the API named 'tf.estimator.train_and_evaluate' for distributed training, but i can't see any evaluation results (like accuracy, AUC) in the worker's LOG.\r\n\r\nAfter reading this pull request Q&A, maybe there are two way to print evaluation results:\r\n1. I have tried Jeff's Exporter, but there's also no logs of export_path or eval_results, i'm confusing for where to get the results. Can you help me? the paper i referenced is https://jeff.is/blog/exporting-eval-results-in-tensorflow.html  @jeffcarp \r\n\r\n2. How can i print the event files used by TensorBoard (as a protobuf, not as JSON, but otherwise the same) in the worker's log? is there any demo for me? Thank you very much! @martinwicke ", "@mufusu - @xiejw mentioned that the behavior is different in distributed mode, perhaps those arguments aren't the same in that case. I haven't tested that exporter in distributed mode. Glancing at the [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/training.py#L910) I can't see where exactly it differs. You could try running your trainer in local mode to see if it works there?"]}]