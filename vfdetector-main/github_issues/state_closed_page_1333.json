[{"number": 13108, "title": "Fix test breakage on Mac Python3 due to assert_called()", "body": "Replace assert_called() with self.assertTrue and called.", "comments": ["Testing mac python3 build with experimental build at: http://ci.tensorflow.org/view/Experimental/job/experimental-cais-master-mac/14/console", "Fixes ongoing nightly build breakages such as http://ci.tensorflow.org/view/Tensorflow%20Jenkins%20Monitored%20builds/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/635/console", "Jenkins, test this please.", "Thanks for the fix. But two questions here\r\n1) Why assert_called is not allowed on Mac Python3?\r\n2) The change passed all internal tests. Do we have a better way to find this broken issue earlier?\r\nThanks"]}, {"number": 13107, "title": "Add new op BytesInUse, similar to MaxBytesInUse", "body": "Adding BytesInUse\r\nThis is more useful than MaxBytesInUse for getting peak memory for a given session.run call because the latter gives maximum memory usage over lifetime of allocator, which can span multiple session.run calls/multiple session objects", "comments": ["@yaroslavvb, thanks for your PR! By analyzing the history of the files in this pull request, we identified @wujingyue, @lukeiwanski and @tensorflower-gardener to be potential reviewers.", "Can one of the admins verify this patch?", "Note that bytes_in_use is also what's used by timeline for memory tracking -- https://github.com/tensorflow/tensorflow/commit/7b5f590224a99d58cb9bb3e01b9189a3cb29715e", "OK, I guess for adding new test I'll actually need to setup TensorFlow development env :)  I'll ping this thread after updating", "added test to make sure intermediate memory usage gets measured correctly, ptal", "Jenkins, test this please.", "Jenkins, test this please.", "MacOS failures unrelated to this cl (GCS cloud tests)", "That is a known flaky test so re-running the tests.\r\n\r\nJenkins, test this please."]}, {"number": 13106, "title": "no such package '@llvm//'", "body": "Trying to build latest git tree (`git describe --tags` calls it `v1.3.0-rc1-2262-g74cfc64734`):\r\n\r\n>ERROR: /build/tensorflow-git/src/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: no such package '@llvm//': java.io.IOException: Error downloading [http://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz, https://github.com/llvm-mirror/llvm/archive/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz] to /build/.cache/bazel/_bazel_builduser/a152fcd393afbe6f0b02d283bc9e6174/external/llvm/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz: Checksum was e8f07137a3a0b95e143c0665cd19160dd5040114b34a48653fa7f5f91cf4c136 but wanted 2a6d4c23f6660d9130d8d5f16267db53a87f8d0104f9618b558c033570f110af and referenced by '//tensorflow/tools/pip_package:licenses'.\r\n\r\nSurely the correct fix is *not* to just to change the expected checksum to match the observed one?\r\n", "comments": ["Attempted the build a second time and it worked. I'm puzzled as to why this failed in the first place, because as far as I can tell, nothing changed. Both build attempts were done in fresh chroots, and the git repo didn't change either as far as I saw.", "This is usually due to GitHub's hash change, and you not downloading from mirror.bazel.build successfully (therefore falling back to GitHub and picking up the changed hash):\r\n\r\n```\r\n$ wget -q -O - http://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz | sha256sum\r\n2a6d4c23f6660d9130d8d5f16267db53a87f8d0104f9618b558c033570f110af  -\r\n$ wget -q -O - http://github.com/llvm-mirror/llvm/archive/9aafb854cc7cb8df8338c50cb411a54ce1e09796.tar.gz | sha256sum\r\ne8f07137a3a0b95e143c0665cd19160dd5040114b34a48653fa7f5f91cf4c136  -\r\n```\r\n\r\nSee also #12986, #13080 (and unreported, LLVM itself in prior versions like the release 1.3.0 where mirror.bazel.build fails with HTTP 403).", "Hm, it's frustrating that the hash on the GitHub copy can change.\r\n\r\nWhat kind of retry logic is there around trying to fetch from mirror.bazel.build, in the case of transient errors? If we can't rely on the github copy's hash to validate then we should try really hard not to get that one, I would think...", "@gunan, I assume this is a duplicate of #12979.", "Looks like, @aselle. Closing in favor of that one."]}, {"number": 13105, "title": "Add implementation of `conv1d_transpose`", "body": "This fix tries to address the issue raised in #8729 by providing the implementation of `conv1d_transpose`.\r\n\r\nThis fix fixes #8729.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@yongtang can you look at the test failure", "Thanks @sb2nov. The failure was caused by `xrange` in python 3. The failure has been fixed and the PR has been updated now. Please take a look and let me know if there are any other issues.", "Jenkins, test this please.", "From the Jenkins output the Python 3 build has been fixed. The \u201cLinux CPU Test Makefile\u201d might be caused by the network issue. I think a rerun will fix it.", "Jenkins, test this please.", "Thanks @alextp for the review and sorry for the late reply as I missed the GitHub previously notification. Indeed `NHWC` and `NCHW` is not appropriate for 1d situations. I have rebased and updated the PR so that `NWC` and `NCW` are used instead.\r\n\r\nPlease take a look and let me know if there are any issues.", "Thanks @alextp for the help.\r\n\r\nNot sure why `fast_tensor_util.cpp` was included in the PR as the filename is actually in `.gitignore`. I have removed the file from PR manually.\r\n\r\nThe conv1d_transpose has also been imported in `tf.contrib.nn` and exposed. Please take a look.", "Jenkins, test this please.", "Jenkins, test this please.", "Failures are unrelated."]}, {"number": 13104, "title": "Fix tf.argmax/argmin documentation", "body": "https://www.tensorflow.org/api_docs/python/tf/argmax\r\n\r\n- The documentation does not explain what an \"index\" is. I can imagine a billion definitions of \"index\". Is it the same as the \"index\" in tf.one_hot?\r\n- The documentation does not explain what happens if axis is set to None.\r\n- \"For vectors, use axis = 0.\": Why?\r\n- The documentation should clearly explain how the rank+shape of the returned tensor correlates with the rank+shape of the input tensor.\r\n- The documentation incorrectly indicates that there is a guide if you click \"See the guide\", but it takes you to a function reference/index. A redundant 3 sentence description is not a \"guide\".\r\n\r\n", "comments": ["@wolffg, could we queue this in our next doc fixit?\r\n\r\n@hnsl, if you have time, a PR would be most welcome to fix some of these things.\r\n", "I would love to do a PR to fix the documentation, but the truth is that I can't write it because I don't know the answer to those questions. That's why I wanted it to be documented in the first place. I've tried to reverse-engineer argmax to understand what it does in higher dimensions, but I can't say that got me anywhere.", "@wolffg  Hi, could you please look into this issue ?", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this has been resolved"]}, {"number": 13103, "title": "Dropout hidden-to-hidden transition within an RNN", "body": "`DropoutWrapper` allows to apply dropout to either the cell's inputs, outputs or states. However, I haven't seen an option to do the same thing for the recurrent weights of the cell (for example, 4 out of the 8 different matrices used in the original LSTM formulation). I specifically refer to the hidden-to-hidden transition within an RNN. Take as an example Section 2 from https://arxiv.org/abs/1708.02182", "comments": ["@ebrevdo, can you take a look at this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I believe we support this now.", "@ebrevdo Can you please comment which Tensorflow function supports this ?", "The DropoutWrapper should support intelligent hidden-to-hidden dropout.\n\nOn Fri, Jun 29, 2018, 11:17 PM vivekverma239 <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Can you please comment which\n> Tensorflow function supports this ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13103#issuecomment-401520651>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7BUIEyduO1K6q6D_c2vkZOTuqKbks5uBxfcgaJpZM4PaQ-T>\n> .\n>\n", "@ebrevdo Can DropoutWrapper achieve DropConnect?\r\nDropConnect is refered as  : http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf"]}, {"number": 13102, "title": "Trouble installing as well as uninstalling Tensorflow", "body": "So as you can see here we have python and anaconda\r\n\r\n```\r\nPython 3.6.1 |Anaconda 4.4.0 (x86_64)| (default, May 11 2017, 13:04:09) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\nThen installed Tensorflow using pip3 and tfBinaryURL eventually testing the installation-\r\n\r\n```\r\n>>> #python\r\n... import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n>>> \r\n```\r\nSo I tried uninstalling Tensorflow using pip uninstall tensorflow but again after the y/n prompt (typed \"y\")\r\n\r\n ```\r\n Installing collected packages: tensorflow\r\n  Found existing installation: tensorflow 1.3.0\r\n  Uninstalling tensorflow-1.3.0:\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/shutil.py\", line 544, in move\r\n    os.rename(src, real_dst)\r\nPermissionError: [Errno 13] Permission denied: '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/external/__pycache__/__init__.cpython-36.pyc' -> '/var/folders/61/1b15w21n0jbc2sndhd3j7xfr0000gn/T/pip-oepk4q_b-uninstall/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/external/__pycache__/__init__.cpython-36.pyc'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_set.py\", line 778, in install\r\n    requirement.uninstall(auto_confirm=True)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_install.py\", line 754, in uninstall\r\n    paths_to_remove.remove(auto_confirm)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/req/req_uninstall.py\", line 115, in remove\r\n    renames(path, new_path)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip/utils/__init__.py\", line 267, in renames\r\n    shutil.move(old, new)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/shutil.py\", line 559, in move\r\n    os.unlink(src)\r\nPermissionError: [Errno 13] Permission denied: '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/external/__pycache__/__init__.cpython-36.pyc'\r\nthis happens.\r\n```\r\n\r\nIf it's pre-installed as it says ideally the program #1 should work - but it doesn't.\r\n\r\nCan you help, please?", "comments": ["The tensorflow is not compatible  python version 3.6 , try installing it with version 3.5\r\nCreate the environment for tensorflow using anaconda using\r\nconda create -n tensorflow python=3.5 anaconda\r\n\r\nTry importing tensorflow after activating anaconda.\r\nHope it helps.", "This worked (just in case somebody comes from a Google search)\r\n \r\n`conda install -c conda-forge tensorflow `\r\n\r\nCloses #13102 "]}, {"number": 13101, "title": "from_generator feedback and questions", "body": "@mrry, thank you for implementing the `from_generator` method in `tf.data`. I just wanted to provide some feedback and ask a few more questions.\r\n\r\n# Interface\r\n\r\nIn addition to having `generator` be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable? E.g. instead of \r\n\r\n```python\r\npool = multiprocessing.Pool()\r\ndataset = tf.contrib.data.Dataset.from_generator(\r\n    lambda: pool.imap(some_function, some_data), dtypes, shapes\r\n)\r\n```\r\n\r\nalso support\r\n\r\n```python\r\npool = multiprocessing.Pool()\r\ndataset = tf.contrib.data.Dataset.from_generator(\r\n    pool.imap(some_function, some_data), dtypes, shapes\r\n)\r\n```\r\n\r\n# Return types\r\n\r\n`from_generator` does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have. E.g. this fails\r\n\r\n```python\r\ndef generator():\r\n    while True:\r\n        yield np.zeros(2, np.float32)\r\n        \r\ndataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\r\nx, y = dataset.make_one_shot_iterator().get_next()\r\nsession = tf.Session()\r\nsession.run([x, y])\r\n```\r\n\r\nbut this runs smoothly\r\n\r\n```python\r\ndef generator():\r\n    while True:\r\n        yield tuple(np.zeros(2, np.float32))\r\n        \r\ndataset = tf.contrib.data.Dataset.from_generator(generator, (tf.float32, tf.float32))\r\nx, y = dataset.make_one_shot_iterator().get_next()\r\nsession = tf.Session()\r\nsession.run([x, y])\r\n```\r\n\r\n# Performance\r\n\r\nI've played around with a *very* naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using `feed_dicts`. The setup is as follows\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import data as tfdata\r\nimport numpy as np\r\nfrom time import time\r\n\r\nnum_batches = 1000\r\nbatch_size = 100\r\n\r\nclass Generator:\r\n    def __init__(self):\r\n        self.times = []\r\n    \r\n    def __iter__(self):\r\n        while True:\r\n            x = np.random.normal()\r\n            y = 3 + 5 * x\r\n            x, y = np.asarray([x, y], np.float32)\r\n            self.times.append(time())\r\n            yield x, y\r\n\r\ngenerator_state1 = Generator()\r\n\r\ndataset = tfdata.Dataset.from_generator(\r\n    lambda: generator_state1, \r\n    (tf.float32, tf.float32),\r\n    (tf.TensorShape([]), tf.TensorShape([]))\r\n)\r\nprefetched = dataset.prefetch(3 * batch_size)\r\nbatches = prefetched.batch(batch_size)\r\niterator = batches.make_one_shot_iterator()\r\n\r\nx, y = iterator.get_next()\r\n\r\nw = tf.Variable([0, 0], dtype=tf.float32)\r\nprediction = w[0] + w[1] * x\r\nloss = tf.losses.mean_squared_error(y, prediction)\r\noptimizer = tf.train.AdamOptimizer(0.1)\r\ntrain_op = optimizer.minimize(loss)\r\ninit_op = tf.global_variables_initializer()\r\n\r\nsession = tf.Session()\r\nsession.run(init_op)\r\n```\r\n\r\nRunning the optimisation gives me\r\n\r\n```python\r\nlosses = []\r\n\r\nstart = time()\r\nfor _ in range(num_batches):\r\n    _, _loss = session.run([train_op, loss])\r\n    losses.append(_loss)\r\ntime() - start  # about seven seconds\r\n```\r\n\r\nDoing the same using feed_dicts gives\r\n\r\n```python\r\nlosses = []\r\n\r\ngenerator_state2 = Generator()\r\niterator = iter(generator_state2)\r\n\r\nstart = time()\r\nfor _ in range(num_batches):\r\n    _x, _y = np.transpose([next(iterator) for _ in range(batch_size)])\r\n    _, _loss = session.run([train_op, loss], {x: _x, y: _y})\r\n    losses.append(_loss)\r\ntime() - start  # about one second\r\n```\r\n\r\nIt seems that the dataset created using the `from_generator` method isn't fetching from the generator fast enough:\r\n\r\n```python\r\nnp.mean(np.diff(generator_state1.times))  # 7.1533812683949508e-05\r\nnp.mean(np.diff(generator_state2.times))  # 1.0633696558370612e-05\r\n```\r\n\r\nThanks again for this, and looking forward to hearing your thoughts.", "comments": ["My 2 cents of feedback: I have been trying to use the `Dataset.from_generator` API (thanks) to return an iterator to another dataset, constructed in the generator - but I can't really make it work without passing the session in:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nrange5 = tf.contrib.data.Dataset.range(5).make_one_shot_iterator()\r\nwith tf.Session() as sess:\r\n    print(tf.__version__)\r\n    def _dataset_generator():\r\n        while True:\r\n            try:\r\n                yield sess.run(range5.get_next())\r\n            except tf.errors.OutOfRangeError:\r\n                return\r\n    gen_dataset = tf.contrib.data.Dataset.from_generator(_dataset_generator,\r\n                                                         tf.int64)\r\n    gen_it = gen_dataset.make_one_shot_iterator()\r\n    while True:\r\n        try:\r\n            print(sess.run(gen_it.get_next()))\r\n        except tf.errors.OutOfRangeError:\r\n            break\r\n```\r\n\r\nI may be missing something obvious but there seems to be no way to \"chain\" the iterators - so just issue `yield range5.get_next()` . Moreover the pattern above does not (easily ?) generalize to more complex datasets (**EDIT 2017.10.06:** see [SO question](https://stackoverflow.com/q/46604371/281545) for an example of what I mean with \"complex\") . Implementing the generator as a python generator over a typical Dataset needs not use the session (but we still must explicitly loop in python)\r\nAnd a note on errors - I erroneously wrote `Dataset.zip((Dataset.from_tensors(...), tf.range(count))` and I got the rather cryptic `errors_impl.UnknownError: AttributeError: 'Tensor' object has no attribute 'output_types'`. It seemed rather natural to pass a tensor as argument - I guess many will be tripped by this. Would help to have some type checks in zip for a clearer error message maybe?\r\nThanks!", "Hi folks, thanks for the feedback. I started with a pretty conservative interface and implementation for `Dataset.from_generator()`, so we should be able to evolve it towards a better state. Responding to @tillahoffmann's and @Utumno's  individual points:\r\n\r\n> In addition to having generator be a callable that returns an iterator, would it be possible to support iterators that aren't wrapped in a callable?\r\n\r\nYes, we could do this, but I wasn't able to figure out a sensible set of type checks to make this work smoothly. There are also some annoying wrinkles: e.g. a generator expression is a Python iterator so you can only iterate over it once, unless you wrap it in a callable. Of course, this only matters if you try to `.repeat()` the resulting dataset! I think the workaround (explicit wrapping) is better than the confusion that could arise here, but I'm open to better suggestions... perhaps these would involve better static information about whether it's possible to repeat a dataset?\r\n\r\n> `from_generator` does not seem to support unpacking numpy arrays at the moment. I don't think it's essential but would be a nice-to-have.\r\n\r\nI suppose we could extend the `nest` library so that it would delve into NumPy arrays [when it does `nest.flatten_up_to(output_types, values)`](http://google3/third_party/tensorflow/python/data/ops/dataset_ops.py?l=309&rcl=170909071) on the elements yielded from the generator. It doesn't seem incompatible with how we already use it, but I'm not sure what it would take. Let's call that \"contributions welcome\". :)\r\n\r\n> I've played around with a *very* naive example using the generator API (in the hope to eventually leverage ipyparallel or some other distributed computing framework). Unfortunately, I can't achieve the same performance that I would get using `feed_dicts`.\r\n\r\nThis is a bit surprising, and we'll need to do some more investigation. It's possible that there's some overhead from acquiring and releasing the GIL in `tf.py_func()` and I don't think it's been heavily optimized. One quick question though: is the steady-state performance 7x worse, or does the difference in mean shrink if you exclude the first step when the iterator is constructed? (That's not an excuse of course, but it would help to focus our efforts!) \r\n\r\n> I have been trying to use the Dataset.from_generator API (thanks) to return an iterator to another dataset, constructed in the generator - but I can't really make it work without passing the session in[.]\r\n\r\nThis is not really the expected use for `Dataset.from_generator()` and as [we discussed on Stack Overflow](https://stackoverflow.com/a/46557087/3574081), you can use `Dataset.map()` if you need to capture a different iterator. However, I'm not quite sure what the use case is here... why don't you want to use the original iterator directly?\r\n\r\n> I erroneously wrote `Dataset.zip((Dataset.from_tensors(...), tf.range(count))` and I got the rather cryptic `errors_impl.UnknownError: AttributeError: 'Tensor' object has no attribute 'output_types'`.\r\n\r\nI coincidentally fixed this in 3c00952c6680d77ee2f10def35fbc7cbd138aea3.\r\n\r\nThanks again for using the new API, and let me know if you have more comments or thoughts on these answers!", "> This is not really the expected use for Dataset.from_generator() and as we discussed on Stack Overflow, you can use Dataset.map() if you need to capture a different iterator. However, I'm not quite sure what the use case is here... why don't you want to use the original iterator directly?\r\n\r\nUse case: I have a dataset (d1, lengths) of variable length tensors (read from tfrecord files) and I want to produce from it another dataset d2 with _slices_ of the tensors in d1.\r\n\r\n> I coincidentally fixed this in 3c00952.\r\n\r\nI was bitten by the list vs tuple in Dataset.zip and took some to figure that out too - good job!", "@Utumno Would `Dataset.flat_map()` work for this? For example, if you have a dataset of variable-length tensors called `var_len_dataset`, you can get all the slices as follows:\r\n\r\n```python\r\nvar_len_dataset = ...  # Element shape (?, x, y, z)\r\nall_slices = var_len_dataset.flat_map(\r\n    lambda x: tf.data.Dataset.from_tensor_slices(x))  # Element shape (x, y, z)\r\n``` ", "I have the zip(d1, num_slices) dataset and I want to produce another dataset that slices each tensor in d1 into num_slices slices (segments of  equal length). I posted a [SO question](https://stackoverflow.com/q/46604371/281545) - I know the code there is not correct (I should be using the dummy dataset) but still I am not sure the approach is at all possible.", "I posted [an answer](https://stackoverflow.com/a/46608404/3574081): `Dataset.flat_map()` should work for your case.", "@mrry, thanks for the feedback.\r\n\r\n> There are also some annoying wrinkles: e.g. a generator expression is a Python iterator so you can only iterate over it once, unless you wrap it in a callable.\r\n\r\nYes, very good point.\r\n\r\nRegarding the performance issues: https://gist.github.com/tillahoffmann/53f902212c1c511e6c3d45abb5f37b9b", "@mrry, do you have any more thoughts on the speed differences with `feed_dict`s, queues and the dataset API?", "I haven't had time to dig into this in depth, but I did notice that when I dial the batch size down to 1, the `Dataset.from_generator()` version is faster than using a `feed_dict`. This suggests that the overhead might be coming from the `Dataset.batch()` transformation. I'm surprised that batching a `std::vector<Tensor>` (even by a pretty naive approach) would be slower than stacking together a batch of ndarrays (which implicitly happens when you feed the list), but perhaps there's something to be learned from the NumPy implementation?\r\n\r\n/cc @jsimsa who has looked at batch performance in other contexts. ", "@tillahoffmann give [map_and_batch](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/batching.py), providing identity for the map, a try. `Dataset.batch()` is serializing memory copies during batch creation, which can affect performance for large batches. `Dataset.map_and_batch()` is doing the memory copies in parallel.", "@mrry, @jsimsa, thank you for the updates.\r\n\r\nI have changed the line\r\n```python\r\nbatches = dataset.batch(BATCH_SIZE).prefetch(PREFETCH_FACTOR * BATCH_SIZE)\r\n```\r\nto\r\n```python\r\nbatches = dataset.apply(map_and_batch(lambda a, b: (a, b), BATCH_SIZE)).prefetch(PREFETCH_FACTOR * BATCH_SIZE)\r\n```\r\nin the [notebook](https://gist.github.com/tillahoffmann/53f902212c1c511e6c3d45abb5f37b9b) linked [above](https://github.com/tensorflow/tensorflow/issues/13101#issuecomment-334788891). But the code hangs indefinitely when I try to train.", "@tillahoffmann apply the following fix to your version of TF: https://github.com/jsimsa/tensorflow/pull/1 and try again ... if that does not help, please use gdb to connect to the running process using `sudo gdb -p <PID>`, then run `thread apply all bt` to collect all stack traces, and share them here", "@jsimsa, thank you for the fix. It seems to have made it into master because I can run the cod using the nightly build. Unfortunately, the performance is slightly worse than the original code that used `batch` instead of `map_and_batch`. I have updated the notebook.", "@tillahoffmann I digged into this a bit more and my conclusion is that the slowdown is due to the (constant) overhead of `tf.data` runtime. In particular, I  have run the following experiment to evaluate my hypothesis:\r\n\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tqdm import tqdm\r\nfrom tensorflow.contrib import data as tfdata\r\n\r\nBATCH_SIZE = 128\r\nFEATURE_SHAPE = (16, 16)\r\nNUM_BATCHES = 1024*2\r\nPREFETCH_FACTOR = 4\r\n\r\nweights = np.random.normal(0, 1, FEATURE_SHAPE)\r\n\r\ndef data_generator():\r\n  for _ in range(NUM_BATCHES * BATCH_SIZE):\r\n    feature = np.random.normal(0, 1, FEATURE_SHAPE)\r\n    value = np.sum(feature * weights) + np.random.normal(0, 1)\r\n    yield feature.astype(np.float32), value.astype(np.float32)\r\n\r\n\r\ndef data_generator_batch():\r\n  for _ in range(NUM_BATCHES):\r\n    features, values = np.empty((BATCH_SIZE, 16, 16)), np.empty((BATCH_SIZE))\r\n    for i in range(BATCH_SIZE):\r\n      features[i] = np.random.normal(0, 1, FEATURE_SHAPE)\r\n      values[i] = np.sum(features[i] * weights) + np.random.normal(0, 1)\r\n    yield features, values\r\n\r\n\r\ndef build_graph(batchX, batchY):\r\n  weights_tensor = tf.get_variable('weights', FEATURE_SHAPE, tf.float32)\r\n  predictions = tf.reduce_sum(weights_tensor * batchX, axis=(1, 2))\r\n  loss_tensor = tf.losses.mean_squared_error(batchY, predictions)\r\n\r\n  optimizer = tf.train.AdamOptimizer()\r\n  train_op = optimizer.minimize(loss_tensor)\r\n  return loss_tensor, train_op\r\n\r\n\r\ndef train(sess, train_op, loss_tensor):\r\n  times = []\r\n  losses = []\r\n  for i in tqdm(range(NUM_BATCHES)):\r\n    start = time.time()\r\n    _, loss = sess.run([train_op, loss_tensor])\r\n    times.append(time.time() - start)\r\n    losses.append(loss)\r\n  return np.asarray(times), np.asarray(losses)\r\n\r\ntimes = {}\r\nlosses = {}\r\n\r\n# Dataset without batch training\r\nprint(\"Dataset with batching generator\")\r\nwith tf.Graph().as_default():\r\n  dataset = tfdata.Dataset.from_generator(\r\n      data_generator_batch, (tf.float32, tf.float32), ((BATCH_SIZE, 16, 16),\r\n                                                       (BATCH_SIZE)))\r\n  batches = dataset.prefetch(PREFETCH_FACTOR)\r\n  iterator = batches.make_one_shot_iterator()\r\n  batch_X, batch_y = iterator.get_next()\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\ntimes['datasets_batch'], losses['datasets_batch'] = train(\r\n    sess, train_op, loss_tensor)\r\n\r\n# Using generator\r\nprint(\"Feed with batching generator\")\r\nwith tf.Graph().as_default():\r\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\r\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\n  tf.train.start_queue_runners(sess)\r\n\r\ndata = data_generator_batch()\r\n_times = []\r\n_losses = []\r\n\r\nfor i in tqdm(range(NUM_BATCHES)):\r\n  X, y = next(data)\r\n  start = time.time()\r\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\r\n  _times.append(time.time() - start)\r\n  _losses.append(loss)\r\ntimes['generator_batch'] = np.asarray(_times)\r\nlosses['generator_batch'] = np.asarray(_losses)\r\n\r\n# Dataset with batch training\r\nprint(\"Dataset without batching generator\")\r\nwith tf.Graph().as_default():\r\n  dataset = tfdata.Dataset.from_generator(\r\n      data_generator, (tf.float32, tf.float32), (FEATURE_SHAPE, ()))\r\n  batches = dataset.batch(BATCH_SIZE).prefetch(PREFETCH_FACTOR)\r\n  iterator = batches.make_one_shot_iterator()\r\n  batch_X, batch_y = iterator.get_next()\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\ntimes['datasets_no_batch'], losses['datasets_no_batch'] = train(\r\n    sess, train_op, loss_tensor)\r\n\r\n# Using generator\r\nprint(\"Feed without batching generator\")\r\nwith tf.Graph().as_default():\r\n  batch_X = tf.placeholder(tf.float32, (BATCH_SIZE, ) + FEATURE_SHAPE)\r\n  batch_y = tf.placeholder(tf.float32, (BATCH_SIZE, ))\r\n\r\n  loss_tensor, train_op = build_graph(batch_X, batch_y)\r\n\r\n  sess = tf.Session()\r\n  sess.run(tf.global_variables_initializer())\r\n  tf.train.start_queue_runners(sess)\r\n\r\ndata = data_generator()\r\n_times = []\r\n_losses = []\r\n\r\nfor i in tqdm(range(NUM_BATCHES)):\r\n  X = []\r\n  y = []\r\n\r\n  for i in range(BATCH_SIZE):\r\n      _X, _y = next(data)\r\n      X.append(_X)\r\n      y.append(_y)\r\n\r\n  start = time.time()\r\n  _, loss = sess.run([train_op, loss_tensor], {batch_X: X, batch_y: y})\r\n  _times.append(time.time() - start)\r\n  _losses.append(loss)\r\ntimes['generator_no_batch'] = np.asarray(_times)\r\nlosses['generator_no_batch'] = np.asarray(_losses)\r\n```\r\n\r\nwhich results in:\r\n\r\n```\r\nDataset with batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:09<00:00, 214.56it/s]\r\nFeed with batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:07<00:00, 291.40it/s]\r\nDataset without batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [01:05<00:00, 31.11it/s]\r\nFeed without batching generator\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2048/2048 [00:08<00:00, 227.84it/s]\r\n```\r\n\r\nIn particular, if a generator that produces whole batches is used (as opposed to a generator that produces single elements batched by `tf.data`), the throughput of `tf.data`-based execution is comparable to the throughput of feed-based execution.\r\n\r\nNormally, I would expect the cost of fetching a single element from a pipeline to include a possibly expensive `tf.data.map` operation, over which the constant overhead of executing the `tf.data` pipeline (e.g. scheduling TF ops) would be amortized. For pipelines that do very little computation (such as the one used in your example), the overhead of the general purpose pipeline execution engine can be non-negligible and pushing the batching logic into the generator is one possible solution to avoid the slowdown.", "@jsimsa Somebody is suggesting [this approach](https://stackoverflow.com/questions/47086599/parallelising-tf-data-dataset-from-generator) on stackoverflow. Is it a workaround or a best practice?", "/cc @matpalm", "I commented on the Stack Overflow thread; if possible, moving expensive processing from a `generator` to a `map` will make it possible to parallelize the processing. I find that preferrable to creating multiple identical generators and combining them using `interleave`.\r\n\r\nOn a related note, the extreme version of the approach discussed on the Stack Overflow page is to replace  `tf.data.Dataset.from_generator` with `tf.data.Dataset.range(N).map(...)` where `map(n)` generates the `n`-th element. If possible, this will maximize the opportunity for parallelism as the only computation that needs to be serialized is the generation of the `map` function inputs.", "@jsimsa I think that `generator` is an important parallel case to handle in the docs cause frequently it needs to handle itself the concept of epoch.", "@jsima Using `tf.data.Dataset.range(N).map(...)` instead of `tf.data.Dataset.from_generator` does not alwais work. As I mentioned in #14448, `tf.data.Dataset.from_generator` wrapps `tf.py_func` to have a generalized `tf.py_func` (i.e. allow nested structures as output instead of only flat outputs. Also it assigns the shapes. In #15121 is a PR that extracts the generalized `tf.py_func`). \r\n\r\nOn Stack Overflow I gave an example how you can replace `tf.data.Dataset.from_generator` with something that can be parallized from tensorflow.", "@boeddeker So mainly  `num_parallel_calls` will be independent from `dataset.batch`: I.e. `num_parallel_calls` could be set to the number of available core and `dataset.batch` to the device capacity? It this use case is `dataset.batch %  num_parallel_calls` needed to be 0?", "@bhack Afaik is `dataset.batch` independent from `num_parallel_calls`. Using the example code for `from_indexable` from Stack Overflow, below a small example. The execution time is `time to get one item` devided by `num_parallel_calls` and is independent from batch size.\r\n\r\n```python\r\nimport time\r\n\r\nclass PyDataSet:\r\n    def __len__(self):\r\n        return 20\r\n        \r\n    def __getitem__(self, item):\r\n        time.sleep(0.5)  # <----------------- Takes 0.5 seconds to \"load\" the data\r\n        return np.random.normal(size=(10))\r\n\r\nds = from_indexable(PyDataSet(), output_types=tf.float64, output_shapes=[None, 10], num_parallel_calls=4)\r\nds = ds.batch(9)\r\nit = ds.make_one_shot_iterator()\r\nentry = it.get_next()\r\nwith tf.Session() as sess:\r\n    start = time.perf_counter()\r\n    try:\r\n        while True:\r\n            print(sess.run(entry).shape)\r\n    except tf.errors.OutOfRangeError:\r\n        pass\r\n    delta = time.perf_counter() - start\r\n    print(delta)  # 2.5277008840057533\r\n```", "Yes what I meant is that if you make 4 parallel calls (i.e. you have 4 cores) for each get_next how much get_next internally will be called to fill a batch of 9?", "I added a print to `__getitem__` and it printed 12 times for 4 cores with batch size 9. So there are 12 `__getitem__` calls.\r\n\r\nFurther, I changed the sleep to only apply to one entry per `num_parallel_calls`\r\n```\r\n        if item in [0, 4, 8]:\r\n            time.sleep(1)\r\n```\r\nand the slowest entry defined the runtime (i.e. 3 seconds, ideally it would take 1 second).\r\nSo my conclusion is that `num_parallel_calls` follows the fork and join idea. It starts `num_parallel_calls` and waits until all of them have finished before launch the next `num_parallel_calls`.\r\n\r\nMaybe @mrry knows more about the internals, my knowledge is based on toy examples.\r\n", "Yes what I am guessing if there is not an explicitly `data.prefetch` where `dataset.batch % num_parallel_calls` is going? Is it in the next batch? Does it make sense to maintain `dataset.batch % num_parallel_calls == 0`?", "I am not sure if I understand your question correctly.\r\n\r\nMy example does not contain `data.prefetch` and `dataset.batch % num_parallel_calls` is `1`.\r\nMy experiments showed that the iterator items are independent of the value `num_parallel_calls`.\r\nThe only reason that I can find for `dataset.batch % num_parallel_calls == 0` is when you are only interested in a single call and does not want to burn/waste computation power and/or RAM.\r\n\r\n`map` caches the not required calculations for the next batch.\r\n\r\nHere an example what I understand that happens:\r\n`num_parallel_calls == 4`\r\n`batch == 9`\r\nlaunch `__getitem__[i] for i in [0, 1, 2, 3]`\r\nwait until all `__getitem__` calls finished, launch `__getitem__[i] for i in [4, 5, 6, 7] `\r\nwait until all `__getitem__` calls finished, launch `__getitem__[i] for i in [8, 9, 10, 11] `\r\nwait until `__getitem__[8]` finished than build a batch\r\nNote: In the background the remaining calls to `__getitem__` may still compute)\r\n\r\nnest session run\r\nwait until all `__getitem__` calls finished\r\nwait until all `__getitem__` calls finished, launch `__getitem__[i] for i in [12, 13, 14, 15]`\r\n... \r\n", ">The only reason that I can find for dataset.batch % num_parallel_calls == 0 is when you are only interested in a single call and does not want to burn/waste computation power and/or RAM.\r\n\r\nYes.\r\nWhere `13, 14, 15`  will go? In the next batch?", "Yes, the next batch will contain the items `9, 10, 11, 12, 13, 14, 15, 16, 17`.", "So there is some kind of buffer that store 9, 10, 11 from the third `__getitem__` call?", "Yes, from my experiments it seems to be so.", "@boeddeker your understanding is almost complete ... the only difference is that in reality there is no barrier of size `num_parallel_calls`. The `__getitem__[i + num_parallel_calls]` call is launched only after the `__getitem__[i]` call finishes but possibly before the `__getitem__[i + 1]` call finishes. Internally, the `map` implementation has `num_parallel_calls` \"slots\" that are used for storing (and possibly prefetching) the invocation results.", "@jsimsa Thanks for the explanation. \r\nI had a bad example, where the element `0, num_parallel_calls, ...` are the slowest.\r\n\r\nDo you also know, why calling `20` times `time.sleep(1)` with  `num_parallel_calls=20` takes 5 seconds on a computer with 4 cores (I expect 1 second)?\r\nIt seems that the \"prefetching\" rule in `map` follows your explanation, but the numbers of threads are limited to `min(num_parallel_calls, cpu_core_count)`.\r\n\r\n", "@jsimsa So basically it has a internal auto prefecthing for the next batch. Right?", "Each `map` invocation itself does not have a thread pool, instead it schedules the invocations using the `interop_thread_pool` (which has a default size of `cpu_core_count`). Try increasing the size of the `interop_thread_pool` to `20` and see what effect it has on the runtime.", "Thanks, `inter_op_parallelism_threads` work. But the second `tf.Session` requires also `use_per_session_threads` to overwrite the value.\r\n```python\r\nconfig = tf.ConfigProto(\r\n    inter_op_parallelism_threads=20,\r\n    # use_per_session_threads=True  # required when not first Session in python interpreter\r\n)\r\n```", "I had opened another issue (https://github.com/tensorflow/tensorflow/issues/16343) which is also related to feedback of the generator process. I think it's beneficial to move those comments here and close that issue in order to consolidate the related discussions to one issue.\r\n\r\n---\r\n\r\nWe're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using `Dataset`:\r\n\r\n1) `tf.data.Dataset.from_generator(generator=my_custom_reader, ...)`\r\n\r\nCreate a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of `interleave(...)` because the generator can't accept a tensor, and this use case is begging to use `interleave(...)`.\r\n\r\nA solution here might be to provide a method for a generator to accept a tensor, as `tf.py_func(...)` does for functions.\r\n\r\n2) `tf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)`\r\n\r\nThe map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with `map`, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.\r\n\r\nA solution here might be to extend the `map` function to support generators.\r\n\r\nUnless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.\r\n\r\nI've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.\r\n\r\nhttps://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\r\n\r\nUpdate to this post:\r\n---------------\r\nThe solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an `unsupported object type numpy.ndarray` error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef mypymap(data_numpy):\r\n  result = np.array([\r\n    np.array([100,101,102]),\r\n    np.array([200,201,202,203])\r\n  ])\r\n  return result\r\n\r\ndata_input = ['fileA','fileB','fileC']\r\n\r\nds = tf.data.Dataset.from_tensor_slices(data_input)\r\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\r\n\r\nelement = ds.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n  for _ in range(3):\r\n    print(sess.run(element))\r\n```\r\nSo that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the `Dataset` pipeline.", "Can we document the relationship between `from_generators` and Dataset cache?", "@mrry I saw dataset.batch(BATCH_SIZE).prefetch(FACTOR * BATCH_SIZE) and dataset.batch(BATCH_SIZE).prefetch(FACTOR), which ones is correct usage?", "@mrry Is it possible to add `inp` argument like `tf.py_func` for `Dataset.from_generator`?\r\nIt would be more flexible for this [approach](https://stackoverflow.com/a/47874044/9644463)", "I'm interested in something like this too @ychfan.", "Has there been any discussion on adding `num_parallel_calls` as an argument to `from_generator()`? This [post](https://stackoverflow.com/questions/47086599/parallelising-tf-data-dataset-from-generator) on Stack Overflow has a solution, but it seems unnecessarily complicated.", "@RylanSchaeffer the function `from_generator` will never get a `num_parallel_calls` becasuse a python generator is something that cannot be parallelized. In #14448 I suggested to add an alternative to `from_generator` that can have a `num_parallel_calls` (Use `__getitem__` instead if of `__iter__`), but the feedback was that it is not necessary.\r\n\r\nI updated my answer in https://stackoverflow.com/a/47884927/5766934, so using that code is now simpler.", "Assuming I/O is the bottleneck, does it suffice to chain `from_generator` with `prefetch`?\r\n\r\nYou could always do multiple `from_generator` in a `flat_map` too. Particularly `tf.contrib.data.parallel_interleave` is sweet.", "@boeddeker , thanks for that information. I'll take a look.\r\n\r\n@carlthome , generating a batch is significantly slower than a single training step, so unfortunately, using `prefetch` doesn't help. Can you link me a tutorial or example of how `flat_map` and `parallel_interleave` are used?", "@mrry \r\n\r\nHi Derek;\r\n\r\nseems to me that the from_generator can not catch up the requirement of the data feed for the convolution neuron network processing on gpu.\r\n\r\nfollowing is the image of the gpu performance\r\n\r\n![image](https://user-images.githubusercontent.com/22060946/43689057-1b46e08c-98a9-11e8-86a6-c39e32dea312.png)\r\n\r\nI am using the dataset.from_generator to get the input data from slicing of pandas dataframe. set the prefetech as 5. the image shows the teeth of the gpu utilization which implies the gpu is waiting for the data feed in, wonder why the from_generator can not catch up the requirement via multithreading of cpu.\r\n\r\npls indicate if there is any trick to improve the feed speed of the from_generator.\r\n\r\ntks.", "@mrry \r\n\r\nfurther will from_generator introduce the parameter as num_parallel_calls which will parallel the process of the generator function.\r\n\r\ntks.\r\n", "@jianxucsm As I mentioned earlier, a python generator cannot run in parallel. So from_generator will never get a num_parallel_calls, as long the input is a generator.", "@boeddeker So what is the preferred method to loading data with tf.data.Dataset ? ", "@zh794390558 It depends, when you have a lightweight generator, then using tf.data.Dataset.from_generator is fine.\r\n\r\nWhen your generator gets expensive, I would rewrite it to something that is indexable and use my code that I posted in https://stackoverflow.com/a/47884927/5766934.\r\n\r\nAlternatives are splitting your generator in a lightweight generator and a map function (https://stackoverflow.com/a/47089278/5766934) or use tf.contrib.data.parallel_interleave (https://stackoverflow.com/a/47874044/5766934). I personally do not like tf.contrib.data.parallel_interleave in cases where loading the examples is independent.\r\n\r\n", "@boeddeker   https://stackoverflow.com/a/47884927/5766934 this need the dataset must be a iterator, how to using the `yield` pipeline?    \r\nThe tf.contrib.data.parallel_interleave  will working easy for `yield` pipeline, and I think the tf.contrib.data.parallel_interleave has a random case  to feed data to shuffle buffer.     \r\n\r\nCan you give a performance comparison with tf.contrib.data.parallel_interleave, TFRecord?\r\n\r\n", "In my opinion is a `yield` pipeline not necessary. The code that you write in your generator is usually something like:\r\n```python\r\ndef gen():\r\n  # pre calculations\r\n  for i in range(10000):\r\n    # example calculations\r\n    yield (i, [1] * i)\r\n```\r\nwhere you have a `for` loop over an index or a file list.\r\nThe counterpart code where this is indexable is then:\r\n```python\r\nclass gen:\r\n  def __init__(self):\r\n    # pre calculations\r\n  def __len__(self):\r\n    return 10000\r\n  def __getitem__(self, i):\r\n    # example calculations\r\n    return (i, [1] * i)\r\n```\r\nSo most `yield` pipelines could be rewritten with a class that has `__getitem__`.\r\nMain advantages:\r\n- `__getitem__` can be parallelized, while a generator can not be parallelized\r\n- you can shuffle without actually loading the data (i.e. no buffer is needed)\r\n\r\nSince I do not use tf.contrib.data.parallel_interleave, TFRecord, I have no performance comparison.\r\nI like to do a shuffle across all my data for each epoch and TFRecord does not fit this use case.", "Can we add something in the doc/faq? ", "IMHO would be the best place to document something like that at a `from_indexable` method in the Dataset source code.\r\nBut @mrry argued in #14448 that `from_indexable` would be to trivial and the user can write the code himself.", "@boeddeker why not using tf.data.Dataset.shuffle \uff1fIf use it, then we not need to control shuffle manually in `class gen`.", "@zh794390558 Because that function uses a buffer.\r\nUsually the buffer size is significant smaller than your number of examples, because the size of your dataset is far away from fitting in memory (you can overcome this when your actual data loading happens in tensorflow).\r\nWhen this is the case, the shuffle is only a local shuffle.\r\nSo when your complete dataset is sorted (e.g., the deprecated `sklearn.datasets.fetch_mldata('MNIST original')` was sorted) the shuffle can only shuffle the similar examples that are close together.\r\n\r\nToy example:\r\nLet's consider your dataset is sorted and the labels are:\r\n`[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9]`\r\nWhen the buffer size is 4 you can never observe a 3, 4, 5, 6, 7, 8 or 9 at the beginning of an epoch.", ">Because that function uses a buffer.\r\n\r\nSpeaking of which, @mrry mentioned on a related issue (maybe Stack Overflow, don't remember sorry!) that the tf.data team is looking into a disk shuffle of the entire index. Have there been any more news/work around this? I guess there are complications with distributed training but it doesn't feel like it should be that hard to write a permutation.index file per shard and maintain close to random access in the underlying data store.\r\n\r\n`tf.data` is great but one of the big missing features is definitely the possibility of shuffling an entire `Dataset` per epoch. After porting some `keras.utils.Sequence` code to `tf.data` I get somewhat worse final test accuracy on a bunch of standard benchmarks (FashionMNIST, Boston house pricing, etc.) and it's greatly affected by shuffle buffer size.", "So I  think `tf.data` should using `iterator` which same to `class gen`, then do index shuffle by `tf.data`.", "In my pipeline dataset shuffling is the bottleneck. Is my use-case (large dataset, small networks. In my case 3e8 samples, 9D input space, 1D output space. Networks 2 layers of 128 neurons) any consideration? Found this way of shuffling 4x faster then TF dataset shuffle in some cases:\r\n\r\n```\r\nperm = np.arange(self._num_examples)\r\nnp.random.shuffle(perm)\r\nself._data = np.take(self._data, perm, axis=0)\r\n```", "how do you take TFRecords of sequence examples and turn them into ragged tensors? It seems like this might be supported but can't find many docs about this... and batch tries to concat our tensors and fails", "Thanks for all the great feedback on this thread! Since this is now an old issue and the `Dataset.from_generator()` API and implementation have evolved, I'm going to close the thread now. Please feel free to open new issues if you have comments about `Dataset.from_generator()` or how it could be made more useful.", "@mrry @aselle @davidparks21 @tillahoffmann @jsimsa \r\nHi , I'm not familiar with tf.data api and huge data training tricks\uff0cin my case, I found the IO is a problem: GPU uti is low, about 15%. I use a generator function to read local multiple pickle files(about 200K),so each batch is a local file in my case, examples are bellow:\r\n```\r\ndef train_data():\r\n    while True:\r\n        for i in shuffle(train_id):\r\n            df = pd.read_pickle(df_train_val['file_name'].iloc[i],\r\n                                compression='gzip')\r\n            feature = df.to_numpy().reshape(df.shape[0], -1)\r\n            yield feature.reshape(-1, 56),feature.reshape(-1, 56)\r\n```\r\nThen\r\n```\r\ngn = tf.data.Dataset.from_generator(train_data, output_types=(tf.float32, tf.float32),\r\n                                          output_shapes=((None, 56),(None, 56))\r\n                                         ).prefetch(tf.data.experimental.AUTOTUNE)\r\n```\r\nI found prefetch not work and IO bottleneck is very high, so any advice for this case, how can I speed up the training process?", "@Anhaoxu, this kind of question should be moved to stack overflow, could you repost it there and put a link to the topic here?\r\n\r\nIn brief, for high IO data Python is a major obstacle here. The Python GIL (global interpreter lock) limits you to one CPU core when processing is done in Python (vs external libraries like tensorflow which handles processing in C external to the Python runtime). I run a model that chews on about 400 MB/sec of data and that requires about 3 cores to handle IO. If your main process tries to do all the work it can delay the training loop, and one core might be enough for the IO period. There are 2 solutions as I have come to understand. \r\n\r\n1) write your data to tfrecords files in subprocesses/multiprocessing (what I currently do). Tensorflow can deserialize tfrecords files in more than one core outside of the Python runtime so you're good. But that's a little cumbersome to implement and demands NVMe speed IO (I push over 1 GB/sec of total IO due to write-then-read).\r\n\r\n2) Use python 3.8 (for which tensorflow is just now starting to support) and use System V shared memory. Use subprocesses (Python multiprocessing) to do the IO and prep work, and hand off a pointer to the prepared sample to the main process (that's the shared memory part). All other forms of Python serialization between processes engage the Python GIL and don't work under high IO scenarios. I have not tested the SystemV shared mem approach yet, I just installed Python 3.8 and TF 2.2RC2 to do so in the past few weeks as TF just gained support for Python 3.8 recently.\r\n\r\nYou might also be interested in this: https://ai.googleblog.com/2020/05/speeding-up-neural-network-training.html", "> In brief, for high IO data Python is a major obstacle here. The Python GIL (global interpreter lock) limits you to one CPU core when processing is done in Python\r\n\r\nI have to correct that statement. The Python GIL limits you to execute the python code on a single CPU, not the C code.\r\n\r\nWhen a function is implemented in C, it can release the GIL. When the GIL is released, you can do parallel IO with threads in python.\r\nA famous example for a function that releases the GIL is `open`, so an IO operation usually releases the GIL. Another example are many numpy functions. So using a threadpool for numpy operations can utilizes all cores of your system (Assuming, the arrays are large enough)\r\n\r\n@Anhaoxu The problem in your example is, that a generator cannot be executed in parallel. When you start your dataset with a range and do the IO in a tensorflow map function, the IO can be executed in parallel. That is the reason, why I proposed to have a `from_indexable`.", "@boeddeker, your clarifications make an excellent point and clarifies something I wasn't clear about. Many libraries, Tensorflow, in particular, release the GIL and can process on multiple cores. However, at high IO data rates (a good fraction of a GB/sec) the problem becomes CPU bound not IO bound because just the deserialization process inside of Python drives up the CPU utilization past 1 core. \r\n\r\nFor example, TensorFlow utilizes 3-4 cores to deserialize the tfrecords files I pass it, and those are in a nice efficient protobuf format. Python cannot read at those rates because the GIL becomes CPU bound on the basic serialization processes that are required to prepare the data for TensorFlow, not on the IO-wait part. Notably, this doesn't typically happen with large image datasets because images are highly compressible, so your actual IO is an order of magnitude less than with un-compressible data.\r\n\r\nI've spent many iterations trying to solve this problem for a relatively small CNN model that reads raw uncompressible scientific data from a 3TB dataset. With threads, I maybe get 120% CPU use out of Python. Subprocesses can't pass data to the main process because serialization becomes CPU bound in Python (GIL issue). AsyncIO with multiple event threads is all GIL based, even using c-based `uvloop`. \r\n\r\nIf someone is using Python/Tensorflow where you see more than about 1.2 cores being used by the main process (the max I've ever seen) I would absolutely love to know about it. Perhaps the only difference in my case is that my data has to be read over a remote filesystem, so it's coming in over HTTP from a local S3-compliant interface, perhaps that changes the equation for me.\r\n\r\nI went around and around with this problem and the only thing that worked was subprocesses dumping to tfrecords and letting tensorflow handle the multi-core deserialization outside of Python. My tf dataset generator function just yields tfrecords filenames as they're written. I believe shared memory is another solution. But trying to push a good fraction of a GB/sec of data through the same python interpreter as the training loop is hopeless in my experience (at least pre-py3.8 shared-memory).\r\n", "@boeddeker @davidparks21 \r\nI just want to use map api to yield parallel generators, but the map api not supported generator type, in my case, I would like to know how to realize this page [https://www.tensorflow.org/guide/data_performance#parallelizing_data_extraction](url), or how to use from_indexable to realize parallel generators, it's not clear to me, and I can't find related information.\r\nin fact, I have multiple files, they are from s3, I just found reading from s3 is slower than read from local file system, so I compressed these files, maybe 200K files, in this process  I also can concat them to be less number, maybe just 1K files, but I think this operation not solve the problem fundamentally, the problem is how can not let the GPU wait for data?\r\nfor example:\r\nI want to shuffle their file id to make every epoch training using all files, so I write this function:\r\n```\r\ndef train_data():\r\n    while True:\r\n        for i in shuffle(train_id):\r\n            df = pd.read_pickle(df_train_val['file_name'].iloc[i],\r\n                                compression='gzip')\r\n            feature = df.to_numpy().reshape(df.shape[0], -1)\r\n            yield feature.reshape(-1, 56),feature.reshape(-1, 56)\r\n```\r\nthis function is very clear, right? so how this function be parallel? if there are some tutorial about parallel and distributed data transformation or training, I would be very appreciate.", "As others have already mentioned, please ask this question on stackoverflow @Anhaoxu ", "> your clarifications make an excellent point and clarifies something I wasn't clear about. Many libraries, Tensorflow, in particular, release the GIL and can process on multiple cores. However, at high IO data rates (a good fraction of a GB/sec) the problem becomes CPU bound not IO bound because just the deserialization process inside of Python drives up the CPU utilization past 1 core.\r\n\r\n@davidparks21 good point. Yes, it highly depends on the data that you use. I mainly work with wav files on a remote file system. Wav files are usually relative large ~ 5MB. \r\nBecause of that (and the file format) I have a purely IO bottleneck.\r\nDepending on the file format and the reader also other data can be efficiently loaded (e.g. decompress images could be done without the GIL in C).\r\nI have some good experiences for parallel deserialization of uncompressed data with `pickle.loads` (note with an `s`).\r\n\r\n> I've spent many iterations trying to solve this problem for a relatively small CNN model that reads raw uncompressible scientific data from a 3TB dataset. With threads, I maybe get 120% CPU use out of Python. Subprocesses can't pass data to the main process because serialization becomes CPU bound in Python (GIL issue). AsyncIO with multiple event threads is all GIL based, even using c-based uvloop.\r\n\r\nI am not sure, how you moved the data from one subprocess to another, but probably you used pickle.\r\nDid you try to save your data as pickle, read all data with open and then call `pickle.dumps`? This can be done in threads and need only low CPU time (pickle data is already prepared for python).\r\n\r\n@davidparks21: When you want to continue this discussion, should we move this discussion to somewhere else?\r\n"]}, {"number": 13100, "title": "- NEON XLA CPU runtime is not compiling because of missing plog function in Eigen", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.3.0-0-g9e76bf324', '1.3.0')\r\n- **Python version**: 3.4.3\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**:\r\nWORKSPACE correctly configure for Android NDK (r12b) and SDK(latest).\r\nbazel build -c opt --cxxopt='-std=c++11' --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon\r\n\r\n### Describe the problem\r\nI use Tensorflow compiled from source to use XLA aot binaries for Arm. For XLA to work, it needs to have runtime support for that architecture but with the above command it fails complaining for a missing function in Eigen.\r\nI fixed the bug in Eigen and submitted a pull request for integration:\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/334/add-support-for-neon-plog-packetmath/diff\r\n\r\nwith the following patch, the compilation completes successfully. Once the pull request is accepted I will submit a pull request here to use the new Eigen version with the fix.\r\n\r\n### Source code / logs\r\nFor the record, this is the error i get:\r\n\r\nbazel build -c opt --cxxopt='-std=c++11' --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon\r\nExtracting Bazel installation...\r\n...........\r\nINFO: Analysed target //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon (17 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/gmichel/work/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:333:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:cpu_runtime_neon' failed (Exit 1)\r\nIn file included from external/eigen_archive/Eigen/Core:372:0,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'Packet Eigen::internal::plog(const Packet&) [with Packet = __vector(4) __builtin_neon_sf]':\r\ntensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:32:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60: error: no matching function for call to 'log(const __vector(4) __builtin_neon_sf&)'\r\n Packet plog(const Packet& a) { using std::log; return log(a); }\r\n                                                            ^\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60: note: candidates are:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:44:0,\r\n                 from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44,\r\n                 from external/eigen_archive/Eigen/Core:78,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:218:8: note: double log(double)\r\n double log(double) __NDK_FPABI_MATH__;\r\n        ^\r\nexternal/androidndk/ndk/platforms/android-14/arch-arm/usr/include/math.h:218:8: note:   no known conversion for argument 1 from 'const __vector(4) __builtin_neon_sf' to 'double'\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:44:0,\r\n                 from external/eigen_archive/Eigen/Core:78,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:357:3: note: constexpr float std::log(float)\r\n   log(float __x)\r\n   ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:357:3: note:   no known conversion for argument 1 from 'const __vector(4) __builtin_neon_sf' to 'float'\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:361:3: note: constexpr long double std::log(long double)\r\n   log(long double __x)\r\n   ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:361:3: note:   no known conversion for argument 1 from 'const __vector(4) __builtin_neon_sf' to 'long double'\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:369:5: note: template<class _Tp> constexpr typename __gnu_cxx::__enable_if<std::__is_integer<_Tp>::__value, double>::__type std::log(_Tp)\r\n     log(_Tp __x)\r\n     ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:369:5: note:   template argument deduction/substitution failed:\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath: In substitution of 'template<class _Tp> constexpr typename __gnu_cxx::__enable_if<std::__is_integer<_Tp>::__value, double>::__type std::log(_Tp) [with _Tp = __vector(4) __builtin_neon_sf]':\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60:   required from 'Packet Eigen::internal::plog(const Packet&) [with Packet = __vector(4) __builtin_neon_sf]'\r\ntensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:32:33:   required from here\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/cmath:369:5: error: no type named '__type' in 'struct __gnu_cxx::__enable_if<false, double>'\r\nIn file included from external/eigen_archive/Eigen/Core:78:0,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'Packet Eigen::internal::plog(const Packet&) [with Packet = __vector(4) __builtin_neon_sf]':\r\ntensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:32:33:   required from here\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:790:5: note: template<class _Tp> std::complex<_Tp> std::log(const std::complex<_Tp>&)\r\n     log(const complex<_Tp>& __z) { return __complex_log(__z); }\r\n     ^\r\nexternal/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/complex:790:5: note:   template argument deduction/substitution failed:\r\nIn file included from external/eigen_archive/Eigen/Core:372:0,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from tensorflow/compiler/xla/service/cpu/cpu_runtime_neon.cc:20:\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:411:60: note:   mismatched types 'const std::complex<_Tp>' and 'const __vector(4) __builtin_neon_sf'\r\n Packet plog(const Packet& a) { using std::log; return log(a); }\r\n                                                            ^\r\nTarget //tensorflow/compiler/xla/service/cpu:cpu_runtime_neon failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 15.227s, Critical Path: 0.76s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@benoitsteiner, could you merge this change back from Eigen?", "@benoitsteiner did you have time to have a look at the pull request from Eigen? I will be happy to assist if necessary.", "Pull request to fix this issue has been merged in Eigen. It is now just a matter of updating Eigen version", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Eigen version has been updated and it fixes the problem."]}, {"number": 13099, "title": "Weights and biases not being updated.", "body": "(I've already tried stackoverflow)\r\n\r\nI've made this neural net to figure out whether a house is a good buy or a bad buy. For some reasons the code is not updating weights and biases. My loss stays same. This is my code:\r\n\r\n    import pandas as pd\r\n    import tensorflow as tf\r\n    \r\n    data = pd.read_csv(\"E:/workspace_py/datasets/good_bad_buy.csv\")\r\n    \r\n    features = data.drop(['index', 'good buy'], axis = 1)\r\n    lbls = data.drop(['index', 'area', 'bathrooms', 'price', 'sq_price'], axis = 1)\r\n    \r\n    features = features[0:20]\r\n    lbls = lbls[0:20]\r\n    \r\n    print(features)\r\n    print(lbls)\r\n    n_examples = len(lbls)\r\n    \r\n    # Model\r\n    \r\n    # Hyper parameters\r\n    \r\n    epochs = 100\r\n    learning_rate = 0.1\r\n    batch_size = 1\r\n    \r\n    input_data = tf.placeholder('float', [None, 4])\r\n    labels = tf.placeholder('float', [None, 1])\r\n    \r\n    weights = {\r\n    \t\t\t'hl1': tf.Variable(tf.random_normal([4, 10])),\r\n    \t\t\t'hl2': tf.Variable(tf.random_normal([10, 10])),\r\n    \t\t\t'hl3': tf.Variable(tf.random_normal([10, 4])),\r\n    \t\t\t'ol': tf.Variable(tf.random_normal([4, 1]))\r\n    \t\t\t}\r\n    \r\n    biases = {\r\n    \t\t\t'hl1': tf.Variable(tf.random_normal([10])),\r\n    \t\t\t'hl2': tf.Variable(tf.random_normal([10])),\r\n    \t\t\t'hl3': tf.Variable(tf.random_normal([4])),\r\n    \t\t\t'ol': tf.Variable(tf.random_normal([1]))\r\n    \t\t\t}\r\n    \r\n    hl1 = tf.nn.relu(tf.add(tf.matmul(input_data, weights['hl1']), biases['hl1']))\r\n    hl2 = tf.nn.relu(tf.add(tf.matmul(hl1, weights['hl2']), biases['hl2']))\r\n    hl3 = tf.nn.relu(tf.add(tf.matmul(hl2, weights['hl3']), biases['hl3']))\r\n    ol = tf.nn.sigmoid(tf.add(tf.matmul(hl3, weights['ol']), biases['ol']))\r\n    \r\n    loss = tf.reduce_mean((labels - ol)**2)\r\n    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\r\n    \r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n    \r\n    iterations = int(n_examples/batch_size)\r\n    \r\n    \r\n    for epoch_no in range(epochs):\r\n        ptr = 0\r\n    \tfor iteration_no in range(iterations):\r\n    \t\tepoch_input = features[ptr:ptr+batch_size]\r\n    \t\tepoch_label = lbls[ptr: ptr+batch_size]\r\n    \t\tptr = ptr + batch_size\r\n    \t\t_, err = sess.run([train, loss], feed_dict={input_data: features, labels: lbls})\r\n    \tprint(\"Error at epoch \", epoch_no, \": \", err)\r\n    \r\n    print(sess.run(ol, feed_dict={input_data: [[2104, 3, 399900, 190.0665]]}))\r\n\r\n\r\nThis is the dataset:\r\n\r\n    Features:\r\n\r\n        area  bathrooms   price    sq_price\r\n    0   2104          3  399900  190.066540\r\n    1   1600          3  329900  206.187500\r\n    2   2400          3  369000  153.750000\r\n    3   1416          2  232000  163.841808\r\n    4   3000          4  539900  179.966667\r\n    5   1985          4  299900  151.083123\r\n    6   1534          3  314900  205.280313\r\n    7   1427          3  198999  139.452698\r\n    8   1380          3  212000  153.623188\r\n    9   1494          3  242500  162.315930\r\n    10  1940          4  239999  123.710825\r\n    11  2000          3  347000  173.500000\r\n    12  1890          3  329999  174.602645\r\n    13  4478          5  699900  156.297454\r\n    14  1268          3  259900  204.968454\r\n    15  2300          4  449900  195.608696\r\n    16  1320          2  299900  227.196970\r\n    17  1236          3  199900  161.731392\r\n    18  2609          4  499998  191.643542\r\n    19  3031          4  599000  197.624546\r\n\r\n    labels:\r\n\r\n        good buy\r\n    0        1.0\r\n    1        0.0\r\n    2        1.0\r\n    3        0.0\r\n    4        1.0\r\n    5        0.0\r\n    6        0.0\r\n    7        1.0\r\n    8        0.0\r\n    9        0.0\r\n    10       1.0\r\n    11       1.0\r\n    12       1.0\r\n    13       1.0\r\n    14       0.0\r\n    15       1.0\r\n    16       0.0\r\n    17       1.0\r\n    18       1.0\r\n    19       1.0\r\n\r\nAny suggestions on how to fix this?", "comments": ["This is a stackoverflow question. When double posting please provide link to your stackoverflow entry  i.e. https://stackoverflow.com/questions/46264133/weights-and-biases-not-updating-in-tensorflow\r\n(which I had to go searching for). I have answered the question there.\r\n"]}, {"number": 13098, "title": "TensorFlow creates different node name for execution", "body": "-----------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nTF v1.3\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\ncuda 8.0/cudnn 5.1.5\r\n- **GPU model and memory**:\r\nTesla P40 \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThis is the tracing result when I executed ptb-lstm application. I wonder the meaning of \"sequence_loss_by_example/add/_1077\". \r\nI wanted to create enqueue and dequeue ops for \"sequence_loss_by_example/add\", but dequeue op is never called, and different node name appears in the tracing file. \r\nCan you let me know what is happened inside session.run?\r\n\r\n<img width=\"439\" alt=\"2017-09-17 18 42 36\" src=\"https://user-images.githubusercontent.com/2465713/30519717-24a28542-9bd8-11e7-9f2c-511275ec059c.png\">", "comments": ["Maybe it's graph rewriting?\r\nCould try turning it off\r\n\r\n```\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess=tf.Session(config=config)\r\n\r\n```", "@sj6077 , did this solve your issue?", "Yes, it works now. Thank you!", "If  your problem is resolved @sj6077, please close the issue."]}, {"number": 13097, "title": "Support 64bit float point gradient", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.1.0\r\n- **Python version**:  3.5.3\r\n- **CUDA/cuDNN version**:Cuda 8.0 cudnn 5.1\r\n- **GPU model and memory**:GTX 1080Ti 11GB\r\n\r\n### Describe the problem\r\nI use Tensorflow to train a multilayer Convolutional network.\r\n\r\nSince my network has too many parameters and my GTX 1080Ti has limited memory(11GB), so the batch size cannot exceed 16 otherwise it would cause OutOfMemory exception.\r\n\r\nI want to update parameters using bigger batch size, so I follow [the answer](https://stackoverflow.com/questions/42156957/how-to-update-model-parameters-with-accumulated-gradients) that is, accumulate and average gradients over multiple batches.\r\n\r\n**Scenario 1**\r\n\r\nIf I use batch size=16, and update parameters after each batch, my network can converge to loss=0.01.\r\n\r\n**Scenario 2**\r\n\r\nIf I use batch size=1, and update parameters after accumulating and averaging gradients after every 16 batches, my network can only converge to loss=0.04.\r\n\r\nTheoreticall the two scenarios should converge to the same loss, but the problem is when the network converge close to the extrema, the magnitude of the gradients is about 1e-5.\r\n\r\nAnd guess how precise is float32 in Tensorflow? I compute gradients and don't update parameters, they differ after 6 significant digits.\r\n\r\nI want my network continues to converge even the magnitude of the gradients is about 1e-5, the float32 cannot satisfy my needs.\r\n\r\nThe obvious solution is to use float64 as the data type of the parameters, but Tensorflow tells me float64 is not supported in Conv2D.", "comments": ["@ZhongBaby See PR #12943 and issue #12941 .", "Thanks! I have two more questions.\r\n1. How long will PR[#12943](https://github.com/tensorflow/tensorflow/pull/12943) be merged?\r\n2. And Do I need to wait for Tensorflow 1.4.0?", "@ZhongBaby PR #12943 is current under review and I don't have information on when it will be merged. It is a feature/enhancement not a critical bug so I assume it make take time.", "@yongtang Can I follow your modification and make it work on my computer? I'm using win10 and not able to build Tensorflow from source...", "@ZhongBaby I don't have access to a Windows dev machine so I may not be able to help in this case.", "You might also try increasing your learning rate, which will raise the magnitude of your gradients update to the variables. This may cause NaNs, so there is a limit to how much you can do.\r\n", "Hi @Zhongbaby, did you find a solution?", "I've just closed out #12943 with support for f64 conv2d, lmk if you have further issues."]}, {"number": 13095, "title": "About the issue: Ran out of GPU memory!", "body": "HI every one, I wrote my first tensorflow code, it is a classic CNN. And when I run it on Ubuntu16. it report error!\r\n\r\n2017-09-17 05:16:34.243946: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-17 05:16:34.243983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-17 05:16:34.244007: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-17 05:16:34.244017: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-17 05:16:34.244023: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-17 05:16:35.679378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0000:89:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.61GiB\r\n2017-09-17 05:16:36.490317: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0xab5bff0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-09-17 05:16:36.492491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: \r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0000:8a:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.61GiB\r\n2017-09-17 05:16:36.496233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 \r\n2017-09-17 05:16:36.496258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y \r\n2017-09-17 05:16:36.496267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y \r\n2017-09-17 05:16:36.496282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0)\r\n2017-09-17 05:16:36.496353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0)\r\nstep 0, train accuracy 0\r\nstep 100, train accuracy 0.845555\r\nstep 200, train accuracy 0.80391\r\nstep 300, train accuracy 0.913599\r\nstep 400, train accuracy 0.893777\r\nstep 500, train accuracy 0.918844\r\nstep 600, train accuracy 0.937555\r\n2017-09-17 05:16:41.502931: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-09-17 05:16:41.503008: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n2017-09-17 05:16:41.503035: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\r\n2017-09-17 05:16:41.503060: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\r\n2017-09-17 05:16:41.503223: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\r\n2017-09-17 05:16:41.503268: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of GPU memory when allocating 0 bytes for \r\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape_2, Reshape_3)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"check_cnn.py\", line 228, in <module>\r\n    TrainNetwork()\r\n  File \"check_cnn.py\", line 147, in TrainNetwork\r\n    sess.run(train_step, feed_dict = {x: xs, y_: ys, keep_prob: 0.5})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Ran out of GPU memory when allocating 0 bytes for \r\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape_2, Reshape_3)]]\r\n\r\nCaused by op 'SoftmaxCrossEntropyWithLogits', defined at:\r\n  File \"check_cnn.py\", line 228, in <module>\r\n    TrainNetwork()\r\n  File \"check_cnn.py\", line 128, in TrainNetwork\r\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = y_)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1597, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2385, in _softmax_cross_entropy_with_logits\r\n    features=features, labels=labels, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): Ran out of GPU memory when allocating 0 bytes for \r\n\t [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape_2, Reshape_3)]]\r\n\r\nThe system is ubuntu16, tensorflow 1.2, CUDA 8, cudnn 6.\r\nthe machine is:\r\n\r\nSun Sep 17 06:30:45 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-SXM2...  On   | 0000:89:00.0     Off |                    0 |\r\n| N/A   39C    P0    34W / 300W |      0MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P100-SXM2...  On   | 0000:8A:00.0     Off |                    0 |\r\n| N/A   38C    P0    32W / 300W |      0MiB / 16276MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\nSo please help me for this, thanks!", "comments": ["Running out of memory is a common problem with neural networks, this is better for stackoverflow :)", "I can not get any useful help from stackoverflow. About this issue:\r\n\r\nRan out of GPU memory when allocating 0 bytes for\r\n[[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape_2, Reshape_3)]]\r\n\r\nNobody answer the question.\r\nso Please give me some advices! and Thank you!"]}, {"number": 13094, "title": "Feature Request - SRU (Simple Recurrent Unit) Cell", "body": "ArXiv path: https://arxiv.org/abs/1709.02755\r\n\r\nThis is a recently proposed, parallelization-friendly RNN cell. The author released his own PyTorch version of the SRU. We are looking forward to an offical tensorflow implementation with Cudnn optimizations. ", "comments": ["Hi, I'm interested in taking a crack at this. Will keep you updated.", "Hi, it's interesting RNN cell, will keep you updated", "Any progress?", "Will submit the initial PR early next week. \r\nSorry I started late because I have to clear some build errors on master.", "I am interested in this question as well. If @tjingrant have some problems, I can also join in.", "I apologize for keeping everyone interested waiting and will do my best to release a PR early this week. @GeorgyZhou: thanks for your offer, I will let you know if I need extra hands!", "Any news?", "I'm working on a fused version of SRU (process on whole time steps)", "Hi @stegben, what's the status of this? It sounds like the fused implementation is really what would make SRU most useful.", "To be honest, after implementing the non-fused version, I suspect whether it's worth going deeper. The [paper](https://openreview.net/forum?id=rJBiunlAW) has been rejected on the ground that it is a special case of Quasi-RNN which makes sense. It's probably better for everyone to work on a better version of Quasi-RNN than keeping implementing SRU.", "@tjingrant it's now published on EMNLP: https://aclanthology.info/papers/D18-1477/d18-1477  \r\nwe tried it on some dataset, and it really works, but we have to use pytorch, since the tf version SRUCell is NOT faster than GRUCell.  \r\n\r\nCould anyone implement a tf-version SRU?", "Closing, as this feature is outside the scope of TensorFlow Core and no longer relevant, from a research perspective. If I'm mistaken, please feel free to reopen and we can route to [TensorFlow Addons](https://www.github.com/tensorflow/addons). Thanks! \ud83d\ude42 "]}, {"number": 13093, "title": "i want to run a batch of images using the label_images example,but this example is only a image,not a batch of image,what should i do,i can't find other examples to refer.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13092, "title": "Errror in creatinig training and inference logits", "body": "I am using ubuntu 16.04\r\nversion of tensorflow is 1.3.0\r\ninstalled and validated using anaconda\r\nWhile trying to create training and inference logits of input[32] of the following python notebook\r\nhttps://github.com/Currie32/Chatbot-from-Movie-Dialogue/blob/master/Chatbot_Attention.ipynb\r\n\r\nI was getting errors while executing the following command:\r\n\r\ntrain_logits, inference_logits = seq2seq_model( tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(answers_vocab_to_int), len(questions_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, questions_vocab_to_int)\r\n\r\nStack trace:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\n  File \"<stdin>\", line 6, in seq2seq_model\r\n  File \"<stdin>\", line 10, in encoding_layer\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 405, in bidirectional_dynamic_rnn\r\n    time_major=time_major, scope=fw_scope)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 598, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 761, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2775, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2604, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2554, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 744, in _time_step\r\n    skip_conditionals=True)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 236, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 732, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 938, in call\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 774, in __call__\r\n    output, new_state = self._cell(inputs, state, scope)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 401, in call\r\n    concat = _linear([inputs, h], 4 * self._num_units, True)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1039, in _linear\r\n    initializer=kernel_initializer)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1405, in wrapped_custom_getter\r\n    *args, **kwargs)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/home/shreyash/.local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 664, in _get_single_variable\r\n    name, \"\".join(traceback.format_list(tb))))\r\nValueError: Variable bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"<stdin>\", line 10, in encoding_layer\r\n  File \"<stdin>\", line 6, in seq2seq_model\r\n  File \"<stdin>\", line 2, in <module>\r\n", "comments": ["Since this is not core TensorFlow code, we typically do not support it. Please file a bug with the author of the chatbot. Thanks!"]}, {"number": 13091, "title": "Tensorflow not detecting my GPU ", "body": "hi @oelmekki @yaroslavvb did you managed to resolve the issue?\r\nI have the same problem able to use GPU before updating tensorflow to V1.3.0. I have also upgraded my Cudnn to V6. My CUDA is v8.0 so I don't seem to understand where the problem is coming from. I can verify that my tensorflow is GPU version because I used tfBInaryUrl for Python2.7-PGU support. Aside this, I have also installed several times with 'pip install tensorflow-gpu' and I still cannot run my codes on gpu. Theano works fine and I could run code with GPU if I use theano. \r\n\r\nWhen I tried to force the computation to be run on GPU , my codes wouldn't run and now, I got this mesage \"Device mapping: no known devices.\r\n\r\nThe frustrating thing is that I was using the GPU before I upgraded to v1.3.0/\r\n\r\nI would appreciate any help as regards this problem.\r\n\"", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13090, "title": "Session creation silently failing on iOS when loading a SavedModel", "body": "## Issue:\r\n\r\nI am trying to integrate a TensorFlow solution into my iOS apps, but inference doesn't seem to work when I try to run simple graphs created in Python. In fact, the `tensorflow_inception_graph` from the examples is the only graph that seems to work with iOS inference. Every other inference attempt is met with the following error:\r\n\r\n> Invalid argument: Session was not created with a graph before Run()!\r\n\r\nSo what I'm finding is that on mobile if we try to run a canned neural network like the `tensorflow_inception_graph`, inference works perfectly but if we try to run any kind of custom model like 1 + 1 = 2, the graph won't run.\r\n\r\nTo demonstrate this, I used Python to write and export to a `SavedModel` the simplest graph I can think of: it just adds 1 + 1:\r\n\r\n```\r\ng = tf.Graph()\r\nwith g.as_default():\r\n    output = tf.add(tf.constant(1), 1, name=\"output_tensor\")\r\n\r\nbuilder = tf.saved_model.builder.SavedModelBuilder('/path/to/export')\r\nwith tf.Session(graph=g) as sess:\r\n    builder.add_meta_graph_and_variables(sess, tags=[])\r\n\r\nbuilder.save()\r\n```\r\n\r\nNow to test that inference is actually possible, I reload the `saved_model.pb` into Python:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    tf.saved_model.loader.load(sess, [], '/path/to/export')\r\n\r\n    print(sess.run('output_tensor:0')) // prints 2\r\n```\r\n\r\nFollowing in the footsteps of the `simple` example located at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios, I go to perform inference on iOS using the static C++ library built using the makefile and the instructions provided at the link.\r\n\r\n```\r\nusing namespace tensorflow;\r\nusing namespace std;\r\nusing namespace ::google::protobuf;\r\nusing namespace ::google::protobuf::io;\r\n\r\nSessionOptions options;\r\nSession* session_pointer = nullptr;\r\nStatus session_status = NewSession(options, &session_pointer);\r\n\r\ncout << session_status.ToString() << endl; // prints OK\r\n\r\nunique_ptr<Session> session(session_pointer);\r\n    \r\nGraphDef model_graph;\r\nNSString* model_path = FilePathForResourceName(@\"saved_model\", @\"pb\");\r\nPortableReadFileToProto([model_path UTF8String], &model_graph);\r\n    \r\nStatus session_init = session->Create(model_graph);\r\n\r\ncout << session_init.ToString() << endl; // prints OK, proves the graph was indeed created before run\r\n\r\nvector<Tensor> outputs;\r\nStatus session_run = session->Run({}, {\"output_tensor:0\"}, {}, &outputs);\r\n\r\ncout << session_run.ToString() << endl; // prints Invalid argument: Session was not created with a graph before Run()!\r\n```\r\n\r\n## Troubleshooting:\r\n\r\nI've tried rebuilding multiple times from `1.3.0` (latest, nightly), `1.2.1` and tried using the `TensorFlow-experimental` Pod. (I also tried using the `TensorFlow-iOS` pod, but it seems to be empty.)\r\n\r\nThere are (unanswered) Stack Overflow questions that refer to this issue occurring on both platforms: \r\n\r\n- **Android**: https://stackoverflow.com/questions/41252122/tensorflow-on-android-error-during-inference-invalid-argument-session-was-not\r\n\r\n- **iOS**: https://stackoverflow.com/questions/46201109/inference-error-with-tensorflow-c-on-ios-invalid-argument-session-was-not-c\r\n\r\nOn GitHub, there's been several issues related to this reported over the last year: [#7088](https://github.com/tensorflow/tensorflow/issues/7088), [#5553](https://github.com/tensorflow/tensorflow/issues/5553), [#3480](https://github.com/tensorflow/tensorflow/issues/3480), [#6806](https://github.com/tensorflow/tensorflow/issues/6806), and [#3352](https://github.com/tensorflow/tensorflow/issues/3352). None of the resolutions (if any) for these issues provide a concrete answer of how to get around this problem. \r\n\r\nHowever, found in the comments of [#3480](https://github.com/tensorflow/tensorflow/issues/3480) , @petewarden's [post](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/) about using `optimize_for_inference`, `quantize_graph`, and `convert_graphdef_memmapped_format` on the model before running inference present a possible solution. So I ran those three commands and got the same error:\r\n\r\n> google.protobuf.message.DecodeError: Truncated message.\r\n\r\nSince we can perform successful inference on the model in Python (as shown above), the possibility of the model being corrupt (as @petewarden suggested in [#3002](https://github.com/tensorflow/tensorflow/issues/3002)) is ruled out. Therefore, the above _\"Truncated Error\"_ message may point to where the problem is on mobile: Unsuccessful parsing of general models stored in protobuf files.\r\n\r\n## Closing thoughts on issue:\r\nThe point is this: The error message _\"Invalid argument: Session was not created with a graph before Run()!\"_ doesn't provide any information to me regarding what should be debugged in my implementation. Additionally, it appears to not even point to the source of the error as I have verified three things:\r\n\r\n1. All arguments passed to the session and graph are valid (_Invalid Argument_)\r\n2. Session was created with a graph before Run() and the `Status` was reported to be `OK` (_Session was not created with a graph before Run()!_)\r\n3. The error is likely triggered due to unsuccessful parsing on mobile devices of custom models serialized in protobuf files\r\n\r\n## Library Versions:\r\n**TensorFlow Python version**: `('v1.3.0-rc2-20-g0787eee', '1.3.0')`\r\n**TensorFlow C++ version**: 1.3.0, built using `build_all_ios.sh`. Also tried with version 1.2.1 and `TensorFlow-experimental` pod\r\n\r\n## System information:\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.1.0.post1)\r\ntensorflow (1.3.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n", "comments": ["I think the problem is that you're trying to load a file in the `SavedModel` format into a `GraphDef` protobuf message, and this is silently failing. While a `SavedModel` may contain one or more `GraphDef` messages, they are not equivalent.\r\n\r\n---\r\n\r\nBased on a quick look at the [`DirectSession::Create()`](https://github.com/tensorflow/tensorflow/blob/6e7539b8bc7ea1c921ab1865d202e543b80ef539/tensorflow/core/common_runtime/direct_session.cc#L368) code, the behavior you're seeing could be explained if `model_graph` were **empty** after this line:\r\n\r\n```objc\r\nPortableReadFileToProto([model_path UTF8String], &model_graph);\r\n```\r\n\r\nCan you try the following two things?\r\n\r\n1. Can you check whether or not `model_graph` contains any nodes after you read the file? Printing `model_graph.node_size()` and checking whether it is greater than 0 should suffice.\r\n2. From the code it looks like [`PortableReadFileToProto()`](https://github.com/tensorflow/tensorflow/blob/6e7539b8bc7ea1c921ab1865d202e543b80ef539/tensorflow/examples/ios/camera/tensorflow_utils.mm#L86) returns a `bool` to indicate success or failure. Can you check its value?", "Yes, it does look like the session creation is silently failing. I modified the code as follows:\r\n\r\n```\r\nGraphDef model_graph;\r\nNSString* model_path = FilePathForResourceName(@\"saved_model\", @\"pb\");\r\nbool success = PortableReadFileToProto([model_path UTF8String], &model_graph);\r\n    \r\ncout << \"PortableReadFileToProto bool: \" << success << endl;\r\n    \r\ncout << \"Number of nodes in model_graph: \" << model_graph.node_size() << endl;\r\n    \r\nStatus session_init = session->Create(model_graph);\r\n    \r\ncout << \"Session creation Status: \" << session_init.ToString() << endl;\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\nPortableReadFileToProto bool: 0\r\nNumber of nodes in model_graph: 0\r\nSession creation Status: OK\r\n```\r\n\r\nThis indicates certain failure to parse the protobuf file, but session creation is being reported as succeeded.\r\n\r\nIs there a C++/iOS equivalent to `tf.saved_model.loader.load`?", "It's valid to create a session with an empty graph, because you can subsequently call `Extend()` on it.\r\n\r\nThere's a C API for loading a session from a SavedModel, called [`TF_LoadSessionFromSavedModel()`](https://github.com/tensorflow/tensorflow/blob/66eed3646677dc7a8c8e94cd1936b45f6cb3609b/tensorflow/c/c_api.cc#L2223). Note that the C API's `TF_Session` is slightly different from the C++ API's `tensorflow::Session`, but they support the same operations. To use the C API, you'd need to modify your `Session::Run()` call to use [`TF_SessionRun()`](https://github.com/tensorflow/tensorflow/blob/66eed3646677dc7a8c8e94cd1936b45f6cb3609b/tensorflow/c/c_api.h#L1198) instead. Alternatively, the implementatoin of `TF_LoadSessionFromSavedModel()` is fairly simple, and you might be able to make the same sequence of calls against the C++ `Session` API."]}, {"number": 13089, "title": "Fix incorrect error message in `conv2d_transpose`", "body": "This fix fixes the incorrect error message in `conv2d_transpose` where `value.get_shape()[3]` should be changed to `value.get_shape()[axis]` for `input_depth` in `NCHW` format.\r\n\r\nThe incorrect error message could be seen from the following where `4 != 4` is quite confusing:\r\n\r\n```\r\nubuntu@ubuntu:~$ cat v.py\r\nimport tensorflow as tf\r\n\r\nstrides = [1, 1, 1, 1]\r\ninput_shape = [2, 3, 6, 4]\r\noutput_shape = [2, 2, 6, 4]\r\nfilter_shape = [3, 3, 2, 4]\r\n\r\ninput = tf.constant(1.0, shape=input_shape, name=\"input\", dtype=tf.float32)\r\nfilter = tf.constant(1.0, shape=filter_shape, name=\"filter\", dtype=tf.float32)\r\n\r\noutput = tf.nn.conv2d_transpose(input, filter, output_shape, strides=strides, padding=\"SAME\", data_format=\"NCHW\")\r\nubuntu@ubuntu:~$\r\nubuntu@ubuntu:~$\r\nubuntu@ubuntu:~$\r\nubuntu@ubuntu:~$\r\nubuntu@ubuntu:~$ python v.py\r\nTraceback (most recent call last):\r\n  File \"v.py\", line 16, in <module>\r\n    output = tf.nn.conv2d_transpose(input, filter, output_shape, strides=strides, padding=\"SAME\", data_format=\"NCHW\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1026, in conv2d_transpose\r\n    )[3]))\r\nValueError: input channels does not match filter's input channels, 4 != 4\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @jhseu to be potential reviewers.", "Jenkins, test this please.", "Could you pull rebase?", "Thanks @drpngx  for the review. The PR is actually already against the HEAD of the current master. Maybe a rerun will fix the `Android Demo App` issue?", "OK, that's odd. Let's try again.\r\n\r\nJenkins, test this please.", "Yay! All passed. Thank you for your PRs over the past month, BTW, much appreciated."]}, {"number": 13088, "title": "Merge pull request #1 from tensorflow/master", "body": "updating from original", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13087, "title": "Deallocation messages broken", "body": "The only way to calculate peak memory in OSS TensorFlow is to make memory timeline by parsing allocation/deallocation messages (possibly using helpers like [this](github.com/yaroslavvb/memory_util))\r\n\r\nSometime between TF 1.0.1 and TF 1.1, the deallocation messages stopped including allocation id, so you can't track when each tensor got deallocated. \r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = ''\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '1'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsess = tf.Session()\r\na = tf.ones((1000,1000))\r\nsess.run(a)\r\n```\r\n\r\nIn TF 1.0.1 you see this\r\n`2017-09-16 13:24:28: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 1 allocator_name: \"cpu\" }\r\n`\r\n\r\nIn latest TF you see this\r\n\r\n`2017-09-16 13:25:04.422108: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }\r\n`\r\n\r\nNote that deallocation id is missing in later version\r\n\r\ncc @panyx0718 who mentioned ongoing work for tracking memory deallocation", "comments": ["BTW, this is only for CPU memory deallocation messages, GPU deallocation messages continue to work", "Fields are only printed if they are not zero, so if it doesn't appear, it's zero.\r\n\r\nTo see this, note the protobuf is printed at [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/log_memory.cc#L36). `ProtoShortDebugString` is defined in a generated file, which you can see after compiling tensorflow with `less bazel-genfiles/tensorflow/core/framework/log_memory.pb_text.cc`. That file has the function\r\n\r\n```c++\r\nvoid AppendProtoDebugString(\r\n    ::tensorflow::strings::ProtoTextOutput* o,\r\n    const ::tensorflow::MemoryLogTensorDeallocation& msg) {\r\n  o->AppendNumericIfNotZero(\"allocation_id\", msg.allocation_id());\r\n  o->AppendStringIfNotEmpty(\"allocator_name\", ProtobufStringToString(msg.allocator_name()));\r\n}\r\n```"]}, {"number": 13086, "title": "Help completely removing Tensorflow, pip and virtualenv", "body": "Hey there,\r\n\r\nI am new to all the tf and Python programming and have installed tensorflow through pip and virtualenv but now I read that in order to use Spyder for Python it is best to use Anaconda. I know that tf can be installed through conda as well but how do I go about it now? Do I have to completely remove the existing installations first and if yes, can someone explain in detail which and how I can do it?\r\n\r\nThanks in advance", "comments": ["This doesn't seem to be a bug in tensorflow, it's better q for stackoverflow"]}, {"number": 13085, "title": "Update `tf.nn.in_top_k` to use v2 kernel.", "body": "This fix is a follow up to PR #11197 so that `tf.nn.in_top_k` uses V2 kernel now.\r\n\r\nThis fix follows the API compatibility workflow (3 weeks).\r\n\r\nThis fix also updates related tests so that `gen_nn_ops` could be removed.\r\n\r\nThis fix fixes #9717.\r\n\r\nNOTE: In the original PR #11197, the `_in_top_kv2` was incorrectly called even the comments specified to wait for 3 weeks. That was fixed by commit https://github.com/tensorflow/tensorflow/commit/3e3eebd6adb3512b69e28b6e7acce9a59527f699\r\n(Thanks @vrv for the fix).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@vrv what do you think?", "Jenkins, test this please.", "/CC: @gunan the windows jenkins appears to be broken.\r\n\r\nI haven't seen the `coordinator` error before, assuming it's some rare flakiness.\r\n\r\nJenkins, test this please.", "@gunan, scratch that, windows build appears to be back online. thanks!"]}, {"number": 13084, "title": "Branch 168957558", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13083, "title": "mismatch between the reported values of tt.nn.zero_fraction", "body": "Has anyone run into this issue before? \r\n\r\n[tt.nn.zero_fraction](https://www.tensorflow.org/api_docs/python/tf/nn/zero_fraction), defined in [nn_imply.py](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/nn_impl.py) reports my convs layers in [MobileNet_v1 slim](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py) net have around 0.5 sparsity (conv1-conv13). However, when I freeze the model's weights and look at them, there is none zero values:\r\n\r\n```\r\n>>>conv=sess.run('MobilenetV1/Logits/Conv2d_1c_1x1/weights/read:0')\r\n>>>numpy.count_nonzero(conv)\r\n5120\r\n>>>numpy.count_nonzero(conv==0)\r\n0\r\n\r\n```\r\n\r\nLooking at the histogram, there are many weights' values close to zero. Does that function do approximation? Can anyone verify this?", "comments": ["I can not see your reply in that thread. Have you deleted it?\n\nOn Sat, Sep 16, 2017 at 8:40 PM, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> What the type of conv? I mean, type(conv)?\n>\n> In [7]: np.count_nonzero([1, 0, 0])\n> Out[7]: 1\n>\n> In [8]: np.count_nonzero([1, 0, 0] == 0)\n> Out[8]: 0\n>\n> In [9]: np.count_nonzero(np.array([1, 0, 0]) == 0)\n> Out[9]: 2\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13083#issuecomment-330003981>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIglHiv97hAplM6rV9TGDvXgRUM-HKzMks5sjGpxgaJpZM4PZ4ZL>\n> .\n>\n", "Yes, I misunderstand your question, @amirjamez .", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13082, "title": "How can I change value by threshold in tensor?", "body": "When I do some 2-class classify practices, I use sigmoid as output layer, and it return a value in [0,1], but I want 1 if the value greater than 0.5,else set the value 0. I can't find a function to finish it.  Thank you for help. \r\nJust like:\r\na=tf.constant([0.1,0.2,0.6,.0.7])--->turn to ---->a=tf.constant([0,0,1,1])\r\n", "comments": ["`tf.to_int32(a > 0.5)`", "You can make your customized activation function.\r\nFollow the link as a guide to do it.\r\n\r\nhttps://stackoverflow.com/questions/39921607/how-to-make-a-custom-activation-function-with-only-python-in-tensorflow\r\n", "Although @ppwwyyxx  and @shreyneil, seem to have already answered your question...\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13081, "title": "i m getting this error while installing tensorflow gpu ,have tried everything help me plezz", "body": "Traceback (most recent call last):\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"H:/tst.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n", "comments": ["Hi viveky444,\r\n  If you already installed anaconda ,try installing tensorflow with \r\n   conda install -c conda-forge tensorflow \r\n", "This is a duplicate of #13196. Please see that issue for more information.\r\n"]}, {"number": 13080, "title": "Incorrect protobuf SHA in workspace.bzl for v1.3.0", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.0 (the tagged version, specifically `9e76bf32`)\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **Exact command to reproduce**: `./configure` (with all the default answers, and the specified python version above), then `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\n\r\nWhen attempting to build the pip package in the repository on the latest tagged release (v1.3.0), I get the following fatal build error:\r\n\r\n`ERROR: /home/ubuntu/tensorflow/tools/pip_package/BUILD:100:1: no such package '@protobuf//': java.io.IOException: Error downloading [https://github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz, http://mirror.bazel.build/github.com/google/protobuf/archive/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz] to /home/ubuntu/.cache/bazel/_bazel_whitlock/8ea56cff279f39ec6c0003641e649819/external/protobuf/0b059a3d8a8f8aa40dde7bea55edca4ec5dfea66.tar.gz: Checksum was e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d but wanted 6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93 and referenced by '//tensorflow/tools/pip_package:licenses'`\r\n\r\nIt looks to me like the SHA changed for that version was not specified correctly. However, this seems strange to me because I had built against this version within the last couple of weeks without issue. `master` does not have this problem.\r\n\r\nReplacing the SHA in `tensorflow/workspace.bzl` with the one in the error message (`e5fdeee`...) solves this issue.", "comments": ["Same problem, same solution on branch r1.3 but with Ubuntu 16.04 and `bazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\ni replaced the sha checksum with vi\r\n`vi tensorflow/workspace.bzl\r\n:s/6d43b9d223ce09e5d4ce8b0060cb8a7513577a35a64c7e3dad10f0703bf3ad93/e5fdeee6b28cf6c38d61243adff06628baa434a22b5ebb7432d2a7fbabbdb13d/g`\r\n\r\nThank you @samwhitlock for sharing", "I think this is related to GitHub changed the download file with different digest. See #12986 for related issues.", "@gunan, PTAL. Thanks! ", "@av8ramit We should also patch master with your patch to 1.3", "Out of curiosity, when can we see a release that includes these updates?", "Once we have all the archives mirrored, we will look into a new release.\r\nFor now, deduplicating this issue in favor of #12979"]}, {"number": 13079, "title": "saved model load method support for android", "body": "I am trying load a saved model on android with java api.\r\n\r\n Session session = SavedModelBundle.load(modelDir, \"serve\").session();\r\n\r\n\r\nIts works on PC. But on android i  got this error message.\r\n\r\nE/tensorflow: CameraActivity: Exception!\r\n                                                                 java.lang.UnsupportedOperationException: Loading a SavedModel is not supported in Android. File a bug at https://github.com/tensorflow/tensorflow/issues if this feature is important to you\r\n                                                                     at org.tensorflow.SavedModelBundle.load(Native Method) \r\nmy reference model for training procedure is  tf estimator for iris data.\r\nhttps://www.tensorflow.org/get_started/estimator\r\n\r\n", "comments": ["@asimshankar do you have any comment on this feature.", "@nnkalita I'm facing the same issues,do you have some suggest?", "@SilentTTxo  You can freeze  the model with  TensorFlow freeze_graph tool  using --input_saved_model_dir  and then load the freezed graph on TensorFlow android  interference code.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Yeah, saved models aren't yet supported on Android mostly because the libraries they depend on are excluded from the Android binary to reduce binary size / dependencies.\r\n\r\nThis is a reasonable feature to work through. Though, the workaround suggested and/or use of [TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/) should be considered as well.\r\n\r\nMarking this as \"Contributions Welcome\" in case anyone wants to take this one (it will mostly involve `BUILD` file management to keep the dependencies to a minimum)."]}, {"number": 13078, "title": "quantized_conv3d and quantized_pool3d features", "body": "Now I am trying to implement 3D CNN on FPGA.\r\nDo you have plan to support quantized_conv3d and quantized_pool3d features these two months?\r\nif yes, when will it be released?\r\n\r\nCheers~\r\n", "comments": ["Unlikely in the short/midterm.\r\n"]}]