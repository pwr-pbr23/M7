[{"number": 48663, "title": "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS focal (kernel 5.4.0-70-generic)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.3 tag\r\n- Python version: Python 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): Bazel 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: cuda 10.2, cuDNN 7.6\r\n- GPU model and memory: Nvidia GeForce GTX 760 4gb\r\n\r\n**Describe the problem**\r\n\r\nI have old GPU (I don't know - 5 years old - that's old? GTA 5 and tons of games running without lags) and it supports Cuda capability is 3.0 only(Kepler core). Binary TensorFlow starts from 3.5, but I read on forums that if I'll compile TensorFlow I can get 3.0.\r\n\r\nI allready solved tons of problems, including patching source code. (here is my wiki where I explained every step http://wiki.aleksashkin.net/wiki/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B8_%D0%91%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D0%B5_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D0%B5#Installation)\r\n\r\nI've tried:\r\n\r\n- Install other numpy version but got this error ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ntensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.20.0 which is incompatible.\r\n- Install binary TensorFlow 2.3 to have all dependecies.\r\n- Remove binary\r\n\r\nBut this problem I can't solve. Please help me. GPUs are very expensive today and I want to use all possibilities.\r\n\r\nThank you!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r2.3\r\n./configure# + CUDA + TensorRT\r\nbazel build --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/neon_check.h:25:0,\r\n                 from ./tensorflow/lite/kernels/internal/common.h:28,\r\n                 from ./tensorflow/lite/toco/runtime/types.h:18,\r\n                 from ./tensorflow/lite/toco/model.h:30,\r\n                 from ./tensorflow/lite/toco/tooling_util.h:32,\r\n                 from tensorflow/lite/toco/tooling_util.cc:15:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:14262:32: warning: \u2018int64x2_t vqdmlsl_s32(int64x2_t, int32x2_t, int32x2_t)\u2019 is deprecated: The function may be very slow due to the serial implementation, please try to avoid it [-Wdeprecated-declarations]\r\n     return vqdmlsl_s32(a, b, vc);\r\n                                ^\r\nIn file included from ./tensorflow/lite/kernels/internal/optimized/neon_check.h:25:0,\r\n                 from ./tensorflow/lite/kernels/internal/common.h:28,\r\n                 from ./tensorflow/lite/toco/runtime/types.h:18,\r\n                 from ./tensorflow/lite/toco/model.h:30,\r\n                 from ./tensorflow/lite/toco/tooling_util.h:32,\r\n                 from tensorflow/lite/toco/tooling_util.cc:15:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:5003:58: note: declared here\r\n _NEON2SSE_INLINE _NEON2SSE_PERFORMANCE_WARNING(int64x2_t vqdmlsl_s32(int64x2_t a, int32x2_t b, int32x2_t c), _NEON2SSE_REASON_SLOW_SERIAL)\r\n                                                          ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:76:109: note: in definition of macro \u2018_NEON2SSE_PERFORMANCE_WARNING\u2019\r\n     #define _NEON2SSE_PERFORMANCE_WARNING(function, explanation)   __attribute__((deprecated(explanation))) function\r\n                                                                                                             ^~~~~~~~\r\nINFO: From Compiling tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc [for host]:\r\nIn file included from external/gemmlowp/public/../internal/../fixedpoint/fixedpoint.h:907:0,\r\n                 from external/gemmlowp/public/../internal/output.h:27,\r\n                 from external/gemmlowp/public/../internal/unpack.h:23,\r\n                 from external/gemmlowp/public/../internal/single_thread_gemm.h:29,\r\n                 from external/gemmlowp/public/../internal/multi_thread_gemm.h:27,\r\n                 from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,\r\n                 from external/gemmlowp/public/gemmlowp.h:19,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_context.h:21,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/sse_tensor_utils_impl.h:20,\r\n                 from tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc:15:\r\nexternal/gemmlowp/public/../internal/../fixedpoint/./fixedpoint_sse.h:47:39: warning: ignoring attributes on template argument \u2018__m128i {aka __vector(2) long long int}\u2019 [-Wignored-attributes]\r\n struct FixedPointRawTypeTraits<__m128i> {\r\n                                       ^\r\nIn file included from external/gemmlowp/public/../internal/simd_wrappers.h:664:0,\r\n                 from external/gemmlowp/public/../internal/output.h:29,\r\n                 from external/gemmlowp/public/../internal/unpack.h:23,\r\n                 from external/gemmlowp/public/../internal/single_thread_gemm.h:29,\r\n                 from external/gemmlowp/public/../internal/multi_thread_gemm.h:27,\r\n                 from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,\r\n                 from external/gemmlowp/public/gemmlowp.h:19,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_context.h:21,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/sse_tensor_utils_impl.h:20,\r\n                 from tensorflow/lite/kernels/internal/optimized/sse_tensor_utils.cc:15:\r\nexternal/gemmlowp/public/../internal/simd_wrappers_sse.h:31:72: warning: ignoring attributes on template argument \u2018gemmlowp::Int32x4 {aka __vector(2) long long int}\u2019 [-Wignored-attributes]\r\n       typename std::conditional<ScalarCount >= 4, Int32x4, std::int32_t>::type;\r\n                                                                        ^\r\nexternal/gemmlowp/public/../internal/simd_wrappers_sse.h:37:72: warning: ignoring attributes on template argument \u2018gemmlowp::Int16x8 {aka __vector(2) long long int}\u2019 [-Wignored-attributes]\r\n       typename std::conditional<ScalarCount >= 8, Int16x8, std::int16_t>::type;\r\n                                                                        ^\r\nexternal/gemmlowp/public/../internal/simd_wrappers_sse.h:45:52: warning: ignoring attributes on template argument \u2018gemmlowp::Uint8x16 {aka __vector(2) long long int}\u2019 [-Wignored-attributes]\r\n                                 std::uint8_t>::type>::type;\r\n                                                    ^\r\nERROR: /home/artem/ml/tensorflow/tensorflow/python/keras/api/BUILD:137:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n2021-04-02 08:03:25.855138: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\nImportError: numpy.core._multiarray_umath failed to import\r\nImportError: numpy.core.umath failed to import\r\n2021-04-02 08:03:26.277871: F tensorflow/python/lib/core/bfloat16.cc:705] Check failed: PyBfloat16_Type.tp_base != nullptr \r\n/bin/bash: line 1: 768875 Aborted                 bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2 --apidir=bazel-out/k8-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2 --loading=default --package=tensorflow.python,tensorflow.python.keras,tensorflow.python.keras.activations,tensorflow.python.keras.applications.densenet,tensorflow.python.keras.applications.efficientnet,tensorflow.python.keras.applications.imagenet_utils,tensorflow.python.keras.applications.inception_resnet_v2,tensorflow.python.keras.applications.inception_v3,tensorflow.python.keras.applications.mobilenet,tensorflow.python.keras.applications.mobilenet_v2,tensorflow.python.keras.applications.nasnet,tensorflow.python.keras.applications.resnet,tensorflow.python.keras.applications.resnet_v2,tensorflow.python.keras.applications.vgg16,tensorflow.python.keras.applications.vgg19,tensorflow.python.keras.applications.xception,tensorflow.python.keras.backend,tensorflow.python.keras.backend_config,tensorflow.python.keras.callbacks,tensorflow.python.keras.callbacks_v1,tensorflow.python.keras.constraints,tensorflow.python.keras.datasets.boston_housing,tensorflow.python.keras.datasets.cifar10,tensorflow.python.keras.datasets.cifar100,tensorflow.python.keras.datasets.fashion_mnist,tensorflow.python.keras.datasets.imdb,tensorflow.python.keras.datasets.mnist,tensorflow.python.keras.datasets.reuters,tensorflow.python.keras.engine.base_layer,tensorflow.python.keras.engine.data_adapter,tensorflow.python.keras.engine.input_layer,tensorflow.python.keras.engine.input_spec,tensorflow.python.keras.engine.sequential,tensorflow.python.keras.engine.training,tensorflow.python.keras.estimator,tensorflow.python.keras.feature_column.sequence_feature_column,tensorflow.python.keras.initializers,tensorflow.python.keras.initializers.initializers_v1,tensorflow.python.keras.initializers.initializers_v2,tensorflow.python.keras.layers.advanced_activations,tensorflow.python.keras.layers.convolutional,tensorflow.python.keras.layers.convolutional_recurrent,tensorflow.python.keras.layers.core,tensorflow.python.keras.layers.cudnn_recurrent,tensorflow.python.keras.layers.dense_attention,tensorflow.python.keras.layers.embeddings,tensorflow.python.keras.layers.local,tensorflow.python.keras.layers.merge,tensorflow.python.keras.layers.noise,tensorflow.python.keras.layers.normalization,tensorflow.python.keras.layers.normalization_v2,tensorflow.python.keras.layers.preprocessing,tensorflow.python.keras.layers.pooling,tensorflow.python.keras.layers.recurrent,tensorflow.python.keras.layers.recurrent_v2,tensorflow.python.keras.layers.serialization,tensorflow.python.keras.layers.wrappers,tensorflow.python.keras.losses,tensorflow.python.keras.metrics,tensorflow.python.keras.mixed_precision.experimental.get_layer_policy,tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer,tensorflow.python.keras.mixed_precision.experimental.policy,tensorflow.python.keras.models,tensorflow.python.keras.optimizer_v2.adadelta,tensorflow.python.keras.optimizer_v2.adagrad,tensorflow.python.keras.optimizer_v2.adam,tensorflow.python.keras.optimizer_v2.adamax,tensorflow.python.keras.optimizer_v2.ftrl,tensorflow.python.keras.optimizer_v2.gradient_descent,tensorflow.python.keras.optimizer_v2.learning_rate_schedule,tensorflow.python.keras.optimizer_v2.nadam,tensorflow.python.keras.optimizer_v2.optimizer_v2,tensorflow.python.keras.optimizer_v2.rmsprop,tensorflow.python.keras.optimizers,tensorflow.python.keras.premade.linear,tensorflow.python.keras.premade.wide_deep,tensorflow.python.keras.preprocessing.image,tensorflow.python.keras.preprocessing.sequence,tensorflow.python.keras.preprocessing.text,tensorflow.python.keras.regularizers,tensorflow.python.keras.saving.model_config,tensorflow.python.keras.saving.save,tensorflow.python.keras.saving.saved_model_experimental,tensorflow.python.keras.utils.data_utils,tensorflow.python.keras.utils.generic_utils,tensorflow.python.keras.utils.io_utils,tensorflow.python.keras.utils.layer_utils,tensorflow.python.keras.utils.losses_utils,tensorflow.python.keras.utils.multi_gpu_utils,tensorflow.python.keras.utils.np_utils,tensorflow.python.keras.utils.vis_utils,tensorflow.python.keras.wrappers.scikit_learn --output_package=tensorflow.python.keras.api._v2 --use_relative_imports=True bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/efficientnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/imagenet_utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet_v2/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/premade/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/k8-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/artem/ml/tensorflow/tensorflow/python/tools/BUILD:282:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Aborted): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nINFO: Elapsed time: 21814.590s, Critical Path: 340.21s\r\nINFO: 11206 processes: 11206 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["INFO: From Executing genrule //tensorflow/python/keras/api:keras_python_api_gen:\r\n2021-04-22 03:01:58.501826: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nINFO: From Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1:\r\n2021-04-22 03:01:58.505888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nINFO: From Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2:\r\n2021-04-22 03:01:58.501818: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nINFO: From Executing genrule //tensorflow:tf_python_api_gen_v2:\r\n2021-04-22 03:01:58.601217: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 338.340s, Critical Path: 32.48s\r\nINFO: 203 processes: 203 local.\r\nINFO: Build completed successfully, 252 total actions\r\n\r\n\r\n**Hold on, looks like numpy 1.20 works**", "@alexartwww  Also, please try with Tensorflow version 2.4 and let us know if it solves your TF installation as well. Thanks!", "artem@ThinkPad-X230:~/ml$ pip install /tmp/tensorflow_pkg/tensorflow-2.3.2-cp38-cp38-linux_x86_64.whl\r\nDefaulting to user installation because normal site-packages is not writeable\r\nProcessing /tmp/tensorflow_pkg/tensorflow-2.3.2-cp38-cp38-linux_x86_64.whl\r\nRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (1.32.0)\r\nCollecting numpy<1.19.0,>=1.16.0\r\n  Using cached numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\r\nRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (0.3.3)\r\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (1.12.1)\r\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (2.10.0)\r\nRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (0.11.0)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (3.3.0)\r\nRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (0.2.0)\r\nRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (1.6.3)\r\nRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/artem/.local/lib/python3.8/site-packages (from tensorflow==2.3.2) (2.3.0)\r\nRequirement already satisfied: six>=1.12.0 in /home/artem/.local/lib/python3.8/site-packages (from tensorflow==2.3.2) (1.15.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (1.1.0)\r\nRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (3.14.0)\r\nRequirement already satisfied: tensorboard<3,>=2.3.0 in /home/artem/.local/lib/python3.8/site-packages (from tensorflow==2.3.2) (2.4.1)\r\nRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (1.1.2)\r\nRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.3.2) (0.36.2)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.0.1)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (0.4.2)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.3.3)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /home/artem/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (2.21.0)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.24.0)\r\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (51.0.0)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.7.0)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (0.2.8)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (4.6)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (4.2.0)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (0.4.8)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/artem/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.0.4)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (2020.12.5)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /home/artem/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (1.24.3)\r\nRequirement already satisfied: idna<2.9,>=2.5 in /home/artem/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (2.8)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.2) (3.1.0)\r\nInstalling collected packages: numpy, tensorflow\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.20.0\r\n    Uninstalling numpy-1.20.0:\r\n      Successfully uninstalled numpy-1.20.0\r\nSuccessfully installed numpy-1.18.5 tensorflow-2.3.2\r\nartem@ThinkPad-X230:~/ml$ python\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-04-23 09:43:50.420927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\n>>> tf.__version__\r\n'2.3.2'\r\n>>> tf.config.list_physical_devices(\"GPU\")\r\n2021-04-23 09:44:20.188655: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-04-23 09:44:20.260895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-23 09:44:20.261514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 760 computeCapability: 3.0\r\ncoreClock: 1.15GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 179.05GiB/s\r\n2021-04-23 09:44:20.262097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\r\n2021-04-23 09:44:20.399953: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-04-23 09:44:20.476808: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-04-23 09:44:20.501241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-04-23 09:44:20.633416: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-04-23 09:44:20.664516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-04-23 09:44:20.928278: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-04-23 09:44:20.928661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-23 09:44:20.929570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-23 09:44:20.930276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1812] Ignoring visible gpu device (device: 0, name: GeForce GTX 760, pci bus id: 0000:04:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n[]\r\n\r\n**3.5 limit is still there. I can't use my GPU. :( maybe it's hasrcoded in source code...**", "@alexartwww,\r\nPlease take a look at these comments [link #1](https://github.com/tensorflow/tensorflow/issues/46653#issuecomment-805062910), [link #2](https://github.com/tensorflow/tensorflow/issues/27505#issuecomment-490768957) from the members of the TensorFlow team and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48663\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48663\">No</a>\n"]}, {"number": 48660, "title": "Why enforcing \"Make sure all arrays contain the same number of samples.\"", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n-   **Python version**: 3.7.5\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIf my inputs is like\r\n```\r\nx = {\r\n  \"x1\": tf.ones([4, 6]),\r\n  \"x2\": tf.ones([10])\r\n}\r\ny = tf.ones([4])\r\n```\r\n\r\nthen in `model.fit(x, y)`, there will be an error \r\n```\r\nValueError: Data cardinality is ambiguous:\r\n  x sizes: 4, 10\r\n  y sizes: 4\r\nMake sure all arrays contain the same number of samples.\r\n```\r\n\r\nwhy we need to enforce this? In my problem by nature the inputs are like that, and inside my model I have specially handlings to make sure it can correctly recognize which element in `x2` belong to which row in a batch.\r\n\r\nplus if we delete the `_check_data_cardinality` function, the actual `x2` that fed into the model is wrong, it is truncated to have only 4 elements and is not even the first 4 elements\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@blackyang \r\nPlease take a look at #48256 . I have explained why this is the case. Please close this issue if your query is resolved.\r\nThanks.", "@blackyang  We call fit(), which will train the model by slicing the data into \"batches\" of size batch_size, and repeatedly iterating over the entire dataset for a given number of epochs. so, it is not possible to train on different sized datasets that go into a single model. \r\n\r\nHope that  clarifies your question. Thanks!", "Related to https://github.com/tensorflow/tensorflow/issues/42280 as well, you can either create your own tf.dataset beforehand (the check does not exists in from_generator ) and use it in the .fit or .predict. or call __call__ of the model directly (no check either) with a custom training loop :( (overriding train_on_batch wont work neither).\r\n\r\nI agree with you but I think that check was added because most users dont use those tricks: it could be very nice to authorize to remove those checks or make them a warning instead of an assert. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@tanguycdls thanks for your reply! Yes currently I am building a tf.dataset beforehead to avoid this issue. This also indicate why this is a bug: inconsistent behavior between using tf.dataset and not\r\n\r\n@saikumarchalla @AdityaKane2001 in each batch, we should be allowed to have features of different sizes / lengths, as long as the model can correctly recognize which element belongs to which batch. For example, if a batch_size is 16, for scalar feature the size will be 16; for features with var-length, we currently flat it , and then within the model we call `tf.RaggedTensor.from_value_rowids`. My point is that keras shouldn't enforce such constrain, it's the customer's job to make sure the model can recognize the batch info", "@blackyang Is this still an issue for you?  Is there any actionable item like raising PR? \r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48660\">No</a>\n"]}, {"number": 48659, "title": "Update micro_speech mbed generation for disco_f746ng", "body": "This PR updates the mbed project generation for the micro_speech example for the disco_f746ng board. @advaitjain ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Addressed your comments @advaitjain "]}, {"number": 48658, "title": "Updated the tensorflow tech docs grammatically", "body": "I have made some small grammatical updates to the tensorflow tech docs.", "comments": []}, {"number": 48657, "title": "How to Modify and Debug the TFLiteConverter", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): built from source\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 0d1805aede03d25aa9d49adcef6903535fa5ad14\r\n\r\n### 2. Code\r\n\r\nModify `tensorflow/compiler/mlir/lite/tests/end2end/add.pbtxt` to [this](https://drive.google.com/file/d/1UxTnG9kQ_SJSDQ7_1mZAttUTlGAY5Gxo/view?usp=sharing) (basically just change the data type from `DT_INT32` to `DT_FLOAT`), then try to convert `add.pbtxt` to a fully quantized `.tflite` model using the following command:\r\n```\r\n./tf_tfl_translate \\\r\n  --tf-input-arrays=input0,input1 \\\r\n  --tf-input-shapes=4:4 \\\r\n  --tf-input-data-types=DT_FLOAT,DT_FLOAT \\\r\n  --tf-output-arrays=Add \\\r\n  --tf-inference-type=DT_QINT8 \\\r\n  --tf-input-min-values='-1,-1' \\\r\n  --tf-input-max-values='1,1' \\\r\n  add.pbtxt \\\r\n  -o output.tflite\r\n```\r\n\r\n### 3. Failure after conversion\r\nThe [output model](https://drive.google.com/file/d/1P-6V_wKkdcH4nAdM6Gl2Tft9FMNUE0Id/view?usp=sharing) `output.tflite` is not fully quantized even though `--tf-inference-type=DT_QINT8` is specified. Here is `output.tflite` when inspected with Netron:\r\n\r\n![add-netron](https://user-images.githubusercontent.com/19867281/115502332-5a1cb100-a2a7-11eb-89a2-2db9e3ee380d.png)\r\n\r\nFYI, [here](https://drive.google.com/file/d/1Ojz3v2mZ4O_nA2iHVpQ0us_0JAcgErL0/view?usp=sharing) is a log of the MLIR after each pass.\r\n\r\n### 4. Any other info / logs\r\nTo be honest, I'm not sure if I am using `tf_tfl_translate` correctly, so I would like to know if this is an issue with `tf_tfl_translate` or if there is another correct way to perform full-integer quantization on a TensorFlow model using `tf_tfl_translate`.\r\n\r\nCurrently, I'm trying to add an MLIR pass to the `TFLiteConverter` (more specifically, in the function `AddTFToTFLConversionPasses` in `tensorflow/compiler/mlir/lite/tf_tfl_passes.cc`), and I want to use `TFLiteConverter` to convert and perform full-integer quantization on a TensorFlow model while running the MLIR pass I added in the process. I want to avoid rebuilding the pip package every time I modify my pass, so I decided to use the internal tool `tf_tfl_translate`, which lead to this problem.\r\n\r\nIf there are any tips and tricks on how to modify the `TFLiteConverter` to add an MLIR pass and be able to debug it without having to rebuild the pip package every time, please let me know. Thanks for your time!\r\n\r\n\r\n\r\n", "comments": ["We don't recommend using `tf_tfl_translate` and that is not an official TensorFlow API.  Please use the TFLiteConverter API instead. Please refer to https://www.tensorflow.org/lite/performance/post_training_quantization for the quantization.\r\n\r\n", "For the pbtxt conversion, please use this path for your case. `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`", "Thanks for the help. I do know how to convert to `.tflite` models using the `TFLiteConverter`, but my problem was how I could add an MLIR pass to the converter such that I can modify and test the pass without rebuilding TensorFlow entirely just to use the python API. My attempt was to use `tf_tfl_translate`, but unfortunately it failed at performing full-integer quantization. I was wondering if it was a problem with `tf_tfl_translate`, or if I used `tf_tfl_translate` incorrectly, or if I can achieve my goal without using `tf_tfl_translate`. Thanks.", "Quantizer is a part of TF Python API. It looks hard to achieve that.", "Okay, so does this mean that if I want to add an MLIR pass to the `TFLiteConverter` and test it, I must rebuild the TensorFlow pip package entirely?", "The Bazel build environment supports building with the only required build targets and also builds the necessary stuffs for the TFLite converter.", "Just to make sure, do I run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` and let bazel figure out what should be rebuilt every time I modify my MLIR pass, or is there another target that just builds the `TFLiteConverter`?", "It is possible to create a Python program under the bazel development env., by depending on the //tensorflow/lite/python:lite build target.", "@RobertChenKFC \r\n\r\nplease confirm if the issue still persist", "Sorry for the late reply, I'm still figuring out how to make this work with my existing code (I'm not really familiar with bazel), but I think this method should work. I'll close this issue first, and reopen it if other issues occur. Thanks for the help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48657\">No</a>\n"]}, {"number": 48656, "title": "Merge 20210421", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48656) for more info**.\n\n<!-- need_sender_cla -->", "wrong pr"]}, {"number": 48655, "title": "[CherryPick:r2.5]Allow ProfilerFactory to return a nullptr ProfilerInterface", "body": "PiperOrigin-RevId: 368888650\nChange-Id: I6d76435f8dfb376c5c8108d778efbf11d2e2b939", "comments": []}, {"number": 48654, "title": "Fix keras.layers.ReLU docs #48646", "body": "threshold should not be negative because ReLU behavior does not match\r\nthe docs and it is not ReLU\r\n\r\n#48646", "comments": ["pylint errors (too long lines) fixed in https://github.com/tensorflow/tensorflow/commit/73bd1ec17354b9a7505d05398c31764e22909826"]}, {"number": 48653, "title": "[CherryPick:r2.5] Use ast instead of astunparse for python 3.9+.", "body": null, "comments": []}, {"number": 48652, "title": "bug while running bazel build", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Sid\r\n- TensorFlow installed from (source or binary): source r2.5\r\n- TensorFlow version: r2.5\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: pip from the system (python3-pip) no venv\r\n- Bazel version (if compiling from source): 3.7.2  (using bazelisk)\r\n- GCC/Compiler version (if compiling from source): gcc (Debian 10.2.1-6) 10.2.1 20210110\r\n- CUDA/cuDNN version: 11 / 8.1.1\r\n- GPU model and memory: NVIDIA GeForce GTX 1070\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen building tensorflow, bazel fails with the following message:\r\n```\r\nERROR: /home/nicolas/.cache/bazel/_bazel_nicolas/13cce9ab7cc322a6b7e908ff0026fedd/external/nccl_archive/BUILD.bazel:54:17: in _prune_relocatable_code rule @nccl_archive//:device_pruned:\r\nTraceback (most recent call last):\r\n        File \"/home/nicolas/.cache/bazel/_bazel_nicolas/13cce9ab7cc322a6b7e908ff0026fedd/external/local_config_nccl/build_defs.bzl\", line 207, column 15, in _prune_relocatable_code_impl\r\n                output.append(outputs)\r\nError: 'File' value has no field or method 'append'\r\n```\r\nAfter looking at the file build_defs.bzl and at this [page](https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/e07069218c39cbfc4bbad79fc50c83d64b0546af%5E%21/third_party/nccl/build_defs.bzl.tpl),\r\nI think that famous line 207 has a confusion between output and outputs. I tried swapping one for the other and it seems ok now. Bazel has started compiling at least. :-)\r\n\r\nI'm not sure if it is really a Tensorflow issue or where is this build_defs.bzl file hosted so I'm not sure where to ask or do a PR. Sorry if I\u1e3f the wrong place.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nbazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n", "comments": ["Adding @chsigg as that [page](https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/e07069218c39cbfc4bbad79fc50c83d64b0546af%5E%21/third_party/nccl/build_defs.bzl.tpl) seems to declare him as the author of that piece of code.", "@Gaasmann \r\nSorry for the delayed response, can you please let us know if you still face the issue.", "@Saduf2019 This seems to be a bug on the line I mentioned. I don't see new commits so I doubt the problem has resolved. I'll try to recompile nonetheless and let you know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Gaasmann \r\nPlease update, if we may move this to closed status if resolved.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48652\">No</a>\n", "I have the same error message while building release 2.5.0.", "@bermeitinger-b  You should use `git cherrypick c8e4f2aa633c4f9b803fdeb5d8463f002387a2bf` to apply that fix to r2.5 branch. \r\nSee #48874 for detail."]}, {"number": 48651, "title": "r2.5-rc2 cherry-pick request: [Intel MKL] Log a warning message when int8 is not supported", "body": "Goal: Improve user experience / reduce confusions.\r\n\r\nOriginal PR (master branch): #48578 \r\n\r\n> This PR logs a warning message to the Tensorflow customers in the following cases when OneDNN optimization does not\r\nsupport INT8 inference/training:\r\n>\r\n> (1) stock Tensorflow, with OneDNN optimizations enabled (in this case, INT8 is not supported at all.\r\n> \r\n> (2) Intel Optimized Tensorflow (aka, build with --config=mkl): in this case, the warning message reminds the user\r\nto set a proper environment variable before running INT8 inference/training.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48651) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes because all commits are from an already merged PR (#48578)."]}, {"number": 48650, "title": "Cannot allocate tf_uniform_replay_buffer to GPU - No registered 'ResourceScatterUpdate' OpKernel for 'GPU' devices compatible with node", "body": "**System information**\r\n\r\ntensorflow 2.4.1\r\ntf-agents 0.7.1\r\nNvidia RTX 2080 Ti\r\nWindows 10\r\nPython 3.8\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to run the TFUniformReplayBuffer on the GPU based on the example outlined here:\r\n\r\n[https://pathtopioneer.com/blog/2020/07/rl-3](https://pathtopioneer.com/blog/2020/07/rl-3)\r\n\r\nWhen adding device='/device:CPU:0' to the initalisation of the replay buffer everything works. When specifying GPU device='/device:GPU:0', an error occurs. It appears there is no GPU op for ResourceScatterUpdate. But how can this be fixed?\r\n\r\nSame error when running code on Colab with GPU enabled instead of my local machine.\r\n\r\n**Describe the expected behavior**\r\n\r\nShould give no error with GPU enabled.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n\r\n\r\nimport tensorflow as tf\r\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\r\nimport numpy as np\r\n\r\ndata_spec =  (\r\n        tf.TensorSpec([1], tf.float32, 'action'),\r\n        tf.TensorSpec([5], tf.float32, 'lidar'),\r\n        tf.TensorSpec([3, 2], tf.float32, 'camera')\r\n)\r\n\r\nbatch_size = 32\r\nmax_length = 1000\r\n\r\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n    data_spec,\r\n    batch_size=batch_size,\r\n    max_length=max_length,\r\n    device='/device:GPU:0') # when using device='/device:CPU:0') everything works\r\n\r\naction = tf.constant(1 * np.ones(\r\n    data_spec[0].shape.as_list(), dtype=np.float32))\r\nlidar = tf.constant(\r\n    2 * np.ones(data_spec[1].shape.as_list(), dtype=np.float32))\r\ncamera = tf.constant(\r\n    3 * np.ones(data_spec[2].shape.as_list(), dtype=np.float32))\r\n\r\nvalues = (action, lidar, camera)\r\nvalues_batched = tf.nest.map_structure(lambda t: tf.stack([t] * batch_size),\r\n                                       values)\r\n\r\nreplay_buffer.add_batch(values_batched)\r\n\r\n# Convert the replay buffer to a tf.data.Dataset and iterate through it\r\ndataset = replay_buffer.as_dataset(\r\n    sample_batch_size=4)\r\n\r\niterator = iter(dataset)\r\nprint(\"Iterator trajectories:\")\r\ntrajectories = []\r\nfor _ in range(3):\r\n  t, _ = next(iterator)\r\n  trajectories.append(t)\r\n\r\nprint(tf.nest.map_structure(lambda t: t.shape, trajectories))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nException has occurred: NotFoundError\r\nNo registered 'ResourceScatterUpdate' OpKernel for 'GPU' devices compatible with node {{node ResourceScatterUpdate}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: Tindices=DT_INT64, dtype=DT_INT64\r\n\t.  Registered:  device='CPU'; dtype in [DT_UINT64]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n [Op:ResourceScatterUpdate]\r\n  File \"C:\\Users\\Martin\\Documents\\GitHub\\rl-investing\\helper\\replay.py\", line 31, in <module>\r\n    replay_buffer.add_batch(values_batched)\r\n", "comments": ["@deeepwin  I can able to reproduce the issue using GPU for TF 2.4.1 where as the issue is fixed in tf-nightly version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/6c1e5b1a5eed9755998c7b3ed30c7485/untitled64.ipynb). Thanks!", "@saikumarchalla tested nightly build on my windows 10 machine (Python 3.7 and 3.8). But, still same error. On Colab it seems to work indeed. \r\n\r\nAlso tested on Ubuntu 20.04 with Python 3.7 on my local machine. Not working. What is different from Colab System? \r\n\r\n[local_machine.pdf](https://github.com/tensorflow/tensorflow/files/6358176/local_machine.pdf)\r\n\r\n\r\n", "@deeepwin  Could you please provide your cuda/cuddn version?", "@saikumarchalla \r\n\r\nOn Ubuntu:\r\n\r\ncuda: 11.3 (NVIDIA-SMI 465.19.01    Driver Version: 465.19.01)\r\ncudnn 8.1.1.33-1+cuda11.2\r\n\r\nOn Windows:\r\n\r\ncuda 11.2\r\ncudnn 8.1.1.33+cuda11.2\r\n\r\nAMD CPU Ryzen 9.\r\n\r\n", "@saikumarchalla \r\n\r\nI have install nightly but unfortunately no success. Could you check it on cuda with higher version or am I doing something wrong?\r\n\r\nCollecting tf-nightly\r\n  Downloading tf_nightly-2.6.0.dev20210427-cp37-cp37m-manylinux2010_x86_64.whl (455.6 MB)\r\n\r\n```\r\nxception has occurred: NotFoundError\r\nNo registered 'ResourceScatterUpdate' OpKernel for 'GPU' devices compatible with node {{node ResourceScatterUpdate}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: Tindices=DT_INT64, dtype=DT_INT64\r\n\t.  Registered:  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]\r\n...\r\ndevice='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n [Op:ResourceScatterUpdate]\r\n  File \"/home/martin/GitHub/rl-investing/tests/test_tf_uniform_replay_buffer.py\", line 38, in <module>\r\n    replay_buffer.add_batch(values_batched_tf)\r\n```", "Adding github [gist](https://colab.research.google.com/gist/ymodak/bf83ea272c11f180a3608a932e22dde1/untitled16.ipynb) that reproduces the reported error for next tensorflower.", "CC @atondwal", "@deeepwin does this unblock your usecase?", "@atondwal works on my local machine with the nightly build. Great thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48650\">No</a>\n"]}, {"number": 48649, "title": "tf.data.experimental.save", "body": "My GIT version and tf.version: unknown, 2.2.0\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to use [tf.data.experimental.save ](https://www.tensorflow.org/api_docs/python/tf/data/experimental/save)to save a tf Dataset\r\n**Describe the expected behavior**\r\nExpected behaviour is that the dataset should be saved\r\n**Standalone code to reproduce the issue**\r\nThe code is very simple:\r\n```\r\nimport tensorflow as tf\r\ndataset = tf.data.Dataset.range(2)\r\ntf.data.experimental.save(dataset, path)\r\n```\r\nThe error I got was:\r\n`AttributeError: module 'tensorflow._api.v2.data.experimental' has no attribute '                                                                                                             save'\r\n`\r\n", "comments": ["@arunraja-hub \r\nCan you please share a reproducible sample code? Thanks", "@AdityaKane2001 I have updated the code above", "@arunraja-hub \r\nThanks\r\n", "@arunraja-hub \r\n\r\n\r\nI ran the code shared on tf 2.4, tf nightly and did not get any error, please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/b08744ea1d950dc44703f6d71cdf11d8/-48649.ipynb) here.\r\nYou are facing this error because of the TF2.2 version.Can you please upgrade your tf version and let us know. Thanks\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@arunraja-hub \r\n\r\nCould you please confirm if the issue is fixed on above mentioned versions. if yes, please feel free to move this issue to closed status.\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48649\">No</a>\n"]}, {"number": 48648, "title": "Update bot_config.yml", "body": "Add myself as a compiler assignee.", "comments": []}, {"number": 48647, "title": "Softmax layer unexpected behaviour for axis=0", "body": "### Description:\r\nThe [Softmax layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax) returns all ones when axis=0 is used. Perhaps this behaviour is not wrong, but it is really confusing since it is not what the user would expect from this layer. \r\nIt would make sense to better explain in the documentation how the axis are numbered, especially since both negative and positive numbers can be used. Moreover, it might also make sense to add an error or warning message when axis=0 is used.\r\n\r\n\r\n### Usage example\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nmodel = keras.Sequential([\r\nkeras.layers.Softmax(axis=0, input_shape=(3, 3))])\r\nx = tf.constant([[[0.8055, 0.0083, 0.4057], [0.1249, 0.9762, 0.5402], [0.0637, 0.1539, 0.0282]]])\r\nprint (model.predict(x,steps=1))\r\n```\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.5\r\n\r\n", "comments": ["```\r\nx = tf.Tensor(\r\n[[[0.8055 0.0083 0.4057]\r\n  [0.1249 0.9762 0.5402]\r\n  [0.0637 0.1539 0.0282]]], shape=(1, 3, 3), dtype=float32)\r\n```\r\n\r\nYou do normalization along axis = 0 (first dimension which is 1) so you get `exp(x)/exp(x) = 1`.\r\n\r\nBy default, softmax does normalization along the last dimension (axis = -1). \r\n\r\nI recommend [Introduction to Tensors - About shapes](https://www.tensorflow.org/guide/tensor?hl=en#about_shapes). Please send PR if it can be improved.", "@szutenberg Thanks for the answer. \r\n\r\n@rschumi0 Please close the issue if this was resolved for you. Thanks!", "@szutenberg  Thank you for your answer. It's true, one is the correct result when the softmax function is applied for axis=0. I still think that the user wouldn't use this layer to turn all inputs to one since this is usually not intended . I think this might happen by accident when users are not aware about the axis notations. Hence, I would suggest a warning in order to avoid unnecessary debugging efforts.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rschumi0 Do you want to update any TF docs with a note to users? Please feel free to create any PR to update the docs. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48646, "title": "ReLU layer wrong result with negative threshold", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\nThe [ReLu](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU) layer seems to have an issue with negative thresholds, even though it should support them according to the documentation. For example, for a  `max_value=1` and `threshold=-1`, ReLu should produce f(x)=0.5 for x=0.5 according to the following formula from the documentation  `f(x) = x if threshold <= x < max_value`. \r\nThe issue is illustrated in the following code.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nmodel = keras.Sequential([\r\nkeras.layers.ReLU(max_value=1, threshold=-1, negative_slope=1, input_shape=(4,))])\r\nx = tf.constant([[1.5, 0.5,-0.5, -1.5]])\r\nprint (model.predict(x,steps=1))\r\n```\r\n`Output: [[ 1.   0.5  0.  -0.5]] Expected Output: [[1   0.5  -0.5  -0.5]]`\r\n", "comments": ["That's how \"ReLU\" looks like if we obey \r\n```\r\n  f(x) = max_value if x >= max_value\r\n  f(x) = x if threshold <= x < max_value\r\n  f(x) = negative_slope * (x - threshold) otherwise\r\n\r\nnegative_slope = 1\r\nthreshold = -1\r\nmax_value = 1\r\n```\r\n![image](https://user-images.githubusercontent.com/37601244/115468201-07091680-a233-11eb-9567-796d263a6ad5.png)\r\n\r\nIt does not look like \"**Re**ctified **L**inear **U**nit anymore so I think it's no point to fix it and I recommend modifying the docs. (https://github.com/tensorflow/tensorflow/pull/48654)", "@szutenberg \r\n>   f(x) = negative_slope * (x - threshold) otherwise\r\n\r\nCan we do\r\n` f(x) =   negative_slope * (x - threshold) + threshold `\r\n\r\nThis will work for the general case as `f(x) .= x for x = threshold` ", "@szutenberg \r\nAlso, there's something more weird going on with `tf.keras.backend.relu` function. Here's how it works:\r\n```python\r\na = np.array( [ [0.5,2] , [-1.5,-0.5] ])\r\n# array([[ 0.5,  2. ],\r\n#       [-1.5, -0.5]])\r\nprint(tf.keras.backend.relu(a, alpha = 1, max_value = 1, threshold=-1)) \r\n# Output :\r\n# tf.Tensor(\r\n# [[ 0.5  1. ]\r\n# [-0.5  0. ]], shape=(2, 2), dtype=float64)\r\n```\r\n\r\nAccording to your graph the (1,1) entry should be -0.5. \r\n", "@AdityaKane2001 \r\n\r\nYes, it does not match my graph because I drew it by following:\r\n```\r\n  f(x) = max_value if x >= max_value\r\n  f(x) = x if threshold <= x < max_value\r\n  f(x) = negative_slope * (x - threshold) otherwise\r\n```\r\n\r\nThat's why I propose PR https://github.com/tensorflow/tensorflow/pull/48654 which solves the problem (negative threshold is simply not supported).\r\n\r\nI tried to understand why the threshold parameter was introduced. I found commit https://github.com/tensorflow/tensorflow/commit/0cf2c612e5e6ff8c5026011e8186056801def747 . I don't understand what is the point to introduce such consolidation. Any ideas?\r\n\r\nNote that in [keras.layers.ThresholdedReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ThresholdedReLU) `theta >= 0` (`theta` is equivalent to `threshold` in ReLU).\r\n\r\nWe should not replace `f(x) = negative_slope * (x - threshold)` with `f(x) = negative_slope * (x - threshold) + threshold` because it would break compatibility with [keras.layers.ThresholdedReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ThresholdedReLU) by modifying behaviour for positive theshold value (f(x) would return threshold instead of 0).", "@szutenberg \r\nFair enough. But then should we modify the relu function itself, asserting that we do have a non-negative threshold? It will remove any ambiguity for the user in case the user tries to enter a negative threshold value.\r\n\r\nHowever, even though the use case is extremely rare, it is perhaps best to keep it like that and clear bugs, if any, because it gives more flexibility, as `ThresholdedReLU` only allows a normal ReLU to be used with a threshold, and not some custom variant of it.\r\n\r\nSo, another possible solution may be to resolve those errors and bugs in the relu function, and eliminate `ThresholdedReLU` as it will be sort of a duplicate.", "@AdityaKane2001 I added additional asserts to ReLU.\r\n\r\nFrom the graph I drew, we can see that such scenario **probably** does not make any sense. It's not continuous at -1, this sudden change may break gradient descent.\r\n\r\nI don't understand what is the idea behind https://github.com/tensorflow/tensorflow/commit/0cf2c612e5e6ff8c5026011e8186056801def747 and I'd rather revert it than eliminate ThresholdedReLU. People use ThresholdedReLU: [example 1](https://github.com/gao-lab/REVA-Data_Source_Code/blob/9da1a9fcfa04663b6a09a9c2549af9dddcc9848a/Variant_annotation/CNNs/model_structures.py#L17), [example 2](https://github.com/tincochan/vGame_bgm_remix/blob/4135b1a5ff9f1de9107facb8623d586200c0246e/rnn-cnn-gan-enhancer.py#L66)\r\n\r\nFor very rare use cases users can implement their own layers.", "@szutenberg \r\nI agree with you. The use case is very (very) small..", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48646\">No</a>\n"]}, {"number": 48645, "title": "[r2.5 port][ROCm] Port PR 47508 to r2.5", "body": "/cc @mihaimaruseac @angerson\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/47508", "comments": []}, {"number": 48644, "title": "AveragePooling3D does not support float64 and produces confusing error message", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.5\r\n\r\n\r\n\r\n**Description**\r\nAveragePooling3D and MaxPool3D layer don't seem to support float64, i.e. they don't work with `tf.keras.backend.set_floatx('float64')`. The MaxPool3D layer shows a nice helpful error message, when it is used with float64, but AveragePooling3D only shows a strange error message that is not helpful: tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'AvgPool3D'\r\n\r\nIt would be great, if AveragePooling3D layer would also produce a similar helpful error message. \r\nEven better would be if both these layers would also support float64. It is quite strange that this is not supported since all other pool layers seem to support float64.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\ntf.keras.backend.set_floatx('float64')\r\nmodel = keras.Sequential([\r\nkeras.layers.AveragePooling3D(pool_size=(3, 1, 3), input_shape=(3, 3, 3, 4))])\r\nx = tf.constant([[[[[2, 2, 2, 1], [1, 1, 2, 2], [1, 1, 1, 1]], [[1, 1, 1, 2], [2, 1, 2, 2], [1, 2, 2, 2]], [[1, 1, 1, 2], [1, 1, 1, 1], [2, 1, 1, 1]]], [[[1, 2, 2, 1], [2, 2, 1, 1], [1, 2, 1, 1]], [[2, 2, 2, 1], [1, 1, 1, 1], [2, 2, 1, 2]], [[1, 2, 2, 1], [2, 2, 2, 1], [1, 1, 2, 1]]], [[[1, 1, 1, 1], [2, 2, 1, 2], [1, 1, 1, 2]], [[1, 1, 2, 1], [1, 1, 1, 1], [2, 2, 1, 2]], [[2, 2, 2, 1], [2, 1, 1, 2], [1, 1, 2, 2]]]]])\r\nprint (np.array2string(model.predict(x,steps=1), separator=', '))\r\n```\r\n\r\nThe same example with a MaxPool3D layer shows a much more helpful error message (TypeError: Value passed to parameter 'input' has DataType float64 not in list of allowed values: float16, bfloat16, float32) as illustrated in the following code\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\ntf.keras.backend.set_floatx('float64')\r\nmodel = keras.Sequential([\r\nkeras.layers.MaxPool3D(pool_size=(1, 2, 2), input_shape=(4, 4, 3, 3))])\r\nx = tf.constant([[[[[1, 2, 2], [1, 2, 2], [2, 2, 1]], [[1, 2, 1], [1, 2, 2], [1, 1, 2]], [[2, 2, 1], [2, 1, 1], [1, 2, 1]], [[2, 1, 2], [1, 1, 2], [1, 1, 1]]], [[[2, 1, 2], [2, 2, 1], [1, 2, 2]], [[1, 2, 2], [2, 1, 1], [2, 2, 2]], [[2, 1, 1], [2, 1, 2], [2, 1, 2]], [[2, 1, 2], [2, 2, 1], [1, 1, 2]]], [[[1, 1, 1], [2, 1, 2], [1, 2, 2]], [[2, 2, 1], [1, 2, 1], [1, 1, 1]], [[1, 1, 2], [2, 2, 2], [2, 2, 1]], [[1, 2, 1], [1, 1, 2], [1, 1, 2]]], [[[2, 1, 1], [1, 1, 1], [2, 1, 2]], [[1, 2, 1], [2, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 1, 2], [2, 2, 2]], [[2, 1, 1], [2, 1, 1], [1, 2, 1]]]]])\r\nprint (np.array2string(model.predict(x,steps=1), separator=', '))\r\n```\r\n\r\n", "comments": ["Hi @rschumi0 ,\r\n\r\nI don't think we have an issue here. Both messages are informative. On TF 2.4.0 I'm getting:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'AvgPool3D' used by {{node sequential/average_pooling3d/AvgPool3D}} with these attrs: [strides=[1, 3, 1, 3, 1], data_format=\"NDHWC\", ksize=[1, 3, 1, 3, 1], padding=\"VALID\", T=DT_DOUBLE]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\n\t [[sequential/average_pooling3d/AvgPool3D]] [Op:__inference_predict_function_70]\r\n```\r\n\r\nLet's have a look at the code:\r\n- [AvgPool3D](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L716-L724) is registered also for `T=double` (which is float64)\r\n- [MaxPool3D](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L739-L747) is not registered for `T=double`\r\n\r\nThese are op registrations. Kernels for CPU are registered in [core/kernels/pooling_ops_3d.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/pooling_ops_3d.cc#L712-L735). We see that there is no kernel registration for double, that's why TF complains. You can use XLA_CPU_JIT.", "@rschumi0 \r\n \r\nPlease take a look at the above mentioned  comment and let us know if you are still facing the same issue. Otherwise move this to closed status Thanks!\r\n", "@szutenberg Thank you for your answer.\r\n\r\nI am not that familiar with the different kernels which made the error message confusing to me. How can I change to XLA_CPU_JIT?\r\n\r\nSo the reason for the different error message is that there is a kernel that supports double for AvgPool3D, but none for MaxPool3D. Is there a particular reason why there is none for MaxPool3D? ", "@rschumi0 \r\nThe following are the supported [devices](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/g3doc/cpu_supported_ops.md) for XLA_CPU_JIT. Could you please check and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48644\">No</a>\n"]}, {"number": 48643, "title": "Why the same model has different behaviours? One requires training = True, the other one not?", "body": "I have Unet model used to train 2 different datasets; one with input_shape = (128,128,1) and the other with input_shape = (128,128,3). After training, model1 does not require training = True, while model2 requires training = True for image segmentation. Why is this happening even I am using the same settings except the input_shapes are different? You can find the model [here](https://github.com/tensorflow/tensorflow/issues/48448#issuecomment-818020142);  \r\n\r\n```\r\n>>> model1.summary()\r\nModel: \"model_1\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_2 (InputLayer)            [(None, 128, 128, 1) 0\r\n__________________________________________________________________________________________________\r\nconv2d_20 (Conv2D)              (None, 128, 128, 64) 640         input_2[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_19 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_20[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_21 (Conv2D)              (None, 128, 128, 64) 36928       leaky_re_lu_19[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_20 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_21[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 64)   0           leaky_re_lu_20[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_22 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_4[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_21 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_22[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_23 (Conv2D)              (None, 64, 64, 128)  147584      leaky_re_lu_21[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_22 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_23[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 128)  0           leaky_re_lu_22[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_24 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_5[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_23 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_24[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_25 (Conv2D)              (None, 32, 32, 256)  590080      leaky_re_lu_23[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_24 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_25[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           leaky_re_lu_24[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_26 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_6[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_25 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_26[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_27 (Conv2D)              (None, 16, 16, 512)  2359808     leaky_re_lu_25[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_26 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_27[0][0]\r\n__________________________________________________________________________________________________\r\ndropout_2 (Dropout)             (None, 16, 16, 512)  0           leaky_re_lu_26[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_28 (Conv2D)              (None, 8, 8, 1024)   4719616     max_pooling2d_7[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_27 (LeakyReLU)      (None, 8, 8, 1024)   0           conv2d_28[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_29 (Conv2D)              (None, 8, 8, 1024)   9438208     leaky_re_lu_27[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_28 (LeakyReLU)      (None, 8, 8, 1024)   0           conv2d_29[0][0]\r\n__________________________________________________________________________________________________\r\ndropout_3 (Dropout)             (None, 8, 8, 1024)   0           leaky_re_lu_28[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_4 (Conv2DTrans (None, 16, 16, 512)  4719104     dropout_3[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_4 (ZeroPadding2D (None, 16, 16, 512)  0           conv2d_transpose_4[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           dropout_2[0][0]\r\n                                                                 zero_padding2d_4[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_30 (Conv2D)              (None, 16, 16, 512)  4719104     concatenate_4[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_29 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_30[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_31 (Conv2D)              (None, 16, 16, 512)  2359808     leaky_re_lu_29[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_30 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_31[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_5 (Conv2DTrans (None, 32, 32, 256)  1179904     leaky_re_lu_30[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_5 (ZeroPadding2D (None, 32, 32, 256)  0           conv2d_transpose_5[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_5 (Concatenate)     (None, 32, 32, 512)  0           leaky_re_lu_24[0][0]\r\n                                                                 zero_padding2d_5[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_32 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_5[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_31 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_32[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_33 (Conv2D)              (None, 32, 32, 256)  590080      leaky_re_lu_31[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_32 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_33[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_6 (Conv2DTrans (None, 64, 64, 128)  295040      leaky_re_lu_32[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_6 (ZeroPadding2D (None, 64, 64, 128)  0           conv2d_transpose_6[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_6 (Concatenate)     (None, 64, 64, 256)  0           leaky_re_lu_22[0][0]\r\n                                                                 zero_padding2d_6[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_34 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_6[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_33 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_34[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_35 (Conv2D)              (None, 64, 64, 128)  147584      leaky_re_lu_33[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_34 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_35[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_7 (Conv2DTrans (None, 128, 128, 64) 73792       leaky_re_lu_34[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_7 (ZeroPadding2D (None, 128, 128, 64) 0           conv2d_transpose_7[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_7 (Concatenate)     (None, 128, 128, 128 0           leaky_re_lu_20[0][0]\r\n                                                                 zero_padding2d_7[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_36 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_7[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_35 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_36[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_37 (Conv2D)              (None, 128, 128, 64) 36928       leaky_re_lu_35[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_36 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_37[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_38 (Conv2D)              (None, 128, 128, 2)  1154        leaky_re_lu_36[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_37 (LeakyReLU)      (None, 128, 128, 2)  0           conv2d_38[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_39 (Conv2D)              (None, 128, 128, 2)  6           leaky_re_lu_37[0][0]\r\n==================================================================================================\r\nTotal params: 34,513,288\r\nTrainable params: 34,513,288\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n\r\n```\r\n```\r\n>>> model2.summary()\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 128, 128, 3) 0\r\n__________________________________________________________________________________________________\r\nconv2d (Conv2D)                 (None, 128, 128, 64) 1792        input_1[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu (LeakyReLU)         (None, 128, 128, 64) 0           conv2d[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_1 (Conv2D)               (None, 128, 128, 64) 36928       leaky_re_lu[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 64) 0           conv2d_1[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)    (None, 64, 64, 64)   0           leaky_re_lu_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_3 (Conv2D)               (None, 64, 64, 128)  147584      leaky_re_lu_2[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 128)  0           conv2d_3[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0           leaky_re_lu_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_4 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_1[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_4[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_5 (Conv2D)               (None, 32, 32, 256)  590080      leaky_re_lu_4[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 256)  0           conv2d_5[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           leaky_re_lu_5[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_6 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_2[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_6[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_7 (Conv2D)               (None, 16, 16, 512)  2359808     leaky_re_lu_6[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 512)  0           conv2d_7[0][0]\r\n__________________________________________________________________________________________________\r\ndropout (Dropout)               (None, 16, 16, 512)  0           leaky_re_lu_7[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_8 (Conv2D)               (None, 8, 8, 1024)   4719616     max_pooling2d_3[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_8 (LeakyReLU)       (None, 8, 8, 1024)   0           conv2d_8[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_9 (Conv2D)               (None, 8, 8, 1024)   9438208     leaky_re_lu_8[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 1024)   0           conv2d_9[0][0]\r\n__________________________________________________________________________________________________\r\ndropout_1 (Dropout)             (None, 8, 8, 1024)   0           leaky_re_lu_9[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose (Conv2DTranspo (None, 16, 16, 512)  4719104     dropout_1[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d (ZeroPadding2D)  (None, 16, 16, 512)  0           conv2d_transpose[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 16, 16, 1024) 0           dropout[0][0]\r\n                                                                 zero_padding2d[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_10 (Conv2D)              (None, 16, 16, 512)  4719104     concatenate[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_10[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_11 (Conv2D)              (None, 16, 16, 512)  2359808     leaky_re_lu_10[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_11 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_11[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_1 (Conv2DTrans (None, 32, 32, 256)  1179904     leaky_re_lu_11[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_1 (ZeroPadding2D (None, 32, 32, 256)  0           conv2d_transpose_1[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 32, 32, 512)  0           leaky_re_lu_5[0][0]\r\n                                                                 zero_padding2d_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_12 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_1[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_12 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_12[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_13 (Conv2D)              (None, 32, 32, 256)  590080      leaky_re_lu_12[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_13 (LeakyReLU)      (None, 32, 32, 256)  0           conv2d_13[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_2 (Conv2DTrans (None, 64, 64, 128)  295040      leaky_re_lu_13[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_2 (ZeroPadding2D (None, 64, 64, 128)  0           conv2d_transpose_2[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_2 (Concatenate)     (None, 64, 64, 256)  0           leaky_re_lu_3[0][0]\r\n                                                                 zero_padding2d_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_14 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_2[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_14 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_14[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      leaky_re_lu_14[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_15 (LeakyReLU)      (None, 64, 64, 128)  0           conv2d_15[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_transpose_3 (Conv2DTrans (None, 128, 128, 64) 73792       leaky_re_lu_15[0][0]\r\n__________________________________________________________________________________________________\r\nzero_padding2d_3 (ZeroPadding2D (None, 128, 128, 64) 0           conv2d_transpose_3[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_3 (Concatenate)     (None, 128, 128, 128 0           leaky_re_lu_1[0][0]\r\n                                                                 zero_padding2d_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_16 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_3[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_16 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_16[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_17 (Conv2D)              (None, 128, 128, 64) 36928       leaky_re_lu_16[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_17 (LeakyReLU)      (None, 128, 128, 64) 0           conv2d_17[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_18 (Conv2D)              (None, 128, 128, 2)  1154        leaky_re_lu_17[0][0]\r\n__________________________________________________________________________________________________\r\nleaky_re_lu_18 (LeakyReLU)      (None, 128, 128, 2)  0           conv2d_18[0][0]\r\n__________________________________________________________________________________________________\r\nconv2d_19 (Conv2D)              (None, 128, 128, 2)  6           leaky_re_lu_18[0][0]\r\n==================================================================================================\r\nTotal params: 34,514,440\r\nTrainable params: 34,514,440\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["@micosacak \r\nHello again. Can you please share a reproducible notebook?", "Sorry, unfortunately I cannot. In case you need any further information from the models, please let me know. \r\n\r\nAs I wrote, the models summary are above.\r\n\r\nIf I do not provide training = True for model1, it still performs segmentation as good as if provide training = True.\r\nHowever, the model2 does not show any segmentation, if do not provide training = True.\r\n\r\n\r\nWhile both models have all options similar excepts the input_shape, why this is happening.\r\n\r\nOr maybe the question could be, how the model is using `training = True` argument?\r\n\r\n\r\n", "@micosacak ,\r\n\r\nI ran the mentioned code and face a different error, please find the [gist](https://colab.research.google.com/gist/tilakrayal/4d573c4a63339374c695689009e72e85/untitled48643.ipynb) here. To replicate the issue can you please provide the Tensorflow verion,dependencies or colab gist with the reported error.\r\n\r\nThanks!", "@tilakrayal \r\nyou can generate a test input as: `test_input = tf.random.normal([1,100])`.\r\n\r\nWhat I do not understand and I do not how it happened, the same model have 2 behaviors. Model1 performs image segmentation either set `training = True` or not. But Model2 only performs prediction correctly only if `training = True`.\r\n\r\nyou can see the models summary above. I also checked the `saved_model.pb` and in both cases the keras version is the same `2.3.0-tf`: `... \"name\": \"model\", \"input_layers\": [[\"input_1\", 0, 0]], \"output_layers\": [[\"conv2d_19\", 0, 0]]}}, \"batch_input_shape\": null, \"name\": \"model\", \"keras_version\": \"2.3.0-tf\", \"class_name\": \"Model\", ...`\r\n\r\nAs I said, I do not what happened or can it be reproduced again. The GPU computer is not working, but I will repeat again with tensorflow 2.4.1 and see if I can reproduce the same results.\r\n\r\nBut anyway whatever happened, it is strange that same model structure with only different input shape works differently!\r\n\r\nOne explanation could be as Dropout is random, this may have different behavior in different datasets!\r\n\r\n", "@tilakrayal \r\nsetting `training = True` is a big problem for me, as I want to use the trained model in Java. That is why finding a way to set `training = True` after training would make it easier to use these models in Java. As you can see [here](https://github.com/tensorflow/java/issues/284), I can not use these models in tensorflow/java directly as I have to find out a way to `feed` a scalar boolean, which is difficult for me to understand. How this argument `training = True ` is used in the model? Do you think is there a way to set it as always True after training? \r\nthanks.", "@micosacak Is it possible to provide simple standalone code to reproduce. We generally don't encourage long code to debug.  Few things to consider are\r\n\r\n1. Did you face same issue when you try using the built-in training (when compared to custom training)\r\n2. did you check whether it is due to autograph (@tf.function decoration)\r\n\r\nHope this helps. If you cannot simplify the code, please consider posting this issue in Stackoverflow where there is a large community to help and support each other. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48643\">No</a>\n", "I had the same error. Did you fix it?"]}, {"number": 48642, "title": "Remove image_recognition_experimental example", "body": "This PR removes the image_recognition_experimental folder from the TFL micro examples. @advaitjain \r\n\r\nIt is a UInt8 network and for that reason has been excluded from the examples in the [makefile](https://github.com/tensorflow/tensorflow/blob/89133a6a74da4b976a585b66a09a6601fce3b217/tensorflow/lite/micro/tools/make/Makefile#L240-L245), so removing it completely will avoid any confusion on part of the end user.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48641, "title": "[TF.Keras]: plotting utilities", "body": "The plot function is defined as follows \r\n\r\n```\r\ntf.keras.utils.plot_model(\r\n    model,\r\n    to_file=\"model.png\",\r\n    show_shapes=False,\r\n    show_dtype=False,\r\n    show_layer_names=True,\r\n    rankdir=\"TB\",\r\n    expand_nested=False,\r\n    dpi=96,\r\n)\r\n```\r\n\r\nFollowing features would be nice (IMO) to have with it:\r\n- Plot subgroup (e.g. form layer index 5 to 20 or by layer's name) of `model`. For example: if we plot `EfficientNet`, this plot utils will plot the whole model from first to last. But let's say for some reason we manipulate intermediate block from `layer_id: 10` to `layer_id:30` and want to check or visualize.", "comments": ["Adding the contributions welcome label to this issue for further investigation by the community. If you are interested in working on this feature, please leave a comment and I will assign it to you. Thanks!", "Hey I want to take up this issue, but I am new in this. Can you please elaborate the issue?", "@nikitamaia , I have started working on this issue as part of PR #49860 . Can you please assign this issue to me?", "@nikitamaia , I think we can close this issue since the change is merged in keras repo after split in [this](https://github.com/keras-team/keras/commit/d9e7b83c70e2117f40465b79e2e8201cea46d36a) commit as part of PR #49860 .\r\n\r\ncc @qlzh727 ", "Yes, this issue can be closed now, since https://github.com/keras-team/keras/commit/d9e7b83c70e2117f40465b79e2e8201cea46d36a has reach nightly build."]}, {"number": 48640, "title": "Tensorflow after 1.15 - No need to install tensorflow-gpu package", "body": "This is basically an info question that was asked by [mon](https://stackoverflow.com/users/4281353/mon) in SO and we answered as far as we know, [here](https://stackoverflow.com/a/67088947/9215780).\r\n\r\nIn the official `tf` installation guide in documentation, it says, \r\n\r\n> For releases 1.15 and older, CPU and GPU packages are separate:\r\n\r\npip install tensorflow==1.15      # CPU\r\npip install tensorflow-gpu==1.15  # GPU\r\n\r\nAnd for later version, e.g `tf : > = 2.0`, we simply need to do \r\n\r\n```\r\npip install tensorflow     # CPU or GPU \r\n```\r\n\r\nBut still, we have \r\n\r\n```\r\n!pip install tensorflow-gpu \r\n....\r\nInstalling collected packages: tensorflow-gpu\r\nSuccessfully installed tensorflow-gpu-2.4.1\r\n\r\n# -------------------------------------------------------------\r\n\r\n!pip install tensorflow-cpu\r\n....\r\nInstalling collected packages: tensorflow-cpu\r\nSuccessfully installed tensorflow-cpu-2.4.1\r\n```\r\n\r\nwhy `tf-gpu/cpu` for `2.4.1` still exits? what is the main reason here? is there any difference for `tf > = 2` installing them separately or the standalone way? ", "comments": ["@innat,\r\n1. As you've already mentioned, starting with TF v2.0, the TensorFlow packages by default support both CPU and GPU.\r\n\r\n\r\n2. > why `tf-gpu/cpu` for `2.4.1` still exits? what is the main reason here?\r\n\r\nIf you explicitly require support for either CPU or GPU **only**, you can opt for the tf-cpu/gpu package.\r\n\r\n\r\n\r\n3. > is there any difference for `tf > = 2` installing them separately or the standalone way?\r\n\r\nApart from the above mentioned point and the package size itself there isn't much difference.\r\n\r\nHope this answers your query. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48640\">No</a>\n", "`tensorflow` and `tensorflow-gpu` after 2.1 are exactly the same package, just with a different name."]}, {"number": 48639, "title": "Is there any offical way of freezing graph to pb in Tensorflow2", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nTo some reason I have to get a pb model. In TF1 I used following code to build a pd file.\r\n```\r\nfrom tensorflow.python.framework import graph_util\r\nconstant_graph = graph_util.convert_variables_to_constants(sess,sess.graph_def,FLAG.output_op)\r\nwith tf.gfile.GFile(FLAG.save_path + '\\\\PBModel.pb',mode='wb') as f:\r\n    f.write(constant_graph.SerializeToString())\r\n```\r\nI tried every thing of savedModel to pb or .h5 to pb but failed. So is there any offical way of freezing graph to pb in Tensorflow2 ?", "comments": ["@Zhuxinpei ,\r\n\r\nCan you please refer to this [link](https://stackoverflow.com/questions/58119155/freezing-graph-to-pb-in-tensorflow2) for similiar issue.\r\n\r\nThanks!", "I have tried all ways from that link but the pb from .h5 was not simillar to the pb in TF1 especially when I built custom model.", "@Zhuxinpei ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide Tensorflow version,the complete code and the dataset you are using. Thanks!", "Tensorflow-2.3.0. Sorry for no code and data cause the issue was asked for the code of freezing graph to pb.", "@Zhuxinpei Can you please check whether [this resource](https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt) is helpful to you or not. \r\n\r\nSome of the `TF1` functionality you can access through `tf.compat.v1.graph_util.convert_variables_to_constants`. Thanks!", "Thanks for your reply but the resource may not helpful. What I need is converting a saved_model from TF2 to a raw .pb file in TF1. The resource documents as \r\n\r\n> \"TensorFlow 2.x saved_models work in TensorFlow 1.x if all the ops are supported\"\r\n\r\n Thus I thought there should be a way of using `tf.compat.v1.graph_util.convert_variables_to_constants` to convert a saved_model. Actually I tried many ideas in stackoverflow but no one works. Such as the [link](https://stackoverflow.com/questions/58119155/freezing-graph-to-pb-in-tensorflow2) and code:\r\n```\r\nimport logging\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1 import graph_util\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow import keras\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nsaved_model_path = 'XXX'\r\nmodel = keras.models.load_model(saved_model_path)\r\nwith K.get_session() as sess:\r\n    output_names = [out.op.name for out in model.outputs]\r\n    input_graph_def = sess.graph.as_graph_def()\r\n    for node in input_graph_def.node:\r\n        node.device = \"\"\r\n    graph = graph_util.remove_training_nodes(input_graph_def)\r\n    graph_frozen = graph_util.convert_variables_to_constants(sess, graph, output_names)\r\n    tf.io.write_graph(graph_frozen, '/path/to/pb/model.pb', as_text=False)\r\nlogging.info(\"save pb successfully\uff01\")\r\n```\r\nIf you have any idea please help. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Freezing is not an operation the TF team has plans to support going forward.\r\n\r\nIt is used as an implementation detail of a few transformations, or was at some point (e.g. TF-Lite, XLA compilation to some extent). But not as a standalone operation.\r\n\r\nIf there's something you could do in TF1 that still requires a frozen graph and it's maintained by the TF team, please do file a bug about that. Endpoints should generally consume SavedModels, and we'd really rather not have a separate format for frozen models floating around that are supported spottily.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48639\">No</a>\n"]}, {"number": 48638, "title": "delete the eager execution check for shared_embedding_columns_v2", "body": "at the line 901 of tensorflow/tensorflow/python/feature_column/feature_column_v2.py:\r\nwe have a eager execution  check:\r\nif context.executing_eagerly():\r\n    raise RuntimeError('shared_embedding_columns are not supported when eager '\r\n                       'execution is enabled.')\r\nbut it shouldn't check because it's a tf2.X code and it use eager execution at default\r\n", "comments": ["@Alspeakeryhl\r\nYes, it is true that TF has eager execution on by default. But many models in production or production testing environments do not use eager mode, they use graph mode. Thus, to tell the user that the aforementioned function cannot be used under such conditions, we raise the `RuntimeError`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Adding to AdityaKane's answer-\r\nEven though eager execution is enabled by default there are certain cases (executing inside of `tf.function`) when it is disabled and in such cases the eager execution check can come in handy.\r\nSee https://www.tensorflow.org/api_docs/python/tf/executing_eagerly to know more.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48638\">No</a>\n"]}, {"number": 48637, "title": "TFlite to 8 bit quantized TFLite conversion", "body": "I have a TFLite model with me sans any pb model and it is operating on float32 (all the parameters including weights,biases and activaions).\r\nI need to get the 8 bit integer quantized version of this.\r\nIs there any way in which I can convert TFlite model to 8 bit quantized TFLite model without having pb file of it??", "comments": ["Please refer to https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization", "> Please refer to https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization\r\n\r\nThanks for the reply sir!!\r\n\r\nI have checked the website mentioned by you but the issue I faced there was there the tflite model is generated from a saved_model ,i.e,  we need to input a pb model ,then convert it to tflite model\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n\r\n\r\nBut my requirement is a bit different. I need to convert an already converted tflite model to  8 bit quantized tflite model.\r\n\r\nIs there anyway in which we can do that sir?", "I am afraid, we cannot do it at this time since the post training full integer quantization happens at the time of tflite conversion which takes `Keras saved model, h5` or `SavedModel format` as input. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48637\">No</a>\n"]}, {"number": 48636, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow for MAC m1 ", "body": "**System information**\r\n-MacBook Air (M1, 2020)\r\n-version 11.2\r\n-python version 3.8.2 \r\n-pip version 21.0.1 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to install tensorflow by below command \r\n`pip install tensorflow \r\n`\r\nand getting below \r\n\r\nerror \r\n\r\n**ERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow**\r\n\r\n\r\nThis is my all attempts log:\r\n\r\n```\r\n(venv) alimonkarim@Alimons-MacBook-Air ml % python -V\r\nPython 3.8.2\r\n(venv) alimonkarim@Alimons-MacBook-Air ml % pip -V\r\npip 21.0.1 from /Users/webnation/ml/venv/lib/python3.8/site-packages/pip (python 3.8)\r\n(venv) alimonkarim@Alimons-MacBook-Air ml % pip install --upgrade pip\r\n\r\nRequirement already satisfied: pip in ./venv/lib/python3.8/site-packages (21.0.1)\r\n(venv) alimonkarim@Alimons-MacBook-Air ml % pip install tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\n  \r\n\r\n", "comments": ["@pitocms ,\r\n\r\nPlease refer to similar issues #44751,#47782 and for installation please take a look at this [link](https://github.com/apple/tensorflow_macos). It helps.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48636\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48636\">No</a>\n", "having same issue on macbook pro M1 \r\n\r\n```\r\nmalik@Apples-MBP arabianconsult % pip3 install tensorflow-macos\r\nERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\r\nERROR: No matching distribution found for tensorflow-macos\r\nmalik@Apples-MBP arabianconsult %                           \r\n```", "ERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\r\nERROR: No matching distribution found for tensorflow-macos\r\n\r\nFacing same issue in my Macbook Pro M1\r\n"]}, {"number": 48634, "title": "Fix handling of last dimension of RaggedTensor in SparseCategoricalLoss.", "body": "The last dimension of a prediction corresponding to the per category scores\r\nmay (or not) be ragged, depending on how the tensor was constructed.\r\nIgnore this last dimension, if present, but do not require it to be there.\r\n\r\nFixes #48609", "comments": ["@tomerk Fixes an issue with RaggedTensor support for SparseCategoricalEntropy from a PR that you reviewed previously."]}, {"number": 48633, "title": "Edit CosineSimilarity documentation", "body": "Match dimensions of input and output of `l2_norm()`.", "comments": []}, {"number": 48632, "title": "TF to TFLite conversion clarification", "body": "**What is  the difference between \r\n\r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\nand \r\n\r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS]\r\n\r\nperformed during conversion of tf model to tflite model??**\r\n\r\n\r\n\r\nI tried using converter.target_spec.supported_ops in 3 ways : \r\n1.  using \r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS]\r\n\r\n2. using \r\nconverter.target_spec.supported_ops =[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\n3. execeuting code without converter.target_spec.supported_ops\r\n\r\nwhen I was converting pb model to 8 bit quantized tflite model.\r\n\r\nAll three of them  yielded me the same results in the output tflite model,which were identical in all respects.\r\n**Why is this so?**", "comments": ["@liufengdb could you triage this issue?", "> @liufengdb could you triage this issue?\r\n\r\nSure sir...\r\nI was converting a custom trained pb model to an 8 bit integer quantized tflite model.\r\nThat's where the question arose in my mind about this line : \r\nconverter.target_spec.supported_ops \r\n\r\nAlthough I experimented with it and generated multiple tflite models,all of them were identical .\r\nAlso I cam across an article which stated that : \r\n\r\n\r\n**type_choice['none'] = None**   \r\n**# for converter.target_spec.supported_ops**\r\n**ops_choice = {**\r\n    **\"int8\": [lite.OpsSet.TFLITE_BUILTINS_INT8],**\r\n    **\"tflite\": [lite.OpsSet.TFLITE_BUILTINS],  # default**\r\n    **\"tf\": [lite.OpsSet.SELECT_TF_OPS, lite.OpsSet.TFLITE_BUILTINS]**  \r\n**}**\r\n\r\nSo basically,I got 3 identical tflite models with 3 configurations,whereas this article states otherwise\r\n\r\nSo sir can you please tell me the significance of \r\n**converter.target_spec.supported_ops** and why the 3 configurations I mentioned in the first post had yielded me identical tflite models?? ", "The 'converter.target_spec.supported_ops' is just the inference the tflite converter exposed to the end users, and internally this flag will be used to specify some parameters to the backend for whether \"allowing\" floating-point ops in a quantized model and whether \"allowing\" flex ops, etc. \"Allowing\" certain ops doesn't necessarily mean the ops will be used. The converter will still try its best to use the most efficient ops.\r\nSpecifically, \"lite.OpsSet.TFLITE_BUILTINS\" is a super-set of \"tf.lite.OpsSet.TFLITE_BUILTINS_INT8\". If your model can be fully quantized, the converter will use the int8 ops, even though floating-point ops is allowed. So it is not surprised to see the results are the same for the three flag settings.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48632\">No</a>\n"]}, {"number": 48631, "title": "model_main.py", "body": "I trained a model with TensorFlow object detection API (TF1) with model_main script but when the model finish training returns me a export/Servo/save_model.pb, when I run the export_inference_graph.py script returns me the frozen graph but when I run object_detection_runner the model doesn't detect anything, ( but when I trained the model with train.py script it detects everything)\r\n\r\nConfig:\r\nModel: reste101\r\nTFVersion : TF1\r\nOS : Windows 10\r\n\r\n```\r\n# R-FCN with Resnet-101 (v1),  configuration for MSCOCO Dataset.\r\n# Users should configure the fine_tune_checkpoint field in the train config as\r\n# well as the label_map_path and input_path fields in the train_input_reader and\r\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n# should be configured.\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    num_classes: 23\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 600\r\n        max_dimension: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_resnet101'\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    second_stage_box_predictor {\r\n      rfcn_box_predictor {\r\n        conv_hyperparams {\r\n          op: CONV\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              stddev: 0.01\r\n            }\r\n          }\r\n        }\r\n        crop_height: 18\r\n        crop_width: 18\r\n        num_spatial_bins_height: 3\r\n        num_spatial_bins_width: 3\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 300\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 1\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.0003\r\n          schedule {\r\n            step: 900000\r\n            learning_rate: .00003\r\n          }\r\n          schedule {\r\n            step: 1200000\r\n            learning_rate: .000003\r\n          }\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint:\"rfcn_resnet101_coco_2018_01_28/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  # Note: The below line limits the training process to 200K steps, which we\r\n  # empirically found to be sufficient enough to train the pets dataset. This\r\n  # effectively bypasses the learning rate schedule (the learning rate will\r\n  # never decay). Remove the below line to train indefinitely.\r\n  num_steps: 20000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"data/train.record\"\r\n  }\r\n  label_map_path: \"training/labelmap.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 120 #12959\r\n  num_visualizations:20\r\n  # Note: The below line limits the evaluation process to 10 evaluations.\r\n  # Remove the below line to evaluate indefinitely.\r\n  #max_evals: 10\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    \r\n    input_path: \"data/test.record\"\r\n  }\r\n  label_map_path: \"training/labelmap.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n```", "comments": ["@epsilon11101 ,\r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@epsilon11101 ,\r\n\r\nWe see that you are using tf version 1.x, there is no support for 1.x, please update to 2.x and let us know if you are using same issue.Thanks", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48631\">No</a>\n"]}]