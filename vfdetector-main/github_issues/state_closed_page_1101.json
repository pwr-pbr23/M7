[{"number": 20233, "title": "Build fails while creating Python API: \"TypeError: argument of type 'NoneType' is not iterable\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Gentoo Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0-rc1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:  7.3.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: building using Gentoo ebuild\r\n\r\n\r\n### Describe the problem\r\nBuild fails while creating Python API.\r\n\r\n### Source code / logs\r\n```\r\n\"/var/tmp/portage/sci-libs/tensorflow-1.9.0_rc1/work/bazel-base-python3_6/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 182, in get_api_init_text\r\n    package not in module.__name__):\r\nTypeError: argument of type 'NoneType' is not iterable\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3202.457s, Critical Path: 83.98s\r\nINFO: 4901 processes, local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nFixed for me with:\r\n```\r\n--- tensorflow-1.9.0-rc1.orig/tensorflow/tools/api/generator/create_python_api.py\r\n+++ tensorflow-1.9.0-rc1/tensorflow/tools/api/generator/create_python_api.py\r\n@@ -179,7 +179,7 @@\r\n   for module in list(sys.modules.values()):\r\n     # Only look at tensorflow modules.\r\n     if (not module or not hasattr(module, '__name__') or\r\n-        package not in module.__name__):\r\n+        module.__name__ is None or package not in module.__name__):\r\n       continue\r\n     # Do not generate __init__.py files for contrib modules for now.\r\n     if '.contrib.' in module.__name__ or module.__name__.endswith('.contrib'):\r\n```\r\n\r\nbecause `module.__name__` can be None.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory", "@cjmayo Maybe you can submit the pull request of your patch?", "Creating a pull request is not a problem but the CLA put me off, not expecting to create any more patches.\r\n\r\nI think this one is trivial, and in any case I'm happy if anyone wants to use it.", "@cjmayo I created a PR #20250 with your patch.", "Great, thanks."]}, {"number": 20232, "title": "Handle scalar input to assert_equal in eager.", "body": "PiperOrigin-RevId: 199274329\r\n\r\nThis change fixes a bug in tf.assert_equal when used in eager mode to compare scalars.", "comments": ["Re-triggering Windows Bazel build for a third time. Not sure why it keeps on failing, but I feel its unlikely due to this change."]}, {"number": 20231, "title": "Feature request: float16 support for sparse_softmax_cross_entropy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe Tensorflow documentation for tf.losses.sparse_softmax_cross_entropy seems to say that only `tf.float32` or `tf.float64` types are supported for `logits`. Is that true? Is `tf.float16` not supported? \r\nhttps://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["It looks like the implementation of sparse_softmax_cross_entropy supports float16, float32, and float64. The `sparse_softmax_cross_entropy` calls `nn.sparse_softmax_cross_entropy_with_logits`:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py#L912\r\n, which converts `float16` to `float32`, so `float16` is supported:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L2035-L2036\r\n\r\nCreated a PR #20242 to update the doctoring to reflect the supported type of float16."]}, {"number": 20230, "title": "fixes estimator model_fn signature in python docs", "body": "", "comments": []}, {"number": 20229, "title": "Branch 201687789", "body": "", "comments": []}, {"number": 20228, "title": "Update cython to 0.28.3", "body": "This fix updates the cython from 3732784 (09/2017)\r\nto the latest versioned release of 0.28.3 (05/2018).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20227, "title": "Update gflags from f8a0efe to v2.2.1", "body": "This fix bumps gflags from f8a0efe to the latest versioned release of v2.2.1\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Assume the CLA is signed ;)", "Nudge for review."]}, {"number": 20226, "title": "Update libxsmm from 1.8.1 to 1.9", "body": "\r\nThis fix updates libxsmm from 1.8.1 (05/12/2017) to 1.9 (03/15/2018)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20225, "title": "Windows Make: Don't (re)build dependencies, use prebuilt ones", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **GCC/Compiler version (if compiling from source)**: 8.0.1\r\n- **Exact command to reproduce**: mingw32-make\r\n\r\n### Describe the problem\r\nThe tool builds the dependencies instead of asking the user to provide location of prebuilt ones. Building dependencies when building TF causes problems: if a dependency build fails, I have to start the build from scratch, this consumes LOT OF TIME. Also I cannot fix an error in the dependencies because it fetches the latest version from git on each insfallation, this also consumes LOT OF TIME. Please provide the way to use prebuilt dependencies. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "The TF team is in the process of migrating Windows builds to Bazel on Windows, so the ming32-make build tool is unlikely to receive official support. Please keep an eye out for that news!", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "New versions of TensorFlow now use Bazel for Windows. I am leaving this issue open as Community supported.", "@KOLANICH Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "IDK. I have given up my attempts to try to build TF for Windows 32-bit because the building cost is unbearable for me (likely because of not enough efficient build process, which was raised in this issue) and I just use the one prebuilt by Google for Ubuntu 64 bit (using a newer computer for running it, fortunately it was not me who has paid for it). It is completely not OK, and it is likely that  the issue still persists, but it doesn't affect me so hard (i'd still would like to be able to use TF on 32-bit Windows 7) anymore.\r\n\r\nThank you for the efficiency in fixing issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20225\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20225\">No</a>\n"]}, {"number": 20224, "title": "Update abseil-py to v0.2.2", "body": "This fix updates the abseil-py from unversioned release of ea8c4d2 (04/18/2018) to a slight newer versioned release of v0.2.2 (05/22/2018)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20223, "title": "Allow stride > window_size for sliding_window_batch", "body": "Fix #20191.", "comments": []}, {"number": 20221, "title": "Update pcre library to 8.42", "body": "This fix updates pcre library 8.39 to 8.42.\r\nPcre 8.39 was released in 14-June-2016, which is quite old.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20220, "title": "Attempt to make sense of reasoning for loss", "body": "I feel the document seems a bit weird. Adding \"when\" seems to make better sense of it. Open to suggestions on alternatives.\r\nThanks", "comments": []}, {"number": 20219, "title": "[FEATURE REQUEST] tf.scatter_nd doesn't support half types", "body": "I try to use tf.scatter_nd and tf.scatter_max for tf.float16 input, but it reports that it doesn't support half types. Can tf.scatter_nd  and tf.scatter_max add support for half types? Thanks.\r\n\r\n@kosklain @dantkz", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I don't see why not.", "That would be a good contribution if you feel up to it. Thanks!", "I'm afraid that [tf.scatter_add](https://www.tensorflow.org/versions/master/api_docs/python/tf/scatter_add) indeed supports half type in the latest version. ", "@facaiy scatter_xxx supports half type only for cpu kernel. No gpu kernel is registered for half type.", "Yes, you're right. I checked the source code and found that float16 is not registered for GPU kernel to avoid compile failure.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c1d223de41838e9d387a48137c76ea39d3b38f3f/tensorflow/core/kernels/scatter_functor_gpu.cu.cc#L43-L49\r\n\r\nPerhaps we can uncomment the last code and let's see what happens :-)", "I register the float16 gpu kernel for scatter function in PR #20244, and then I run those related tests. All seems passed. ", "@facaiy \r\nIn scatter_nd_op.cc there are several TF_CALL_GPU_NUMBER_TYPES_NO_HALF.  Do they need to be fixed?\r\n\r\nWhy I cannot find tests for tf.scatter_nd in the test cases?", "@x10000year Do you need it (gpu kernerl for `scatter_nd`)? Would you like to create a pull request and make the contribution? Take a try to fix it. \r\n\r\nThe test file of `scatter_nd` is [here](https://github.com/tensorflow/tensorflow/blob/cb401f09be5b816e704a70babc0facad63e84636/tensorflow/python/kernel_tests/scatter_nd_ops_test.py#L146). If I'm not wrong, we could fix `scatter_nd` by the similar way as we do in PR #20244.", "@facaiy sorry I know almost nothing about c++. I will appreciate if you can create a PR :)", "@x10000year Sure, I have registered float16 gpu kernel for scatter_nd in\r\ne369dbb .", "@x10000year, Sorry for the late response. \r\n\r\n`tf.scatter_nd ` supports half type. For more details please refer to this commit 5e28f0c5c83afc3ede7758b2b541777e719cb146. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20218, "title": "ran out of memory in eager execution", "body": "### System information\r\n- **Have I written custom code**:Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**:source\r\n- **TensorFlow version**:1.8.0-gpu\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**:GeForce GTX 1060 / 6G\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Exact command to reproduce**:\r\nGeneral training model steps\r\n\r\n### Describe the problem\r\nI wrote a Neural Language Model using TF eager mode, which has only one LSTM cell.\r\nSome of these hyperparameters are:\r\n\r\nhidden_dim : 128\r\nvocab_size : 7393\r\nseq_len : <=50\r\nbatch_size : 32\r\n\r\nIn particular, I did not use the inherited keras base class to write the model, but instead used tfe.Variable to gradually implement some of the details of the model. The model was normal during the early training and performed well.  However, a memory exhaustion error occurs every time the model runs to 960 steps or so.  I think it's because some variables haven't released this error that has been accumulated during training.  Because I still have this error in 960 steps after adjusting batch-size. \r\nThe following is the error log, I think this is a TF eager mode memory usage problem, however the custom model is a necessary choice for some attempts.\r\nHow can we avoid this problem and let the model train smoothly?\r\nThanks a lot.\r\n\r\n### Source code / logs\r\nloss at epoch 0 step 954: 6.665831\r\nloss at epoch 0 step 955: 6.716213\r\nloss at epoch 0 step 956: 6.761374\r\nloss at epoch 0 step 957: 6.712702\r\nloss at epoch 0 step 958: 6.753476\r\nloss at epoch 0 step 959: 6.481436\r\nloss at epoch 0 step 960: 6.544363\r\nloss at epoch 0 step 961: 6.562449\r\nloss at epoch 0 step 962: 6.701943\r\nloss at epoch 0 step 963: 6.415178\r\nloss at epoch 0 step 964: 6.630089\r\nloss at epoch 0 step 965: 6.580114\r\n2018-06-22 11:53:38.547278: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 92.05MiB.  Current allocation summary follows.\r\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in\r\n use in bin.\r\n2018-06-22 11:53:38.547428: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-r[1767/1952]\r\n use in bin.\r\n2018-06-22 11:53:38.547459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-req\r\nuested in use in bin.\r\n2018-06-22 11:53:38.547482: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-reque\r\nsted in use in bin.\r\n2018-06-22 11:53:38.547504: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use \r\nin bin.\r\n2018-06-22 11:53:38.547525: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 1, Chunks in use: 0. 4.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in \r\nuse in bin.\r\n2018-06-22 11:53:38.547544: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use \r\nin bin.\r\n2018-06-22 11:53:38.547568: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 4. 135.5KiB allocated for chunks. 110.0KiB in use in bin. 108.8Ki\r\nB client-requested in use in bin.\r\n2018-06-22 11:53:38.547593: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 920, Chunks in use: 918. 28.87MiB allocated for chunks. 28.81MiB in use in bin. 28.\r\n69MiB client-requested in use in bin.\r\n2018-06-22 11:53:38.547617: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 410, Chunks in use: 410. 26.12MiB allocated for chunks. 26.12MiB in use in bin. 25.\r\n66MiB client-requested in use in bin.\r\n2018-06-22 11:53:38.547640: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 20, Chunks in use: 20. 2.69MiB allocated for chunks. 2.69MiB in use in bin. 2.69MiB\r\n client-requested in use in bin.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n2018-06-22 11:53:38.548461: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc700 of size 512                                                                         [1691/1952]\r\n2018-06-22 11:53:38.548478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089bc900 of size 131072\r\n2018-06-22 11:53:38.548494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dc900 of size 512\r\n2018-06-22 11:53:38.548510: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089dcb00 of size 131072\r\n2018-06-22 11:53:38.548526: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcb00 of size 512\r\n2018-06-22 11:53:38.548542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102089fcd00 of size 131072\r\n2018-06-22 11:53:38.548558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cd00 of size 512\r\n2018-06-22 11:53:38.548574: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a1cf00 of size 131072\r\n2018-06-22 11:53:38.548589: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3cf00 of size 512\r\n2018-06-22 11:53:38.548605: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a3d100 of size 131072\r\n2018-06-22 11:53:38.548622: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d100 of size 512\r\n2018-06-22 11:53:38.548638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a5d300 of size 131072\r\n2018-06-22 11:53:38.548654: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d300 of size 512\r\n2018-06-22 11:53:38.548669: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a7d500 of size 131072\r\n2018-06-22 11:53:38.548685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d500 of size 512\r\n2018-06-22 11:53:38.548702: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10208a9d700 of size 7570432\r\n2018-06-22 11:53:38.548718: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091d5b00 of size 29696\r\n2018-06-22 11:53:38.548735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091dcf00 of size 32768\r\n2018-06-22 11:53:38.548752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091e4f00 of size 32768\r\n2018-06-22 11:53:38.548768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091ecf00 of size 32768\r\n2018-06-22 11:53:38.548783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x102091f4f00 of size 32768\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \r\nLimit:                  5272961024\r\n2018-06-22 11:53:38.796751: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:                                                                                                       [19/1952]\r\nLimit:                  5272961024\r\nInUse:                  3087996928\r\nMaxInUse:               3174426368\r\nNumAllocs:                 2214674\r\nMaxAllocSize:            140052992\r\n\r\n2018-06-22 11:53:38.796832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *************_************************************_***********_**_**_**_**_**_******_**_***_**_*****\r\n2018-06-22 11:53:38.796866: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at bias_op.cc:341 : Resource exhausted: OOM when allocating tensor with shape[3200,7393] and type float on \r\n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "Interesting. Can you share some code about how you're creating your variables?\r\n\r\nAlternatively, an easy way to tell that you're leaking variables is turning log_device_placement=True in the configproto you can pass to enable_eager_execution. If you see it creating VarHandleOp many times after the first minibatch then you know somewhere in your code you're creating variables in every iteration.", "Hi alextp, thank you very much for you helping. The following is the code of part of my model:\r\n\r\n`\r\n    import tensorflow as tf\r\n    import tensorflow.contrib.eager as tfe\r\n    import numpy as np\r\n\r\n    class Model():\r\n        def __init__(self,params,vocab_size):\r\n\r\n            # Hy-parameters\r\n            self.vocab_size = vocab_size\r\n            self.embedding_dim = params.embedding_dim\r\n            self.hidden_dim = params.hidden_dim\r\n            self.num_layers = params.num_layers\r\n            self.keep_ratio = params.keep_ratio\r\n            self.time_major = params.time_major\r\n\r\n            # Embeddings\r\n            self.embedding = Embedding(self.vocab_size,self.embedding_dim,self.time_major)\r\n\r\n            # Dynamic RNN\r\n            self.rnn = RNN(self.hidden_dim, self.embedding_dim,self.num_layers,\r\n                                   self.keep_ratio,self.time_major, name='Forward')\r\n\r\n            # Fully Connections Layer\r\n            self.fc = FC(self.embedding_dim,self.vocab_size,self.time_major)\r\n\r\n            # Build all trainable variables\r\n            self.build()\r\n\r\n            # Add saver\r\n            self.saver = tfe.Saver(self.variables)\r\n\r\n        def build(self):\r\n            # initialize trainable variables\r\n            self.variables = []\r\n\r\n            self.variables.append(self.embedding.embedding)\r\n            for variable in self.rnn.variables:\r\n                self.variables.append(variable)\r\n            self.variables.append(self.fc.weights)\r\n            self.variables.append(self.fc.bias)\r\n\r\n        def __call__(self,inputs,training):\r\n            # Parallel computing\r\n            if self.time_major:\r\n                inputs = tf.transpose(inputs,(1,0))\r\n            # Get word embeddings for all\r\n            embed = self.embedding(inputs)\r\n            # moving\r\n            rnn_outputs = self.rnn(embed,training=training)\r\n\r\n            # Fully connections\r\n            logits = self.fc(rnn_outputs)\r\n            if self.time_major:\r\n                seq_len,batch_size,last_dim=logits.shape.as_list()\r\n                logits = tf.reshape(logits,(batch_size,seq_len,-1))\r\n\r\n            return logits\r\n    class Embedding():\r\n        # Embedding with different lengths.\r\n        def __init__(self, vocab_size, embedding_dim,time_major=True):\r\n            self.vocab_size = vocab_size\r\n            self.embedding_dim = embedding_dim\r\n            self.time_major = time_major\r\n\r\n            self.embedding = tfe.Variable(initial_value=tf.random_normal(shape=(vocab_size,embedding_dim)),dtype=tf.float32,name='embedding',trainable=True)\r\n\r\n        def __call__(self, x):\r\n            # implement word embedding\r\n            # whether parallel computing\r\n            if self.time_major:\r\n                    #all_embedded=tf.nn.embedding_lookup(self.embedding,x)\r\n                    seq_len, batch_size = x.shape.as_list()\r\n                    x = tf.one_hot(x, depth=self.vocab_size)\r\n                    flat_x = tf.reshape(x, shape=(-1,self.vocab_size))\r\n                    all_embedded = tf.matmul(flat_x,self.embedding)\r\n                    all_embedded = tf.reshape(all_embedded,(seq_len,batch_size,-1))\r\n            else:\r\n                    all_embedded = []\r\n                    for seq in x:\r\n                          #embedded = tf.nn.embedding_lookup(self.embedding,seq)\r\n                          seq = tf.one_hot(seq, depth=self.vocab_size)\r\n                          embedded = tf.matmul(seq, self.embedding)\r\n                          all_embedded.append(embedded)\r\n\r\n            return all_embedded\r\n    class LSTMCell():\r\n        def __init__(self,inputs_dim, hidden_dim, name):\r\n\r\n            self.inputs_dim = inputs_dim\r\n            self.hidden_dim = hidden_dim\r\n\r\n            with tf.name_scope(name=name):\r\n                # Forget gate\r\n                self.W_f = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='forget_weights', trainable=True)\r\n                self.b_f = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                        dtype=tf.float32, name='forget_bias', trainable=True)\r\n                # Input gate\r\n                self.W_i = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='input_weights', trainable=True)\r\n                self.b_i = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                        dtype=tf.float32, name='input_bias', trainable=True)\r\n                self.W_c = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='input_filter_weights', trainable=True)\r\n                self.b_c = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                        dtype=tf.float32, name='input_filter_bias', trainable=True)\r\n                # Output gate\r\n                self.W_o = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\r\n                                        dtype=tf.float32, name='output_weights', trainable=True)\r\n                self.b_o = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\r\n                                    dtype=tf.float32, name='output_bias', trainable=True)\r\n    \r\n                self.build()\r\n\r\n        def build(self,):\r\n            # build all variables\r\n            self.variables = [self.W_f, self.b_f,self.W_i, self.b_i,\r\n                              self.W_c, self.b_c,self.W_o, self.b_o]\r\n\r\n        def zero_state(self,batch_size):\r\n            c_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='c_t')\r\n            h_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='h_t')\r\n            return c_t, h_t\r\n\r\n        def __call__(self,x_t,c_t,h_t):\r\n            # Define LSTM forward propagation\r\n\r\n            # Forget\r\n            f_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_f,self.b_f),name='Forget_Gate')\r\n            # Input\r\n            i_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_i,self.b_i),name='Input_Gate')\r\n            uc_t=tf.tanh(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_c,self.b_c),name='Input_Filter')\r\n            # Update Cell State\r\n            c_t = f_t*c_t + i_t*uc_t\r\n            # Output\r\n            o_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_o,self.b_o),name='Output_Gate')\r\n            h_t = o_t * tf.tanh(c_t)\r\n\r\n            return o_t,c_t,h_t\r\n    class RNN():\r\n        # A static RNN. Similar to tf.nn.static_rnn, implemented as a class.\r\n        def __init__(self, hidden_dim,inputs_dim, num_layers, keep_ratio,time_major=True,name=''):\r\n            self.keep_ratio = keep_ratio\r\n            self.time_major = time_major\r\n            self.inputs_dim=inputs_dim\r\n            self.hidden_dim=hidden_dim\r\n\r\n            self.cells = [\r\n                LSTMCell(inputs_dim,hidden_dim,name=name+'_LSTMCell'+str(idx))\r\n                for idx in range(num_layers)\r\n            ]\r\n\r\n            self.build()\r\n\r\n        def build(self):\r\n            # build all trainable variables\r\n            self.variables = []\r\n\r\n            for cell in self.cells:\r\n                for variable in cell.variables:\r\n                    self.variables.append(variable)\r\n\r\n\r\n        def __call__(self, inputs_seq, training=False):\r\n            # rnn with different length sequences, inputs : [batch, (embedded tensor)]\r\n\r\n            # parallel computing\r\n            if self.time_major:\r\n                seq_len,batch_size,last_dim = inputs_seq.shape.as_list()\r\n                for cell in self.cells:\r\n                    c_t, h_t = cell.zero_state(batch_size)\r\n                    outputs = []\r\n                    for inp in inputs_seq:\r\n                        output, c_t, h_t = cell(inp, c_t, h_t)\r\n                        outputs.append(output)\r\n    \r\n                    inputs_seq = tf.stack(outputs, axis=0)\r\n                    if training:\r\n                        inputs_seq = tf.nn.dropout(inputs_seq, self.keep_ratio)\r\n                batch_outputs = inputs_seq\r\n            # linear computing\r\n            else:\r\n                for cell in self.cells:\r\n                    c_t,h_t = cell.zero_state(1)\r\n                    outputs = []\r\n                    for seq in inputs_seq:\r\n                        seq_outputs = []\r\n                        for word in seq:\r\n                            word = tf.reshape(word,(1,-1))\r\n                            output, c_t,h_t = cell(word, c_t,h_t)\r\n                            if training:\r\n                                output = tf.nn.dropout(output, self.keep_ratio)\r\n                            seq_outputs.append(output)\r\n                        outputs.append(seq_outputs)\r\n                    inputs_seq = outputs\r\n                batch_outputs = inputs_seq\r\n            return batch_outputs\r\n    class FC():\r\n        # A Fully Connection Layer.\r\n        def __init__(self, inputs_dim, outputs_dim,time_major=True,active=tf.nn.relu,name=None):\r\n            self.inputs_dim = inputs_dim\r\n            self.outputs_dim= outputs_dim\r\n            self.time_major = time_major\r\n            self.active = active\r\n\r\n            # Build trainable variables\r\n            self.weights = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim,outputs_dim)),\r\n                                      dtype=tf.float32,name=name,trainable=True)\r\n            self.bias = tfe.Variable(initial_value=tf.random_normal(shape=(outputs_dim,)),\r\n                                      dtype=tf.float32,name=name,trainable=True)\r\n    \r\n        def __call__(self, x):\r\n            # different length sequences, inputs : [batch, (RNN outputs tensor)]\r\n            # whether to parallel computing\r\n            if self.time_major:\r\n                seq_len,batch_size,last_dim = x.shape.as_list()\r\n                # flatten\r\n                flat = tf.reshape(x,(-1,last_dim))\r\n                fc = tf.nn.xw_plus_b(flat,self.weights,self.bias)\r\n                outputs = self.active(fc)\r\n                outputs = tf.reshape(outputs,(seq_len,batch_size,-1))\r\n            else:\r\n                outputs = []\r\n                for seq in x:\r\n                    fc = tf.nn.xw_plus_b(seq, self.weights, self.bias)\r\n                    fc = self.active(fc)\r\n                    outputs.append(fc)\r\n            return outputs\r\n`\r\n\r\n\r\nPart of the training code is as follows:\r\n\r\n`\r\n     with tf.GradientTape() as tape:\r\n             logits = model(batch_forward,batch_backward,training=True)\r\n             loss = training_helper.loss_fn_time_major(batch_labels,logits,batch_lengths)\r\n             # add loss summary\r\n             tf.contrib.summary.scalar('loss', loss)\r\n             grads = tape.gradient(loss, model.variables)\r\n             optimizer.apply_gradients(zip(grads,model.variables))\r\n             print('loss at epoch %d step %d: %f' % (epoch,train_step,loss))\r\n`\r\n\r\nI used your recommended settings turning log_device_placement=True, then I observed an interesting phenomenon. At every certain training step, there are three operation logs that are cycled, and the interval between the training steps is gradually decreasing. \r\nThe following is the log after the setting log_device_placement=True is turned on. It can be seen, at the beginning, a log of three operations is printed every 16 training steps. As the training process progresses, a log of three operations is printed every three training steps until the OOM problem happened. I don't know why and what these three operations represent respectively. I hope this information will help solve the problem. Thank you again for your reply.\r\n\r\n### some logs\r\n\r\nloss at epoch 0 step 805: 6.740626\r\nloss at epoch 0 step 806: 7.095036\r\nloss at epoch 0 step 807: 7.000319\r\nloss at epoch 0 step 808: 6.904347\r\nloss at epoch 0 step 809: 6.922751\r\nloss at epoch 0 step 810: 6.838681\r\nloss at epoch 0 step 811: 6.905680\r\nloss at epoch 0 step 812: 7.053514\r\nloss at epoch 0 step 813: 6.855484\r\nloss at epoch 0 step 814: 6.755508\r\nloss at epoch 0 step 815: 7.050679\r\nloss at epoch 0 step 816: 7.023479\r\nloss at epoch 0 step 817: 6.827959\r\nloss at epoch 0 step 818: 6.776998\r\nloss at epoch 0 step 819: 6.765797\r\nloss at epoch 0 step 820: 6.759283\r\n2018-06-25 14:57:15.878743: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:15.935075: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:15.970760: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 821: 6.984869\r\nloss at epoch 0 step 822: 6.748662\r\nloss at epoch 0 step 823: 6.772003\r\nloss at epoch 0 step 824: 6.942709\r\nloss at epoch 0 step 825: 6.892979\r\nloss at epoch 0 step 826: 6.680350\r\nloss at epoch 0 step 827: 6.867269\r\nloss at epoch 0 step 828: 7.118308\r\nloss at epoch 0 step 829: 6.601588\r\nloss at epoch 0 step 830: 6.652170\r\nloss at epoch 0 step 831: 7.263901\r\nloss at epoch 0 step 832: 6.902826\r\nloss at epoch 0 step 833: 6.791662\r\nloss at epoch 0 step 834: 7.087551\r\nloss at epoch 0 step 835: 6.820101\r\n2018-06-25 14:57:18.650329: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:18.710324: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:18.746490: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 836: 7.031796\r\nloss at epoch 0 step 837: 6.905032\r\nloss at epoch 0 step 838: 6.889287\r\nloss at epoch 0 step 839: 6.732944\r\nloss at epoch 0 step 840: 6.715842\r\nloss at epoch 0 step 841: 6.757300\r\nloss at epoch 0 step 842: 6.882929\r\nloss at epoch 0 step 843: 6.658679\r\nloss at epoch 0 step 844: 6.788300\r\nloss at epoch 0 step 845: 6.952966\r\nloss at epoch 0 step 846: 7.061527\r\nloss at epoch 0 step 847: 6.603632\r\nloss at epoch 0 step 848: 7.163596\r\nloss at epoch 0 step 849: 7.038760\r\n2018-06-25 14:57:21.325933: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:21.386127: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:21.423910: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 850: 6.885034\r\nloss at epoch 0 step 851: 6.931499\r\nloss at epoch 0 step 852: 6.713628\r\nloss at epoch 0 step 853: 6.777599\r\nloss at epoch 0 step 854: 6.799969\r\nloss at epoch 0 step 855: 6.985339\r\nloss at epoch 0 step 856: 6.620006\r\nloss at epoch 0 step 857: 6.878152\r\nloss at epoch 0 step 858: 6.783222\r\nloss at epoch 0 step 859: 6.712417\r\nloss at epoch 0 step 860: 6.734530\r\nloss at epoch 0 step 861: 7.009552\r\n2018-06-25 14:57:23.706079: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:23.769647: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:23.809172: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 862: 6.570765\r\nloss at epoch 0 step 863: 6.818643\r\nloss at epoch 0 step 864: 6.707229\r\nloss at epoch 0 step 865: 6.860226\r\nloss at epoch 0 step 866: 6.832708\r\nloss at epoch 0 step 867: 6.746671\r\nloss at epoch 0 step 868: 6.925843\r\nloss at epoch 0 step 869: 6.845281\r\nloss at epoch 0 step 870: 6.901897\r\nloss at epoch 0 step 871: 6.936728\r\nloss at epoch 0 step 872: 6.793565\r\n2018-06-25 14:57:25.894904: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:25.958699: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:25.998658: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 873: 6.795174\r\nloss at epoch 0 step 874: 6.897914\r\nloss at epoch 0 step 875: 6.964663\r\nloss at epoch 0 step 876: 6.933444\r\nloss at epoch 0 step 877: 6.889087\r\nloss at epoch 0 step 878: 6.728238\r\nloss at epoch 0 step 879: 6.543335\r\nloss at epoch 0 step 880: 6.785100\r\nloss at epoch 0 step 881: 6.867868\r\nloss at epoch 0 step 882: 6.626728\r\n2018-06-25 14:57:28.011803: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:28.078783: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:28.121272: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 883: 6.632171\r\nloss at epoch 0 step 884: 6.638273\r\nloss at epoch 0 step 885: 6.727106\r\nloss at epoch 0 step 886: 6.755214\r\nloss at epoch 0 step 887: 6.788674\r\nloss at epoch 0 step 888: 6.618323\r\nloss at epoch 0 step 889: 6.802135\r\nloss at epoch 0 step 890: 6.812809\r\nloss at epoch 0 step 891: 6.905511\r\n2018-06-25 14:57:29.982565: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:30.048872: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:30.089943: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 892: 6.660409\r\nloss at epoch 0 step 893: 6.817567\r\nloss at epoch 0 step 894: 6.782594\r\nloss at epoch 0 step 895: 6.746886\r\nloss at epoch 0 step 896: 6.679479\r\nloss at epoch 0 step 897: 6.696771\r\nloss at epoch 0 step 898: 6.642287\r\nloss at epoch 0 step 899: 6.870078\r\n2018-06-25 14:57:31.693334: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:31.761710: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:31.805339: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 900: 6.774796\r\nloss at epoch 0 step 901: 6.841524\r\nloss at epoch 0 step 902: 6.682564\r\nloss at epoch 0 step 903: 6.693571\r\nloss at epoch 0 step 904: 6.603614\r\nloss at epoch 0 step 905: 6.705801\r\nloss at epoch 0 step 906: 6.713558\r\nloss at epoch 0 step 907: 6.889330\r\n2018-06-25 14:57:33.459800: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:33.528563: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:33.573187: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 908: 6.726191\r\nloss at epoch 0 step 909: 6.945642\r\nloss at epoch 0 step 910: 6.768640\r\nloss at epoch 0 step 911: 6.801554\r\nloss at epoch 0 step 912: 6.867862\r\nloss at epoch 0 step 913: 6.563491\r\nloss at epoch 0 step 914: 6.853379\r\nloss at epoch 0 step 915: 6.724048\r\n2018-06-25 14:57:35.341966: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:35.412082: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:35.456774: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 916: 6.639415\r\nloss at epoch 0 step 917: 6.881902\r\nloss at epoch 0 step 918: 6.735335\r\nloss at epoch 0 step 919: 6.688423\r\nloss at epoch 0 step 920: 6.636367\r\nloss at epoch 0 step 921: 6.717305\r\nloss at epoch 0 step 922: 6.629582\r\n2018-06-25 14:57:36.960963: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:37.033898: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:37.079990: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 923: 6.682681\r\nloss at epoch 0 step 924: 6.882014\r\nloss at epoch 0 step 925: 6.731138\r\nloss at epoch 0 step 926: 6.588252\r\nloss at epoch 0 step 927: 6.824276\r\nloss at epoch 0 step 928: 6.707603\r\nloss at epoch 0 step 929: 6.748030\r\n2018-06-25 14:57:38.690123: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:38.767782: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:38.816409: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 930: 6.658401\r\nloss at epoch 0 step 931: 6.748807\r\nloss at epoch 0 step 932: 6.802845\r\nloss at epoch 0 step 933: 6.590845\r\nloss at epoch 0 step 934: 6.749659\r\nloss at epoch 0 step 935: 6.701960\r\n2018-06-25 14:57:40.158598: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:40.234263: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:40.282647: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 936: 6.775355\r\nloss at epoch 0 step 937: 6.781133\r\nloss at epoch 0 step 938: 6.600548\r\nloss at epoch 0 step 939: 6.747424\r\nloss at epoch 0 step 940: 6.605201\r\n2018-06-25 14:57:41.416798: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:41.496970: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:41.550523: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 941: 6.672260\r\nloss at epoch 0 step 942: 6.614303\r\nloss at epoch 0 step 943: 6.726663\r\nloss at epoch 0 step 944: 6.694652\r\nloss at epoch 0 step 945: 6.590765\r\n2018-06-25 14:57:42.720835: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:42.800870: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:42.852718: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 946: 6.639668\r\nloss at epoch 0 step 947: 6.581154\r\nloss at epoch 0 step 948: 6.780156\r\nloss at epoch 0 step 949: 6.619957\r\nloss at epoch 0 step 950: 6.680293\r\n2018-06-25 14:57:44.034047: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:44.116253: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:44.167625: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 951: 6.767905\r\nloss at epoch 0 step 952: 6.694592\r\nloss at epoch 0 step 953: 6.586446\r\nloss at epoch 0 step 954: 6.691846\r\n2018-06-25 14:57:45.122539: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:45.204973: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:45.258423: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 955: 6.994371\r\nloss at epoch 0 step 956: 6.858140\r\nloss at epoch 0 step 957: 6.646007\r\nloss at epoch 0 step 958: 6.721205\r\n2018-06-25 14:57:46.238911: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:46.325236: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:46.380011: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 959: 6.589066\r\nloss at epoch 0 step 960: 6.683684\r\nloss at epoch 0 step 961: 6.672047\r\n2018-06-25 14:57:47.046723: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:47.203337: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:47.260355: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 962: 6.725653\r\nloss at epoch 0 step 963: 6.720684\r\nloss at epoch 0 step 964: 6.636925\r\n2018-06-25 14:57:47.932850: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:48.021669: I tensorflow/c/eager/c_api.cc:856] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:48.077942: I tensorflow/c/eager/c_api.cc:856] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\r\nloss at epoch 0 step 965: 6.461943\r\nloss at epoch 0 step 966: 6.716685\r\nloss at epoch 0 step 967: 6.541767\r\n2018-06-25 14:57:48.839285: I tensorflow/c/eager/c_api.cc:856] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-25 14:57:58.895255: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 90.19MiB.  Current allocation summary follows.\r\n2018-06-25 14:57:58.895406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin. 12B client-requested in use in bin.\r\n2018-06-25 14:57:58.895440: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895466: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895488: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-06-25 14:57:58.895517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-06-25 14:57:58.895538: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-06-25 14:57:58.895565: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 5, Chunks in use: 3. 124.0KiB allocated for chunks. 79.0KiB in use in bin. 78.9KiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895592: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 903, Chunks in use: 901. 28.30MiB allocated for chunks. 28.24MiB in use in bin. 28.15MiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895618: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 402, Chunks in use: 402. 25.56MiB allocated for chunks. 25.56MiB in use in bin. 25.16MiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895639: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 19, Chunks in use: 19. 2.51MiB allocated for chunks. 2.51MiB in use in bin. 2.47MiB client-requested in use in bin.\r\n2018-06-25 14:57:58.895673: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8, Chunks in use: 8. 2.97MiB allocated for chunks. 2.97MiB in use in bin. 2.75MiB client-requested in use in bin.", "Very interesting. So this is not a straightforward leak of tf variables.\n\nOn Mon, Jun 25, 2018 at 1:37 AM jaye <notifications@github.com> wrote:\n\n> Hi alextp, thank you very much for you helping. The following is the code\n> of part of my model:\n>\n> `import tensorflow as tf\n> import tensorflow.contrib.eager as tfe\n> import numpy as np\n>\n> class Model():\n> def *init*(self,params,vocab_size):\n>\n>     # Hy-parameters\n>     self.vocab_size = vocab_size\n>     self.embedding_dim = params.embedding_dim\n>     self.hidden_dim = params.hidden_dim\n>     self.num_layers = params.num_layers\n>     self.keep_ratio = params.keep_ratio\n>     self.time_major = params.time_major\n>\n>     # Embeddings\n>     self.embedding = Embedding(self.vocab_size,self.embedding_dim,self.time_major)\n>\n>     # Dynamic RNN\n>     self.rnn = RNN(self.hidden_dim, self.embedding_dim,self.num_layers,\n>                            self.keep_ratio,self.time_major, name='Forward')\n>\n>\n>     # Fully Connections Layer\n>     self.fc = FC(self.embedding_dim,self.vocab_size,self.time_major)\n>\n>     # Build all trainable variables\n>     self.build()\n>\n>     # Add saver\n>     self.saver = tfe.Saver(self.variables)\n>\n> def build(self):\n>     # initialize trainable variables\n>     self.variables = []\n>\n>     self.variables.append(self.embedding.embedding)\n>     for variable in self.rnn.variables:\n>         self.variables.append(variable)\n>     self.variables.append(self.fc.weights)\n>     self.variables.append(self.fc.bias)\n>\n> def __call__(self,inputs,training):\n>     # Parallel computing\n>     if self.time_major:\n>         inputs = tf.transpose(inputs,(1,0))\n>     # Get word embeddings for all\n>     embed = self.embedding(inputs)\n>     # moving\n>     rnn_outputs = self.rnn(embed,training=training)\n>\n>     # Fully connections\n>     logits = self.fc(rnn_outputs)\n>     if self.time_major:\n>         seq_len,batch_size,last_dim=logits.shape.as_list()\n>         logits = tf.reshape(logits,(batch_size,seq_len,-1))\n>\n>     return logits\n>\n> class Embedding():\n> # Embedding with different lengths.\n> def *init*(self, vocab_size, embedding_dim,time_major=True):\n> self.vocab_size = vocab_size\n> self.embedding_dim = embedding_dim\n> self.time_major = time_major\n>\n>     self.embedding = tfe.Variable(initial_value=tf.random_normal(shape=(vocab_size,embedding_dim)),\n>                                   dtype=tf.float32,name='embedding',trainable=True)\n>\n> def __call__(self, x):\n>     # implement word embedding\n>     # whether parallel computing\n>     if self.time_major:\n>         #all_embedded=tf.nn.embedding_lookup(self.embedding,x)\n>         seq_len, batch_size = x.shape.as_list()\n>         x = tf.one_hot(x, depth=self.vocab_size)\n>         flat_x = tf.reshape(x, shape=(-1,self.vocab_size))\n>         all_embedded = tf.matmul(flat_x,self.embedding)\n>         all_embedded = tf.reshape(all_embedded,(seq_len,batch_size,-1))\n>     else:\n>         all_embedded = []\n>         for seq in x:\n>           #embedded = tf.nn.embedding_lookup(self.embedding,seq)\n>           seq = tf.one_hot(seq, depth=self.vocab_size)\n>           embedded = tf.matmul(seq, self.embedding)\n>           all_embedded.append(embedded)\n>\n>     return all_embedded\n>\n> class LSTMCell():\n> def *init*(self,inputs_dim, hidden_dim, name):\n>\n>     self.inputs_dim = inputs_dim\n>     self.hidden_dim = hidden_dim\n>\n>     with tf.name_scope(name=name):\n>         # Forget gate\n>         self.W_f = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n>                                 dtype=tf.float32, name='forget_weights', trainable=True)\n>         self.b_f = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n>                                 dtype=tf.float32, name='forget_bias', trainable=True)\n>         # Input gate\n>         self.W_i = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n>                                 dtype=tf.float32, name='input_weights', trainable=True)\n>         self.b_i = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n>                                 dtype=tf.float32, name='input_bias', trainable=True)\n>         self.W_c = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n>                                 dtype=tf.float32, name='input_filter_weights', trainable=True)\n>         self.b_c = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n>                                 dtype=tf.float32, name='input_filter_bias', trainable=True)\n>         # Output gate\n>         self.W_o = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim + hidden_dim, hidden_dim)),\n>                                 dtype=tf.float32, name='output_weights', trainable=True)\n>         self.b_o = tfe.Variable(initial_value=tf.random_normal(shape=(hidden_dim,)),\n>                                 dtype=tf.float32, name='output_bias', trainable=True)\n>\n>         self.build()\n>\n> def build(self,):\n>     # build all variables\n>     self.variables = [self.W_f, self.b_f,self.W_i, self.b_i,\n>                       self.W_c, self.b_c,self.W_o, self.b_o]\n>\n> def zero_state(self,batch_size):\n>     c_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='c_t')\n>     h_t = tf.zeros(shape=(batch_size,self.hidden_dim),name='h_t')\n>     return c_t, h_t\n>\n> def __call__(self,x_t,c_t,h_t):\n>     # Define LSTM forward propagation\n>\n>     # Forget\n>     f_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_f,self.b_f),name='Forget_Gate')\n>     # Input\n>     i_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_i,self.b_i),name='Input_Gate')\n>     uc_t=tf.tanh(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_c,self.b_c),name='Input_Filter')\n>     # Update Cell State\n>     c_t = f_t*c_t + i_t*uc_t\n>     # Output\n>     o_t=tf.sigmoid(tf.nn.xw_plus_b(tf.concat((h_t,x_t),-1),self.W_o,self.b_o),name='Output_Gate')\n>     h_t = o_t * tf.tanh(c_t)\n>\n>     return o_t,c_t,h_t\n>\n> class RNN():\n> # A static RNN. Similar to tf.nn.static_rnn, implemented as a class.\n> def *init*(self, hidden_dim,inputs_dim, num_layers,\n> keep_ratio,time_major=True,name=''):\n> self.keep_ratio = keep_ratio\n> self.time_major = time_major\n> self.inputs_dim=inputs_dim\n> self.hidden_dim=hidden_dim\n>\n>     self.cells = [\n>         LSTMCell(inputs_dim,hidden_dim,name=name+'_LSTMCell'+str(idx))\n>         for idx in range(num_layers)\n>     ]\n>\n>     self.build()\n>\n> def build(self):\n>     # build all trainable variables\n>     self.variables = []\n>\n>     for cell in self.cells:\n>         for variable in cell.variables:\n>             self.variables.append(variable)\n>\n>\n> def __call__(self, inputs_seq, training=False):\n>     # rnn with different length sequences, inputs : [batch, (embedded tensor)]\n>\n>     # parallel computing\n>     if self.time_major:\n>         seq_len,batch_size,last_dim = inputs_seq.shape.as_list()\n>         for cell in self.cells:\n>             c_t, h_t = cell.zero_state(batch_size)\n>             outputs = []\n>             for inp in inputs_seq:\n>                 output, c_t, h_t = cell(inp, c_t, h_t)\n>                 outputs.append(output)\n>\n>             inputs_seq = tf.stack(outputs, axis=0)\n>             if training:\n>                 inputs_seq = tf.nn.dropout(inputs_seq, self.keep_ratio)\n>         batch_outputs = inputs_seq\n>     # linear computing\n>     else:\n>         for cell in self.cells:\n>             c_t,h_t = cell.zero_state(1)\n>             outputs = []\n>             for seq in inputs_seq:\n>                 seq_outputs = []\n>                 for word in seq:\n>                     word = tf.reshape(word,(1,-1))\n>                     output, c_t,h_t = cell(word, c_t,h_t)\n>                     if training:\n>                         output = tf.nn.dropout(output, self.keep_ratio)\n>                     seq_outputs.append(output)\n>                 outputs.append(seq_outputs)\n>             inputs_seq = outputs\n>         batch_outputs = inputs_seq\n>     return batch_outputs\n>\n> class FC():\n> # A Fully Connection Layer.\n> def *init*(self, inputs_dim,\n> outputs_dim,time_major=True,active=tf.nn.relu,name=None):\n> self.inputs_dim = inputs_dim\n> self.outputs_dim= outputs_dim\n> self.time_major = time_major\n> self.active = active\n>\n>     # Build trainable variables\n>     self.weights = tfe.Variable(initial_value=tf.random_normal(shape=(inputs_dim,outputs_dim)),\n>                               dtype=tf.float32,name=name,trainable=True)\n>     self.bias = tfe.Variable(initial_value=tf.random_normal(shape=(outputs_dim,)),\n>                               dtype=tf.float32,name=name,trainable=True)\n>\n> def __call__(self, x):\n>     # different length sequences, inputs : [batch, (RNN outputs tensor)]\n>     # whether to parallel computing\n>     if self.time_major:\n>         seq_len,batch_size,last_dim = x.shape.as_list()\n>         # flatten\n>         flat = tf.reshape(x,(-1,last_dim))\n>         fc = tf.nn.xw_plus_b(flat,self.weights,self.bias)\n>         outputs = self.active(fc)\n>         outputs = tf.reshape(outputs,(seq_len,batch_size,-1))\n>     else:\n>         outputs = []\n>         for seq in x:\n>             fc = tf.nn.xw_plus_b(seq, self.weights, self.bias)\n>             fc = self.active(fc)\n>             outputs.append(fc)\n>     return outputs\n>\n> `\n>\n> Part of the training code is as follows:\n>\n> with tf.GradientTape() as tape: logits =\n> model(batch_forward,batch_backward,training=True) loss =\n> training_helper.loss_fn_time_major(batch_labels,logits,batch_lengths) # add\n> loss summary tf.contrib.summary.scalar('loss', loss) grads =\n> tape.gradient(loss, model.variables)\n> optimizer.apply_gradients(zip(grads,model.variables)) print('loss at epoch\n> %d step %d: %f' % (epoch,train_step,loss))\n>\n> I used your recommended settings turning log_device_placement=True, then I\n> observed an interesting phenomenon. At every certain training step, there\n> are three operation logs that are cycled, and the interval between the\n> training steps is gradually decreasing.\n> The following is the log after the setting log_device_placement=True is\n> turned on. It can be seen, at the beginning, a log of three operations is\n> printed every 16 training steps. As the training process progresses, a log\n> of three operations is printed every three training steps until the OOM\n> problem happened. I don't know why and what these three operations\n> represent respectively. I hope this information will help solve the\n> problem. Thank you again for your reply.\n> some logs\n>\n> loss at epoch 0 step 805: 6.740626\n> loss at epoch 0 step 806: 7.095036\n> loss at epoch 0 step 807: 7.000319\n> loss at epoch 0 step 808: 6.904347\n> loss at epoch 0 step 809: 6.922751\n> loss at epoch 0 step 810: 6.838681\n> loss at epoch 0 step 811: 6.905680\n> loss at epoch 0 step 812: 7.053514\n> loss at epoch 0 step 813: 6.855484\n> loss at epoch 0 step 814: 6.755508\n> loss at epoch 0 step 815: 7.050679\n> loss at epoch 0 step 816: 7.023479\n> loss at epoch 0 step 817: 6.827959\n> loss at epoch 0 step 818: 6.776998\n> loss at epoch 0 step 819: 6.765797\n> loss at epoch 0 step 820: 6.759283\n> 2018-06-25 14:57:15.878743: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:15.935075: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:15.970760: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 821: 6.984869\n> loss at epoch 0 step 822: 6.748662\n> loss at epoch 0 step 823: 6.772003\n> loss at epoch 0 step 824: 6.942709\n> loss at epoch 0 step 825: 6.892979\n> loss at epoch 0 step 826: 6.680350\n> loss at epoch 0 step 827: 6.867269\n> loss at epoch 0 step 828: 7.118308\n> loss at epoch 0 step 829: 6.601588\n> loss at epoch 0 step 830: 6.652170\n> loss at epoch 0 step 831: 7.263901\n> loss at epoch 0 step 832: 6.902826\n> loss at epoch 0 step 833: 6.791662\n> loss at epoch 0 step 834: 7.087551\n> loss at epoch 0 step 835: 6.820101\n> 2018-06-25 14:57:18.650329: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:18.710324: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:18.746490: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 836: 7.031796\n> loss at epoch 0 step 837: 6.905032\n> loss at epoch 0 step 838: 6.889287\n> loss at epoch 0 step 839: 6.732944\n> loss at epoch 0 step 840: 6.715842\n> loss at epoch 0 step 841: 6.757300\n> loss at epoch 0 step 842: 6.882929\n> loss at epoch 0 step 843: 6.658679\n> loss at epoch 0 step 844: 6.788300\n> loss at epoch 0 step 845: 6.952966\n> loss at epoch 0 step 846: 7.061527\n> loss at epoch 0 step 847: 6.603632\n> loss at epoch 0 step 848: 7.163596\n> loss at epoch 0 step 849: 7.038760\n> 2018-06-25 14:57:21.325933: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:21.386127: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:21.423910: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 850: 6.885034\n> loss at epoch 0 step 851: 6.931499\n> loss at epoch 0 step 852: 6.713628\n> loss at epoch 0 step 853: 6.777599\n> loss at epoch 0 step 854: 6.799969\n> loss at epoch 0 step 855: 6.985339\n> loss at epoch 0 step 856: 6.620006\n> loss at epoch 0 step 857: 6.878152\n> loss at epoch 0 step 858: 6.783222\n> loss at epoch 0 step 859: 6.712417\n> loss at epoch 0 step 860: 6.734530\n> loss at epoch 0 step 861: 7.009552\n> 2018-06-25 14:57:23.706079: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:23.769647: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:23.809172: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 862: 6.570765\n> loss at epoch 0 step 863: 6.818643\n> loss at epoch 0 step 864: 6.707229\n> loss at epoch 0 step 865: 6.860226\n> loss at epoch 0 step 866: 6.832708\n> loss at epoch 0 step 867: 6.746671\n> loss at epoch 0 step 868: 6.925843\n> loss at epoch 0 step 869: 6.845281\n> loss at epoch 0 step 870: 6.901897\n> loss at epoch 0 step 871: 6.936728\n> loss at epoch 0 step 872: 6.793565\n> 2018-06-25 14:57:25.894904: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:25.958699: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:25.998658: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 873: 6.795174\n> loss at epoch 0 step 874: 6.897914\n> loss at epoch 0 step 875: 6.964663\n> loss at epoch 0 step 876: 6.933444\n> loss at epoch 0 step 877: 6.889087\n> loss at epoch 0 step 878: 6.728238\n> loss at epoch 0 step 879: 6.543335\n> loss at epoch 0 step 880: 6.785100\n> loss at epoch 0 step 881: 6.867868\n> loss at epoch 0 step 882: 6.626728\n> 2018-06-25 14:57:28.011803: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:28.078783: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:28.121272: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 883: 6.632171\n> loss at epoch 0 step 884: 6.638273\n> loss at epoch 0 step 885: 6.727106\n> loss at epoch 0 step 886: 6.755214\n> loss at epoch 0 step 887: 6.788674\n> loss at epoch 0 step 888: 6.618323\n> loss at epoch 0 step 889: 6.802135\n> loss at epoch 0 step 890: 6.812809\n> loss at epoch 0 step 891: 6.905511\n> 2018-06-25 14:57:29.982565: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:30.048872: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:30.089943: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 892: 6.660409\n> loss at epoch 0 step 893: 6.817567\n> loss at epoch 0 step 894: 6.782594\n> loss at epoch 0 step 895: 6.746886\n> loss at epoch 0 step 896: 6.679479\n> loss at epoch 0 step 897: 6.696771\n> loss at epoch 0 step 898: 6.642287\n> loss at epoch 0 step 899: 6.870078\n> 2018-06-25 14:57:31.693334: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:31.761710: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:31.805339: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 900: 6.774796\n> loss at epoch 0 step 901: 6.841524\n> loss at epoch 0 step 902: 6.682564\n> loss at epoch 0 step 903: 6.693571\n> loss at epoch 0 step 904: 6.603614\n> loss at epoch 0 step 905: 6.705801\n> loss at epoch 0 step 906: 6.713558\n> loss at epoch 0 step 907: 6.889330\n> 2018-06-25 14:57:33.459800: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:33.528563: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:33.573187: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 908: 6.726191\n> loss at epoch 0 step 909: 6.945642\n> loss at epoch 0 step 910: 6.768640\n> loss at epoch 0 step 911: 6.801554\n> loss at epoch 0 step 912: 6.867862\n> loss at epoch 0 step 913: 6.563491\n> loss at epoch 0 step 914: 6.853379\n> loss at epoch 0 step 915: 6.724048\n> 2018-06-25 14:57:35.341966: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:35.412082: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:35.456774: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 916: 6.639415\n> loss at epoch 0 step 917: 6.881902\n> loss at epoch 0 step 918: 6.735335\n> loss at epoch 0 step 919: 6.688423\n> loss at epoch 0 step 920: 6.636367\n> loss at epoch 0 step 921: 6.717305\n> loss at epoch 0 step 922: 6.629582\n> 2018-06-25 14:57:36.960963: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:37.033898: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:37.079990: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 923: 6.682681\n> loss at epoch 0 step 924: 6.882014\n> loss at epoch 0 step 925: 6.731138\n> loss at epoch 0 step 926: 6.588252\n> loss at epoch 0 step 927: 6.824276\n> loss at epoch 0 step 928: 6.707603\n> loss at epoch 0 step 929: 6.748030\n> 2018-06-25 14:57:38.690123: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:38.767782: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:38.816409: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 930: 6.658401\n> loss at epoch 0 step 931: 6.748807\n> loss at epoch 0 step 932: 6.802845\n> loss at epoch 0 step 933: 6.590845\n> loss at epoch 0 step 934: 6.749659\n> loss at epoch 0 step 935: 6.701960\n> 2018-06-25 14:57:40.158598: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:40.234263: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:40.282647: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 936: 6.775355\n> loss at epoch 0 step 937: 6.781133\n> loss at epoch 0 step 938: 6.600548\n> loss at epoch 0 step 939: 6.747424\n> loss at epoch 0 step 940: 6.605201\n> 2018-06-25 14:57:41.416798: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:41.496970: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:41.550523: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 941: 6.672260\n> loss at epoch 0 step 942: 6.614303\n> loss at epoch 0 step 943: 6.726663\n> loss at epoch 0 step 944: 6.694652\n> loss at epoch 0 step 945: 6.590765\n> 2018-06-25 14:57:42.720835: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:42.800870: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:42.852718: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 946: 6.639668\n> loss at epoch 0 step 947: 6.581154\n> loss at epoch 0 step 948: 6.780156\n> loss at epoch 0 step 949: 6.619957\n> loss at epoch 0 step 950: 6.680293\n> 2018-06-25 14:57:44.034047: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:44.116253: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:44.167625: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 951: 6.767905\n> loss at epoch 0 step 952: 6.694592\n> loss at epoch 0 step 953: 6.586446\n> loss at epoch 0 step 954: 6.691846\n> 2018-06-25 14:57:45.122539: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:45.204973: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:45.258423: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 955: 6.994371\n> loss at epoch 0 step 956: 6.858140\n> loss at epoch 0 step 957: 6.646007\n> loss at epoch 0 step 958: 6.721205\n> 2018-06-25 14:57:46.238911: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:46.325236: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:46.380011: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 959: 6.589066\n> loss at epoch 0 step 960: 6.683684\n> loss at epoch 0 step 961: 6.672047\n> 2018-06-25 14:57:47.046723: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:47.203337: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:47.260355: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 962: 6.725653\n> loss at epoch 0 step 963: 6.720684\n> loss at epoch 0 step 964: 6.636925\n> 2018-06-25 14:57:47.932850: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:48.021669: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:48.077942: I tensorflow/c/eager/c_api.cc:856] Executing\n> op AddN in device /job:localhost/replica:0/task:0/device:GPU:0\n> loss at epoch 0 step 965: 6.461943\n> loss at epoch 0 step 966: 6.716685\n> loss at epoch 0 step 967: 6.541767\n> 2018-06-25 14:57:48.839285: I tensorflow/c/eager/c_api.cc:856] Executing\n> op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n> 2018-06-25 14:57:58.895255: W\n> tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc)\n> ran out of memory trying to allocate 90.19MiB. Current allocation summary\n> follows.\n> 2018-06-25 14:57:58.895406: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): Total\n> Chunks: 3, Chunks in use: 3. 768B allocated for chunks. 768B in use in bin.\n> 12B client-requested in use in bin.\n> 2018-06-25 14:57:58.895440: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): Total\n> Chunks: 17, Chunks in use: 16. 8.5KiB allocated for chunks. 8.0KiB in use\n> in bin. 8.0KiB client-requested in use in bin.\n> 2018-06-25 14:57:58.895466: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): Total\n> Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in\n> bin. 1.0KiB client-requested in use in bin.\n> 2018-06-25 14:57:58.895488: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): Total\n> Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B\n> client-requested in use in bin.\n> 2018-06-25 14:57:58.895517: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): Total\n> Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B\n> client-requested in use in bin.\n> 2018-06-25 14:57:58.895538: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): Total\n> Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B\n> client-requested in use in bin.\n> 2018-06-25 14:57:58.895565: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): Total\n> Chunks: 5, Chunks in use: 3. 124.0KiB allocated for chunks. 79.0KiB in use\n> in bin. 78.9KiB client-requested in use in bin.\n> 2018-06-25 14:57:58.895592: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): Total\n> Chunks: 903, Chunks in use: 901. 28.30MiB allocated for chunks. 28.24MiB in\n> use in bin. 28.15MiB client-requested in use in bin.\n> 2018-06-25 14:57:58.895618: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): Total\n> Chunks: 402, Chunks in use: 402. 25.56MiB allocated for chunks. 25.56MiB in\n> use in bin. 25.16MiB client-requested in use in bin.\n> 2018-06-25 14:57:58.895639: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): Total\n> Chunks: 19, Chunks in use: 19. 2.51MiB allocated for chunks. 2.51MiB in use\n> in bin. 2.47MiB client-requested in use in bin.\n> 2018-06-25 14:57:58.895673: I\n> tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): Total\n> Chunks: 8, Chunks in use: 8. 2.97MiB allocated for chunks. 2.97MiB in use\n> in bin. 2.75MiB client-requested in use in bin.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20218#issuecomment-399873140>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbOLEdJJdr-VX_sTRU0d_OFNXawVks5uAKFUgaJpZM4UzWuc>\n> .\n>\n\n\n-- \n - Alex\n", "Looks like one issue is that summaries are adding to global collections. Could you try taking summaries out and seeing if there's still a leak? Or post a training loop and I can give it a try.", "Hi @allenlavoie , I tried to remove the summaries, but got the same result. So it looks like this is not an error about summaries. Still thank you for your reply.", "@alextp , I think so. Is there any solution? Or I sent you the complete program code, can you help me to debug it? Can I communicate with you about this issue via email? Thank you very much!", "Well, the summary issue is fixed at least. I'm happy to track down the other memory issue, but you'll need to post a training loop which reproduces the issue (and which I can run).", "(Leaving it closed for now, but feel free to re-open with a repro.)", "Still facing this problem in 1.10.0, hope it could be fixed soon.", "@LeoLai930603 could you post a snippet to reproduce?", "Sure, I post it on my github: [https://github.com/LeoLai930603/Eager_OOM_reproduce/tree/master](url)\r\n\r\nBTW, I tried this code on Windows 10 with Anaconda (Python 3.5, TensorFlow 1.8.0), but I think this error could also occur on Linux. I met this error on both CPU and GPU, hope such information would be helpful for you.", "@LeoLai930603 thanks, that's super helpful. Looks like one issue was assigning functions to EagerTensors (as when slicing); I have a workaround in the process of submitting for that (and will properly patch it soon).\r\n\r\nLooks like something RNN related is still holding on to LSTMStateTuples, so I'll take a look at that too.", "I will follow up and properly patch variable slicing.\r\n\r\nAfter that it's still leaking memory, but it looks like it's a zeros/ones caching issue with backprop. @akshaym was going to take a look (at https://github.com/LeoLai930603/Eager_OOM_reproduce/tree/master ).\r\n\r\nI think it's a caching issue because clearing the cache after `train_step` (in eager_biLSTM_trainer.py) keeps the memory stable and means the number of PyObjects doesn't increase:\r\n\r\n```\r\nfrom tensorflow.python.eager import context\r\ncontext.context()._clear_caches()  # Increasing memory and PyObject count without this\r\nimport gc\r\ngc.collect()\r\nprint(len(gc.get_objects()))\r\n```\r\n", "Thanks a lot! I will give it a try later. Would this work for Eager on GPU? I am not sure whether this issue is only related to the gc on host. As the author mentioned at first, the problem could also be caused by insufficient GPU memory allocation.", "The PyObject issue was probably just host-side (and now that I think about it, you may not have run into it unless you were using a nightly from the past week or so). Not sure the gc.collect() is necessary there; just using it to get a stable count of objects in case there are reference cycles.\r\n\r\nThe zeros caching issue will affect GPU too, since (I assume) we cache GPU Tensors. But all my testing was CPU-only so far.\r\n\r\nThis caching helps avoid re-creating zero Tensors during backprop and is limited to 256 entries, but when running your model ~60 entries was tens of gigabytes. Sounds from Akshay like the solution will be to cap the size of the cache. (And maybe we should cap it more aggressively for GPU Tensors since memory will be more constrained?)", "Agree, and I was wondering whether it is possible to expose this setting (caching size or entry limit) to the user, so we can manage the training and deployment setting by ourselves? And I believe it deserves more attention to the RNN model implementation with Eager mode, for example, it is very inconvenient to process the trainable variables in LSTMCells during training, because 1) it is lazy-initialized, and I was expecting to have a explicit way to initialize the weights in it like Conv2d, 2) the trainable variables are nested list, which makes the following pair-matching with gradients more difficult (you have to flatten the list by your own in stead of just a zip function in tutorial). I hope such cells could provide more features that make them more eager-friendly. At least that would help debugging when facing memory leak.", "Akshay's change limits caching to a few megabytes at most, so hopefully never an issue.\r\n\r\nI believe the future of RNN APIs are being decided now (until Sept. 10): https://github.com/tensorflow/community/pull/15 May be worth reading over the design document and seeing if your concerns are already addressed. Describing the pain points in a comment would be helpful if not.", "Much appreciate that! I will wait for the dev20180831 version to test it. Hope this could be fixed perfectly.", "There seems to be still a problem with slicing operator resulting in ResourceExhausteError, when using gradient tape, I was able to get around it by avoiding the slicing operators in the loss calculation. "]}, {"number": 20217, "title": "Compile Error: tensorflow/tensorflow/examples/label_image/main.cc ", "body": "I have compiled c++ lib tf_r1.3 with cuda 8.0 ,cudnn 6, vs2015 update3  on Windows10 successfullly.\r\n\r\nWhen I try to compile example \"tensorflow/tensorflow/examples/label_image/main.cc\" on WIndows10, it reports error:\r\nError\tC3861\t'EndsWith': identifier not found\t\r\n\r\nAnd after I replace 'EndsWith(X,Y)' with '*Split(X,'.').end()==Y' to bypass the first error, it reports another error:\r\nLNK2001\tunresolved external symbol \"public: static unsigned __int64 const tensorflow::StringPiece::npos\" (?npos@StringPiece@tensorflow@@2_KB)\t\r\n\r\nplease help to debug.\r\nThanks!\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20216, "title": "Question about tf.unsorted_segment_max", "body": "I'ven seen the tf r1.8 has add some segment ops such as tf.unsorted_segment_max. But the output dimention doesn't keep the same with the input. I wonder if there is any op just do the same thing as tf.unsorted_segment_max, but output Tensor with the same dim as input. For example, if I put the parameter keep_dims=True to tf.reduce_sum, the output dim is as my wish, but without id segmentation. \r\n", "comments": ["Could you add an example of your desired output  for a given input? tf.unsorted_segment_max doesn't reduce over multiple axes as tf.reduce_max does, so I'm not exactly sure where you'd like to keep the dimension.", "@PhilJd for exampe\r\ninput = [[1,2,3], [4,5,6], [7,8,9]]\r\nids =[[0,0,0],[1,1,1],[2,2,2]]\r\ntf.unsorted_segment_max(input, ids) outputs [3,6,9]\r\nI want to keep dims as the input so that the output can be [[3,3,3],[6,6,6],[9,9,9]]\r\nIf it is not clear enough, please tell me.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@angersson I don\u2018t  think you should close this issue, because as far as I'm concerned, tf doesn't have the feature I need. And in fact this issue is a feature request. #18311 seems to be the same feature request as me. But it seems not to be noticed by the members or contributors. So what should I do to add this feature in tensorflow?", "@zhaoxin19 The behaviour of what you describe is different from what keepdims in the reduce operations does,  you can obtain your result with \r\ninput = [[1,2,3], [4,5,6], [7,8,9]]\r\nids =[[0,0,0],[1,1,1],[2,2,2]]\r\nsegment_max = tf.unsorted_segment_max(input, ids)\r\nresult = tf.gather(segment_max, ids)   # outputs  [[3,3,3],[6,6,6],[9,9,9]]\r\nThis should be as efficient as a native op.\r\n", "@PhilJd thank you very much! That's what I want!"]}, {"number": 20215, "title": "Tensorflow does not work", "body": "### System information\r\n- **I used stock example script provided in TensorFlow)**:\r\n- **Windows -7 64 bit**:\r\n- **installed tensorflow \"pip install tensorflow --upgrade\"**:\r\n- **TensorFlow version 1-8-0**:\r\n- **Python version 3.6.5**: \r\n- **Exact command to reproduce**:\r\nimport tensorflow\r\n\r\n### Describe the problem\r\nWhen i run in python\r\n_import tensorflow_,\r\ni receive\r\n\"failed to load native tensorflow runtime\"\r\n\r\n### Source code / logs\r\nSource:\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n\r\nFull traceback:\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"K:\\Python\\testTensorflow.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Have I written custom code -NO\r\nOS Platform and Distribution Win7 64bit AMD\r\nTensorFlow installed from \"pip install --upgrade tensorflow\"\r\nBazel version NO BAZEL\r\nCUDA/cuDNN version NO CUDA\r\nGPU model and memory NO GPU\r\n", "https://github.com/tensorflow/tensorflow/issues/18530\r\nrecommended to install tensorflow 1.5 (i installed without GPU)\r\nIt Works!", "I am experiencing same problem in current TF 1.9.\r\nDowngrading to TF1.5 helps but since it is old version it is missing some features I need.\r\nPlease reopen the issue.\r\n", "@janstrelka : Assuming your issue is the same as what was talked about in #18530 then this means that the CPU you're running on does not support AVX instructions. Since TensorFlow 1.6, the release binaries require CPUs with AVX support. For older CPUs, you'll have to either build from source or use binaries built by others in the community (e.g. https://github.com/yaroslavvb/tensorflow-community-wheels )", "@asimshankar : Understand. Thank you."]}, {"number": 20214, "title": "tf.flags not compatible with old versions", "body": "The new `tf.flags` APIs (>1.4.1) perform badly, not compatible with old ones, even raise weird exception like: \r\n\r\n```\r\nabsl.flags._exceptions.IllegalFlagValueError: flag --env_worker_samples=100000.0: Expect argument to be a string or int, found <class 'float'>\r\n```\r\n\r\nIn fact, I use '--env_worker_samples 100000' in command line.\r\n\r\nWhy does the official not provide any instruction on migrating `tf.flags` to new version TF?", "comments": ["Some related, unsolved but closed issues including #17132", "I think @asimshankar answered the similar question in #17734.", "@facaiy I see that since TensorFlow 1.5, __absl-py__ is used as arguments parser. I have fixed this exception. Thanks."]}, {"number": 20213, "title": "Resource Exhausted  Error", "body": "```\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[98384,15003]\r\n         [[Node: dynamic_seq2seq/decoder/output_projection/Tensordot/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dynamic_seq2seq/decoder/output_projection/Tensordot/Reshape, dynamic_seq2seq/decoder/output_projection/Tensordot/Reshape_1)]]\r\n         [[Node: dynamic_seq2seq/truediv/_217 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_783_dynamic_seq2seq/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\ni get the above error when i try to train my model using the tensorflow gpu", "comments": ["@jaiaravind4u this error means that you do not have enough memory to create a tensor with this  shape: _[98384,15003]_ it's a very huge tensor though.\r\n", "Thanks i solved the isuues on my side its working now"]}, {"number": 20212, "title": "key ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 20211, "title": "C++ compilation of rule '@nsync//:nsync_cpp' failed", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: ubuntu18.04\r\n- **TensorFlow installed from**: anaconda3\r\n- **TensorFlow version**: 1.8.0\r\n- **Python version**:  3.6.4\r\n- **Bazel version**: 0.14.1\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures\r\n\r\n```\r\n$ bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/DataType.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Graph.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/NativeLibrary.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Operand.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Operation.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/OperationBuilder.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Output.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/SavedModelBundle.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Session.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Shape.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Tensor.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/TensorFlow.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/TensorFlowException.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Tensors.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/package-info.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/types/UInt8.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/contrib/android/BUILD:44:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/types/package-info.java' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/tech1/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analysed target //tensorflow/examples/android:tensorflow_demo (2 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/tech1/.cache/bazel/_bazel_tech1/1de281b1fba5f525d1ddad248a9536d4/external/nsync/BUILD:462:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): false failed: error executing command \r\n  (cd /home/tech1/.cache/bazel/_bazel_tech1/1de281b1fba5f525d1ddad248a9536d4/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH='/home/tech1/anaconda3/envs/tensorflow/bin:~/anaconda3/bin:/home/tech1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:$/home/tech1/android/android-sdk/platform-tools:$/home/tech1/android/android-sdk/tools:/home/tech1/android/android-sdk/tools:/home/tech1/android/android-sdk/platform-tools:/home/tech1/android/android-ndk' \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /bin/false -MD -MF bazel-out/android-armeabi-v7a-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/sem_wait.pic.d -fPIC -iquote external/nsync -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/android-armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/android-armeabi-v7a-opt/genfiles/external/nsync/public -isystem bazel-out/android-armeabi-v7a-opt/bin/external/nsync/public -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/arm -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix '-D_POSIX_C_SOURCE=200809L' -pthread -c external/nsync/internal/sem_wait.c -o bazel-out/android-armeabi-v7a-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/sem_wait.pic.o)\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nINFO: Elapsed time: 151.160s, Critical Path: 18.85s\r\nINFO: 196 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\ncould you help me?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: ubuntu18.04\r\n- **TensorFlow installed from**: anaconda3\r\n- **TensorFlow version**: 1.8.0\r\n- **Python version**:  3.6.4\r\n- **Bazel version**: 0.14.1\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: bazel build -c opt //tensorflow/examples/android:tensorflow_demo --verbose_failures", "once changed android NDK from r17b to r15c, bazel build completed successfully.\r\nsorry for silly question."]}, {"number": 20210, "title": "Support openmp in tflite for operations like depthwise convolution", "body": "In tflite, some operations like convolution have openmp support from Eigen library, while for others like depthwise convolution they don't have openmp optimization yet.\r\n\r\nDo we have any plans to add openmp to all operations in tflite?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: NO\r\nOS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: binary\r\nTensorFlow version: 1.18.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "This is an interesting idea, we would be open to external contribution if it was shown to be performant and not increase the binary size too much.", "I'm not sure when we'll get to this, so adding to contributions welcome.", "We have no immediate plans to explore OpenMP support."]}, {"number": 20209, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match:", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n", "@skye @shantianyang \r\nhi ,did you solve the proble ,I train the yolov2 and use it in office demo,there is the problem.\r\n2018-12-27 09:41:33.780 17423-17443/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input], outputs:[output]\r\n2018-12-27 09:41:33.783 17423-17443/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 17423\r\n    java.lang.IllegalArgumentException: ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,1,256] vs. shape[13] = [0,1,1,256]\r\n    \t [[{{node concat_9}} = ConcatV2[N=19, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Reshape_152, Reshape_153, Reshape_154, Reshape_155, Reshape_156, Reshape_157, Reshape_158, Reshape_159, Reshape_160, Reshape_161, Reshape_162, Reshape_163, Reshape_164, Reshape_165, Reshape_166, Reshape_167, Reshape_168, Reshape_169, Reshape_170, concat_9/axis)]]\r\n        at org.tensorflow.Session.run(Native Method)\r\n        at org.tensorflow.Session.access$100(Session.java:48)\r\n        at org.tensorflow.Session$Runner.runHelper(Session.java:314)\r\n        at org.tensorflow.Session$Runner.run(Session.java:264)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:173)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:289)\r\ncan anyone help me,thanks a lot"]}, {"number": 20208, "title": "tf.distributions.Gamma quantile not implemented, yet exists in documentation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6\r\n\r\nAtempting to run\r\n\r\n`tf.distributions.Gamma(concentration=10.,rate=0.2).quantile(0)`\r\nwe see that\r\n\r\n> NotImplementedError: quantile is not implemented\r\n\r\nBut in the [documentation](https://www.tensorflow.org/api_docs/python/tf/distributions/Gamma) there is no indication of this function not being implemented, as the function is documented as all others are.\r\n\r\nIs there something missing from my installation and the function should be present, or is it wrongly documented as existing?", "comments": ["I think gamma is not a monotone function, hence its `quantile` cannot be implemented. \r\n\r\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Gamma_plot.svg/650px-Gamma_plot.svg.png)", "@facaiy Sorry, did you mean to say \r\n\r\n> hence its `quantile` is **not** implemented\r\n\r\nIf that is so, I think we could remove it from the documentation. Same goes for exponential distribution and a few others. Their current documentation is written as if these functions are implemented.", "@MarkGreeny I afraid that `quantile` of gamma doesn't exist, and that's why we leave it NotImplementedError. I agree that we'd better document it.", "Hi,\r\n  This is documented incorrectly. The quantile function can exist but would require some implementation of something like this: http://reference.wolfram.com/language/ref/InverseGammaRegularized.html.\r\n\r\nThe issue here is that all these public methods exist on distributions (which call private methods), and hence docstrings always exist (even though the private methods might not be implemented).", "@srvasude so can or should we mark this \"docs\"? Is there a way to override the docstring for tf.distributions.Gamma? ", "The same problem exists for Beta - this is very confusing. Is it not implemented for beta?", "Hi, it was implemented in R based on Best, D. J. and D. E. Roberts (1975); Percentage Points of the Chi-Squared Distribution; Applied Statistics 24, page 385, where they added some Newton-Raphson algorithm for stability on top of it.\r\n[https://github.com/griffithlab/regtools/blob/master/src/utils/rmath/qgamma.c](https://github.com/griffithlab/regtools/blob/master/src/utils/rmath/qgamma.c)\r\nIs it OK to use that implementation as the basis for the Tensorflow one?", "hi, what about cheating and using scipy? Is it legit?\r\n\r\n/######################\r\n/### EXAMPLE CODE ####\r\n/######################\r\n\r\nimport tensorflow as tf\r\nfrom scipy import stats\r\n\r\nconcentration=3.\r\nrate=1.5\r\nquantile = 0.000001\r\n\r\ndef my_func(x):\r\n    return np.float32(stats.gamma(a=concentration, loc=0., scale=1/rate).ppf(x))\r\n\r\nquantile = tf.py_func(my_func, [quantile], tf.float32)\r\n\r\nwith tf.Session() as sess:\r\n    print(quantile.eval())", "Same problem goes for Mixture (in tensorflow_probability.distributions). Very disappointing to see after all my struggles getting to the point were I was going to use the quantiles for a plot.\r\n\r\n@facaiy If I am not wrong, a quantile can be derived from the CDF (if available) by interpolation, since the CDF is always monotonically increasing wrt. x. Besides, the Gamma distribution is not the same as the Gamma function, although the Gamma function is used in the definition of the Gamma distribution. Anyway, interpolation may be a big pillow to swallow (to invent a new saying). I am sure there exist better methods among the links in this discussion.", "I just ran into this too, and it's a major problem for me -- I need the gamma quantile.\r\n\r\nEliasHasle is correct -- the quantile exists, and is simply the inverse of the CDF, which is monotonic.  \r\n\r\nNote also that scipy has an implementation -- see scipy.special.gammaincinv(). I assume it shares code with scipy.stats.gamma.ppf().", "I've come up with a work-around, based on Newton-Raphson root finding and the tf.while_loop() construct.  The following code computes the inverse of the regularized lower incomplete gamma function (as implemented by tf.igamma()), which is equivalent to the gamma distribution quantile method:\r\n\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom scipy.special import gammaln, gammainc\r\nfrom scipy.stats import gamma\r\n\r\nnsamp=10 # Dimension of vector of cdfs to be inverted\r\nndim = 40 # 'a' (shape) parameter of gamma distribution\r\neps = 1.0E-04 # Termination accuracy\r\nmaxiter = 20 # Maximum number of iterations\r\n\r\nNdim = tf.constant(ndim, dtype=tf.float32)\r\nlnorm = gammaln(ndim)\r\n\r\n\r\ndef body(tgt, currx, currf, currdf, iter):\r\n    iter = iter + 1\r\n    nextx = currx - (currf - tgt)/ currdf\r\n    nextf = tf.igamma(Ndim, nextx)\r\n    nextdf = tf.exp(-nextx + tf.log(nextx)*(Ndim-1) - lnorm)\r\n    return tgt, nextx, nextf, nextdf, iter\r\n\r\ndef cond(tgt, currx, currf, currdf, iter):\r\n    disc = tf.abs((tgt - currf)/tgt)\r\n    not_converged = tf.logical_and( tf.reduce_any(disc > eps) , iter < maxiter )\r\n    return not_converged\r\n\r\nx = gamma.rvs(ndim, size=nsamp) # true x values to be located\r\nu = gammainc(ndim, x) # true target function values\r\nU = tf.constant(u, dtype=tf.float32)\r\nF = tf.constant(np.zeros(nsamp), dtype=tf.float32)\r\ndF = tf.constant(np.ones(nsamp), dtype=tf.float32)\r\nX = tf.constant(np.ones(nsamp)*(ndim-1), dtype=tf.float32) # inital guesses\r\nIter = tf.constant(0)\r\n\r\nloop_vars = [U, X, F, dF, Iter]\r\n\r\nres = tf.while_loop(cond, body, loop_vars)\r\n\r\nwith tf.Session() as sess:\r\n    tgt, currx, currf, currdf, iter = sess.run(res)\r\n\r\n    print(\"Target x:\\n\", x)\r\n    print(\"Target funcvals:\\n\", tgt)\r\n    print(\"Solved x:\\n\", currx)\r\n    print(\"Solved funcvals:\\n\", currf)\r\n    print(\"Iterations: %d\" % iter)\r\n\r\n```\r\n\r\nI've done some restricted testing, and it seems to work for a good range of ndim values. In the range 2-40 convergence occurs in 5-8 iterations at for eps=1.0E-04.", "This is not an ideal workaround, because it is inefficient from the standpoint of autodiff. The graph of the forward-flowed function will consist of those 5-8 iterations that have to be differentiated through. The derivatives of this function are known in closed form, and a sane implementation would use those expressions.\r\n\r\nWhich is to say, an effort commitment by a tensorflower to fix this problem would still be welcome. ", "A similar issues is found for tfp.distributions.TruncatedNormal.\r\n\r\nNotImplementedError: quantile is not implemented: TruncatedNormal, yet it exists in the [documentation ](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/TruncatedNormal#quantile) ", "This thread is sad. We're not talking about the Gamma function, but the gamma distribution. Where you use `gamma(alpha)` as a **constant**. \r\n\r\nFor a workaround implementation, which also works with the mixture of gamma distributions see https://github.com/tensorflow/probability/issues/659 ", "@Demetrio92 , what you are suggesting in no way addresses the issue discussed here.  The gamma distribution inverse  (and yes, we already knew it was a distribution, and you would be aware of it had you read the thread in full) has known specific approximations and algorithms, as do its derivatives.  A generic quadrature or iterative approach is at best a work-around.  What is required here is for known working code from scipy.special (for example) to be tensorflow-ified by a tensorflower. It is not helpful to suggest that the issue is addressed by your work-around (or by mine, above, for that matter).", "Closing this as the Gamma distribution and related friends have quantiles now."]}, {"number": 20207, "title": "Build sqlite3 with json1 extension", "body": "Builds of SQLite by default often with with the [builtin json1 Extension enabled](https://www.sqlite.org/json1.html).\r\n\r\nE.g. the `libsqlite3-dev` enables this (as used in TF's CI!) or sqlite3 that comes with Python has this enabled.\r\n\r\nFor TF, this impacts TF's [SqlDataset reader](https://www.tensorflow.org/api_docs/python/tf/contrib/data/SqlDataset):\r\n\r\nFor example, if you have a field that contains a json-encoded list `[1,2,3]` or dictionary `{ 'foo': 'bar'}`, you can simply extract those f.e. in your SQL Query:\r\n```sql\r\njson_extract(data, '$.foo')\r\n```\r\n\r\nI think this is very useful for cases where you have f.e. a variable list of classes `['car', 'boat']` or (a variable number of) coordinates (e.g. polygons, bounding boxes) and you want to save them in one encoded field: e.g. `\"[[0.0, 1.0, 2.0, 2.0], ..]\"`\r\n\r\nThe added code simply adds the flag to enable json1, and a test to make sure it works.", "comments": []}, {"number": 20206, "title": "Reproducing the exact tensorflow_inception_v3_stripped_optimized_quantized.pb file", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14:04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.9\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.13\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 7/6\r\n- **GPU model and memory**: 1060\r\n- **Exact command to reproduce**:\r\n\r\nInspecting the available quantized/frozen model of inception-v3 to be used with HVX (`tensorflow_inception_v3_stripped_optimized_quantized.pb`), I see that the spatial_squeeze and some other operations are removed (stripped) and the input and output layers are 'Mul' and 'Softmax' (as they are not defined in nnlib). I tried to reproduce this graph using a pretrained model of my own, but still, end up with spatial_squeeze and thus, it fails to execute on HVX. Also, my input and output layers are called `input` and `InceptionV3/Predictions/Reshape_1`. \r\n\r\nCould you provide the steps to reproduce the exact `tensorflow_inception_v3_stripped_optimized_quantized.pb` using the `graph_transform`/`Optimize_for_inference`/`Quantize_graph` tools, so others can follow these steps to build their own model for HVX? Thanks, @satok16 @petewarden ", "comments": ["Unfortunately the old HVX code path is no longer supported, since we're focused on the NN API's implementation instead. One unappetizing alternative is to use an older version of TensorFlow to generate the correct models, but I don't have great directions on that. Closing this as obsolete."]}, {"number": 20205, "title": "F-RCN protobuf issue decoding string when reading in label_map", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Stock\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.6.0 (GPU)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 (This is what I deduced from the help script provided)\r\n- **GPU model and memory**:Tesla P100-PCIE-16GB\r\n- **Exact command to reproduce**:python /home/_myhomedir_/tensorflow/models/research/object_detection/train.py --pipeline_config_path=/home/_myhomedir_/test_rfcn/rfcn_resnet101-1.config --train_dir=gs://mygcsbucket/mydir --ps_tasks=1\r\nConfig File: [rfcn_resnet101-1.txt](https://github.com/tensorflow/tensorflow/files/2125622/rfcn_resnet101-1.txt)\r\n\r\n### Describe the problem\r\nThe bug occurs when trying to read in the label map (stack trace below). I am using the exact same setup when I test with ssd_mobilenet config file (points to ssd_mobilenet initial checkpoint) and training runs fine. Same for F-RCNN. Only when using the R-FCN config file is when this error is thrown. It seems to be related to protobuf and I have tried some various fixes found in the issues section. None seem to work, and I am unsure how/why that is a problem if it works for ssd and frcnn. I also have attache dmy config file (as .txt due to file type restrictions). Thanks for the help.\r\n\r\nZach\r\n\r\n### Source code / logs\r\n```\r\nWARNING:tensorflow:From /home/zachary_mostowsky/tensorflow/models/research/object_detection/trainer.py:257: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nTraceback (most recent call last):\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/train.py\", line 184, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/train.py\", line 180, in main\r\n    graph_hook_fn=graph_rewriter_fn)\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/trainer.py\", line 264, in train\r\n    train_config.prefetch_queue_capacity, data_augmentation_options)\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/trainer.py\", line 59, in create_input_queue\r\n    tensor_dict = create_tensor_dict_fn()\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/train.py\", line 121, in get_next\r\n    dataset_builder.build(config)).get_next()\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/builders/dataset_builder.py\", line 155, in build\r\n    label_map_proto_file=label_map_proto_file)\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/data_decoders/tf_example_decoder.py\", line 245, in __init__\r\n    use_display_name)\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/utils/label_map_util.py\", line 152, in get_label_map_dict\r\n    label_map = load_labelmap(label_map_path)\r\n  File \"/home/zachary_mostowsky/tensorflow/models/research/object_detection/utils/label_map_util.py\", line 137, in load_labelmap\r\n    label_map.ParseFromString(label_map_string)\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n\r\n```", "comments": ["this looks like an object detection issue.\r\n\r\n@derekjchow WDYT?", "I think you have your label map and TF record paths mixed up:\r\n\r\n```\r\ntrain_input_reader {\r\n  label_map_path: \"/home/zachary_mostowsky/rev20180614/xview_train_mapped_id-1.record\"\r\n  tf_record_input_reader {\r\n    input_path: \"/home/zachary_mostowsky/rev20180614/label_map.pbtxt\"\r\n  }\r\n}\r\n```\r\n\r\n`label_map_path` should be \"\\*\\*/\\*.pbtxt\", while `input_path` should be \"\\*\\*/\\*.record\". They are mixed up in your config.", "Thank you. I start thinking it is these complicated errors when they are in reality very simple. Sorry to waste your time."]}, {"number": 20204, "title": "1.9.0rc2 cherry-pick request: [tf.data] Properly export `choose_from_datasets()`", "body": "This change adds a missing import statement in the `tf.contrib.data` library. The `tf.contrib.data.choose_from_datasets()` is already mentioned in the release notes, and its implementation is there, but I neglected to check that it was accessible via the public API. Not including it in the release would make the release notes incorrect.", "comments": ["Thanks Mike!"]}, {"number": 20203, "title": "Add a single positional argument mode for shape inference in subclass\u2026", "body": "\u2026ed Models.\r\n\r\nAllows fit() when call's signature looks something like call(x, training=True).\r\n\r\nCalling conventions are \"inputs\", single positional, and multiple positional. Right now the distinction between \"inputs\" and single positional calling conventions is the text of one error message. Both support shape inference (which just hasn't been implemented for multiple positional input arguments yet).\r\n\r\nPiperOrigin-RevId: 198815483", "comments": []}]