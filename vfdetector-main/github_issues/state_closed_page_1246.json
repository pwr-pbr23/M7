[{"number": 15773, "title": "How to define multiple loss function and train_op  in tf.estimator.EstimatorSpec", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0\r\n- **Python version**: 2.7.12\r\n- **CUDA/cuDNN version**: 6.0\r\n- **GPU model and memory**: 1080 Ti + 1080\r\n\r\n### Describe the problem\r\nI'm currently implementing a pose estimation system and I defined my network with 3 loss and train_op in each of degree, yaw, pitch and roll. And I'm current using your tf.estimator API which I think is pretty convenient to monitor the system, however I found that I may only be able to define one loss and train_op using this set of API. I would like to know if there is any solution to train and monitor the system with multiple loss and train_op at the same time. Thanks.\r\n\r\n### Source code / logs\r\n```\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions=predictions,\r\n        loss=[yaw_total_loss, pitch_total_loss, roll_total_loss],  # error\r\n        train_op=[yaw_train_op, pitch_train_op, roll_train_op],  # error\r\n        eval_metric_ops=None)\r\n```\r\n", "comments": ["I'm also struggling with similar issues. I see that the Estimator framework includes experimental support for multiple \"Heads\" which will probably address this particular issue. There is paper on Arxiv that includes a section on Heads: https://arxiv.org/pdf/1708.02637.pdf. However, the document is just an outline, and the details are still unclear to me.", "@ispirmustafa any comments on this?", "for train op:\r\n`EstimatorSpec(train_op = tf.group(yaw_train_op, pitch_train_op, roll_train_op), ...)`\r\n\r\nfor loss:\r\nto see each of them separately on Tensorboard use: \r\n```\r\ntf.summary.scalar('yaw_total_loss', yaw_total_loss)\r\ntf.summary.scalar('pitch_total_loss', pitch_total_loss)\r\ntf.summary.scalar('roll_total_loss', roll_total_loss)\r\n\r\nEstimatorSpec(loss=yaw_total_loss + pitch_total_loss + roll_total_loss, ...)\r\n```", "@ispirmustafa \r\nQuestion on loss. I think your code makes the three train_op the same loss, the sum of three different losses. However, @developer-mayuan wants to run the three train_op with three separate losses I think.\r\n@developer-mayuan Do you have any solution yet?", "@GHAIGIT it runs tree train op with different losses. something like:\r\n```\r\ntrain_op_1 = optimizer.minimize(loss_1)\r\ntrain_op_2 = optimizer.minimize(loss_2)\r\ntrain_op = tf.group(train_op_1, train_op2)\r\n```", "@GHAIGIT @ispirmustafa is correct. Here is my implementation which works well:\r\n\r\n```\r\nfc_yaw_train_op = fc_optimizer.minimize(yaw_total_loss, var_list=fc_yaw_variables)\r\nfc_pitch_train_op = fc_optimizer.minimize(pitch_total_loss, var_list=fc_pitch_variables)\r\nfc_roll_train_op = fc_optimizer.minimize(roll_total_loss, var_list=fc_roll_variables)\r\n\r\ntrain_ops = tf.group(fc_yaw_train_op, fc_pitch_train_op, fc_roll_train_op)\r\n```", "@developer-mayuan @ispirmustafa \r\nThanks for your answer!\r\nI still don't understand the `loss` of `EstimatorSpec`, does not it join in the back propagation process? What is the effect of `loss` of `EstimatorSpec`? only for summary? If so, can I summary multiple losses by feed it with an array?\r\nI did not find any explanation in documentation, looking forward to your reply!", "@GHAIGIT According to my understanding, the loss is an operation (a node in the computing graph), Tensorflow will automatically calculate the gradient starting from this loss , do the back propagation and update the weights in `var_list`.\r\n\r\nThe `+` in `EstimatorSpec(loss=yaw_total_loss + pitch_total_loss + roll_total_loss, ...)` is not the value addition but connect 3 different loss operation to the `loss` operation. It will be more clear if you visualize your graph in TensorBoard.\r\n\r\nI believe `EstimatorSpec` will periodically evaluate the `loss` operator and get the loss value after certain steps.", "@GHAIGIT @developer-mayuan \r\nyes, `loss` in `EstimatorSpec` is just for summary and tensorboard. it doesn't impact any training operation.", "I'm struggling with the same thing (multiple losses in an estimator) and, following the template by @developer-mayuan , I get an additional error:\r\n\r\nNotFoundError (see above for traceback): Key dense_2/bias/Adam not found in checkpoint\r\n\r\nI'm using the standard 'optimizer = tf.train.AdamOptimizer(learning_rate=0.01),' but when I try to use that optimizer with multiple losses, I'm getting this error.\r\n\r\nAny ideas? @developer-mayuan - how did you initialize your optimizer?\r\n", "@mdfry I didn't do the optimization on the total loss for all variables, instead I doing the optimization separately for each loss.\r\n\r\n```\r\n# extract trainable variables\r\nres_variables = tf.get_collection(\r\n    tf.GraphKeys.GLOBAL_VARIABLESscope = 'resnet')\r\nfc_yaw_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                     scope='FC/yaw')\r\nfc_pitch_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                       scope='FC/pitch')\r\nfc_roll_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                      scope='FC/roll')\r\n\r\nfc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate * 5)\r\nres_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n\r\n# Batch norm requires update_ops to be added as a train_op dependency.\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    fc_yaw_train_op = fc_optimizer.minimize(yaw_total_loss,\r\n                                            var_list=fc_yaw_variables)\r\n    fc_pitch_train_op = fc_optimizer.minimize(pitch_total_loss,\r\n                                              var_list=fc_pitch_variables)\r\n    fc_roll_train_op = fc_optimizer.minimize(roll_total_loss,\r\n                                             var_list=fc_roll_variables)\r\n    res_train_op = res_optimizer.minimize(total_loss, global_step,\r\n                                          var_list=res_variables)\r\n``` ", "@developer-mayuan May I ask which dataset did you used to train your HPE model?", "@OceanWong1991 300W-LP.", "Are you sure `tf.group()` won't create separate graphs? @ispirmustafa \r\nThis [post](https://stackoverflow.com/questions/49953379/tensorflow-multiple-loss-functions-vs-multiple-training-ops) said `tf.group()` actually creates independent gradients.", "Hi @tengerye ,\r\nwhat do you mean by separate graphs?", "According to the [post](https://stackoverflow.com/questions/49953379/tensorflow-multiple-loss-functions-vs-multiple-training-ops) said, there are different sets of gradients, because the updates are performed at the same time @ispirmustafa . I don't know if my understanding is correct. The issue here looks the same as that post.\r\n\r\nThank you for your kind reply.", "Hi @tengerye \r\nNow I see the confusion. The answer in that [post](https://stackoverflow.com/questions/49953379/tensorflow-multiple-loss-functions-vs-multiple-training-ops) is wrong. Ignore that please. I'll answer that post too.\r\nThanks for pointing out that.", "```\r\n# optimization\r\n  all_vars = tf.trainable_variables()\r\n  gqn_var = [v for v in all_vars if 'GQN' in v.op.name]\r\n  dis_var = [v for v in all_vars if 'Discriminator' in v.op.name]\r\n\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    lr = _linear_lr_annealing(params['gqn_params'])\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\r\n    optimizer_dis = tf.train.AdamOptimizer(learning_rate=lr*4)\r\n    train_op_GQN = optimizer.minimize(\r\n        loss=elbo+g_loss,\r\n        global_step=tf.train.get_global_step(),\r\n        var_list= gqn_var\r\n    )\r\n    train_op_dis = optimizer_dis.minimize(\r\n        loss= d_loss,\r\n        global_step= tf.train.get_global_step(),\r\n        var_list=dis_var\r\n    )\r\n    train_op = tf.group(train_op_GQN, train_op_dis)\r\n    estimator_spec = tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        loss=elbo+g_loss+d_loss,\r\n        train_op=train_op)\r\n   return estimator_spec\r\n```\r\nIn my project I am designing a encoder-decoder network with a discriminator using the new tf Estimator API  but I am having some problems. I have 3 losses: elbo, g_loss,d_loss. As you can see in the code above, I manage to train them 3 using 2 different Adam Optimizer, thanks to @ispirmustafa 's comment.\r\nIn my case, I am working with a GAN architecture and I want to update the encoder-decoder part and Discriminator part using the [two time-scale update rule](https://arxiv.org/abs/1706.08500) (only Generator trains in the first two step, only Discriminator  trains in the 3rd step and so on)  that possible to do that using tf estimator ? \r\nAlso the another question is how can i access to the training step during training. For example, I want to train my encoder-decoder network first with only elbo loss function in the first 200k iter then train the discriminator after that. \r\nPlease help me since I am new to this tf Estimator.", "hi,\r\nfor both of your quenstion, you can use tf.cond. it will be something like:\r\n```\r\ntrain_op: tf.cond(step mod 3, train_op_discriminator, train_op_generator)\r\n```", "@ispirmustafa Hello, thanks! But how did you define \"step\" variable in your answer. This is something I tried based on your suggestion but it didnt work.Also which tensorflow function will you use for the \"mod 3\" part. I am sorry I am a beginner to tensorflow.\r\n```\r\nstep = tf.convert_to_tensor(tf.train.get_global_step())\r\nnumber  = tf.cast(tf.convert_to_tensor(tf.constant(500)),tf.int64)\r\ntrain_op = tf.cond(tf.greater(step,number),train_op_gen, train_op_dis)\r\n```\r\nThis is the error:\r\n\r\n> INFO:tensorflow:Calling model_fn.\r\n> Traceback (most recent call last):\r\n>   File \"/home/storm/data/Work/code/tf-gqn-master/train_gqn_draw.py\", line 185, in <module>\r\n>     tf.app.run(argv=[sys.argv[0]] + UNPARSED_ARGV)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n>     _sys.exit(main(argv))\r\n>   File \"/home/storm/data/Work/code/tf-gqn-master/train_gqn_draw.py\", line 159, in main\r\n>     hooks=[logging_hook],\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 843, in _train_model\r\n>     return self._train_model_default(input_fn, hooks, saving_listeners)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 856, in _train_model_default\r\n>     features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\", line 831, in _call_model_fn\r\n>     model_fn_results = self._model_fn(features=features, **kwargs)\r\n>   File \"/home/storm/data/Work/code/tf-gqn-master/gqn/gqn_model.py\", line 218, in gqn_draw_model_fn\r\n>     train_op_gen, train_op_dis)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2039, in cond\r\n>     raise TypeError(\"true_fn must be callable.\")\r\n> TypeError: true_fn must be callable.\r\n", "Following is a working example:\r\n```\r\nwith tf.Graph().as_default():\r\n  step = tf.train.get_or_create_global_step()\r\n  \r\n  train_op_d = tf.constant('train_op_d')\r\n  train_op_g = tf.constant('train_op_g')\r\n  train_op = tf.cond(tf.equal(0, tf.to_int32(tf.mod(step, 3))), \r\n                     true_fn=lambda: train_op_d,\r\n                     false_fn= lambda: train_op_g)\r\n  increment_step = step.assign_add(1)\r\n  \r\n  \r\n  with tf.train.MonitoredTrainingSession() as sess:\r\n    for _ in range(6):\r\n      print sess.run(train_op)\r\n      sess.run(increment_step)\r\n```", "@ispirmustafa Hello! Is it possible to pass the \"step\" in the above example into model_fn as a placeholder? Thanks! I'm working on a problem that requires using an external switch to control the loss function, something like:\r\n\r\nloss = a * loss_1 + (1 - a) * loss_2,\r\n\r\nwhere \"a\" is defined outside of the estimator and can change during the training procedure."]}, {"number": 15772, "title": "remove trailing semicolon at the end of line", "body": "removed trailing semicolon(;) in the statement\r\n\r\naccording to [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html?showone=Semicolons#Semicolons)\r\n _\"Do not terminate your lines with semi-colons and do not use semi-colons to put two commands on the same line.\"_ ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 15770, "title": "fix todo: make a class which constrcuts resource and provide get_next.", "body": "", "comments": ["Can one of the admins verify this patch?", "Not sure about the just the `get_next` one time. Maybe it would be better to have an generator there.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15769, "title": "Out of range: End of sequence", "body": "\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**:   python3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:   Cuda 8.0  cudnn6_6.0.\r\n- **GPU model and memory**:  Titax 12G\r\n- **Exact command to reproduce**:\r\n\r\npython -m nmt.nmt \\\r\n    --src=vi --tgt=en \\\r\n    --vocab_prefix=/tmp/nmt_data/vocab  \\\r\n    --train_prefix=/tmp/nmt_data/train \\\r\n    --dev_prefix=/tmp/nmt_data/tst2012  \\\r\n    --test_prefix=/tmp/nmt_data/tst2013 \\\r\n    --out_dir=/tmp/nmt_model \\\r\n    --num_train_steps=12000 \\\r\n    --steps_per_stats=100 \\\r\n    --num_layers=2 \\\r\n    --num_units=128 \\\r\n    --dropout=0.2 \\\r\n    --metrics=bleu\r\n\r\n2018-01-01 21:14:49.071267: W tensorflow/core/kernels/lookup_util.cc:362] Table trying to initialize from file /tmp/nmt_data/vocab.vi is already initialized.\r\n2018-01-01 21:14:49.071378: W tensorflow/core/kernels/lookup_util.cc:362] Table trying to initialize from file /tmp/nmt_data/vocab.en is already initialized.\r\n2018-01-01 21:14:49.071554: W tensorflow/core/kernels/lookup_util.cc:362] Table trying to initialize from file /tmp/nmt_data/vocab.en is already initialized.\r\n  loaded infer model parameters from /tmp/nmt_model/translate.ckpt-9000, time 0.08s\r\n\r\n  decoding to output /tmp/nmt_model/output_dev.\r\n2018-01-01 21:14:58.585992: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n2018-01-01 21:14:58.586255: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n  done, num sentences 1553, num translations per input 1, time 9s, Mon Jan  1 21:14:58 2018.\r\n  bleu dev: 5.2\r\n  saving hparams to /tmp/nmt_model/hparams\r\n\r\n  decoding to output /tmp/nmt_model/output_test.\r\n2018-01-01 21:15:08.289098: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n2018-01-01 21:15:08.289145: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n  done, num sentences 1268, num translations per input 1, time 9s, Mon Jan  1 21:15:08 2018.\r\n  bleu test: 4.4\r\n\r\n", "comments": ["Hi @gameking157 ! Sorry for the late response.\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. You can migrate to 2.x version using this [document](https://www.tensorflow.org/guide/migrate/migrate_tf2) ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 15768, "title": "Training broke with ResourceExausted error", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: TITAN X, 12207MiB\r\n\r\n----------------------\r\n\r\nMost Probably everyone will be asking about this is a question for StackOverflow. Here is the link to the StackOverflow question,  But please take a look at the problem. There could be some bug in tensorflow as the error occurs after 32 epoch.\r\nhttps://stackoverflow.com/questions/48007984/training-broke-with-resourceexausted-error\r\n\r\nHere is the code of the model, https://paste.ubuntu.com/26298336/\r\nA short description of the model would be,\r\n\r\n- Character level Embedding Vector -> Embedding lookup -> LSTM1\r\n- Word level Embedding Vector->Embedding lookup -> LSTM2\r\n- [LSTM1+LSTM2] -> single layer MLP-> softmax layer/CRF layer\r\n- [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\r\n\r\nWhile running the code it produces the following error output at the epoch 32,\r\n\r\n`ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[24760,100] [[Node: chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split = Split[T=DT_FLOAT, num_split=4, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y, chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] [[Node: bi-lstm/bidirectional_rnn/bw/bw/stack/_167 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]`\r\n\r\nMy question is if there is any error then it should occur in the first epoch why at 32 epoch?\r\n\r\nI am using embedding_lookup in following way,\r\n\r\n```\r\n_word_embeddings = tf.Variable(\r\n                        embeddings,\r\n                        name=\"_word_embeddings\",\r\n                        dtype=tf.float32,\r\n                        trainable=False)\r\n            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=\"word_embeddings\")\r\n\r\n```\r\n\r\nWhere `embeddings` is a `(61698, 100)` sized vector. which is only 24 MB. However in the error message, it showed the error with, `(24760, 100)` sized vector which is only 10MB. It also produces warning while declaring optimizers for the model. it was suggested as below,\r\n\r\n> gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n\r\n", "comments": ["As you suspected, I'm gonna close this as it appears you're getting some traction on StackOverflow :) Please open a new issue if further investigation reveals a more concrete TensorFlow bug."]}, {"number": 15767, "title": "Decode_raw_op_test failure on s390x", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Ubuntu 16.04 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: bazel test -c opt //tensorflow/python/kernel_tests:decode_raw_op_test\r\n\r\n### Describe the problem\r\nObserving a failure in [testToFloat16](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/kernel_tests/decode_raw_op_test.py#L77) sub test, while running `decode_raw_op_test`. It seems that the [byte-reversal logic](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/core/kernels/decode_raw_op.cc#L80) for big endian is not needed for **float16 input**(and float16 output). While the other sub tests like int16/uint16 which have byte array input need byte-swapping for consistent results. What would be the best way to correct this?\r\n@rmlarsen, @jiefangxuanyan , Could you please share your thoughts on this?\r\n \r\n### Source code / logs\r\n```\r\nFAIL: testToFloat16 (__main__.DecodeRawOpTest)\r\n----------------------------------------------------------------------\r\n.\r\nAssertionError:\r\nArrays are not equal\r\n(mismatch 100.0%)\r\n x: [repr failed for <matrix>: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()]\r\n y: array([[  3.576279e-06,   1.144409e-05,   1.156330e-05,   4.053116e-06]], dtype=float16)\r\n----------------------------------------------------------------------\r\nRan 7 tests in 0.089s\r\n\r\nFAILED (failures=1)\r\nnot equal where =  (array([0, 0, 0, 0]), array([0, 1, 2, 3]))\r\nnot equal lhs =  [[ 1. -2. -3.  4.]]\r\nnot equal rhs =  [  3.57627869e-06   1.14440918e-05   1.15633011e-05   4.05311584e-06]\r\n```\r\n", "comments": ["I think this problem is caused by `np.ndarray.tostring` [here](https://github.com/tensorflow/tensorflow/blob/438604fc885208ee05f9eef2d0f2c630e1360a83/tensorflow/python/kernel_tests/decode_raw_op_test.py#L84). This method [\"constructs Python bytes showing a copy of the raw contents of data memory\"](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tostring.html). So we get bytes in the native byte order, and the test will always fail on big-endian machines as long as `decode_raw` used little-endian as default. Maybe we should re-write the tests for float numbers.", "@jiefangxuanyan, thank you for your inputs.\r\n\r\nAs you pointed out, output of `tostring` on x86 and s390x as seen is: \r\n```\r\nx86:\r\n expected_result.tostring() --  ['0x0', '0x3c', '0x0', '0xc0', '0x0', '0xc2', '0x0', '0x44']\r\ns390x:\r\n expected_result.tostring() --  ['0x3c', '0x0', '0xc0', '0x0', '0xc2', '0x0', '0x44', '0x0']\r\n```\r\nI tried below change in that test case:\r\n```\r\nif sys.byteorder == \"big\":\r\n  result = decode.eval(feed_dict={in_bytes: [expected_result.byteswap().tostring()]})\r\nelse:\r\n   result = decode.eval(feed_dict={in_bytes: [expected_result.tostring()]})\r\n```\r\nWith this change, test passes on both platforms.\r\n\r\nBut I wonder if the functionality of decode_raw is correct for float? What will happen in actual scenario during decode_raw operation for float? Will `decode_raw_op.cc` need changes when input is not a byte array?\r\n\r\n", "A simple solution could be just specify endianness in numpy dtype:\r\n```\r\nexpected_result = np.matrix([[1, -2, -3, 4]], dtype=\"<f2\")\r\n```"]}, {"number": 15766, "title": "tf.assert_equal raises incorrect traceback in Eager mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1 LTS\r\n- **TensorFlow installed from (source or binary)**: pip binary\r\n- **TensorFlow version (use command below)**: 1.5.0-dev20171227\r\n- **Python version**: 3.5.0\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: python main.py\r\n\r\n\r\n### Describe the problem\r\n\r\nIn eager mode, tf.assert_equal only shows `[]` in traceback message when two inputs are different. However, in graph mode, it does show different values in the message. \r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nx = tf.constant([1,2,3])\r\ny = tf.constant([3,2,1])\r\n\r\nwith tf.control_dependencies([tf.assert_equal(x, y)]):\r\n    output = tf.reduce_sum(x)\r\n\r\n```\r\n\r\n\r\nEager Mode Traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/matt/PycharmProjects/scratch/main.py\", line 9, in <module>\r\n    with tf.control_dependencies([tf.assert_equal(x, y)]):\r\n  File \"/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 376, in assert_equal\r\n    summary_msg)))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: \r\nCondition x == y did not hold.\r\nIndices of first 0 different values:\r\n[]\r\nCorresponding x values:\r\n[]\r\nCorresponding y values:\r\n[]\r\n```\r\n\r\n\r\nGraph Mode Traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/matt/PycharmProjects/scratch/main.py\", line 9, in <module>\r\n    with tf.control_dependencies([tf.assert_equal(x, y)]):\r\n  File \"/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 391, in assert_equal\r\n    _assert_static(condition_static, data)\r\n  File \"/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 104, in _assert_static\r\n    message='\\n'.join(data_static))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: \r\nCondition x == y did not hold element-wise:\r\nx (Const:0) = \r\n[1 2 3]\r\ny (Const_1:0) = \r\n[3 2 1]\r\n```\r\n", "comments": ["Thanks for the report. @iganichev - could you take a look?", "The fix is in review. In the meantime, you can use `summarize=3` parameter to `assert_equals`.", "Thank you very much!"]}, {"number": 15765, "title": "Trivial python syntax fixes", "body": "", "comments": ["Can one of the admins verify this patch?", "Nevermind, python2 doesn't support `yield from`."]}, {"number": 15764, "title": "Revert \"C++ gradient for Select (#14862)\"", "body": "This reverts commit dc355dc491836f1202a2c3fcef0a9da6902fd7da.", "comments": ["CC @facaiy "]}, {"number": 15763, "title": "Branch 180441903", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 15762, "title": "Make unused variable warning an error during TF builds.", "body": "We will need to eyeball build logs and see if this is really working as intended", "comments": ["Thank you! I did some fixes during merge, so maybe I need to push first.", "@gunan @drpngx It looks like that the change has caused many broken builds.", "Yes, now I have pushed from internal, let's see if there are more errors in OSS.\r\n\r\nJenkins, test this please.", "ping @gunan, any progress or updates on this?\r\n\r\n@tensorflow-jenkins test this please", "The tests have a problem I am not sure how to tackle.\r\n```\r\nFAILED: Build did NOT complete successfully\r\nbazel-out/k8-py3-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc:4634:9: error: unused variable 'i' [-Werror,-Wunused-variable]\r\n    int i = 0;\r\n        ^\r\n```\r\n\r\n`pywrap_tensorflow_internal.cc` is a file generated by swig. I do not know why it adds an unused variable, but unfortunately it does. @drpngx WDYT?", "Can we just set a flag for that file like `-Wno-unused-variable`?", "I remember something like this, but we may need to set these in bzl files, or in crosstools?\r\nI think we already had a similar situation and @mhlopko @lberki already provided us a solution.\r\nIs there a way to globally set a compiler flag, but disable it for certain files? Note that we are still using the built-in c++ toolchain for our CPU C++ builds.", "It possible that the flags set at a file level are put at the end and override the global behavior?", "@gunan do you still want to merge this?", "This will take some time to resolve, so let's close it for now. I can locally test and verify and reopen the PR>"]}, {"number": 15761, "title": "Revert \"add c++ gradient for op: Pow (#15245)\"", "body": "This reverts commit e1ded7fa7cfacaeea43a903e738dd3fe2baabc57.\r\n\r\nCC @facaiy ", "comments": ["Oh, the error persists. The test is\r\n\r\n```\r\n[ RUN      ] NaryGradTest.Select\r\ntensorflow/cc/gradients/math_grad_test.cc:699: Failure\r\nExpected: (max_error) < (1e-3), actual: 0.314623 vs 0.001\r\n[  FAILED  ] NaryGradTest.Select (9 ms)\r\n```\r\nwhich is not in this PR.", "a-ha, so maybe we need to revert https://github.com/tensorflow/tensorflow/pull/14862 ?", "@gunan @drpngx Thank you for letting me know the problem. I have rechecked #14862 and found its unit test might be unstable."]}, {"number": 15760, "title": "Custom gradient aggregation methods", "body": "I would like a way to apply some custom gradient aggregation ops. Probably the simplest thing to do is just allow `tf.gradients` (and `Optimizer.compute_gradients`) to return un-aggregated gradients so I could work with those?\r\nAnyway, seems like an easy fix? I will do this myself in a month or so (cant now as on holiday), but if someone else wants to do it/has some thoughts, I am interested.\r\n", "comments": ["@act65 it's not so straightforward because there's no explicit aggregation step. The \"aggregation\" happens as part of backprop. You need to create custom backprop to get per-example gradients. See thread https://github.com/tensorflow/tensorflow/issues/4897 for discussion and an example of using khatri-rao product to do this efficiently", "Ah yea, I see the problem. In that case, I think I am after a nicer interface for adding new aggregation methods. Maybe the `aggregation_method` could be passed into 'tf.gradients` as a function/class with the right signature instead?\r\n\r\nRegardless, I think I can do what I want (a sketch -- possibly online -- of the grads that returns the principle component) if I just hack in another option to `_AggregatedGrads`.", "The classic graph way would be to provide another registration mechanism. IE, using Theano [notation](http://deeplearning.net/software/theano/library/gradient.html) in addition to \"right-operator\" and \"left-operator\", each op would support \"per-example left-operator\", which interprets input nxb matrix as a vectors of  backprops from b independent per-example losses, stacked together into a matrix.\r\n\r\nHowever, things seem to be moving in direction of \"eager\" mode which has much nicer autograd-style wrappers. Maybe @mattjj has some trick up his sleeve on how to accomplish this -- ie, how do you batch things up to compute k gradients for k examples efficiently?", "I am not sure it is necessary to add another registration mechanism to allow me to write some custom aggregation ops? It also seems like a lot of work...?\r\nCan we not just think of aggregation as some fn that takes gradients (batch x vectorised parameters) and returns the aggregated grads (1 x vectorised parameters)? So a sum is one instance of this? And we can just drop in some other fn in??\r\n\r\nReplace code from [source](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/gradients_impl.py).\r\n```python\r\nfor i, out_grad in enumerate(out_grads):\r\n    out_grads[i] = math_ops.accumulate_n(out_grad)\r\n```\r\nwith \r\n```python\r\nfor i, out_grad in enumerate(out_grads):\r\n    mat = tf.stack(out_grad)\r\n    u, s ,v = tf.svd(mat)\r\n    out_grads[i] = u[:, 0]*s[0]  # the principle component (rather than the sum).\r\n```\r\n\r\nBut, I feel like I might be missing something? I still dont understand your comment\r\n\r\n>  because there's no explicit aggregation step. The \"aggregation\" happens as part of backprop\r\n\r\nI though aggregation over the batch dimension only occurs at the end of the backward propagation of the gradients?! ", "Aggregation over batch dimension happens at each backprop step. The `_AggregatedGrads` method inside `grad_impl` refer to aggregation over multiple output tensors rather than over examples.\r\n\r\nI have a detailed example of backprop derived in [Backprop and systolic arrays](https://medium.com/@yaroslavvb/backprop-and-systolic-arrays-24e925d2050) -- even though the loss in that example is the sum of squared losses, gradients with respect to individual squared losses are never formed, you only have gradient from summed-over loss at any point in time.\r\n\r\nIf you care about computing some property of per-example gradients rather than full batch of per-example gradients, there are tricks you can do such as @goodfeli 's https://arxiv.org/pdf/1510.01799.pdf\r\n  ", "Ok, I have some reading to do. I will come back to this. Thanks @yaroslavvb ", "@yaroslavvb Hi Yaroslav, I was hoping to get some help implementing the method discussed in https://arxiv.org/pdf/1510.01799.pdf.\r\n\r\n@goodfeli suggested the [pseudocode](https://github.com/tensorflow/tensorflow/issues/4897) below:\r\n```\r\nexamples = tf.split(batch)\r\nweight_copies = [tf.identity(weights) for x in examples]\r\noutput = tf.stack(f(x, w) in zip(examples, weight_copies))\r\ncost = cost_function(output)\r\nper_example_gradients = tf.gradients(cost, weight_copies)\r\n```\r\n\r\nBut I'm not sure how to implement this still. Would it be possible to show a toy example of this?"]}, {"number": 15759, "title": "Update license year", "body": "TO DO:\r\n\r\n- [x] Wait to the next year (100% done, depends on timezone)\r\n- [x] Merge!\r\n\r\n:octocat:", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Signed!", "CLAs look good, thanks!\n\n<!-- ok -->", "Let's wait until tomorrow.", "Jenkins, test this please.", "I guess it is technically Jan 1 on GMT. Happy New Year!"]}, {"number": 15758, "title": "Getting rid of all_opensource_files", "body": "This ticket is for tracking the progress of removing the `all_opensource_files` bazel target.\r\nSee the discussion in #15368.\r\n\r\n- [x] Turn `check_futures_test` into a sanity check #15671\r\n- [x] De-Bazel `check_load_py_test` sanity check #15677\r\n- [x] De-Bazel `pip_smoke_test` sanity check #15678\r\n- [x] Rewrite `//tensorflow/contrib/makefile:build_all_linux`\r\n- [x] Remove `all_opensource_files` bb582f1b6fad474bc446c78a6683247a8eb6048e", "comments": ["@yifeif Which parts of the `makefile` build depend on `all_opensource_files` ?", "@gunan What do we need the wrapper around `makefile` builds for?", "New build infra requires bazel to execute builds. We need to make changes in the infra config to get rid of all opensource files.\r\n\r\n@yifeif will need to take a look at that.", "Is the `//tensorflow/contrib/makefile:build_all_linux` target even used anywhere?\r\n\r\n`grep -r \"build_all_linux\"` yields just this:\r\n```\r\n./tensorflow/contrib/makefile/README.md:You should then be able to run the `build_all_linux.sh` script to compile:\r\n./tensorflow/contrib/makefile/README.md:tensorflow/contrib/makefile/build_all_linux.sh\r\n./tensorflow/contrib/makefile/BUILD:    name = \"build_all_linux\",\r\n./tensorflow/contrib/makefile/BUILD:    srcs = [\"build_all_linux.sh\"],\r\n./tensorflow/contrib/makefile/build_with_docker.sh:COMMAND=\"tensorflow/contrib/makefile/build_all_linux.sh\"\r\n```\r\n\r\nOr is the target used somewhere outside this repo?", "Yes, the infra config we have is outside the repository.", "FYI we have removed all our dependencies on `all_opensource_files`.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @Androbin: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15757, "title": "Fixes #15736", "body": "This will allow users to import keras submodules without typing `from tensorflow.python.keras...` but just directly from `tensorflow.keras`.\r\nThis change also does not create a duplicate of the module object but just assign it two names, one with `tensorflow.python.keras` keeping current functionality and `tensorflow.keras` allowing a much more consistent API use.\r\n\r\nWith this change the user is able to import keras objects directly, for example\r\n```python\r\nfrom tensorflow.keras.layers import Dense\r\n```", "comments": ["Can one of the admins verify this patch?", "Sorry, this is not a scenario we want to support. You are supposed to access all modules from `tf.`", "This wouldn't be so annoying if importing from tf as advised in the docs actually worked. "]}, {"number": 15756, "title": "Fixes #15736", "body": "This will allow users to import keras submodules without typing `from tensorflow.python.keras...` but just directly from `tensorflow.keras`. \r\nThis change also does not create a duplicate of the module object but just assign it two names, one with `tensorflow.python.keras` keeping current functionality and `tensorflow.keras` allowing a much more consistent API use. \r\n\r\nWith this change the user is able to import keras objects directly, for example\r\n```python\r\nfrom tensorflow.keras.layers import Dense\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 15755, "title": "Tensorflow Dataset.from_generator blocks input?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: both win7 and CentOS 7.2.1511\r\n- **TensorFlow installed from (source or binary)**: pip3\r\n- **TensorFlow version (use command below)**: 'v1.4.0-rc1-11-g130a514 1.4.0' and '1.5.0-dev20180102'\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\n\r\nI submitted a problem on stackoverflow but nobody solved it. So I open it here. Does anybody can help to solve it?\r\n\r\nHere is the problem:\r\n[https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input](https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input)\r\n\r\nI installed tensorflow 1.4.0 via pip, without gpu support. My python version is 3.5.3\r\n  ", "comments": ["I think this is working as intended: the `generator` passed to `tf.data.Dataset.from_generator()` is an opaque piece of Python as far as TensorFlow is concerned, and the iterator will block until a value is produced. The code in your [Stack Overflow question](https://stackoverflow.com/q/47917288/3574081) doesn't block when I run it (nor did it when my colleague @jsimsa ran it).", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@mrry I don't think it is a problem to Python generator, because if I change the `def task()` method as below to using `tf.placeholder`, then everything goes well:\r\n\r\n    def task():\r\n        with tf.Graph().as_default():\r\n            sample=tf.placeholder(tf.int32, [1, 8])\r\n            iter=data_iter()\r\n            with tf.Session() as sess:\r\n                while True:\r\n                    try:\r\n                        result=sess.run(sample, feed_dict={sample:iter.__next__()})\r\n                        print(result)\r\n                    except:\r\n                        break\r\n\r\nI think it is implausible that the python `queue.get()` or the `generator.__next__()` method could block when the queue is not empty within that simple context. As the `tf.placeholder` version goes well, I think there must be something happened in the `Dataset.from_generator` method which blocks the `generator`.\r\n\r\nYou mentioned that my code didn't block in both your and your colleague's machine. Could you please tell what your output looked like? Like the first output or the second one provide in my [Stack Overflow question](https://stackoverflow.com/questions/47917288/tensorflow-dataset-from-generator-blocks-input)? If it looked like the second one, there must be some block. Because I make the main thread wait long enough for the tensorflow thread to process the data. My expectation is the output will come immediately after the main thread put it into the queue, as shown in the first output. In other word, the input log and the output log must shown one by one no matter how many threads is running.\r\n\r\nWould your please help to check again? Thanks a lot.", "It prints out all hundred outputs, then raises an exception because your code enqueues `None` into the queue:\r\n\r\n> 2018-01-04 07:44:40.490573: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n\r\nPerhaps the problem is in your shutdown logic, because it's possible that one worker could dequeue more than one `None`, leaving another worker blocked? I would have written `data_iter()` as:\r\n\r\n```python\r\ndef data_iter():\r\n    next_val = request_queue.get()\r\n    while next_val is not None:\r\n        yield next_val\r\n        next_val = request_queue.get()\r\n```\r\n \r\n...to avoid that possibility.", "The point is not the exception at the end of the procedure, but the order of the log printed. Also I know nothing missed. But as I mentioned before, the main thread will sleep long enough time after each `put()` to the queue. So I expect the tensorflow thread would print out the result immediately after the \"put log\" if there is no block happen. Thus the \"put log\" and the \"tensorflow log\" should be printed one after another like below regardless how many threads running:\r\n\r\n    [[0 0 0 0 0 0 0 0]]\r\n    round 0, request_queue size is about 0, sleeping 8 secs...\r\n    [[1 1 1 1 1 1 1 1]]\r\n    round 1, request_queue size is about 0, sleeping 7 secs...\r\n    [[2 2 2 2 2 2 2 2]]\r\n    round 2, request_queue size is about 0, sleeping 7 secs...\r\n    [[3 3 3 3 3 3 3 3]]\r\n    round 3, request_queue size is about 0, sleeping 9 secs...\r\n    ...\r\n\r\nBut if the log printed as more than one \"put log\" after a \"tensorflow log\" or more than one \"tensorflow log\" after a \"put log\" (like below), it must indicate that there were some blocks happened so that some thread failed to process the data in time. \r\n\r\n    round 0, request_queue size is about 1, sleeping 9 secs...\r\n    [[0 0 0 0 0 0 0 0]]\r\n    [[1 1 1 1 1 1 1 1]]\r\n    round 1, request_queue size is about 0, sleeping 5 secs...\r\n    round 2, request_queue size is about 0, sleeping 8 secs... # \"[[2 2 2 2 2 2 2 2]]\" did not be printed after this line indicates tensorflow thread failed to give the output in even 8 seconds after the queue received the data\r\n    round 3, request_queue size is about 0, sleeping 10 secs...\r\n    [[4 4 4 4 4 4 4 4]] # these 3 lines printed after 10 seconds after the time the line \"round 3\" printed, and right before the time the line \"round 4\" printed, which indicates that the tensorflow thread failed to process both the \"round 2\" and \"round 3\" in time.\r\n    [[2 2 2 2 2 2 2 2]]\r\n    [[3 3 3 3 3 3 3 3]]\r\n    round 4, request_queue size is about 0, sleeping 8 secs...\r\n    round 5, request_queue size is about 0, sleeping 6 secs...\r\n    round 6, request_queue size is about 0, sleeping 10 secs...\r\n    [[6 6 6 6 6 6 6 6]]\r\n    [[5 5 5 5 5 5 5 5]]\r\n    ...\r\n\r\nI wrote several comments in the log above, providing the proof that tensorflow thread failed to process data in time. Moreover, I believe the python generator got the data right after it was put in the queue, since the size of `request_queue` was always zero.", "Ah, so the problem is that your program produces elements in a non-deterministic order, and not that it deadlocks?\r\n\r\nWell, there are several potential races in that code:\r\n\r\n1. The worker threads race to acquire an element from the queue.\r\n2. The worker threads race to acquire the interpreter when they transition out of the TensorFlow runtime and into Python.\r\n3. There's a race between printing the `qsize` and threads accessing it, so it's quite possible that there are multiple elements in the queue at some points, even if your logging indicates otherwise.\r\n4. The worker threads race to print their result to the screen. It's possible that the events are being processed in the right order, but printed in the wrong order!\r\n\r\nAdding `time.sleep()` calls between enqueuing events is insufficient to serialize the execution. It's possible for your thread to be woken after less time than you requested, and it's also possible for the thread contention on the GIL to be large enough that multiple elements end up in the queue at once.\r\n\r\nIf you still think there is a bug in the `Dataset.from_generator()`, the first step must be coming up with a test program that is demonstrably correct... there are too many potential sources of non-determinism in that example program to conclude that the library is the source of the error.", "Yeah, I agree that there are two many races in that code. However, do you really think that it is normal that the entire work flow failed to process a single simple request in at least 5 seconds (the minimum sleep time after each `queue.put()`)? I don't think this is reasonable. Thus the log must be printed in a deterministic order as long as the entire work flow can finish to process a single request in no more than 5 seconds.\r\n\r\nSo actually it is not important that which thread win the race, but the program failed to process the data in even 5 seconds. If you check the log carefully when the program is running, you can find that some results which should have been print out immediately delayed until several `put()` actions executed, which spanned to tens of seconds or more.\r\n\r\nAnother proof is that when I changed the `task()` method to using `tf.placeholder`, as I mentioned before, the log printed very well just as what I expected.\r\n\r\nI found this problem in building a request server. The clients frequently received the result after incredibly long time. During the debug, I cannot find any other place which might block the work flow. I am sorry I can't work out a better way to reproduce the problem."]}, {"number": 15754, "title": "Protobuf Compilation /object_detection/protos/anchor_generator_pb2.py: Permission denied issue", "body": "I need to implement object detection using Tensorflow but.\r\nIm getting ,\r\n**Protobuf Compilation /object_detection/protos/anchor_generator_pb2.py: Permission denied issue** when executing \r\n`protoc object_detection/protos/*.proto --python_out=.`  in Protobuf Compilation\r\n\r\nthis is the issue i already having.\r\n\r\n![error](https://user-images.githubusercontent.com/34977438/34461460-ae4e5120-ee50-11e7-8d43-1ba5ecab2bc3.jpg)\r\n\r\nHere is my link for Tensorflow model [https://github.com/tensorflow/models](url)\r\nand i used **Protocol Buffers v3.4.0**\r\n So, can anyone knows how to solve and execute that command?\ufeff", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since this seems like an issue with your environment. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15753, "title": " ImportError: cannot import name 'checkpoint_ops'", "body": "keras version =2.0.8\r\ntensorflow version =1.2.1\r\nanconda and windows 10\r\ninstall a binary \r\npython version is 3.5 64bit\r\n\r\n> `from tensorflow.contrib.framework.python.ops.checkpoint_ops import *\r\n  File \"C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\checkpoint_ops.py\", line 22, in <module>\r\n    from tensorflow.python.training import checkpoint_ops\r\nImportError: cannot import name 'checkpoint_ops'`\r\n  \r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15752, "title": "Eager: Incompatible rnn model shapes inferred when using more than one CudnnGRU/LSTM", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.5.0dev20171230\r\n- **Python version**: 3.6\r\n\r\n### Describe the problem\r\nWhen I use more than one CudnnGRU in eager, I got an error:\r\n```Python\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-f618cd483215> in <module>()\r\n     26 with tf.device(device):\r\n     27     images = tf.constant(toy_data)\r\n---> 28     logits = net(images)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    651 \r\n    652         if not in_deferred_mode:\r\n--> 653           outputs = self.call(inputs, *args, **kwargs)\r\n    654           if outputs is None:\r\n    655             raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n<ipython-input-1-f618cd483215> in call(self, x)\r\n     17         x = tf.transpose(x, [1, 0, 2])\r\n     18         x, s = self.gru1(x)\r\n---> 19         x, s = self.gru2(x)\r\n     20         x = tf.transpose(x, [1, 0, 2])\r\n     21         x = self.fc(x)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    651 \r\n    652         if not in_deferred_mode:\r\n--> 653           outputs = self.call(inputs, *args, **kwargs)\r\n    654           if outputs is None:\r\n    655             raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py in call(self, inputs, initial_state, training)\r\n    400       c = array_ops.constant([], dtype=dtype)\r\n    401     outputs, (output_h, output_c) = self._forward(inputs, h, c, self.kernel,\r\n--> 402                                                   training)\r\n    403     if self._rnn_mode == CUDNN_LSTM:\r\n    404       return outputs, (output_h, output_c)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py in _forward(self, inputs, h, c, opaque_params, training)\r\n    475         direction=self._direction,\r\n    476         dropout=self._dropout,\r\n--> 477         seed=self._seed)\r\n    478     return output, (output_h, output_c)\r\n    479 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py in _cudnn_rnn(inputs, input_h, input_c, params, is_training, rnn_mode, input_mode, direction, dropout, seed, name)\r\n    858       seed=seed,\r\n    859       seed2=seed2,\r\n--> 860       name=name)\r\n    861   return (outputs, output_h, output_c)\r\n    862 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\r\n    120               \"seed2\", seed2, \"is_training\", is_training)\r\n    121     _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\r\n--> 122                                attrs=_attrs, ctx=_ctx, name=name)\r\n    123   _execute.record_gradient(\r\n    124       \"CudnnRNN\", _inputs_flat, _attrs, _result, name)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   # pylint: enable=protected-access\r\n     68   return tensors\r\n\r\n~\\Anaconda3\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Incompatible rnn model shapes inferred: expecting [num_layers, input_size, num_units, dir_count]: [1, 28, 100, 1], getting [num_layers, input_size, num_units, dir_count]: [1, 100, 100, 1]. [Op:CudnnRNN]\r\n```\r\nIf I use same network in graph mode, there is no problem.\r\nreproduce error code:\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ntfe.enable_eager_execution(config=config)\r\nclass CudnnCrashNet(tfe.Network):\r\n    def __init__(self, name):\r\n        super(CudnnCrashNet, self).__init__(name)\r\n        self.gru1 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)\r\n        self.track_layer(self.gru1)\r\n        self.gru2 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)\r\n        self.track_layer(self.gru2)\r\n        self.fc = self.track_layer(tf.layers.Dense(10))\r\n    def call(self, x):\r\n        x = tf.reshape(x, [-1, 28, 28])\r\n        x = tf.transpose(x, [1, 0, 2])\r\n        x, s = self.gru1(x)\r\n        x, s = self.gru2(x)\r\n        x = tf.transpose(x, [1, 0, 2])\r\n        x = self.fc(x)\r\n        return x\r\ntoy_data = np.ones((100, 784)).astype(np.float32)\r\ndevice = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\r\nnet = CudnnCrashNet('net')\r\nwith tf.device(device):\r\n    images = tf.constant(toy_data)\r\n    logits = net(images)\r\n```\r\nnormal graph-mode code:\r\n```Python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nclass CudnnNormalNet(tf.layers.Layer):\r\n    def __init__(self, name):\r\n        super(CudnnNormalNet, self).__init__(name)\r\n        self.gru1 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)\r\n        self.gru2 = tf.contrib.cudnn_rnn.CudnnGRU(1, 100)\r\n        self.fc = tf.layers.Dense(10)\r\n    def call(self, x):\r\n        x = tf.reshape(x, [-1, 28, 28])\r\n        x = tf.transpose(x, [1, 0, 2])\r\n        x, s = self.gru1(x)\r\n        x, s = self.gru2(x)\r\n        x = tf.transpose(x, [1, 0, 2])\r\n        x = self.fc(x)\r\n        return x\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ntoy_data = np.ones((100, 784)).astype(np.float32)\r\nwith tf.Graph().as_default():\r\n    net = CudnnNormalNet('net')\r\n    x_p = tf.placeholder(tf.float32, [None, 784])\r\n    logits = net(x_p)\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(logits, feed_dict={x_p: toy_data})\r\n```", "comments": ["@alextp can you take a look or redirect? Thanks.", "@allenlavoie looks like variables are getting reused when we don't want them to. Want to take a look?", "@alextp, looks like this is due to kernel caching rather than variable sharing. This also reproduces the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\nwith tf.device(\"gpu:0\"):\r\n  tf.contrib.cudnn_rnn.CudnnGRU(1, 100)(tf.zeros([28, 100, 28]))\r\n  tf.contrib.cudnn_rnn.CudnnGRU(1, 100)(tf.zeros([28, 100, 100]))\r\n```\r\n\r\nThe kernel [holds on to its input shape](https://github.com/tensorflow/tensorflow/blob/020001e1e38d99e1b7b49c26fc808201da22bc06/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L902) for error checking. I've verified that it works when executing eagerly with kernel caching disabled.\r\n\r\nSo I think the right fix is to make the kernel non-stateful (maybe use an attribute). Shouldn't be a terrible fix; I'll give it a shot.", "Hmm, it's not just shape checking. The kernel also uses some persistent temporary storage, so it needs one kernel per Layer. The right solution may be to have it create a resource for that storage, then pass around a handle to it? A hacky solution would be to take a unique ID as an attribute to the kernel. We can chat tomorrow.", "@traveller59 thank you for the report! Should be fixed now.\r\n\r\nWe ended up adding a shape cache for the scratch space in the kernel itself (so the same kernel will be re-used every call, but it will allocate space for whichever shapes it sees).", "@allenlavoie Can you provide me a sample python tensorflow code for this fix for cudnn_gru? Thanks!", "@burglarhobbit if you mean Python code that previously exercised the bug, check the unit tests: https://github.com/tensorflow/tensorflow/commit/93f2998add91f9c7a53b1fcd13ab6d43e4397297#diff-fa99f9b309ff8d5c073c0ac7714f27db\r\n\r\nIf you just want to use the fix, you'll need to use one of the 1.6 release candidates or a nightly build.", "I just want to use the fix. So I should just use tensorflow 1.6? It demands cuda-9.0 but the server I'm using has cuda-8.0 and most probably unlikely to change. Just to make sure, we're on the same page about [this issue](https://github.com/tensorflow/tensorflow/issues/17009#issuecomment-367940419) right?", "@burglarhobbit still I'm not sure whether these issues are related. It'd be good to rule this one out before spending time debugging that one separately, though.\r\n\r\nCan you build from source with cuda 8? Annoying for sure, but the fix is in C++ and so needs to be compiled."]}, {"number": 15751, "title": "Uninitialised Classifier or invalid context", "body": "I was able to run the tensorflowlite demo app with no error. But it is not detecting the objects as it is supposed to. I have followed the instructions to be followed  in android studio in readme.\r\n![Uploading Screenshot_20171231-151052.png\u2026]()\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "I'm having the same issue. \r\n\r\nIn \"Camera2BasicFragment.java -> classifyFrame()\"\r\n\r\n    if (classifier == null || getActivity() == null || cameraDevice == null) {\r\n        showToast(\"Uninitialized Classifier or invalid context.\");\r\n        return;\r\n    }\r\n\r\nthis function throws the error. I checked to see exactly what was wrong and apparently, classifier was null.\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes..I am having the same issue. @tensorflowbutler. What is the resolution ? ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I have the same issue also.", "Hi there , \r\n\r\nDid you copy the tflite file in the assets folder ? \r\n\r\nI had the same issue , but got the demo project working by copying this file  **_mobilenet_quant_v1_224.tflite_** in the assets folder.\r\n\r\n\r\n", "3 ways to correct this error (of course, only do one of them).\r\n\r\n1. You can add a labels file in assets called labels.txt with the content of labels_mobilenet_quant_v1_224.txt\r\n2. OR rename the given file in assets called labels_mobilenet_quant_v1_224.txt to labels.txt\r\n3. OR change the returned value in `getLabelPath()` in ImageClassifierQuantizedMobileNet.java to `return \"labels_mobilenet_quant_v1_224.txt\";`", "I get the same error!\r\nIt caused by MISSING .tflite/.\r\nfitstly  you need download the files at:[https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip)", "I have the same issue also.\r\n\r\nmobilenet_quant_v1_224.tflite missing copying this file in the assets folder.\r\n\r\nissue solved. !!\r\n\r\nthanks @ahdidou-mohamed @HandsomeL ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I follow the \"image retrain\" retrain my own data. if I down load official .pb and convert to .lite , I can run on android device successfully.  but i I train my own data using mobile v2, error occurs \"Uninitialised Classifier or invalid context\"", "@tensorflowbutler I'm facing the same issue. I have renamed my labels.txt and model.tflite as per my codes. Still getting the same error. Anybody please to help?\r\n![image](https://user-images.githubusercontent.com/6847030/63915401-d5d54480-ca57-11e9-9b95-84c5e6e420eb.png)\r\n", "how to solve the problem,Does anyone have a solution", "Having the same problem, was anyone able to fix it?", "Hi there! I worked on the problem for several days and would like to share how I solved it.\r\nI use `adb logcat > log.txt` to get the error code. \r\nProgram crashes with `FATAL EXCEPTION: CameraBackground` and `java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (image) with 150528 bytes from a Java Buffer with 602112 bytes.`. \r\nAfter searching on Stack Overflow, I solved the problem with [this link](https://stackoverflow.com/q/67419320).\r\nHope this is helpful!"]}, {"number": 15750, "title": "[XLA] Fix std::array initialization take 2", "body": "#15511 did not fix the issue on MSVC.\r\n\r\nThe actual root cause is due to the presence of `(` and `)` around const C array. For `std::array<int, 2> a ({1, 2, 3})`, MSVC seems to see `({1, 2, 3})` as a pointer, which triggers [C2100 compile error](https://msdn.microsoft.com/en-us/library/bzf3eha6.aspx). Removing `(` and `)` fixes the issue.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15749, "title": "[XLA] Define TF_COMPILE_LIBRARY for two libraries", "body": "Both [`index_ops_kernel_argmax_float_1d.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_1d.cc#L48) and [`index_ops_kernel_argmax_float_2d.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/index_ops_kernel_argmax_float_2d.cc#L50) use `TF_EXPORT` macro. We need to define `TF_COMPILE_LIBRARY` (comes from [`tf_copts`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L170)) to make sure `TF_EXPORT` is expanded into `__declspec(dllexport)`.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15748, "title": "[XLA] Add missing win header deps to framework_lite", "body": "Continue from #15579. Most Tensorflow components depend on `framework` rather than `framework_lite`, so I did not notice this until I try to build XLA/tfcompile locally again.\r\n\r\n#15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15747, "title": "An easy problem about tensorflow GPU unilization", "body": "Today , I run an easy RNN model to deal with a  NLP task, which the traindata is just about 4000 short sentences and the iteration is 30. However, the time cost about 1 hour to finish the procedure. my computer GPU is nvidia 1080Ti and CPU is i7 8700K. \r\n\r\nIn the console, the information is: \r\nTotal memory: 10.91GiB\r\nFree memory: 10.30GiB\r\n\r\nbut in the terminal, i use the command\uff1a nvidia-smi -l\r\nit return another information:\r\n10713MiB / 11171MiB \r\n\r\nmy question are : 1. Does the GPU really work ??? \r\n                             2. if work, why the time cost still large???\r\n\r\nThx\uff01", "comments": ["@cancan101  you please take a look? Thanks", "Perhaps you can try profiling your code while you wait for a tensorflower. Here is an issue that might help: #1824 ", "While you're at it also check these out:\r\n[tensorflow low gpu usage on titan x](https://stackoverflow.com/questions/37533863/tensorflow-low-gpu-usage-on-titan-x?rq=1)\r\n[tensorflow gpu utilization only 60% (gtx 1070)](https://stackoverflow.com/questions/40201594/tensorflow-gpu-utilization-only-60-gtx-1070?rq=1)", "@MarkSonn Thank you for your good answer, i will check my code according the solutions~~", "@ zheng-xq do you have any comments?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Can you clarify what you mean by \"In the console, the information is:\"? What is the console, and how is it different from the terminal?\r\n\r\nAlso can you provide an example? We cannot debug or reproduce this without an self-contained code sample.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15746, "title": "CIFAR10 slows down every 100th step", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: Have tried both binary and source\r\n- **TensorFlow version**: 1.4.0-19-ga52c8d9, 1.4.1\r\n- **Python version**: 2.7.12\r\n- **CUDA/cuDNN version**: 8.0.61\r\n- **Hardware**: GPU: NVIDIA GeForce GTX 1080 Ti (11GB), RAM: 64GB, CPU: Intel i7-6850K\r\n- **Exact command to reproduce**: python cifar10_train.py\r\n\r\n### Describe the problem\r\nThe [CIFAR10 Tensorflow tutorial](https://www.tensorflow.org/tutorials/deep_cnn) seems to have a few odd patterns when it comes to the number of examples per second it can compute:\r\n - Step 0 is extremely slow \r\n - Every 100th step is significantly slow \r\n - The step after a really slow step is either a little slow or average\r\n - The next 10-30 steps after that are slightly boosted (faster than average)\r\n - The rest of the steps are average speed\r\n\r\nI'm hoping for (in order of importance):\r\n - An explanation and fix for every 100th step being so slow\r\n - An explanation and instructions showing me how to make every step run at the boosted speed (the speed shortly after a slow step)\r\n - An explanation and fix for the slow 0th and 1st step\r\n\r\nI can't find any additional logging or processing that happens on every 100th step. Could it be `tf.train.MonitoredSession`?\r\n\r\n### Reproducible:\r\n- when training on CPU rather than GPU\r\n- independent of batch size\r\n- on MacBook Pro (Retina, 13-inch, Mid 2014)\r\n\r\n### Hardware utilization: \r\n1. Average:\r\n- CPU: 82-84%\r\n- GPU: 70-85%\r\n- RAM: 3.7GB\r\n\r\n2. Every 100th step:\r\n- CPU: 9%\r\n- GPU: 0%\r\n- RAM: 3.7GB\r\n\r\n3. Boosted after slow step: \r\n- GPU: 92%\r\n- CPU: 82-84%\r\n- RAM: 3.7GB\r\n\r\n4. Idle:\r\n- CPU: 1%\r\n- GPU: 0%\r\n- RAM: 1.6GB\r\n\r\n*Overall CPU and RAM usage (clearly showing CPU trough every 100 steps)*:\r\n![Overall CPU and RAM usage](https://user-images.githubusercontent.com/7654904/34998310-6b153646-fae7-11e7-8e0c-00ccf01349bf.png)\r\n\r\n### Logs excerpt [(full logs)](https://github.com/tensorflow/tensorflow/files/1635342/terminalOutput.txt):\r\n\r\n> step 0: (587.3 examples/sec; 6.974 sec/batch)\r\n> step 1: (22630.6 examples/sec; 0.181 sec/batch)\r\n> step 2: (36253.6 examples/sec; 0.113 sec/batch)\r\n> step 3: (37966.0 examples/sec; 0.108 sec/batch)\r\n> step 4: (38511.4 examples/sec; 0.106 sec/batch)\r\n> step 5: (38554.6 examples/sec; 0.106 sec/batch)\r\n> step 6: (32112.4 examples/sec; 0.128 sec/batch)\r\n> step 7: (38912.4 examples/sec; 0.105 sec/batch)\r\n> step 8: (39377.0 examples/sec; 0.104 sec/batch)\r\n> step 9: (38206.2 examples/sec; 0.107 sec/batch)\r\n> step 10: (38222.1 examples/sec; 0.107 sec/batch)\r\n> step 11: (38757.5 examples/sec; 0.106 sec/batch)\r\n> step 12: (38833.1 examples/sec; 0.105 sec/batch)\r\n> step 13: (39774.8 examples/sec; 0.103 sec/batch)\r\n> step 14: (39795.9 examples/sec; 0.103 sec/batch)\r\n> step 15: (37850.5 examples/sec; 0.108 sec/batch)\r\n> step 16: (38443.5 examples/sec; 0.107 sec/batch)\r\n> step 17: (39194.6 examples/sec; 0.105 sec/batch)\r\n> step 18: (39164.0 examples/sec; 0.105 sec/batch)\r\n> step 19: (39057.5 examples/sec; 0.105 sec/batch)\r\n> step 20: (33268.7 examples/sec; 0.123 sec/batch)\r\n> step 21: (39459.7 examples/sec; 0.104 sec/batch)\r\n> step 22: (39336.2 examples/sec; 0.104 sec/batch)\r\n> step 23: (39207.1 examples/sec; 0.104 sec/batch)\r\n> step 24: (39330.5 examples/sec; 0.104 sec/batch)\r\n> step 25: (38783.9 examples/sec; 0.106 sec/batch)\r\n> step 26: (39038.9 examples/sec; 0.105 sec/batch)\r\n> step 27: (39214.2 examples/sec; 0.104 sec/batch)\r\n> step 28: (39525.9 examples/sec; 0.104 sec/batch)\r\n> step 29: (37209.0 examples/sec; 0.110 sec/batch)\r\n> step 30: (38356.7 examples/sec; 0.107 sec/batch)\r\n> step 31: (36077.0 examples/sec; 0.114 sec/batch)\r\n> step 32: (37143.8 examples/sec; 0.110 sec/batch)\r\n> step 33: (35961.1 examples/sec; 0.114 sec/batch)\r\n> step 34: (33378.4 examples/sec; 0.123 sec/batch)\r\n> step 35: (37830.3 examples/sec; 0.108 sec/batch)\r\n> step 36: (36789.5 examples/sec; 0.111 sec/batch)\r\n> step 37: (36638.2 examples/sec; 0.112 sec/batch)\r\n> step 38: (36848.1 examples/sec; 0.111 sec/batch)\r\n> step 39: (36041.4 examples/sec; 0.114 sec/batch)\r\n> step 40: (36612.0 examples/sec; 0.112 sec/batch)\r\n> step 41: (35623.9 examples/sec; 0.115 sec/batch)\r\n> step 42: (37589.3 examples/sec; 0.109 sec/batch)\r\n> step 43: (37462.9 examples/sec; 0.109 sec/batch)\r\n> step 44: (35823.6 examples/sec; 0.114 sec/batch)\r\n> step 45: (35911.8 examples/sec; 0.114 sec/batch)\r\n> step 46: (36073.8 examples/sec; 0.114 sec/batch)\r\n> step 47: (36930.2 examples/sec; 0.111 sec/batch)\r\n> step 48: (36142.9 examples/sec; 0.113 sec/batch)\r\n> ...\r\n> step 99: (36434.8 examples/sec; 0.112 sec/batch)\r\n> step 100: (1215.0 examples/sec; 3.371 sec/batch)\r\n> step 101: (35952.9 examples/sec; 0.114 sec/batch)\r\n> step 102: (38422.5 examples/sec; 0.107 sec/batch)\r\n> step 103: (39315.8 examples/sec; 0.104 sec/batch)\r\n> step 104: (38989.1 examples/sec; 0.105 sec/batch)\r\n> step 105: (39091.4 examples/sec; 0.105 sec/batch)\r\n> step 106: (39247.6 examples/sec; 0.104 sec/batch)\r\n> step 107: (38009.7 examples/sec; 0.108 sec/batch)\r\n> step 108: (38746.7 examples/sec; 0.106 sec/batch)\r\n> step 109: (39505.4 examples/sec; 0.104 sec/batch)\r\n> step 110: (39340.0 examples/sec; 0.104 sec/batch)\r\n> step 111: (39065.0 examples/sec; 0.105 sec/batch)\r\n> step 112: (38561.1 examples/sec; 0.106 sec/batch)\r\n> step 113: (39109.0 examples/sec; 0.105 sec/batch)\r\n> step 114: (39203.7 examples/sec; 0.104 sec/batch)\r\n> step 115: (39144.4 examples/sec; 0.105 sec/batch)\r\n> step 116: (38317.6 examples/sec; 0.107 sec/batch)\r\n> step 117: (33757.5 examples/sec; 0.121 sec/batch)\r\n> step 118: (34115.4 examples/sec; 0.120 sec/batch)\r\n> step 119: (35671.4 examples/sec; 0.115 sec/batch)\r\n> step 120: (35297.2 examples/sec; 0.116 sec/batch)\r\n> step 121: (36152.8 examples/sec; 0.113 sec/batch)\r\n> step 122: (35780.1 examples/sec; 0.114 sec/batch)\r\n> step 123: (35847.1 examples/sec; 0.114 sec/batch)\r\n> step 124: (36888.9 examples/sec; 0.111 sec/batch)\r\n> step 125: (36946.2 examples/sec; 0.111 sec/batch)\r\n> ...", "comments": ["Perhaps a checkpoint is being written or something? @nealwu do you know who owns the CIFAR10 tutorial?", "I don't think anyone is particularly maintaining that code. For CIFAR-10 I would recommend looking at our [example in the official models](https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py) instead. As a plus, it also achieves much higher accuracy.", "If you're looking for multi-GPU, you can also check out [cifar10_estimator](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator).\r\n\r\nAnd for the future you should open models issues in the [models repo](https://github.com/tensorflow/models).\r\n\r\nDoes the code slow down every 100 steps or every 10 steps? 10 steps would make more sense because the file has an argument called [log_frequency](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py#L57) which defaults to 10.", "Thanks @skye and @nealwu I have searched through every file for what is written every 100 steps (even used the search functionality for things like \"100\" and \"%\" in cifar10_train.py, cifar10.py and cifar10_input.py which are the 3 files that run) and haven't found anything useful.\r\n\r\nGood idea @nealwu I've looked at those models briefly but I'll consider switching over to them. It's not the log frequency, I have set my log frequency to 1 for those logs above so that you can see that the issue is every 100 steps.", "Now that I have optimized my algorithm this slows it by over 30%. Is it possible to assign someone please?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@tfboyd is that something that we're aware of?", "With queues deprecated, I would highly suggest using the model Neal suggested which has multi-gpu done decently as well as flipping data formats for CPU vs. GPU and uses tf.data.  It also likely trains to a better accuracy.  I do not want to make any wild guesses about step 100.  \r\n\r\nStep 0 likely includes starting up the queue and preparing part of the data.  Debugging this on the cifar10_estimator would be better, it also has better support all around.  I worked to get the cifar10_estimator version created because I didn't want to look at this example anymore.  :-)  ", "Thanks @tfboyd ! Closing then, since we have a better recommended way."]}, {"number": 15745, "title": "Eager: variable created in @tfe.defun is invalid and raise error when print", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.5.0dev20171230\r\n- **Python version**: 3.6\r\n\r\n### Describe the problem\r\nI want to use defun to speed up static rnn compute in eager:\r\n```Python\r\ndef _eager_dynamic_rnn(cell,\r\n                       inputs,\r\n                       sequence_length=None,\r\n                       initial_state=None,\r\n                       dtype=tf.float32,\r\n                       parallel_iterations=None,\r\n                       swap_memory=False,\r\n                       time_major=False,\r\n                       scope=None):\r\n    time_axis = 0 if time_major else 1\r\n    input_shape = inputs.shape.as_list()\r\n    seq_len = input_shape[time_axis]\r\n    if time_major:\r\n        batch_size = input_shape[1]\r\n    else:\r\n        batch_size = input_shape[0]\r\n    if initial_state is None:\r\n        initial_state = cell.zero_state(batch_size, dtype)\r\n    inputs = tf.unstack(inputs, num=seq_len, axis=time_axis)\r\n    outputs = []\r\n    for inp in inputs:\r\n        output, initial_state = cell(inp, initial_state)\r\n        outputs.append(output)\r\n    outputs = tf.stack(outputs, axis=time_axis)\r\n    return outputs, initial_state\r\n\r\n\r\n@tfe.defun\r\ndef _eager_compiled_dynamic_rnn(cell,\r\n                                inputs,\r\n                                sequence_length=None,\r\n                                initial_state=None,\r\n                                dtype=tf.float32,\r\n                                parallel_iterations=None,\r\n                                swap_memory=False,\r\n                                time_major=False,\r\n                                scope=None):\r\n    return _eager_dynamic_rnn(cell, inputs, sequence_length, initial_state,\r\n                              dtype, None, False, time_major, scope)\r\n```\r\nIf I directly use `_eager_compiled_dynamic_rnn` in forward, because of `tf.layers.Layer` create variables in its first __call__, then variables created in `_eager_compiled_dynamic_rnn` is invalid, if print it, get a error:\r\n```Python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-308544234cc4> in <module>()\r\n      1 for var in net.variables:\r\n----> 2     print(var)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in __repr__(self)\r\n    233       return \"<tf.Variable '%s' shape=%s dtype=%s, numpy=%s>\" % (\r\n    234           self.name, self.get_shape(), self.dtype.name,\r\n--> 235           ops.numpy_text(self.read_value(), is_repr=True))\r\n    236     else:\r\n    237       return \"<tf.Variable '%s' shape=%s dtype=%s>\" % (\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in read_value(self)\r\n    679       # Ensure we read the variable in the same device as the handle.\r\n    680       with ops.device(self._handle_device):\r\n--> 681         value = self._read_variable_op()\r\n    682     # Return an identity so it can get placed on whatever device the context\r\n    683     # specifies instead of the device where the variable is.\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in _read_variable_op(self)\r\n    657       tape.watch_variable(self)\r\n    658     return gen_resource_variable_ops.read_variable_op(self._handle,\r\n--> 659                                                       self._dtype)\r\n    660 \r\n    661   def read_value(self):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py in read_variable_op(resource, dtype, name)\r\n    209     _attrs = (\"dtype\", dtype)\r\n    210     _result = _execute.execute(b\"ReadVariableOp\", 1, inputs=_inputs_flat,\r\n--> 211                                attrs=_attrs, ctx=_ctx, name=name)\r\n    212   _execute.record_gradient(\r\n    213       \"ReadVariableOp\", _inputs_flat, _attrs, _result, name)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     59                                                op_name, inputs, attrs,\r\n---> 60                                                num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nTypeError: provided list of inputs contains objects other than 'EagerTensor'\r\n```\r\nTo solve this problem, I must call function which isn't decorated by tfe.defun in first forward, then switch to `_eager_compiled_dynamic_rnn`:\r\n```Python\r\nif self._cell.built is True:\r\n    func = _eager_compiled_dynamic_rnn\r\nelse:\r\n    func = _eager_dynamic_rnn\r\noutputs, state = func(\r\n    self._cell,\r\n    inputs,\r\n    seq_len,\r\n    state,\r\n    dtype=self._rnn_dtype,\r\n    time_major=self._time_major, )\r\n```\r\nlocate this error cost me much time. please consider to fix it.", "comments": ["@alextp can you please take a look? Thanks", "Yes for now creating variables isn't supported in tfe.defun. We are working on what the right answer for this should be", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@akshayka is this fixed now?", "Apologies for the delayed response. Yes, commit dbb0feec02a6c34c457ebe83ea307043939b0fc3 fixed this issue: if a Layer is called within a defun, variables are automatically lifted out. "]}, {"number": 15744, "title": "Summary op crashes when run multiple times", "body": "### System information\r\n- **Windows 10 16299**:\r\n- **pip install tensorflow-gpu**:\r\n- **b'unknown' 1.4.0**:\r\n- **3.5**: \r\n- **CUDA: V9.1.85, cuDNN version: 6**:\r\n- **Geforce 930 MX, 2GB**:\r\n\r\n### Describe the problem\r\nOpening a session for the second time and trying to run a summary op causes crash for some reason. The error message is misleading as a placeholder is provided.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MyModel:\r\n    def __init__(self, sess, i):\r\n        self.x = tf.placeholder(tf.float32, [2, 2])\r\n        self.W = tf.Variable(tf.truncated_normal([2, 2]))\r\n        self.y = tf.matmul(self.W, self.x)\r\n\r\n        tf.summary.scalar('y',tf.reduce_sum(self.y))\r\n\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        self.summary = tf.summary.merge_all()\r\n        self.writer = tf.summary.FileWriter('./logs/test' + str(i),sess.graph)\r\n\r\n    def run(self, sess):        \r\n        feed_dict = {self.x:[[0,0],[0,0]]}\r\n        sess.run(self.y,feed_dict)\r\n        print('inside1')\r\n        s = sess.run(self.summary,feed_dict)\r\n        print('inside2')\r\n        self.writer.add_summary(s,0)\r\n\r\n    def close(self):\r\n        self.writer.close()        \r\n\r\n# Swapping order of line 1 and 2 still causes crash\r\nfor i in range(3): # line 1\r\n    with tf.Session() as sess: # line 2\r\n        print('outside')\r\n        M = MyModel(sess, i)\r\n        M.run(sess)\r\n        M.close()\r\n    #sess.close() # No luck\r\n```\r\n\r\nOUTPUT:\r\n```\r\noutside\r\ninside1\r\ninside2\r\noutside\r\ninside1\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Sum_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\TensorFlow\\tensorflow-experiments\\ravens-matrix-autoencoder\\minimum.py\", line 31, in <module>\r\n    M.run(sess)\r\n  File \"D:\\TensorFlow\\tensorflow-experiments\\ravens-matrix-autoencoder\\minimum.py\", line 20, in run\r\n    s = sess.run(self.summary,feed_dict)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Sum_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'Placeholder', defined at:\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\idlelib\\run.py\", line 124, in main\r\n    ret = method(*args, **kwargs)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\idlelib\\run.py\", line 351, in runcode\r\n    exec(code, self.locals)\r\n  File \"D:\\TensorFlow\\tensorflow-experiments\\ravens-matrix-autoencoder\\minimum.py\", line 30, in <module>\r\n    M = MyModel(sess, i)\r\n  File \"D:\\TensorFlow\\tensorflow-experiments\\ravens-matrix-autoencoder\\minimum.py\", line 5, in __init__\r\n    self.x = tf.placeholder(tf.float32, [2, 2])\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1599, in placeholder\r\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3090, in _placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\windows\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [2,2]\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[2,2], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: Sum_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_7_Sum_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n", "comments": ["I believe the problem is that you're creating a new MyModel (which creates the placeholder, summary ops etc.) on every iteration, so after the first iteration you're no longer feeding the first MyModel's placeholders which are needed by the summary ops. Does this make sense?", "Closing because I don't believe this is a bug."]}, {"number": 15743, "title": "Including common.h with NEON_2_SSE.h", "body": "Including common.h to make sure that USE_NEON is defined in case of NEON_2_SSE.h is used; otherwise USE_NEON will not be propagated to this file and `portable_tensor_utils.h` will be used", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Can one of the admins verify this patch?", "This does not build in all configurations:\r\n```\r\n./tensorflow/contrib/lite/kernels/internal/common.h:42:10: fatal error: 'NEON_2_SSE.h' file not found\r\n#include \"NEON_2_SSE.h\"\r\n```\r\nwhile building `tensor_utils`.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Still no go on MacOS.", "I've tried to build those tests on my MacOS and everything was fine, I've used similar parameters to the build that failed, so I think that it was related to caching or something like that, so I wanted to rerun builds to make sure that it's not the case\r\n", "Jenkins, test this please.", "MacOS link error. Can you check?\r\n\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/contrib/lite/kernels/BUILD:139:1: Couldn't build file tensorflow/contrib/lite/kernels/activations_test: Linking of rule '//tensorflow/contrib/lite/kernels:activations_test' failed (Exit 1): cc_wrapper.sh failed: error executing command \r\n  (cd /Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=10.12 \\\r\n    PATH=/Volumes/BuildData/tmpfs/src/tf_build_env/bin:/Users/kbuilder/bin:/Users/kbuilder/.rvm/gems/ruby-2.4.1/bin:/Users/kbuilder/.rvm/gems/ruby-2.4.1@global/bin:/Users/kbuilder/.rvm/rubies/ruby-2.4.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/Applications/CMake.app/Contents/bin:/Applications/Unity/Unity.app/MacOS:/Users/kbuilder/Library/Android/sdk/platform-tools:/Users/kbuilder/Library/Android/sdk/tools/bin:/usr/local/sbin:/Users/kbuilder/.rvm/bin \\\r\n    PYTHON_BIN_PATH=/Volumes/BuildData/tmpfs/src/tf_build_env/bin/python2 \\\r\n    PYTHON_LIB_PATH=/Volumes/BuildData/tmpfs/src/tf_build_env/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TMPDIR=/tmpfs/tmp \\\r\n    XCODE_VERSION_OVERRIDE=8.1.0 \\\r\n  external/local_config_cc/cc_wrapper.sh -fobjc-link-runtime -o bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/activations_test '-Wl,-rpath,$ORIGIN/../../../../_solib_darwin_x86_64/_U_S_Stensorflow_Scontrib_Slite_Skernels_Cactivations_Utest___Utensorflow' -Lbazel-out/darwin_x86_64-opt/bin/_solib_darwin_x86_64/_U_S_Stensorflow_Scontrib_Slite_Skernels_Cactivations_Utest___Utensorflow bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/_objs/activations_test/tensorflow/contrib/lite/kernels/activations_test.o -ltensorflow_framework bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/libtest_util.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/libbuiltin_ops.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/internal/libquantization_util.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/internal/libtensor_utils.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/internal/libportable_tensor_utils.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/libstring_util.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/libframework.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/kernels/libgemm_support.a bazel-out/darwin_x86_64-opt/bin/tensorflow/contrib/lite/libcontext.a bazel-out/darwin_x86_64-opt/bin/external/com_google_absl/absl/base/libbase.a bazel-out/darwin_x86_64-opt/bin/external/com_google_absl/absl/base/libspinlock_wait.a bazel-out/darwin_x86_64-opt/bin/external/com_google_absl/absl/base/libdynamic_annotations.a bazel-out/darwin_x86_64-opt/bin/external/gif_archive/libgif.a bazel-out/darwin_x86_64-opt/bin/external/jpeg/libjpeg.a bazel-out/darwin_x86_64-opt/bin/external/jpeg/libsimd_none.a bazel-out/darwin_x86_64-opt/bin/external/com_googlesource_code_re2/libre2.a bazel-out/darwin_x86_64-opt/bin/external/farmhash_archive/libfarmhash.a bazel-out/darwin_x86_64-opt/bin/external/fft2d/libfft2d.a bazel-out/darwin_x86_64-opt/bin/external/highwayhash/libsip_hash.a bazel-out/darwin_x86_64-opt/bin/external/highwayhash/libarch_specific.a bazel-out/darwin_x86_64-opt/bin/external/png_archive/libpng.a bazel-out/darwin_x86_64-opt/bin/external/zlib_archive/libzlib.a bazel-out/darwin_x86_64-opt/bin/external/com_google_googletest/libgtest.a -lpthread -lm -Wl,-rpath,@loader_path/,-rpath,@loader_path/..,-rpath,@loader_path/../..,-rpath,@loader_path/../../.. -lpthread -ldl -ldl -lpthread -pthread -lm -lm -pthread -headerpad_max_install_names -lc++ -no-canonical-prefixes).\r\nclang: warning: argument unused during compilation: '-pthread'\r\nclang: warning: argument unused during compilation: '-pthread'\r\nUndefined symbols for architecture x86_64:\r\n  \"tflite::tensor_utils::NeonClipVector(float const*, int, float, float*)\", referenced from:\r\n      tflite::tensor_utils::ClipVector(float const*, int, float, float*) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonSub1Vector(float const*, int, float*)\", referenced from:\r\n      tflite::tensor_utils::Sub1Vector(float const*, int, float*) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonVectorShiftLeft(float*, int, float)\", referenced from:\r\n      tflite::tensor_utils::VectorShiftLeft(float*, int, float) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonReductionSumVector(float const*, float*, int, int)\", referenced from:\r\n      tflite::tensor_utils::ReductionSumVector(float const*, float*, int, int) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonVectorVectorDotProduct(float const*, float const*, int)\", referenced from:\r\n      tflite::tensor_utils::VectorVectorDotProduct(float const*, float const*, int) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonVectorVectorCwiseProduct(float const*, float const*, int, float*)\", referenced from:\r\n      tflite::tensor_utils::VectorVectorCwiseProduct(float const*, float const*, int, float*) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonBatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int)\", referenced from:\r\n      tflite::tensor_utils::BatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonVectorVectorCwiseProductAccumulate(float const*, float const*, int, float*)\", referenced from:\r\n      tflite::tensor_utils::VectorVectorCwiseProductAccumulate(float const*, float const*, int, float*) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)\", referenced from:\r\n      tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int) in libtensor_utils.a(tensor_utils.o)\r\n  \"tflite::tensor_utils::NeonVectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*)\", referenced from:\r\n      tflite::tensor_utils::VectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*) in libtensor_utils.a(tensor_utils.o)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```", "@slevental any progress?", "@drpngx wanna take one more look?", "@rmlarsen is the reason for failure the same, or it's a label that's missing causing it?", "@slevental it looks like an internal infrastructure failure unrelated to your code. I'll run the tests again.", "@rmlarsen Is there any way to see compiler parameters and what architecture it's using for this build, is it x86_64? Would appreciate any extra info on that, as I said, it's very hard to reproduce since everything is working great on my MacOS, but I assume that target architecture could be different. Maybe it's not a good idea to try to push using neon2sse from the beginning. Anyway, it's unlikely that TF Lite will be run at x86. ", "You should be able to click through the links for the failed build and see the logs. I cannot see them at the moment due to a network outage. I'll try again later.", "The error seems unrelated to your change:\r\n\r\n```\r\n======================================================================\r\nERROR: test_long_eval (__main__.ARModelTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: PruneForTargets: Some target nodes not found: init \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/bazel_pip/tensorflow/contrib/timeseries/python/timeseries/ar_model_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/timeseries/python/timeseries/ar_model_test.py\", line 279, in test_long_eval\r\n    variables.global_variables_initializer().run()\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2281, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4932, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: PruneForTargets: Some target nodes not found: init \r\n\r\n----------------------------------------------------------------------\r\nRan 12 tests in 40.883s\r\n\r\nFAILED (errors=1)\r\n```", "It failed the package inclusion rule:\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:264:1: Couldn't build file tensorflow/contrib/lite/kernels/internal/_objs/neon_tensor_utils/tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.o: undeclared inclusion(s) in rule '//tensorflow/contrib/lite/kernels/internal:neon_tensor_utils':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.cc':\r\n  '/Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/contrib/lite/kernels/internal/common.h'\r\n  '/Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/contrib/lite/kernels/internal/types.h'\r\n  '/Volumes/BuildData/tmpfs/src/github/tensorflow/tensorflow/contrib/lite/kernels/internal/compatibility.h'.\r\n```", "Ah, thanks @drpngx !", "Ugh, this is not really the way we do this, but I guess it's consistent with usage. FTR, normally we don't include the same file in two different rules. Typically, only a single rule includes a file. For instance, `:dtypes` appears to be a rule that's \"logical\" for `compatibility.h`. So, you would depend on that library.\r\n\r\n(note the edit)", "@drpngx thanks for your help!", "Yay!"]}]