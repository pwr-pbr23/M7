[{"number": 11878, "title": "SVD-operation on the GPU", "body": "This merge request implements the SVD on the GPU using cuSolver.\r\n\r\nWe are building a sparse Gaussian Mixture Model using Tensorflow and for that we need the SVD and determinant computation (see another merge request). Since these operations were not implemented on the GPU yet and the communication resulted in a large overhead, I implemented them on the GPU.\r\n\r\nThe SVD is implemented using cuSolver. \r\ncuSolver, however, has the downside of only supporting matrices with m>=n. The CPU-Version of SVD supports also matrices with m<n. So this kernel throws an error when matrices with m<n are passed and notifies the user to explicitly switch to the CPU kernel.", "comments": ["Can one of the admins verify this patch?", "@shamanDevel, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @josh11b and @andrewharp to be potential reviewers.", "It took me a while to find a way how to deal with all the needed matrix transpositions when also supporting matrices of shape m<n. The result included a lot of refactoring, but I think that it is now also clearer to understand.\r\n\r\nNote: don't merge yet!\r\nNot all python tests pass. \r\nFirst, the GPU version is numerically much less precise. Even after explicitly increasing the tolerance by a factor of 1000 when the GPU version is running, I sometimes get errors that 0.014% (or so) of the matrices don't match.\r\nSecond: The GPU version fails if one dimension of the input matrix is one. Then the SVD returns zeros for U or V instead of 1. I have to investigate that more.", "@shamanDevel OK, thanks for the update. I have noticed the GPU version being less accurate on occasion, so you might have to increase the tolerance in the tests. Please ping the PR when you are ready to proceed.", "Now it should also work for matrices with shape of m==1 or n==1.\r\nThe unit tests succeed now.\r\n\r\nCan you have a look at the changes?\r\nI'll fix the merge conflicts to master now", "@tensorflow-jenkins test this please", "@shamanDevel Thanks for the updates!\r\n@tensorflow-jenkins test this please", "@shamanDevel It looks like there are a few test failures because tol=1e-14 is too tight for double precision. You can just loosen the tolerance a little bit.", "@rmlarsen all cleanup tasks should now be implemented\r\n@tensorflow-jenkins test this please", "I think I made conflicts for you by merging a bunch of stuff -- can you resolve?", "@martinwicke the conflict was a single blank line with different whitespaces ^^. The conflict is fixed now.", "@shamanDevel Thanks!\r\n@tensorflow-jenkins test this please", "@shamanDevel It looks like you have a compilation error (bad merge?):\r\n\r\n....out/local-opt/bin/tensorflow/core/kernels/_objs/svd_op_gpu/tensorflow/core/kernels/svd_op_gpu.cu.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/core/kernels/svd_op_gpu.cu.cc:413:1: error: expected declaration before '}' token\r\n }  // namespace tensorflow\r\n ^\r\n", "@rmlarsen The error is fixed now.\r\n\r\nOh man, I learned a lot during this pull request. Like this one:\r\nIf you have a fallback in your code (here: no CUDA support), then continue testing this fallback as well!\r\nThis was what caused the error: a stupid typo that only appears if GOOGLE_CUDA is not defined.\r\n\r\nFor the support of complex values, what do you think of the following procedure:\r\nOnce this pull request is merged, I open an issue to discuss how to implement the complex value support. This is needed because cuSolver returns the singular values directly as reals while Eigen returns them as complex values and hence they are casted to real in the python wrapper. So the kernel definition and the python wrapper have to be adapted as well.\r\nThen I'll implement this as a new pull request.", "@shamanDevel Happy to hear it was a \"learning experience\" :-) Thanks for all your hard work! I like your suggestion for adding complex support. I think we should probably clean up the existing interfaces to return real singular values instead of the ugly casting. This might require renaming the kernels (adding V2 suffix for the new version) for backwards compatibility. \r\n@tensorflow-jenkins test this please\r\n", "Yeah, all checks have passed!", "Woohoo! Thanks a lot for the contribution @shamanDevel!", "I don't know why in my PC, SVD on GPU is slower than SVD on CPU... version r1.4(4196d6d)\r\n\r\nfile main.py\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\n\r\nD = 1024\r\ndA = np.random.normal(size=(D,D))\r\n\r\ndev = \"/gpu:0\" if len(sys.argv)==1 else \"/cpu:0\"\r\n\r\nwith tf.device(dev):\r\n    A = tf.placeholder(shape=(D,D),dtype=tf.float32)\r\n    S, U, V = tf.svd(A)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.log_device_placement = True\r\nconfig.graph_options.optimizer_options.global_jit_level=tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\n\r\nfor _ in xrange(10):\r\n    dS, dU, dV = sess.run((S, U, V), feed_dict={A:dA})\r\n```\r\n\r\n## run on GPU\r\n`time python main.py`\r\n```\r\n2017-10-10 16:28:49.047703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-10 16:28:49.048176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2017-10-10 16:28:49.048205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n2017-10-10 16:28:49.064960: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n\r\nSvd: (Svd): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-10-10 16:28:49.067234: I tensorflow/core/common_runtime/placer.cc:874] Svd: (Svd)/job:localhost/replica:0/task:0/device:GPU:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-10-10 16:28:49.067302: I tensorflow/core/common_runtime/placer.cc:874] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/device:GPU:0\r\n2017-10-10 16:28:49.074053: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x488e860\r\npython main.py  27.50s user 2.30s system 100% cpu 29.658 total\r\n```\r\n\r\n## run on CPU\r\n`time python main.py -`\r\n```\r\n2017-10-10 16:29:53.252138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-10 16:29:53.252572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2017-10-10 16:29:53.252600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n2017-10-10 16:29:53.269242: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0\r\n\r\nSvd: (Svd): /job:localhost/replica:0/task:0/device:CPU:0\r\n2017-10-10 16:29:53.271505: I tensorflow/core/common_runtime/placer.cc:874] Svd: (Svd)/job:localhost/replica:0/task:0/device:CPU:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:CPU:0\r\n2017-10-10 16:29:53.271544: I tensorflow/core/common_runtime/placer.cc:874] Placeholder: (Placeholder)/job:localhost/replica:0/task:0/device:CPU:0\r\npython main.py -  34.33s user 10.68s system 621% cpu 7.241 total\r\n```", "@hzhangxyz I'm confused.\r\nYour first log shows that the ops are placed on the GPU while the second log shows the CPU placement. This is exactly opposite to what the execution command 'time python main.py' indicates.\r\nIf I just look at the logs, then the GPU version takes 2.3sec (100% CPU load) and the CPU version takes 10.68sec (621% CPU load).\r\nBased on that, the SVD is clearly faster on the GPU than the CPU version. Even if the CPU version uses multiple cores, indicated by a CPU load of 621%.", "@shamanDevel Hi\r\n\r\n> Your first log shows that the ops are placed on the GPU while the second log shows the CPU placement. This is exactly opposite to what the execution command 'time python main.py' indicates.\r\n\r\nyeah, in `time python main.py` len of `sys.argc` is `1` so it is running on GPU.\r\n\r\n> If I just look at the logs, then the GPU version takes 2.3sec (100% CPU load) and the CPU version takes 10.68sec (621% CPU load).\r\n\r\nbut 2.3sec and 10.68sec are system time, which isn't the exact time cost. the exact time is 29.658sec(gpu) and 7.241sec(cpu). For example `time sleep 1` outputs `sleep 1  0.00s user 0.00s system 0% cpu 1.001 total`, in which 1.001 is the exact timecost, although cpu calculates nothing.\r\n\r\nSo, the problem is that SVD on GPU is slower than SVD on CPU sometimes, which is very strange."]}, {"number": 11877, "title": "Why tf.FIFOQueue didn't removed when using tf.reset_default_graph?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Pip\r\n- **TensorFlow version (use command below)**:  1.2.1\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI want to empty the entire session and grapth while every loop . But the FIFOQueue seems didn't  removed when I  using tf.reset_default_graph.  \r\n\r\nSo how to make everything clean in currrent process(without create a subprocess \\ or change FIFO's name)?\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\"ps\": [\"localhost:65062\"], \"worker\": [\"localhost:65063\"]})\r\nps = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\nworker = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\n\r\nwhile True:\r\n  print \"begin a new job\"\r\n\r\n  print(\"PS: {0}\".format(ps.target))\r\n  print(\"Worker: {0}\".format(worker.target))\r\n\r\n  with tf.Session(worker.target) as sess:\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        W = tf.Variable(tf.zeros([784, 10]))\r\n        b = tf.Variable(tf.zeros([10]))\r\n        file_queue = tf.FIFOQueue(10,\r\n                                     [tf.int32, tf.bool, tf.string, tf.string],\r\n                                     shared_name = 'global_queue')\r\n    init = tf.global_variables_initializer()\r\n    sess.run([init, file_queue.close()])\r\n\r\n  tf.reset_default_graph()\r\n  time.sleep(2)\r\n```\r\n\r\n### ERROR\r\n```\r\nCancelledError (see above for traceback): Queue 'global_queue' is already closed.\r\n\t [[Node: fifo_queue_Close = QueueCloseV2[cancel_pending_enqueues=false, _device=\"/job:ps/replica:0/task:0/cpu:0\"](fifo_queue)]]\r\n```", "comments": ["I believe this is working as intended (you have to reset the session/containers to get rid of persistent ops)", "I have already reset the session by using `tf.reset_default_graph()`  @yaroslavvb ", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (30/25 (120%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o3.sgmail.github.com ([192.254.112.98]:62857)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1dcDpx-0004VZ-83\n\tfor mazecreator@mazecreator.com; Mon, 31 Jul 2017 11:47:03 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=WYoOVBfeONo9LRetAs4v7YBhTcQ=; b=ZgbISU+AtkiLQLrz\n\tt330bqsAVZMKcsOJLDemohwG4DdbkGycxsRQm5cI6UijVYHOAl4QEWAkLn5tNwyC\n\t2ajKdFPa4RLM/pDYo8xaR+hS3vhUe1fq2w9/m8OmsHIQisOAPMtWptzDhkOELpno\n\tc7Xw9y3K5jZLXPtldUkNEth/fHE=\nReceived: by filter1157p1mdw1.sendgrid.net with SMTP id filter1157p1mdw1-3486-597F6161-1E\n        2017-07-31 16:57:05.685394948 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0037p1mdw1.sendgrid.net (SG) with ESMTP id 7_cXyGsZT4OtelFaQAmRoQ\n\tfor <mazecreator@mazecreator.com>; Mon, 31 Jul 2017 16:57:05.647 +0000 (UTC)\nDate: Mon, 31 Jul 2017 16:57:06 +0000 (UTC)\nFrom: Eric Yue <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/11877/319130663@github.com>\nIn-Reply-To: <tensorflow/tensorflow/issues/11877@github.com>\nReferences: <tensorflow/tensorflow/issues/11877@github.com>\nSubject: Re: [tensorflow/tensorflow] Why tf.FIFOQueue didn't removed when\n using tf.reset_default_graph? (#11877)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_597f6161590f4_1f63fba65f29c34367785\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: ericyue\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0b1c70cea1a110cf155b9924b9e38e520116af33892cf000000011597236192a169ce0eb24f54@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoNXqzWRiqlW8Zn091IHg5wQtHXpYks5sTgdhgaJpZM4Onjuq>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1JqxFkI8H/1qRPSchRGXQ5mNwbHrFWff94iJI5f\n tZ1I0uTpgQk+nfiFoULHPDQ6IcZUPr0BTz4EGNcUGUja2VdaV9eEZpK7NnqBzpWih9B367jUBWezt1\n xYIC+JmFvhdMyx9vlrUZuDa0BLWWi3joaB6ltFRyDvlRPMdjLO8W22MkoufDwkj286LzhRp6evfbxa\n E=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_597f6161590f4_1f63fba65f29c34367785\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nI have already reset the session by using `tf.reset_default_graph()`  @yaroslavvb \n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/11877#issuecomment-319130663\n----==_mimepart_597f6161590f4_1f63fba65f29c34367785\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>I have already reset the session by using <code>tf.reset_default_graph()</code>  <a href=\"https://github.com/yaroslavvb\" class=\"user-mention\">@yaroslavvb</a></p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/issues/11877#issuecomment-319130663\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoKAxBX2neC5Y9Jnr1IZvkYum-apEks5sTgdhgaJpZM4Onjuq\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoNUyUKkEhVjCXR2JZTiHicRzOjA4ks5sTgdhgaJpZM4Onjuq.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/issues/11877#issuecomment-319130663\"></link>\n  <meta itemprop=\"name\" content=\"View Issue\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Issue on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@ericyue in #11877: I have already reset the session by using `tf.reset_default_graph()`  @yaroslavvb \"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/11877#issuecomment-319130663\"}}}</script>\n----==_mimepart_597f6161590f4_1f63fba65f29c34367785--\n"]}, {"number": 11876, "title": "All Windows GPU build links are broken.", "body": "The link (bolded) is broken in the webpage of \"https://github.com/tensorflow/tensorflow\".\r\n\r\n<li>Windows GPU: <a href=\"**https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.3.0rc1-cp35-cp35m-win_amd64.whl**\">Python 3.5 64-bit</a> (<a href=\"https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/\">build history</a>) / <a href=\"https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.3.0rc1-cp36-cp36m-win_amd64.whl\">Python 3.6 64-bit</a> (<a href=\"https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/\">build history</a>)</li>\r\n\r\nSome other links in this webpage are also broken. \r\n\r\nBut the file of the corresponding link in the webpage of the \"Build history\" can be downloaded. ", "comments": ["The link https://github.com/tensorflow/tensorflow works for me.\r\n\r\nWhich link pointing to what page is broken?", "@clemens-tolboom I mean the link pointed to \"Python 3.6 64-bit\" is \r\n\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.3.0rc1-cp36-cp36m-win_amd64.whl\r\n\r\nis broken.\r\n\r\nIt should be the following link:\r\n\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.3.0rc0-cp36-cp36m-win_amd64.whl", "tnx ... Maybe change the title to\r\n\r\n> All Windows build link are broken.", "@clemens-tolboom yeah, that would be better. Thanks.", "@gunan, @av8ramit, could you take a look", "https://github.com/tensorflow/tensorflow/pull/11914\r\n\r\nThe root cause of this is that coincidentally after merging rc1 content back into master, the nightly builds for GPU started breaking. This is why the link was updated, but there is no file in that location. Thanks for pointing this out @glhfgg1024 ", "The original links are working now.", "@av8ramit thanks for revisions."]}, {"number": 11875, "title": "Dynamic ksize and strides with MaxPool", "body": "This fix is the renewed effort to fix #4746 with Jenkins failures fixed. (the previous PR was #9514).\r\n\r\nThis fix tries to fix the issue raised in #4746 where ksize and strides is static (attr) with max_pool (and avg_pool).\r\n\r\nThis fix changes ksize and strides to input tensor with MaxPoolV2 so that it is dynamic now. \r\n\r\nThis fix add MaxPoolV2 but hasn't points `nn_ops.max_pool` to `gen_nn_ops.max_pool_v2` yet. It tries to following the same api workflow as PR #10840 (3 weeks before API changes).\r\n\r\nThis fix fixes #4746.", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @jart and @benoitsteiner to be potential reviewers.", "/cc @ayushchd @hgaiser FYI.", "@tensorflow-jenkins test this please", "Thanks @rmlarsen for the review. The PR has been updated. Please take a look.", "@tensorflow-jenkins test this please", "@yongtang It looks like you still have to squash a bug for the GPU test:\r\n\r\n```\r\n======================================================================\r\nERROR: testKernelSmallerThanStrideSame (__main__.PoolingTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/pooling_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 626, in testKernelSmallerThanStrideSame\r\n    v2=v2)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/pooling_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 172, in _VerifyValues\r\n    data_format, expected, use_gpu, v2)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/pooling_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 149, in _VerifyOneTest\r\n    data_format, dtypes.float32, expected, use_gpu, v2)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/pooling_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/pooling_ops_test.py\", line 124, in _VerifyOneType\r\n    t = test_util.NCHWToNHWC(t)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/bin/tensorflow/python/kernel_tests/pooling_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 224, in NCHWToNHWC\r\n    return array_ops.transpose(input_tensor, new_axes[ndims])\r\nKeyError: None\r\n```", "Thanks @rmlarsen. I forgot to enable `MaxPoolV2` in GPU. The PR has been updated now.", "@tensorflow-jenkins test this please", "@martinwicke does this require API review? Should we follow the same route as non_max_suppression and hide the kernels and add a Python wrapper?", "Yes, MaxPoolV2 should be hidden. \r\n\r\nThe automatically generated python doesn't seem to be exposed to the API, so it appears everything is in order. I suspect that in order to expose it it needs to be added to _allowed_symbols somewhere.\r\n\r\nOnce three weeks have passed, we need to add the wrapper which runs MaxPoolV2 whenever tf.max_pool is called.\r\n\r\n", "@tensorflow-jenkins test this please", "@yongtang please add MaxPoolV2 to hidden_ops.txt.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/hidden_ops.txt", "@tensorflow-jenkins test this please", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (27/25 (108%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o9.sgmail.github.com ([167.89.101.2]:20928)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1demvV-0005ix-Lz\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 13:39:23 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=pvZCicpUPTlciaIxigIUQDHrZ7o=; b=AnGbocy65u52o7Bd\n\tI3JYafC/25XSdqXWn9iEydR+8mFaXiP5wRI/yevi1WXWaDK9ExDPtuHrjs57BrxL\n\tKFLiThhV71YDTx7CMaLK6ULBHLwt8iY2fMUJiw4D9sIAwns05AGduh6KMi2mb8H1\n\tgOQULf1JTVOnChGlu7uFF/j+Tew=\nReceived: by filter0966p1mdw1.sendgrid.net with SMTP id filter0966p1mdw1-25672-5988B64A-23\n        2017-08-07 18:49:46.615311371 +0000 UTC\nReceived: from github-smtp2a-ext-cp1-prd.iad.github.net (github-smtp2a-ext-cp1-prd.iad.github.net [192.30.253.16])\n\tby ismtpd0006p1iad1.sendgrid.net (SG) with ESMTP id SOaTTrD3TwCKH9kms9r5pg\n\tfor <mazecreator@mazecreator.com>; Mon, 07 Aug 2017 18:49:46.594 +0000 (UTC)\nDate: Mon, 07 Aug 2017 18:49:46 +0000 (UTC)\nFrom: Rasmus Munk Larsen <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/11875/c320747971@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/11875@github.com>\nReferences: <tensorflow/tensorflow/pull/11875@github.com>\nSubject: Re: [tensorflow/tensorflow] Dynamic ksize and strides with MaxPool\n (#11875)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988b64a7e0e0_527f3fd423369c3c2984ee\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: rmlarsen\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a0f43e119a8280fc94f325b899fb0d3698bb3ced7492cf0000000115a0784a92a169ce0eb22d91@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoCGnBh2HHmtNWHjkKxcF81v0HJS1ks5sV1xKgaJpZM4OnhQn>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1Jqypw2sbL+LJ9Ez0KQmXCbx8OELgCBGmtiPevD\n 0VS5ybRe/e9FzpMzwBv984jYhG0IQQhVRGiujCO9oW8DktU5BoWTACcVA4g47ptSDed0W2sH3Znbdy\n K5LKszqaVQY8qgUu707BbquHzN587NN6EZWmKsJlgkjOB3uvlGag0NKZWcaajYZ8si21yZyT4+X4cW\n KbIIIZxCP2ALJnmPvcuUqH\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5988b64a7e0e0_527f3fd423369c3c2984ee\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@tensorflow-jenkins test this please\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/11875#issuecomment-320747971\n----==_mimepart_5988b64a7e0e0_527f3fd423369c3c2984ee\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://github.com/tensorflow-jenkins\" class=\"user-mention\">@tensorflow-jenkins</a> test this please</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/11875#issuecomment-320747971\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoOXf3fGg12akfOicI4SvTi71h6lSks5sV1xKgaJpZM4OnhQn\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoPnX2YXILyd7QAoa4stAv0H9aoCJks5sV1xKgaJpZM4OnhQn.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/11875#issuecomment-320747971\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@rmlarsen in #11875: @tensorflow-jenkins test this please\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/11875#issuecomment-320747971\"}}}</script>\n----==_mimepart_5988b64a7e0e0_527f3fd423369c3c2984ee--\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "The PR has been updated.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thank for the contribution!", "Is there already a way to use that in python?", "When will we be able to use this in python?", "You can do the following for now:\r\n```python\r\nfrom tensorflow.python.ops import gen_nn_ops\r\npool = gen_nn_ops._max_pool_v2(\r\n    conv,\r\n    ksize=[1, tf.shape(conv)[1], tf.shape(conv)[2], 1],\r\n    strides=[1, 1, 1, 1],\r\n    padding='VALID',\r\n    name=\"pool\")\r\n```\r\n\r\nWhere `conv` is the output of a `tf.layers.conv2d` for example.", "@MichaelScaria @sleighsoft A PR  #14983 is created for the v2 in python fix.", "@sleighsoft gen_nn_ops._max_pool_v2 is no longer available in version 1.7. So your workaround doesn't work anymore.\r\n@yongtang what's worse is that your PR #14983 is reverted. \r\nSo what should we do to use this dynamic ksize?", "@hlzz I believe `_` in generated ops like (`get_nn_ops`) is not needed anymore so I think it is worth to give `gen_nn_ops.max_pool_v2` a try (caution: I haven't tried it myself yet).\r\n\r\nPR #14983 was reverted because it caused some issues with MKL. I don't have a lot of experience with MKL, but will give it a try if I had a chance.\r\n", "@yongtang I've tried, gen_nn_ops.max_pool_v2 is also not there. Don't know what's going on with nn_ops, but it was still there when you made this PR, which is quite recent.", "gen_nn_ops is machine \"generated\" locally. I am using v2 for a while now and it works well.", "@yongtang excuse me, i am working on dynamic k-max pooling also, so i need to use _max_pool_v2 in my program. But it has been reverted as you say few months ago, so i wonder if there is any older version of tensorflow we can use _max_pool_v2. Thanks very much!", "@sleighsoft excuse me, which version of tensorflow you can use _max_pool_v2, or any modifies you make in your tensorflow, ", "No, I did not modify anything. I am currently on some 1.8 nightly windows build.", "@xdsilly As was mentioned in https://github.com/tensorflow/tensorflow/pull/11875#issuecomment-348149489 (@sleighsoft \ud83d\udc4d ), you could use:\r\n\r\n```\r\nfrom tensorflow.python.ops import gen_nn_ops\r\npool = gen_nn_ops.max_pool_v2(....)\r\n```\r\n\r\nMake sure there is no `_` before `max_pool_v2` (`gen_nn_ops.max_pool_v2` not `gen_nn_ops._max_pool_v2`)", "Oh sorry, forgot to mention that the `_` got dropped", "@sleighsoft  Is the same modification applied also for the average pooling layer?"]}, {"number": 11874, "title": "Minor typo correction", "body": "", "comments": ["@adhulipa, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @zjj2wry and @Lewuathe to be potential reviewers.", "Can one of the admins verify this patch?"]}, {"number": 11873, "title": "Fix a minor typo", "body": "Just a fix for minor typo", "comments": ["mute the thread \u53d1\u81ea\u7f51\u6613\u90ae\u7bb1\u5927\u5e08 \u57282017\u5e7407\u670830\u65e5 12:26\uff0cVijay Vasudevan \u5199\u9053\uff1a Merged #11873. \u2014 You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread."]}, {"number": 11872, "title": "How to shutdown a tf.train.Server's port?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Pip\r\n- **TensorFlow version (use command below)**:  1.2.1\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen I  createt a tf.train.Server object, It's open a port immediately, But I can not find any way to close the Server(I means the port tf.train.Server started).   \r\n\r\nWhat can I do if I want to start-run-stop a tf.train.Server in a loop?\r\n\r\nI need to start tf.train.Server in a while loop.\r\n\r\n**one more thing: on my macbook, the code  can run without error, but on a linux server failed.**\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nwhile True:\r\n  print \"begin a new job\"\r\n  cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:65062\"], \"worker\": [\"localhost:65063\"]})\r\n  ps = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\n  worker = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\r\n\r\n  print(\"PS: {0}\".format(ps.target))\r\n  print(\"Worker: {0}\".format(worker.target))\r\n\r\n  with tf.Session(worker.target) as sess:\r\n    with tf.device(\"/job:ps/task:0\"):\r\n        W = tf.Variable(tf.zeros([784, 10]))\r\n        b = tf.Variable(tf.zeros([10]))\r\n\r\n    init = tf.global_variables_initializer()\r\n    print(\"RUNNING SESSION\")\r\n    sess.run(init)\r\n    print(\"SESSION FINISHED\")\r\n  tf.reset_default_graph()\r\n  time.sleep(2)\r\n```\r\n\r\nError\r\n```\r\nE0730 01:49:45.923951794   57304 server_chttp2.c:159]        {\"created\":\"@1501350585.923875462\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/grpc/src/core/ext/transport/chttp2/server/insecure/server_chttp2.c\",\"file_line\":125,\"referenced_errors\":[{\"created\":\"@1501350585.923871295\",\"description\":\"Failed to add port to server\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_posix.c\",\"file_line\":634,\"referenced_errors\":[{\"created\":\"@1501350585.923863108\",\"description\":\"Unable to configure socket\",\"fd\":18,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_posix.c\",\"file_line\":355,\"referenced_errors\":[{\"created\":\"@1501350585.923845040\",\"description\":\"OS Error\",\"errno\":98,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_posix.c\",\"file_line\":331,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}],\"target_address\":\"ipv6:[::]:65062\"}]}\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 16, in <module>\r\n    ps = tf.train.Server(cluster, job_name=\"ps\", task_index=0)\r\n  File \"/home/mk/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/server_lib.py\", line 145, in __init__\r\n    self._server_def.SerializeToString(), status)\r\n  File \"/home/mk/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/mk/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\r\n```\r\n", "comments": ["#4713  this issue might help you.\r\nAlso checkout [Coordinator and QueueRunner](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/coordinator_and_queuerunner) \r\nand there is a high level class [Supervisor](https://www.tensorflow.org/programmers_guide/supervisor) you may be interested.", "There was an attempt to add this functionality in this PR https://github.com/tensorflow/tensorflow/pull/6185 by @llhe \r\n\r\nOtherwise server stays open until the Python process quits.\r\n\r\nIn current situation you could implement this functionality by having a parent process running `while` loop and launching child Python processes which start the server", "@yaroslavvb  thanks, it's exactly what I want to know. It seems that using a subprocess is a good choice currently since the grpc shutdown's incomplete  :)  "]}, {"number": 11871, "title": "Error during compilation of tensorflow-GPU using bazel 0.5.3", "body": "Using Bazel 0.5.3 (from installer, both sh and deb) on Ubuntu 16.04 to compile tensorflow-gpu, I get the following error that fails compilation:\r\n\r\n` Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/nicola/Software/tensorflow/gpu/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)`\r\n\r\nCompilation proceeds just fine using Bazel 0.5.2. Note: the error occurs when I try to compile ANY version of tensorflow (tested 1.2.1, 1.3rc0, 1.3rc1, git).\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from **: source\r\n- **TensorFlow version**: v1.2.0-2210-g49961e5 1.2.1, v.1.3 (rc0, rc1), HEAD\r\n- **Python version**: 3.5.2\r\n- **Bazel version**: (Both 0.5.2 and 0.5.3 from deb installer, and sh installer)\r\n- **CUDA/cuDNN version**: CUDA 8, CuDNN 6.1\r\n- **GPU model and memory**: GeForce 1050Ti - 4Gb\r\n\r\n### Describe the problem###\r\nMake sure you clean the bazel cache. \r\nInstall bazel 0.5.2 (from installer, deb or sh).\r\nTry compilation of tensorflow-gpu. it should work fine. \r\nRemove bazel 0.5.2 and clean its cache. \r\nInstal bazel 0.5.3 (from installer, deb or sh)\r\nRepeat compilation of tensorflow and the error appears, preventing the full compilation.\r\n", "comments": ["I ran into this too building Bazel from head. Looks like we can just cast the returned paths from _get_cxx_inc_directories_impl with str() to make them immutable (not sure what they are by default, but apparently not strings). I'll make that a pull request.", "Is this fixed also in the r1.3 branch?", "No, just master. You could apply the same change to the r1.3 branch manually; it's a bit late to merge it.", "While I can do this easily, it's not clear to the end user without any advisory why it will fail to compile  r1.3 with bazel 0.5.3... A note on the  \"known issues\" in the release notes would be helpful. ", "@av8ramit can we put something in the release notes about building with Bazel 0.5.3 and CUDA? https://github.com/tensorflow/tensorflow/pull/11949 does fix it, but I'm guessing cherrypicks over.", "@allenlavoie I'll create a PR to 1.3 and add you as a reviewer.", "https://github.com/tensorflow/tensorflow/pull/12092/files", "I'm still encountering this, even on r1.3", "Oh... I install from apt-get... how do I get a different version?", "This is fixed in head, but it still present in 1.3. \r\n\r\nYou can patch the source as indicated above (so you can use bazel 0.5.3).", "Closing as this will be fixed in 1.4 and we have listed a workaround for 1.3"]}, {"number": 11870, "title": "[XLA] Remove overly chatty output in fusion operation", "body": "when fusing a lot of operations in a large graph, this trace produces a lot of output.  It isn't really output that is helpful when trying to trace the high level operations occuring.  Perhaps it is better at a lower log level?\r\n\r\n", "comments": ["@DavidNorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @tensorflower-gardener and @meheffernan to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "@tatatodd @benoitsteiner \r\n\r\nThe MacOS CPU tests seem to have been failing a lot recently.  The code which has been changed in this PR is not even compiled into these (non-XLA) tests, so I would be very surprised if they have affected this failure.\r\n", "@tatatodd @benoitsteiner\r\n\r\nHi guys.  Do you think we could get this merged.  I've got quite a few outstanding merge requests, and it's making me nervous  \ud83d\ude30", "Jenkins, test this please.", "@DavidNorman Sorry for the delay.  Although the change is pretty trivial, we'll need to get a clean run of the tests.\r\n\r\n@benoitsteiner Can you merge this once the tests pass, thanks!", "Actually - I think the mac test failures may be due to a recent physical move; perhaps the mac infrastructure isn't all set up yet.\r\n\r\n@benoitsteiner Perhaps you can enlighten us of the process.  Given that this is a pretty trivial change, can we merge it even without mac tests passing?", "@tensorflow-jenkins test this please", "FYI it looks like the `MacOS CPU Tests` are failing because of a grappler issue, and the `Windows Cmake Tests` are failing because of a failure in `tensorflow/python/training/coordinator_test.py`.  Both of these are unrelated to this PR.\r\n\r\nI don't know whether our policy is to always wait for passing tests, or whether we make one-off exceptions.  FWIW this PR is only changing a `VLOG(1)` to `VLOG(3)`, and should be safe to merge, despite the unrelated test failures.", "Test failure is unrelated. Merging."]}, {"number": 11869, "title": "fix link for installing page", "body": "", "comments": ["@superyyrrzz, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@superyyrrzz Can you sign the CLA please ?", "Can one of the admins verify this patch?", "I have signed it using superyyrrzz@gmail.com. It's my GitHub account's primary email.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 11868, "title": "keras resnet50 example yields different predictions than in stand-alone keras", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No (using example from Keras documentation here: https://keras.io/applications/#classify-imagenet-classes-with-resnet50\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: Binary (CPU Version)\r\n- **TensorFlow version (use command below)**: `('v1.2.0-5-g435cdfc', '1.2.1')`\r\n- **Python version**: 2.7 (OS X system version)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nfrom tensorflow.contrib.keras.python.keras.applications.resnet50 import ResNet50\r\nfrom tensorflow.contrib.keras.python.keras.preprocessing import image\r\nfrom tensorflow.contrib.keras.python.keras.applications.resnet50 import preprocess_input, decode_predictions\r\n\r\nimport numpy as np\r\n\r\nmodel = ResNet50(weights='imagenet')\r\n\r\nimg_path = 'elephant.jpg'\r\nimg = image.load_img(img_path, target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x)\r\n\r\npreds = model.predict(x)\r\n\r\nprint('Predicted:', decode_predictions(preds, top=3)[0])\r\n```\r\n\r\n### Describe the problem\r\n\r\nThe above code yields the following output:\r\n\r\n```python\r\n('Predicted:', [(u'n02098286', u'West_Highland_white_terrier', 1.0), \r\n                (u'n15075141', u'toilet_tissue', 0.0), \r\n                (u'n02319095', u'sea_urchin', 0.0)])\r\n```\r\n\r\nHowever, the same code run using Stand-alone Keras yields this:\r\n\r\n```python\r\n('Predicted:', [(u'n02504013', u'Indian_elephant', 0.91937912), \r\n                (u'n01871265', u'tusker', 0.070962951), \r\n                (u'n02504458', u'African_elephant', 0.0095201703)])\r\n```\r\n\r\nNote: to reproduce under stand-alone Keras substitute this code for the imports at the top:\r\n\r\n```python\r\nfrom keras.applications.resnet50 import ResNet50\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\n```\r\n\r\nAlso note that you need the 'elephant.jpg' file in the working directory to reproduce. You can find that file here: https://github.com/rstudio/keras/blob/master/docs/articles/elephant.jpg\r\n\r\n", "comments": ["Observed the same behavior with `('v1.3.0-rc1-447-g4b50313', '1.2.1-rc1')`", "I have noticed this issue for long. See [https://github.com/tensorflow/tensorflow/issues/11160](https://github.com/tensorflow/tensorflow/issues/11160). The predefined weights seem to be essentially random in tf.contrib.keras and issues do not seem to get much attention.  After some time trying to port my keras models to tf.contrib.keras, I have moved to tf-slim models/api which is excellent.", "@fchollet, could you take a look, please. Thanks!\r\n", "@aselle I won't have time to look into these issues for at least a week or so. Can we find someone else? Currently every single issue related to Keras is routed to me personally, which isn't scalable.", "There is no significant change in the resnet script, but several tf-keras layers have been refactored in [here](https://github.com/tensorflow/tensorflow/commit/35253fa89c5f8af25e3d84f76980729569091a6c). Thus, I tested the four individual layers mainly used in the resnet, and found something weird in `Conv2D` as follows:\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = np.random.random((1, 6, 6, 3))\r\nx1 = keras.layers.Input((6, 6, 3))\r\nx2 = tf.contrib.keras.layers.Input((6, 6, 3))\r\n\r\nlayers1 = [\r\n    keras.layers.Conv2D(10, (5, 5))(x1),\r\n    keras.layers.BatchNormalization()(x1),\r\n    keras.layers.MaxPooling2D()(x1),\r\n    keras.layers.AveragePooling2D()(x1)\r\n]\r\n\r\nlayers2 = [\r\n    tf.contrib.keras.layers.Conv2D(10, (5, 5))(x2),\r\n    tf.contrib.keras.layers.BatchNormalization()(x2),\r\n    tf.contrib.keras.layers.MaxPooling2D()(x2),\r\n    tf.contrib.keras.layers.AveragePooling2D()(x2)\r\n]\r\n\r\nfor (k, t) in zip(layers1, layers2):\r\n    model1 = keras.models.Model(x1, k)\r\n    model2 = tf.contrib.keras.models.Model(x2, t)\r\n    y1 = model1.predict(x)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        y2 = model2.predict(x)\r\n    print(\"%s: %.3f\" % (t.name, np.sum((y1 - y2) ** 2)))\r\n```\r\nThe results are:\r\n```\r\nconv2d/BiasAdd:0: 9.476\r\nbatch_normalization/batchnorm/add_1:0: 0.000\r\nmax_pooling2d/MaxPool:0: 0.000\r\naverage_pooling2d/AvgPool:0: 0.000\r\n```", "I examined each of the layers in ResNet50 and found a slight difference between `BatchNormalization` layers of keras and tf-keras. Would you please check this comment @aselle, @fchollet?\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\r\nimg = image.load_img('elephant.png', target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x)\r\n\r\nresnet1 = keras.applications.ResNet50(weights='imagenet')\r\nresnet2 = tf.contrib.keras.applications.ResNet50(weights='imagenet')\r\n\r\nfor i in range(1, 3):\r\n    model1 = keras.models.Model(inputs=resnet1.input, outputs=resnet1.layers[i].output)\r\n    model2 = tf.contrib.keras.models.Model(inputs=resnet2.input, outputs=resnet2.layers[i].output)\r\n    y1 = model1.predict(x)\r\n    y2 = model2.predict(x)\r\n    print(\"- %s -\" % model1.layers[-1].name)\r\n    print(\"Residual sum: %.3f\" % np.sum((y1 - y2) ** 2))\r\n    print(\"Keras weights: %s\" % model1.layers[-1].weights)\r\n    print(\"TF-Keras weights: %s\" % model2.layers[-1].weights)\r\n```\r\nThe results are:\r\n```python\r\n- conv1 -\r\nResidual sum: 0.000\r\nKeras weights: [<tf.Variable 'conv1/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref>, <tf.Variable 'conv1/bias:0' shape=(64,) dtype=float32_ref>]\r\nTF-Keras weights: [<tf.Variable 'conv1/kernel_1:0' shape=(7, 7, 3, 64) dtype=float32_ref>, <tf.Variable 'conv1/bias_1:0' shape=(64,) dtype=float32_ref>]\r\n- bn_conv1 -\r\nResidual sum: 9147763.000\r\nKeras weights: [<tf.Variable 'bn_conv1/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_mean:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_variance:0' shape=(64,) dtype=float32_ref>]\r\nTF-Keras weights: [<tf.Variable 'bn_conv1/beta_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/gamma_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_mean_1:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'bn_conv1/moving_variance_1:0' shape=(64,) dtype=float32_ref>]\r\n```\r\n\r\nWhile the gamma is defined first in original keras, the beta is done first in tf keras. This order is important because the `load_weights()` of keras API checks shapes of symbolic and numpy tensors according to the order. In order words, pretrained variables are stored as [gamma, beta] format but tf-keras reads them as [beta, gamma] without error due to the same shape. The `load_weights()` doesn't check tensor names.\r\n\r\nI listed up two concise solutions as follows:\r\n- Swapping the two `self.add_variable` calls for gamma and beta in `tensorflow/python/layers/normalization.py`: I think this might be the best if the swapping doesn't break someone's code.\r\n- Adding `self.trainable_weights = self.trainable_weights[::-1]` after layer building in `tensorflow/contrib/keras/python/keras/layers/normalization.py`: This seems to be a patchwork but no one's codes will break.\r\nWhich one of these looks better?", "This has been fixed last week and will soon appear in GitHub.", "I just checkout the latest tree with\r\n   git fetch -a\r\n   git checkout master\r\n   git pull\r\n\r\nBut, I still cannot find the fix for issue #11868. Can you please double-check if it was merged to GitHub ?\r\nThanks!\r\n\r\ncounterexample: #13562 is visible.\r\n"]}, {"number": 11867, "title": "Dense layer throwing error with inputs from Keras Concatenate layer", "body": "I think this is the most weird error I have ever seen. I concatenated the outputs of `max_pool` and `average_pool` using two different methods and passed the output to another dense layer as :\r\n\r\n```\r\n1)   concatenated_layer = tf.concat([max_pool_output, avg_pool_output], axis=1)\r\n      fc_layer = tf.layers.dense(concatenated_layer, 1024, tf.nn.relu)     # works fine\r\n\r\n2) concatenated_layer = keras.layers.Concatenate([max_pool_output, avg_pool_output])\r\n    fc_layer = tf.layers.dense(concatenated_layer, 1024, tf.nn.relu)     # throws error\r\n```\r\n\r\nThe error thrown is this:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-26-7320655fd554> in <module>()\r\n----> 1 fc_layer = tf.layers.dense(concatenated_layer, 1024,tf.nn.relu,)\r\n\r\n~/anaconda2/envs/Kaggle/lib/python3.5/site-packages/tensorflow/python/layers/core.py in dense(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)\r\n    213                 trainable=trainable,\r\n    214                 name=name,\r\n--> 215                 dtype=inputs.dtype.base_dtype,\r\n    216                 _scope=name,\r\n    217                 _reuse=reuse)\r\n\r\nAttributeError: 'Concatenate' object has no attribute 'dtype'\r\n```\r\n\r\nAs the incoming inputs are all `float32`, shouldn't the dense layer infer that itself?", "comments": ["`keras.layers.Concatenate`  is a class. `Concatenate()` returns a class instance, and it only expects an `axis` argument.\r\n\r\nYou could either:\r\n\r\n1) Instantiate the class, then call it:\r\n\r\n```python\r\ny = keras.layers.Concatenate(axis=-1)([x1, x2])\r\n```\r\n\r\n2) Use its functional interface, which is equivalent\r\n\r\n```python\r\ny = keras.layers.concatenate([x1, x2], axis=-1)\r\n```\r\n\r\nI recommend 2)."]}, {"number": 11866, "title": "error in docs", "body": "The docs [here]( https://www.tensorflow.org/versions/r0.12/api_docs/python/image/working_with_bounding_boxes)  for bounding boxes  have an error :\r\nThe paragraph reading \r\n\r\n`For example, if an image is 100 x 200 pixels and the bounding box is [0.1, 0.2, 0.5, 0.9], the bottom-left and upper-right coordinates of the bounding box will be (10, 40) to (50, 180).`\r\n\r\nwould better go something like:\r\n\r\n`For example, if an image is 100 x 200 pixels and the bounding box is [0.1, 0.2, 0.5, 0.9], the upper-left and bottom-right coordinates of the bounding box will be (10, 40) to (50, 180), using a somewhat idiosyncratic (y,x) notation, or (40,10) to (180,50) using the rather more common (x,y) notation for points in a plane.  All this is with 'usual' coordinate axes (origin in the top left corner, increasing x to right and increasing y going down)`", "comments": ["Is it really _height x width_ ? Yes as said on https://www.tensorflow.org/versions/r0.12/api_docs/python/image/working_with_bounding_boxes\r\n\r\n> The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max].\r\n\r\nYou could create a PR for around line 700 in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L700\r\n\r\nI agree It is not intuitive but it's mentioned as swapped.\r\n\r\n---\r\nWouldn't it be better to mention __width__ and __height__ to trigger the users?\r\n\r\n> For example, if an image is 100 __height__ x 200 __width__ pixels\r\n\r\nor\r\n> For example, if an image is 100 x 200 (__height__  x __width__) pixels\r\n\r\n", "I tried doing a pr [11889](https://github.com/tensorflow/tensorflow/pull/11889) as you suggested, thanks. ", "see #11889 for fix.", "Hi, it should be closed if fixed?", "the pr was merged so I suppse you are right, will close if I can"]}, {"number": 11865, "title": "Batchnorm errors in \"successful\" Windows builds", "body": "* Current system configuration: \r\n* Windows 10 64 bit, intel i7-7700HQ latest microcode, Nvidia 1050 4GB.\r\n* Driver: 384.94\r\n* Python used: Anaconda 4.4.0 Python 3.6.2 and 3.5.3\r\n* CUDA/cuDNN: 8.0.61/5.1 or 8.0.61.2/6.0\r\n* swigwin 3.0.12\r\n* Built 1.2.1 from source using VS 2015 Update 3, CMake 3.9.0 or 3.9.0 RC5, swigwin 3.0.12.\r\n* Code modifications: in builds with both cuDNN and AVX enabled, the code was [modified accord to this comment](https://github.com/tensorflow/tensorflow/issues/11096#issuecomment-312049089)\r\n* Issue description: In certain conditions \"successful\" builds of tensorflow with GPU support, results in broken batchnorm functionality. An example error:\r\n > InvalidArgumentError (see above for traceback): indices[1] is out of range\r\n\t [[Node: gradients/batch_normalization/moments/Mean_1_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/Mean_1_grad/range, gradients/batch_normalization/moments/Mean_1_grad/mod, gradients/batch_normalization/moments/Mean_1_grad/Shape, gradients/batch_normalization/moments/Mean_1_grad/Fill)]]\r\n\r\nThis error was encountered in a variety of different builds. But it was most surprising when it occurred in an unmodified python 3.6 gpu build. Files and [configuration can be found here](https://github.com/aluo-x/tensorflow_windows).\r\n", "comments": ["A complete error print out:\r\n\r\n\r\n\r\n> C:\\Users\\ALuo\\Anaconda3\\python.exe C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\r\n> Extracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\r\n> Extracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\r\n> 2017-07-29 00:09:56.891124: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.891322: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.891533: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.891720: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.891906: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.892095: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.892276: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:56.892467: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-07-29 00:09:57.558636: I c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:940] Found device 0 with properties: \r\n> name: GeForce GTX 1050\r\n> major: 6 minor: 1 memoryClockRate (GHz) 1.493\r\n> pciBusID 0000:01:00.0\r\n> Total memory: 4.00GiB\r\n> Free memory: 3.30GiB\r\n> 2017-07-29 00:09:57.558862: I c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:961] DMA: 0 \r\n> 2017-07-29 00:09:57.559124: I c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   Y \r\n> 2017-07-29 00:09:57.559258: I c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\n> step 0, training accuracy 0.24\r\n> 2017-07-29 00:09:59.180245: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Invalid argument: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> 2017-07-29 00:09:59.180255: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Invalid argument: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> 2017-07-29 00:09:59.180265: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Invalid argument: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> 2017-07-29 00:09:59.180277: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Invalid argument: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> 2017-07-29 00:09:59.180283: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Invalid argument: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> 2017-07-29 00:10:00.245425: W c:\\optimae\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Invalid argument: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1139, in _do_call\r\n>     return fn(*args)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1121, in _run_fn\r\n>     status, run_metadata)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\contextlib.py\", line 88, in __exit__\r\n>     next(self.gen)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n>     pywrap_tensorflow.TF_GetCode(status))\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\", line 60, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\", line 54, in main\r\n>     train_step.run(feed_dict={x: batch[0], y_: batch[1], train_phase: True})\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1706, in run\r\n>     _run_using_default_session(self, feed_dict, self.graph, session)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3963, in _run_using_default_session\r\n>     session.run(operation, feed_dict)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 789, in run\r\n>     run_metadata_ptr)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 997, in _run\r\n>     feed_dict_string, options, run_metadata)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1132, in _do_run\r\n>     target_list, options, run_metadata)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> \r\n> Caused by op 'gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch', defined at:\r\n>   File \"C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\", line 60, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\", line 42, in main\r\n>     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 315, in minimize\r\n>     grad_loss=grad_loss)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 386, in compute_gradients\r\n>     colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 540, in gradients\r\n>     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 346, in _MaybeCompile\r\n>     return grad_fn()  # Exit early\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 540, in <lambda>\r\n>     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 94, in _MeanGrad\r\n>     sum_grad = _SumGrad(op, grad)[0]\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 56, in _SumGrad\r\n>     output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2272, in reduced_shape\r\n>     array_ops.fill(axes_shape, 1)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 481, in dynamic_stitch\r\n>     name=name)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n>     op_def=op_def)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n>     original_op=self._default_original_op, op_def=op_def)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n>     self._traceback = _extract_stack()\r\n> \r\n> ...which was originally created as op 'batch_normalization/moments/shifted_mean', defined at:\r\n>   File \"C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\", line 60, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n> [elided 0 identical lines from previous traceback]\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"C:/Users/ALuo/.PyCharm2017.2/config/scratches/scratch.py\", line 25, in main\r\n>     h_norm4 = tf.layers.batch_normalization(h_conv4, training=train_phase)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 441, in batch_normalization\r\n>     return layer.apply(inputs, training=training)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 492, in apply\r\n>     return self.__call__(inputs, *args, **kwargs)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 441, in __call__\r\n>     outputs = self.call(inputs, *args, **kwargs)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 287, in call\r\n>     mean, variance = nn.moments(inputs, reduction_axes)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 642, in moments\r\n>     math_ops.subtract(y, shift), axes, keep_dims=True, name=\"shifted_mean\")\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1355, in reduce_mean\r\n>     name=name)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1290, in _mean\r\n>     keep_dims=keep_dims, name=name)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n>     op_def=op_def)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n>     original_op=self._default_original_op, op_def=op_def)\r\n>   File \"C:\\Users\\ALuo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n>     self._traceback = _extract_stack()\r\n> \r\n> InvalidArgumentError (see above for traceback): indices[1] is out of range\r\n> \t [[Node: gradients/batch_normalization/moments/shifted_mean_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](gradients/batch_normalization/moments/shifted_mean_grad/range, gradients/batch_normalization/moments/shifted_mean_grad/mod, gradients/batch_normalization/moments/shifted_mean_grad/Shape, gradients/batch_normalization/moments/shifted_mean_grad/Fill)]]\r\n> \r\n> \r\n> Process finished with exit code 1\r\n> \r\n\r\n\r\nAn entry point might be comparing my failing python 3.6.2 build with no modifications with the official tensorflow build.\r\n\r\nRunning a diff -rqb:\r\n\r\n```\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/contrib/rnn/python/ops/_gru_ops.dll and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/contrib/rnn/python/ops/_gru_ops.dll differ\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/contrib/rnn/python/ops/_lstm_ops.dll and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/contrib/rnn/python/ops/_lstm_ops.dll differ\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/contrib/seq2seq/python/ops/_beam_search_ops.dll and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/contrib/seq2seq/python/ops/_beam_search_ops.dll differ\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/python/_pywrap_tensorflow_internal.pyd and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/python/_pywrap_tensorflow_internal.pyd differ\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/python/pywrap_tensorflow_internal.lib and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/python/pywrap_tensorflow_internal.lib differ\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/python/pywrap_tensorflow_internal.py and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.data/purelib/tensorflow/python/pywrap_tensorflow_internal.py differ\r\nFiles tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.dist-info/RECORD and official_tensorflow_gpu-1.2.1-cp36-cp36m-win_amd64/tensorflow_gpu-1.2.1.dist-info/RECORD differ\r\n```", "[Here is the code](https://github.com/aluo-x/tensorflow_windows/issues/1#issuecomment-312762361) that triggered the error in my builds but not in official whls. You can simply uncomment any of the batchnorm layers and connect them to the graph.", "With version r1.3, Under Windows 10, msvc 2015, i noticed a warning about macro TF_BATCHTOSPACE_BLOCK_DIMS_CASE called with the wrong number of parameters in tensorflow\\core\\kernels\\batchtospace_op.cc, l.197. Can it be related?\r\n", "This was never resolved on my end, since the error was not always deterministic, and failed at different layers. I will try and build 1.4 from source w/ cuDNN 7 on windows 10 this week, and report back.", "Closing this issue for now since it is stale. ", "Seems like this issue is still happening when compiling with AVX. [See here](https://github.com/aluo-x/tensorflow_windows/issues/6)", "I won't have time to look into this, so marking it as \"Contributions Welcome.\"", "This issue can be solved by update msvc compiler (cl.exe) from 19.0.24210.0 to 19.0.24215.1."]}, {"number": 11864, "title": "tf.nn.conv2d produces incorrect results", "body": "conv2d doesn't seem to produce results that are correct when compared with a C++ implementation and MATLAB's conv2. These errors are most pronounced when importing parameters from an external model. The same network in tf produces an accuracy of 60% while the original network has an accuracy of 97%. To verify that the problem indeed is in tf's conv2d, I used the following simple example:\r\n**MATLAB code**\r\n`k2=reshape(1:9,[3,3])'`\r\n`k2=repmat(k2,[1,1,3])`\r\n`x2=reshape(1:16,[4,4])'`\r\n`x2=repmat(x2,[1,1,3])`\r\n`c=conv2(x2(:,:,1),k2(:,:,1),'same') + conv2(x2(:,:,2),k2(:,:,2),'same')+ conv2(x2(:,:,3),k2(:,:,3),'same')`\r\n\r\n**output:**\r\n`          87         186         249         225`\r\n`         297         576         711         594`\r\n`         621        1116        1251         990`\r\n`         789        1338        1455        1095`\r\n\r\n**Python TF code**\r\n`import tensorflow as tf`\r\n`import numpy as np`\r\n\r\n`k=np.reshape(range(1,10),[3,3,1,1])`\r\n`k=np.repeat(k,3,2).astype('float16')`\r\n\r\n`x=np.reshape(range(1,17),[1,4,4,1])`\r\n`x=np.repeat(x,3,3).astype('float16')`\r\n\r\n`dev_X = tf.Variable(x,dtype=tf.float16)`\r\n`dev_K = tf.Variable(k,dtype=tf.float16)`\r\n\r\n`dev_C=tf.nn.conv2d(dev_X, dev_K, strides=[1, 1, 1, 1], padding='SAME')`\r\n\r\n`session=tf.Session(); `\r\n`session.run(tf.global_variables_initializer()); `\r\n`print(session.run(dev_C)[0,:,:,0])`\r\n\r\n**output:**\r\n`[[  333.   534.   651.   435.]`\r\n` [  693.  1044.  1179.   756.]`\r\n` [ 1089.  1584.  1719.  1080.]`\r\n` [  591.   822.   885.   525.]]`\r\n\r\n\r\n\r\n\r\nWhat exactly is different in tf's implementation of convolution? Is this a bug?", "comments": ["It's not a bug. Tensorflow's convolution is actually cross-correlation. This is mentioned in [the doc](https://www.tensorflow.org/api_guides/python/nn#Convolution).\r\n\r\nIf you add `k = k[::-1,::-1]` you'll see the same result.", "Okay, thanks. I don't understand what is the justification behind this though, especially when considering how standard it is to use convolution and not cross-correlation.", "In deep learning world the standard implementation is to use cross-correlation, I think."]}, {"number": 11863, "title": "AbortionError(code=StatusCode.NOT_FOUND, details=\"FeedInputs: unable to find feed output ToFloat:0\")", "body": "Hello\r\nI am trying to host the *ssd_mobilenet_v1_coco* model from the Tensorflow Object Detection API model zoo with Tensorflow Serving. \r\n\r\nI was able to successfully export the model with the exporter script in `models/object_detection/exporter.py` as a SavedModel. The issue arises when I tried to run the modified client\r\n\r\n```\r\nfrom __future__ import print_function\r\nfrom grpc.beta import implementations\r\nimport tensorflow as tf\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\n\r\n\r\ntf.app.flags.DEFINE_string('server', 'localhost:9000',\r\n                           'PredictionService host:port')\r\ntf.app.flags.DEFINE_string('image', '', 'path to image in JPEG format')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\ndef main(_):\r\n  host, port = FLAGS.server.split(':')\r\n  channel = implementations.insecure_channel(host, int(port))\r\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n  with open(FLAGS.image, 'rb') as f:\r\n    data = f.read()\r\n    request = predict_pb2.PredictRequest()\r\n    request.model_spec.name = 'mobilenet_v1'\r\n    request.model_spec.signature_name = 'serving_default'\r\n    request.inputs['inputs'].CopyFrom(\r\n        tf.contrib.util.make_tensor_proto(data, shape=[1]))\r\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\r\n    print(result)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nHere's the traceback for the gRPC request\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 56, in <module>\r\n    tf.app.run()\r\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/mcw-nn/Documents/SSP/serving/bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving/example/inception_client.py\", line 51, in main\r\n    result = stub.Predict(request, 10.0)  # 10 secs timeout\r\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 324, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/grpc/beta/_client_adaptations.py\", line 210, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.NOT_FOUND, details=\"FeedInputs: unable to find feed output ToFloat:0\")\r\n```\r\n\r\nI am using this [commit](https://github.com/tensorflow/tensorflow/tree/16d39e94e3724417fcaed87035434e098e892842) of tensorflow to build the model server which is the default submodule of tensorflow serving", "comments": ["I fixed it.\r\n I'm leaving the solution here in case anyone faces this issue in the future.\r\nTurns out I was trying to host with an older commit of tensorflow/models (which also happens to be the default submodule of tensorflow serving).\r\nWhen I switched to the master branch of tensorflow/models, the error went away!"]}, {"number": 11862, "title": "Branch 163534909", "body": "", "comments": []}, {"number": 11861, "title": "grpc changed to only build Release flavor", "body": "cmake builds are broken for Debug/RelWithDebInfo builds because grpc now only builds Release binaries. Always point to the Release bits to fix it.", "comments": ["Can one of the admins verify this patch?", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @dandelionmane, @jhseu and @tensorflower-gardener to be potential reviewers.", "@tensorflow-jenkins test this please\r\n\r\nThanks!", "I guess the change that broke it is:\r\nBUILD_COMMAND ${CMAKE_COMMAND} --build . --config **Release** --target grpc++_unsecure\r\nand we could use  $(Configuration) .\r\nThere is also -DCMAKE_BUILD_TYPE:STRING=Release passed to the grpc cmake but assume that is never looked at,, else this this would have been broken in past.\r\nEither way is fine with me - your pick.\r\n@mrry  - any preference ?\r\n\r\nA little upside for rpc Release builds only: for gpu builds there is an issue that the total size of the symbols info of the python_wrapper.dll is > 4GB which hits some limit in link.exe (it fails) ... less symbols is a good thing.\r\n", "perfect - Thanks!"]}, {"number": 11860, "title": "Fixed issue: Conv_ops python unit test fails", "body": "Fixed an issue where the Conv_ops python unit test fails for MKL.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11859, "title": "no such package '@local_config_cuda//cuda'", "body": "Please go to Stack Overflow for help and support:\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux \u2014 Ubuntu Server 16.04 LTS\r\n\r\nLinux PowerEdge-R810 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nmaster, r1.3, r1.2.1\r\n\r\n- **Python version**: \r\n\r\n2.7\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nBuild label: 0.5.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 28 08:34:59 2017 (1501230899)\r\nBuild timestamp: 1501230899\r\nBuild timestamp as int: 1501230899\r\n\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nCUDA 8, cuDNN 6\r\n\r\n- **GPU model and memory**:\r\n\r\nGTX 1080, 8gb\r\n\r\n- **Exact command to reproduce**:\r\n\r\n`./configure;bazel build -c opt \u2014config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n-**Premade script**:\r\n```\r\nalex@PowerEdge-R810:~$ sh tf_env_collect.sh \r\nCollecting system information...\r\ntf_env_collect.sh: 39: [: Linux: unexpected operator\r\ntf_env_collect.sh: 41: [: Linux: unexpected operator\r\nTraceback (most recent call last):\r\n  File \"/tmp/check_tf.py\", line 1, in <module>\r\n    import tensorflow as tf;\r\nImportError: No module named tensorflow\r\nWrote environment to tf_env.txt. You can review the contents of that file.\r\nand use it to populate the fields in the github issue template.\r\n\r\ncat tf_env.txt\r\n\r\nalex@PowerEdge-R810:~$ cat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nLinux PowerEdge-R810 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux PowerEdge-R810 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Jul 28 18:12:37 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.59                 Driver Version: 384.59                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:0E:00.0 Off |                  N/A |\r\n| 37%   32C    P0    32W / 180W |      0MiB /  8105MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n```\r\n\r\n### Describe the problem\r\n\r\nI am trying to compile tensorflow from source. When I run the above command, the bazel build fails. This is the configuration I used as well as the error. It seems like some other people had this issue but the threads were for much older versions and none of the solutions fixed it: \r\n\r\n\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]: y\r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]: \r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: y\r\nOpenCL support will be enabled for TensorFlow.\r\n\r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: \r\nPlease specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: \r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\"Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: \r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n...............\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/alex/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nINFO: Elapsed time: 13.663s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\n", "comments": ["@jrvb Can you take a look? This is similar to #9435, but the error message is a bit different.", "I have been having this same problem for the last day. I have installed the latest version of bazel (0.5.3). I previously got a build working, but then uninstalled it. When I went back to reinstall, I encountered this error. Two days ago I also was able to get this working on docker, but now it also does not work in docker.\r\n\r\n# Environment Data\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux learningOne 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux learningOne 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.11.0)\r\nprotobuf (3.3.0)\r\ntensorflow-tensorboard (1.3.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/david/torch/install/lib:/home/david/torch/install/lib::/usr/local/cuda/lib64\r\nDYLD_LIBRARY_PATH /home/david/torch/install/lib:/home/david/torch/install/lib:\r\n\r\n== nvidia-smi ===================================================\r\nFri Jul 28 16:46:56 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 381.22                 Driver Version: 381.22                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 0000:01:00.0      On |                  N/A |\r\n| 23%   39C    P8    19W / 250W |    886MiB / 11172MiB |     14%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1077    G   /usr/lib/xorg/Xorg                             485MiB |\r\n|    0      2144    G   compiz                                         239MiB |\r\n|    0      2445    G   /proc/self/exe                                 114MiB |\r\n|    0      9261    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    44MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n```\r\n\r\n# Bazel Build Version\r\n```\r\n\r\nBuild label: 0.5.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 28 08:34:59 2017 (1501230899)\r\nBuild timestamp: 1501230899\r\nBuild timestamp as int: 1501230899\r\n```\r\n\r\n# Error Message\r\n\r\n```\r\ndavid@learningOne:~/realtf$ bazel build --config=opt --config=cuda --explain=result.txt --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/david/realtf/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nINFO: Elapsed time: 0.561s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```", "I have the same setup and problem as @Helique.", "Rolling bazel back to 0.5.2 seems to fix the problem", "@wentaol How do you rollback to 5.2?", "`sudo apt-get purge bazel `\r\nfollowed by installation from 0.5.2 binaries \r\nhttps://docs.bazel.build/versions/master/install-ubuntu.html\r\n\r\n(not certain if it's the \"right way\", just how I got it to compile)", "@wentaol Thank you, i have downgraded to 0.5.2 and `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`seems to be running.", "Same problem and thanks @wentaol . Btw it is weird that bazel doesn't provide old versions in `apt-cache` once upgraded and rollback has to be done via binary installer.", "Yep bazel 0.5.2 fixed the problem for me as well, very strange", "I met the same problem. Rolling back to bazel 0.5.2 does not help >_<", "I had the \"depsets cannot contain mutable items\" error, rolling back from bazel 0.5.3 to 0.5.2 fixed it.\r\n\r\nIt's still not building properly (honestly what's wrong with all this bazel crap ?!?!) but that first error is gone.\r\n\r\nRegards, Adam.", "Hi guys, I just found that this issue had been fixed. We do not need to roll back bazel now! :)", "Closing since this issue is fixed.", "this problem still exist in bazel 5.3 and 5.4. If you build for the 1.2.1 branch. Going back to 5.2 fixed the problem.", "My machine updated Bazel to 0.6.0 today and now I have the problem. Reverting to 0.5.4 fixed the issue.\r\n\r\n/CC @gunan can you comment on why updating Bazel seems to temporarily break TensorFlow and what is done to fix the issue when it breaks?", "I have no context on the issue. Could you provide full repro instructions?\r\nYou may want to use a clean docker container to reproduce the problem, then I can work on that to see what is going on.", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master, commit 4270c4a00bb90d97418df5b0e9b6b3e148a72a1e\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.6.0\r\n- **CUDA/cuDNN version**: Cuda 8, cuDNN 6\r\n- **GPU model and memory**: GTX 1080 8 GB\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nbazel build -c opt \u2014config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nWhen I run the above command with Bazel 0.6.0 after running `./configure` with GPU support, I get the error\r\n```\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 1042\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 975, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\nThe `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 1042\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 975, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/usr/local/google/home/reedwm/tensorflows/tensorflow2/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\nThe `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false\r\n```\r\nThe issue does not occur with Bazel 0.5.4", "@vladmos Any ideas?", "It's a planned incompatible change in Bazel. `set` is deprecated and was removed by default from Bazel starting with 0.6. You can use the flag from the error message to temporarily use `set` in .bzl files, but the flag will be removed at some point too (probably at Bazel 0.8), so it's recommended to fix the .bzl files.", "The initial issue has nothing to do with that, the `set` deprecation starts with Bazel 0.6 which was released today.\r\n\r\nWe have Tensorflow on our CI and we tried to clean up everything before the release but it looks like we don't run some targets that reproduce the issue. //cc @damienmg ", "We do not run all the config of TensorFlow, that's why we do ask for a full run of TF CI with Bazel RC. Seems like even that have missed them", "I solved the issue by editing line 120 in tensorflow/third_party/gpus/cuda_configure.bzl \r\nI changed set to depset as recommended in error output. ", "@vladmos sent a Pr to fix this in master. Closing this issue.\r\nPlease let us know if you still see issues in master.\r\nFor the release branch, using bazel versions older than 0.6.0 is recommended.", "I was just having the same and just wanted to comment and confirm that updating to `master` immediately fixed it with no other changes needed :+1: ", "I am facing this issue with bazel 0.11.1 on r1.7\r\n\r\n```\r\ntf-latest.img:~/tensorflow> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n............\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1001, in _create_local_cuda_repository\r\n\t\t_find_nvvm_libdevice_dir(repository_ctx, cuda_config)\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 724, in _find_nvvm_libdevice_dir\r\n\t\tauto_configure_fail((\"Cannot find libdevice.10.bc un...))\r\n\tFile \"/root/tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n```", "@jerrin92 can you file a new issue? I think you are encountering a different error since you have the error message \"Cuda Configuration Error\".", "@jerrin92 @gunan Any idea about https://github.com/tensorflow/tensorflow/issues/11859#issuecomment-379799380 error?", "Can anyone help with this. The issue seems to be similar but on different machine https://github.com/tensorflow/tensorflow/issues/22400", "Please avoid commenting on old issues.\r\nYou can file a new issue with all the information from your system.\r\nFeel free to reference old issues in the new ones you files.", "> Rolling bazel back to 0.5.2 seems to fix the problem\r\nsure? 0.5.2 has released\r\n", "> Hi guys, I just found that this issue had been fixed. We do not need to roll back bazel now! :)\r\n\r\nplease send solution", "fixed it creating new environment variable : 'BAZEL_SH' value: 'C:\\msys64\\usr\\bin\\bash.exe'", "Done\n\nOn Thu 21 Mar, 2019, 3:30 AM LenaGer, <notifications@github.com> wrote:\n\n> fixed it creating new environment variable : 'BAZEL_SH' value:\n> 'C:\\msys64\\usr\\bin\\bash.exe'\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11859#issuecomment-475044408>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AkNdC0cKNdlsjShcSCmUmZUjRYHgO7dOks5vYq-LgaJpZM4OnI4I>\n> .\n>\n", "@gunan I have faced with the same error during the building tensorflow v2.3.0 ...\r\n```console\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n        _create_local_cuda_repository(<1 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n        _get_cuda_config(repository_ctx, <1 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n        find_cuda_config(repository_ctx, <2 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n        _exec_find_cuda_config(<3 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n        execute(repository_ctx, <1 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n        fail(<1 more arguments>)\r\nRepository command failed\r\nscript.py:124: DeprecationWarning: invalid escape sequence \\d\r\n  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\nscript.py:260: DeprecationWarning: invalid escape sequence \\d\r\n  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\nscript.py:553: DeprecationWarning: invalid escape sequence \\w\r\n  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-10.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/include/cudnn.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n        _create_local_cuda_repository(<1 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n        _get_cuda_config(repository_ctx, <1 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n        find_cuda_config(repository_ctx, <2 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n        _exec_find_cuda_config(<3 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n        execute(repository_ctx, <1 more arguments>)\r\n    File \"/home/{User}/Projects/AI/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n        fail(<1 more arguments>)\r\nRepository command failed\r\nscript.py:124: DeprecationWarning: invalid escape sequence \\d\r\n  match = re.match(\"#define %s +(\\d+)\" % name, line)\r\nscript.py:260: DeprecationWarning: invalid escape sequence \\d\r\n  pattern = \"Cuda compilation tools, release \\d+\\.\\d+, V(\\d+\\.\\d+\\.\\d+)\"\r\nscript.py:553: DeprecationWarning: invalid escape sequence \\w\r\n  match = re.match(\"^(/[^/ ]*)+/lib/\\w+-linux-gnu/?$\", os.environ[env_name])\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda-10.2/include/cuda.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/cuda/include/cublas_api.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/include/cudnn.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nscript.py:123: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/include/cudnn_version.h' mode='r' encoding='utf-8'>\r\n  for line in io.open(path, \"r\", encoding=\"utf-8\").readlines():\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\nINFO: Elapsed time: 8.508s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```"]}, {"number": 11858, "title": "Bug in builds/pip.sh causes failure if NO_TEST_ON_INSTALL=1", "body": "do_virtualenv_pip_test() uses ${CLEAN_VENV_DIR} which is defined in\r\ndo_clean_virtualenv_smoke_test(). But if NO_TEST_ON_INSTALL is set to 1\r\n${CLEAN_VENV_DIR} would never be defined, and pip.sh will fail.\r\n\r\nIt is very likely that we actually meant to use ${VENV_DIR} instead.", "comments": ["@tensorflow-jenkins test this please", "Thanks for the fix, @sumitgouthaman ", "@tensorflow-jenkins test this please"]}, {"number": 11857, "title": "Testing PRs.", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 11856, "title": "tf.variables_initializer seems broken.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: The nightly wheel.\r\n- **TensorFlow version (use command below)**: The nightly build that went out at 2AM on 7/28/2017: https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 1080\r\n- **Exact command to reproduce**:\r\n\r\nTo reproduce, run this snippet in the python interpreter:\r\n```\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n  v = tf.Variable(42, dtype=tf.int32)\r\n  with tf.control_dependencies([tf.variables_initializer([v])]):\r\n    result = tf.assign(v, v + 1)\r\n  print(sess.run(result))\r\n```\r\n\r\n### Describe the problem\r\nRunning the snippet above yields an exception:\r\n\r\n```\r\nCaused by op u'Variable/read', defined at:\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\r\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1408, in identity\r\n    _result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 328, in apply_op\r\n    op_type_name, name, **keywords)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2619, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1205, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\r\n\t [[Node: Variable/read = Identity[T=DT_INT32, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]\r\n```\r\n\r\nIt seems like `tf.variables_initializer` is broken - it does not initialize the variables passed to it.\r\n\r\nThis has caused a TensorBoard test (:summary_test) to fail today (TensorBoard runs tests on nightly TensorFlow).\r\nhttps://travis-ci.org/tensorflow/tensorboard/jobs/258655621\r\nThe test had been passing yesterday (on nightly built on 7/27).\r\n", "comments": ["It seems like indenting `print(sess.run(result))` one to the right does away with the exception. Like this:  \r\n```\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n  v = tf.Variable(42, dtype=tf.int32)\r\n  with tf.control_dependencies([tf.variables_initializer([v])]):\r\n    result = tf.assign(v, v + 1)\r\n    print(sess.run(result))\r\n```\r\nHowever, it doesn't seem like that was necessary yesterday. And it would be nice if that were not necessary (to save one level of indentation for most of the function).", "Actually ... indeed, that won't work because mostly likely, I'm not running the session within a control dependency block.", "This also fails on 1.2.1 version of TensorFlow. The reason is that \"v+1\" is relying on `Variable/read` node which is created outside of `tf.control_dependencies` block (created during tf.Variable construction in  [this line](https://github.com/tensorflow/tensorflow/blob/faf7c32ab27dad24d2d806a16d1371ecb4671fc8/tensorflow/python/ops/variables.py#L330)), and therefore doesn't have proper control dependency. The following makes it pass. cc @alextp \r\n\r\n    result = tf.assign(v, v.read_value() + 1)\r\n", "Also swapping the variables with resource variables (using\nget_variable(..., use_resource=True) to construct the variables) makes this\nissue go away.\n\nOn Fri, Jul 28, 2017 at 12:11 PM, Yaroslav Bulatov <notifications@github.com\n> wrote:\n\n> This also fails on 1.2.1 version of TensorFlow. Theory -- \"v+1\" is relying\n> on Variable/read node which is created outside of tf.control_dependencies\n> block, and therefore doesn't have proper control dependency. The following\n> makes it pass. cc @alextp <https://github.com/alextp>\n>\n> result = tf.assign(v, v.read_value() + 1)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11856#issuecomment-318738286>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTXVWhHcR4WVrlkj0HlefurKvgpMks5sSjJhgaJpZM4Om9Cy>\n> .\n>\n\n\n\n-- \n - Alex\n", "I think I still want a `tf.Variable` instead of a resource one because the test uses a `ScatterAdd` op here: https://github.com/tensorflow/tensorboard/blob/02f97383951f57dacb462fb6335f461f32ac7141/tensorboard/plugins/pr_curve/summary.py#L162\r\n\r\nAnd the scatter add seems to err when passed resource variables:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/home/agent007/.cache/bazel/_bazel_agent007/e5cf4ac2d3d80f539d85c4d9419a6bd7/bazel-sandbox/68\r\n677344569147791/execroot/org_tensorflow_tensorboard/bazel-out/local-fastbuild/bin/tensorboard/plugins/pr_curve/summa\r\nry_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/summary_test.py\", line 48, in test1Class\r\n    num_thresholds=10)\r\n  File \"/usr/home/agent007/.cache/bazel/_bazel_agent007/e5cf4ac2d3d80f539d85c4d9419a6bd7/bazel-sandbox/68\r\n677344569147791/execroot/org_tensorflow_tensorboard/bazel-out/local-fastbuild/bin/tensorboard/plugins/pr_curve/summa\r\nry_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/summary.py\", line 165, in op\r\n    tp_buckets_v, bucket_indices, true_labels, use_locking=True)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\"\r\n, line 212, in scatter_add\r\n    name=name)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_libr\r\nary.py\", line 328, in apply_op\r\n    op_type_name, name, **keywords)\r\n  File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_libr\r\nary.py\", line 641, in _apply_op_helper\r\n    \"(e.g.: a tf.Variable)\") % (op_type_name, input_name))\r\nTypeError: 'ScatterAdd' Op requires that input 'ref' be a mutable tensor (e.g.: a tf.Variable)\r\n```\r\n\r\nIs there a way to include that `Variable/read` op in the control dependency list? Thank you!", "There's a resource_scatter_add op which can be used instead.\n\nOn Fri, Jul 28, 2017 at 1:11 PM, Chi Zeng <notifications@github.com> wrote:\n\n> I think I still want to use a tf.Variable instead of a resource one\n> because the test uses a ScatterAdd op here: https://github.com/tensorflow/\n> tensorboard/blob/02f97383951f57dacb462fb6335f461f32ac7141/tensorboard/\n> plugins/pr_curve/summary.py#L162\n>\n> And the scatter add seems to err when passed resource variables:\n>\n> Traceback (most recent call last):\n>   File \"/usr/home/agent007/.cache/bazel/_bazel_agent007/e5cf4ac2d3d80f539d85c4d9419a6bd7/bazel-sandbox/68\n> 677344569147791/execroot/org_tensorflow_tensorboard/bazel-out/local-fastbuild/bin/tensorboard/plugins/pr_curve/summa\n> ry_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/summary_test.py\", line 48, in test1Class\n>     num_thresholds=10)\n>   File \"/usr/home/agent007/.cache/bazel/_bazel_agent007/e5cf4ac2d3d80f539d85c4d9419a6bd7/bazel-sandbox/68\n> 677344569147791/execroot/org_tensorflow_tensorboard/bazel-out/local-fastbuild/bin/tensorboard/plugins/pr_curve/summa\n> ry_test.runfiles/org_tensorflow_tensorboard/tensorboard/plugins/pr_curve/summary.py\", line 165, in op\n>     tp_buckets_v, bucket_indices, true_labels, use_locking=True)\n>   File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\"\n> , line 212, in scatter_add\n>     name=name)\n>   File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_libr\n> ary.py\", line 328, in apply_op\n>     op_type_name, name, **keywords)\n>   File \"/usr/home/agent007/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_libr\n> ary.py\", line 641, in _apply_op_helper\n>     \"(e.g.: a tf.Variable)\") % (op_type_name, input_name))\n> TypeError: 'ScatterAdd' Op requires that input 'ref' be a mutable tensor (e.g.: a tf.Variable)\n>\n> Is there a way to include that Variable/read op in the control dependency\n> list? Thank you!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11856#issuecomment-318750405>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxaWsTbZ1nJ27MWGHaXpFmphdCPM8ks5sSkB-gaJpZM4Om9Cy>\n> .\n>\n\n\n\n-- \n - Alex\n", "It seems like `resource_scatter_add` is only supported internally for now? I don't see it in the `tf.` namespace, and the test is open-source.", "I want to spend some time to fix this and move the read node into the dependency chain. Could someone please give me some guidance?\r\n\r\nMy alternative would be to rewrite a project to avoid scatter_add. That is a relatively big task. Furthermore, after a variable is \"initialized\", it seems reasonable for a user to assume that they can read from and update the variable.", "@chihuahua replacing the line with `tf.assign(v, v.read_value() + 1)` will refer to the proper read node.\r\n\r\nThe issue is that the tensor is retrieved from `_snapshot` in [AsTensor](https://github.com/tensorflow/tensorflow/blob/faf7c32ab27dad24d2d806a16d1371ecb4671fc8/tensorflow/python/ops/variables.py#L367) which is initialized during variable construction. For a more sweeping fix one might modify `AsTensor` to check whether you are inside any active context managers for control dependencies, and if so, create a new read node `read_value` rather than referring to the snapshot. That might get complicated (ie, what if someone is inside control dependency context referring to variable in a long loop? That explodes the graph)", "I think it should be safe to always call read_value when you reference the\nvariable. Indeed I do this in resource variables which do not have this bug\nand for which a scatter_add operation exists (resource_scatter_add). I do\nnot understand what's the remaining issue here.\n\nOn Sun, Jul 30, 2017 at 9:12 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @chihuahua <https://github.com/chihuahua> replacing the line with tf.assign(v,\n> v.read_value() + 1) will refer to the proper read node.\n>\n> The issue is that the tensor is retrieved from _snapshot in AsTensor\n> <https://github.com/tensorflow/tensorflow/blob/faf7c32ab27dad24d2d806a16d1371ecb4671fc8/tensorflow/python/ops/variables.py#L367>\n> which is initialized during variable construction. One might modify\n> AsTensor to check whether you are inside any active context managers for\n> control dependencies, and if so, create a new read node read_value rather\n> than referring to the snapshot. That might get complicated (ie, what if\n> someone is inside control dependency context referring to variable in a\n> long loop? That explodes the graph)\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11856#issuecomment-318911787>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxf0HNbhFj4pMoZpNRxDLTJTDa_Kzks5sTKtjgaJpZM4Om9Cy>\n> .\n>\n\n\n\n-- \n - Alex\n", "Does calling `read_value` append to the graph/invalidate graph cache?", "Yes\n\nOn Mon, Jul 31, 2017 at 9:17 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> Does calling read_value append to the graph/invalidate graph cache?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11856#issuecomment-319118922>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRDSm3qNOfY-VGyhQLf5WjUQbiYYks5sTf4ygaJpZM4Om9Cy>\n> .\n>\n\n\n\n-- \n - Alex\n", "Is this issue solved / irrelevant?", "@chihuahua IMHO this is a counter-intuitive property of graph-based system rather than a bug --\r\n you always need to create a new graph node if you want constraints enforced by surrounding context managers to be enforced.\r\n\r\nIn your case,`v` reuses previously created graph node which doesn't know about your desired execution order, whereas  `v.read_value()` will create a new node which respects the control dependency", "SGTM. Thank you!\r\n\r\nI think it might be worth clarifying this in the `tf.variable_initializer` docs. Otherwise, it might be a bit unclear to some folks to use `v.read_value`.\r\n\r\nAnd indeed, I have worked around this issue by avoiding `scatter_add` entirely."]}, {"number": 11855, "title": "Branch 163490703", "body": "", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please, wakey wakey", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 11854, "title": "Android demo: Explicitly import R.java", "body": "Add explicit import of the R class to fix Google-internal Android demo builds.", "comments": ["For context, this is necessary for internal Google builds as it makes it easier for us to change the application package via a find/replace operation.", "@tensorflow-jenkins Test this, please.", "@tensorflow-jenkins test this please"]}, {"number": 11853, "title": "Add option for build more python tests in Cmake", "body": "* Add option tensorflow_BUILD_MORE_PYTHON_TESTS for testing some major packages under contrib directory\r\n* Tested, fixed and enabled some UTs on Windows/Linux ", "comments": ["Can one of the admins verify this patch?", "@mrry could you take a look at this, please?", "Thanks @rmlarsen @mrry ", "@tensorflow-jenkins test this please", "Hi @mrry \r\nAs I check the jennkins i found failure message as below:\r\n\r\n```\r\n//tensorflow/contrib/keras:core_test                                     FAILED in 6.9s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/contrib/keras/core_test/test.log\r\n\r\nExecuted 1182 out of 1182 tests: 1181 tests pass and 1 fails locally.\r\n\r\nParameterized build ends with FAILURE at: Wed Aug  9 18:35:12 UTC 2017 (Elapsed time: 2116 s)\r\nBuild step 'Execute shell' marked build as failure\r\n[Set GitHub commit status (universal)] ERROR on repos [] (sha:19cad03) with context:tensorflow-pull-requests-cpu\r\nUnable to get pull request builder trigger!!\r\nSetting status of 49e83b063f14167577fdd440950ecba22db8a36a to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6245/ and message: 'FAILURE\r\n '\r\nUsing context: Linux CPU Tests\r\nFinished: FAILURE\r\n```\r\n\r\nNot sure what happened.. and cannot re-produce on local/own jennkins bazel build.. would you help me look into the log or help me getting the detailed info?", "@tensorflow-jenkins test this please\r\n@raymondxyang the failure you saw was most likely a tool failure unrelated to your PR.", "Hi @rmlarsen I've added a missing module related to the failure. May I have it re-tested by jenkins?\r\nThanks again for following up the request J", "@tensorflow-jenkins test this please.", "The test `tensorflow/python/training/server_lib_test.py `  started but never finished.. killed for over time limit. touched nothing related to it and it passed in the previous Windows run both locally and on the jenkins.. Can jenkins re-test it? Thanks J", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thanks for the contribution!"]}, {"number": 11852, "title": "fixed tf.pow edge case", "body": "The tf.pow() function has an edge case which causes it to hang with no error message.\r\n\r\nIf you try to evaluate tf.pow(x,y), when x is an integer (and thus the output tensor is also an integer), while y is a negative value, tensorflow hangs trying to cast the fraction as an integer.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "We also need the CLA signed if we are to accept this PR, thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I have already signed the CLA.", "Looks like you used two different commit emails -- can you use the one you used for the CLA and squash your commits ?", "CLAs look good, thanks!\n\n<!-- ok -->", "@galvk Thanks for the contribution, but I don't think introducing this level of complexity in the Python wrapper is the right solution. We should fix the underlying C++ functor. I'd recommend opening an issue instead.", "Closing this out due to @rmlarsen 's comment. Feel free to reopen with a fix in C++ or file an issue instead. Thanks!"]}, {"number": 11851, "title": "'No module named tensorflow'  error in jupyter notebook after installing tensorflow-gpu using pip in the same virtual environment. I am using python 2.7.12.", "body": " I am trying to use tensorflow-gpu on Ubuntu16.04. I installed tensorflow via pip inside virtual environment, and other required libraries. When I start jupyter notebook in the same environment and try to run a code that uses tensorflow, the line 'import tensorflow as tf' gives an error. \r\nIt says no module named tensorflow. I had a similar problem with skimage(I initially installed scikit-image instead of python-skimage and I was getting an error. But it got resolved after I installed the right library. However I can't make sense of why it can't find the tensorflow module. ", "comments": ["I also tried \"import tensorflow-gpu as tf\" but it gives an \"invalid syntax\" error. \r\nin the terminal, inside the virtual environment for tensorflow, \"pip show tensorflow\" yields nothing. However \"pip show tensorflow-gpu\" shows the version information below:\r\nName: tensorflow-gpu\r\nVersion: 1.2.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: ~/venv4Tensorflow/lib/python2.7/site-packages\r\nRequires: mock, numpy, bleach, markdown, wheel, six, protobuf, backports.weakref, html5lib, werkzeug\r\n", "$ pip freeze\r\nadium-theme-ubuntu==0.3.4\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.weakref==1.0rc1\r\nbleach==1.5.0\r\nconfigparser==3.5.0\r\ncycler==0.10.0\r\nCython==0.23.4\r\ndecorator==4.0.6\r\nentrypoints==0.2.2\r\nenum34==1.1.6\r\nextractframes==0.16\r\nfuncsigs==1.0.2\r\nfunctools32==3.2.3.post2\r\nh5py==2.7.0\r\nhtml5lib==0.9999999\r\nimageio==2.2.0\r\nipykernel==4.6.1\r\nipython==5.4.1\r\nipython-genutils==0.2.0\r\nipywidgets==6.0.0\r\nJinja2==2.8\r\njsonschema==2.6.0\r\njupyter==1.0.0\r\njupyter-client==5.0.1\r\njupyter-console==5.1.0\r\njupyter-core==4.3.0\r\nMarkdown==2.6.8\r\nMarkupSafe==0.23\r\nmatplotlib==2.0.2\r\nmistune==0.7.4\r\nmock==2.0.0\r\nnbconvert==5.2.1\r\nnbformat==4.3.0\r\nnetworkx==1.11\r\nnose==1.3.7\r\nnotebook==5.0.0\r\nnumpy==1.13.1\r\nolefile==0.44\r\nopencv==1.0.1\r\nopencv-python==3.2.0.7\r\npandocfilters==1.4.1\r\npathlib2==2.2.1\r\npbr==3.1.1\r\npexpect==4.0.1\r\npickleshare==0.7.4\r\nPillow==4.1.1\r\npkg-resources==0.0.0\r\nprogressbar==2.3\r\nprompt-toolkit==1.0.14\r\nprotobuf==3.3.0\r\nptyprocess==0.5\r\npycurl==7.43.0\r\nPygments==2.2.0\r\nPyMySQL==0.7.2\r\npyparsing==2.2.0\r\nPyste==0.9.10\r\npython-apt==1.1.0b1\r\npython-dateutil==2.6.0\r\npytz==2017.2\r\nPyWavelets==0.5.2\r\nPyYAML==3.12\r\npyzmq==15.2.0\r\nqtconsole==4.3.0\r\nscandir==1.5\r\nscikit-image==0.13.0\r\nscikit-learn==0.18.1\r\nscikit-video==0.1.2\r\nscipy==0.19.0\r\nsimplegeneric==0.8.1\r\nsix==1.10.0\r\nsklearn==0.0\r\nsubprocess32==3.2.7\r\ntensorflow-gpu==1.2.1\r\nterminado==0.6\r\ntestpath==0.3.1\r\ntorch==0.1.12.post2\r\ntorchvision==0.1.8\r\ntornado==4.2.1\r\ntraitlets==4.3.2\r\nunity-lens-photos==1.0\r\nvirtualenv==15.0.1\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nWerkzeug==0.12.2\r\nwidgetsnbextension==2.0.0\r\n", "I think I have a hint about the source of the problem:\r\n\r\n~$ which python\r\n~/venv4Tensorflow/bin/python\r\n~$ which jupyter\r\n/usr/local/bin/jupyter\r\n~$ which ipython\r\n/usr/local/bin/ipython\r\n\r\n", "And here's the solution:\r\nhttp://help.pythonanywhere.com/pages/IPythonNotebookVirtualenvs\r\nHope this helps anyone else who gets stuck with this issue.", "Thank you for posting the solution once you resolved the issue.", "thank you so much! it works well!", "> And here's the solution:\r\n> http://help.pythonanywhere.com/pages/IPythonNotebookVirtualenvs\r\n> Hope this helps anyone else who gets stuck with this issue.\r\n\r\n@arthitag : This solution didn't work for me, I have tensorflow installed in a venv virtualenv and my jupyter-notebook is installed in miniconda global install "]}, {"number": 11850, "title": "Fixed two typos", "body": "Fixed two typos.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 11849, "title": "[XLA] RNG root instruction can be fused as it effectively has one user", "body": "The existing test checks for a single user when trying to fuse a random number generator instruction.   This is because if it is fused but has multiple users, the value may be generated an incorrect number of times.  (multiple users should receive the same random number, not different ones).\r\n\r\nThe test checks for users() length == 1.  This ignores the case there the instruction is the root, where users() length == 0, but there is effectively one user.", "comments": ["Can one of the admins verify this patch?", "no problem.  test forthcoming.", "@blakehechtman done.\r\n\r\ni tried adding a death test for the 2+ case, but the testing framework complains that death tests are unstable with multiple threads. ", "cheers :)", "@vrv @blakehechtman \r\n\r\nI think you need to do the 'test this' command to get the tests to run.\r\n", "@tensorflow-jenkins test this please", "Thanks.  The failure in the windows build is this:\r\n\r\n```\r\n10:23:57 error: cannot lock ref 'refs/remotes/origin/pr/5992/head': Unable to create 'c:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/.git/refs/remotes/origin/pr/5992/head.lock': File exists.\r\n```\r\n\r\nThink it is a different problem - maybe even a CI system issue.", "Yup, seems like a flaky failure that can be ignored!  This is ready to merge (I'm not on merge rotation this week so I don't want to screw anything up)", ":)  sure thing", "@blakehechtman @vrv \r\n\r\nHi gues.  Do you think we could get this merged?  I have quite a few outstanding merge requests and it is making me nervous that our repo is drifting away from the public one.\r\n", "ping @rmlarsen ", "@tensorflow-jenkins test this please"]}]