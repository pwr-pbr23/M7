[{"number": 36900, "title": "when using mixed_precision.Policy('mixed_float16'), training is stuck at saving checkpoints for 0 ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Debian GNU/Linux 9 (stretch) \r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.5.3\r\n- Bazel\r\nversion (if compiling from source): \r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen using tpu estimator in cpu mode and setting the precision policy to 'mixed_float16' training hangs at saving checkpoints for 0\r\n**Describe the expected behavior**\r\nContinue training using mixed precision, with default policy training is successfull\r\n\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\nUse an example tpu_estimator from one of the tensorflow examples and set precision policy:\r\n```\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nwith TF_CPP_MIN_VLOG_LEVEL=2:\r\nthis message is getting repeated periodically:\r\n2020-02-19 13:54:50.181246: I tensorflow/core/framework/model.cc:892] Starting optimization of tunable parameters with HillClimb\r\n2020-02-19 13:54:50.181532: I tensorflow/core/framework/model.cc:943] Number of tunable parameters: 1\r\n2020-02-19 13:54:50.181634: I tensorflow/core/framework/model.cc:946] Setting tunable parameter MapAndBatch(id:1) to 4\r\n2020-02-19 13:54:50.181729: I tensorflow/core/kernels/data/model_dataset_op.cc:192] Waiting for 60000 ms.\r\n\r\n\r\n", "comments": ["@petkovacs19 \r\n\r\nCould you please provide us colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Thanks for the quick response! \r\nreplication in colab: https://colab.research.google.com/drive/1UGx65koDCgR_h8doLYsbHnYMG4cdEyRf", "I don't know what the `df_tpu_estimator.py` file is. Can you post a self-contained colab or example to reproduce?\r\n\r\nAlso note, the `'mixed_float16'` policy only will have a performance improvement on GPUs, and otherwise there is no reason to use it. However, the policy should still work on CPUs, it will just be slow. Similarly, the `'mixed_bfloat16'` policy only has a performance improvement on TPUs.", "@petkovacs19 \r\n\r\nAny update on this issue please. Thanks!", "@reedwm Thanks for the clarification on the use of policies. The plan here was to do a simple few step test run on CPUs before running full training on GPUs\r\n\r\nThe issue is gone when getting rid of main method and running directly in notebook. ", "@petkovacs19 \r\n\r\nPlease close this thread if your issue was resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36899, "title": "Autograd cannot handle if conditions with line brakes when accessing fields", "body": "### System information\r\n- custom code:\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from binary:\r\n-  tensorflow-gpu 2.1.0:\r\n-  3.6.8:\r\n- cuda 10.0:\r\n- Nvidia Geforce 1080ti 11Gb:\r\n\r\n\r\n### Describe the problem\r\nBUG: Autograd cannot handle if conditions with line brakes when accessing fields.\r\n\r\n### Source code / logs\r\nThis works:\r\n\r\nimport tensorflow as tf\r\nclass Test:\r\n    a = tf.Variable(1.0)\r\n\r\n    @tf.function\r\n    def test_function(self):\r\n        if False or self.a < 1.0:\r\n            pass\r\nt = Test()\r\nt.test_function()\r\n\r\nBut this does not:\r\n\r\nimport tensorflow as tf\r\nclass Test:\r\n    a = tf.Variable(1.0)\r\n\r\n    @tf.function\r\n    def test_function(self):\r\n        if False or\\\r\n                self.a < 1.0:\r\n            pass\r\nt = Test()\r\nt.test_function()\r\n\r\nError Message: \r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\n\r\n\r\n", "comments": ["@MaximusMutschler I have tried to replicate the code shared by you in our local environment and the error faced is not the same ,please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/8ac933dbbc61451341ec65164107318c/36899.ipynb) attached.\r\nplease share complete code for us to replicate your issue.", "@Saduf2019  Just changed the code so that it reproduces the error: see [my_gist](https://colab.research.google.com/gist/MaximusMutschler/9d41805c5bd7cd91a974871e42e08f65/36899.ipynb)\r\nInterestingly the error does not occur when using jupyter notebooks.\r\nBut if the code is run directly on the system i can recreate the error in google colab\r\nIt works with tf2.0 but fails with tf2.1\r\n", "@MaximusMutschler \r\nPlease, go through the below [link](https://github.com/tensorflow/tensorflow/issues/32383) and see if it helps you.Thanks!\r\n#32319\r\n", "@Saduf2019  thanks!! Unfortunately, the linked thread does not help, they solved their problem by installing gast==0.2.2, which is already installed in my case.\r\ntf 2.0 also uses gast==0.2.2 and it works there, thus gast might not be the problem", "@MaximusMutschler This was resolved in recent `tf-nightly`. Please check a [similar issue](https://github.com/tensorflow/tensorflow/issues/35765#event-2963029936) that initiated the resolution.  I ran your code with `tf-nightly` and I cannot reproduce the issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/45dabcc649839fc9d8fbd5a1c87b1a50/36899.ipynb).\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n\r\n", "@jvishnuvardhan  It works. Thanks a lot!"]}, {"number": 36898, "title": "Improve performance of NewTensor and Value for string tensors", "body": "- add benchmark to highlight and track the issue\r\n- follows on from a similar improvement for non-string tensors https://github.com/tensorflow/tensorflow/pull/36578\r\n- avoids allocations forced by using encoding/binary\r\n- avoids CGO calls encoding and decoding TF strings. Do this in Go instead.\r\n- avoid allocations for individual slices\r\nPerformance improvements are significant\r\n\r\n```name                          old time/op    new time/op    delta\r\nTensor/New/[]string-16           455ms \u00b1 2%      33ms \u00b1 2%   -92.74%  (p=0.001 n=7+7)\r\nTensor/New/[][]string-16         461ms \u00b1 2%      35ms \u00b1 5%   -92.51%  (p=0.000 n=8+8)\r\nTensor/New/[][][]string-16       461ms \u00b1 1%      36ms \u00b1 1%   -92.15%  (p=0.000 n=8+7)\r\nTensor/Value/[]string-16         289ms \u00b1 1%      21ms \u00b1 2%   -92.77%  (p=0.000 n=7+8)\r\nTensor/Value/[][]string-16       292ms \u00b1 1%      21ms \u00b1 2%   -92.89%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]string-16     295ms \u00b1 4%      21ms \u00b1 1%   -92.92%  (p=0.001 n=8+6)\r\n\r\nname                          old alloc/op   new alloc/op   delta\r\nTensor/New/[]string-16          48.0MB \u00b1 0%     0.0MB \u00b1 0%  -100.00%  (p=0.002 n=7+8)\r\nTensor/New/[][]string-16        48.3MB \u00b1 0%     0.0MB \u00b1 0%  -100.00%  (p=0.000 n=7+8)\r\nTensor/New/[][][]string-16      48.3MB \u00b1 0%     0.0MB \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[]string-16        72.0MB \u00b1 0%    23.0MB \u00b1 0%   -68.04%  (p=0.001 n=6+7)\r\nTensor/Value/[][]string-16      74.5MB \u00b1 0%    23.3MB \u00b1 0%   -68.78%  (p=0.001 n=7+7)\r\nTensor/Value/[][][]string-16    74.5MB \u00b1 0%    23.3MB \u00b1 0%   -68.78%  (p=0.001 n=7+7)\r\n\r\nname                          old allocs/op  new allocs/op  delta\r\nTensor/New/[]string-16           4.00M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/New/[][]string-16         4.01M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.002 n=7+8)\r\nTensor/New/[][][]string-16       4.01M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[]string-16         6.00M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[][]string-16       6.02M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)\r\nTensor/Value/[][][]string-16     6.02M \u00b1 0%     0.00M \u00b1 0%  -100.00%  (p=0.000 n=8+8)```", "comments": ["@philpearl Could you please resolve the conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Yep plan to sort this out soon. Was waiting for the preceding PR to merge\n\nOn Sun, 15 Mar 2020, 00:43 Alfred Sorten Wolf, <notifications@github.com>\nwrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this PR still valid? Assigning the stalled label. Please\n> comment to reassure me that this is still being worked on.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/36898#issuecomment-599152720>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADPPXDZV52DTTLN33IDBOLRHQQCRANCNFSM4KXZUAKA>\n> .\n>\n", "@philpearl #36578 got rolled back due to internal failures , how do we proceed with this PR ?", "Hopefully the subsequent PR to reinstate it (\nhttps://github.com/tensorflow/tensorflow/pull/37190) hasn't been rolled\nback.\n\nOn Mon, 16 Mar 2020 at 18:37, Rajeshwar Reddy T <notifications@github.com>\nwrote:\n\n> @philpearl <https://github.com/philpearl> #36578\n> <https://github.com/tensorflow/tensorflow/pull/36578> got rolled back due\n> to internal failures , how do we proceed with this PR ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/36898#issuecomment-599698189>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADPPXANVGGPHGIE3DPLT2DRHZWYFANCNFSM4KXZUAKA>\n> .\n>\n", "Dammit the re-instating PR (https://github.com/tensorflow/tensorflow/pull/37190) was rolled back. It just says \"due to internal failure\" with no further explanation.\r\n\r\nAt this point I think I give up. I believe in these changes, but dealing with this codebase with its missing generated files and mysterious extra background testing processes is just too painfull"]}, {"number": 36897, "title": "Feature Request: Pickle an entire model.", "body": "**System information** \r\nTensorFlow version 2.0.0\r\n\r\n**Describe the current behavior**\r\nThis request follows the issue [#33204](https://github.com/tensorflow/tensorflow/issues/33283).\r\nWhen trying to pickle a model to save its current state I get the error: `TypeError: can't pickle _thread._local objects`. Even when doing a low-level code and just trying to save keras layers it gets the same error message.\r\n\r\n**Describe the expected behavior**\r\nIt would be nice to be able to save models with pickle.\r\nPickling the entire model object is easier than saving just the weights. Especially for complex models with multiple networks involved. It is very convenient as long as it does not introduce performance issues.\r\n\r\nA potential current fix can be [this](https://github.com/tensorflow/tensorflow/issues/33283#issuecomment-561772070) for the moment.\r\n\r\n**Code to reproduce the issue** \r\nThe problem can be reproduced with this [gist](https://colab.research.google.com/gist/gadagashwini/079ecb24dcfb0130a41c4cc8f7a69aba/untitled209.ipynb).\r\n", "comments": ["I think the recommend way to save a model is saving it in `Checkpoints` or `SavedModel` format.\r\n\r\n[Traning checkpint](https://www.tensorflow.org/guide/checkpoint)", "I agree it is the recommended way. In my particular case I did my own model using keras layers and that's why I said:\r\n\r\n>  Even when doing a low-level code and just trying to save keras layers it gets the same error message.\r\n\r\nSo this way does not apply to my case and therefore I wanted to use pickle.\r\n", "Another use case of being able to pickle is to support `KerasClassifier` objects, which may be useful for folks using sklearn pipelines or sklearn grid search capabilities.  ", "Since keras model can be pickled, if one could take some compromise and using tensorflow and keras together, here might be one of the roundabouts:\r\n\r\nSituation: say we have a model built and trained with tensorflow, we want it to be pickled:\r\n1. we get model weights \r\n      a. use `ModelCheckpoint(save_weigths_only=True)` to save model weights\r\n      b. use `model.layer[i].get_weights()` to get the weights for each layer\r\n2. build a duplicate model (model_keras) structure with keras not tf.keras\r\n3. pass the model weights into model_keras \r\n      a. use `model_keras.load_weights()`\r\n      b. `for i, layer in enumerate(model_keras.layers): \r\n                   weights = model.layer[i].get_weights()\r\n                   layer.set_weights(weights)`\r\n\r\n", "@NEGU93  Is it still an issue for you?. Please go ahead and close the issue if it was resolved already.Thanks!", "See https://github.com/tensorflow/community/pull/286", "@NEGU93,\r\nSorry for the delayed response. Can you please let us know if we can close this issue as the [associated PR](https://github.com/tensorflow/community/pull/286) has been merged? Thanks! ", "Yes sorry. You can choose it now."]}, {"number": 36896, "title": "Use bazel crosstool to compile other arch wheel file raise lots of undefined reference error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9 (stretch)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  N/A\r\n- TensorFlow version: v2.0.0 tag\r\n- Python version: Python 3.5.3\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 6.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to compile MIPS tensorflow wheel file with bazel crosstool, refer to `tensorflow/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh and tensorflow/third_party/toolchains/cpus/arm/*`\r\n\r\nMy related CROSSTOOL refers to: https://paste.ubuntu.com/p/JRgnfkVrPw/ And then, I build it:\r\n```\r\nroot@container:~/tensorflow# git diff --unified=0\r\ndiff --git a/.bazelrc b/.bazelrc\r\nindex 4b673d3fcf..caa301c8eb 100644\r\n--- a/.bazelrc\r\n+++ b/.bazelrc\r\n@@ -113,0 +114,3 @@ build -c opt\r\n+build --show_timestamps --verbose_failures --subcommands --color=yes --cpu mips64 --crosstool_top //tools/linux_mips64:linuxmips64\r\n\r\nroot@container:~/tensorflow# cat .tf_configure.bazelrc \r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild:opt --copt=-march=mips64\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\nroot@container:~/tensorflow# bazel build -c opt --config=v1 --config=noaws --config=nonccl --cpu mips64 //tensorflow/tools/pip_package:build_pip_package\r\n... ...\r\n(12:36:27) ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/com_google_protobuf/BUILD:405:1: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -o bazel-out/host/bin/external/com_google_protobuf/protoc -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/external/com_google_protobuf/protoc-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nbazel-out/host/bin/external/com_google_protobuf/libprotobuf.a(descriptor.o): In function `void std::call_once<void (*)(google::protobuf::FileDescriptorTables const*), google::protobuf::FileDescriptorTables const*>(std::once_flag&, void (*&&)(google::protobuf::FileDescriptorTables const*), google::protobuf::FileDescriptorTables const*&&)::{lambda()#2}::_FUN()':\r\ndescriptor.cc:(.text._ZZSt9call_onceIPFvPKN6google8protobuf20FileDescriptorTablesEEJS4_EEvRSt9once_flagOT_DpOT0_ENUlvE0_4_FUNEv[_ZZSt9call_onceIPFvPKN6google8protobuf20FileDescriptorTablesEEJS4_EEvRSt9once_flagOT_DpOT0_ENKUlvE0_clEv]+0x3): undefined reference to `std::__once_callable'\r\n... ...\r\n```\r\n\r\nI have tried google and then add/comment different gcc compile flag to try again, but undefined reference similar error happened always. So, I ask for help\r\n\r\nSince undefined reference similar error is too long, I redirect to file and paste it, see: https://paste.ubuntu.com/p/V7ccND89nZ/\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nUse debian:stretch image to build it, and container has installed `binutils-mips64el-linux-gnuabi64 gcc-mips64el-linux-gnuabi64 gfortran-mips64el-linux-gnuabi64 g++-mips64el-linux-gnuabi64 libpython3-all-dev:mips64el` toolchains\r\n\r\nPrepare related env and then  bazel build, lots of undefined reference errors will raise\r\n\r\nAny help will be appreciate, thanks!", "comments": ["@uddmorningsun \r\n\r\nCan you please upgrade with Bazel 0.26.1 and GCC/Compiler version 7.3.1 and see how it progresses. Please, find the tested build configurations from [here](https://www.tensorflow.org/install/source#cpu).Thanks!", "About bazel version, since I refer to https://github.com/tensorflow/tensorflow/blob/v2.0.0/configure.py, so choose the minimum version\r\n```\r\n_TF_MIN_BAZEL_VERSION = '0.24.1'\r\n_TF_MAX_BAZEL_VERSION = '0.26.1'\r\n```\r\n\r\n @ravikyram Thanks, I will follow your suggestions and update related env to try it !", "Found 0.24.1 to 0.26.1 has a big change in toolchain configuration, increase learning cost to unexperenced bazel user\r\n\r\n```\r\n(11:58:29) ERROR: /root/tensorflow/tools/linux_mips64/BUILD:13:1: //tools/linux_mips64:mips64_toolchain: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\n(11:58:29) ERROR: /root/tensorflow/tools/linux_mips64/BUILD:27:1: //tools/linux_mips64:cc-compiler-x86: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\n```\r\nMaybe CROSSTOOL file doesn't work any more, I found or guess\r\n\r\nLoading `*.bzl` extension file to configure toolchain, refer https://docs.bazel.build/versions/master/tutorial/cc-toolchain-config.html to learn and adjust\r\n\r\nAfter update toolchain configuration in V0.26.1, I don't know how to drop `-msse3` flag(in V0.24.1, it' common for /usr/bin/mips64el-linux-gnuabi64-gcc, so I guess V0.24.1 will overwrite default compiler flag. But V0.26.1, it seems not overwrite). \r\n```\r\n(13:03:07) ERROR: /root/tensorflow/tensorflow/core/platform/BUILD:98:1: C++ compilation of rule '//tensorflow/core/platform:cpu_info' failed (Exit 1): mips64el-linux-gnuabi64-gcc-7 failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/mips64el-linux-gnuabi64-gcc-7 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/core/platform/_objs/cpu_info/cpu_info.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/platform/_objs/cpu_info/cpu_info.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -g0 -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/platform/cpu_info.cc -o bazel-out/host/bin/tensorflow/core/platform/_objs/cpu_info/cpu_info.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nmips64el-linux-gnuabi64-gcc-7: error: unrecognized command line option '-msse3'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n# another error\r\n... ...\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nmips64el-linux-gnuabi64-gcc-7: error: unrecognized command line option '-msse4.2'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n```\r\nroot@container2:~/tensorflow# grep \"msse3\" -rn -I --color *\r\n... ...\r\ntensorflow/tensorflow.bzl:299:        if_linux_x86_64([\"-msse3\"]) +\r\nthird_party/sycl/crosstool/CROSSTOOL.tpl:95:  compiler_flag: \"-msse3\"\r\nthird_party/toolchains/clang6/CROSSTOOL.tpl:553:    value: \"-no-canonical-prefixes --target=x86_64-unknown-linux-gnu -fno-omit-frame-pointer -fno-tree-vrp -msse3\"\r\n```\r\n\r\nSo, I modify tensorflow source code to build again(delete \"-msse3\" and \"-msse4.2\" flag, empty list). And then, anthoer errors raise:\r\n```\r\n(14:22:58) ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/hwloc/BUILD.bazel:229:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1): mips64el-linux-gnuabi64-gcc-7 failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/mips64el-linux-gnuabi64/lib \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/mips64el-linux-gnuabi64-gcc-7 -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/hwloc/_objs/hwloc/topology-x86.pic.d '-frandom-seed=bazel-out/host/bin/external/hwloc/_objs/hwloc/topology-x86.pic.o' -fPIC -iquote external/hwloc -iquote bazel-out/host/bin/external/hwloc -isystem external/hwloc/hwloc -isystem bazel-out/host/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/host/bin/external/hwloc/include -g0 -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/hwloc/hwloc/topology-x86.c -o bazel-out/host/bin/external/hwloc/_objs/hwloc/topology-x86.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from external/hwloc/hwloc/topology-x86.c:23:0:\r\nexternal/hwloc/hwloc/topology-x86.c: In function 'cpuid_or_from_dump':\r\nexternal/hwloc/include/private/cpuid-x86.h:67:3: error: invalid 'asm': invalid use of '%k'\r\n   __asm__(\r\n   ^~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n@ravikyram About those, do you have any ideas?\r\n\r\n* Why V0.26.1 doesn't overwrite defaule compiler flag? That is, V0.24.1 is ok but V0.26.1 is fail for \"-msse3\" and \"-msse4.2\" flag\r\n* Does have examples to be used as reference in V0.26.1 syntax configuration? Raspberry Pi examples is stale in `tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`\r\n* How to resolve error?\r\n\r\nI have update my build env with my best effort:\r\n```\r\nroot@container2:~/tensorflow/tools/linux_mips64# gcc --version\r\ngcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nroot@container2:~/tensorflow/tools/linux_mips64# mips64el-linux-gnuabi64-gcc-7 --version\r\nmips64el-linux-gnuabi64-gcc-7 (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nroot@container2:~/tensorflow/tools/linux_mips64# bazel version\r\nBuild label: 0.26.1\r\n\r\n```\r\n\r\n", "@ravikyram @ymodak When I update repo today, I found `third_party/toolchains/cpus/arm/` directory has been updated in master to support new bazel syntax:\r\n```\r\n[yancy@localhost tensorflow]$ git branch -v -v\r\n* master 77842e636c [origin/master] Internal change\r\n[yancy@localhost tensorflow]$ ls third_party/toolchains/cpus/arm/\r\narm_compiler_configure.bzl  BUILD  cc_config.bzl.tpl\r\n```\r\nSo, I refer to it and try again, I don't know how to resolve it above new error:\r\n```\r\n(07:55:44) ERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/org_tensorflow/tensorflow/BUILD:494:1: Linking of rule '@org_tensorflow//tensorflow:libtensorflow_framework.so.1.14.0' failed (Exit 1): mips64el-linux-gnuabi64-gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /usr/bin/mips64el-linux-gnuabi64-gcc -shared -o bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/libtensorflow_framework.so.1.14.0 -Wl,--version-script,external/org_tensorflow/tensorflow/tf_framework_version_script.lds '-Wl,-rpath,$ORIGIN/' -Wl,-soname,libtensorflow_framework.so.1 -pthread -pthread -pthread -pthread -Wl,@bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/libtensorflow_framework.so.1.14.0-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/core/libframework_internal_impl.pic.lo(batch_util.pic.o): in function `Eigen::internal::(anonymous namespace)::get_random_seed()':\r\nbatch_util.cc:(.text+0x2c): relocation truncated to fit: R_MIPS_CALL16 against `clock_gettime@@GLIBC_2.2'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: batch_util.cc:(.text+0x3c): relocation truncated to fit: R_MIPS_CALL16 against `random@@GLIBC_2.0'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/core/libframework_internal_impl.pic.lo(batch_util.pic.o): in function `tensorflow::batch_util::(anonymous namespace)::ValidateInput(tensorflow::Tensor const&, tensorflow::Tensor const&, long long)':\r\nbatch_util.cc:(.text+0x394): relocation truncated to fit: R_MIPS_CALL16 against `tensorflow::TensorShapeRep::DebugString[abi:cxx11]() const'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: batch_util.cc:(.text+0x3b0): relocation truncated to fit: R_MIPS_CALL16 against `tensorflow::TensorShapeRep::DebugString[abi:cxx11]() const'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: batch_util.cc:(.text+0x428): relocation truncated to fit: R_MIPS_CALL16 against `tensorflow::TensorShape::~TensorShape()'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/core/libframework_internal_impl.pic.lo(batch_util.pic.o): in function `tensorflow::Status tensorflow::batch_util::(anonymous namespace)::HandleElementToSlice<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(tensorflow::Tensor, tensorflow::Tensor*, long long, bool)':\r\nbatch_util.cc:(.text+0x4cc): relocation truncated to fit: R_MIPS_CALL16 against `tensorflow::TTypes<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, 2ul, long>::Tensor tensorflow::Tensor::flat_outer_dims<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, 2ul>()'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: batch_util.cc:(.text+0x590): relocation truncated to fit: R_MIPS_CALL16 against `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::operator=(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&)'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: batch_util.cc:(.text+0x5cc): relocation truncated to fit: R_MIPS_CALL16 against `Eigen::TensorBase<Eigen::TensorMap<Eigen::Tensor<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, 2, 1, long>, 16, Eigen::MakePointer>, 1>::chip(long, long)'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/core/libframework_internal_impl.pic.lo(batch_util.pic.o): in function `tensorflow::Status tensorflow::batch_util::(anonymous namespace)::HandleElementToSlice<tensorflow::Variant>(tensorflow::Tensor, tensorflow::Tensor*, long long, bool)':\r\nbatch_util.cc:(.text+0x784): relocation truncated to fit: R_MIPS_CALL16 against `Eigen::TensorBase<Eigen::TensorMap<Eigen::Tensor<tensorflow::Variant, 2, 1, long>, 16, Eigen::MakePointer>, 1>::chip(long, long)'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: bazel-out/mips64-py2-opt/bin/external/org_tensorflow/tensorflow/core/libframework_internal_impl.pic.lo(batch_util.pic.o): in function `void tensorflow::batch_util::(anonymous namespace)::HandleSliceToElement<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(tensorflow::Tensor*, tensorflow::Tensor*, long long, bool)':\r\nbatch_util.cc:(.text+0x838): relocation truncated to fit: R_MIPS_CALL16 against `tensorflow::TTypes<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, 2ul, long>::Tensor tensorflow::Tensor::flat_outer_dims<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, 2ul>()'\r\n/usr/lib/gcc-cross/mips64el-linux-gnuabi64/8/../../../../mips64el-linux-gnuabi64/bin/ld: batch_util.cc:(.text+0x8fc): additional relocation overflows omitted from the output\r\ncollect2: error: ld returned 1 exit status\r\nTarget @org_tensorflow//tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "@ravikyram @ymodak Apologize for my operation mistakes in above `relocation truncated to fit: R_MIPS_CALL16 against clock_gettime@@GLIBC_2.2` similar error.\r\n\r\nAfter I google, in order to save time to try different compile flags, I only copy and run failed command in single file, it always fail sadly. I ask for help whether it can be resolved or not.\r\n\r\nFinally, since lots of similar answer maybe add `-mxgot -mlong-calls`, so I decide to refresh and start to run `bazel build ... --copt=-mxgot --copt=-mlong-calls` again, it does work! Thanks!\r\n```\r\nroot@69783928b3ce:/# python -c \"import tensorflow; from distutils.util import get_platform; print('import ok in %s' % get_platform())\" 2>/dev/null\r\nimport ok in linux-mips64\r\n\r\n>>> import tensorflow as tf  \r\n>>> x=tf.constant([[1,2],[1,2]])  \r\n>>> y=tf.constant([[1,1],[1,2]])\r\n>>> z=tf.add(x,y)\r\n>>> z\r\n<tf.Tensor 'Add_1:0' shape=(2, 2) dtype=int32>\r\n```\r\n\r\nRefer:\r\n* https://www.linux-mips.org/archives/linux-mips/2006-04/msg00182.html\r\n* https://stackoverflow.com/questions/15577557/undefined-symbol-in-shared-library-for-jni-executable-is-working\r\n* https://forums.gentoo.org/viewtopic-t-643676-start-0.html", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36896\">No</a>\n"]}, {"number": 36895, "title": "Custom implementations for AvgPool3D, Conv3D, MaxPool3D", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, FULLY_CONNECTED, MUL, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: AvgPool3D, Conv3D, MaxPool3D.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jashshah,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/35449#issuecomment-570318723) comment from a similar issue and let us know if it works. Thanks!", "@amahendrakar this still does not solve the problem. I keep getting a segmentation fault. \r\n\r\nI am using Tensorflow version: `2.2.0-dev20200218`\r\n \r\nIf it helps here is the code that gets me the error:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pathlib\r\nimport pandas as pd\r\nimport inputs\r\n\r\nfrom tensorflow.keras.layers import Conv3D, MaxPool3D, Flatten, Dense\r\nfrom tensorflow.keras.layers import Input, BatchNormalization, ReLU\r\nfrom tensorflow.keras.layers import AvgPool3D\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Concatenate, Add, Dropout\r\nfrom tensorflow.keras import regularizers\r\n\r\nNUM_FILTERS_0 = 96\r\nNUM_FILTERS_2 = 64\r\nNUM_FILTERS_3 = 64\r\nNUM_FILTERS_4 = 128\r\nNUM_FILTERS_5 = 128\r\nNUM_FILTERS_6 = 128\r\nNUM_FILTERS_7 = 128\r\nNUM_FILTERS_8 = 64\r\nNUM_FILTERS_9 = 64\r\n\r\ndef conv_block(inputs, filters, kernel_size, strides=(1, 1, 1), padding='valid', activation=True, block_name='conv3d'):\r\n    with tf.name_scope(block_name):\r\n        conv = Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, activation=None,\r\n                      name='{}_conv'.format(block_name))(inputs)\r\n        \r\n        batch_norm = BatchNormalization(name='{}_batch_norm'.format(block_name))(conv)\r\n\r\n        if activation:\r\n            relu = ReLU(max_value=6, name='{}_relu'.format(block_name))(batch_norm)\r\n            res_layer = relu\r\n        else:\r\n            res_layer = batch_norm\r\n\r\n    return res_layer\r\n\r\ndef fire(inputs, n_hidden, block_name):\r\n    with tf.name_scope(block_name):\r\n        f1_sq_1 = conv_block(inputs, filters=n_hidden//4, kernel_size=(1, 1, 1), padding='same',\r\n                             block_name='{}_squeeze_1'.format(block_name))\r\n        f1_e_1 = conv_block(f1_sq_1, filters=n_hidden, kernel_size=(1, 1, 1), activation=False, padding='same',\r\n                             block_name='{}_expand_1'.format(block_name))\r\n        f1_e_2 = conv_block(f1_sq_1, filters=n_hidden, kernel_size=(5, 5, 5), activation=False, padding='same',\r\n                             block_name='{}_expand_2'.format(block_name))\r\n\r\n        concat_output = Concatenate(name='{}_concatenate'.format(block_name))([f1_e_1, f1_e_2])\r\n        relu = ReLU(max_value=6, name='{}_relu'.format(block_name))(concat_output)\r\n\r\n    return relu\r\n\r\ndef squeezenet(n_class, input_shape=(128, 128, 50, 1))\r\n    inputs = Input(shape=input_shape, name='inputs')\r\n    layer_1 = conv_block(inputs, filters=NUM_FILTERS_0, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding='same', block_name='layer_1')\r\n    fire_2 = fire(layer_1, NUM_FILTERS_2, 'fire_2')\r\n    fire_3 = fire(fire_2, NUM_FILTERS_3, 'fire_3')\r\n    add_4 = Add(name='add_4')([fire_2, fire_3])\r\n    fire_4 = fire(add_4, NUM_FILTERS_4, 'fire_4')\r\n    maxpool_4 = MaxPool3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='valid', name='max_pooling_4')(fire_4)\r\n    fire_5 = fire(maxpool_4, NUM_FILTERS_5, 'fire_5')\r\n    add_6 = Add(name='add_6')([maxpool_4, fire_5])\r\n    fire_6 = fire(add_6, NUM_FILTERS_6, 'fire_6')\r\n    fire_7 = fire(fire_6, NUM_FILTERS_7, 'fire_7')\r\n    add_8 = Add(name='add_8')([fire_6, fire_7])\r\n    fire_8 = fire(add_8, NUM_FILTERS_8, 'fire_8')\r\n    maxpool_8 = MaxPool3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='valid', name='max_pooling_8')(fire_8)\r\n    fire_9 = fire(maxpool_8, NUM_FILTERS_9, 'fire_9')\r\n    add_10 = Add(name='add_10')([maxpool_8, fire_9])\r\n    conv_10 = Conv3D(filters=32, kernel_size=(1, 1, 1), strides=(1, 1, 1), padding='same', name='conv_10')(add_10)\r\n    avgpool_10 = AvgPool3D(pool_size=(conv_10.get_shape()[1], conv_10.get_shape()[2], conv_10.get_shape()[3]),\r\n                          name='average_pool_10')(conv_10)\r\n    flatten_1 = Flatten(name='flatten_1')(avgpool_10)\r\n    outputs = Dense(n_class, activation=\"softmax\", name='outputs', kernel_regularizer=regularizers.l2(0.001))(flatten_1)\r\n\r\n    model = Model(inputs = inputs, outputs = outputs, name=\"squeezenet_arch\")\r\n    return model\r\n\r\n\r\nn_classes = 3\r\nmodel = squeezenet(n_class=n_classes, input_shape=(128, 128, 50, 1))\r\n\r\ncheckpoint_dir = \"gs://path/to/model/squeeznet_model.ckpt.0093\"\r\nmodel.load_weights(checkpoint_dir)\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n\r\ntflite_models_dir = pathlib.Path(\"./squeezenet_quantized/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n\r\n# convert to a tflite but still using 32-bit float values\r\ntflite_model_file = tflite_models_dir/\"squeezenet_quantized_model.tflite\"\r\ntflite_model_file.write_bytes(tflite_model) # 10.6 MB\r\n\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n\r\neval_data_dir = \"gs://path/to/test_df_for_test.csv\"\r\neval_df = pd.read_csv(eval_data_dir)\r\nbatch_size = 1\r\nvalid_data = test_input_function(eval_csv_path=eval_data_dir, eval_batch_size=batch_size)\r\n\r\ndef representative_data_gen():\r\n    for input_value in valid_data:\r\n        yield [input_value[0]]\r\n        \r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model_quant = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/\"squeezenet_model_final_quant.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_model_quant)\r\n```\r\n\r\nThe jupyter kernel crashes/there is a segmentation fault at this line `tflite_model_quant = converter.convert()`", "@jashshah,\r\nI am facing an error stating `NameError: name 'squeezenet' is not defined` while running the above code. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/72371de010687ddcac69328260f707e3/36895.ipynb). \r\n\r\nAlso, I see that you are using `.csv` and `.ckpt` files in your code. Cloud you please provide all the supporting files to run the code. Thanks!", "For confidentiality reasons I can't share the csv or the ckpt files but I can share with you the code and I can explain the logic to you.\r\n\r\n\r\n```\r\nimport multiprocessing\r\nfrom multiprocessing import cpu_count\r\nimport ast\r\nimport random\r\nfrom io import BytesIO\r\nimport numpy as np\r\nimport scipy\r\nfrom scipy.ndimage import rotate, interpolation\r\nfrom skimage import transform as tf1\r\nimport tensorflow as tf\r\nfrom tensorflow.python.lib.io import file_io\r\nimport io\r\n\r\nNUM_COLS = 2\r\n\r\ndef parser_csv(line):\r\n  \"\"\"\r\n  Decodes the lines from a csv file\r\n  Helper function in parsing a csv file\r\n\r\n  Args:\r\n    line : Each record/row in a csv file\r\n\r\n  Returns:\r\n    The parsed row from the csv\r\n  \"\"\"\r\n  parsed_line = tf.io.decode_csv(line, [['string']]*NUM_COLS)\r\n  filename = parsed_line[config.CUBE_PATH_IDX]\r\n  label = parsed_line[config.LABEL_IDX]\r\n  return filename, label\r\n\r\nMIN_BOUND = -1000.0\r\nMAX_BOUND = 1000\r\n\r\ndef normalize(image):\r\n  \"\"\"\r\n  Normalizes the image, creates whitened images\r\n  Args:\r\n    image : The image to be normalized\r\n  Returns:\r\n    The image with zero means and unit variance\r\n  \"\"\"\r\n  image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\r\n  image[image > 1] = 1.\r\n  image[image < 0] = 0.\r\n  return image\r\n\r\ndef preprocess(filename, label):\r\n  \"\"\"\r\n  Helper function which takes an image and\r\n  applies a random augmentation technique\r\n  Args:\r\n    filename : Path of the image to be preprocessed\r\n    augmentation : augmentation technique to apply\r\n  Returns:\r\n    Augmented image and the corresponding label\r\n  \"\"\"\r\n  image = BytesIO(file_io.read_file_to_string(filename.numpy(), binary_mode=True))\r\n  image = np.load(image).astype(np.float32)\r\n  image = np.expand_dims(image, 3)\r\n  image = normalize(image)\r\n  label = ast.literal_eval(str(label.numpy(), 'utf-8')) # this is the earlier one\r\n  \r\n  return image, label\r\n\r\nINPUT_SHAPE = (128, 128, 50, 1)\r\n\r\ndef test_input_parser(filename, label, augment=False):\r\n  \"\"\"\r\n  Parsing function for the input images (testing)\r\n  Args:\r\n    filename : Path of the input image\r\n  Returns:\r\n    Parsed image and label\r\n  \"\"\"\r\n  if augment:\r\n    augmentation = random.choice(['rotateit', 'translateit', 'gaussian_noise',\r\n                                  'fliplr', 'flip', 'scaleit', 'original'])\r\n  else:\r\n    augmentation = \"original\"\r\n\r\n  inputs = tf.py_function(preprocess, [filename, label],\r\n                      [tf.float32, tf.int64])#.set_shape([128,128,128,1], [1,3])\r\n#   inputs.set_shape([128,128,128,1], [1,3])\r\n  inputs[0] = tf.cast(inputs[0], tf.float32)\r\n  inputs[0].set_shape(INPUT_SHAPE)\r\n  inputs[1].set_shape([3])\r\n  return inputs[0], inputs[1]\r\n\r\n\r\ndef test_input_function(eval_csv_path, eval_batch_size):\r\n\r\n        \"\"\"\r\n        Creates an input function for the eval dataset\r\n        \"\"\"\r\n        dataset = tf.data.TextLineDataset(eval_csv_path).skip(1).map(parser_csv, num_parallel_calls=cpu_count())\r\n        dataset = dataset.map(test_input_parser, num_parallel_calls=cpu_count()).prefetch(tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.repeat(1).batch(eval_batch_size)\r\n\r\n        return dataset\r\n\r\n```\r\n\r\nThis code simply reads a csv file where one column contains the paths to the numpy arrays whose shape is (128, 128, 50) and other column contains a label for the said numpy file in one-hot encoded string such as '[1, 0, 0]' or '[0, 0, 1']. If it is also of any use I am reading the csv and the numpy files from with the csv from a Google Cloud Storage bucket. The paths are GCS bucket links.\r\n\r\nI believe @rmothukuru was able to replicate a similar kind of dataset.\r\n\r\nI have also made the edits to the earlier comment. Let me know if you need anything else.", "@jashshah You can take a look at this [doc](https://www.tensorflow.org/lite/guide/ops_custom) which explains how to convert this custom operations of tensorflow to tflite.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36894, "title": "tf.keras.layers.ReLU() drops tensor._keras_mask (sample_weight) unintentionally.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\nWindows 10 1903, TF 2.0 or 2.1, Python 3.7, CUDA10.1, cuDNN 7.6, GTX 1070 8G\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.ReLU()(x) sets `x._keras_mask` to None.\r\n\r\n`tf.keras.layers.Embedding(mask_zero=True)(x)` and `tf.keras.layers.GRU(mask=self_defined_mask)(x)` set a bool Tensor `mask` to `x._keras_mask`. This mask will be passed on to Loss and Metrics and serves as `sample_weight` when calculating the cross_entropy_loss and metrics.accuracy etc. However, it just disappeared after a ReLU layer.\r\n\r\nI found tf.keras.layers.ReLU() call K.relu() which in turn calls tf.nn.relu(). `tf.nn.relu()`, a legacy of TF 1.x which doesn't support `_keras_mask`, seems to be the cause of the unintentional drop of the mask.\r\n\r\n**Describe the expected behavior**\r\ntf.keras.layers.ReLU()(x) just leaves `x._keras_mask` as it was.\r\n\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\n\r\n```\r\nclass GRU4REC(tf.keras.Model):\r\n    def __init__(self, vocabulary_size, embedding_dim, hidden_units):\r\n        super(GRU4REC, self).__init__()\r\n        self.Embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dim, mask_zero=True)\r\n        self.FC1 = tf.keras.layers.Dense(hidden_units * 3, tf.nn.relu)\r\n        self.BN1 = tf.keras.layers.BatchNormalization()\r\n        self.RELU = tf.keras.layers.ReLU()\r\n        self.GRU = tf.keras.layers.GRU(hidden_units, return_sequences=True)\r\n        self.FC2 = tf.keras.layers.Dense(vocabulary_size)\r\n        self.BN2 = tf.keras.layers.BatchNormalization()\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        x = self.Embedding(inputs)\r\n        x = self.FC1(x)\r\n        x = self.BN1(x)\r\n        print(f'before relu: {x._keras_mask}') # print a Tensor\r\n        x = self.RELU(x)\r\n        print(f'after relu: {x._keras_mask}') # print None\r\n        x = self.GRU(x)\r\n        x = self.FC2(x)\r\n        x = self.BN2(x)\r\n        x = self.RELU(x)\r\n        # pred = tf.argmax(x, -1)\r\n        # tf.print('++++++++++')\r\n        # tf.print(tf.shape(pred))\r\n        # eq = tf.equal(inputs, tf.cast(pred, tf.int32))\r\n        # tf.print(tf.reduce_sum(tf.cast(eq, tf.float32)))\r\n        # tf.print('---------')\r\n        return x\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nOn TF 2.0\r\n\r\n```\r\n2020-02-19 20:24:05.261124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-02-19 20:24:05.292473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7845GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-02-19 20:24:05.292814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-02-19 20:24:05.425947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-02-19 20:24:05.503908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-02-19 20:24:05.538431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-02-19 20:24:05.660730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-02-19 20:24:05.738706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-02-19 20:24:05.944062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-19 20:24:05.945225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-02-19 20:24:05.947055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7845GHz coreCount: 15 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-02-19 20:24:05.947607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-02-19 20:24:05.947889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-02-19 20:24:05.948150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-02-19 20:24:05.948418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-02-19 20:24:05.948677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-02-19 20:24:05.948922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-02-19 20:24:05.949206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-19 20:24:05.950821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-02-19 20:24:06.889483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-02-19 20:24:06.889680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-02-19 20:24:06.889787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-02-19 20:24:06.891077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6376 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nbefore relu: Tensor(\"gr_u4rec/embedding/NotEqual:0\", shape=(None, None), dtype=bool)\r\nafter relu: None\r\nbefore relu: Tensor(\"model/gr_u4rec/embedding/NotEqual:0\", shape=(None, None), dtype=bool)\r\nafter relu: None\r\n2020-02-19 20:24:08.567321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n\r\nProcess finished with exit code -1\r\n```\r\n\r\nWhen I was training a lots-of-zero-padded sequences, the loss went down smoothly as expected while the accuracy soared dramatically. However, everything went just right when I removed ReLU. Hope fix it or help point it out if I was wrong. Thanks~\r\n", "comments": ["@Rayarrow, Thanks for reporting this issue.\r\nCan you share the complete code to replicate the reported issue. Thanks!", "The real dataset and code I used to train are too large and messy to be attached here, so I faked a dataset produced by a random sequence generator but I think it's enough to explain my issue.\r\n\r\nThis script does an evaluation on the model, and then 10 training steps (10 batch) followed by another round of evaluation. Please run this script twice:\r\n    1. With ReLU. The accuracy will soar (~40%) during the training process and also in the second round of evaluation compared to the first round.\r\n    2. Without ReLU (comment line 21 and line 26, the two lines of `x = self.RELU(x)`). The accuracy just stays low (~1e-4) as the model goes through training and evaluation.\r\n    \r\nBesides, object `x` will be printed twice to the console before and after it goes through layer `ReLU`, and it turn into `None` after it went through ReLU. \r\n\r\nThe random sequences are batch-padded and so lots of zeros are padded at the end of each sequence. Without the mask, these padded zeros will be included when calculating the loss and accuracy.\r\n\r\nSo I think `ReLU` is the cause.\r\n\r\nThanks!\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass GRU4REC(tf.keras.Model):\r\n    def __init__(self, vocabulary_size, embedding_dim, hidden_units):\r\n        super(GRU4REC, self).__init__()\r\n        self.Embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dim, mask_zero=True)\r\n        self.FC1 = tf.keras.layers.Dense(hidden_units * 3, tf.nn.relu)\r\n        self.BN1 = tf.keras.layers.BatchNormalization()\r\n        self.RELU = tf.keras.layers.ReLU()\r\n        self.GRU = tf.keras.layers.GRU(hidden_units, return_sequences=True)\r\n        self.FC2 = tf.keras.layers.Dense(vocabulary_size)\r\n        self.BN2 = tf.keras.layers.BatchNormalization()\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        x = self.Embedding(inputs)\r\n        x = self.FC1(x)\r\n        x = self.BN1(x)\r\n        print(f'before relu: {x._keras_mask}')  # print a Tensor\r\n        x = self.RELU(x)\r\n        print(f'after relu: {x._keras_mask}')  # print None\r\n        x = self.GRU(x)\r\n        x = self.FC2(x)\r\n        x = self.BN2(x)\r\n        x = self.RELU(x)\r\n        return x\r\n\r\n\r\nX_input = tf.keras.layers.Input([None])\r\nencoder = GRU4REC(20000, 128, 100)\r\nmodel = tf.keras.Model(inputs=X_input, outputs=encoder(X_input))\r\n\r\n# After compile, GRU4REC.call() will be invoked and the Tensor `x` before and after ReLU will be printed to the console.\r\nmodel.compile(tf.optimizers.Adam(), tf.losses.SparseCategoricalCrossentropy(),\r\n              metrics=[tf.metrics.SparseCategoricalAccuracy()])\r\n\r\n\r\ndef random_sequence_generator():\r\n    for i in range(1000):\r\n        sequence = np.random.randint(0, 19999, np.random.randint(3, 20, 1))\r\n        yield sequence[:-1], sequence[1:]\r\n\r\n\r\ntrain_dataset = tf.data.Dataset.from_generator(random_sequence_generator, output_types=(tf.int32, tf.int32)).shuffle(\r\n    1000).padded_batch(32, ((None,), (None,)))\r\n\r\nvalid_dataset = tf.data.Dataset.from_generator(random_sequence_generator,\r\n                                               output_types=(tf.int32, tf.int32)).padded_batch(32, ((None,), (None,)))\r\n\r\nmodel.evaluate(valid_dataset)\r\nmodel.fit(train_dataset, epochs=1, steps_per_epoch=10)\r\nmodel.evaluate(valid_dataset)\r\n```", "Was able to reproduce the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/f813eb95dccaff1af7e9e33dd949a363/untitled398.ipynb). Thanks", "@Rayarrow Looks like this was resolved in `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d97f138de1da3f8e50646692a37830c2/untitled398.ipynb).\r\n\r\nCan you please verify once and let us know whether it was resolved or not. Thanks!", "Ok. Too busy right now and I will try it a few days later. Wait for my message please.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36894\">No</a>\n"]}, {"number": 36893, "title": "tf.io.gfile.listdir does not with with S3 buckets", "body": "**System information** - MacOS, Tensorflow 2.1, Python 3.7.6\r\n\r\n**Describe the current behavior**\r\n\r\nExecuting `tf.io.gfile.listdir(\"s3://bucket-name/\")` results in:\r\n\r\n```\r\n/private/var/folders/bn/36k9myl51_v23yyxm629ly4w0000gn/T/tmp.PygT1otw/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py in list_directory_v2(path)\r\n    644   return [\r\n    645       compat.as_str_any(filename)\r\n--> 646       for filename in pywrap_tensorflow.GetChildren(compat.as_bytes(path))\r\n    647   ]\r\n    648\r\n\r\nInvalidArgumentError: S3 path doesn't contain an object name: s3://bucket-name/\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should return a list of objects within the bucket.\r\n\r\nS3 is quite limited in features for searching/globbing S3 objects but simple operations like listing objects under a given key (in the case above, `/`), are well supported and it would be good to include in Tensorflow.\r\n\r\nThe error message is also somewhat confusing.", "comments": ["@orf \r\nCould you please share simple stand alone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!", "Hey @ravikyram , so I provided it in the description: `tf.io.gfile.listdir(\"s3://bucket-name/\")` is the sample code.\r\n\r\nYou obviously need a S3 bucket set up. I can provide this privately if required, but I expect you have the ability to create one yourselves.\r\n\r\nFor reference executing `tf.io.gfile.stat(\"s3://bucket-name/key\")` works fine, but anything involving listing or globbing fails with the exception listed in the description.", "The same stuff. Have to use `s3://bucket/data` instead of just `s3://bucket/` to store files and for globing to work.", "Any updates on this?", "@orf \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36893\">No</a>\n", "I think this will get fixed in the 2.4 final release, after [rewriting implementation of matching code](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/file_system_helper.cc;l=123-266;drc=8b5b9dc96666a3a5d27fad7179ff215e3b74b67c)"]}, {"number": 36892, "title": "Unable to build r1.15 using gcc 4.8.5-39 because of -std=c++14 argument.", "body": "The crux of this issue is **Does tensorflow r1.15 compile with gcc 4.8.5** when I check out r1.15, I cannot compile due to an invalid argument \"-std=c++14\" similar to #31760 but I do not understand the resolution from that. \r\n\r\nI am preparing to migrate to version 2.X so I tried to build r1.15. I get two errors. \r\n\r\nThe first error is resolved in #28824\r\n\r\nThe second error I have not gotten around. Everthing seems to build fine then I get.\r\n\r\n>ERROR: /home/username/.cache/bazel/_bazel_smithm3/6bda0689d89415301f8ee7c16fbd66e8/external/local_config_mlir/BUILD:418:1: C++ compilation of rule '@local_config_mlir//:Support' failed (Exit 1)  \r\n>gcc: error: unrecognized command line option '-std=c++14'\r\n\r\n\r\n**System information**\r\n- Centos7\r\n- checkout r1.15 and v1.15.1\r\n- Python 3.8.1\r\n- Bazel version 0.25.2\r\n- GCC/Compiler version 4.8.5-039\r\n- CUDA/cuDNN version 10.1/7\r\n- GPU model and memory Nvidia 1080Ti\r\n\r\nThis appears to be caused by mlir requiring a newer version of gcc, since it requires `-std=c++14` and if you replace it with it with '-std=c++1y' you'lll get an error about `std::make_unique`\r\n\r\nIs tensorflow r1.15 supposed to be compatible with gcc 4.8.5 (default on centos 7)?\r\n\r\n **Reproducing the problem.**\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.15\r\n./configure\r\n**Select default options + CUDA**\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n", "comments": ["@odinsbane tensorflow r1.15 is compatible with gcc 4.8.5 , you can check this link for [reference](https://www.tensorflow.org/install/source#tested_build_configurations).\r\n\r\nplease share entire error logs and steps follwed before you faced this issue.", "I have included all of the steps, but Ill include all of the log output too. PART 1\r\n\r\n    ~/local/Python3.8/bin/python3 -m venv tensorflow-build-env\r\n    git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n> Cloning into 'tensorflow'...\r\n> remote: Enumerating objects: 833165, done.\r\n> remote: Total 833165 (delta 0), reused 0 (delta 0), pack-reused 833165\r\n> Receiving objects: 100% (833165/833165), 483.03 MiB | 33.02 MiB/s, done.\r\n> Resolving deltas: 100% (674581/674581), done.\r\n> Checking out files: 100% (19594/19594), done.\r\n\r\n    cd tensorflow\r\n    git checkout r1.15\r\n\r\n> Checking out files: 100% (11771/11771), done.\r\n> Branch r1.15 set up to track remote branch r1.15 from origin.\r\n> Switched to a new branch 'r1.15'\r\n\r\n    . ../tensorflow-build-env/bin/activate\r\n    pip install Keras-Preprocessing numpy six wheel\r\n\r\n> Collecting Keras-Preprocessing\r\n>   Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\r\n> Collecting numpy\r\n>   Using cached https://files.pythonhosted.org/packages/41/38/b278d96baebc6a4818cfd9c0fb6f0e62013d5b87374bcf0f14a0e9b83ed5/numpy-1.18.1-cp38-cp38-manylinux1_x86_64.whl\r\n> Collecting six\r\n>   Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\r\n> Collecting wheel\r\n>   Using cached https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\r\n> Installing collected packages: numpy, six, Keras-Preprocessing, wheel\r\n> Successfully installed Keras-Preprocessing-1.1.0 numpy-1.18.1 six-1.14.0 wheel-0.34.2\r\n> WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\r\n> You should consider upgrading via the 'pip install --upgrade pip' command.\r\n\r\n    ./configure\r\n\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> You have bazel 0.25.2 installed.\r\n> Please specify the location of python. [Default is /home/USERNAME/Desktop/tensorflow-issue/tensorflow-build-env/bin/python]: \r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   /home/USERNAME/Desktop/tensorflow-issue/tensorflow-build-env/lib/python3.8/site-packages\r\n> Please input the desired Python library path to use.  Default is [/home/USERNAME/Desktop/tensorflow-issue/tensorflow-build-env/lib/python3.8/site-packages]\r\n> \r\n> Do you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\n> XLA JIT support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\n> No OpenCL SYCL support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: \r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with TensorRT support? [y/N]: \r\n> No TensorRT support will be enabled for TensorFlow.\r\n> \r\n> Found CUDA 10.1 in:\r\n>     /usr/local/cuda/lib64\r\n>     /usr/local/cuda/include\r\n> Found cuDNN 7 in:\r\n>     /usr/local/cuda/lib64\r\n>     /usr/local/cuda/include\r\n> \r\n> \r\n> Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n> Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1]:\r\n\r\n> Do you want to use clang as CUDA compiler? [y/N]: \r\n> nvcc will be used as CUDA compiler.\r\n> \r\n> Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n> \r\n> \r\n> Do you wish to build TensorFlow with MPI support? [y/N]: \r\n> No MPI support will be enabled for TensorFlow.\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n> \r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\n> Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> \t--config=mkl         \t# Build with MKL support.\r\n> \t--config=monolithic  \t# Config for mostly static monolithic build.\r\n> \t--config=gdr         \t# Build with GDR support.\r\n> \t--config=verbs       \t# Build with libverbs support.\r\n> \t--config=ngraph      \t# Build with Intel nGraph support.\r\n> \t--config=numa        \t# Build with NUMA support.\r\n> \t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n> \t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n> \t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n> \t--config=nogcp       \t# Disable GCP support.\r\n> \t--config=nohdfs      \t# Disable HDFS support.\r\n> \t--config=noignite    \t# Disable Apache Ignite support.\r\n> \t--config=nokafka     \t# Disable Apache Kafka support.\r\n> \t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n> Configuration finished\r\n\r\n    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n> Starting local Bazel server and connecting to it...\r\n> WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n> INFO: Options provided by the client:\r\n>   Inherited 'common' options: --isatty=1 --terminal_columns=160\r\n> INFO: Reading rc options for 'build' from /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc:\r\n>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --copt=-w --config=v1\r\n> INFO: Reading rc options for 'build' from /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.tf_configure.bazelrc:\r\n>   'build' options: --action_env PYTHON_BIN_PATH=/home/USERNAME/Desktop/tensorflow-issue/tensorflow-build-env/bin/python --action_env PYTHON_LIB_PATH=/home/USERNAME/Desktop/tensorflow-issue/tensorflow-build-env/lib/python3.8/site-packages --python_path=/home/USERNAME/Desktop/tensorflow-issue/tensorflow-build-env/bin/python --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,6.1 --action_env LD_LIBRARY_PATH=:/usr/local/cuda/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0\r\n> INFO: Found applicable config definition build:v1 in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc: --define=tf_api_version=1 --action_env=TF2_BEHAVIOR=0\r\n> INFO: Found applicable config definition build:xla in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\n> INFO: Found applicable config definition build:xla in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true\r\n> INFO: Found applicable config definition build:cuda in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n> INFO: Found applicable config definition build:using_cuda in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\n> INFO: Found applicable config definition build:opt in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\n> INFO: Found applicable config definition build:cuda in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\n> INFO: Found applicable config definition build:using_cuda in file /home/USERNAME/Desktop/tensorflow-issue/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\n> INFO: An error occurred during the fetch of repository 'io_bazel_rules_docker'\r\n> INFO: Call stack for the definition of repository 'io_bazel_rules_docker':\r\n>  - /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n>  - /home/USERNAME/Desktop/tensorflow-issue/tensorflow/WORKSPACE:35:1\r\n> ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n> \tFile \"/home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n> \t\t_clone_or_update(ctx)\r\n> \tFile \"/home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n> \t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\n> error cloning io_bazel_rules_docker:\r\n> + cd /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external\r\n> + rm -rf /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker\r\n> + git clone '' https://github.com/bazelbuild/rules_docker.git /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker\r\n> Too many arguments.\r\n> \r\n> usage: git clone [options] [--] <repo> [<dir>]\r\n> \r\n>     -v, --verbose         be more verbose\r\n>     -q, --quiet           be more quiet\r\n>     --progress            force progress reporting\r\n>     -n, --no-checkout     don't create a checkout\r\n>     --bare                create a bare repository\r\n>     --mirror              create a mirror repository (implies bare)\r\n>     -l, --local           to clone from a local repository\r\n>     --no-hardlinks        don't use local hardlinks, always copy\r\n>     -s, --shared          setup as shared repository\r\n>     --recursive           initialize submodules in the clone\r\n>     --recurse-submodules  initialize submodules in the clone\r\n>     --template <template-directory>\r\n>                           directory from which templates will be used\r\n>     --reference <repo>    reference repository\r\n>     -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n>     -b, --branch <branch>\r\n>                           checkout <branch> instead of the remote's HEAD\r\n>     -u, --upload-pack <path>\r\n>                           path to git-upload-pack on the remote\r\n>     --depth <depth>       create a shallow clone of that depth\r\n>     --single-branch       clone only one branch, HEAD or --branch\r\n>     --separate-git-dir <gitdir>\r\n>                           separate git dir from working tree\r\n>     -c, --config <key=value>\r\n>                           set config inside the new repository\r\n> \r\n> + git clone https://github.com/bazelbuild/rules_docker.git /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker\r\n> + git -C /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\n> Unknown option: -C\r\n> usage: git [--version] [--help] [-c name=value]\r\n>            [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n>            [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n>            [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n>            <command> [<args>]\r\n> + git -C /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\n> Unknown option: -C\r\n> usage: git [--version] [--help] [-c name=value]\r\n>            [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n>            [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n>            [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n>            <command> [<args>]\r\n> + git -C /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\n> Unknown option: -C\r\n> usage: git [--version] [--help] [-c name=value]\r\n>            [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n>            [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n>            [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n>            <command> [<args>]\r\n> ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n> \tFile \"/home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n> \t\t_clone_or_update(ctx)\r\n> \tFile \"/home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n> \t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\n> error cloning io_bazel_rules_docker:\r\n> + cd /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external\r\n> + rm -rf /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker\r\n> + git clone '' https://github.com/bazelbuild/rules_docker.git /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker\r\n> Too many arguments.\r\n> \r\n> usage: git clone [options] [--] <repo> [<dir>]\r\n> \r\n>     -v, --verbose         be more verbose\r\n>     -q, --quiet           be more quiet\r\n>     --progress            force progress reporting\r\n>     -n, --no-checkout     don't create a checkout\r\n>     --bare                create a bare repository\r\n>     --mirror              create a mirror repository (implies bare)\r\n>     -l, --local           to clone from a local repository\r\n>     --no-hardlinks        don't use local hardlinks, always copy\r\n>     -s, --shared          setup as shared repository\r\n>     --recursive           initialize submodules in the clone\r\n>     --recurse-submodules  initialize submodules in the clone\r\n>     --template <template-directory>\r\n>                           directory from which templates will be used\r\n>     --reference <repo>    reference repository\r\n>     -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n>     -b, --branch <branch>\r\n>                           checkout <branch> instead of the remote's HEAD\r\n>     -u, --upload-pack <path>\r\n>                           path to git-upload-pack on the remote\r\n>     --depth <depth>       create a shallow clone of that depth\r\n>     --single-branch       clone only one branch, HEAD or --branch\r\n>     --separate-git-dir <gitdir>\r\n>                           separate git dir from working tree\r\n>     -c, --config <key=value>\r\n>                           set config inside the new repository\r\n> \r\n> + git clone https://github.com/bazelbuild/rules_docker.git /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker\r\n> + git -C /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\n> Unknown option: -C\r\n> usage: git [--version] [--help] [-c name=value]\r\n>            [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n>            [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n>            [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n>            <command> [<args>]\r\n> + git -C /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\n> Unknown option: -C\r\n> usage: git [--version] [--help] [-c name=value]\r\n>            [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n>            [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n>            [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n>            <command> [<args>]\r\n> + git -C /home/USERNAME/.cache/bazel/_bazel_USERNAME/5aa6c3674f37090388bbfbf0ef73a503/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\n> Unknown option: -C\r\n> usage: git [--version] [--help] [-c name=value]\r\n>            [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n>            [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n>            [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n>            <command> [<args>]\r\n> INFO: Elapsed time: 5.267s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (0 packages loaded)", "PART 2: After modifying the WORKSPACE to include the following:\r\n \r\n \r\n```\r\n  # Download the rules_docker repository at release v0.14.1\r\n  http_archive(\r\n      name = \"io_bazel_rules_docker\",\r\n      sha256 = \"dc97fccceacd4c6be14e800b2a00693d5e8d07f69ee187babfd04a80a9f8e250\",\r\n      strip_prefix = \"rules_docker-0.14.1\",\r\n      urls = [\"https://github.com/bazelbuild/rules_docker/releases/download/v0.14.1/rules_docker-v0.14.1.tar.gz\"],\r\n )\r\n\r\n``` \r\n Then the building gets started:\r\n \r\n     vi WORKSPACE \r\n     bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n ", "It makes it through over 20K jobs before failing. You can see from the log that it is using the -std=c++14, which is not supported in gcc 4.8.\r\n\r\nAlso, the reference you have provided doesn't say anything about tensorflow 1.15\r\n\r\n[build-log.txt](https://github.com/tensorflow/tensorflow/files/4230238/build-log.txt)\r\n\r\n", "It looks like mlir was made to require c++14 which is not compatible with gcc 4.8.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commits/r1.15/third_party/mlir/CMakeLists.txt\r\n\r\nIt sounds like there is a way to disable mlir, but I do not understand how.", "I ran into this one while working on this PR: https://github.com/tensorflow/ngraph-bridge/pull/456 and for now I'm sticking with `GCC 5.3` and newer.\r\nI spend way too much time trying to figure out building with `GCC 4.8.5` and gave up for now \ud83d\ude12 ", "@odinsbane please let us know if above comment helps", "@Saduf2019 it pretty much says that tensorflow 1.15 is not compatible with\ngcc 4.8.5. I think the docs should be updated to reflect that. The tested\npage, https://www.tensorflow.org/install/source#tested_build_configurations\nthat you referenced,  should have a tested tf 1.15 and say the accepted gcc\nto use.\n\nCentos7 still uses 4.8.5 by default, and it would be good to know either\nstick with tf 1.14 or upgrade gcc for tf 1.15. I'm surprised tf 1.15 should\nchange the gcc requirement, since it appears to be a legacy version.\n\n\n\nOn Tue, Feb 25, 2020 at 10:21 AM Saduf2019 <notifications@github.com> wrote:\n\n> @odinsbane <https://github.com/odinsbane> please let us know if above\n> comment helps\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36892?email_source=notifications&email_token=AA2NNEPZQRYUGVWZ7TZGW2DRETWJ3A5CNFSM4KXX3I4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM3MVBI#issuecomment-590793349>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA2NNEP6GWXDTCOFFXTCC2TRETWJ3ANCNFSM4KXX3I4A>\n> .\n>\n", "I think this is just a form of discouragement from using tensorflow 1.x. Compatibility has been broken, so one would need to use tensorflow 1.14, where they do not backport or provide fixes because it is outdated.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36892\">No</a>\n"]}, {"number": 36891, "title": "Fix typo in tf.data.Dataset.list_files example code", "body": "This should be `Dataset`, not `dataset`.", "comments": []}, {"number": 36890, "title": "How does the frozen model for speech recognition has been created?", "body": "After following the steps train.py and freeze.py from \"https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md\", The inputs of my frozen model(https://imgur.com/a/JtNVkHw) is quite different from the actual frozen model conv_actions_frozen.pb(https://imgur.com/a/KJXExbV). What changes should I make to get the original frozen model for speech recognition?. \r\nMy Python version is 3.7.3 and Tensor flow version is 2.1.0", "comments": ["@erksch Kindly go through the question", "Hey @vihari1729 \r\nI will try to give a detailed walkthrough (if I remember :D).\r\nI am a little bit occupied at the moment but will try to do it today or tomorrow.\r\n\r\n", "> Hey @vihari1729\r\n> I will try to give a detailed walkthrough (if I remember :D).\r\n> I am a little bit occupied at the moment but will try to do it today or tomorrow.\r\n\r\n@erksch Thanks for the reply. From the tutorial i suspect something need to change in freeze.py. Because in the [netron](https://lutzroeder.github.io/netron/) the structure of the model is changing after the creation of frozen(.pb) model.", "I have the exact same differences in the models on the Netron view, but honestly, it works nonetheless in my case. Have you tried using the model anyway?", "Those are the commands / code I use for freezing and converting to tflite which results in a working model that I can run on android.\r\n\r\nFreezing:\r\n```\r\npython freeze.py \\\r\n--start_checkpoint=/path/to/checkpoint \\\r\n--output_file=/tmp/frozen_graph.pb \\\r\n--data_url='' \\\r\n--data_dir=<path to data> \\\r\n--wanted_words=<comma separated list of wanted words>\r\n```\r\nfreeze.py from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/freeze.py).\r\n\r\nConverting to tflite:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1.lite import TFLiteConverter\r\n\r\nconverter = TFLiteConverter.from_frozen_graph(\r\n    'path/to/frozen_graph.pb', \r\n    input_arrays=['decoded_sample_data', 'decoded_sample_data:1'], \r\n    output_arrays=['labels_softmax'])\r\n\r\nconverter.allow_custom_ops=True\r\nmodel = converter.convert()\r\nopen('model.tflite', 'wb').write(model)\r\n```", "@erksch After following the steps my tflite model looks like [this](https://lutzroeder.github.io/netron/).When I am opening it in android studio by copying it into assets folder the app is getting crashed and not opening.", "@vihari1729 That's just a link to Netron. Can you post an imgur link? :) was is the error you get in android studio after opening your app that crashes it? ", "@erksch Sorry for that.The frozen model which I [got](https://imgur.com/a/JtNVkHw), when converted to tflite model it looks like [this](https://imgur.com/a/uceoHlo), where as the Official frozen [model ](https://imgur.com/a/KJXExbV) when converted looks like [this](https://imgur.com/a/lWmxl9d).\r\n\r\nYeah, It is getting crashed whenever I am opening the app", "I think I have the same models. So I guess it is to debug the android application. What are the errors in your logcat?", "@erksch     I am getting this error in logcat while running tflite model in android studio\r\n\r\n java.lang.RuntimeException: Unable to start activity ComponentInfo{com.google.tflite.speechrecognition/com.google.tflite.speechrecognition.MainActivity}: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Encountered unresolved custom op: AddV2.\r\n    Node number 4 (AddV2) failed to prepare", "Hmm okay, I checked my models and they have indeed no AddV2 nodes.\r\nActually it might be that that's because you are using tensorflow 2.1.0.\r\nMaybe something was changed in that release regarding tflite conversion which breaks the compatibility with the android application. What version of tensorflow lite are you including in your build.gradle on app level? For me it's:\r\n\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:2.0.0'\r\n```\r\n\r\nSo either: \r\n1. try to use the correct tensorflow-lite:2.1.0 java library \r\nor\r\n2. try to freeze and convert again with tensorflow version 2.0.0\r\n\r\nI would prefer you to do number 2, because I can not confirm if the first option works.", "@erksch First of all Thank you very much, Finally it is working.\r\nBut now i did training for 22 variables (12+ zero to nine numbers in the dataset) and freeze it and converted to tflite model.But now when i am running it in android studio, I am getting error like this:\r\n\r\n2020-02-26 18:42:46.015 32272-32335/com.google.tflite.speechrecognition E/AndroidRuntime: FATAL EXCEPTION: Thread-3\r\n    Process: com.google.tflite.speechrecognition, PID: 32272\r\n    java.lang.RuntimeException: The results for recognition should contain 12 elements, but there are 22\r\n        at com.google.tflite.speechrecognition.tflite.RecognizeCommands.processLatestResults(RecognizeCommands.kt:65)\r\n        at com.google.tflite.speechrecognition.MainActivity.recognize(MainActivity.kt:287)\r\n        at com.google.tflite.speechrecognition.MainActivity.access$recognize(MainActivity.kt:60)\r\n        at com.google.tflite.speechrecognition.MainActivity$startRecognition$1.run(MainActivity.kt:241)\r\n        at java.lang.Thread.run(Thread.java:764)\r\n\r\nFor this Do i need to do change anything manually in the MainActivity.kt code or somewhere?\r\nKindly Waiting for your response", "There should be a file containing your labels. I think it is namend labels.txt or something in the assets folder of your android studio project. Make sure to add the labels you are using there. And maybe just search for the number 12 if its hard coded somewhere and replace it with 22. ", "@erksch I am using Kotlin code in android studio instead of java.I have given the labels.txt which has 22 labels in assets folder and. What i understood is i need to change the labels path both in speechInterpreter and MainActivity. But now the app is running and giving the same 10 labels font [page.](https://imgur.com/a/fNRcn9d), Though i trained with 22 labels.\r\n\r\nPS : When i freeze the graph and test the audio files from validation.txt with the command label_wav.py,it is correctly recognising all the 20(+unkown and silence) labels. I also checked the tflite model and the output of labels_softmax showing the shape of the array as (1,22)```", "Great that you have it running!\r\nYeah now you have to start coding and developing your app. The start screen of the example app has the 10 values hardcoded so you have to make adjustments yourself. :D I have no app ready for you that has a frontpage that is dynamic to the labels.\r\n\r\nJust try to understand how everthing works and make more fields for the rest of your labels. ", "Closing this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks!", "I follow the exact same step but when I convert to TFLite I got the following error. Any idea? Similar to this issue [https://stackoverflow.com/questions/59443695/different-model-on-speech-recognition](https://stackoverflow.com/questions/59443695/different-model-on-speech-recognition)\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-24-92cfb1ecefd4> in <module>()\r\n      8 \r\n      9 converter.allow_custom_ops=True\r\n---> 10 model = converter.convert()\r\n     11 open('models/model.tflite', 'wb').write(model)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    225       stdout = _try_convert_to_unicode(stdout)\r\n    226       stderr = _try_convert_to_unicode(stderr)\r\n--> 227       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    228   finally:\r\n    229     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-05-09 01:29:04.994909: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:89] Ignored output_format.\r\n2020-05-09 01:29:04.994950: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:95] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Input arrays can only have op with single output. Node op:DecodeWav"]}, {"number": 36889, "title": "ValueError: We need at least 1 word to plot a word cloud, got 0.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** - Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): - OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: - TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): - Python version: - Bazel\r\nversion (if compiling from source): - GCC/Compiler version (if compiling from\r\nsource): - CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nnormal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]]) \r\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \r\nplt.figure(figsize=(10, 7)) \r\nplt.imshow(wordcloud, interpolation=\"bilinear\") \r\nplt.axis('off') \r\nplt.show()\r\nValueError: We need at least 1 word to plot a word cloud, got 0.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@Jyothif \r\n\r\nLooks like code is incomplete.Can you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "**here is the code**\r\n\r\n\r\nimport re\r\nimport nltk\r\nimport string\r\nimport warnings\r\nimport numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\npd.set_option(\"display.max_colwidth\",200)\r\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\r\n\r\ntrain = pd.read_csv('train_E6oV3lV.csv')\r\ntest = pd.read_csv('test_tweets_anuFYb8.csv')\r\ntrain.head()\r\n\r\ntrain[train['label']==0].head(10)\r\n\r\ntrain[train['label']==1].head(10)\r\n\r\nlength_train = train['tweet'].str.len()\r\nlength_test = test['tweet'].str.len()\r\nplt.hist(length_train,bins = 20,label = \"train_tweets\")\r\nplt.hist(length_test,bins = 20,label = \"test_tweets\")\r\nplt.legend()\r\nplt.show()\r\n\r\ncombi = train.append(test,ignore_index=True)\r\ncombi.shape\r\n\r\ndef remove_pattern(input_txt, pattern):\r\n    r =re.findall(pattern, input_txt)\r\n    for i in r:\r\n        input_txt = re.sub(i,'',input_txt)\r\n    return input_txt\r\n\r\ncombi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'],\"@\\w*\")\r\ncombi.head()\r\n\r\ncombi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\r\ncombi.head(10)\r\n\r\ncombi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\r\ncombi.head()\r\n\r\ntokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) #tokenizing\r\ntokenized_tweet.head()\r\n\r\nfrom nltk.stem.porter import *\r\nstemmer = PorterStemmer()\r\ntokenized_tweet = tokenized_tweet.apply(lambda x:[stemmer.stem(i) for i in x]) #stemming\r\nfor i in range(len(tokenized_tweet)):\r\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\r\ncombi['tidy_tweet'] = tokenized_tweet\r\n\r\nall_words = ' '.join([text for text in combi['tidy_tweet']]) \r\nfrom wordcloud import WordCloud\r\nwordcloud = WordCloud().generate(all_words) \r\nplt.figure(figsize =(10,7))\r\nplt.imshow(wordcloud, interpolation = \"bilinear\")\r\nplt.axis('off')\r\nplt.show()\r\n\r\nI have attached code in my repository please check, if required.\r\n\r\n\r\n    ", "@ravikyram  hey it is resolved...Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36889\">No</a>\n", "> @ravikyram hey it is resolved...Thanks!\r\n\r\nhow did you resolve the problem?, I got the same problem", "please how did you solve it, having the same issue", "please how did you solve it, having the same issue"]}, {"number": 36887, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\python.exe C:/AI/imageClassification.py\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/AI/imageClassification.py\", line 1, in <module>\r\n    from keras.preprocessing.image import ImageDataGenerator\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\ravikumarm\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["I believe this happens because you lack the latest Visual C++ package. You can find it here: \r\nhttps://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\nI recall having the same problem on Windows for TF2.1, but yeah, this error could use some clarification.", "> I believe this happens because you lack the latest Visual C++ package. You can find it here:\r\n> https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n> I recall having the same problem on Windows for TF2.1, but yeah, this error could use some clarification.\r\n\r\nIt worked. Thanks a lot.", "@ravirama12001  Please, refer #36167 and see if it helps you. Thanks!\r\n\r\n\r\nMake sure to download the latest microsoft visual c++ redistributable from here.\r\nMake sure you update environment path for cuda(please refer this https://www.tensorflow.org/install/gpu#windows_setup ). Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from Tensorflow website.\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you. Thanks", "@ravirama12001 please let us know the tensorflow version used and  if we may proceed to move this issue to resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36887\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36887\">No</a>\n"]}, {"number": 36886, "title": "[Docs] update mathjax for lbeta", "body": "This PR fixes some issues in the document of `tf.math.lbeta`, e.g., missing a `\\` before `int`. Besides, the style of characters that appear at both interline and inline formulas should be consistent.  ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36886) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36886) for more info**.\n\n<!-- ok -->"]}, {"number": 36885, "title": "protobuf error when saving model", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04.3 \r\n- TensorFlow installed from: pip\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de\r\n- Python version: 3.7.0 from anaconda\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: GTX 2080 Ti, 11091 MB\r\n\r\n**Describe the current behavior**\r\n\r\nwhen save the model by\r\n```\r\nsaver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=3)\r\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\r\n                            print(\"Saved model checkpoint to {}\".format(path))\r\n```\r\nerror occurs:\r\n\r\n```\r\n[libprotobuf ERROR google/protobuf/wire_format_lite.cc:577] String field 'tensorflow.TensorShapeProto.Dim.name' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 374, in <module>\r\n    train(args, x_train, y_train, vocab_size, x_dev, y_dev)\r\n  File \"cnn.py\", line 279, in train\r\n    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\r\n  File \"xxx/anaconda3/envs/li/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 1203, in save\r\n    save_debug_info=save_debug_info)\r\n  File \"xxx/anaconda3/envs/li/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\", line 1246, in export_meta_graph\r\n    graph_def=ops.get_default_graph().as_graph_def(add_shapes=True),\r\n  File \"xxx/anaconda3/envs/li/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3134, in as_graph_def\r\n    result, _ = self._as_graph_def(from_version, add_shapes)\r\n  File \"xxx/anaconda3/envs/li/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3051, in _as_graph_def\r\n    graph.ParseFromString(compat.as_bytes(data))\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\n\r\n", "comments": ["my protobuf version: protobuf=3.11.3=pypi_0", "when I call ` print(tf.compat.v1.get_default_graph().as_graph_def())`\r\nthis error also appears:\r\n```\r\nFile \"cnn.py\", line 267, in train\r\n    print(tf.compat.v1.get_default_graph().as_graph_def())\r\n  File \"xxx/anaconda3/envs/li/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3135, in as_graph_def\r\n    result, _ = self._as_graph_def(from_version, add_shapes)\r\n  File \"xxx/anaconda3/envs/li/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3052, in _as_graph_def\r\n    graph.ParseFromString(compat.as_bytes(data))\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```", "@yanyanglu, Can you provide the complete code to reproduce the reported issue. Thanks!", "@yanyanglu, I tried it on colab with Tf 2.1 and with Protobuf version 3.10.0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/51980b41e0bdc4346b0fde1be7a8646f/untitled397.ipynb). Can you try with Protobuf 3.10.0. Let us know how it works. Thanks!", "@yanyanglu, Is this still an issue?", "Closing this issue, since its resolved.\r\n Please feel free to open if still issue persists. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36885\">No</a>\n", "i meet this  error too \r\ntensorlfow = 2.2 \r\nprotobuf=3.15.6"]}, {"number": 36884, "title": "sue: Build/Installation Issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@RAHUL9545416 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@RAHUL9545416\r\nplease update on the above comment", "window10 64.8gb ram  grapic card 2bg pyton3.8\r\n", "@RAHUL9545416 \r\nCould you please downgrade your python version and let us know if it helps, please refer to this [link](https://www.tensorflow.org/install/pip) as Tensorflow is only supported till python 3.7 as of now.\r\nIn case you still face errors, as request please share the steps before you faced an error and error logs for us to help you resolve the issue.", "@RAHUL9545416 \r\nplease confirm if the above comment helps\r\nand the tensorflow version used.", "@RAHUL9545416,\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36884\">No</a>\n"]}, {"number": 36883, "title": "Not able to load a saved model with custom layer", "body": "Please make sure that this is a bug. As per our\r\n\r\n**System information** - Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): - OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: - TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): - Python version: - Bazel\r\nversion (if compiling from source): - GCC/Compiler version (if compiling from\r\nsource): - CUDA/cuDNN version: - GPU model and memory:\r\n\r\n\r\nOS - CentOs\r\ntensorflow-gpu = 2.1.0\r\ntensorflow = 2.0.0\r\ninstalled in the conda version - conda version : 4.8.1\r\nusing cuda - CUDA Version: 10.1\r\nGpu - Tesla K80\r\npython - 3.6\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have created a custom Layer and trained the model using that layer, but when I load the model back, it is not able to load all the weight values.\r\n**Describe the expected behavior**\r\nIt should be able to load the weight values.\r\n**Code to reproduce the issue** Provide a reproducible test case that is the\r\nbare minimum necessary to generate the problem.\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nfrom tensorflow.keras import initializers\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras.utils import tf_utils\r\nfrom tensorflow.keras.layers import LeakyReLU\r\nimport tensorflow as tf\r\nimport tensorflow as tf\r\nimport tensorflow.keras\r\nfrom tensorflow.keras.callbacks import *\r\nfrom tensorflow.keras.losses import mse\r\n\r\n### create a custom layer\r\n\r\nclass CustomBatchNormalization(layers.Layer):\r\n    def __init__(self, momentum=0.99, epsilon=1e-3,beta_initializer='zeros',\r\n                 gamma_initializer='ones', moving_mean_initializer='zeros',\r\n                 moving_range_initializer='ones',**kwargs):\r\n        super(CustomBatchNormalization, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.momentum = momentum\r\n        self.epsilon = epsilon\r\n        self.beta_initializer = initializers.get(beta_initializer)\r\n        self.gamma_initializer = initializers.get(gamma_initializer)\r\n        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\r\n        self.moving_range_initializer = (\r\n            initializers.get(moving_range_initializer))\r\n\r\n    \r\n    def build(self,input_shape):\r\n        dim = input_shape[-1]\r\n        shape = (dim,)\r\n        self.gamma = self.add_weight(shape=shape,\r\n                             name='gamma',\r\n                             initializer=self.gamma_initializer,trainable=True)\r\n        self.beta = self.add_weight(shape=shape,\r\n                            name='beta',\r\n                            initializer=self.beta_initializer,\r\n                                   trainable=True)\r\n        \r\n        self.moving_mean = self.add_weight(\r\n            shape=shape,\r\n            name='moving_mean',\r\n            initializer=self.moving_mean_initializer,\r\n            trainable=False)\r\n        \r\n        self.moving_range = self.add_weight(\r\n            shape=shape,\r\n            name='moving_range',\r\n            initializer=self.moving_range_initializer,\r\n            trainable=False)\r\n\r\n\r\n\r\n    def call(self, inputs,training=None):\r\n        input_shape = inputs.shape\r\n        \r\n        if training == False:\r\n            scaled = (inputs-self.moving_mean)/(self.moving_range+self.epsilon)\r\n            return self.gamma*scaled + self.beta\r\n        \r\n        mean = tf.math.reduce_mean(inputs,axis=0)\r\n        maxr = tf.math.reduce_max(inputs,axis=0)\r\n        minr = tf.math.reduce_min(inputs,axis=0)\r\n        \r\n        range_diff = tf.math.subtract(maxr,minr)\r\n        self.moving_mean = tf.math.add(self.momentum*self.moving_mean, (1-self.momentum)*mean)\r\n        self.moving_range = tf.math.add(self.momentum*self.moving_range,(1-self.momentum)*range_diff)\r\n        scaled = tf.math.divide(tf.math.subtract(inputs,mean),(range_diff+self.epsilon))\r\n        return tf.math.add(tf.math.multiply(self.gamma,scaled),self.beta)\r\n    \r\n    def get_config(self):\r\n        config = {\r\n            'momentum': self.momentum,\r\n            'epsilon': self.epsilon,\r\n            'beta_initializer': initializers.serialize(self.beta_initializer),\r\n            'gamma_initializer': initializers.serialize(self.gamma_initializer),\r\n            'moving_mean_initializer':\r\n                initializers.serialize(self.moving_mean_initializer),\r\n            'moving_range_initializer':\r\n                initializers.serialize(self.moving_range_initializer)\r\n        }\r\n        base_config = super(CustomBatchNormalization, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n    \r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n\r\n### create your model\r\ninp = Input(shape=(4,))\r\nbatch_norm_1 = CustomBatchNormalization(dynamic=True)(inp)\r\ndensout = Dense(24, activation='linear')(batch_norm_1)\r\ndensout = LeakyReLU(alpha=0.3)(densout)\r\nbatch_norm_2 = CustomBatchNormalization(dynamic=True)(densout)\r\ndensout = Dense(128, activation='linear')(batch_norm_2)\r\ndensout = LeakyReLU(alpha=0.3)(densout)\r\nbatch_norm_out = CustomBatchNormalization(dynamic=True)(densout)\r\nout = Dense(5, activation='linear')(batch_norm_out)\r\ntest_nw = tf.keras.models.Model(inp, out)\r\n\r\n##compile it\r\ntest_nw.compile(tf.keras.optimizers.Adam(), loss=mse,experimental_run_tf_function=False)\r\n\r\npath_HDF5 = 'PATH_TO_SAVE_THIS_MODEL'\r\nearlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\r\nmcp_save_RH = ModelCheckpoint(path_HDF5,save_best_only=True, monitor='val_loss', mode='min')\r\n\r\nX = np.random.randn(4,4)\r\ny = np.random.randn(4,5)\r\nX_val = np.random.randn(4,4)\r\ny_val = np.random.randn(4,5)\r\ntest_nw.fit(X,y,batch_size=4, epochs=10,validation_data = (X_val,y_val),callbacks=[earlyStopping, mcp_save_RH] )\r\n\r\n######## Now restart the kernel and load the model\r\ndict_lay = {'CustomBatchNormalization':CustomBatchNormalization}\r\nmod = load_model(path_HDF5,custom_objects=dict_lay)\r\n```\r\n\r\nI get an error saying that\r\n ```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-8655e6764114> in <module>\r\n----> 1 mod = load_model(path_HDF5+'CI01_RH_CBN_test.hdf5',custom_objects=dict_lay)\r\n\r\n~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    144   if (h5py is not None and (\r\n    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\r\n--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n    147 \r\n    148   if isinstance(filepath, six.string_types):\r\n\r\n~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    169 \r\n    170     # set weights\r\n--> 171     load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n    172 \r\n    173     if compile:\r\n\r\n~/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)\r\n    695                        str(len(symbolic_weights)) +\r\n    696                        ' weights, but the saved weights have ' +\r\n--> 697                        str(len(weight_values)) + ' elements.')\r\n    698     weight_value_tuples += zip(symbolic_weights, weight_values)\r\n    699   K.batch_set_value(weight_value_tuples)\r\n\r\nValueError: Layer #0 (named \"custom_batch_normalization_17\" in the current model) was found to correspond to layer custom_batch_normalization_17 in the save file. However the new layer custom_batch_normalization_17 expects 4 weights, but the saved weights have 2 elements.\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ankitesh97 I have executed your code on tensorflow 2.0 please find the gist [here](https://colab.sandbox.google.com/gist/Saduf2019/52f7e8ca6a28871831992b2dc3013b05/36883_2-0.ipynb) the issue does not exist in 2.0, also executed it on tensorflow 2.1 gist for the [same](https://colab.sandbox.google.com/gist/Saduf2019/a9d49a5f67faf7e8ffb05a331a69aeb7/36883_2-1.ipynb) where the error is not the same as faced by you.", "Thanks, for the reply. I downgraded my TensorFlow to 2.0 it is working now. I don't know what error is happening in tf 2.1 though.", "This works successfully with tf-nighly version '2.2.0-dev20200310'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36883\">No</a>\n"]}, {"number": 36882, "title": "Is NonMaxSuppressionV4 supported\uff1f", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow 2.1.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n```\r\nHere is a list of operators for which you will need custom implementations: NonMaxSuppressionV4.\r\n```\r\n\r\n**Code**\r\n```\r\nselected_indices_padded,  num_valid = tf.image.non_max_suppression(boxes=boxes_squeezed,\r\n                                                  scores=scores,\r\n                                                  max_output_size=20,\r\n                                                  score_threshold=0.7,\r\n                                                  iou_threshold=0.5,\r\n                                                  pad_to_max_output_size=True,\r\n                                                  )\r\n```\r\n\r\nAccording to https://www.tensorflow.org/lite/guide/ops_compatibility,  `NON_MAX_SUPPRESSION_V4` was already supported.\r\nHowever,  when I use it to build my model and try to convert it into tflite, the error came out.\r\n\r\nI tried a solution with `tf.lite.OpsSet.SELECT_TF_OPS`, and the error gone.\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nBut I cannot run it properly with native TFLite intepreter... Another error showed as below:\r\n\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 92 (FlexNonMaxSuppressionV4) failed to prepare.\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Use new MLIR converter:\r\n\r\nconverter.experimental_new_converter = True", "> Use new MLIR converter:\r\n> \r\n> converter.experimental_new_converter = True\r\n\r\nIt works. Thank you very much", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36882\">No</a>\n"]}, {"number": 36881, "title": "How could I can add a new OS on the support on the \"tested and supported\" list?", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/install\r\n\r\n## Description of issue (what needs changing):\r\nCould someone instruct me how to add a new OS on the support on the \"tested and supported\" list, I would like to make some efforts for Opensuse and SLE. \r\n\r\nThere might have some steps to make sure an OS is \"tested and supported\", right?\r\n\r\n", "comments": ["@Hui-Zhi \r\nOpensuse and SLE are Linux flavours. Can you please follow the instructios in [TensorFlow website](https://www.tensorflow.org/install/source) and see how it progresses. Thanks!", "Yes, you're right, I should not use new OS, thanks for figuring it out. And Ubuntu is also Linux flavors. On this website, it only have instructions for Ubuntu and Mac OS, what I am trying to do is adding Opensuse and SLE to the \"tested and supported\" list to give users more options, sure I believe the instructions for Opensuse and SLE is needed, and it's different from Ubuntu and Mac.\r\n\r\nBut what I am not sure is should I just add Opensuse and SLE to the website source code (instructions will also be updated), the make a pull request? Or there has some previous steps should be done? \r\n\r\nI don't even know if the community would like to accept this change.", "Hi @Hui-Zhi,\r\n\r\nThe tensorflow.org/install pages are for officially-supported install and build configs by the TensorFlow team. So we should not add new OSes here.\r\n\r\nBut organizing an \"extended\" support list sounds like it could fall under [SIG Build](https://www.tensorflow.org/community/forums). Maybe send a message to the [build@tensorflow.org list](https://groups.google.com/a/tensorflow.org/forum/#!forum/build)?\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36881\">No</a>\n", "Our python packages are built according to the \"manylkinux2010\" standard.\r\nSo if you use the prebuilt packages from pypi, they should work on 90% of the linux distributions."]}, {"number": 36880, "title": "No gradients for bias and kernels in Dense layer. ", "body": "**System information** - Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): - Yes\r\nOS: Mac OS Catalina \r\nTensorFlow installed from: pip\r\nTensorFlow version:  2.1 \r\n Python version: - 3.7\r\n\r\n\r\nI'm getting this error, \r\n\r\n`**WARNING:tensorflow:Gradients do not exist for variables ['flight_2/layer1/kernel:0', 'flight_2/layer1/bias:0', 'flight_2/layer2/kernel:0', 'flight_2/layer2/bias:0', 'flight_2/layer3/kernel:0', 'flight_2/layer3/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['flight_2/layer1/kernel:0', 'flight_2/layer1/bias:0', 'flight_2/layer2/kernel:0', 'flight_2/layer2/bias:0', 'flight_2/layer3/kernel:0', 'flight_2/layer3/bias:0'] when minimizing the loss.**\r\n\r\n\r\nHere's my full code, which is enough for re-producing problem\r\n```\r\n\r\nimport dask.dataframe as dd\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\ntf.keras.backend.set_floatx('float64')\r\nimport time\r\npd.options.display.float_format = '{:,.2f}'.format\r\nfrom scipy import stats\r\nimport matplotlib as mpl\r\nmpl.rcParams['agg.path.chunksize'] = 10000\r\nimport os\r\n\r\n\r\n\r\n\r\nfrom tensorflow.keras.layers import LSTM, GRU\r\n\r\n\r\n\r\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\r\n\r\n\r\n%matplotlib inline\r\n\r\ntf.__version__\r\n\r\n# data = dd.read_csv(\"s3://search-curated-sec/bom_pat_6e_aberogate_new.csv/*.csv\")\r\ndata = pd.read_csv('../data/bom_pat_6e_abrogate.csv')\r\n\r\ndata.dropna(inplace=True)\r\n\r\ndata['deptime'] = data['deptime'].astype(str).str.pad(4, fillchar='0')\r\ndata['depinf'] = data['depdate'].astype(str) + data['deptime'].astype(str)\r\nfor i in ['booktime','changeDateTime_1', 'changeDateTime_2',\r\n       'changeDateTime_3', 'changeDateTime_4', 'changeDateTime_5',\r\n       'changeDateTime_6', 'changeDateTime_7', 'changeDateTime_8',\r\n       'changeDateTime_9', 'changeDateTime_10', 'aberogateTime']:\r\n    data[i] = pd.to_datetime(data[i], format = '%Y%m%d%H')\r\n\r\ndata['depinf'] = pd.to_datetime(data['depinf'], format='%Y%m%d%H%M')\r\n\r\ndata.drop(['bookdate', 'depdate', 'deptime'], axis=1, inplace=True)\r\n\r\ndata.head()\r\n\r\ndata['diff_book_3'] = data.apply(lambda x: (x['changeDateTime_3'] - x['booktime']).total_seconds(), axis=1)\r\n\r\ndata.to_csv('../data/bom_pat_6e_abrogate.csv', index=False)\r\n\r\ndata[data['diff_book_3']>=0].shape\r\n\r\ndata.shape\r\n\r\ndata['diff_book_3'] = (pd.to_datetime(data['changeDateTime_3']) - pd.to_datetime(data['booktime'])).dt.total_seconds() // 3600\r\n\r\n### Features\r\n\r\ndata.columns\r\n\r\nlbl1 = LabelEncoder()\r\nstd = StandardScaler()\r\n\r\ndata['trans_fno'] = lbl1.fit_transform(data['fno'])\r\n\r\nfeatures = np.array([data['duration'], data['htg'], data['fare'],  data['trans_fno'], pd.to_datetime(data['booktime']).dt.month, pd.to_datetime(data['booktime']).dt.day, pd.to_datetime(data['depinf']).dt.month, pd.to_datetime(data['depinf']).dt.day]).T\r\n\r\ntarget = data['diff_book_3'].values\r\n\r\n#Standardizing all features\r\nfeatures = std.fit_transform(features)\r\n\r\n# Starting Model\r\n\r\nclass Flight(tf.keras.Model):\r\n    def __init__(self, layer1_units = 512, layer2_units = 256, layer3_units = 128):\r\n        super(Flight, self).__init__()\r\n        self.layer1 = layer1_units\r\n        self.layer2 = layer2_units\r\n        self.layer3 = layer3_units\r\n\r\n        \r\n        self.linear1 = tf.keras.layers.Dense(self.layer1, activation='relu', input_shape = (9, ), kernel_regularizer=tf.keras.regularizers.l2(1e-3), name='layer1')\r\n        self.linear2 = tf.keras.layers.Dense(self.layer2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3), name='layer2')\r\n        self.linear3 = tf.keras.layers.Dense(self.layer3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3), name='layer3')\r\n        self.linear4 = tf.keras.layers.Dense(1, activation='linear', kernel_regularizer=tf.keras.regularizers.l2(1e-3), name='layer4')\r\n\r\n\r\n    def call(self, x):\r\n        output = self.linear1(x)\r\n        output = self.linear2(x)\r\n        output = self.linear3(x)\r\n        output = self.linear4(x)\r\n        return output\r\n\r\n#### Creating a tf.data dataset \r\n\r\nbatch_size = 10000\r\n\r\n\r\ndata_size = len(features)\r\nsteps_per_epoch = len(features)//batch_size\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((features, target))\r\ndataset = dataset.batch(batch_size, drop_remainder=True)\r\n\r\n#Checking sample data size\r\nexample_input_batch, example_target_batch = next(iter(dataset))\r\nprint(\"This is sample input_batch shape and target_batch shape: \",example_input_batch.shape, example_target_batch.shape)\r\n\r\nflight = Flight()\r\nsample_output = flight(example_input_batch)\r\n\r\nfor l in flight.layers:\r\n    print(l.name, l.trainable)\r\n\r\nsample_output = flight(example_input_batch)\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss_calc = tf.keras.losses.MeanAbsoluteError()\r\n\r\n# Saving checkpoints\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n                                flight = flight)\r\n\r\n@tf.function\r\ndef train_step(inp, targ):\r\n    loss = 0\r\n\r\n    with tf.GradientTape() as tape:\r\n        predicted = flight(inp)\r\n\r\n        loss = loss_calc(targ, predicted)\r\n        \r\n        variables = flight.variables\r\n\r\n        gradients = tape.gradient(loss, variables)\r\n\r\n        optimizer.apply_gradients(zip(gradients, variables))\r\n\r\n    return loss\r\n\r\nepochs = 5\r\nfor epoch in range(epochs):\r\n        start = time.time()\r\n\r\n#         flight_hidden = flight.initialize_hidden_state()\r\n        total_loss = 0\r\n\r\n        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\r\n            batch_loss = train_step(inp, targ)\r\n            total_loss += batch_loss\r\n\r\n        # saving (checkpoint) the model every 2 epochs\r\n        checkpoint.save(file_prefix = checkpoint_prefix)\r\n\r\n        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\r\n                                        total_loss / steps_per_epoch))\r\n        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\r\n\r\n```\r\n\r\n\r\nI've added a sample dataset for reproducing the problem. \r\n[tensorflow_issue.csv.zip](https://github.com/tensorflow/tensorflow/files/4223717/tensorflow_issue.csv.zip)\r\n\r\n\r\n", "comments": ["Your model's call method returns the output of `layer4(x)`, meaning that the first three layers have no effect on the predictions, hence the absence of gradients.\r\n\r\nYou probably wanted to write :\r\n```python\r\ndef call(self, x):\r\n    output = self.linear1(x)\r\n    output = self.linear2(output)\r\n    output = self.linear3(output)\r\n    output = self.linear4(output)\r\n    return output\r\n```\r\nso that all four layers are being used sequentially in processing the inputs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36880\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36880\">No</a>\n"]}, {"number": 36879, "title": "Basic LSTM TFLite model created with new experimental converter results in model with incorrect output shape", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0-dev20200218\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"mnist_lstm_f32.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nConversion is successful\r\n```\r\n\r\n**Failure details**\r\nI am creating a basic MNIST classification model which includes a single LSTM layer. Using the new experimental converter option, the model can be converted to TFLite format successfully and inferring from the TFLite model using the Python Interpreter API gives correct results.\r\n\r\nHowever, the model's output shape is just an empty array and running the model on Android required the output buffer size to be specified so the model crashes. I expect the output shape to be ```[1, 10]``` however that is not the case as shown below.\r\n```\r\n[{'name': 'Identity', 'index': 52, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n[Colab notebook](https://colab.research.google.com/drive/1X5B2O2aF9HlVX0GHl1IAASPqrz45ujff)\r\n", "comments": ["@renjie-liu could you take a look?", "{'name': 'input', 'index': 0, 'shape': array([ 1, 28, 28], dtype=int32), 'shape_signature': array([-1, 28, 28], dtype=int32),\r\n\r\nlooks like it is because the recent change that we no longer hard code the batch size to be 1.\r\n\r\nHi Nupur, do you have any workaround for Keras model?\r\n\r\nThanks!", "@bilalsoomro This was resolved in recent `tf-nightly`. When I use `tf-nightly` the code correctly output the output shape as [1, 10].  Please find the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5e9c836af6fe32405f7544f4652f98aa/mnist.ipynb).\r\n\r\n### Previous output\r\n\r\n`[{'name': 'Identity', 'index': 33, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]`\r\n\r\n### New output \r\n\r\n`[{'name': 'Identity', 'index': 37, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]`\r\n\r\n\r\nI am closing the issue as this was resolved. Please feel free to reopen if the issue persists for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36879\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36879\">No</a>\n"]}, {"number": 36878, "title": "Tflearn import error in Tensorflow 2.1: \"ModuleNotFoundError: No module named 'tensorflow.contrib'\"", "body": "So, basically, I am trying to migrate the entire code to TF 2.1.0. However, it seems that **tensorflow.contrib** has been deprecated. Due to this, importing tflearn always throws an error of the missing module of tf.contrib. So, how am I supposed to implement this code without using tflearn?\r\n```python\r\ndef get_vocab_processor(model_dir):\r\n    vocab_path = os.path.join(model_dir, \"..\", \"vocab\")\r\n    #vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)\r\n    #tflearn import error. tflearn support removed from Tensorflow 2.1.0\r\n    import tflearn\r\n    vocab_processor = tflearn.data_utils.VocabularyProcessor.restore(filename=vocab_path)\r\n    return vocab_processor\r\n```\r\n\r\nAnd then, I use this method like this:\r\n```python\r\ndef process(FLAGS, dataset):\r\n    x_raw, y_test = data_helpers.load_data_labels(dataset)\r\n    y_test = np.argmax(y_test, axis=1)\r\n\r\n    vocab_processor = get_vocab_processor(FLAGS.model_dir)\r\n    x_test = np.array(list(vocab_processor.transform(x_raw)))\r\n\r\n    model_file = tf.train.latest_checkpoint(FLAGS.model_dir)\r\n```\r\n**Edit:**\r\n**This is the error I get after execution:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\koustubh.p\\Downloads\\multiple-signal-detection\\multiple-signal-detection\\churn_updated.py\", line 179, in <module>\r\n    process(FLAGS, dataset)\r\n  File \"c:\\Users\\koustubh.p\\Downloads\\multiple-signal-detection\\multiple-signal-detection\\churn_updated.py\", line 110, in process\r\n    vocab_processor = get_vocab_processor(FLAGS.model_dir)\r\n  File \"c:\\Users\\koustubh.p\\Downloads\\multiple-signal-detection\\multiple-signal-detection\\churn_updated.py\", line 102, in get_vocab_processor\r\n    import tflearn\r\n  File \"c:\\Users\\koustubh.p\\Downloads\\multiple-signal-detection\\multiple-signal-detection\\venv\\lib\\site-packages\\tflearn\\__init__.py\", line 4, in <module>\r\n    from . import config\r\n  File \"c:\\Users\\koustubh.p\\Downloads\\multiple-signal-detection\\multiple-signal-detection\\venv\\lib\\site-packages\\tflearn\\config.py\", line 5, in <module>\r\n    from .variables import variable\r\n  File \"c:\\Users\\koustubh.p\\Downloads\\multiple-signal-detection\\multiple-signal-detection\\venv\\lib\\site-packages\\tflearn\\variables.py\", line 7, in <module>\r\n    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n```", "comments": ["tensorflow.contrib has been removed from tensorflow after 1.15\r\n", "I encountered that error when I tried to run code based on TF 1.x using TF 2.1. As hinted by @ayushmankumar7, a potential resolution (which did fix the issue in my case) is to downgrade from 2.1, and you may need to iterate through a few releases (I tried TF 1.13.2 after 1.15, 1.14, and 1.13.1 did not resolve the issue).\r\nAlternatively, you may want to consult the TF 1 to TF2 migration guide: https://www.tensorflow.org/guide/migrate", "@KoustubhPhalak Agree with @ayushmankumar7. You could try installing `TF1.14` and use `tflearn`. I think this issue more close to `tflearn` repo. There is another [issue](https://github.com/tensorflow/tensorflow/issues/30794) that was closer to yours. \r\n\r\nI am closing this issue as it is more related to `tflearn' and can be resolved by installing `TF1.x` verison. Thanks!\r\n", "tensorflow.contrib is being removed in version 2.0, you therefore need version <= 1.14 to operate tflearn (this is a TFlearn issue, not a tensorflow one).\r\n\r\nso in your case, I would consider moving to tensorflow and using the tf.keras , which provides the higher-level API tflearn\r\n\r\nyou have to solutions to solve this:\r\n-1 move to tensorflow and keras\r\n-2 uninstall tensorflow which you have and install  \"  conda  or  pip  install tensorflow=1.14  ''\r\n\r\ngood luck", "Yeah, will go ahead with TF 1.14\n\nOn Tue, Apr 7, 2020, 1:25 PM Hardy <notifications@github.com> wrote:\n\n> tensorflow.contrib is being removed in version 2.0, you therefore need\n> version <= 1.14 to operate tflearn (this is a TFlearn issue, not a\n> tensorflow one).\n>\n> so in your case, I would consider moving to tensorflow and using the\n> tf.keras , which provides the higher-level API tflearn\n>\n> you have to solutions to solve this:\n> -1 move to tensorflow and keras\n> -2 uninstall tensorflow which you have and install \" conda or pip install\n> tensorflow=1.14 ''\n>\n> good luck\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36878#issuecomment-610236114>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AF7UAZLGRG3WGVJE3VS4DBTRLLL75ANCNFSM4KXTKBTQ>\n> .\n>\n", "I cannot install tensorflow ==1.14\r\ncommand: pip install tensorflow==1.14\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)\r\nERROR: No matching distribution found for tensorflow==1.14\r\n\r\nNeed help\r\n", "The wheel files have been deleted\r\nhttps://pypi.org/project/tensorflow/#files\r\n![image](https://user-images.githubusercontent.com/48051237/88916694-1907eb00-d299-11ea-831c-55b7f7f7ca1b.png)\r\nIt does not exist anymore, which causes the following issue:\r\n![image](https://user-images.githubusercontent.com/48051237/88916652-08f00b80-d299-11ea-9f56-b043e73f5280.png)"]}, {"number": 36877, "title": "Memory leak in TensorFlow 2.0 DataSet when using group_by_window.", "body": "**System information**\r\n- OS Platform: - Google Cloud Linux Ubuntu 16.04 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.01\r\n- GPU model and memory: Tesla P100, 16GB\r\n- Running on GRAPH MODE:- YES\r\n\r\nThis is creating memory leak.\r\n```\r\ndef pairwise_batch_iterator(tf_records,\r\n                           no_threads=14,\r\n                           batch_size=64,\r\n                           num_epochs=50):\r\n    \r\n    dataset = make_dataset(tf_records, no_threads)\r\n    dataset = dataset.repeat(num_epochs)\r\n    \r\n    dataset = dataset.apply(tf.data.experimental.group_by_window(\r\n        key_func=lambda elem, *args: elem,\r\n        reduce_func=lambda _, window: window.batch(batch_size),\r\n        window_size=batch_size))\r\n    \r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    \r\n    return dataset\r\n```\r\n\r\nThis works fine\r\n```\r\ndef pairwise_batch_iterator(tf_records,\r\n                           no_threads=14,\r\n                           batch_size=64,\r\n                           num_epochs=50):\r\n    \r\n    dataset = make_dataset(tf_records, no_threads)\r\n    dataset = dataset.repeat(num_epochs)\r\n    \r\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n    \r\n    return dataset\r\n```\r\n***\r\nI am training a pairwise ranking model where I have to group by query id that's why I am using tf.data.experimental.group_by_window and this is creating a memory leak. If I use the second version of code I don't face any issue but I have to group by query id.\r\n***\r\n", "comments": ["@akanyaani,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Could you change \r\n\r\n   `buffer_size=tf.data.experimental.AUTOTUNE`\r\n\r\nto a fixed size and see if the problem persists?", "@akanyaani,\r\nAny updates regarding the reproducible code? Also, could you please check @plooney's comment and check if it works. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36877\">No</a>\n", "Hi, @amahendrakar I was out of town, now I have updated the issue please reopen it.", "@akanyaani,\r\nCould you please provide the complete code to reproduce the issue reported here? Thanks!", "Hi @amahendrakar ,\r\n\r\nI have created a new issue, link is:-https://github.com/tensorflow/tensorflow/issues/37969", "Hello @akanyaani,\r\nSince the issue is being tracked there, can we mark this as a duplicate and close this issue? Thanks!", "Hi @amahendrakar, Yes you can mark it as duplicate.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36877\">No</a>\n"]}, {"number": 36876, "title": "Add leaky_relu operator property", "body": "Adding Leaky_Relu into operator property, which was missing, although quantized leaky relu was implemented. This resolve the issue: https://github.com/tensorflow/tensorflow/issues/33397", "comments": ["Thanks."]}, {"number": 36875, "title": "Add a Model.fit-like API for seq2seq models", "body": "Many models can be trained by `tf.keras.Model.fit` API, which is very convenient. But for seq2seq models, we have to use a custom training loop. As we know, seq2seq Models are used very often, so why not build a API to train these models in a convenient way?", "comments": ["Hi,\r\nCould you please elaborate on this statement, and maybe provide a small example?\r\nI personnally use custom sequence-to-sequence models and have no problem whatsoever wrapping them up in keras and using the built-in training loop, so at this point I do not quite understand your issue, but surely I am missing something.", "Sorry, I didn't make it clear. I mean, I have to use a loop to generate the whole sequence during the prediction in seq2seq models.\r\nHere is an example from [Transformer](https://www.tensorflow.org/tutorials/text/transformer#evaluate):\r\n\r\n```python\r\ndef evaluate(inp_sentence):\r\n  start_token = [tokenizer_pt.vocab_size]\r\n  end_token = [tokenizer_pt.vocab_size + 1]\r\n  \r\n  # inp sentence is portuguese, hence adding the start and end token\r\n  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\r\n  encoder_input = tf.expand_dims(inp_sentence, 0)\r\n  \r\n  # as the target is english, the first word to the transformer should be the\r\n  # english start token.\r\n  decoder_input = [tokenizer_en.vocab_size]\r\n  output = tf.expand_dims(decoder_input, 0)\r\n    \r\n  for i in range(MAX_LENGTH):\r\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\r\n        encoder_input, output)\r\n  \r\n    # predictions.shape == (batch_size, seq_len, vocab_size)\r\n    predictions, attention_weights = transformer(encoder_input, \r\n                                                 output,\r\n                                                 False,\r\n                                                 enc_padding_mask,\r\n                                                 combined_mask,\r\n                                                 dec_padding_mask)\r\n    \r\n    # select the last word from the seq_len dimension\r\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\r\n\r\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n    \r\n    # return the result if the predicted_id is equal to the end token\r\n    if predicted_id == tokenizer_en.vocab_size+1:\r\n      return tf.squeeze(output, axis=0), attention_weights\r\n    \r\n    # concatentate the predicted_id to the output which is given to the decoder\r\n    # as its input.\r\n    output = tf.concat([output, predicted_id], axis=-1)\r\n\r\n  return tf.squeeze(output, axis=0), attention_weights\r\n```\r\n\r\nObviously, this process can not be finished by `Model.predict` API.\r\n\r\nWhat i want is, once i create my seq2seq model, I can use something like `predict_loop` method to finish this process.\r\n\r\n```python\r\n\r\nclass Seq2Seq(tf.keras.Model):\r\n    pass\r\n\r\nmodel = Seq2Seq()\r\n# training ... \r\n\r\ngenerated_sequence = model.predict_loop(inputs, callbacks,...)\r\n\r\n```", "Oh, I see! So, your problem is indeed with the prediction loop rather than the training one?\r\n\r\nIn one of my personal working cases, I implemented a custom looping prediction method for the Layer that wraps my model's architecture, but having a general mechanism available in TF would be great. Let's wait for a tensorflower to pick up this issue and give their opinion and expertise on the topic then :)", "Adding @qlzh727 who is a seq2seq model expert to advice on this. Thank you!", "Consider creating a subclass of `Model` that overrides the training step (see method `_train_step`) or the `fit` loop. This is now very easy to do. You'll be able to use such a `Model` subclass to create new models either via the Functional API or via subclassing.", "@fchollet Thank you for the tip. I was personally unaware of the renewed API of `Model`, and it feels amazing. To what extent may we (i.e. individual users) consider `_train_step` a stable API feature? I have been using a homemade gradient stacking framework through a keras Model wrapper for a few months, but re-implementing its core mechanism as a specific `Model` subclass using `_train_step` would be so much more elegant (and would make it possible to suggest it through a PR submission).", "@fchollet @pavithrasv thanks, I will have a try."]}, {"number": 36874, "title": "time of tensorflow1.14 built from source perfroms worse than tensorflow1.14 from pypi", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version:1.14.0\r\n- Python version:3.6.2\r\n- Installed using :pip\r\n- Bazel version: 0.25.2\r\n- GCC/Compiler version: 5.4.0\r\n- CPU: Intel Core i9-9900K\r\n\r\n\r\nI want to speed up CPU inference on trained Mask R-CNN pb model.  So, I followed instructions on tensorflow webpage [here](https://www.tensorflow.org/install/source) and method from intel [here](https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide#linux_B_S).  \r\n\r\n**Here is .tf_configure.bazelrc after running ./configure**\r\n- build --action_env PYTHON_BIN_PATH=\"/home/allen/miniconda3/envs/tf1_mkl/bin/python\"\r\n- build --action_env PYTHON_LIB_PATH=\"/home/allen/miniconda3/envs/tf1_mkl/lib/python3.6/site-packages\"\r\n- build --python_path=\"/home/allen/miniconda3/envs/tf1_mkl/bin/python\"\r\n- build:xla --define with_xla_support=true\r\n- build --config=xla\r\n- build --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\n- build --action_env TF_NEED_ROCM=\"0\"\r\n- build --action_env TF_NEED_CUDA=\"0\"\r\n- build --action_env TF_DOWNLOAD_CLANG=\"0\"\r\n- build:opt --copt=-march=native\r\n- build:opt --copt=-Wno-sign-compare\r\n- build:opt --host_copt=-march=native\r\n- build:opt --define with_default_optimizations=true\r\n- build:v2 --define=tf_api_version=2\r\n- test --flaky_test_attempts=3\r\n- test --test_size_filters=small,medium\r\n- test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\n- test --build_tag_filters=-benchmark-test,-no_oss\r\n- test --test_tag_filters=-gpu\r\n- test --build_tag_filters=-gpu\r\n- build --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n**Here is my bazel build command:**\r\n`bazel build --config=mkl --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-march=sandybridge --copt=-mtune=ivybridge --copt=-O3 //tensorflow/tools/pip_package:build_pip_package\r\n`\r\n**Next, I generated whl and installed successfully.**\r\nBut time of sess.run of Mask R-CNN is about 1s per image, which is worse than tf1.14 installed from pypi.\r\n**Here is the output using tensorflow installed from source **\r\n\r\n> WARNING:tensorflow:\r\n> The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\n> For more information, please see:\r\n>   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n>   * https://github.com/tensorflow/addons\r\n>   * https://github.com/tensorflow/io (for I/O related ops)\r\n> If you depend on functionality not listed there, please file an issue.\r\n> \r\n> WARNING:tensorflow:From time_compare.py:314: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n> \r\n> WARNING:tensorflow:From time_compare.py:43: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.gfile.GFile.\r\n> W0219 12:14:37.498566 140296832321280 deprecation.py:323] From time_compare.py:43: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.gfile.GFile.\r\n> WARNING:tensorflow:From time_compare.py:44: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\r\n> \r\n> W0219 12:14:37.498702 140296832321280 module_wrapper.py:139] From time_compare.py:44: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\r\n> \r\n> Graph loaded.\r\n> WARNING:tensorflow:From time_compare.py:50: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n> \r\n> W0219 12:14:38.048038 140296832321280 module_wrapper.py:139] From time_compare.py:50: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n> \r\n> 2020-02-19 12:14:38.048334: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\r\n> To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-02-19 12:14:38.070567: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n> 2020-02-19 12:14:38.071554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed8c786280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-02-19 12:14:38.071582: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-02-19 12:14:38.072303: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n> Image No.0 time cost:2.7566s/home/allen/ytzx/dataset/cwfp/v4/val2014/001002_r180_81445.jpg, time:2.756582736968994\r\n> Image No.1 time cost:0.9806s/home/allen/ytzx/dataset/cwfp/v4/val2014/001006_r270_80042.jpg, average time:0.9805862903594971\r\n\r\n**And here is inference output using tensorflow installed from pypi**\r\n\r\n> WARNING:tensorflow:From time_compare.py:314: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n> \r\n> WARNING:tensorflow:From time_compare.py:43: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.gfile.GFile.\r\n> W0219 12:15:57.491406 139925355824896 deprecation.py:323] From time_compare.py:43: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.gfile.GFile.\r\n> WARNING:tensorflow:From time_compare.py:44: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\r\n> \r\n> W0219 12:15:57.491531 139925355824896 deprecation_wrapper.py:119] From time_compare.py:44: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\r\n> \r\n> Graph loaded.\r\n> WARNING:tensorflow:From time_compare.py:50: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n> \r\n> W0219 12:15:58.086142 139925355824896 deprecation_wrapper.py:119] From time_compare.py:50: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n> \r\n> 2020-02-19 12:15:58.086468: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2020-02-19 12:15:58.110562: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n> 2020-02-19 12:15:58.111640: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56350f20bfb0 executing computations on platform Host. Devices:\r\n> 2020-02-19 12:15:58.111652: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n> 2020-02-19 12:15:58.891970: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n> Image No.0 time cost:1.2104s/home/allen/ytzx/dataset/cwfp/v4/val2014/001002_r180_81445.jpg, time:1.2104294300079346\r\n> Image No.1 time cost:0.376s/home/allen/ytzx/dataset/cwfp/v4/val2014/001006_r270_80042.jpg, average time:0.3759894371032715\r\n> \r\nAny suggestions or thoughts would be appreciated.\r\n", "comments": ["@allen-ash please note during the bazel build command: everything is done from scratch and the whl file is generated, which is not the case during pip installation, hence the time difference.", "@Saduf2019 Thanks for your replay. Yes I understand that, I just want to know In which way from scratch can be optimizing time cost. \r\nConsidering currently command is `bazel build --config=mkl --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-march=sandybridge --copt=-mtune=ivybridge --copt=-O3 //tensorflow/tools/pip_package:build_pip_package`  didn't perfrom well.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36874\">No</a>\n"]}, {"number": 36873, "title": "HybridStraregy (ParameterServerStrategy  + CollectiveAllReduceStrategy)", "body": "Implemented HybridStrategy by mixing ParameterServerStrategy and CollectiveAllReduceStrategy\r\n\r\n**Major feature:**\r\n  - Only in embedding layer case, HybridStrategy creates variable on PS. And in other cases, it creates Mirrored variables.\r\n  - HybridStrategy chooses the different ways to reduce depending on the variable type.\r\n  - ParameterServerStrategy trains model asynchronously. So i added codes to synchronize gradient updates in optimizer_v2.py file.\r\n\r\n**Minor feature:**\r\n- added trainable property and value property to AggregatingVariable", "comments": ["This paper maybe helpful:\r\nDistributed Equivalent Substitution Training for Large-Scale Recommender Systems\r\nhttps://arxiv.org/abs/1909.04823 "]}, {"number": 36872, "title": "How to quant TFLite model in int8 format", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   -- x86 Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\n   -- binary\r\n- Tensorflow version (commit SHA if source):\r\n   -- 2.1.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n   -- x86 Linux\r\n\r\n**Describe the problem**\r\n   -- How to quant TF model to int8 format so that I can use CMSIS-NN int8 APIs\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n   -- I try the toco command:\r\n       toco --saved_model_dir /home/swai01/tfl-m_speech_model --output_file=./conv_int8.tflite --input_shapes=1,49,40,1 --input_arrays=Reshape_2 --output_arrays='labels_softmax' --inference_type=QUANTIZED_INT8 --mean_values=0 --std_dev_values=9.8077\r\n      but got the error message:\r\n          ValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n   Thanks for your time.\r\n\r\nruey-an\r\n", "comments": ["@rayeh5,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/34350#issuecomment-576786774) comment on a similar issue and let us know if it works. Thanks!", "@amahendrakar, thanks for your feedback.\r\nThe model file is trained via tensorflow/examples/speech_commands.\r\nI try the following python code:\r\n----------\r\nimport tensorflow as tf\r\nmodel = tf.saved_model.load('/home/swai01/tfl-m_speech_model')\r\nprint(model.signatures.keys())\r\nconcrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\nconcrete_func.inputs[0].set_shape([1, 49, 40, 1])\r\nconverter = TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\n-----------------\r\nI got the error message:\r\n---------------------\r\nKeysView(_SignatureMap({}))\r\nTraceback (most recent call last):\r\n  File \"my_quant.py\", line 4, in <module>\r\n    concrete_func = model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n  File \"/home/swai01/tf20_venv/lib/python3.5/site-packages/tensorflow_core/python/saved_model/signature_serialization.py\", line 207, in __getitem__\r\n    return self._signatures[key]\r\nKeyError: 'serving_default'\r\n--------------\r\n   My main purpose is to quant a model to int8, so that I can run this model on TF-Lite micro for CMSIS-NN library.\r\n   Thanks for your time.\r\n\r\nruey-an", "@rayeh5 can you please share a standalone code to reproduce the issue? thanks!", "\r\n[tfl-m_speech_model.zip](https://github.com/tensorflow/tensorflow/files/4233814/tfl-m_speech_model.zip)\r\nThe zip file contains checkpoint and save_model (via freeze.py) format. The my_quant.py is the script use to quant the model.\r\nWhen use toco command, I got the same error message.\r\nThanks for your time.\r\n\r\nruey-an", "Any suggestion for this problem?\r\nI try to train the speech_command model (tensorflow/examples/speech_commands), and got the same error message. The followings are the python scripts:\r\npython train.py \\\r\n --model_architecture conv --window_size_ms 40 --window_stride_ms 20 \\\r\n --learning_rate 0.0005,0.0001,0.00002 --how_many_training_steps 10000,10000,10000 \\\r\n --summaries_dir work/retrain_logs --train_dir work/training \\\r\n --data_dir=/home/swai01/rayeh/kws/wave_test_data \\\r\n --preprocess=micro  --silence_percentage=25 --unknown_percentage=25\r\n\r\npython freeze.py \\\r\n--model_architecture=conv --window_size_ms 40 --window_stride=20 \\\r\n--preprocess=micro  --silence_percentage=25 --unknown_percentage=25 \\\r\n--output_file=conv.pb --start_checkpoint=work/training/conv.ckpt-30000\r\n\r\ntoco \\\r\n--saved_model_dir=my_model --output_file=my_model/conv_u8.tflite \\\r\n--input_shapes=1,49,40,1 --input_arrays=Reshape_2 --output_arrays='labels_soft\r\nmax' --inference_type=QUANTIZED_UINT8 --mean_values=0 --std_dev_values=9.8077\r\n", "Hi @rayeh5! Could you please visit this[ thread ](https://stackoverflow.com/questions/66551794/tflite-cant-quantize-the-input-and-output-of-tensorflow-model-to-int8/67547272#67547272)for answer?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36872\">No</a>\n"]}, {"number": 36871, "title": "whether the netwrok code is needed to run the prediction procedure", "body": "Hi,\r\n\r\nI have the trained .pb model, and I would like to load it and then run the prediction procedure.\r\n\r\nIs the network code needed to run the prediction?, or I only need the following code but don't need the network code?\r\n\r\n\r\n\r\n```cpp\r\nModel model(\"../ssd_inception/frozen_inference_graph.pb\");\r\n    auto outNames1 = new Tensor(model, \"num_detections\");\r\n    auto outNames2 = new Tensor(model, \"detection_scores\");\r\n    auto outNames3 = new Tensor(model, \"detection_boxes\");\r\n    auto outNames4 = new Tensor(model, \"detection_classes\");\r\n\r\n    auto inpName = new Tensor(model, \"image_tensor\");\r\n\r\n    // Read image\r\n    cv::Mat img, inp;\r\n    img = cv::imread(\"../test.jpg\", CV_LOAD_IMAGE_COLOR);\r\n\r\n    int rows = img.rows;\r\n    int cols = img.cols;\r\n\r\n    cv::resize(img, inp, cv::Size(300, 300));\r\n    cv::cvtColor(inp, inp, CV_BGR2RGB);\r\n\r\n    // Put image in Tensor\r\n    std::vector<uint8_t > img_data;\r\n    img_data.assign(inp.data, inp.data + inp.total() * inp.channels());\r\n    inpName->set_data(img_data, {1, 300, 300, 3});\r\n\r\n    model.run(inpName, {outNames1, outNames2, outNames3, outNames4});\r\n\r\n```\r\n\r\n\r\nThanks,\r\nArdeal\r\n\r\n", "comments": ["@ardeal, Please provide the details about Tensorflow version and full error log. Thanks! ", "@gadagashwini , I'm now using Tensorflow 1.14.0.\r\nIn my C++ code, I would like to load the weight model of YoloV3 downloaded from web site, and then write the C++ code to run prediction. \r\n\r\nMy question is: \r\nwhether I should write the network code in C++?\r\n\r\nThanks,\r\nArdeal\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 36870, "title": "How to override gradient in a FuncGraph created by tf.function", "body": "With tensorflow 1.x API, I can override gradients with a gradient_override_map during building a graph as follows\r\n\r\n```    \r\n@tf.RegisterGradient('CustZero')\r\ndef _custom_grad(op, grad):\r\n    return tf.zeros_like(op.inputs[0])\r\n    \r\nwith tf.compat.v1.Session() as sess:\r\n    with sess.graph.as_default() as g:\r\n        x = tf.convert_to_tensor([-1.0, 0.0, 1.0, 2.0])\r\n        with g.gradient_override_map({'Relu': 'CustZero'}):\r\n            y = tf.nn.relu(x)\r\n    dy = tf.gradients(y, x)\r\n\r\n# dy: [0., 0., 0., 0.]\r\n```\r\n\r\nAs we are moving to TF2, I'd like to use native TF2 API to do the same task with `tf.function` and `tf.GradientTape`. As I have learnt, `tf.function` creates a FuncGraph for each different `input_signature`, and I can use `get_concrete_function` to create one ConcreteFunction with a specific `input_signature`. Then I can get the `FuncGraph` associated with the `ConcreteFunction`. This is what I have tried, which failed to override gradients.\r\n\r\n```\r\n# create a concrete function with the specified input signatures\r\n@tf.function(input_signature=(tf.TensorSpec(shape=(None,), dtype=tf.float32),))\r\ndef my_relu(x):\r\n    return tf.nn.relu(x)\r\nmy_relu_conc = my_relu.get_concrete_function()\r\n\r\nx = tf.constant([-1.0, 0.0, 1.0, 2.0], dtype=tf.float32)\r\nwith tf.GradientTape() as tape:\r\n    with my_relu_conc.graph.gradient_override_map({'Relu': 'CustZero'}):\r\n        tape.watch(x)\r\n        y = my_relu_conc(x)\r\ndy = tape.gradient(y, x).numpy()\r\n\r\n# dy: [0., 0., 1., 1.]\r\n```\r\n\r\nFrom my understanding, since the FuncGraph is already traced, the graph cannot be changed and I cannot override with custom gradients. \r\n\r\nMy question is: are there any methods to override gradient using native TF2 APIs?\r\n\r\nIf not, could we add an additional parameter named `gradient_override_map` to `tf.function` or `get_concrete_function` such that we can override gradients in the process where a FuncGraph is created? \r\n\r\n```\r\n@tf.function(\r\n    input_signature=(tf.TensorSpec(shape=(None,), dtype=tf.float32),), \r\n    gradient_override_map={'Relu': 'CustZero'})\r\ndef my_relu(x):\r\n    return tf.nn.relu(x)\r\nmy_relu_conc = my_relu.get_concrete_function()\r\n```\r\nor \r\n\r\n```\r\n@tf.function()\r\ndef my_relu(x):\r\n    return tf.nn.relu(x)\r\nmy_relu_conc = my_relu.get_concrete_function(\r\n    input_signature=(tf.TensorSpec(shape=(None,), dtype=tf.float32),), \r\n    gradient_override_map={'Relu': 'CustZero'})\r\n```", "comments": ["Could you try using `tf.custom_gradient`\r\n\r\n```\r\n@tf.function(input_signature=(tf.TensorSpec(shape=(None,), dtype=tf.float32),))\r\ndef my_relu(x):\r\n  @tf.custom_gradient\r\n  def relu_with_zeros_grad(x):\r\n    output = tf.nn.relu(x)\r\n    def grad_fn(grad):\r\n       return tf.zeros_like(output.op.inputs[0])\r\n    return output, grad_fn\r\n  return relu_with_zeros_grad(x)\r\n```", "???"]}]