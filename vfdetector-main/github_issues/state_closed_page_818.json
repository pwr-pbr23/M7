[{"number": 28994, "title": "[Translation] Translation for Korean", "body": "We've translated this part of guidelines to Korean on our repository with github pages.\r\n\r\nThis is URL of Korean repository : https://github.com/BangSeongJin/translation_to_korean.git", "comments": ["@polpolie I understand that you have translated this github page, can you please let us know the purpose of this issue in detail, so that we can take this forward.", "@muddham We learned about the tensor flow that is becoming an issue these days through various media,\r\nSo we have been working on the Korean version Tensor Flow Community.(Translation)\r\nAnd we are looking for other ways to apply open source.", "@polpolie you could raise Pull Requests and merge your contributions into the `master` branch of TensorFlow or TF2.0. Thanks!", "@polpolie Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this due to lack of recent activity. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28994\">No</a>\n"]}, {"number": 28993, "title": "Performance issue with ParameterServerStrategy and Gather", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI've developed a custom logistic regression using Estimator API. The model itself is quite simple and is composed of two variables, one for the bias b (scalar) and one for weights w which shape is (n,). n~60 millions:\r\n```\r\n...\r\nwith tf.variable_scope(\"weights\"):\r\n        w = tf.get_variable(\r\n            \"weights\", initializer=tf.zeros([n]),\r\n             trainable=True, partitioner=tf.fixed_size_partitioner(w_nb_shards)\r\n        )\r\n\r\nwith tf.variable_scope(\"bias\"):\r\n        b = tf.get_variable(\"bias\", initializer=tf.zeros([1]), trainable=True)\r\n\r\nlogits = \\\r\n        tf.add(\r\n            b,\r\n            tf.reduce_sum(\r\n                tf.multiply(\r\n                    tf.cast(tf.gather(w, feature_indices), tf.float32),\r\n                    feature_values\r\n                ),\r\n                axis=1\r\n            )\r\n        )\r\nbatch_size = tf.shape(feature_indices)[0]\r\nlogits = tf.reshape(logits, [batch_size, -1])\r\n\r\npredictions = tf.sigmoid(logits)\r\n\r\nif mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions={\"predictions\": predictions}\r\n        )\r\n\r\ntf.losses.sigmoid_cross_entropy(\r\n        labels,\r\n        logits,\r\n        reduction=tf.losses.Reduction.SUM,\r\n        weights=weight_column\r\n)\r\n\r\nif mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer())\r\n        train_op = optimizer.minimize(\r\n            loss=loss, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode, loss=loss, train_op=train_op)\r\n...\r\n```\r\n\r\nThe training is distributed on yarn with tf-yarn (https://github.com/criteo/tf-yarn) and ParameterServerStrategy. The variable w (weights) is sharded (fixed size) across all parameter servers. The training itself runs successfully but I have performance issues, it is slow. I've used the profiler hook (tf.train.ProfilerHook) to detect what ops take time (cumulative or not) and I noticed that most of the time is spent on op Ftrl/update_weights/weights/part_0/Unique (about 6 sec). Besides the execution time, I was surprised to see that this op , run by all parameter servers for weight updates, only take that much time for ps/1. The same op takes about 5ms for other parameter servers.\r\n\r\n![image](https://user-images.githubusercontent.com/41445208/58318802-d0dd7500-7e18-11e9-8085-db8ff087bdf4.png)\r\n\r\nAfter having taken a closer look at events Ftrl/update_weights/weights/part_X/Unique, I noticed that op Unique is applied to ~1 millions integers for ps/1 and less than 15000 integers for other parameter servers. ~1 milions integers seem to match with the number of weights in the model. You can find a subset of events matching ops Unique in the file below.\r\n\r\nIs it an expected behavior or known issue ? Should I implement the linear regression part of the model in a different way ? Thanks a lot in advance.\r\n\r\n[EventsOpsUnique.txt](https://github.com/tensorflow/tensorflow/files/3216246/EventsOpsUnique.txt)\r\n", "comments": ["@nateagr Please provide a code snippet to reproduce the performance issue reported here. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Same issue", "@liyi193328, \r\nCan you please post a new issue by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!"]}, {"number": 28992, "title": "failed call to cuInit: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu==2.0.0-alpha0`\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6.8\r\n- CUDA version: 10\r\n- GPU model and memory: GeForce GTX 1050 / 4Gb\r\n\r\n**Describe the current behavior**\r\nTensorflow does not detect my GPU (it used to - I don't know what changed).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n\r\nprint(c)\r\n```\r\nOutput:\r\n```sh\r\n2019-05-24 10:16:30.726799: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-24 10:16:30.740369: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-05-24 10:16:30.777178: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal\r\n2019-05-24 10:16:30.777232: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:166] retrieving CUDA diagnostic information for host: joao-Yoga730\r\n2019-05-24 10:16:30.777242: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:173] hostname: joao-Yoga730\r\n2019-05-24 10:16:30.777331: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 410.48.0\r\n2019-05-24 10:16:30.777364: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 410.48.0\r\n2019-05-24 10:16:30.777371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version seems to match DSO: 410.48.0\r\n2019-05-24 10:16:30.799754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1992000000 Hz\r\n2019-05-24 10:16:30.801312: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55e937b11800 executing computations on platform Host. Devices:\r\n2019-05-24 10:16:30.801384: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\n**Other info / logs**\r\n`nvidia-smi`\r\n```sh\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1050    Off  | 00000000:3B:00.0 Off |                  N/A |\r\n| N/A   52C    P0    N/A /  N/A |    509MiB /  4042MiB |     33%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1474      G   /usr/lib/xorg/Xorg                           301MiB |\r\n|    0      1650      G   /usr/bin/gnome-shell                         148MiB |\r\n|    0      2107      G   ...uest-channel-token=16113907172495644879    56MiB |\r\n|    0      4229      G   /snap/pycharm-community/128/jre64/bin/java     2MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\np.s. I have tried doing sudo apt install nvidia-modprobe. It didn't work.", "comments": ["I have verified with TensorFlow : 2.0.0-alpha0, GPU : Tesla T4  and Quadro K1200. It is working fine. Can you check with latest nightly build for 2.0 version and let us know if that resolves the error. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28991, "title": "Merge pull request #1 from tensorflow/master", "body": "Merge with latest changes", "comments": []}, {"number": 28990, "title": "bazel build failed on tf1.6 on tx2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 (aarch64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source (trying to install)\r\n- TensorFlow version: v1.6.0\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.11 (installed from sourcecode)\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory: Nvidia Jetson TX2 (DJI Manifold2)\r\n\r\n\r\n\r\n**Describe the problem**\r\nOn my DJI Manifold2 (Nvidia TX2) I am trying to install tensorflow. I am trying to install from scratch. \r\nThe tx2 was not setup with the jetpack, however, it has cuda9 and cudnn7 installed. \r\nI am following these instructions: https://gist.github.com/vellamike/7c26158c93e89ef155c1cc953bbba956 \r\n\r\nI could install bazel (0.11.0) successfully. \r\n`./configure` works alright. I have said no to everything accept for cuda and jemalloc. \r\n\r\n```\r\nbazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n\r\nThis fails to give: \r\n````\r\nINFO: From Compiling tensorflow/core/kernels/tile_functor_gpu.cu.cc:\r\nKilled\r\nERROR: /home/dji/Downloads/tensorflow/tensorflow/core/kernels/BUILD:848:1: output 'tensorflow/core/kernels/_objs/tile_ops_gpu/tensorflow/core/kernels/tile_functor_gpu.cu.pic.o' was not created\r\nERROR: /home/dji/Downloads/tensorflow/tensorflow/core/kernels/BUILD:848:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 84.543s, Critical Path: 82.14s\r\nFAILED: Build did NOT complete successfully\r\n```\r\nPlease see attachement for full lowg\r\n[errorlog.txt](https://github.com/tensorflow/tensorflow/files/3216060/errorlog.txt)\r\n", "comments": ["@mpkuse Please have look on [Tested build configuration](https://www.tensorflow.org/install/source#tested_build_configurations) for Tf-gpu 1.6.0.Thanks! ", "Were you able to resolve this?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28989, "title": "Unexpected behavior when scheduling different learning rates", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI've implemented a custom logistic regression using Estimator API which gives good results when learning with a fixed set of hyperparameters. However, I'm getting unexpected results when learning the model with a learning rate which varies over time. As you can see in the screenshot below, variables of the model (weights and bias), losses and metric prediction_mean abruptly changed twice during the learning, coinciding with learning rate modification.\r\n\r\nHere is the piece of code describing instanciation of the estimator:\r\n```\r\ntf.estimator.Estimator(\r\n    model_fn=my_model_fn,\r\n    params={\r\n        \"optimizer\": lambda: tf.train.FtrlOptimizer(\r\n            learning_rate=tf.where(\r\n                tf.less(tf.train.get_global_step(), 100000),\r\n                3e-3,\r\n                tf.where(\r\n                    tf.less(tf.train.get_global_step(), 200000),\r\n                    9e-4,\r\n                    tf.where(\r\n                        tf.less(tf.train.get_global_step(), 300000),\r\n                        6e-4,\r\n                        3e-4\r\n                    )\r\n                )\r\n            ),\r\n            l2_regularization_strength=1e-2\r\n        )\r\n    },\r\n    model_dir=model_dir\r\n)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/41445208/58311641-b8b22980-7e09-11e9-9460-a33cd94f713e.png)\r\n\r\nI was wondering if the modification of the learning rate could trigger the modification of the state of FTRL which could explain this behavior. Any clue ? Thanks a lot in advance.", "comments": ["Can you please provide full minimal code snippet that can reproduce the issue at hand. This would help us to dig in easily. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28988, "title": "how to compile the Tensorflow source code  on Mac", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  r1.12\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen I build the TensorFlow, The error result is:\r\nERROR: /private/var/tmp/_bazel_xmly/88507af8aae72d6d7037a76bc898c925/external/protobuf_archive/BUILD:260:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)\r\nld: unknown option: -no-as-needed\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.394s, Critical Path: 0.40s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThe methods I already tried are:\r\n./configure\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@HOTFIGHTER Please refer the steps mentioned in this [link](https://www.tensorflow.org/install/source#tested_build_configurations)", "@HOTFIGHTER Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28987, "title": "R1.12", "body": "need down", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28987) for more info**.\n\n<!-- need_sender_cla -->", "We don't merge release branches back into master"]}, {"number": 28986, "title": "Re-add 1.14 release notes", "body": "Somehow this got lost in the fast forward. This is incomplete and does not mention changes from the last few weeks as it was created before the fast forward. Will update it soon.", "comments": []}, {"number": 28985, "title": "test streaming accuracy python implementation", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28985) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28985) for more info**.\n\n<!-- ok -->", "@gbaned did you mean to request a review from someone else? I don't generally work on the python tf implementation.", "@gbaned, I would like to skip reviewing this CL. I'm not familiar with this code.", "Neither am I a good reviewer for this CL.", "@yifeif Can you please review this PR ?  ", "Can one of the admins verify this patch?", "@zhizunbao-y \r\n\r\nThanks for the PR! In addition to my comments in the doc, can you also do the following:\r\n\r\n1. Add your script to the BUILD file so that folks can run this as part of the bazel build environment\r\n2. Add instructions to your main file (test_stream_accuracy.py) detailing how to run this", "@zhizunbao-y Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "> @zhizunbao-y Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!\r\nOkay, thank you for informing me. I will revise and update it as soon as possible.", "I have done the refinement followed by @frankchn .", "@zhizunbao-y Could you please check reviewer comments? Thanks!", "@zhizunbao-y Could you please address Ubuntu Sanity errors? Thanks!", "@zhizunbao-y Still, Ubuntu Sanity checks failing. Could you please check again?  Thanks!", "@gbaned I don't know which part of the code made the Ubuntu sanity check failing. Please give me some guidance.", "I am sorry. It might be an input type error that caused this check error. I have fixed it. Please run check again. Thanks! ", "Here are the errors from Ubuntu Sanity:\r\n\r\nFrom pylint:\r\n```\r\ntensorflow/examples/speech_commands/recognize_commands.py:107: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\ntensorflow/examples/speech_commands/recognize_commands.py:128: [C0330(bad-continuation), ] Wrong hanging indentation (remove 35 spaces).\r\ntensorflow/examples/speech_commands/recognize_commands.py:129: [C0330(bad-continuation), ] Wrong hanging indentation (remove 28 spaces).\r\ntensorflow/examples/speech_commands/recognize_commands.py:135: [C0330(bad-continuation), ] Wrong continued indentation (add 10 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:136: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:138: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:140: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:148: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:149: [C0330(bad-continuation), ] Wrong continued indentation (add 9 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:154: [C0330(bad-continuation), ] Wrong continued indentation (remove 1 space).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:169: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:172: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:173: [C0330(bad-continuation), ] Wrong continued indentation (remove 5 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:174: [C0330(bad-continuation), ] Wrong continued indentation (remove 5 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:175: [C0330(bad-continuation), ] Wrong continued indentation (remove 5 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:184: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:185: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:186: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:187: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:189: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:190: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:191: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:192: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:194: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:195: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:196: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:197: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:199: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:200: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:201: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:202: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:204: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:205: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:206: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:207: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:207: [C0326(bad-whitespace), ] Exactly one space required after comma\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:208: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:210: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:211: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:212: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:213: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:215: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:216: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:217: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:218: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:220: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:221: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:222: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:223: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:225: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:226: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:227: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:228: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:230: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:231: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:232: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:233: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:235: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:236: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:237: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:238: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:240: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:241: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:242: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:243: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:246: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:247: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:248: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/test_streaming_accuracy.py:249: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/accuracy_utils.py:64: [C0330(bad-continuation), ] Wrong continued indentation (remove 10 spaces).\r\ntensorflow/examples/speech_commands/accuracy_utils.py:144: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/examples/speech_commands/accuracy_utils.py:151: [C0330(bad-continuation), ] Wrong continued indentation (add 3 spaces).\r\n```\r\n\r\nFrom Ubuntu Sanity:\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/examples/speech_commands/BUILD:\r\n16,17c16,17\r\n<     main=\"accuracy_utils.py\",\r\n<     srcs = [\"accuracy_utils.py\",],\r\n---\r\n>     srcs = [\"accuracy_utils.py\"],\r\n>     main = \"accuracy_utils.py\",\r\n23c23\r\n<         ],\r\n---\r\n>     ],\r\n28,29c28,29\r\n<     main=\"recognize_commands.py\",\r\n<     srcs = [\"recognize_commands.py\",],\r\n---\r\n>     srcs = [\"recognize_commands.py\"],\r\n>     main = \"recognize_commands.py\",\r\n35c35\r\n<         ],\r\n---\r\n>     ],\r\n39a40\r\n>     srcs = [\"test_streaming_accuracy.py\"],\r\n41d41\r\n<     srcs = [\"test_streaming_accuracy.py\",],\r\n50c50\r\n<         ],\r\n---\r\n>     ],\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```", "I used the pylintrc in tensorflow/tensorflow/tools/ci_build/ to check my code.\r\nThis time might be ok."]}, {"number": 28984, "title": "Refactor some logic into IsRootEnter and IsRootExit", "body": "", "comments": []}, {"number": 28983, "title": "[TF 1.13]: Mishandling dropout and recurrent dropout when feeding them as placeholder inputs to SimpleRNNCell in tf.keras.layers", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA (Working on Google Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA (Working on Google Colab)\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): NA (Working on Google Colab)\r\n- GCC/Compiler version (if compiling from source): NA (Working on Google Colab)\r\n- CUDA/cuDNN version: NA (Working on Google Colab)\r\n- GPU model and memory: NA (Working on Google Colab)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to use `tf.keras.layers` in my code to see how it works. While trying to forecast time series in Tensorflow using RNN, I am using dropout method. If I hard-code the values of `dropout` and `recurrent_dropout`, my program runs perfectly. However, I need to perform inference on the model and for that the dropout needs to be turned off. I am using `tf.placeholder_with_default` but this is throwing an error.\r\n\r\n**Describe the expected behavior**\r\nIdeally there should be no error while using `tf.placeholder_with_default` but since `recurrent.py` allows only float values for `dropout` and `recurrent_dropout` this error is coming up. Is there any way to pass `dropout` and `recurrent_dropout` values at runtime?\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```Python\r\n# Library imports\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Variable initialization\r\nt_min = 0\r\nt_max = 30\r\nresolution = 1e-2\r\nn_steps = 20\r\nn_inputs = 1\r\nn_outputs = 1\r\nn_neurons = 100\r\nlearning_rate = 1e-2\r\n\r\nTimeSeriesDropout_graph = tf.Graph()\r\nwith TimeSeriesDropout_graph.as_default():\r\n  keep_prob_input_default = np.array(1.0,dtype='float32')\r\n  keep_prob_input = tf.placeholder_with_default(keep_prob_input_default,shape=())\r\n  keep_prob_state_default = np.array(1.0,dtype='float32')\r\n  keep_prob_state = tf.placeholder_with_default(keep_prob_state_default,shape=())\r\n#   keep_prob_input = 0.5\r\n#   keep_prob_state = 0.5\r\n  X = tf.placeholder(dtype=tf.float32,shape=(None,n_steps,n_inputs))\r\n  y = tf.placeholder(dtype=tf.float32,shape=(None,n_steps,n_outputs))\r\n  multiple_rnn_cell = [tf.keras.layers.SimpleRNNCell(units=n_neurons,activation='relu'\r\n                                                     ,dropout=keep_prob_input\r\n                                                     ,recurrent_dropout=keep_prob_state)\r\n                     ,tf.keras.layers.SimpleRNNCell(units=n_neurons//2,activation='relu'\r\n                                                     ,dropout=keep_prob_input\r\n                                                    ,recurrent_dropout=keep_prob_state)\r\n                     ,tf.keras.layers.SimpleRNNCell(units=n_neurons//4,activation='relu'\r\n                                                     ,dropout=keep_prob_input\r\n                                                    ,recurrent_dropout=keep_prob_state)\r\n                     ,tf.keras.layers.SimpleRNNCell(units=n_neurons//8,activation='relu'\r\n                                                     ,dropout=keep_prob_input\r\n                                                    ,recurrent_dropout=keep_prob_state)]\r\n  stacked_rnn_cell = tf.keras.layers.StackedRNNCells(multiple_rnn_cell)\r\n  rnn_output = tf.keras.layers.RNN(stacked_rnn_cell,return_sequences=True,return_state=True)(X)\r\n  outputs_reshaped = tf.reshape(rnn_output[0],(-1,n_neurons//8))\r\n  output = tf.keras.layers.Dense(units=n_outputs)(outputs_reshaped)\r\n  output_stacked = tf.reshape(output,(-1,n_steps,n_outputs))\r\n  loss = tf.reduce_mean(tf.square(y-output_stacked))\r\n  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n  training_op = optimizer.minimize(loss)\r\n  init = tf.global_variables_initializer()\r\n  saver = tf.train.Saver()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-30-16da420b4722> in <module>()\r\n     10   multiple_rnn_cell = [tf.keras.layers.SimpleRNNCell(units=n_neurons,activation='relu'\r\n     11                                                      ,dropout=keep_prob_input\r\n---> 12                                                      ,recurrent_dropout=keep_prob_state)\r\n     13                      ,tf.keras.layers.SimpleRNNCell(units=n_neurons//2,activation='relu'\r\n     14                                                      ,dropout=keep_prob_input\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py in __init__(self, units, activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, kernel_regularizer, recurrent_regularizer, bias_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, **kwargs)\r\n   1061     self.bias_constraint = constraints.get(bias_constraint)\r\n   1062 \r\n-> 1063     self.dropout = min(1., max(0., dropout))\r\n   1064     self.recurrent_dropout = min(1., max(0., recurrent_dropout))\r\n   1065     self.state_size = self.units\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __bool__(self)\r\n    651       `TypeError`.\r\n    652     \"\"\"\r\n--> 653     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\r\n    654                     \"Use `if t is not None:` instead of `if t:` to test if a \"\r\n    655                     \"tensor is defined, and use TensorFlow ops such as \"\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n```\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a full minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "@achandraa The code given in the original post reproduces the issue. The only caveat is that this code was run on Google Colab. Please let me know if you need any more information from me.", "Will it be possible to provide the values for various parameters you are trying as in n_steps, n_inputs, n_outputs etc to check the exact code snippet for the issue. This will help us to proceed faster. Thanks!", "@achandraa My apologies. I have updated the original code snippet with all the relevant library imports and variable initializations. Please let me know if you need any more information from me.", "Thanks for sharing the information. I have tried on Colab  with TensorFlow version 1.13.1 and was able to reproduce the issue.", "If you check the implementation of the dropout in keras, it actually has a switch for turning on/off based on whether its in training mode or not. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L2662. \r\n\r\nAs long as your custom training code is populating the \"learning_phase\" value correctly, then it will correctly enable/disable dropout during training and inferencing. The value is auto populated if you are using keras model.fit/eval/predict. If you are using different training loop, then just set the value by keras.backend.set_learning_phase(). See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L307", "Also, as the doctring stated, the dropout/recurrent_dropout should just be float, rather than tensor or placeholder. So the code is working as intended.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L1287", "@qlzh727 Thanks for your response and apologies for coming back so late on this. I understand that as the code is functioning as per the documentation. While I understand that Keras disables training at the time of inference, this is not something which seems applicable to Tensorflow. Could you please help me understand why was the decision of not allowing tensors as dropout in RNN taken?", "Keras dropout is designed exactly to help user avoid feeding placeholder and turn on/off between training and inference. \r\n\r\nSince dropout does not make sense during inference, what user should worry about is only the dropout rate during training. Asking user to be aware of training/inference context, and managing the switching is too much overhead. That's why we design the API to consume the complexity for user, and let them focus on building the model."]}, {"number": 28982, "title": "[Intel MKL] Fix Relu bug", "body": "Return empty when input tensor has no elements. Also added unit test to check for this condition", "comments": ["@guizili0 Can you please check build failures.", "@gbaned updated, please help to check. "]}, {"number": 28981, "title": "[Intel Mkl] Adding Mkl + horovod partials and Dockerfiles", "body": "This PR adds support for MPI+Horovod to the Dockerfile partials and generated Dockerfiles. \r\n\r\nMKL support is not explicit because it can be built with the CPU Dockerfiles as they are.", "comments": ["@claynerobison Could you please address the reviewer comments. Thanks!", "@claynerobison Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "@angersson You can test these yourself quite easily. Just use any Xeon container/machine on GCP (which is probably all of them!)", "Can one of the admins verify this patch?", "@gbaned Since @angerson has already reviewed and requested changes, I'd rather not review this too because the PR is rather large. I'm removing my review request.\r\n\r\n@claynerobison Could you please address @angerson's change request?\r\n\r\n> Can you put the new partials in a new directory to emphasize what they're used for -- like mkl_horovod, for example?\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Hi @penpornk, sorry this took so long. I believe I've addressed @angerson request by moving these into `mkl_horovod` folders to reflect usage. Please let me know if you'd like any more changes.", "Hi @angerson, could you help review again since we have create the dir for those Dockerfiles?"]}, {"number": 28980, "title": "Hamming distance support", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTake binary representation class (predicted label ) number into account and compute loss.\r\nSay, for example, the loss for true label = 0 and predicted label = 1 should be less than the loss for true label = 0 and predicted = 100\r\n\r\n**Will this change the current API? How?**\r\nNo. This will add a new function\r\n\r\n**Who will benefit with this feature?**\r\nHelpful for users with the above-mentioned requirement.\r\n\r\nIf this is of interest, I would like to create a PR.\r\n", "comments": ["Can you please provide some more details on the proposed feature and its use case. This would help us to get more understanding on the same. Thanks!", "@achandraa Thanks for the review comment. I will update about the same.", "@SSaishruthi Please let us know use cases where this can be useful. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28979, "title": "Adding multiclass support to BoostedTreesClassifier", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe BoostedTreesClassifier supports binary classification only.\r\n\r\n**Will this change the current api? How?**\r\nNo. The API already has a placeholder parameter `n_classes`.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who need to perform multiclass classification.\r\n\r\n**Any Other info.**", "comments": ["Hi! Has there been any progress on this? If not, can anyone describe an implementation path? I'd be willing to take a crack at it if it is not too involved. Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Multi-class support for BoostedTreesClassifier is now added. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier#args_1"]}, {"number": 28978, "title": "NotImplementedError needs to be raised", "body": "raise missing in update_state()", "comments": []}, {"number": 28977, "title": "check raise invalid shape of _log_weight_as_image", "body": "Raise ValueError in _log_weight_as_image()", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28976, "title": "TF2.0 tf.keras.estimator.model_to_estimator does not store input_names from tf.keras.layers.DenseFeatures", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI created a Keras model that takes as input 2 feature columns named 'store' and 'loc'.\r\n\r\nConverting it to an Estimator and then training it throws an exception because it assumes the input names are 'input_1' and 'input_2'.\r\n\r\n**Describe the expected behavior**\r\nTrain the keras estimator without errors\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nPlease see this gist: https://gist.github.com/hsm207/ed5a28f4156e0088b0d1098e225e70fb\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was able to get the mentioned error when trying with TensorFlow version 2.0.0-alpha on Colab.", "@hsm207 Cannot load the shared gist. Can you please test it against tf 2.0 nightly version and check if the issue still persists. Thanks!\r\n```\r\npip install tf-nightly-2.0-preview\r\n```", "@ymodak The last cell is throwing an exception.\r\n\r\nThis is the last cell:\r\n\r\n```\r\nest_1 = tf.keras.estimator.model_to_estimator(keras_model=model_1)\r\n\r\n\r\ndef input_fn():\r\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels)).batch(2)\r\n    return ds.repeat()\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn = input_fn, max_steps=4 * 3)\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\r\n                                  steps=12)\r\n\r\ntf.estimator.train_and_evaluate(est_1, train_spec, eval_spec)\r\n```\r\n\r\nThis is the stack trace:\r\n\r\n\r\n```\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/estimator/__init__.py\", line 175, in model_to_estimator_v2\r\n    from tensorflow_estimator.python.estimator import keras as keras_lib  # pylint: disable=g-import-not-at-top\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/__init__.py\", line 10, in <module>\r\n    from . import estimator\r\nImportError: cannot import name 'estimator'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-d191f2777eb2>\", line 1, in <module>\r\n    est_1 = tf.keras.estimator.model_to_estimator(keras_model=model_1)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/estimator/__init__.py\", line 178, in model_to_estimator_v2\r\n    'tf.keras.estimator.model_to_estimator function not available in your '\r\nNotImplementedError: tf.keras.estimator.model_to_estimator function not available in your installation.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'NotImplementedError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.estimator'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/estimator/__init__.py\", line 175, in model_to_estimator_v2\r\n    from tensorflow_estimator.python.estimator import keras as keras_lib  # pylint: disable=g-import-not-at-top\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/__init__.py\", line 10, in <module>\r\n    from . import estimator\r\nImportError: cannot import name 'estimator'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-d191f2777eb2>\", line 1, in <module>\r\n    est_1 = tf.keras.estimator.model_to_estimator(keras_model=model_1)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/estimator/__init__.py\", line 178, in model_to_estimator_v2\r\n    'tf.keras.estimator.model_to_estimator function not available in your '\r\nNotImplementedError: tf.keras.estimator.model_to_estimator function not available in your installation.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'NotImplementedError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2899, in run_code\r\n    self.showtraceback()\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1826, in showtraceback\r\n    value, tb, tb_offset=tb_offset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1411, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1182, in structured_traceback\r\n    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\nTypeError: must be str, not list\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.estimator'\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/estimator/__init__.py in model_to_estimator_v2(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)\r\n    174   try:\r\n--> 175     from tensorflow_estimator.python.estimator import keras as keras_lib  # pylint: disable=g-import-not-at-top\r\n    176   except ImportError:\r\n\r\n6 frames\r\nImportError: cannot import name 'estimator'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\nAttributeError: 'NotImplementedError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\r\n   1180             exception = self.get_parts_of_chained_exception(evalue)\r\n   1181             if exception:\r\n-> 1182                 formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\n   1183                 etype, evalue, etb = exception\r\n   1184             else:\r\n\r\nTypeError: must be str, not list\r\n```\r\n", "Can you please add a link to the colab ? ", "@goldiegadde here's the [link](https://gist.github.com/hsm207/3ab2a5b31bc3b21f8466d426abaa8b81)", "**Facing the same issue.** \r\n\r\nCreated a Keras model with DenseFeatures layers using tf.feature_column array. After converting Keras model to TF estimator and running the train_and_evaluate, the system gives error \r\n\r\nThe serving function as well as densefeatures in Keras had column names but seems that Tensorflow estimator doesn't understand it. Tensorflow estimator completely ignores the feature names provided in DenseFeatures layers in Keras.\r\n\r\n`The dictionary passed into features does not have the expected inputs keys defined in the keras model.`\r\n\r\nCode Snippet:\r\n```\r\n# Define feature columns\r\ndef create_feature_cols():\r\n#   lat_buck = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'), \r\n#                                                  boundaries = np.arange(32.0, 42, 1).tolist())\r\n#   long_buck = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),\r\n#                                                   boundaries = np.arange(1, 52, 1).tolist())\r\n    werks_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key='WERKS',\r\n            vocabulary_list=['ML01','ML02','ML03'])\r\n    scenario_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key='SCENARIO',\r\n            vocabulary_list=['1','2','3','4'])\r\n    ktokk_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key='KTOKK',\r\n            vocabulary_list=['1','2'])    \r\n    vstatu_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key='VSTATU',\r\n            vocabulary_list=['1','2'])\r\n    ekorg_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key='EKORG',\r\n            vocabulary_list=['1','2'])   \r\n    ekgrp_c = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key='EKGRP',\r\n            vocabulary_list=['A','B','C'])\r\n\r\n    return [\r\n        tf.feature_column.indicator_column(werks_c),\r\n        tf.feature_column.indicator_column(scenario_c),\r\n        tf.feature_column.indicator_column(ktokk_c),\r\n        tf.feature_column.indicator_column(vstatu_c),\r\n        tf.feature_column.indicator_column(ekorg_c),\r\n        tf.feature_column.indicator_column(ekgrp_c),\r\n        tf.feature_column.numeric_column('VPATD'),\r\n        tf.feature_column.numeric_column(\"TOTGRQTY\"),\r\n        tf.feature_column.numeric_column(\"TOTIRQTY\"),\r\n        tf.feature_column.numeric_column(\"NODLGR\"),\r\n        tf.feature_column.numeric_column(\"NODLIR\"),\r\n        tf.feature_column.numeric_column(\"DIFGRIRD\"),\r\n        tf.feature_column.numeric_column(\"DIFGRIRV\"),\r\n        tf.feature_column.numeric_column(\"grminusirbyvpatd\")\r\n  ]\r\n\r\ndef create_keras_model(params, feature_cols):\r\n    model = tf.keras.Sequential()\r\n\r\n    #First step is feature engineering\r\n    # model.add(tf.keras.backend.map_fn(feature_engg_features))\r\n\r\n    #Tensorflow style feature columns\r\n    model.add(tf.keras.layers.DenseFeatures(feature_cols))\r\n\r\n    #Other columns\r\n    model.add(tf.keras.layers.Dense(32, activation='relu',kernel_initializer='he_uniform'))\r\n    model.add(tf.keras.layers.BatchNormalization())\r\n\r\n    for l_ in range(params['hidden_layers']):\r\n        model.add(tf.keras.layers.Dense(32, activation='relu',kernel_initializer='he_uniform'))\r\n        model.add(tf.keras.layers.BatchNormalization())\r\n\r\n    model.add(tf.keras.layers.Dense(1,   activation='sigmoid'))\r\n    return model\r\n\r\n# Serving function for external call\r\ndef serving_fn():\r\n    feature_placeholders  = {'WERKS' : tf.placeholder(tf.string, [None]),\r\n            'SCENARIO' : tf.placeholder(tf.string, [None]),\r\n            'KTOKK' : tf.placeholder(tf.string, [None]),\r\n            'VSTATU' : tf.placeholder(tf.string, [None]),\r\n            'EKORG' : tf.placeholder(tf.string, [None]),\r\n            'EKGRP' : tf.placeholder(tf.string, [None]),\r\n            'VPATD' : tf.placeholder(tf.float32, [None]),\r\n            'TOTGRQTY' : tf.placeholder(tf.float32, [None]),\r\n            'TOTIRQTY' : tf.placeholder(tf.float32, [None]),\r\n            'NODLGR' : tf.placeholder(tf.float32, [None]),\r\n            'NODLIR' : tf.placeholder(tf.float32, [None]),\r\n            'DIFGRIRD' : tf.placeholder(tf.float32, [None]),\r\n            'DIFGRIRV' : tf.placeholder(tf.float32, [None])\r\n    }\r\n\r\n    #Features with transformation logic\r\n    features = {\r\n                key: tf.expand_dims(tensor, -1)\r\n                for key, tensor in feature_placeholders.items()\r\n            }\r\n    \r\n    #feat_changed = add_engineered(features.copy())\r\n    return tf.estimator.export.ServingInputReceiver(feature_engg_features(features), feature_placeholders )\r\n\r\n# Create estimator train and evaluate function\r\ndef est_train_and_evaluate(keras_model, output_dir, num_train_steps, train_file, eval_file):    \r\n    run_config = tf.estimator.RunConfig(save_checkpoints_secs = 40, \r\n                                        keep_checkpoint_max = 10)\r\n    \r\n    #Convert Keras model to TF estimator\r\n    keras_model.compile()\r\n    estimator = tf.keras.estimator.model_to_estimator(keras_model=keras_model, config=run_config)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn =  make_input_fn(filename = train_file,\r\n                                                    mode = tf.estimator.ModeKeys.TRAIN,\r\n                                                    batch_size = 512),\r\n                                        max_steps = num_train_steps)\r\n    \r\n    #Create exporter\r\n    # exp = tf.estimator.LatestExporter(\"decision\", serving_fn)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn =    make_input_fn(filename = eval_file,\r\n                                                    mode = tf.estimator.ModeKeys.TRAIN,\r\n                                                    batch_size = 512),\r\n                                      steps = None, \r\n                                      # exporters = exp,\r\n                                      start_delay_secs = 20, # start evaluating after N seconds, \r\n                                      throttle_secs = 45)  # evaluate every N seconds\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\nm_ = create_keras_model(params = params_default, feature_cols = create_feature_cols())\r\nest_train_and_evaluate(m_, None, 20000, train_file, eval_file)\r\n```\r\n\r\nError:\r\n\r\n```\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/keras.py in _to_ordered_tensor_list(obj, key_order, obj_name, order_name)\r\n    138                 order_name=order_name, order_keys=set(key_order),\r\n    139                 obj_name=obj_name, obj_keys=set(obj.keys()),\r\n--> 140                 different_keys=different_keys))\r\n    141 \r\n    142       return [_convert_tensor(obj[key]) for key in key_order]\r\n\r\nKeyError: \"The dictionary passed into features does not have the expected inputs keys defined in the keras model.\\n\\tExpected keys: {'input_5', 'input_9', 'input_12', 'input_4', 'input_8', 'input_10', 'input_14', 'input_6', 'input_3', 'input_13', 'input_7', 'input_1', 'input_2', 'input_11'}\\n\\tfeatures keys: {'WERKS', 'TOTIRQTY', 'DIFGRIRV', 'VPATD', 'VSTATU', 'grminusirbyvpatd', 'EKORG', 'DIFGRIRD', 'EKGRP', 'TOTGRQTY', 'KTOKK', 'NODLIR', 'SCENARIO', 'NODLGR'}\\n\\tDifference: {'WERKS', 'input_5', 'DIFGRIRV', 'input_14', 'input_1', 'input_2', 'SCENARIO', 'input_10', 'grminusirbyvpatd', 'DIFGRIRD', 'input_3', 'KTOKK', 'input_7', 'input_8', 'input_9', 'NODLGR', 'TOTIRQTY', 'input_12', 'VPATD', 'input_4', 'input_11', 'VSTATU', 'input_6', 'EKORG', 'EKGRP', 'TOTGRQTY', 'input_13', 'NODLIR'}\"\r\n\r\n\r\n```", "> Can you please add a link to the colab ?\r\n\r\nAny update on the issue ? ", "> Can you please add a link to the colab ?\r\n> \r\n> Any update on the issue ?\r\n\r\n@rafiqhasan Here's the [link ](https://gist.github.com/hsm207/3ab2a5b31bc3b21f8466d426abaa8b81)to the notebook.\r\n\r\nIt's still throwing the same error in TF version v2.1.0-rc1-0-g064e1535a7", "Same error +1", "Any updates or workarounds?", "Unfortunately, model_to_estimator wouldn't be able to infer input names for Sequential models.\r\nI think you can make it work by:\r\n```python\r\ndef create_model():\r\n    store_feature = tf.feature_column.categorical_column_with_vocabulary_list('store', vocabulary_list=['a', 'b'])\r\n\r\n    store_feature = tf.feature_column.embedding_column(store_feature, dimension=64)\r\n\r\n    loc_feature = tf.feature_column.categorical_column_with_vocabulary_list('loc', vocabulary_list=['x', 'y', 'z'])\r\n\r\n    loc_feature = tf.feature_column.embedding_column(loc_feature, dimension=32)\r\n\r\n    inp_1 = tf.keras.Input(name='store', dtype=tf.string, shape=(1,))\r\n    inp_2 = tf.keras.Input(name='loc', dtype=tf.string, shape=(1,))\r\n    keras_dict_input = {'store': inp_1, 'loc': inp_2}\r\n    x = tf.keras.layers.DenseFeatures(feature_columns=[store_feature, loc_feature])(keras_dict_input)\r\n    x = tf.keras.layers.Dense(32, activation='relu')(x)\r\n    x = tf.keras.layers.Dense(1, activation='relu')(x)\r\n    model = tf.keras.Model(keras_dict_input, x)\r\n\r\n    return model\r\n```", "FWIW, please check https://github.com/tensorflow/community/pull/188", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28976\">No</a>\n", "Using the Keras functional API to define a model and calling `tf.keras.Input` to define inputs seems works, so I rewrite my model using functional API to solve it.", "Yes , thanks ! This still solves the problem."]}, {"number": 28975, "title": "Support fused recurrent edges in auto_mixed_precision graph optimizer", "body": "Some models produce optimized graphs that contain multiple Merge nodes connected to a single NextIteration node. This PR adds support for such cases (which would previously report an error and skip the pass), along with a new test case.\r\n\r\nAttn. @reedwm ", "comments": []}, {"number": 28974, "title": "Cherry-pick for fixes for dotprod detection on iOS", "body": "", "comments": []}, {"number": 28973, "title": "Two cherrypicks", "body": "", "comments": []}, {"number": 28972, "title": "Adding R-Square and FBeta Score metric under tf.metric", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdding R-Square and FBeta metrics as one of the metrics under tf.metrics\r\n\r\n**Will this change the current api? How?**\r\nI will not affect the current API\r\n\r\n**Who will benefit with this feature?**\r\nR-Square and FBeta are quite popular metrics. Adding this will benefit a lot of people\r\n\r\nI would like to create a PR if this is of our interest and not existing.\r\n\r\nThanks in advance.\r\n", "comments": ["Hi @pavithrasv \r\nAre these two metrics will be of our interest to get added tf.metric?\r\nThanks", "I think any new metric should be added to the SIG add ons repo and moved to the code repo based on usage statistics. Please feel free to send a PR to the add ons repo."]}, {"number": 28971, "title": "Added examples", "body": "", "comments": ["@dynamicwebpaige Could you review it and please let me know if anything is missing. Thanks!", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 28970, "title": "BUG: tf.function cannot handle exception raised by custom op", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28970) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28970) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 28969, "title": "Unexpected behavior with warm start", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not relevant\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): Not relevant\r\n- GCC/Compiler version (if compiling from source): Not relevant\r\n- CUDA/cuDNN version: Not relevant\r\n- GPU model and memory: Not relevant\r\n\r\nI've implemented a custom logistic regression which is working as expected. My implementation is based on Estimator API. I managed to train the model with a set of hyperparameter. After  that, I wanted to train another model but with a different learning rate and a warm start using the previously trained model. The learning itself ran succesfully. Since the new learning started from the last checkpoint of the previous learning (due to the warm start), I would expect the new model to start with the variable values of the last checkpoint of the previous learning. But looking at the metrics of tensorboard, It seems the new learning started from scratch. As we can see in the screenshot, the prediction_mean metric starts above 5e-4 and converges around 2.5e-4. The problem here is the same metric prediction_mean already converged around 2.5e-4 for the previous learning. Same observation regarding the metric auc.\r\n\r\n![image](https://user-images.githubusercontent.com/41445208/58267669-63800480-7d84-11e9-92db-85d547cf8812.png)\r\n\r\nModel trainings are run on yarn and are distributed according to the parameter server strategy.\r\n\r\nThe following is the piece of code to instantiate the estimator with a warm start:\r\n```\r\ntf.estimator.Estimator(\r\n    model_fn=model_fn_babaflow.model_fn,\r\n    params={\r\n        \"optimizer\": tf.train.FtrlOptimizer(\r\n            1e-1,\r\n            l2_regularization_strength=1e-1\r\n        )\r\n    },\r\n    model_dir=model_dir,\r\n    warm_start_from=\"viewfs://path_to_model_dir_of_previous_learning\"\r\n)\r\n```\r\n\r\nThe following is an extract of the log of the chief:\r\n\r\n> 2019-05-22 14:59:36,321:INFO:tensorflow: Starting execution chief:0\r\n> 2019-05-22 14:59:36,322:INFO:tensorflow: Broadcasting chief:0/train_eval_start_time = '1558537176.3222742'\r\n> 2019-05-22 14:59:36,323:INFO:tensorflow: Broadcasting chief:0/start = ''\r\n> 2019-05-22 14:59:36,331:INFO:tensorflow: Not using Distribute Coordinator.\r\n> 2019-05-22 14:59:36,912:INFO:tensorflow: Calling model_fn.\r\n> 2019-05-22 14:59:39,613:WARNING:tensorflow: From /tmp/9e20594d-0554-4d16-8b21-122638b68185/install/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\n> 2019-05-22 14:59:50,953:INFO:tensorflow: Done calling model_fn.\r\n> 2019-05-22 14:59:50,953:INFO:tensorflow: Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='viewfs://...', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\n> 2019-05-22 14:59:50,953:INFO:tensorflow: Warm-starting from: ('viewfs://...',)\r\n> 2019-05-22 14:59:50,957:INFO:tensorflow: Warm-starting variable: weights/weights; prev_var_name: Unchanged\r\n> 2019-05-22 15:00:02,025:DEBUG:tensorflow: Initialize variable weights/weights/part_0:0,weights/weights/part_1:0,weights/weights/part_2:0,weights/weights/part_3:0,weights/weights/part_4:0,weights/weights/part_5:0,weights/weights/part_6:0,weights/weights/part_7:0,weights/weights/part_8:0,weights/weights/part_9:0,weights/weights/part_10:0,weights/weights/part_11:0,weights/weights/part_12:0,weights/weights/part_13:0,weights/weights/part_14:0,weights/weights/part_15:0,weights/weights/part_16:0,weights/weights/part_17:0,weights/weights/part_18:0,weights/weights/part_19:0,weights/weights/part_20:0,weights/weights/part_21:0,weights/weights/part_22:0,weights/weights/part_23:0,weights/weights/part_24:0,weights/weights/part_25:0,weights/weights/part_26:0,weights/weights/part_27:0,weights/weights/part_28:0,weights/weights/part_29:0,weights/weights/part_30:0,weights/weights/part_31:0,weights/weights/part_32:0,weights/weights/part_33:0,weights/weights/part_34:0,weights/weights/part_35:0,weights/weights/part_36:0,weights/weights/part_37:0,weights/weights/part_38:0,weights/weights/part_39:0,weights/weights/part_40:0,weights/weights/part_41:0,weights/weights/part_42:0,weights/weights/part_43:0,weights/weights/part_44:0,weights/weights/part_45:0,weights/weights/part_46:0,weights/weights/part_47:0,weights/weights/part_48:0,weights/weights/part_49:0,weights/weights/part_50:0,weights/weights/part_51:0,weights/weights/part_52:0,weights/weights/part_53:0,weights/weights/part_54:0,weights/weights/part_55:0,weights/weights/part_56:0,weights/weights/part_57:0,weights/weights/part_58:0,weights/weights/part_59:0,weights/weights/part_60:0,weights/weights/part_61:0,weights/weights/part_62:0,weights/weights/part_63:0,weights/weights/part_64:0,weights/weights/part_65:0,weights/weights/part_66:0,weights/weights/part_67:0,weights/weights/part_68:0,weights/weights/part_69:0,weights/weights/part_70:0,weights/weights/part_71:0,weights/weights/part_72:0,weights/weights/part_73:0,weights/weights/part_74:0,weights/weights/part_75:0,weights/weights/part_76:0,weights/weights/part_77:0,weights/weights/part_78:0,weights/weights/part_79:0,weights/weights/part_80:0,weights/weights/part_81:0,weights/weights/part_82:0,weights/weights/part_83:0,weights/weights/part_84:0,weights/weights/part_85:0,weights/weights/part_86:0,weights/weights/part_87:0,weights/weights/part_88:0,weights/weights/part_89:0,weights/weights/part_90:0,weights/weights/part_91:0,weights/weights/part_92:0,weights/weights/part_93:0,weights/weights/part_94:0,weights/weights/part_95:0,weights/weights/part_96:0,weights/weights/part_97:0,weights/weights/part_98:0,weights/weights/part_99:0,weights/weights/part_100:0,weights/weights/part_101:0,weights/weights/part_102:0,weights/weights/part_103:0,weights/weights/part_104:0,weights/weights/part_105:0,weights/weights/part_106:0,weights/weights/part_107:0,weights/weights/part_108:0,weights/weights/part_109:0,weights/weights/part_110:0,weights/weights/part_111:0,weights/weights/part_112:0,weights/weights/part_113:0,weights/weights/part_114:0,weights/weights/part_115:0,weights/weights/part_116:0,weights/weights/part_117:0,weights/weights/part_118:0,weights/weights/part_119:0,weights/weights/part_120:0,weights/weights/part_121:0,weights/weights/part_122:0,weights/weights/part_123:0,weights/weights/part_124:0,weights/weights/part_125:0,weights/weights/part_126:0,weights/weights/part_127:0,weights/weights/part_128:0,weights/weights/part_129:0,weights/weights/part_130:0,weights/weights/part_131:0,weights/weights/part_132:0,weights/weights/part_133:0,weights/weights/part_134:0,weights/weights/part_135:0,weights/weights/part_136:0,weights/weights/part_137:0,weights/weights/part_138:0,weights/weights/part_139:0,weights/weights/part_140:0,weights/weights/part_141:0,weights/weights/part_142:0,weights/weights/part_143:0,weights/weights/part_144:0,weights/weights/part_145:0,weights/weights/part_146:0,weights/weights/part_147:0,weights/weights/part_148:0,weights/weights/part_149:0 from checkpoint viewfs://... with weights/weights\r\n> 2019-05-22 15:00:02,026:INFO:tensorflow: Warm-starting variable: bias/bias; prev_var_name: Unchanged\r\n> 2019-05-22 15:00:09,259:DEBUG:tensorflow: Initialize variable bias/bias:0 from checkpoint viewfs://root/... with bias/bias\r\n> 2019-05-22 15:00:09,262:INFO:tensorflow: Create CheckpointSaverHook.\r\n> 2019-05-22 15:00:37,061:INFO:tensorflow: Graph was finalized.\r\n\r\nIs there anything wrong about what I'm doing ? Issue already known ?", "comments": ["Thanks, @nateagr , for the report. It's hard for us to debug the example as it is, though-- can you provide a minimal code sample that allows us to reproduce? The logs indicate that at least something is being warm-started, but it's hard to tell from that what might be missing.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "If your new estimator contains new Variables that are not contained in the old estimator from the checkpoint, will those new Variables be automatically initialized? Thanks."]}, {"number": 28968, "title": "Tensorflow lite elementwise operation not working in gpu delegate (ADD & SUB)", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NIL\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus 3 , Android 8.0.0\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): TF 1.13\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 5.4.0\r\n- GPU model and memory: NIL\r\n\r\n\r\n**Describe the current behavior**\r\n \r\nWhen we are trying to run a tflite model with element-wise operations like ADD and SUB (appended via custom code) on an android application (**org.tensorflow:tensorflow-lite:0.0.1-gpu-experimental**)   it throws an error message and crashes the application.\r\n\r\nAlso it fails during android benchmark test in gpu mode, with the same error message. However  the same model runs in CPU mode in the same android application and also clears the android benchmark test in CPU mode without errors.\r\n\r\nWe also tried the same model with tensorflow-lite gpu nightly version. But it does not give any any information about where the nodes are being run (i.e CPU or GPU). The problem is with the last couple of elementwise operations at the end of the model and it looks like they automatically falls back to CPU with '**org.tensorflow:tensorflow-lite:0.0.0-nightly**'. Even the cast operator (not supported by GPU), runs in nighlty tflite GPU delegate without any warning. We believe it is running in CPU, based on the speed or time taken to execute those nodes. TF-lite nightly doesn't give any info of whether it falls back to CPU or runs in GPU (unlike experimental version), when it encounters an unsupported op in GPU.Also it looks like errors and warnings are suppressed in nightlty version.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe tensorflow lite model should run without errors in GPU and CPU with element-wise operators.\r\n\r\n**Code to reproduce the issue**\r\n\r\nHere is the snapshot of the models which produced the aforementioned errors.\r\n\r\n1. Model 1:-\r\n\r\n![2](https://user-images.githubusercontent.com/1130185/58264257-2c691d80-7d9b-11e9-8f8f-0d10b9c94e32.png)\r\n\r\n\r\n2. Model 2:-\r\n\r\n![3](https://user-images.githubusercontent.com/1130185/58264318-473b9200-7d9b-11e9-8838-68871f946df3.png)\r\n\r\nModels: \r\n[Models.zip](https://github.com/tensorflow/tensorflow/files/3212972/Models.zip)\r\n\r\nBoth run in CPU mode; but fails in GPU !!!\r\n\r\n**Other info / logs**\r\n\r\n1. **Model 1 Error Log :  With ADD and SUB**\r\n\r\nadb shell /data/local/tmp/benchmark_model_gpu --graph=/data/local/tmp/work_test_trial_fulltest_257op_dm05_257_blend_cast_dbadd6.tflite --use_gpu=true\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/work_test_trial_fulltest_257op_dm05_257_blend_cast_dbadd6.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nUse legacy nnapi : [0]\r\nUse gpu : [1]\r\nAllow fp16 : [0]\r\nnnapi error: requires android sdk version to be at least 27\r\nLoaded model /data/local/tmp/work_test_trial_fulltest_257op_dm05_257_blend_cast_dbadd6.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\n**SUB: Incorrect operation type passed**\r\nFirst 77 operations will run on the GPU, and the remaining 2 on the CPU.\r\n**ERROR: TfLiteGpuDelegate Prepare: fuse_auto_input failed**\r\nERROR: Node number 79 (TfLiteGpuDelegate) failed to prepare.\r\n\r\nFailed to apply GPU delegate.\r\nAborted \r\n\r\n2. **Model 2 Error Log: With ADD only**\r\n\r\nadb shell /data/local/tmp/benchmark_model_gpu --graph=/data/local/tmp/work_test_trial_fulltest_257op_dm05_257_blend_cast_dbadd8.tflite --use_gpu=true\r\nadb: /opt/intel/intelpython27/lib/libcrypto.so.1.0.0: no version information available (required by adb)\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [/data/local/tmp/work_test_trial_fulltest_257op_dm05_257_blend_cast_dbadd8.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nUse legacy nnapi : [0]\r\nUse gpu : [1]\r\nAllow fp16 : [0]\r\nnnapi error: requires android sdk version to be at least 27\r\nLoaded model /data/local/tmp/work_test_trial_fulltest_257op_dm05_257_blend_cast_dbadd8.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: TfLiteGpuDelegate Prepare: fuse_auto_input failed\r\nERROR: Node number 76 (TfLiteGpuDelegate) failed to prepare.\r\n\r\nFailed to apply GPU delegate.\r\nAborted\r\n\r\nAlso refer: Issue #28606\r\n\r\nIs the issue fixed in latest nightly or experimental versions?\r\nWe tried the following gradle dependencies in app:-\r\n1. org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly\r\n2. org.tensorflow:tensorflow-lite:0.0.1-gpu-experimental\r\n\r\nBut it does not run properly with either of the versions ...", "comments": ["I managed to build the tflite and tflite-gpu aar from source by downloading the latest tensorflow and compiling them using bazel. I followed the instruction from the following  link- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/BUILD. (Hope this way we can get the latest updates..)\r\n\r\nNow the error ''SUB: Incorrect operation type passed' seems to be resolved.Also this time we were able to get the complete error log about the node execution( CPU or GPU) in android.Also the NNAPI package not found error was also resolved(but it's still there when we use Jcenter gradle dependency).\r\n\r\nHowever two other issues still remains unresolved...\r\nHere is the error log \r\n\r\n```\r\n05-25 12:47:13.384 15160-15190/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n    Process: android.example.com.tflitecamerademo, PID: 15160\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    **SUB: Expected 2 input tensor(s), but node has 1 runtime input(s).**\r\n    **First 77 operations will run on the GPU, and the remaining 4 on the CPU.TfLiteGpuDelegate Prepare: Number of outputs exceeds 1Node number 81 (TfLiteGpuDelegate) failed to prepare.**\r\n\r\n```\r\n\r\n![error_tflite](https://user-images.githubusercontent.com/1130185/58366387-0f456380-7eef-11e9-8fe6-888c312f0650.png)\r\n\r\nIssues:- \r\n1. SUB: Expected 2 input tensor(s), but node has 1 runtime input(s).\r\n2. Number of outputs exceeds 1Node number\r\n3. Even though all nodes are GPU supported ops, its falling back to CPU\r\n4.  TfLiteGpuDelegate Prepare: fuse_auto_input failed (ADD & SUB)\r\n\r\n![error2](https://user-images.githubusercontent.com/1130185/58366472-51bb7000-7ef0-11e9-9a51-e6b1381cdcdc.png)\r\n\r\nThe same issue (Number of outputs exceeds 1Node number) exists in the above model.\r\n\r\nHowever in CPU mode the models run without any issues...\r\n\r\n[models2.zip](https://github.com/tensorflow/tensorflow/files/3219213/models2.zip)\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Here is another example where add, sub and mul have different behaviours..\r\n\r\n1. Add\r\n\r\n```\r\nError:- ERROR: TfLiteGpuDelegate Prepare: fuse_auto_input failed\r\nERROR: Node number 76 (TfLiteGpuDelegate) failed to prepare.\r\n\r\n```\r\nFailed to apply GPU delegate.\r\nAborted\r\n\r\n![add](https://user-images.githubusercontent.com/1130185/58468457-33a57800-815b-11e9-92d4-d291884496ff.png)\r\n\r\n2. Sub\r\n\r\nError:- Falls back to CPU\r\n\r\n```\r\nERROR: Next operations are not supported by GPU delegate:\r\nSUB: Incorrect operation type passed\r\nFirst 75 operations will run on the GPU, and the remaining 1 on the CPU.\r\nApplied GPU delegate.\r\n```\r\n\r\n\r\n![sub](https://user-images.githubusercontent.com/1130185/58468501-48820b80-815b-11e9-8d69-bde2c9095750.png)\r\n\r\n**But the same model works with mul !!!**\r\n\r\n![mul](https://user-images.githubusercontent.com/1130185/58468544-5c2d7200-815b-11e9-8865-c510a4de38d4.png)\r\n\r\nAlso all the above models works with CPU ...", "We tried to benchmark the above models in latest android benchmark tool; but got \"fuse_auto_input failed\" error for SUB and ADD operators", "@anilsathyan7 \r\n\r\nThanks for atttaching the model.  I'll pass that on to one of our engineers.\r\n\r\nre: fuse_auto_input, you can see that the error comes from gpu/gl/compiler.cc when options_.auto_input_fusion is enabled.  This is not the external option, but an internal one.  If this is blocking you, I would suggest disabling fuse_auto_input.", "How can we disable fuse_auto_input during tflite conversion?", "Not during conversion.\r\n\r\nIn `tensorflow/lite/delegates/gpu/gl_delegate.cc`, add `compile_options.auto_input_fusion = false;` to where the rest of `compile_options` are specified.", "@impjdi Any updates on this? Will there be any performance hit if auto_input_fusion is disabled?", "@impjdi When will this bug be resolved? The compile options for `auto_input_fusion` does not seem to work for TFLiteConverter in TF 2.0.", "Sorry, forgot to reply to your question in December.\r\n\r\nSince I haven't heard back from @anilsathyan7 , I assume he's good.  I usually don't close issues myself, leaving it up to the OP.\r\n\r\nI ran his TFLite models just now and things seem to run fine--that's with TFLiteGpuDelegateV2 on a Pixel 4.  Please feel free to reopen if this issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28968\">No</a>\n", "Nope, never mind.  I can see this fails when I force things to be run on OpenGL.", "Okay.  I looked deeper into this.  I found the issue but the fix is not trivial at the moment, i.e. it's gonna take some time to fix this.\r\n\r\nThe pattern this `fuse_auto_input` is not able to handle is the following diamond case:\r\n\r\n`A -> B -> D`\r\n`A -> C -> D`\r\n\r\nWhen it comes to fusing `B`, `C`, and `D` into a fused `D`, the rule assumes that the inputs of `B` and `C` are different tensors, but in this case, they are the same (`A`) and the fusion fails.", "commit 8eaea75c6d89239e88c8c4b0a0077cba5c519ef2 should guard this issue.  There will be a proper fix, but for now, this should make your network not crash anymore.", "Came across here while googling same issue.\r\n\r\nNo problem\r\n``` python\r\nxy_div0 = tf.fill(tf.shape(feat_xy0), 1.23)\r\nbox_xy0 = tf.sigmoid(feats0[:, :, :, 0:1]) / xy_div0\r\n\r\n# or\r\n\r\nbox_xy0 = tf.sigmoid(feats0[:, :, :, 0:1]) / 1.23\r\n```\r\n\r\nProblem\r\n``` python\r\nxy_div0 = tf.fill(tf.shape(feat_xy0), div_shape[0])\r\nbox_xy0 = tf.sigmoid(feats0[:, :, :, 0:1]) / xy_div0\r\n\r\n# or, of course\r\n\r\nbox_xy0 = tf.sigmoid(feats0[:, :, :, 0:1]) / div_shape[0]\r\n```\r\n\r\nThis is frustrating. All I wanted to do is simple divide and I cannot do it.\r\nI will keep digging whether there is a workaround to do this.", "@JeiKeiLim does mul work instead of div?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28968\">No</a>\n"]}, {"number": 28967, "title": "windows 10 installation with cpu version", "body": "**System Information:**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?: virtualenv with pip v19.1.1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A (CPU version)\r\n\r\n\r\n\r\n**Describe the Problem:**\r\nI have installed the Microsoft Visual C++ 2015 Redistributable Update 3 and Python 3.6. I have tried several releases of Python 3.6, but each time I receive a runtime load error as described below\r\n\r\n**Commands:**\r\n```\r\n>.\\venv\\Scripts\\activate\r\n(venv) > pip install --upgrade tensorflow\r\n(venv) > python -c \"import tensorflow as tf;\"\r\n```\r\n**Stack Trace:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\aherk\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\aherk\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\aherk\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\aherk\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\aherk\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n```\r\n", "comments": ["@aherkel09 Can you confirm your CPU supports AVX2 instructions? ", "@gadagashwini Unfortunately, my CPU does not support AVX2. Thanks for the response."]}, {"number": 28966, "title": "[TF2.0] Gradients become None when using a SparseTensor kernel in a custom layer.", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 10.14.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: running on mac/cpu\r\n- GPU model and memory: running on mac/cpu\r\n\r\n**Describe the current behavior**\r\nIn a DeepRL setting, using a DQN implementation in TF2. After a certain number of calls, the custom training loop with gradient tape is not able to compute gradient for the SparseMatrices used in the model (weights inside the sparse matrice), the gradient for biases is computed without any issue. \r\nThe model can still be called, the weights are not zero. The gradients in _SparseTensorDenseMatMulGrad() are not None, and seem ok. That is, a_values_grad and b_grad are not None (also not None in _gradient_function)\r\n\r\n`(None, a_values_grad, None, b_grad)`\r\n\r\nBut the output of `pywrap_tensorflow.TFE_Py_TapeGradient()` in `imperative_grad` has None for the weights in the SparseTensor. \r\n\r\nIt is worth noting that if I do not call the model before the first training loop, the gradients are OK. Next loop they become None. \r\n\r\n**Describe the expected behavior**\r\nValid gradient for the weights in the SparseTensor whatever the number of calls done previously to the model.\r\n\r\n**Code to reproduce the issue**\r\nThe model is quite complex. But the issue seem to come from one of the custom Layer which has a kernel defined like so:\r\n```python \r\nkernel_weights = self.add_weight('kernel_weights',\r\n                                         shape=[self.indices.shape[0]],\r\n                                         initializer=self.kernel_initializer,\r\n                                         regularizer=self.kernel_regularizer,\r\n                                         constraint=self.kernel_constraint,\r\n                                         dtype=self.dtype,\r\n                                         trainable=self.trainable)\r\n\r\nself.kernel = tf.SparseTensor(self.indices,\r\n                                      kernel_weights,\r\n                                      self.dense_shape)\r\n```\r\n\r\nAnd a call function defined as so:\r\n```python\r\ndef call(self, x, **kwargs): \r\n        x = tf.transpose(tf.sparse.sparse_dense_matmul(self.kernel, x, adjoint_b=True))\r\n        if self.use_bias:\r\n            x += self.bias\r\n        x = self.activation(x)\r\n        return x\r\n```\r\n\r\nI tried to optimize the layer outside of the RL setting, with a dummy loss, and the issue remains. Here is the code for the custom loop:\r\n\r\n```python\r\noptimizer = Adam(learning_rate=1e-3)\r\nx = tf.Variable(tf.ones((160, 41)))\r\ndata = Dataset.from_tensor_slices((x)).batch(20) \r\nfor obs0 in data:\r\n    with tf.GradientTape() as tape: \r\n        x = model(obs0) \r\n        \r\n        loss_value = tf.reduce_mean(x, name='dummy_loss')\r\n        \r\n        grads = tape.gradient(loss_value, model.trainable_weights)\r\n        # Here the gradient for the sparse kernel is None on the second loop\r\n        #The gradient for the bias is fine\r\n        \r\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nW0523 16:11:28.381706 4529878464 optimizer_v2.py:928] Gradients does not exist for variables ['custom_layer/kernel_weights:0'] when minimizing the loss.\r\n```", "comments": ["Ok so I solved the issue, at least the gradients are not None. I'm not sure if its a bug or not.\r\n\r\nBut if instead of storing the SparseTensor as a kernel i store the weights and build the sparse tensor at each call it work (albeit quite slow..):\r\n\r\nbuild()\r\n```python\r\n\r\n        self.kernel_weights = self.add_weight('kernel_weights',\r\n                                         shape=[self.indices.shape[0]],\r\n                                         initializer=self.kernel_initializer,\r\n                                         regularizer=self.kernel_regularizer,\r\n                                         constraint=self.kernel_constraint,\r\n                                         dtype=self.dtype,\r\n                                         trainable=self.trainable)\r\n```\r\n\r\nAnd call()\r\n```python\r\nkernel = tf.SparseTensor(self.indices, self.kernel_weights, self.dense_shape)\r\nx = tf.transpose(tf.sparse.sparse_dense_matmul(kernel, x, adjoint_b=True))        \r\n``` \r\n\r\nHappy to close the issue, although im not sure if was the intended behaviour.", "Closing this issue since its resolved. Feel free to reopen if have any further questions. Thanks!", "Wanted to bump this up because I am running into a similar issue. I have a couple of layers I have written with \r\n`tf.sparse.sparse_dense_matmul` that are not producing a gradient.\r\nand also produce this warning:\r\n`WARNING:tensorflow:Gradients do not exist for variables ['rgcn_autencoder/RGCN_Input0/graph_order_0_PcP_W_1:0', 'rgcn_autencoder/RGCN_Input0/graph_order_0_PbA_W_1:0',...when minimizing the loss.`\r\n```\r\n\r\nclass RGCN_Input(Layer):\r\n    def __init__(self,out_dim,graph_index,\r\n                 relations= ['PcP','PbA','PiV','PuK','PbP','AwP','VpP','KpP','self'],\r\n                 name=\"RGCN_Input\",\r\n                 activation=None,\r\n                 use_bias=False,\r\n                 kernel_initializer='glorot_uniform',\r\n                 bias_initializer='zeros',\r\n                 kernel_regularizer=None,\r\n                 bias_regularizer=None,\r\n                 **kwargs\r\n                 ):\r\n        super(RGCN_Input, self).__init__(name=name+str(graph_index),**kwargs)\r\n        self.out_dim = out_dim\r\n        self.relation = relations\r\n        self.support = len(self.relation)\r\n        self.graph_index = graph_index\r\n        self.use_bias = use_bias\r\n        self.activation = tf.keras.activations.get(activation)\r\n        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\r\n        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\r\n        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\r\n        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\r\n\r\n    def build(self,input_shape):# define w & b\r\n        self.kernel = tf.concat([\r\n            self.add_weight(shape=(input_shape[1][1],self.out_dim),\r\n                            name='graph_order_{}_{}_W_1'.format(self.graph_index,self.relation[i]),\r\n                            trainable=True,\r\n                            initializer=self.kernel_initializer,\r\n                            regularizer=self.kernel_regularizer,\r\n                           )\r\n                            for i in range(self.support)]\r\n            ,0)\r\n        \r\n        if self.use_bias:\r\n            self.bias = self.add_weight(shape=(self.units,),\r\n                                        initializer=self.bias_initializer,\r\n                                        name='graph_order_{}_B_1'.format(self.graph_index),\r\n                                        regularizer=self.bias_regularizer,\r\n                                        )\r\n        else: self.bias = None\r\n        \r\n        super(RGCN_Input, self).build(input_shape)\r\n\r\n    def call(self,x): # define one r-gcn architecture\r\n        assert isinstance(x, (list,tuple))\r\n        adj_list,feature = x\r\n        supports=[]\r\n        if isinstance(adj_list[0], tf.SparseTensor):\r\n            for i in range(self.support):\r\n                supports.append(tf.sparse.sparse_dense_matmul(adj_list[i],feature))\r\n        else:\r\n            for i in range(self.support):\r\n                supports.append(tf.matmul(adj_list[i],feature))\r\n        support = tf.concat(supports,1) #squish them all together\r\n        H = tf.matmul(support,self.kernel)\r\n\r\n        if self.use_bias:\r\n            H+=self.bias\r\n\r\n        \r\n        if self.activation is not None:\r\n            H = self.activation(H)\r\n        \r\n        return H\r\n    \r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[1][0], self.output_dim)\r\n```\r\n"]}, {"number": 28965, "title": "3 more cherrypicks", "body": "", "comments": []}]