[{"number": 22557, "title": "include <cstring> in a few places that use it", "body": "This is a bit of a drive-by, but I was doing some builds with c++17 and triSYCL, but fell across these 3 errors with gcc version 8.1.1 20180712 (Red Hat 8.1.1-5) (GCC)  on Fedora 28.\r\n\r\nThese must be getting via another header somehow in normal builds.\r\n\r\nI work for Red Hat so should be covered under a CLA already.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 22556, "title": "fix: slot and primary can be different shape", "body": "See github issue #19457", "comments": ["@allenlavoie \r\nthe pr \u201chttps://github.com/tensorflow/tensorflow/pull/20539\u201d relateed commit is wrong, i create a new pr again."]}, {"number": 22555, "title": "[Feature Request]Add support of model.eval() and model.train()", "body": "For pytorch, you can use model.eval or mdoel.train to make sure different behavior like dropout.\r\nFor tf keras interface however if you write class using keras.Model, you still need to pass all training=training in call function, I think this is less elegant. \r\nSo can we support model.train, model.eval ? when set model.train then all layer and model declared in __init__ is in training mode.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@chenghuige This is a valid point. We will note it down.", "@wt-huang any update about this? Why did you close the issue? @chenghuige  did you find any beautiful solution?\r\n", "@Oktai15  \r\nfrom keras import backend as K\r\nprint K.learning_phase()\r\ntrain_step.run(feed_dict={x: batch[0], labels: batch[1], K.learning_phase(): 1})\r\n\r\nthis might help", "Are there any updates on this? It's not clear if the feature request was added or not. "]}, {"number": 22554, "title": "Adding custom op instructions lacking the GPU building instructions", "body": "Tensorflow [Add a new op tutorial](https://www.tensorflow.org/extend/adding_an_op) never mention the building instructions for GPU ops. \r\n\r\nI find one [here](https://gist.github.com/Sergio0694/fc94fb14388ee4b7b92be6e33704e5b9), but it met the similar issue with [Building custom op instructions out of date ](https://github.com/tensorflow/tensorflow/issues/13607).\r\n`\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./libcuda_op.so: undefined symbol: _ZTIN10tensorflow8OpKernelE`\r\n\r\n\r\nTensorflow really needs to give some suggestions on how to build GPU ops.", "comments": ["The one from the above link:\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.5)\r\n\r\nset(CMAKE_C_COMPILER /opt/cuda/bin/gcc)\r\nset(CMAKE_CXX_COMPILER /opt/cuda/bin/g++)\r\n\r\nexecute_process(COMMAND python3 -c \"import tensorflow; print(tensorflow.sysconfig.get_include())\" OUTPUT_VARIABLE Tensorflow_INCLUDE_DIRS)\r\n\r\nfind_package(CUDA)\r\n\r\nset(CMAKE_CXX_FLAGS \"-std=c++11 -O2 ${CMAKE_CXX_FLAGS}\")\r\n\r\nif (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 5.0 OR CMAKE_CXX_COMPILER_VERSION VERSION_EQUAL 5.0)\r\n  set(CMAKE_CXX_FLAGS \"-D_GLIBCXX_USE_CXX11_ABI=0 ${CMAKE_CXX_FLAGS}\")\r\nendif()\r\n\r\nSET(CUDA_PROPAGATE_HOST_FLAGS ON)\r\n\r\ninclude_directories(${Tensorflow_INCLUDE_DIRS})\r\n\r\ncuda_add_library(\r\n    add_one SHARED\r\n    cuda_op_kernel.cu\r\nadd_one.cc)\r\n```\r\n\r\nCurrently Tensorflow only provides instructions for CPU ops as:\r\n\r\n```\r\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2\r\n```", " The tutorial does mention the build instructions for GPU ops: https://www.tensorflow.org/extend/adding_an_op#gpu_support", "Thanks! @ppwwyyxx "]}, {"number": 22553, "title": "fixed calling Estimator '_distribution' method", "body": "The code is calling a '_distribution' method which no longer exists.  It will produce error message <AttributeError: 'Estimator' object has no attribute '_distribution'> when training mirrored strategy with hooks.   \r\n\r\nChanging '_distribution' to '_train_distribution' can fix the issue.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @caisq: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for fixing @vbvg2008 . Looks like there is another PR for this on the new repo: https://github.com/tensorflow/estimator/pull/3#pullrequestreview-168946557\r\nWe will get that merged. "]}, {"number": 22552, "title": "TFLite Model Optimizer: Doesn't work on Transformer Model", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n```\r\ntflite_convert \\\r\n  --output_file=foo.tflite \\\r\n  --saved_model_dir=./saved\r\n```\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have a SavedModel of the Transformer from the Models repo (https://github.com/tensorflow/models/tree/master/official/transformer). I am trying to use the 'Model Optimization Toolkit for TensorFlow' available in TFLite. The documentation states that I should be able to convert a SavedModel into a TFLite Model using the above command. However, I get an error: ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[None, None]'. (see full log below).\r\n\r\nThis is a feature request to allow TFLite to quantize variable length sequence models. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n2018-09-26 16:06:20.690105: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-09-26 16:06:20.712622: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2018-09-26 16:06:23.327033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:344] Starting optimization for grappler item: tf_graph\r\n2018-09-26 16:06:25.991930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:344] Starting optimization for grappler item: tf_graph\r\n2018-09-26 16:06:26.384859: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:344] Starting optimization for grappler item: tf_graph\r\nTraceback (most recent call last):\r\n  File \"/home/<>/int8/virt2.7/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/<>/int8/virt2.7/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 408, in convert\r\n    \"invalid shape '{1}'.\".format(_tensor_name(tensor), shape))\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[None, None]'\r\n```", "comments": ["@vkmenon Currently quantization for variable length sequence models is not supported. ", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello, I was wondering if this is now supported?"]}, {"number": 22551, "title": "tf.distributions.Uniform does not produce unique samples", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: debian 9 (stretch)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: see below (short version: `N=int(1e4); assert len(tf.unique(tf.distributions.Uniform().sample(N)).y.eval()) == N`)\r\n\r\n\r\n### Describe the problem\r\n\r\nTraced subtle bug in my code back to the fact that the TF Uniform distribution (maybe others?) does not produce unique samples.. This is bad.\r\n\r\n```\r\nN = int(1e4)\r\n\r\nlen(np.unique(np.random.rand(N))) # 10000\r\n\r\ntf.InteractiveSession()\r\nlen(tf.unique(tf.distributions.Uniform().sample(N)).y.eval()) # 9993\r\n```", "comments": ["If I understand correctly, uniform is a distribution. It makes sense that we might get duplication values from sample method.", "@meereeum @facaiy   Can this be closed ?", "Sure, it's a distribution! But, computationally speaking, there are >2^24 possible 32-bit floats between 0 and 1, at a low estimate (see e.g. https://lemire.me/blog/2017/02/28/how-many-floating-point-numbers-are-in-the-interval-01 ). So - even accounting for bias in generating randomness - there should not be ~7 conflicts / 10000. (Probability theory-wise, we would draw two identical samples with probability 0.)\r\n\r\nAt the very least, it should be noted in the docs that TF uses less entropy for random number generation than other libraries (if this is the source of the issue), since this behavior deviates from user expectations.", "Actually, you _would_ expect about that many collisions in 10,000 draws from a space of 2^23 possibilities, because of the \"[birthday paradox](https://en.wikipedia.org/wiki/Birthday_problem)\".  For instance,\r\n\r\n```\r\nN = int(1e4)\r\nlen(np.unique(np.random.randint(low=0, high=2**23, size=N)))\r\n9993\r\n```\r\n\r\nThere are 2^23 32-bit floats between 1.0 and 2.0.  In that range, the behavior of numpy and of TensorFlow agree (once you correct for numpy's default floating point precision being 64 bits):\r\n```\r\nlen(np.unique(np.random.uniform(low=1., high=2., size=N).astype(np.float32)))\r\n9992\r\nlen(np.unique(tf.distributions.Uniform(low=1., high=2.).sample(N).eval()))\r\n9994\r\n```\r\n\r\nThe range 0.0 to 1.0 is more interesting, because floating-point precision increases exponentially near 0.  There are ~2^30 32-bit floats between 0.0 and 1.0, but, of course, the continuous-uniform distribution on [0.0, 1.0) does not map into the discrete-uniform distribution on those 2^30 floats, because the floats near 0 are denser and thus lower probability.  Here TensorFlow and numpy disagree somewhat: In the interest of speed, TensorFlow loses precision near 0 (internally, it actually generates a float between 1.0 and 2.0, and then subtracts 1).  Numpy goes to greater lengths to conserve this precision, but even that doesn't defeat the brithday attack completely: there are 2^23 32-bit floats between 0.5 and 1.0, so that space is sparse enough to generate collisions by itself regardless of how careful one is at 0:\r\n\r\n```\r\nfor _ in range(10):\r\n  print(len(np.unique(np.random.uniform(low=0., high=1., size=N).astype(np.float32))))\r\n10000\r\n9999\r\n9999\r\n9998\r\n9996\r\n9995\r\n9998\r\n9999\r\n9996\r\n10000\r\n```", "If uniqueness is important in your application, you could increase precision to 64 bits:\r\n```\r\nfor _ in range(10):\r\n  print(len(np.unique(tf.distributions.Uniform(\r\n    low=np.float64(0.), high=np.float64(1.)).sample(N).eval())))\r\n10000\r\n10000\r\n10000\r\n10000\r\n10000\r\n10000\r\n10000\r\n10000\r\n10000\r\n10000\r\n```\r\nor, of course, detect duplicates explicitly.\r\n\r\nIf you really need a floating-point uniform distribution in TensorFlow with high precision near 0, please file a feature request for that.  It's likely to be enough slower to be a separate code path, and probably is a better fit for [TensorFlow Probability](https://github.com/tensorflow/probability) than for TensorFlow proper. Closing this issue as \"intended behavior\".", "@axch interesting, and great catch - thanks for the thorough response.\r\nalso, I hadn't realized it was possible to generate 64-bit uniforms by modifying the default params.\r\n\r\nfor posterity, the chance of having at least 1 collision with 10,000 samples & 2^23 slots - as per default TF uniform sampling on [0,1) - is:\r\n```\r\nfrom scipy.special import gammaln\r\n\r\nlog_factorial = lambda x: gammaln(x+1)\r\nlogprob_unique = lambda n, k: log_factorial(n) - (k * np.log(n) + log_factorial(n-k))\r\n\r\n1 - np.exp(logprob_unique(2**23, 10**4)) # 0.997425855296802\r\n```"]}, {"number": 22550, "title": "MirroredStrategy and Optimizer Compatibility", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, duplicated the minimal code from documentation snippet [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api)\r\n\r\nBut the included [keras_mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/keras_mnist.py) example is broken as well.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDocker container `tensorflow/tensorflow:1.11.0-rc2-gpu-py3`\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n2 x 1080ti\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n[keras_mnist.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/keras_mnist.py) example seems to break. Investigating further... \r\n\r\nOptimizers other than `tf.train.GradientDescentOptimizer` are not working with `MirroredStrategy` for Keras models as shown in this official [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api)\r\n\r\n\r\n### Source code / logs\r\n**Setup**, copied from the [example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute#example-with-keras-api):\r\n\r\n```python\r\ninputs = tf.keras.layers.Input(shape=(1,))\r\npredictions = tf.keras.layers.Dense(1)(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n\r\nfeatures = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\nlabels = tf.data.Dataset.from_tensors([1.]).repeat(10000).batch(10)\r\ntrain_dataset = tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\n```\r\n\r\n**Then, this works**:\r\n```python\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.2),\r\n              distribute=distribution)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\n\r\n\r\nBut, **Keras optimizers** don't seem to work:\r\n```python\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=tf.keras.optimizers.SGD(lr=0.2, momentum=0.9),\r\n              distribute=distribution)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\n\r\nand these **Tensorflow Optimizers** are throwing errors as well:\r\n\r\n```python\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=tf.train.AdamOptimizer(learning_rate=0.2),\r\n              distribute=distribution)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\n\r\n```python\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=tf.train.MomentumOptimizer(learning_rate=0.2, momentum=0.9),\r\n              distribute=distribution)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\n\r\n```python\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=tf.train.RMSPropOptimzier(learning_rate=0.2),\r\n              distribute=distribution)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=10)\r\n```\r\n\r\n\r\n\r\nError looks like:\r\n```bash\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 1590, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 125, in fit_loop\r\n    orig_model_weights = model.get_weights()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py\", line 483, in get_weights\r\n    return backend.batch_get_value(weights)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 2717, in batch_get_value\r\n    return get_session().run(tensors)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 465, in get_session\r\n    _initialize_variables(session)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 710, in _initialize_variables\r\n    variables = _get_variables(ops.get_default_graph())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 704, in _get_variables\r\n    variables.update(opt.optimizer.variables())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 785, in variables\r\n    optimizer_variables = [v for v in self._non_slot_variables()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 786, in <listcomp>\r\n    if _from_current_graph(v)]\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 779, in _from_current_graph\r\n    return variable.op.graph is current_graph\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 305, in op\r\n    return self.get().op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 73, in get\r\n    (device, self._index.keys(), device_util.current())), e)\r\n  File \"<string>\", line 3, in raise_from\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:0']) (current device )\r\n```\r\n\r\n\r\n\r\n", "comments": ["I meet same problem.\r\nI specify the cpu\r\nstrategy = tf.contrib.distribute.MirroredStrategy(\r\n   ['/device:CPU:0', '/device:GPU:0', '/device:GPU:1'])\r\nthe code can be run\r\nbut GPU:0 Util more than 90%\r\nGPU:1 Util is lower", "This was fixed recently. Can you retry using the latest TensorFlow nightly build? Thanks!", "Hi @anj-s ,\r\n\r\nCould you tell me which commit fixed this issue? Also, is the fix in 1.12.0-rc0?", "@erwa The commit that fixed this issue is: 6ebe9baae06c06d0a70a424a55c78f5af07b49f7\r\nThis fix is in 1.12.0-rc0. \r\n\r\nClosing this issue now. Please reopen this issue if needed. Thanks!", "the issue is still present in release version 1.12.0. I've just called `K.get_session()` inside of an estimator hook in  the `end` method", "Still the same problem in 1.13.0-rc0. ", "Have this issue in the release version of 1.12.0.. Any tips on how to get around this? @anj-s \r\nI tried @tksj09  suggestion of manually specifying the CPU, but getting a different error as specified in the issue I just opened"]}, {"number": 22549, "title": "Add abseil_cpp cmake dependence.", "body": "This patch enhance cmake build and add proper dependency to both **external** path (by fetching and compiling from scratch) and **system** path (existing library on the system).\r\n\r\n\r\n  **Abseil_CPP** is required and referenced in tensorflow code:\r\n\r\n```\r\ntensorflow]$ find . -name '*.h' -exec grep -H '#include \"absl/' {} \\; | wc -l\r\n214\r\n```\r\n\r\n---\r\n\r\nFew outlines:\r\n\r\n***New module***\r\n\r\n*  **1** New specialized ```modules/FindAbseilCpp.cmake``` module was added in new folder. The ```abseil_cpp``` upstream project doesn't expose cmake or pkgconfig helpers so a custom module capable of searching it system wide was added. More modules like this for all existing packages (that have poor/buggy/lacks support in its cmake exposure) will be proposed. The module is reliable on mswin and linux too. It has minimal lines of code that can be maintained.\r\n\r\n* **2** ```-DABSEIL_CPP_INCLUDE_DIR_HINTS``` and ```-DABSEIL_CPP_LIBRARIES_DIR_HINTS``` can be used as extra path hints for the search task (e.g. in case of very obscure places). These two VARS will be default feature for any future modules like this.\r\n\r\n* **3** The proposed module can search through specified list of COMPONENTS, otherwise will export all available components from **system**. It happens that in the case of abseil_cpp we have more libraries as components (quite a lot).\r\n\r\n***Rule change proposal***\r\n\r\n* **4** ```ExternalProject_Add(abseil_cpp_build)``` notice the **_build** suffix of label. More changes (appending **_build**) for all existing module will be proposed. The reason is that in the case of **system** libraries, the very project name e.g. **re2** will conflict inside cmake with the library name **re2** appended into ```tensorflow_EXTERNAL_DEPENDENCIES```. Thus extra discrimination e.g. **_build** tag is required otherwise cmake will throw error on it (it will appear only in **system** scenario).\r\n\r\n* **5** ```tensorflow_EXTERNAL_LIBRARIES``` and ```tensorflow_EXTERNAL_DEPENDENCIES``` plus  ```include_directories()```  are updated right in this related module ```external/abseil_cpp.cmake``` and not from very main ```CMakeLists.txt```. More changes like this will come for all rest of packages.\r\n\r\n@perfinion ,\r\n\r\n  If this is passed and rule changes agreed, very next PR will update the rest of existing modules. Then will continue with PR series relating other issues.\r\n\r\n\r\nThank you !", "comments": ["Thanks for your contribution. Please be aware that there is a [proposal](https://github.com/tensorflow/community/pull/18/files#diff-eef567fcebc6a5988c0fa92c7bb38526R173) to remove `tensorflow/contrib/cmake/` now that Bazel is supported on all of the platforms for which we build TensorFlow binaries. You might want to comment on that discussion if you rely on this build system.", "Nagging Assignee @perfinion: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@cbalint13 Are you also going to upstream the .cmake files to abseil-cpp directly as well? Then in a future version we'd be able to drop it from TF.", "@mrry yeah cmake is going to go away but cbalint13 is up for maintaining it (discussed on the build@ mailing list) so we'll get it in now and see how things go in future. If it works well then we can have cmake as a separate repo or something.", "Hm can't see why the last test failed. This looks good to me tho. Unassigning myself so it'll assign and merge."]}, {"number": 22547, "title": "R1.11", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I'm assuming this pull request wasn't intentional?", "Yes, apologies.\r\n\r\nFrom: Jonathan Hseu <notifications@github.com>\r\nSent: Wednesday, September 26, 2018 5:20 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: De St Aubin, Michael <mdestaubin@gsd.harvard.edu>; Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] R1.11 (#22547)\r\n\r\n\r\nI'm assuming this pull request wasn't intentional?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_pull_22547-23issuecomment-2D424871898&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=yP5L7kbeUduP66L9SQ4EagY2QANCXhFhraH_JGDqMeM&m=QqarJ_S0_qWEaagNUGm5wvUTSVbOddQNtFNN-cNG9Rc&s=X0nb_jcW7ctSVofffEM3Wp8ZICYoq8uLyzitnCmHD8c&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AedFuOtXKMo0KeBKiead9Nn6Z59W24-5Fnks5ue-2D-2DbgaJpZM4W7Z5T&d=DwMCaQ&c=WO-RGvefibhHBZq3fL85hQ&r=yP5L7kbeUduP66L9SQ4EagY2QANCXhFhraH_JGDqMeM&m=QqarJ_S0_qWEaagNUGm5wvUTSVbOddQNtFNN-cNG9Rc&s=kgid2Yf8I6wHh8X7W4dgDFm6AfgKjKuB8TdjGVy7uUM&e=>.\r\n"]}, {"number": 22546, "title": "Number of intra_op_parallelism_threads Issue with TF 1.10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra version 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: None (It's a CPU build)\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Please see sections below\r\n\r\n### Describe the problem\r\n\r\n\r\nIn tensorflow 1.7.0, getting the number of threads in the intra_op pool was possible with the following line of code in C++:\r\n`context->device()->tensorflow_cpu_worker_threads()->num_threads`\r\nwhere context is an object of type OpKernelContext. (because of https://github.com/tensorflow/tensorflow/blob/7bccde15ce0dd29dce62092a5e9d48ffdc772963/tensorflow/core/common_runtime/local_device.cc#L43).\r\n\r\nIf the corresponding ConfigProto was set using the following python code:\r\n```\r\nimport tensorflow as tf \r\nconf = tf.ConfigProto(inter_op_parallelism_threads=5, intra_op_parallelism_threads=4) \r\nsess = tf.Session(config=conf)\r\n```\r\n\r\nTF 1.7 would output the value of `context->device()->tensorflow_cpu_worker_threads()->num_threads` as `4`.\r\n\r\nTF 1.10 outputs the value of `context->device()->tensorflow_cpu_worker_threads()->num_threads` as `1`, irrespective of the intra_op_parallelism_threads value specified in the ConfigProto.\r\n\r\nI was wondering if something changed in the TF upgrade from 1.7.0 to 1.10.0 to cause this piece of code to give a different value or is the number of threads in the threadpool truly 1?\r\n\r\n\r\nOther relevant information:\r\n\r\n- macOS High Sierra version `10.13.6`\r\n\r\n- Working TF version: `1.7.0`\r\n\r\n- Breaking TF version: `1.10.0`\r\n\r\n- Python information: Python 2.7.10 (default, Oct 6 2017, 22:29:07) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)] on darwin\r\n\r\n- Detailed debugging information from tensorflow (by setting `TF_CPP_MIN_VLOG_LEVEL=3` before running the python program) says that TF 1.10 is indeed planning to create a thread pool of size 4 on this line of C++ code. The C++ piece of code at the start of this post however outputs value 1.\r\n\r\n- This is the first and only session being created in the program.\r\n", "comments": ["I have tried a small test with direct session and it worked. Note there are a few places where this parameter is not used or being overridden. For example, whenever we run the graph via GraphRunner.  If you have an op and the context in which it was called I can try to find a more precise explanation.", "@priyankjain Hi, can you provide some example code snippet relevant to the context.", "Here is the C++ OP code\r\n```cpp\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/common_shape_fns.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"NumIntraOpThreads\")\r\n.Output(\"num_intra_op_threads: int32\")\r\n.SetShapeFn(tensorflow::shape_inference::ScalarShape)\r\n.Doc(R\"doc(\r\nA tensorflow OP that returns the number of threads in the intra_op_parallelism pool\r\n)doc\");\r\n\r\nclass NumIntraOpThreads : public OpKernel {\r\n public:\r\n  explicit NumIntraOpThreads(OpKernelConstruction* context)\r\n      : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    int num_intra_op_threads = context->device()->tensorflow_cpu_worker_threads()->num_threads;\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}), &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n    output_flat(0) = num_intra_op_threads;\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"NumIntraOpThreads\").Device(DEVICE_CPU), NumIntraOpThreads);\r\n```\r\n\r\nHere are the flags I am using to compile the OP\r\n```bash\r\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\ng++ -std=c++11 -shared -undefined dynamic_lookup num_intra_op_threads.cc -o num_intra_op_threads.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2\r\n```\r\n\r\nThe python code to test the output of the OP:\r\n```python\r\nimport tensorflow as tf\r\nprint('Tensorflow version is ', tf.__version__)\r\nn_module = tf.load_op_library('./num_intra_op_threads.so')\r\nprint(tf.__version__)\r\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)):\r\n  print(n_module.num_intra_op_threads().eval())\r\n```\r\n\r\nOutput of the above python script with TF 1.7:\r\n```bash\r\n('Tensorflow version is ', '1.7.0')\r\n1.7.0\r\n2018-10-02 12:25:36.949725: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n10\r\n```\r\nOutput with TF 1.8:\r\n```bash\r\n('Tensorflow version is ', '1.8.0')\r\n1.8.0\r\n2018-10-02 13:05:41.198519: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n1\r\n```\r\n\r\nOutput with TF 1.9:\r\n```bash\r\n('Tensorflow version is ', '1.9.0')\r\n1.9.0\r\n2018-10-02 13:04:50.161943: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n1\r\n```\r\n\r\nOutput with TF 1.10:\r\n```bash\r\n('Tensorflow version is ', '1.10.0')\r\n1.10.0\r\n2018-10-02 12:24:26.858385: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n1\r\n```\r\n\r\nOutput with TF 1.11:\r\n```bash\r\n('Tensorflow version is ', '1.11.0')\r\n1.11.0\r\n2018-10-02 12:27:45.802153: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n1\r\n```\r\n\r\nI am recompiling the OP with different TF versions, before running the python script. Looks like the bug was introduced in TF 1.8.", "Will try your example to confirm but the best guess so far is that your kernel is run as part of constant folding which does use 1 thread. Adding input should help to see the 10 inter threads.", "Indeed, adding an input to the custom OP works for all tf versions until 1.9. Unfortunately, not for 1.10.\r\nHere is the modified C++ code\r\n```cpp\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/common_shape_fns.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"NumIntraOpThreads\")\r\n.Input(\"placeholder: float32\")\r\n.Output(\"num_intra_op_threads: int32\")\r\n.SetShapeFn(tensorflow::shape_inference::ScalarShape)\r\n.Doc(R\"doc(\r\nA tensorflow OP that returns the number of threads in the intra_op_parallelism pool\r\n)doc\");\r\n\r\nclass NumIntraOpThreads : public OpKernel {\r\n public:\r\n  explicit NumIntraOpThreads(OpKernelConstruction* context)\r\n      : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    int num_intra_op_threads = context->device()->tensorflow_cpu_worker_threads()->num_threads;\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}), &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n    output_flat(0) = num_intra_op_threads;\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"NumIntraOpThreads\").Device(DEVICE_CPU), NumIntraOpThreads);\r\n```\r\n\r\nThe OP compilation code remains the same.\r\nHere is the updated python code:\r\n```python\r\nimport tensorflow as tf\r\nprint('Tensorflow version is ', tf.__version__)\r\nn_module = tf.load_op_library('./num_intra_op_threads.so')\r\nprint(tf.__version__)\r\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)):\r\n  print(n_module.num_intra_op_threads(tf.constant(2.0)).eval())\r\n```\r\nOutput with TF 1.9:\r\n```bash\r\n('Tensorflow version is ', '1.9.0')\r\n1.9.0\r\n2018-10-02 15:10:56.503960: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n10\r\n```\r\n\r\nOutput with TF 1.10:\r\n```bash\r\n('Tensorflow version is ', '1.10.0')\r\n1.10.0\r\n2018-10-02 15:11:47.141032: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n1\r\n```", "@azaks2 Checking in to see whether you got a chance to run my code and confirm? Happy to provide more information if needed, thanks!", "Bump, wondering whether you got a chance to look into this.", "If you run under gdb you can see the call stack is from tensorflow::GraphOptimizer::Optimize to perform constant folding. To try for yourself:\r\ngdb python\r\nbr NumIntraOpThreads::Compute\r\nrun code.py\r\nwhere\r\n\r\nSo things work as expected. If you really want to see 10 and avoid constant folding try:\r\n                                                                                                                                                          \r\nimport tensorflow as tf\r\nprint('Tensorflow version is ', tf.__version__)\r\nn_module = tf.load_op_library('./num_intra_op_threads.so')\r\nprint(tf.__version__)\r\nx = tf.placeholder(tf.float32, name=\"x\")\r\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)):\r\n  print(n_module.num_intra_op_threads(x).eval(feed_dict={x:2.0}))\r\n\r\n\r\n"]}, {"number": 22545, "title": "Confusion matrix and printing CNN kernel with TF", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hello. Please reopen if this is a real issue.", "Hi !!\n\nI do not how to reopen the issue !!! and I need really help ...\n\nLe jeu. 27 sept. 2018 \u00e0 00:47, Amit Patankar <notifications@github.com> a\n\u00e9crit :\n\n> Closed #22545 <https://github.com/tensorflow/tensorflow/issues/22545>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22545#event-1869764003>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlYs1TSUwfPc0SgW5rNlhKo7LA22O9EBks5ufBH8gaJpZM4W7ZKh>\n> .\n>\n", "@Issam-Jmili I have reopened the issue. What do you need help with?", "Hi thanks soooo muuuch\nwhat I need is just to to visualize the confusion matrix and 5 kernels of\nthe first convolutional layer.\n\nI am following the tutorial provided on the website of tensorflow to\nretrain flower_photos by transfer knowledge using the Inception V3 here the\ncode in the retrain.py and it works.\nSo what I have to change or add in this file to visualize the confusion\nmatrix and 5 kernels of the first convolutional layer.\n\nLike I have mentioned I am new in this field and it is my first with python\n... and it is emergency and thanks again\n\n\nLe jeu. 27 sept. 2018 \u00e0 18:25, Amit Patankar <notifications@github.com> a\n\u00e9crit :\n\n> @Issam-Jmili <https://github.com/Issam-Jmili> I have reopened the issue.\n> What do you need help with?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22545#issuecomment-425172950>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlYs1cl6Qrln4CjWX-Xd98OoiS5dfjQaks5ufQn8gaJpZM4W7ZKh>\n> .\n>\n", "Hi @Issam-Jmili unfortunately please keep GitHub for the following as per our policy:\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n\r\nThis question is better suited for Stack Overflow.\r\n\r\nThat being said I'll provide some advice in passing. [Here](https://www.tensorflow.org/api_docs/python/tf/confusion_matrix) is some information on producing a confusion matrix, and as for seeing the 5 kernels of the first layer are you using Keras with Eager?", "about the code ot tf.confusion.matrix can you tell me where should I put in\nthe main code of retrain.py please\nand about the kernels I am just using tensorflow as virtualenv and what I\nkonw is keras uses tensorflow as backend to be to be specific I do not use\nKeras with Eager ...\n\nthank you by the way for you help it is so kind\n\nLe jeu. 27 sept. 2018 \u00e0 23:40, Amit Patankar <notifications@github.com> a\n\u00e9crit :\n\n> Hi @Issam-Jmili <https://github.com/Issam-Jmili> unfortunately please\n> keep GitHub for the following as per our policy:\n>\n>    1. It must be a bug, a feature request, or a significant problem with\n>    documentation (for small docs fixes please send a PR instead).\n>\n> This question is better suited for Stack Overflow.\n>\n> That being said I'll provide some advice in passing. Here\n> <https://www.tensorflow.org/api_docs/python/tf/confusion_matrix> is some\n> information on producing a confusion matrix, and as for seeing the 5\n> kernels of the first layer are you using Keras with Eager?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22545#issuecomment-425263960>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlYs1VTPGEXN7PyuvQUP19LaFkxHPmu6ks5ufVP2gaJpZM4W7ZKh>\n> .\n>\n", "Can you link me your tutorial I'm curious as to what you are using. Once again this would be much better if you posted it on Stack Overflow.", "I did post my question on Stack Overflow and no help :\\\n\nthis is the tutorial that I am following from the website of tensorflow but\nthey didn't mention how to show the confusion matrix wich it is my project\nyou see ...\n\nhttps://www.tensorflow.org/hub/tutorials/image_retraining\n\n\n\n\nLe ven. 28 sept. 2018 \u00e0 00:11, Amit Patankar <notifications@github.com> a\n\u00e9crit :\n\n> Can you link me your tutorial I'm curious as to what you are using. Once\n> again this would be much better if you posted it on Stack Overflow.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22545#issuecomment-425269566>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlYs1a6WNP7sR9f6mY3LpGx4KRZLCmHoks5ufVs0gaJpZM4W7ZKh>\n> .\n>\n", "@Issam-Jmili You can retrieve the predictions from output and plot confusion matrix using a third party tool such as sklearn, seaborn or matlab. Kernels are specified by you, so any of those tools would make visualization work. Hope this helps.", "Closing this issue, feel free to reopen if any additional questions."]}, {"number": 22544, "title": "Artifact links for ppc64le GPU builds.", "body": "whl files for tensorflow_gpu..._ppc6le are now hosted on the OSU\r\nJenkins build server. Nightly builds and Stable Release builds\r\nare provided. I didn't include the version number so we won't need\r\nto update the readme file for every new release", "comments": ["Links can be tested by looking at the README.md file here: https://github.com/wdirons/tensorflow/blob/update_readme_pp64le_gpu_artifacts/README.md", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @gunan Both Nightly build and release build artifacts have been generated. Could you merge?", "Thank you @gunan @case540, we appreciate all the reviews you have been doing for the power specific changes."]}, {"number": 22543, "title": "sparse_tensor_to_dense() does not seem to support back propagation", "body": "**Have I written custom code: Yes\r\n**OS Platform and Distribution: Windows 10 (primary), Ubunto 16.04.3 LTS (secondary)\r\n**TensorFlow installed from (source or binary): Binary\r\n**TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n**Python version: Python 3.6.6 :: Anaconda, Inc.\r\nCUDA/cuDNN version: N/A (not relevant)\r\nGPU model and memory: N/A (not relevant)\r\n\r\n### Describe the problem\r\nThe function sparse_tensor_to_dense() does not seem to support backpropagation. I've written a test script below, which yielded \r\n\r\n> Fetch argument None has invalid type <class 'NoneType'>\r\n\r\nIf you change the line\r\n`grad_A00 = tf.gradients(A_dense_transformed[0,0], par)`\r\nto the next (currently commented-out)\r\n`grad_A00 = tf.gradients(A_dense_manual[0,0], par)`\r\nthe script can be executed.\r\n\r\nIf the sparse tensor formulation is necessary, the workaround is to use the commented lines \r\n```\r\nidentity = tf.Variable(eye(2), dtype = float32)\r\nA_dense_transformed = A = tf.sparse_tensor_dense_matmul(tf.sparse_transpose(A_sparse), identity)\r\n```\r\nto replace original `A_dense_transformed = A = tf.sparse_tensor_to_dense(A_sparse)`. \r\nSuch an exercise allows the program to execute the original `grad_A00 = tf.gradients(A_dense_transformed[0,0], par)`.\r\n\r\n### Source code / logs\r\n```\r\n\r\nimport tensorflow as tf\r\nfrom numpy import *\r\n# showing backprop fails if using sparse tensor\r\n\r\ntf.reset_default_graph()\r\n\r\npar = tf.Variable(1.0, dtype = float32)\r\nA_sparse = tf.SparseTensor(indices=[[0,0],], values=tf.stack([par,]), dense_shape = (2,1))\r\n\r\nA_dense_manual = tf.stack([[par],[0]])\r\nA_dense_transformed = A = tf.sparse_tensor_to_dense(A_sparse)\r\n#identity = tf.Variable(eye(2), dtype = float32)\r\n#A_dense_transformed = A = tf.sparse_tensor_dense_matmul(tf.sparse_transpose(A_sparse), identity)\r\n\r\ngrad_A00 = tf.gradients(A_dense_transformed[0,0], par)\r\n#grad_A00 = tf.gradients(A_dense_manual[0,0], par)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    \r\n    init.run()\r\n    vA_manual, vA_transformed, vGrad = sess.run([A_dense_manual, A_dense_transformed, grad_A00])    \r\n    \r\nprint(vA_manual)\r\nprint(vA_transformed)\r\nprint(vGrad)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce\nMobile device", "> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n> Bazel version\r\n> Exact command to reproduce\r\n> Mobile device\r\n\r\nThank you for your inquiry.\r\n* I installed from binary, and not from source, so Bazel version does not matter. \r\n* The exact command to reproduce is just by simply running the attached source script (simple and short), `python script.py`\r\n* The mobile device doesn't apply to my case: I did not test on mobile devices. \r\n", "@dblueeye You are correct that sparse tensor works for back propagation when you use `tf.Variable`. This is not a bug, but we will add the improved functionality in the future.", "Wait... what do you mean, @wt-huang? Are you claiming sparse_tensor_to_dense() works for back propagation, and you did not reproduce my error message?", "@dblueeye Yes, I reproduced your error message. As of now sparse_tensor_to_dense() doesn't work for back propagation without tweaks, the function itself works in general. We will make this more user-friendly.", "@wt-huang Thanks for the clarification!"]}, {"number": 22542, "title": "Misleading full description on Tensorflow's Docker Hub public repository", "body": "The instructions on https://hub.docker.com/r/tensorflow/tensorflow/ still refer to the old `nvidia-docker`, when new images support nvidia-docker2, which works a bit differently.. The full description from Docker Hub doesn't seem to be under version control, so I can't submit a diff, but the following changes are needed, under the \"Start GPU (CUDA) container\" section:\r\n\r\n* In \"Install nvidia-docker and run\", change \"nvidia-docker\" for \"nvidia-docker2\" instead. (The link stays the same).\r\n* Instead of  \"nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu\", now the command is `docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu`.\r\n\r\nThis was the best place I could find where to submit this bug report. If there's a better place, please let me know. Thanks!", "comments": ["Thanks for pointing this out @chrish42 I'll take a look.", "@tfboyd if you can sign off on this I can update dockerhub.", "@av8ramit \r\n\r\nI am am pretty sure I am nvidia-docker2, which I use daily, and it still has the alias so you do not have to do `--runtime=nvidia` that is only needed if you want to trigger is without the alias. But their docuemtnation says to use what @chrish42 states.   \r\n\r\nChange the install to indicate nvidia-docker2 and go ahead and change the command to `docker run --runtime=nvidia` since that is how nvidia [documents](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)) it.  Both may work but no reason to deviate from their instructions.", "Done. https://hub.docker.com/r/tensorflow/tensorflow/"]}, {"number": 22540, "title": "conflicting shape of tflite conv2d weight tensor shape vs conv2d documentation", "body": " https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\r\nshows that conv2d expects kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\r\nThe mobilenet_v1_1.0_224/mobilenet_v1_1.0_224.tflite from the following link contains shape for the first conv2d of [32,3,3,3]\r\nwhile the mobilenet_v1_1.0_224/mobilenet_v1_1.0_223_frozen.pb has conv2d tensor shape of [3,3,3,32]\r\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nSo, since these are conflicting shapes, both going into the first conv2d, then one of them does not match the documentation kernel tensor shape.  It appears to me the .pb file matches the documentation, since HWCN is 3,3,3,32 for the first conv2d\r\n\r\nIs the .tflite format going to persist the weight tensors in a different shape than in the .pb?   If so, I don't see that documented.\r\n\r\nYou can see this by downloading the netron viewer from \r\nhttps://github.com/lutzroeder/netron\r\nand clicking on the weight tab in the first conv2d, and look at the shape in the weights node properties.\r\nI'm using tensorflow 1.10 on ubuntu 18.04, and using netron viewer 2.1.8  \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I think you are right that the order of conv2d weights is OHWI, I didn't see any related doc, but it's all in source code, see e.g., [1](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/model.h#L162-L172), [2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/import_tensorflow.cc#L550-L557) and [3](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/conv.cc#L252-L258)", "The compatibility list  comment  is a bit cryptic, but perhaps it is acknowledging that the conv2d filter /weight order is incompatible.   So, is there some plan to resolve the incompatibilities between tflite and tf operations (incuding these persisted filter/weight shape differences)?    \r\n\r\nhttps://www.tensorflow.org/lite/tf_ops_compatibility\r\n\"The following TensorFlow operations are usually mapped to their TensorFlow Lite counterparts:\"\r\n\"tf.nn.conv2d - as long as the filter is constant\"", "Thanks for the links.   \"TensorFlow vs Our\" differences, with \"Our\" meaning tflite, could be important documentation for anyone trying to use tflite.   I guess we can close this, since the differences are obviously known.\r\n  kOHWI,     // Our standard for conv weights\r\n  kHWIO,     // TensorFlow conv weights\r\n  k1HWO,     // Our standard for DepthwiseConv weights\r\n  kHWIM,     // TensorFlow DepthwiseConv weights"]}, {"number": 22539, "title": "R0.7", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 22538, "title": "tfe.Saver only stores last 5 checkpoints", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: CUDA 9.0\r\n- **GPU model and memory**: 940 MX\r\n- **Exact command to reproduce**:\r\n```\r\nimport numpy as np\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\n\r\n '''The following should be irrelevant, but nevertheless'''\r\nclass CNNClassifier(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(CNNClassifier, self).__init__()\r\n        xav_init = tf.contrib.layers.xavier_initializer\r\n\r\n        self.layer_1 = tf.layers.Conv2D(\r\n            filters=32,\r\n            kernel_size=[3, 3],\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            kernel_initializer=xav_init())\r\n        self.layer_2 = tf.layers.Conv2D(\r\n            filters=64,\r\n            kernel_size=[3, 3],\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            kernel_initializer=xav_init())\r\n        self.layer_3 = tf.layers.Conv2D(\r\n            filters=128,\r\n            kernel_size=[3, 3],\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            kernel_initializer=xav_init())\r\n        self.layer_4 = tf.layers.Conv2D(\r\n            filters=256,\r\n            kernel_size=[3, 3],\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            kernel_initializer=xav_init())\r\n        self.layer_5 = tf.layers.Conv2D(\r\n            filters=512,\r\n            kernel_size=[3, 3],\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            kernel_initializer=xav_init())\r\n        self.dense = tf.layers.Dense(units=10)\r\n\r\n    def call(self, inputs):\r\n        inputs = tf.convert_to_tensor(inputs)\r\n        x = tf.layers.max_pooling2d(\r\n            self.layer_1(inputs), pool_size=[2, 2], strides=[2, 2])\r\n        x = tf.layers.max_pooling2d(\r\n            self.layer_2(x), pool_size=[2, 2], strides=[2, 2])\r\n        x = tf.layers.max_pooling2d(\r\n            self.layer_3(x), pool_size=[2, 2], strides=[2, 2])\r\n        x = tf.layers.max_pooling2d(\r\n            self.layer_4(x), pool_size=[2, 2], strides=[2, 2])\r\n        x = tf.layers.max_pooling2d(\r\n            self.layer_5(x), pool_size=[2, 2], strides=[2, 2])\r\n        x = tf.layers.flatten(x)\r\n        x = self.dense(x)\r\n        return x\r\n\r\nif __name__ == \"__main__\":\r\n\tos.mkdir(\"saved_models\")\r\n    tfe.enable_eager_execution()\r\n\r\n    c = CNNClassifier()\r\n    c(np.random.randn(1, 32, 32, 3))\r\n\r\n    saver = tfe.Saver(c.variables)\r\n\r\n    for i in range(100):\r\n        print(i)\r\n        saver.save(f\"saved_models/step_{i}.ckpt\")\r\n\r\n```\r\n\r\n### Problem Description\r\nI am using eager execution and the `tfe.Saver` class to save my model's weights. However, if I am saving many checkpoints, only last five checkpoints are saved. This is very similar to issues with `tf.train.Saver` where the `max_to_keep` argument defaults to `5`. Since `tfe.Saver` doesn't have a corresponding argument, it seems to me that it is impossible to store more than the 5 most recent checkpoints.\r\n\r\nI am happy to submit PRs if there's any guidance on how to fix this issue.\r\n\r\n\r\n", "comments": ["Quick check of the codebase reveals that [`tfe.Saver`](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/eager/python/saver.py#L116) uses [`tf.train.saver.Saver`](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/training/saver.py#L1072) internally. A \"simple fix\" would be to bubble up the agrument of the the constructor of the latter to the former, thereby allowing the users to modify these args themselves instead of just using the defaults", "I'd recommend using `tf.train.Checkpoint` instead of `tfe.Saver`: https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint\r\n\r\nThat won't delete anything by default (in more recent versions; initially it saved 5). For controlling the number of saved checkpoints, `tf.contrib.checkpoint.CheckpointManager` provides options and has better semantics around restoring lists of existing checkpoints than `Saver`: https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/contrib/checkpoint/CheckpointManager\r\n\r\nDoes that work for you?", "Thanks for the suggestion.\r\n\r\nI think it should work, though I would much prefer if `Checkpoint.restore` didn't do its own numbering and allowed the user to customize it (via a `global_step` arg) :)\r\n\r\nNevertheless, it seems good enough for me.\r\nQuick question, do we still need to do a dummy pass to initialize the variables berfore restoring the saved weights?\r\n\r\n\r\nThanks again, upto the devs to decide whether to close this issue because the bug still remains in `tfe.Saver` ", "Custom numbering will work with `CheckpointManager.save(checkpoint_number=...)`, and you can set `max_to_keep=None` to keep everything if you don't want it cleaning up checkpoints.\r\n\r\nThere's also [`tf.train.Checkpoint.write`](https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/train/Checkpoint#write), which `CheckpointManager` calls, if you want to do string formatting/checkpoint management yourself.\r\n\r\nWill close since `tfe.Saver` is going away (deprecating it is on my list).", "Oh, and for initialization, there's `tf.train.Checkpoint.restore(...).initialize_or_restore()` for initializing variables which won't get restored while graph building. When executing eagerly, variables are initialized eagerly, so it's optional there."]}, {"number": 22537, "title": "122", "body": "111", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 22536, "title": "wrong version of protobuf for tensorflow1.10", "body": "I'm trying to build the C++ shared library with the following steps. [ubuntu18.04]\r\n1, git clone -b r1.10 https://github.com/tensorflow/tensorflow.git\r\n2, run /tensorflow/contrib/makefile/build_all_linux.sh to get third-party dependencies\r\n3, ./configure and babzel build //tensorflow:libtensorflow_cc.so\r\n4, Get all the needed header files to INCLUDE and libraries to LIB\r\n\r\nWhen testing the example \"label_image\", I got an error \"missing header google/protobuf/inlined_string_fieled.h\". \r\n\r\nThen I checked downloaded version of protobuf, it was version 3.5.0.\r\nAs I checked somewhere I didn't remember, tensorflow r1.10 requires protobuf version over 3.6.0. Then I replaced the /google directory with version 3.6.0. It works quite well.\r\n\r\nCould you please fix it to get version 3.6.0 of protobuf installed rather than verson 3.5.0 by running build_linux_all.sh?\r\n\r\nThank you in advance!!", "comments": ["@angersson any idea if we have fixed this in 1.11?", "No, this persists in r1.11 as of a few days ago", "Nagging Assignee @angersson: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I don't think TF team officially supports the Makefile any more, so I'm marking this as \"contributions welcome\".", "I am working on this. Will create a pull request soon", "@sahilbadyal  Thank you so much!\r\nAnd thank you all.", "@angersson  Currently, the script downloads Protobuf from this URL. https://mirror.bazel.build/github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz\r\nThe solution would be  to update the link to a new/updated tar file location https://mirror.bazel.build/github.com/google/protobuf/archive/v3.6.0.tar.gz", "Please review the pull request. I have tested it on my Fedora 27 machine.", "@Fantelia  @angersson  This has been fixed and merged in the master branch.", "Great! I'll close the issue.", "Thanks a lot for your efforts!\n\nOn Fri, 16 Nov 2018 at 20:27, Austin Anderson <notifications@github.com>\nwrote:\n\n> Closed #22536 <https://github.com/tensorflow/tensorflow/issues/22536>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22536#event-1971924841>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AmxmMWlD3NBJ3cwYxXUzHSODJnuQuHY-ks5uvwOggaJpZM4W6rE7>\n> .\n>\n"]}, {"number": 22535, "title": "After post-training quantization -- Internal error: Cannot allocate memory for the interpreter", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: tf-nightly-1.12.0.dev20180926\r\n- **TensorFlow version (use command below)**: tf-nightly-1.12.0.dev20180926\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: no\r\n- **GCC/Compiler version (if compiling from source)**: no\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**: see follows\r\n\r\n### Describe the problem\r\nWhen I tried to use the quantized model for the example [TensorFlow for Poets 2: TFLite Android](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#0), I encountered the following problem.\r\nbeginning of crash\r\n09-26 13:08:10.249 31824-31824/? E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: android.example.com.tflitecamerademo, PID: 31824\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: **java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare.**\r\n    \r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2665)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6119)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)\r\n     Caused by: java.lang.NullPointerException: Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:75)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:97)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:299)\r\n        at android.app.Fragment.performActivityCreated(Fragment.java:2362)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1014)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1171)\r\n        at android.app.BackStackRecord.run(BackStackRecord.java:816)\r\n        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:1578)\r\n        at android.app.FragmentController.execPendingActions(FragmentController.java:371)\r\n        at android.app.Activity.performStart(Activity.java:6695)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2628)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726)\u00a0\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n        at android.os.Looper.loop(Looper.java:154)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6119)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:886)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:776)\r\n\r\nThe cmds were as follows:\r\n```\r\nimport tensorflow as tf\r\ngraph_def_file='retrained_graph.pb'\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"final_result\"]\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\nconverter.post_training_quantize = True\r\ntflite_quantized_model = converter.convert()\r\nopen(\"quantized_graph.tflite\", \"wb\").write(tflite_quantized_model)\r\n```\r\nBut if post-training quantization is not enabled, the app works.\r\n\r\nThe quantized model could work in python env (though the result is not accurate).\r\n```\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=\"quantized_graph.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nfrom label_image import read_tensor_from_image_file\r\ninput_data = read_tensor_from_image_file('1.jpg',input_height=224,input_width=224, input_mean=128, input_std=128)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\n### Source code / logs\r\nModels are attached.\r\n[tf_files.zip](https://github.com/tensorflow/tensorflow/files/2419845/tf_files.zip)\r\n\r\n", "comments": ["After the post-training quantization, the type of input tensors are changed from floating point into quantized uint8. You need to change your code accordingly.", "> After the post-training quantization, the type of input tensors are changed from floating point into quantized uint8. You need to change your code accordingly.\r\n\r\nBut in the python env, I found the dtype of input was still float\r\n```\r\n>>>input_details\r\n[{'name': 'input', 'index': 89, 'shape': array([  1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n>>> output_details\r\n[{'name': 'final_result', 'index': 85, 'shape': array([1, 5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n```\r\n\r\nWhether additional setting params are required for the post-training quantization? Could you give any detailed tutorials?", "@minizon sorry about confusion. What I meant to say is that for post-training quantization, the type of input tensors remains floating point. For real quantized model, the type of input tensors is expected to be quantized uint8 (type 3). To put it simply, a post-training model should be treated as a floating point model rather than a quantized one. That's why it doesn't work in the demo app (which, if not modified, expects a quantized one, see tflitecamera source [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java#L334)\r\n", "@freedomtan What I did before is that I just simply replaced the retrained_graph.lite by the quantized_graph.lite in the assets folder ([here](https://github.com/googlecodelabs/tensorflow-for-poets-2/tree/master/android/tflite/app/src/main/assets)), the classifier for the tf interpreter was not changed ([here](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/android/tflite/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L97)). I think this demo is still for the floating point model, but the error ( **Internal error: Cannot allocate memory for the interpreter: tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)Node 4 failed to prepare**) did happen. Or do you mean only after post-training quantization, the model is still not proper for android deployment?\r\n\r\nAny way, I couldn't find clear guides for how to change the floating point model into a real quantized one. The official model zoo provides some pre-converted models ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-quantized-models)) with no details for the conversion process.", "@minizon I finally figured out what your environment/setting is. Put it simply, **the TF Lite runtime used by the TensorFlow for Poets 2: TFLite Android example at this time doesn't support post-training quantization**. Quick solution, build your own tensorflow-lite aar with more recent source code.\r\n\r\nMore details:\r\n1. yes, the app you used uses floating-point model, not quantized uint8. I thought you were trying the tflitecamera demo in the TF source code rather than the one in the TensorFlow for Poets 2.\r\n2. post-training quantization is to quantize weights of convolutions, that is, it's weights only quantization. Weights are quantized from float32 to uint8. But computation is still in floating point, that is, dequantization should be done before computing. Code related to this dequantization was added relative late. So it's not in AAR you used.\r\n3. for full quantization / quantization-aware training see [this doc]( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md)\r\n4. Since TensorFlow 1.11.0 was released. Maybe there will be updated AAR soon. Maybe @wt-huang can help?\r\n", "@freedomtan Thank you very much!", "@minizon  Hello, I met the same problem with you. After reading your communication contents with  listed above, I saw a promising solution. But when I tried in practice, there still existed problems, which confused me long. Therefore, I wonder whether you could share your experience in fixing the problem?  And, how on earth to conduct the quantization using the command tool TOCO? Could you give me some detailed tips? Thank you very much! \r\n\r\nMy conversion is as follows,and the conversion process failed:\r\n\r\nIMAGE_SIZE=28\r\ntoco \\\r\n  --graph_def_file=./lenet5.pb \\\r\n  --output_file=./lenet5_qt_uint8.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},1 \\\r\n  --input_array=conv2d_1_input \\\r\n  --output_array=output_node0 \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --inference_input_type=QUANTIZED_UINT8 \\\r\n  --post_training_quantize\r\n\r\nAlso thanks to @freedomtan for your instructions! I would appreciate it if you gave me some detailed tips fixing this bug.", "@GQYYY I didn't use the CLI toco. Please follow [the python tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tutorials/post_training_quant.ipynb)  to make it. You can use tf.contrib.lite.TocoConverter.from_frozen_graph for the pb file. **Please make sure that you've installed a recent tf release (maybe tf-nightly) version**.  And be aware that after this kind of conversion, the model is still muck like a floating model, so maybe better set the inference_type and inference_input_type to float. What I understood is that once you had quantization-aware training, you could set them to uint8.", "@minizon Have you already solved your problem? If so, can you share it with us? Because I also encountered this error. I used tensorflow 11.0 in converting my pb model to tflite format. I can't run the post-quantized model in my phone. Thanks", "Same here. Could anyone show how to build tensorflow-lite AAR from source? Many thanks.", "Fixed! Use the tflite 0.0.0-nightly in android. ", "@brucechou1983 See this [reference](https://heartbeat.fritz.ai/compiling-a-tensorflow-lite-build-with-custom-operations-cf6330ee30e2)", "@minizon seems to be helpful! Thank you~", "@minizon @brucechou1983 did you guys find the solution for this issue? I have the same issue and I'm using the last version of nightly.", "@damhurmuller \r\n\r\nIf you use the toco command line tool for generating post-training quantization models, make sure the `--inference_type` and `--inference_input_type` are both *FLOAT* rather than *QUANTIZED_UINT8*. If you want a full quantized model, go for the quantization aware training flow.\r\n\r\nAlso you might need to build the TensorflowLite AAR from source because none of the online-available versions supported this feature when I tried them. I used this command:\r\n```\r\nbazel build --cxxopt='--std=c++11' -c opt        \\\r\n--fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   \\\r\n//tensorflow/lite/java:tensorflow-lite\r\n```\r\nMake sure to edit the `WORKSPACE` file so that bazel knows where the android sdk and ndk are.", "@brucechou1983\r\n\r\nCan you share your newly built Tensorflow Lite AAR file? Because for the mean time I cannot use my desktop    and I'm just using my mobile phone. Many thanks", "@ jazzystring1 using tf nightly didn't solve my issue. Also @brucechou1983 your advice seems helpful. I have a tflite quantized model right now, could you tell me the terminal command to covert that into a post-training quantization model? Thanks!", "@minizon Hello! I wonder whether your model  after the post-training quantization can have correct results. My post-training quantized model just can't. Could you plz share your experience?", "@chenlh14 After quantization the model's performance will degenerate. If you want to minimize the disparity, you should use quantization-aware training first. For other situations, you should check the whole conversion process.", "@minizon Thank you for your advice.\r\nCodes are simple, as follows.\r\n\r\n`import tensorflow as tf\r\ntf.enable_eager_execution()\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph('resnet_v1_frozen.pb', ['input'], ['output'])\r\ntflite_model = converter.convert()\r\nopen(\"converted_resnet.tflite\", \"wb\").write(tflite_model)\r\n\r\nconverter.post_training_quantize = True\r\ntflite_model = converter.convert()\r\nopen(\"quantized_resnet.tflite\", \"wb\").write(tflite_model)`\r\n\r\nMy model is based on ResNet V1, but without BN layers.\r\n.pb model can reach an accuracy of 88%, while .tflite nearly none.\r\nAm I wrong in the conversion process or has it something to do with my model?\r\nAny advice will be appreciated.", "Did you check the performance of the non-quatized tflite model?", "@minizon @brucechou1983 Don't know why I got wrong results before, but it works now. Maybe just my mistake. Thank you~", "i have the same error with a normal trained tensorflow model (so no quantization aware training) and porting it to tflite and running it on android works, but fails for the quantized version.\r\nI tried both:\r\n```\r\ntflite_convert \\\r\n--output_file=model_quantized_uint8.tflite \\\r\n--graph_def_file=model.pb \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_shapes=1,224,224,3 \\\r\n--input_arrays=inputs \\\r\n--output_arrays=outputs \\\r\n--default_ranges_min=0 \\\r\n--default_ranges_max=6 \\\r\n--mean_values=128 \\\r\n--std_dev_values=127\r\n```\r\nand\r\n```\r\ntflite_convert \\\r\n--output_file=model_quantized_float.tflite \\\r\n--graph_def_file=model.pb \\\r\n--inference_type=FLOAT \\\r\n--inference_input_type=FLOAT \\\r\n--input_shapes=1,224,224,3 \\\r\n--input_arrays=inputs \\\r\n--output_arrays=outputs \\\r\n--post_training_quantize\r\n```"]}, {"number": 22534, "title": "Performance optimization on SparseConcatOp.", "body": "Previous 'SparseConcatOp' uses a reorder-concat-reorder implementation. In the 'Reorder' function, a sort would be performed, which decides that the overall complexity would be O(NlogN). In large (maybe rare but occurred in our production env) cases, time consumption is significant.\r\n\r\nThis commit includes optimizations below:\r\n  - A quickpath for dim-0 concatenation. The reorder phase would not be done, which reaches O(N) complexity.\r\n  - Parallel implementation for concatenating and sorting phases, with a parallel quick sort implementation with Eigen threadpool.\r\n  - A naive dimension comparator used for sorting.\r\n\r\nHere is the performance comparison on my Mac Pro (i7-4770HQ @ 2.2GHz 4c8T, 16GB DDR3 1600) with the benchmarks in this commit:\r\nThe benchmark uses 3D string tensor, the numbers 'BM_SparseConcat_X_Y_Z_T' in the the benchmark indicate 'X: valid points of each sparse; Y: number of input tensors; Z: concat dimension; T: number of threads'.\r\nOld:\r\n![image](https://user-images.githubusercontent.com/10669111/46080373-5d715e00-c1cc-11e8-92ae-d79da8e21c95.png)\r\nNew:\r\n![image](https://user-images.githubusercontent.com/10669111/46080385-65310280-c1cc-11e8-8663-b61aa7298e68.png)\r\n\r\n\r\n\r\n", "comments": ["Any reviewer has time to take a look about this commit?  @caisq thx", "Since there's no response for this PR in last decades of days. I'll give some of comments and concern by myself here.\r\n\r\nFirst of all, this optimization for sparse concat might break the current use of util/sparse_tensor, because I don't think construct such an object from inputs and do re-order is valuable. Maybe we could discuss about whether is a better way to keep the uniform operation in util/sparse_tensor.\r\nSecondly, the fast-path of concatenation on dim0 should be performed. This definitely simplify the complexity of the operation for large scale sparse features.\r\nThirdly, this work used some additional memory for index sorting, and implemented a parallel quick-sort. We could also bring out some other idea here.\r\n\r\n", "@ebrevdo @gunan @josh11b Could you take a look or reassign?", "@shahzadlone , any further comments? Thanks.", "I'm so sorry for our delay. I will review this PR soon!", "Can one of the admins verify this patch?", "@penpornk gentle ping to review this PR , @lowintelligence can you please resolve conflicts.", "@lowintelligence Could you please address the reviewer comments and resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "Sorry for missing all the messages due to my work change.\r\n@penpornk Thanks for the review comments. This PR was hanging there for a long period thus I didn't check its status very often. I'll try to fix the codes and reopen another PR later."]}, {"number": 22533, "title": "Use scatter operations to speed up adam optimizer", "body": "Original implement uses the whole m and t tensor to do computation, which is not efficient for sparse tensors.\r\nMy job speed up by 40x (3500ms -> 90ms for a single step) with this patch", "comments": ["By using the new syntax suggested, the code becomes much simpler, no wrappers and resource_variable logic", "I fixed Ubuntu Santiy, and verified locally.\r\nFor the rest of failures, after checking logs one bye one, they are mostly environment/configuration problems that has nothing to do with this chang. PTAL\r\n   OSError: [Errno 2] No such file or directory: '/home/kbuilder/.pyenv/shims/python'\r\n   Could not find the 'go' tool in PATH or /usr/local/go\r\n   ERROR: offline_analyzer output didn't match expectation: sh: sysctl: command not found\r\n\r\n"]}, {"number": 22532, "title": "Tensorflow quantization on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1 CPU\r\n- **Python version**: 3.6\r\n\r\n### Describe the problem\r\nI've freezed my model and got .pb file. Then I've quantize my model using tocoConverter on Linux, as it's not supported on Windows. I've got quantized_model.tflite. I can load it and get predictions on Linux, but I have issues to make it on Windows, as my project requires. I've tried to load it using tf.contrib.lite.Interpreter using this code:\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\ninterpreter=tf.contrib.lite.Interpreter(model_path=\"quantized_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\n\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'],input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n*//ImportError: No module named 'tensorflow.contrib.lite.python.Interpreter*`\r\n\r\nBut it failed with \"No module named 'tensorflow.contrib.lite.python.interpreter\" error. I always get this errors on Windows, when trying to use something from tf.contrib.lite. Maybe there is a way to load this on Windows? Or can you advice alternative options to quantize a model on Windows?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@monInspiration This issue is similar to [this](https://github.com/tensorflow/tensorflow/issues/20832). Currently TOCO is not supported on Windows. "]}, {"number": 22530, "title": "Inconsistent behaviour of tf.range", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**: GTX Titan X + 2xGTX Titan Black\r\n\r\n\r\n### Describe the problem\r\nthe tensor produced by tf.range exceeds the limit with the configuration of parameters below.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n\tmax_value=379.0\r\n\tdelta=379.0/94.0\r\n\r\n\trange_tensor = tf.range(max_value,delta=delta,dtype=tf.float32)\r\n\r\n\tprint(sess.run(range_tensor))\r\n```\r\nResult:\r\n\r\n> [  0.          4.0319147   8.063829   12.095744   16.127659   20.159573\r\n  24.191486   28.2234     32.255314   36.287228   40.31914    44.351055\r\n  48.38297    52.414883   56.446796   60.47871    64.51063    68.54254\r\n  72.574455   76.60637    80.63828    84.6702     88.70211    92.734024\r\n  96.76594   100.79785   104.829765  108.86168   112.89359   116.92551\r\n 120.95742   124.989334  129.02126   133.05318   137.0851    141.11702\r\n 145.14894   149.18086   153.21278   157.2447    161.27663   165.30855\r\n 169.34047   173.37239   177.40431   181.43623   185.46815   189.50008\r\n 193.532     197.56392   201.59584   205.62776   209.65968   213.6916\r\n 217.72353   221.75545   225.78737   229.81929   233.85121   237.88313\r\n 241.91505   245.94698   249.9789    254.01082   258.04272   262.07465\r\n 266.10657   270.1385    274.1704    278.20233   282.23425   286.26617\r\n 290.2981    294.33002   298.36194   302.39386   306.42578   310.4577\r\n 314.48962   318.52155   322.55347   326.5854    330.6173    334.64923\r\n 338.68115   342.71307   346.745     350.77692   354.80884   358.84076\r\n 362.87268   366.9046    370.93652   374.96844   379.00037  ]\r\n\r\n379.00037 should not be there!\r\n\r\nEquivalent numpy code\r\n```\r\nimport numpy as np\r\nmax_value=379.0\r\ndelta=379.0/94.0\r\nprint(np.arange(0,max_value,delta))\r\n```\r\nResult:\r\n> [  0.           4.03191489   8.06382979  12.09574468  16.12765957\r\n  20.15957447  24.19148936  28.22340426  32.25531915  36.28723404\r\n  40.31914894  44.35106383  48.38297872  52.41489362  56.44680851\r\n  60.4787234   64.5106383   68.54255319  72.57446809  76.60638298\r\n  80.63829787  84.67021277  88.70212766  92.73404255  96.76595745\r\n 100.79787234 104.82978723 108.86170213 112.89361702 116.92553191\r\n 120.95744681 124.9893617  129.0212766  133.05319149 137.08510638\r\n 141.11702128 145.14893617 149.18085106 153.21276596 157.24468085\r\n 161.27659574 165.30851064 169.34042553 173.37234043 177.40425532\r\n 181.43617021 185.46808511 189.5        193.53191489 197.56382979\r\n 201.59574468 205.62765957 209.65957447 213.69148936 217.72340426\r\n 221.75531915 225.78723404 229.81914894 233.85106383 237.88297872\r\n 241.91489362 245.94680851 249.9787234  254.0106383  258.04255319\r\n 262.07446809 266.10638298 270.13829787 274.17021277 278.20212766\r\n 282.23404255 286.26595745 290.29787234 294.32978723 298.36170213\r\n 302.39361702 306.42553191 310.45744681 314.4893617  318.5212766\r\n 322.55319149 326.58510638 330.61702128 334.64893617 338.68085106\r\n 342.71276596 346.74468085 350.77659574 354.80851064 358.84042553\r\n 362.87234043 366.90425532 370.93617021 374.96808511]\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "@AlessioTonioni That's a good catch !  I tried with couple of examples with different max_value and delta and tf.range works fine in all those scenarios. But we will definitely consider your finding and feel free to share other scenarios(if any). ", "@harshini-gadige I have only found this configuration thus far, but I think it might be something related to numerical precision during the numerical computation.\r\nWith the same max_value and delta as in the initial post \r\n\r\n`range_tensor = tf.range(max_value-1E-4,delta=delta,dtype=tf.float32)`\r\n\r\nWorks as expected, i.e. the last number in the range is 374.96808511.\r\n\r\n`range_tensor = tf.range(max_value-1E-5,delta=delta,dtype=tf.float32)`\r\n\r\nShows the bug, i.e. the last number in the range is 379.00037.", "@rmlarsen Is this happening due to numerical computation or this could be a bug ?", "I guess that `max_value = 379` is converted to float32, which might lose precision.", "> I guess that `max_value = 379` is converted to float32, which might lose precision.\r\n\r\nI was thinking something similar at the beginning, but I tried and reported the result with `max_value=379.0` that is already a python float, the only conversion going on should be from python float to tf.float32.", "Hi, I have a similar problem with the precision in tensorflow-gpu version 2.0.0\r\n\r\nthis is the version.\r\n\r\n    >>># note: this is tensorflow-gpu I installed\r\n    >>>tf.__version__\r\n    '2.0.0'\r\n\r\nthis is to reproduce the bug.\r\n\r\n    >>>tf.range(0,5,0.1)\r\n    <tf.Tensor: id=11, shape=(50,), dtype=float32, numpy=\r\n    array([0.        , 0.1       , 0.2       , 0.3       , 0.4       ,\r\n              0.5       , 0.6       , 0.70000005, 0.8000001 , 0.9000001 ,\r\n              1.0000001 , 1.1000001 , 1.2000002 , 1.3000002 , 1.4000002 ,\r\n              1.5000002 , 1.6000003 , 1.7000003 , 1.8000003 , 1.9000003 ,\r\n              2.0000002 , 2.1000001 , 2.2       , 2.3       , 2.3999999 ,\r\n              2.4999998 , 2.5999997 , 2.6999996 , 2.7999995 , 2.8999994 ,\r\n              2.9999993 , 3.0999992 , 3.199999  , 3.299999  , 3.399999  ,\r\n              3.4999988 , 3.5999987 , 3.6999986 , 3.7999985 , 3.8999984 ,\r\n              3.9999983 , 4.0999985 , 4.1999984 , 4.2999983 , 4.399998  ,\r\n              4.499998  , 4.599998  , 4.699998  , 4.799998  , 4.8999977 ],\r\n            dtype=float32)>", "@AlessioTonioni It looks like you are using an older Version of Tensorflow (1.x) which is out of support window. Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.6.0) and let us know if the issue still persists? Please refer to the [link](https://www.tensorflow.org/api_docs/python/tf/range) and let us know if it helps ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22530\">No</a>\n"]}, {"number": 22529, "title": "Feature request: Make contrib.receptive_field extensible", "body": "### System information\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n\r\n### Describe the problem\r\n\r\nFor the problem I am working on, I need to compute a cross-correlation where the filters are a function of some inputs, i.e. `c_i = conv2d(f(a_i), g(b_i))` for the `i`-th example `(a_i, b_i)` in the batch. After doing this, the receptive field of `c` with respect to `a` is still well-defined.\r\n\r\nI can think of several ways to implement this, but none of them are compatible with `receptive_field`!\r\n\r\nFor example, I usually use `reshape`-`depthwise_conv2d`-`reshape`-`sum` to transform a batch of `B` convolutions with `C` channels into a batch of `1` depthwise-convolution with `B*C` channels, then sum over the `C` channels at the end. Another way to do it would be using `tf.map_fn`. However, neither of these will work with `compute_receptive_field_from_graph_def`.\r\n\r\nI understand that it would be very difficult to support edge cases like this. However, it breaks the workflow a bit to keep track of every place where the graph is incompatible with `receptive_field` and perform the composition manually here.\r\n\r\nPerhaps it would be possible to add a manual override parameter to `compute_receptive_field_from_graph_def` that allows a user to manually specify the receptive field from one node to another, instead of using the graph?\r\n\r\nAnother way could be to support custom layers, either by checking if the custom layer has a receptive field method, or allowing users to register custom layers with the `receptive_field` module. However, this would require me to write my own layer for the \"dynamic\" convolution described above, which is not currently necessary.\r\n\r\nI'm open to suggestions!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a", "@jvlmdr Thanks for sharing the use case. Customizing your own layer would be a quicker route but adding knobs to `compute_receptive_field_from_graph_def` is a viable approach especially when your use case may change frequently in the future. You can modify TensorFlow graph in place using [Graph Editor via tf.contrib.graph_editor](https://www.tensorflow.org/api_guides/python/contrib.graph_editor)", "Glad to help :)"]}, {"number": 22528, "title": "TfLite Quantization Failure: Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data,", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0-rc1\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\nI am trying to save Quantized version of graph in tflite format but getting the following error\r\n\r\nF tensorflow/contrib/lite/toco/tooling_util.cc:1633] Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n\r\nI am not able to understand why isn't Min/Max information stored for dense_prev operator if for dense_typed it's not giving any error.  \r\n\r\nThe code to reproduce the problem  \r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.lite.toco import types_pb2 as _types_pb2\r\nQUANTIZED_UINT8 = _types_pb2.QUANTIZED_UINT8\r\nimport numpy as np\r\n\r\ndef train_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\r\n    g = tf.Graph()\r\n    with tf.Session(graph=g) as sess:\r\n\r\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\r\n        label = tf.placeholder(tf.float32, shape=(vocab_output_size), name='labels')\r\n\r\n        data_x_typed = data_x[:vocab_typed_size]\r\n        data_x_prev = data_x[vocab_typed_size:]\r\n        \r\n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\r\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\r\n\r\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\r\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\r\n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\r\n\r\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\r\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\r\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\r\n\r\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \r\n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\r\n\r\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\r\n\r\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\r\n\r\n        # Loss function and optimizer\r\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_message, labels=label), name=\"loss\")\r\n\r\n        # Call the training rewrite which rewrites the graph in-place with\r\n        # FakeQuantization nodes and folds batchnorm for training. It is\r\n        # often needed to fine tune a floating point model for quantization\r\n        # with this training tool. When training from scratch, quant_delay\r\n        # can be used to activate quantization after training to converge\r\n        # with the float graph, effectively fine-tuning the model.\r\n\r\n        tf.contrib.quantize.create_training_graph(quant_delay=0)\r\n\r\n        optimizer = tf.train.AdamOptimizer().minimize(loss)\r\n\r\n        sess.run(tf.global_variables_initializer())\r\n        saver = tf.train.Saver()\r\n        print \"Dumping model\"\r\n        saver.save(sess, './test/oink.ckpt')\r\n\r\n        writer = tf.summary.FileWriter(\"./test/\", sess.graph)\r\n        writer.close()\r\n        \r\n\r\ndef eval_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\r\n    g = tf.Graph()\r\n    with tf.Session(graph=g) as sess:\r\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\r\n        data_x_typed = data_x[:vocab_typed_size]\r\n        data_x_prev = data_x[vocab_typed_size:]\r\n\r\n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\r\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\r\n\r\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\r\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\r\n        \r\n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\r\n\r\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\r\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\r\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\r\n\r\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \r\n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\r\n\r\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\r\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\r\n\r\n        tf.contrib.quantize.create_eval_graph()\r\n\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, \"./test/oink.ckpt\")\r\n\r\n        converter = tf.contrib.lite.TocoConverter.from_session(sess, [data_x], [probs_message])\r\n        converter.quantize_weights = True\r\n        converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\n        input_arrays = converter.get_input_arrays()\r\n        print (input_arrays)\r\n        converter.quantized_input_stats = {input_arrays[0] : (0, 2)}\r\n        #converter.default_ranges_stats = (-1.0,1.0)\r\n        converter.change_concat_input_ranges = True\r\n        tflite_model = converter.convert()\r\n        open(\"sr.tflite\", \"wb\").write(tflite_model)\r\n\r\nif __name__ == \"__main__\":  \r\n    train_model(50000, 50000, 100, 10000)\r\n    eval_model(50000, 50000, 100, 10000)\r\n ```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@abladdha It appears that your model is word2vec based, for which quantization task is not supported at the moment but will be in the future. Will keep you posted on the progress.", "@wt-huang I agree that word2vec based approaches are not yet supported. But the operations I am using for model are all supported for the quantization in the current version of tensorflow (not using embedding lookup or LSTMs) ? If i am using all the quantized supported operations in my graph then it should be able to do quantization or could you elaborate more on what type of combination of operators are allowed in network...     ", "@abladdha For one, `quantized_weights` is not supported but it is in working progress.", "@wt-huang I don't understand what you're referring to by `quantized_weights` with regards to the code given. But I have the same problem, and was wondering whether matmul operation is supported at all for quantization.\r\n\r\nIf it is indeed supported at the moment, could you explain why that particular error is being thrown in this particular case? The error being: `Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data, which is necessary for quantization.` ... where `dense_prev` is a matmul.", "Hi @johncf , please use converter.post_training_quantize=True (formerly quantize_weights)\r\n\r\nAlso, set your inference type to Float for post_training_quantize=True. This is because the edges in the graph are still float for this mode, but the weights are quantized and the kernels dynamically quantize inputs.", "Closing this issue, feel free to reopen if any error comes up.", "Closing this issue because proper quantization with training is unsupported? Or did someone already fix the issue raised here in nightly?\r\n\r\nPost-training quantization is not what this issue is about.", "Hi John, right now we do not have a good solution for any arbritrary model for quantized training. For graphs with Concatenation (in your case) you will need to add FakeQuant nodes (with min max variables) before and after the Concat during training (like what the contrib/quantize rewrtier tries to do) for things to work. That is why we recommend doing post_training_quantize and only resort to trying quantized training if that doesn't work well (also let us know if that doesn't work well )\r\n\r\nWe are also working on better quantization tools for fully integer quantization and will keep you posted.", "@suharshs  Can you guide me how to use \"tf.contrib.quantize.create_training_graph()\" to fine tune from a floating point checkpoint?\r\nI alredy trained a floating point checkpoint, but when I use it to be a pretrained weight and train with \"tf.contrib.quantize.create_training_graph()\", I meet some problems, like the following:\r\n\r\n![00000000](https://user-images.githubusercontent.com/43241182/78626119-5f87a300-78c0-11ea-951d-d8b7cbff8f91.PNG)\r\nHow can I solve this problem? Thank you.\r\n", "Hi @aluds123 , \r\n\r\nThe contrib.quantize framework creates new TF variables to store min/max state for various tensors. When restoring from a float checkpoint, these values will not be initialized. You need to modify your restore your checkpoint and then initialize_all_variables to fill in default inital values for these remaining tf.variables.\r\n\r\n", "Hi @suharshs ,\r\n\r\nI find a function like this \"slim.assign_from_checkpoint_fn()\", it has a parameter \"ignore_missing_vars=True\". I use it to import floating point checkpoint and train with \"tf.contrib.quantize.create_training_graph()\", and it works.\r\nIs this use possible?\r\nSorry my poor English.\r\n\r\n"]}, {"number": 22527, "title": "[Compile Error] Compiling tf r1.8 from source in cuda9.0_cudnn7.0 container failed.", "body": "When I compile tf r1.8 from source in cuda9.0_cudnn7.0 container following this [doc](https://www.tensorflow.org/install/source), I encountered error, the error is:\r\n```\r\n./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: \r\n/tf_compile/tensorflow_cuda9.0_cudnn7.0/tensorflow/contrib/lite/toco/python/BUILD:22:1: Linking of rule '//tensorflow/contrib/lite/toco/python:_tensorflow_wrap_toco.so' failed (Exit 1)\r\ngcc: error: tensorflow_wrap_toco_versionscript.lds: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1417.439s, Critical Path: 262.20s\r\nINFO: 7170 processes: 7170 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@ChengduoZhao Hi, are you running into the same error with tf r1.9 and tf r1.10 also ?", "@harshini-gadige I have not compiled tf r1.9 or tf r1.10, but I compile the master branch, it is ok. ", "@harshini-gadige tf r1.9 and tf r1.10 is ok.", "Hi, I am also getting same error, I am compiling tf r1.7 on ubuntu 16.04 with cuda support & using cuda-9.0 and cudnn7.0\r\n\r\nERROR: /data3/tensorflow_git/tensorflow/tensorflow/contrib/lite/toco/python/BUILD:22:1: Linking of rule '//tensorflow/contrib/lite/toco/python:_tensorflow_wrap_toco.so' failed (Exit 1)\r\ngcc: error: tensorflow_wrap_toco_versionscript.lds: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1173.957s, Critical Path: 170.61s\r\nINFO: 5492 processes: 5492 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nI am trying to build from source because I get \"Illegal instruction\" error on importing tensorflow with tf1. installation.  Probably because my in xeon cpu does not support avx instruction.", "@manishkm , @ChengduoZhao \r\nHi, I got the same error lately too, bug error info cannot be searched except this new page.\r\nI guess it's the problem of new version for some dependency packages.\r\n\r\nI compared several packages(gcc, cuda, cudnn, bazel and...) with other tensorflow  installation position, I doubt it the bug of bazel(latest version: 0.17.2). So...\r\n\r\n1. rm -fr ~/.bazel ~/.bazelrc\r\n2. install bazel (version: 0.11.1, maybe others)\r\n3. bazel build ...\r\n\r\nIt's okay!", "@ChengduoZhao Hi, does it still persist after trying the workaround suggested by @dreambear1234  ?", "I ran into the same problem while building TF 1.8 with cuda support (toolkit 9.2, cudnn 7.1).\r\n\r\nBuilding with older version of bazel (0.16.1) solved the problem.", "@ChengduoZhao @kochkov92   Hi, you can find the compatible Bazel versions [here](https://www.tensorflow.org/install/source#tested_build_configurations). Please make sure you are using the correct combination of these with respect to the OS. Thank you!\r\n\r\n@ChengduoZhao Could you please confirm if this issue still persist after trying the correct and compatible Bazel version ?", "I had the same problem with TF 1.7 (toolkit 9.0, cudnn 7.0) and was able to fix the problem using Bazel 0.16.1 as well", "@dreambear1234 @yelojakit  Thanks! I meet the problem about contrib which is  same with you !And  What I sloved  the problem is uninstalling Bazel 0.17.2  and return to  use Bazel 0.16.1 as well.", "> @ChengduoZhao @kochkov92 Hi, you can find the compatible Bazel versions [here](https://www.tensorflow.org/install/source#tested_build_configurations). Please make sure you are using the correct combination of these with respect to the OS. Thank you!\r\n> \r\n> @ChengduoZhao Could you please confirm if this issue still persist after trying the correct and compatible Bazel version ?\r\n\r\nClosing this issue as there is no response from the user. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22526, "title": "How to reset per_process_gpu_memory_fraction in tf using c++?", "body": "I have develop two applications based on tf in c++ language, these applications are served as libraries. In the caller execuable program, library1 is called then library2. In library1 initialization,  gpu memory fraction is set to 0.5, run some inference, and session closed. then library2 is called, gpu memory fraction is set to 0.8, but the setting can not work, gpu memory allocation did not change. Both library have the same initialization code but differnet fraction value\r\n```\r\nint XXXLib::init(double per_process_gpu_memory_fraction)\r\n{\r\n    SessionOptions options;\r\n    ConfigProto* config = &options.config;\r\n    GPUOptions* gpu_options = config->mutable_gpu_options();\r\n\r\n    // for library1, fraction = 0.5; for library2, fraction = 0.8\r\n    gpu_options->set_per_process_gpu_memory_fraction(per_process_gpu_memory_fraction);\r\n    Status status = NewSession(options, &_session);\r\n}\r\n```\r\nIt seems that when set_per_process_gpu_memory_fraction() is called, the gpu memory in this process is fixed, even new another Newsession(), the original fraction value is used.\r\n1. Should different app(library) use different session ?\r\n2. gpu memory fraction is related to session or to process ?\r\n3. How  to change the fraction in different session but the same process? \r\n\r\nSome env info:\r\n- Have I written custom code? NO\r\n-  OS Platform and Distribution? Win10 Pro\r\n-  TensorFlow installed from? Source code\r\n-  TensorFlow version? 1.9\r\n-  CUDA/cuDNN version? CUDA9.0, cudnn 7.05\r\n-  GPU model and memory? GTX1080 with 8GB memory", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@7oud You can have multiple sessions to conduct trainings in parallel. Each session will need to have its own set of variables in its own thread. Alternatively, you can have multiple processes with its own session and graph. To change the gpu memory, you can try something like this instead:\r\n```\r\nconfig = tf.contrib.learn.RunConfig(session_config = config))\r\n```\r\nNote that graph can grow very fast and the session may be running out memory very soon.", "There will be some trouble of data access, when using multi-process with its own session. So I choose only ONE process to run multi-session in sequential not parallel. Each session has similar procedure, as in code\r\n```\r\n// define option and new session\r\nSessionOptions options;\r\noptions.config.mutable_gpu_options().set_per_process_gpu_memory_fraction(fraction);\r\n...\r\nNewSession(options, &_session);\r\n\r\n// run inference\r\n...\r\n\r\n// close session\r\n_session->Close();\r\ndelete _session;\r\n```\r\n\r\nThe problem is when the start the 2nd session, function **set_per_process_gpu_memory_fraction()** with another fraction value will not work, the process still use the gpu memory fraction setting in the 1st session.\r\n\r\n@wt-huang Could you give the c++ version of your code snippet ?", "@7oud You may need to reset your session. Try `tf.Session.reset()` before starting the second session. I have a couple of C++ versions and some could be bit outdated, for later versions you can also try `tf.contrib.estimate.RunConfig` to specify GPU memory fraction. ", "@wt-huang Before I start the 2nd session(2nd library application in the same process), I have already close the 1st session and delete the 1st session pointer. Which session should I reset? Bcz need to realize in C++, I cannot use estimate interface, May I watch your c++ code snippet, even out of date. Thanks for your reply!", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@7oud If you start the second session after completing the first one, just run the second session like any other session. Add the following to the second session to release graph resource:\r\n\r\n`tf.reset_default_graph()` \r\n\r\nThis is probably a better option as `_session->Close()` may encounter memory leak.\r\n\r\nTo allocate GPU for the second session try this from C++ side:\r\n```\r\n// define option and new session\r\nSessionOptions options;\r\noptions.config.mutable_gpu_options()->set_allow_growth(allow_growth);\r\noptions.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);\r\n```\r\n\r\n\r\n\r\n", "Closing this, feel free to reopen if any other issues come up."]}, {"number": 22525, "title": "kernel version 384.130.0 does not match DSO version 375.66.0 -- cannot find working device", "body": ">>> sess = tf.Session()\r\n2018-09-26 16:20:01.272192: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2018-09-26 16:20:01.273306: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2018-09-26 16:20:01.273383: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: hito-Lenovo\r\n2018-09-26 16:20:01.273406: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: hito-Lenovo\r\n2018-09-26 16:20:01.273480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.66.0\r\n2018-09-26 16:20:01.273609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  384.130  Wed Mar 21 03:37:26 PDT 2018\r\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10) \r\n\"\"\"\r\n2018-09-26 16:20:01.273659: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 384.130.0\r\n2018-09-26 16:20:01.273681: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version 384.130.0 does not match DSO version 375.66.0 -- cannot find working devices in this configuration\r\n", "comments": ["Who can help me solve this problem?", "You CUDA driver configuration isn't entirely correct.\r\n\r\nTry `nvidia-smi` and `sudo dmesg` and paste their output.", "@scamples Is this issue resolved ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]