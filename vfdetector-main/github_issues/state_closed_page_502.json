[{"number": 38704, "title": "[TFLite] Fully quantize the uint8 concatenation kernel to make it pure integer", "body": "Hi,\r\n\r\nThis PR fixes the issue raised by #37544 and fully quantizes the uint8 concatenation kernel in TFLite and TFLite micro so that it only uses integer operations..\r\n\r\nIt pre-calculates the scaling multiplier and shift with QuantizeMultiplier and then uses MultiplyByQuantizedMultiplier instead of a floating-point multiplication.\r\n\r\nThibaut", "comments": ["Since there are models running with the existing rescaling operation, the change could theoretically cause a change in accuracy (or even just a change in one input's prediction). Therefore, if we add this change we should probably add an attribute to the operation that signals a version change (similar to what @wwwind  has done here\u00a0https://github.com/tensorflow/tensorflow/pull/35985) , and set the corresponding version in the converter.", "@Tessil Can you please check @suharshs comments and keep us posted. Thanks!", "Hi,\r\n\r\nThank you for your answer. Changing from a floating-point scaling to a fixed-point scaling may effectively cause a small change in accuracy though it should be quite rare. I am currently creating a new version of the concatenation operator with an additional `fixed_point_scaling:bool = false` field in the flatbuffers to avoid any accuracy change with older tflite files. I will update the PR when everything is ready.\r\n", "Hi,\r\n\r\nI created a new version for the concatenation operator. I used 4 as new version number to avoid a future conflict with PR #38869 which creates a version 3 of the same operator. This can be changed if necessary.\r\n\r\nThe new version has an extra `fixed_point_scaling:bool` field in the TFLite flatbuffer. It defaults to 'false' when importing an old '.tflite' file without this field and to 'true' with new exports.\r\n\r\nI currently edited the `schema_generated.h` manually as I ended up with the same problem as @wwwind with `flatc` in PR https://github.com/tensorflow/tensorflow/pull/35985#discussion_r402458171 . What is the correct way to generate this file?\r\n\r\nI followed the documentation regarding the [versioning of the operators](https://www.tensorflow.org/lite/guide/ops_version) though it doesn't seem totally complete. Let me know if there is something that I have forgotten regarding the versioning.", "@Tessil  Can you please resolve conflicts? Thanks!", "@gbaned Done, I also adapted the new concatv2I64Axis test that was added in the master branch.", "Feng, could you verify if the changes for the mlir converter are required. Or if because of the same scales trait, we can avoid needing to set this field in the mlir converter.\r\n\r\nthanks!", "Thank you for the confirmation regarding the `SameOperandsAndResultsScale` trait, I removed the `fixed_point_scaling` field of the MLIR `TFL_ConcatenationOp` to simplify the code. As the trait guarantees that the input and output scales will be the same, it doesn't matter if the `fixed_point_scaling` is set to true for the MLIR converter, the kernel will just do a `memcpy` in any case.", "@suharshs I resolved a merge conflict that arised, the PR will need a re-approval. Thanks", "@Tessil Can you please resolve conflicts? Thanks!", "@gbaned Done, it will again need re-approval. Thanks.", "Looks like this is a very general optimization. I'm wondering whether we have a general design for this. \r\n\r\nFor example, we can make two optional fields in the quantization parameters of the flatbuffer tensors. Then the fixed point quantization parameter there. Then we can update all the kernels to support this optional fields.", "I am not sure it is worth to generalize the `fixed_point_scaling` parameter. Looking at some of the other layers the usage of a floating-point scaling seems to be an exception here, other layers tend to use a fixed-point scaling. \r\n\r\nI resolved a conflict that arised again, the PR will need re-approval if we don't plan to generalise the fixed-point scaling parameter. Thanks!", "@Tessil Can you please resolve conflicts? Thanks!", "Hi,\r\n\r\n@gbaned Thanks, I fixed the the merge conflict.\r\n\r\nI also changed the `tflite::ops::micro::concatenation::Prepare` switch case introduced by 6a22ad8f67f25a5a321ff6306ccbac4a3e33ee19 a bit. Setting-up the scalings for int8 kernels is not necessary as `EvalUnquantized` is called with int8 anyway.\r\n\r\n@liufengdb, @suharshs Could you take a look when you have a bit of time? The merge conflicts start to accumulate.\r\n\r\nThanks,\r\nThibaut", "@Tessil Can you please resolve conflicts? Thanks!", "Hi,\r\n\r\n@gbaned, @suharshs I fixed the merge conflicts, the PR will need re-approval. Thanks.", "@gbaned It seems there is an error with copybara \"An error happened while migrating the change\", is there anything I can do? Thanks.", "> @gbaned It seems there is an error with copybara \"An error happened while migrating the change\", is there anything I can do? Thanks.\r\n\r\n@Tessil Nothing to do at this moment from your side. We will come back if anything required. Thank you!", "@Tessil  Can you please resolve conflicts? Thanks!", "@gbaned I fixed the conflicts. It seems though that once the PR is approved and all the tests are successful, the PR is not merged automatically even after staying for one week in the 'ready to pull' state. Could you eventually check if there is a problem internally regarding the PR? Thanks!", "> @gbaned I fixed the conflicts. It seems though that once the PR is approved and all the tests are successful, the PR is not merged automatically even after staying for one week in the 'ready to pull' state. Could you eventually check if there is a problem internally regarding the PR? Thanks!\r\n\r\n@Tessil  This is waiting for an internal approval.  We will let you know if anything required from you. Thank you!", "@Tessil  Can you please resolve conflicts? Thanks!", "@gbaned @rthadur I fixed the merge conflict, the PR will need re-approval. Thanks.", "@jianlijianli, @daverim Can you please have a look at this PR? Thank you!", "Fixed a merge conflict that arisen. The PR will need re-approval, thanks.", "The TFLite micro build is failing on the `using tflite::testing::F2Q` statement. I can't find the definition of this symbol, nor the name `tflite::testing::F2QS` used elsewhere in Micro. Adding a Micro team member to take a look at the CI failure and advise. ", "@advaitjain can you take a look at the Micro build failure and advise? Thanks very much!", "Thank you very much for looking into it. It seems `F2Q` was removed by https://github.com/tensorflow/tensorflow/commit/d39ff659206f7a08583608f7971abd747f5f6709 26 days ago and I missed it during the merge conflict resolution. I will check to update the PR tomorrow.", "There might be a few additional micro-specific things to fix.\r\n\r\nPlease use the following command to reproduce locally:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test\r\n```", "@talumbau I adapted the TFL and TFL\u00b5 tests to the changes from the master branch and tested them locally. Let me know if there is any problem. Thanks!", "@talumbau I fixed a problem in the TF\u00b5 test, the PR will need re-approval, sorry. \r\n\r\nI used `bazel test //tensorflow/lite/micro/kernels:concatenation_test` to test locally instead of `make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test` which didn't show the initializer warning.", "The Linux GPU and TFLite Micro CI build failures seem spurious. Is that what is blocking the copybara import? Will have to ask around to get an answer. ", "I approved the copybara import and can now see the internal change up for review.", "@Tessil can you please check below error \r\n\r\n```\r\ntensorflow/lite/micro/kernels/concatenation.cc:227:46: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n          QuantizeMultiplier(input_scales[i] * inverse_output_scale,\r\n```", "@rthadur Thank you, I fixed the error.", "Thanks for the quick fix! There is one last question that came up in the review. It appears that `fixed_point_scaling` is never set to `true` in the MLIR conversion path. The default setting is `true` for a toco-based conversion path, which is now deprecated. Can you explain your intended usage of this flag?\r\n\r\nOverall, it appears that, if one is still using a toco-based path to convert, one would get this flag set to true as default, but if one is using the MLIR-based conversion path, one would get this flag as false. If that is correct, that is probably not the best path to go down. In other words, it seems not desirable to add new behavior to a deprecated path. Please correct me if I have misunderstood what's happening here. Thanks!", "I initially added a `fixed_point_scaling` parameter to the MLIR `TFL_ConcatenationOp` which defaulted to true (see https://github.com/tensorflow/tensorflow/pull/38704#discussion_r418516214) but it seems it wasn't necessary as the new MLIR converter forces the scaling of the input and output to be the same. As far as I know it isn't possible with the MLIR converter to create a model with an uint8 concatenation layer that needs rescaling.\r\n\r\nThis PR is quite old and was initially created when the TOCO converter was still kind of used. As the TOCO converter is now completely deprecated and if what I said above is correct, we can probably abandon this PR as in the end I don't think it's really useful any more. I'll check in intern if we can close this.\r\n", "I believe the main reason for this PR is captured in the first line of its description - to fix the reference implementation so that it does not contain floating point operations. The `fixed_point_scaling` flag was introduced to distinguish between the old and new version of the operator. Changing the behaviour of the old converter and not the MLIR-based path is the result of the [this disucssion](https://github.com/tensorflow/tensorflow/pull/38704#issuecomment-637745934), and seems to be an unfortunate artefact of the fact that two converters treat relation between input and output scales differently (following the change highlighted [here](https://github.com/tensorflow/tensorflow/issues/37544#issuecomment-658580281), if I follow things correctly).\r\n\r\n@dominicsymes, what is the best way forward here from your point of view?\r\n", "Oh yes, effectively. @liufengdb added an `extraClassDeclaration` with the https://github.com/tensorflow/tensorflow/commit/ac9969a2762d4068edef7e451b53256d2adb2692 commit later on. The comment above (https://github.com/tensorflow/tensorflow/pull/38704#discussion_r418516214) is thus not valid anymore and we need to re-add the `fixed_point_scaling` parameter in the `TFL_ConcatenationOp`. I will check to update the PR today.", "@talumbau I updated the PR, the `fixed_point_scaling` is now set to true for both new TOCO and MLIR converted models as it was done before. Please let me know if there is any problem, thanks.", "Thanks for the MLIR changes. Looks good to me!", "@Tessil  Can you please resolve conflicts? Thanks!", "@gbaned Resolved, thanks!", "@Tessil Can you please resolve conflicts? Thanks!", "With uint8 support being removed from TFLite Micro and deprecated in TFLite since some time, the PR is not much more relevant any more and it's probably not worth to spend more time on it. I will close the PR."]}, {"number": 42800, "title": "[ar] How to show the titles in Arabic for already translated tutorials in the side navigation menu?", "body": "When I choose the Arabic language, for a given tutorial, like this one: https://www.tensorflow.org/tutorials/keras/classification?hl=ar, I was expecting to see the title of this already translated tutorial in Arabic in the right side navigation menu, but it's showing in English as you can see in the following picture:\r\n \r\n<img width=\"1265\" alt=\"image\" src=\"https://user-images.githubusercontent.com/2883926/79729137-55699980-82ef-11ea-890b-3dab9c932652.png\">\r\n\r\nShowing the translated tutorials' title in Arabic for already translated tutorial can be useful to reader as a visual identification of what is already in Arabic.\r\n\r\n@lamberta\r\nCould you explain whether it's something I can configure myself with a PR or it requires extra development?\r\n\r\nThanks.", "comments": ["Hi @mohamed-ali \r\n\r\nThe site navigation is displayed using YAML files, for example, here's the English source for the tutorials: [site/en/tutorials/_toc.yaml](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/_toc.yaml).\r\n\r\nUnfortunately, these are out-of-scope for community translations. In the past we've allowed this, but they quickly fall out of sync with the source material\u2014and the cost is very high, since users will miss entire documents (sometimes sections). These were added to the [Do not translate list](https://github.com/tensorflow/docs-l10n#do-not-translate).\r\n\r\nSome languages on tensorflow.org are \"officially\" supported, meaning we use an internal process to translate the navigation. But, at this time, Arabic is still \"unofficial\" and not part of this internal process. Occasionally new languages are added to the \"official\" list, but that depends on a number of things. Will keep Arabic in mind, though, especially as the ML community grows."]}, {"number": 38703, "title": "Mixed Precision: which 'endpoint layer' to set dtype='float32' in Dueling DQN", "body": "Hi, following the [mixed precision documentation](https://www.tensorflow.org/guide/keras/mixed_precision#overview) , it is recommended to have dtype of output layer as float32\r\n\r\n>  And regardless of what your model ends in, make sure the output is float32.\r\n\r\nHowever, I am not sure which 'layer' to apply float32 for Dueling DQN model. The architecture is as follows:\r\n```\r\nbody = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'),\r\n                             tf.keras.layers.Dense(256, activation='relu'),\r\n                             tf.keras.layers.Dense(256, activation='relu'),])\r\n\r\nvalue = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'),\r\n                              tf.keras.layers.Dense(1),])\r\n\r\nadvantage = tf.keras.Sequential([tf.keras.layers.Dense(256, activation='relu'),\r\n                                  tf.keras.layers.Dense(3, use_bias=False),])\r\n\r\nq = body(training_input)\r\n\r\nv = value(q)\r\n\r\na = advantage(q)\r\na -= tf.reduce_mean(a, axis=-1, keepdims=True)\r\n\r\nfinal_q = v + a \r\n# final_q is used to calculate loss\r\n```\r\nshould I set dtype='float32' at cases (1, 2, or 3):\r\n\r\n1. value **&** advantage `Sequential` function\r\n2. value `Sequential` function **&**`tf.reduce_mean()` function\r\n3. **cast** final_q as float32\r\n\r\nor maybe something else?", "comments": ["Case 3: cast final_q to float32. Because the model does not end in softmax, the only important aspect is that the loss is done in float32, so final_q should be cast to float32.\r\n\r\nFYI, there are two motivations in doing the loss in float32\r\n1. Often, `tf.reduce_sum` is used to reduce a batch of losses into a single loss. If the batch size is very large, this can overflow in float16. With `Model.fit`, the reduce_sum is done in float32 by Keras, so this recommendation only applies to custom training loops.\r\n2. If custom loss are used, the loss may not support float16."]}, {"number": 38702, "title": "Data cardinality", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): preinstalled\r\n- TensorFlow version (use command below): v2.2.0-rc3-0-gaad398b5e9, 2.2.0-rc3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI build a model with to independent sequential sub-models like:\r\n![image](https://user-images.githubusercontent.com/44928904/79725338-5229da00-82fe-11ea-943b-58c2af493e8b.png)\r\n but number of samples for two sub-models are different and i am seeing this error:\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-33-a01d55a188be> in <module>()\r\n----> 5 model.fit({'input1':x1.to_numpy(), 'input2':x2.to_numpy()}, {'output1':y1.to_numpy(), 'output2':y2.to_numpy()}, epochs=10)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\r\n    280             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\r\n    281       msg += \"Please provide data which shares the same first dimension.\"\r\n--> 282       raise ValueError(msg)\r\n    283     num_samples = num_samples.pop()\r\n    284 \r\n\r\nValueError: Data cardinality is ambiguous:\r\n  x sizes: 2408, 2371\r\n  y sizes: 2408, 2371\r\nPlease provide data which shares the same first dimension.\r\n**Describe the expected behavior**\r\nbecause the number of batches are the same, so i think it must work...\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nYou can also share your Colab notebook. Select 'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!", "It's a very simple model, like:\r\n\r\ninput1 = Input((310,), name='input1')\r\ninput2 = Input((310,), name='input2')\r\n\r\nl1 = Dense(64, activation='relu')(input1)\r\nl1 = Dense(1, activation='sigmoid', name='output1')(l1)\r\n\r\nl2 = Dense(64, activation='relu')(input2)\r\nl2 = Dense(1, activation='sigmoid', name='output2')(l2)\r\n\r\nmodel = keras.Model(inputs=[input1, input2], outputs=[l1, l2])\r\nkeras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)\r\nmodel.compile(optimizer='adam', loss={'output1':keras.losses.MeanSquaredError(),'output2':keras.losses.MeanSquaredError()})\r\nmodel.summary()\r\n\r\nand you can check it by som dummy x1 and x2 but with different first dimension, for example:\r\nx1.shape = (1000,310)\r\ny1.shape = (1000,)\r\nx2.shape = (2000,310)\r\ny2.shape = (2000,)\r\nmodel.fit({'input1':x1, 'input2':x2}, {'output1':y1, 'output2':y2}, epochs=10)", "@mohammadalimehdizadeh,\r\nOn running the above code, I am facing a different error stating `ValueError: All input arrays (x) should have the same number of samples. Got array shapes: [(1000, 310), (2000, 310)]`. \r\n\r\nOn changing the dimensions, I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/829637c2c12963876c294a7930b5fe0f/38702.ipynb#scrollTo=8oU1vMyzaEEJ). Thanks", "@amahendrakar thanks for your response.\r\nI solve it by using tf.GradientTape and optimizater.apply_gradients instead of fit method it work nice!!!\r\n\r\nBut why is fit method design in this manner?", "it is the feature of problem, i have two input with different number of samples. it's the nature of  problem.", "@mohammadalimehdizadeh,\r\nAs the error message states, all input arrays should be of the same size. In your case the shape of `x1` and `x2` was different. Any mismatch in the input data raises a ValueError. You can read more about the fit method [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\r\n\r\nIs this still and issue? Please feel free to close the issue if resolved. Thanks!", "> @amahendrakar thanks for your response.\r\n> I solve it by using tf.GradientTape and optimizater.apply_gradients instead of fit method it work nice!!!\r\n> \r\n> But why is fit method design in this manner?\r\n\r\nhow did you use the tf.GradientTape and optimizater.apply_gradients  to deal with that problem, can you share the details? I have been stuck for a long time. Thanks!", "Same issue for me, \r\n\r\nI designed a custom tensorflow model with multiple inputs and one binary output\r\nAnd when I use the .fit method, even though the batch size is correct they throw an error because the inner dimensions do not match\r\n", "@Luckilyeee, @MaxHeuillet,\r\nCould y'all please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "> > @amahendrakar thanks for your response.\r\n> > I solve it by using tf.GradientTape and optimizater.apply_gradients instead of fit method it work nice!!!\r\n> > But why is fit method design in this manner?\r\n> \r\n> how did you use the tf.GradientTape and optimizater.apply_gradients to deal with that problem, can you share the details? I have been stuck for a long time. Thanks!\r\n\r\nHi, have you know how to use the tf.GradientTape and optimizater.apply_gradients to solve the problem? Thanks! I am facing the same problem and would you like to share the information with me?\r\n"]}, {"number": 38701, "title": "TFLite Concatenation Fails on GPU delegate", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Tested on Samsung Galaxy\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nConcatenation Op for non BCHW dimensions fails on GPU delegate, same model runs fine on CPU delegate.\r\n\r\n(See below code for creating `model.tflite`)\r\n\r\nThe following benchmark runs successfully (CPU):\r\n```\r\nadb shell /data/local/tmp/benchmark_model      --graph=/data/local/tmp/model.tflite     --num_threads=4 --use_gpu=false --num_runs=1000\r\n```\r\n\r\nThe following benchmark run fails (GPU):\r\n```\r\nadb shell /data/local/tmp/benchmark_model      --graph=/data/local/tmp/model.tflite     --num_threads=4 --use_gpu=true --num_runs=1000\r\n```\r\nError message:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: TfLiteGpuDelegate Init: CONCATENATION: Dimensions are not BHWC: 2 1 30\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 1 (TfLiteGpuDelegateV2) failed to prepare.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTFLite Model should run concat op succesfully both on CPU and GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Construct a basic model.\r\nroot = tf.train.Checkpoint()\r\nroot.v1 = tf.Variable(np.ones((1, 10), dtype=np.float32))\r\nroot.v2 = tf.Variable(np.ones((1, 20), dtype=np.float32))\r\nroot.f = tf.function(lambda x: tf.concat([root.v1, root.v2, x], axis=1))\r\n\r\n# Save the model.\r\nexport_dir = \"/tmp/test_saved_model\"\r\ninput_data = tf.constant(np.ones((1, 30), dtype=np.float32))\r\nto_save = root.f.get_concrete_function(input_data)\r\ntf.saved_model.save(root, export_dir, to_save)\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\r\ntflite_model = converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n", "comments": ["The gpu delegate is not really friendly with non-constant tensors. Could you tried to use constant tensors in your model, convert it and try again?", "Do you mean changing `tf.Variable` to `tf.constant`?\r\n\r\nTried that, same issue occurs.", "@mtngld You can use tf.Variable but set trainable=False", "Still fails with the same error\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nshape = (1, 10)\r\n\r\n# Construct a basic model.\r\nroot = tf.train.Checkpoint()\r\nroot.v1 = tf.Variable(np.ones(shape, dtype=np.float32), trainable=False)\r\nroot.v2 = tf.Variable(np.ones(shape, dtype=np.float32), trainable=False)\r\nroot.f = tf.function(lambda x: tf.concat([root.v1, root.v2, x], axis=1))\r\n\r\n# Save the model.\r\nexport_dir = \"/tmp/test_saved_model\"\r\ninput_data = tf.constant(np.ones(shape, dtype=np.float32))\r\nto_save = root.f.get_concrete_function(input_data)\r\ntf.saved_model.save(root, export_dir, to_save)\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\r\ntflite_model = converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nOutput:\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: TfLiteGpuDelegate Init: CONCATENATION: Dimensions are not BHWC\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 1 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply GPU delegate.\r\nBenchmarking failed.\r\n```\r\n\r\nBTW, even when changing shape to `shape = (1, 1, 1, 10)` in order to match the BHWC (and concating on `axis=3`) requirement the benchmark fails on GPU with a different error:\r\n\r\n```\r\nLoaded model /data/local/tmp/model.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: No selector for const\r\nERROR: Falling back to OpenGL\r\nINFO: Initialized OpenGL-based API.\r\nERROR: TfLiteGpuDelegate Init: Suitable node shader is not found:\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 1 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply GPU delegate.\r\nBenchmarking failed.\r\n```\r\n", "@impjdi Do you think this is a bug?\r\nNo suitable shader found for concat and no error message explain about that.", "ERROR: TfLiteGpuDelegate Init: CONCATENATION: Dimensions are not BHWC\n\nI think that's the reason?\n\n\nOn Sun, Apr 26, 2020 at 22:05 Thai Nguyen <notifications@github.com> wrote:\n\n> @impjdi <https://github.com/impjdi> Do you think this is a bug?\n> No suitable shader found for concat and no error message explain about\n> that.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38701#issuecomment-619721229>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKKUT2RWCIKJN2OQ3RAXJ3ROUHAPANCNFSM4MMGKERQ>\n> .\n>\n", "@impjdi I am mentioning about the later part. When BHWC was being used but there was another error.", "@thaink, @impjdi - thank you for taking a look at this issue.\r\n\r\nJust to summarize - \r\n\r\nLooks like there are 2 different issues here:\r\n1. BHWC concat op using GPU delegate -> `Suitable node shader is not found`\r\n2. Non-BHWC concat op using GPU delegate -> `ERROR: TfLiteGpuDelegate Init: CONCATENATION: Dimensions are not BHWC`\r\n\r\n(Note: Both operations work fine on CPU delegate)\r\n\r\n", "Thanks for elaborating.  The first thing happens here`//tensorflow/lite/delegates/gpu/gl/kernels/registry.cc` Could you please add a print statement and tell me which key is causing the issue?", "Sure,\r\n\r\nI have added the following print \r\n\r\n```\r\n    std::vector<std::string> errors;\r\n    std::cout << \"ctx.node->operation.type: \" <<  ctx.node->operation.type << \"\\n\";\r\n    auto it = shaders_.find(ctx.node->operation.type);\r\n```\r\n\r\nResult - \r\n```\r\nctx.node->operation.type: const\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "That's quite interesting.  To me it feels like it should have failed way before getting to this point for const not being a supported op.  Not sure why this slips through.\r\n\r\nIf you change tf constant to tf placeholder, does it work?", "I am using TF2.1 and TF2.2-RC3, I thought placeholders and `sess.run` are deprecated, not sure how to change the `tf.constant` into `tf.placeholder` and export the model.", "Hm, not a proficient TF user myself and can't give you guidance there =/\r\n\r\nIn the meantime, I found why it's letting `const` through.  For whatever reason, it's registered as an available op.  I created a change to make it clearer in the log (under review).", "Hi @impjdi - has the logging change that you mentioned passed the review and merged into master?", "Sorry, that got some comments and I never revisited.  Will take another look.", "Hi, any update on this issue? \r\n\r\nJust tested exporting in both 2.3.1 and tf-nightly (2.4.0-dev20200930), running on device with the [pre-built benchmark app](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model.apk) - still seeing the same issue.\r\n\r\nError message is a bit more clear \r\n\r\n```\r\nTfLiteGpuDelegate Init: CONCATENATION: Expected a 4D tensor of shape BxHxWxC but got 1x10\r\n```\r\n", "Also,\r\n\r\nThe even when using BHWC, the shader error is still there:\r\n\r\n```\r\nTfLiteGpuDelegate Init: Suitable node shader is not found:\r\n```\r\n\r\nSo anyway, both issues from https://github.com/tensorflow/tensorflow/issues/38701#issuecomment-620398307 are still there in tf-nightly", "Ah, the logging change dropped under the reviewer's radar, and I got the approval like a month ago, but then it got buried deep inside my list of changes, that I didn't notice the approval.  I saw that change last week, but saw that the approval was conditional and required some changes which I never got the time to do.  Let me do that quickly...", "commit 04a8d6e3f4eabae3b06cfdba942ec75da56e55d8 should improve logging.", "Thank you for the improved logging - \r\n\r\n```\r\n10-06 14:17:17.732 19342 19342 E tflite  : TfLiteGpuDelegate Init: No shader implementation for const\r\n```\r\n\r\nThere is definitely some issue with the shader implementation for constants, however I think this is not the root problem of my model and only arises from the toy example I provided (model composed of two constants)\r\n\r\n\r\nI have edited my toy example to use Keras API, here are three different variants -\r\n\r\n### Concatenate input layer and constant layer, BxHxWxC - Works Fine\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (1, 1, 1, 10)\r\n# Create a model using high-level tf.keras.* APIs\r\ninput = tf.keras.Input(shape=shape[1:], batch_size=1)\r\ninitializer = tf.keras.initializers.Constant(value=0)\r\nconst = initializer(shape=shape)\r\noutput = tf.keras.layers.concatenate([input, const])\r\nmodel = tf.keras.Model(inputs=[input], outputs=output)\r\nmodel.summary()\r\nmodel.compile() # compile the model\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n### Concatenate two input layers BxC - Works fine\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (1, 10)\r\n# Create a model using high-level tf.keras.* APIs\r\ninput = tf.keras.Input(shape=shape[1:], batch_size=1)\r\ninput_2 = tf.keras.Input(shape=shape[1:], batch_size=1)\r\noutput = tf.keras.layers.concatenate([input, input_2])\r\nmodel = tf.keras.Model(inputs=[input, input_2], outputs=output)\r\nmodel.summary()\r\nmodel.compile() # compile the model\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n### Concatenate input layer and constant layer, BxC - Fails\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (1, 10)\r\n# Create a model using high-level tf.keras.* APIs\r\ninput = tf.keras.Input(shape=shape[1:], batch_size=1)\r\ninitializer = tf.keras.initializers.Constant(value=0)\r\nconst = initializer(shape=shape)\r\noutput = tf.keras.layers.concatenate([input, const])\r\nmodel = tf.keras.Model(inputs=[input], outputs=output)\r\nmodel.summary()\r\nmodel.compile() # compile the model\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\nfailed logcat:\r\n```\r\n10-06 14:49:30.819 23274 23274 I tflite_BenchmarkModelActivity: Running TensorFlow Lite benchmark with args: --graph=/data/local/tmp/model.tflite \\     --num_threads=4 --use_gpu=true\r\n10-06 14:49:30.827 23274 23274 I tflite  : Initialized TensorFlow Lite runtime.\r\n10-06 14:49:30.827 23274 23274 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n10-06 14:49:30.828 23274 23274 E tflite  : TfLiteGpuDelegate Init: CONCATENATION: Expected a 4D tensor of shape BxHxWxC but got 1x10\r\n10-06 14:49:30.828 23274 23274 I tflite  : Created 0 GPU delegate kernels.\r\n10-06 14:49:30.828 23274 23274 E tflite  : TfLiteGpuDelegate Prepare: delegate is not initialized\r\n10-06 14:49:30.828 23274 23274 E tflite  : Node number 1 (TfLiteGpuDelegateV2) failed to prepare.\r\n10-06 14:49:30.828 23274 23274 E tflite  : Restored original execution plan after delegate application failure.\r\n```\r\n\r\n### Conclusion\r\n\r\nThe specific case where the input layer is concatenated with constant layer AND the dimensions are not BHWC fails, other use cases are fine.\r\n\r\n ", "Thanks for the experiments.  So it's just the matter of `CONCAT` not being able to handle a const tensor as input tensor.  In that case, could you use `PAD` the input tensor with zeroes and expand the constant tensor with zeroes, so that you can achieve a `CONCAT` through a `PAD` followed by an `ADD`?", "The PAD workaround seems to be partially working \r\n\r\nHowever PAD by itself doesn't seem to support 2D tensors - \r\n\r\n```\r\n10-06 23:18:51.113  9281  9281 I tflite  : Initialized TensorFlow Lite runtime.\r\n10-06 23:18:51.115  9281  9281 I tflite  : Created TensorFlow Lite delegate for GPU.\r\n10-06 23:18:51.117  9281  9281 E tflite  : Following operations are not supported by GPU delegate:\r\n10-06 23:18:51.117  9281  9281 E tflite  : PAD: Invalid paddings tensor shape: expected 4x2 or 3x2, got 2x2\r\n10-06 23:18:51.117  9281  9281 E tflite  : 81 operations will run on the GPU, and the remaining 38 operations will run on the CPU.\r\n10-06 23:18:53.464  9281  9281 I tflite  : Initialized OpenCL-based API.\r\n10-06 23:18:53.681  9281  9281 I tflite  : Created 1 GPU delegate kernels.\r\n```\r\n\r\nSo at least it doesn't fail on GPU, but seems like some of the graph still runs on CPU.\r\n\r\n", "Thanks for trying.  That's annoying.  The only cheating I can think of is inserting the content of the constant tensor as the graph's input tensor.  Beyond this, I'm out of ideas...\r\n\r\nWe might have to support the constant input tensor case properly, but that requires a larger plumbing (it's not just updating the shader impl); we probably can't prioritize that at the moment.", "Was able to replicate the issue with TF 2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/0601e2344a547c11e4afd1ca59deb8a9/untitled287.ipynb) ..Thanks!", "I cannot immediately work on this because I have other higher priorities.  Attaching model for now.\r\n[concat_fail.tflite.zip](https://github.com/tensorflow/tensorflow/files/6687903/concat_fail.tflite.zip)\r\n", "I am getting output in both [GPU](https://colab.research.google.com/drive/1EPjFqyfwLcsPdBo4Y6wamZnay2t1IcFr?resourcekey=0-6w6oN4cJJPuLMB_Nhr4qdg#scrollTo=6_Ygo8suaYQN) and[ CPU](https://colab.research.google.com/gist/sushreebarsa/0601e2344a547c11e4afd1ca59deb8a9/untitled287.ipynb) with TF v2.5 in Colab....Please find the gists and let us know if it is good to close ..Thanks !", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38701\">No</a>\n"]}, {"number": 38700, "title": "[1.14] get_graph_def_from_url_tarball more explicit exception when unfinished donwload.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): -\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalin 10.15.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: - \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: - \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHi, currently in version `1.14.0` using `contrib.gan` package. When using `get_graph_def_from_url_tarball` internally by default it fetch file from remote, however when such file name presented on disk or you stopped download in progress after you restart it you will get `EOFError: Compressed file ended before the end-of-stream marker was reached`. This is a bit confusion and may take some time to explore reason. It would be better if `get_graph_def_from_url_tarball` raised more verbose exception when it can find file with such name here \r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py#L212-L223\r\n\r\nIf you consider it reasonable, i can work on PR.", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 38699, "title": "Tensorflow hangs, requires reboot", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nLinux Ubuntu 18.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\nN/A\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\nbinary\r\n\r\n- TensorFlow version (use command below):\r\n\r\nv2.1.0-rc2-17-ge5bf8de \r\n2.1.0\r\n\r\n- Python version:\r\n\r\n3.7.7\r\n\r\n- Bazel version (if compiling from source):\r\n\r\nN/A\r\n\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n\r\nN/A\r\n\r\n- CUDA/cuDNN version:\r\n\r\n10.1\r\n\r\n- GPU model and memory:\r\n\r\nGTX 1080ti 11GB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUsing TF dataset after ~1000 batches TF hangs, nvidia-smi reports 100% usage (but fan is at 0% so that is bogus).  After I kill the process I get an XLI warning in nvidia-smi and have to reboot to try again.  \r\n\r\nI have the same version of this code running with a Keras sequence and multiprocessing and it runs fine.  It is actually faster than the TF dataset code, keeping my GPU pegged at 60% where the TF dataset only does around 50% (while it is running but it hangs pretty fast).  \r\n\r\nI have the GUI turned off for Ubuntu and TF is the only process on my GPU and it is set to keep all the memory.  There are no other signficant processes on the machine.  The machine is brand new with 128GB of RAM, NVM hard drives and 12 core AMD Ryzen (so speed should not be an issue).\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nTF not to hang and force me to reboot, as well as TF dataset to be faster than a keras sequence with multiprocessing.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis is really hard to do, I have thousands of lines of code that go into running this, plus 400 GB of data.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nGDB output below, when it hangs out, I ctrl-c and bt to get this.  If there is a better way to get a stacktrace where it hangs let me know and I'll do it.\r\n\r\n```\r\n(gdb) bt\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n#1  0x00007fff7f9d5b1b in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff7f9d5139 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff7f9d26fb in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff7f9d2bd3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff77ab888c in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::CancellationManager*, absl::optional<tensorflow::EagerRemoteFunctionParams> const&) () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fff77ab8ce9 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::EagerKernelArgs const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::CancellationManager*, absl::optional<tensorflow::EagerRemoteFunctionParams> const&) () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fff77a95b38 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, absl::optional<tensorflow::EagerRemoteFunctionParams> const&, std::unique_ptr<tensorflow::KernelAndDevice, tensorflow::core::RefCountDeleter> const&, tensorflow::GraphCollector*, tensorflow::CancellationManager*, absl::Span<tensorflow::TensorHandle*>) () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fff77a9612d in tensorflow::ExecuteNode::Run() () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fff77ad6cf5 in tensorflow::EagerExecutor::RunItem(std::unique_ptr<tensorflow::EagerExecutor::NodeItem, tensorflow::core::RefCountDeleter>) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fff77ad744b in tensorflow::EagerExecutor::AddOrExecute(std::unique_ptr<tensorflow::EagerNode, std::default_delete<tensorflow::EagerNode> >) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fff77a9078e in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007fff77a93d60 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) ()\r\n---Type <return> to continue, or q <return> to quit---\r\n   from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007fff7792219d in TFE_Execute () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007fff778a8844 in TFE_Py_ExecuteCancelable(TFE_Context*, char const*, char const*, absl::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, TFE_CancellationManager*, absl::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007fff778a8d81 in TFE_Py_Execute(TFE_Context*, char const*, char const*, absl::InlinedVector<TFE_TensorHandle*, 4ul, std::allocator<TFE_TensorHandle*> >*, _object*, absl::InlinedVector<TFE_TensorHandle*, 2ul, std::allocator<TFE_TensorHandle*> >*, TF_Status*) () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007fff76f1940a in _wrap_TFE_Py_Execute () from /home/jostheim/virtualenvs/data_science/lib/python3.7/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#17 0x0000555555684f77 in _PyMethodDef_RawFastCallKeywords ()\r\n#18 0x0000555555684d80 in _PyCFunction_FastCallKeywords ()\r\n#19 0x00005555556f7de5 in _PyEval_EvalFrameDefault ()\r\n#20 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#21 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#22 0x00005555556f4406 in _PyEval_EvalFrameDefault ()\r\n#23 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#24 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#25 0x00005555556f4406 in _PyEval_EvalFrameDefault ()\r\n#26 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#27 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#28 0x00005555556f3840 in _PyEval_EvalFrameDefault ()\r\n#29 0x000055555568663a in _PyFunction_FastCallKeywords ()\r\n#30 0x00005555556f3840 in _PyEval_EvalFrameDefault ()\r\n#31 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#32 0x0000555555686db1 in _PyObject_Call_Prepend ()\r\n#33 0x00005555556c9319 in ?? ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#34 0x00005555556871d1 in PyObject_Call ()\r\n#35 0x00005555556f4d75 in _PyEval_EvalFrameDefault ()\r\n#36 0x00005555556f2d15 in _PyEval_EvalCodeWithName ()\r\n#37 0x0000555555686db1 in _PyObject_Call_Prepend ()\r\n#38 0x00005555556871d1 in PyObject_Call ()\r\n#39 0x00005555556f4d75 in _PyEval_EvalFrameDefault ()\r\n#40 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#41 0x0000555555686db1 in _PyObject_Call_Prepend ()\r\n#42 0x00005555556c9319 in ?? ()\r\n#43 0x0000555555685792 in _PyObject_FastCallKeywords ()\r\n#44 0x00005555556f790d in _PyEval_EvalFrameDefault ()\r\n#45 0x00005555556f28ac in _PyEval_EvalCodeWithName ()\r\n#46 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#47 0x00005555556f35e6 in _PyEval_EvalFrameDefault ()\r\n#48 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#49 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#50 0x00005555556f4406 in _PyEval_EvalFrameDefault ()\r\n#51 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#52 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#53 0x00005555556f4406 in _PyEval_EvalFrameDefault ()\r\n#54 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#55 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#56 0x00005555556f4406 in _PyEval_EvalFrameDefault ()\r\n#57 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#58 0x000055555568671a in _PyFunction_FastCallKeywords ()\r\n#59 0x00005555556f4406 in _PyEval_EvalFrameDefault ()\r\n#60 0x000055555568663a in _PyFunction_FastCallKeywords ()\r\n#61 0x00005555556f35e6 in _PyEval_EvalFrameDefault ()\r\n#62 0x00005555556f2715 in _PyEval_EvalCodeWithName ()\r\n#63 0x00005555556f2413 in PyEval_EvalCode ()\r\n#64 0x00005555557bc802 in ?? ()\r\n#65 0x00005555557bcb7d in PyRun_FileExFlags ()\r\n#66 0x00005555557bca26 in PyRun_SimpleFileExFlags ()\r\n#67 0x0000555555794203 in ?? ()\r\n#68 0x0000555555793eac in _Py_UnixMain ()\r\n#69 0x00007ffff6bf2b97 in __libc_start_main (main=0x5555556818a0 <main>, argc=3, argv=0x7fffffffe058, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe048)\r\n    at ../csu/libc-start.c:310\r\n#70 0x0000555555793d8a in _start ()```\r\n", "comments": ["The XID error is:\r\n\r\n```GPU 00000000:0B:00.0: Detected Critical Xid Error```", "@jostheim \r\n\r\nWill it be possible to share colab link or simple standalone code with supporting files to reproduce the issue reported here. It helps us in localizing the issue faster. Thanks!", "It is pretty hard to do that, as it involves interleaved TFrecord files totaling around 400GB, I am not sure where I would put that much data (or even a subset), it would also be a significant amount of code to put somewhere, I could probably eventually get the code in a notebook with some significant effort.  Is there anything I can do to help debug on my end?", "BTW since the use of a Keras sequence runs fine, I can\u2019t rule out some interplay between tf datasets and the issues I am seeing, so I can\u2019t really simplify too much...", "@jostheim could you please verify this in latest versions ? I could not reproduce the same.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 38698, "title": "NaN's in saved model only when training with tensorflow-gpu", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  I believe Debian 9 \"Stretch\" on google cloud and whatever distro Google colab uses.\r\n- TensorFlow installed from (source or binary): Installed as binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 and v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: Uncertain\r\n- GPU model and memory: Tesla V100 on google cloud, uncertain on colab\r\n\r\n**Describe the current behavior**\r\nI have been building off of the [tensorflow pix2pix example](https://www.tensorflow.org/tutorials/generative/pix2pix).\r\n\r\nSteps:\r\n1. Train the network and save checkpoints along the way. Training proceeds normally with reasonable losses, and intermediate outputs are look correct. \r\n\r\n2. Reload the network from a checkpoint and export the generator as an .h5: `generator.save('generator.h5', save_format=\"h5\")`.\r\n\r\n3. Reload the saved h5 network using `tf.keras.models.load_model('generator.h5')`\r\n\r\n4. If the network was trained with tensorflow-gpu, a block of weights on one layer (only part of sequential_9) will be NaN. If the network was trained with tensorflow cpu, the weights and outputs are appropriate.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the saved and loaded network to behave in the same way as the network as it is training, and the weights to not contain any nan values.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nApologies for including the full pix2pix script. I'll attempt to slim down the code later. Here is a colab notebook that contains the pix2pix script, and the weights from the `sequential_9` layer trained on tf-gpu and tf-cpu. To reproduce, change the runtime type between GPU and CPU.\r\nhttps://colab.research.google.com/drive/1im9dVf63vuldIGsnQ48sHBKHHpDVuHvG\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["It seems like this may be the same issue present in #35307. Unfortunately, I don't see a way to pass `clear_devices=True` in TF2", "Was able to reproduce the issue with [TF v2.2.0rc3](https://colab.research.google.com/gist/amahendrakar/3ea664961193083b74e8866646e7c386/38698-gpu.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3560262fb4639f439943e9a183af3f0f/38698-tf-nightly.ipynb). Please find the attached gist. Thanks!", "I'm making a little progress in troubleshooting. It looks like the issue is coming from the BatchNormalization layer in the upsample block. If I comment out the BatchNorm layer in the upsampling function (line 129 in the [linked gist](https://colab.research.google.com/drive/1im9dVf63vuldIGsnQ48sHBKHHpDVuHvG)), there are no NaNs in the saved model. ", "Isolating BatchNorm allowed me to find related issues. In this issue #29478, I noticed that the original code for generating predictions from the network uses `training=True` as an argument to the prediction. When the model is called, the network no longer produces NaN values with this argument.\r\n\r\nHowever, the issue with the NaN weights in the saved model persists. This means that the saved model cannot be used in TensorFlow JS as TFJS does not have a `training=true` function that I am aware of.", "You can clean it by hand as below. It works for me.\r\n<pre><code>\r\nmodels.load_weights(\"{path}\")\r\nmodel_refresh_without_nan(models)\r\n\r\ndef model_refresh_without_nan(models):\r\n    import numpy as np\r\n    valid_weights = []\r\n    for l in models.get_weights():\r\n        if np.isnan(l).any():\r\n            valid_weights.append(np.nan_to_num(l))\r\n            print(\"!!!!!\", l)\r\n        else:\r\n            valid_weights.append(l)\r\n    models.set_weights(valid_weights)\r\n</pre></code>", "Was able to reproduce your issue in Tensorflow GPU 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b8520107f45c3500954e2593ec047e61/38698.ipynb). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38698\">No</a>\n"]}, {"number": 38697, "title": "Some Refactoring", "body": "I performed some refactoring. Thank you!", "comments": ["@allenlavoie, Can you please take a look this PR ? Thanks!", "Looks more readable to me the way it is in the codebase now, and AFAIK our styleguide says nothing about the issue. Let's keep it the way it is."]}, {"number": 38696, "title": "Tensorflow Lite iOS opengl delegate", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMetal delegate works perfectly. But in some cases, I am forced to use opengles 3.0 as the render pipeline because of essential external libraries. It will then be great to have opengl delegate in iOS so users could bind SSBO to tensors.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\niOS developers who can't use Metal render pipeline and want high inference performance.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @brucechou1983, sorry for my late response.\r\nTo understand your question clearly, are you saying that when you are using OpenGL as the rendering pipeline in your app, you can't use Metal delegate in the same app?", "@yyoon Sorry that I didn't explain the question well. Using Metal delegate with OpenGL rendering pipeline works. The problem is we have to copy input and output data back and forth rather than exchanging the ownership of the data.\r\n\r\nAs you can see in MediaPipe [tflite inference calculator](https://github.com/google/mediapipe/blob/master/mediapipe/calculators/tflite/tflite_inference_calculator.cc#L495), memcpy() is required in every single inference. When I opened this issue, I thought if the kernel is written is OpenGL I can just bind the buffers to tensors.\r\n\r\nHowever, I realized that OpenGL kernels in TFLite are generated as the form of compute shaders, which are not supported in OpenGL ES 3.0. Since iOS devices generally have faster GPU, I could live with current solution.", "Thanks for the clarification!"]}, {"number": 38695, "title": "keras.layers.Add() uses 'tf_op_layer_add', which can not be renamed", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): \r\npip\r\n- TensorFlow version (use command below): \r\n2.1\r\n- Python version: \r\n3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nI currently have a trained model and I want to add the output of an intermediate layer with a numpy array that I have. I first converted the numpy array to a tensor using `tf.keras.backend.constant`, and then I used `tf.keras.layers.Add()` to sum them up. \r\n\r\nBut I got the error: `The name \"tf_op_layer_add\" is used 2 times in the model. All layer names should be unique`. Because there exists another add operation in the model. So I tried to name this add layer, and I got this error: `Can't set the attribute \"name\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name`.\r\n\r\nNormally, `tf.keras.layers.Add()` should be of type `<tensorflow.python.keras.layers.merge.Add>`, but in my case, it became into `<tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer>`, which does not allow me to change its name.\r\n\r\nI simply want to change the name of that `tf.keras.layers.Add()` layer, is there a way I can do that or another way around? Thanks!\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere's a way to reproduce the error. You'll see that `tf.keras.layers.Add()` uses tensorflow operation layer which can not be named.\r\n\r\n```\r\nbias = np.ones((32, 32, 3))\r\nbias = tf.keras.backend.constant(bias)\r\nbias = keras.backend.reshape(bias, [-1,32,32,3])\r\n               \r\ninputs = tf.keras.Input(shape=(32, 32, 3))\r\nx = tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3),padding = 'same')(inputs)\r\nx = tf.keras.layers.Add(name = 'add_1')([x, bias])\r\nmodel = tf.keras.Model(inputs, x)\r\nmodel.summary()\r\nadd_layer = model.get_layer('tf_op_layer_add')\r\nadd_layer.name = 'add_1'\r\n```\r\n", "comments": ["@CXYCarson \r\ncan you please refer to these links and let us know if it helps:\r\n[link1](https://stackoverflow.com/questions/53596571/error-with-keras-inceptionv3-base-model-get-layercustom-valueerror-no-suc) [link2](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\r\n\r\n", "Those two links helped, thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38695\">No</a>\n"]}, {"number": 38694, "title": "TF1 tf.image.resize_image does not work with unbatched variable size image anymore since 1.12", "body": "(Sorry for not adhering to the template, I can point to the problematic commit)\r\n\r\nI use tf.image.resize_images to resize variable sized images in an unbatched manner, like so:\r\n\r\n    tf.image.resize_images(\r\n                tf.reshape(x, [-1, 100, 3]),\r\n                size,\r\n                method=tf.image.ResizeMethod.BILINEAR\r\n            )\r\n\r\nI.e. some images are 100x100, some 110x100 etc. Upon updating tensorflow I received the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n[...]\r\n  File \"data_preprocess_amass.py\", line 32, in __init__\r\n    method=tf.image.ResizeMethod.BILINEAR\r\n  File \"/mnt/HDD1/julian/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py\", line 1187, in resize_images\r\n    skip_resize_if_same=True)\r\n  File \"/mnt/HDD1/julian/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py\", line 1053, in _resize_images_common\r\n    new_height_const = size_const_as_shape.dims[0].value\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\nI investigated it and it seems that the commit f43d458a318d4d97298710654f1692f6e8364f82 is the culprit, introducing the following lines to image_ops_impl.py:\r\n\r\n    new_height_const = size_const_as_shape.dims[0].value\r\n    new_width_const = size_const_as_shape.dims[1].value\r\n\r\nblame: https://github.com/tensorflow/tensorflow/blame/r1.15/tensorflow/python/ops/image_ops_impl.py#L1053\r\n\r\nIt is included since 1.12.1. Downgrading to 1.11 solved it for me. Might be a very special use case on a already deprecated function but I thought its worth reporting.", "comments": ["@jejay \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "No wait I'll do it, just give me some time I have a deadline end of the month.", "@jejay \r\n\r\nAny update on this issue please. Thanks!", "So its basically just the code I already presented:\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32)\r\nsize = tf.placeholder(tf.int32)\r\n\r\nresult = tf.image.resize_images(tf.reshape(x, [-1, 100, 3]), size)\r\n\r\nsession = tf.Session()\r\nr = session.run(result, {\r\n    x: np.random.normal(size=(100, 100, 3)),\r\n    size: [20, 20]\r\n})\r\n```\r\n\r\nworks on pre 1.12 and fails afterwards with:\r\n```\r\nTraceback (most recent call last):\r\n  File \"py.py\", line 10, in <module>\r\n    method=tf.image.ResizeMethod.BILINEAR)\r\n  File \"[...]/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 960, in resize_images\r\n    name=None)\r\n  File \"[...]/lib/python3.6/site-packages/tensorflow/python/ops/image_ops_impl.py\", line 1041, in resize_images_v2\r\n    new_height_const = size_const_as_shape.dims[0].value\r\nTypeError: 'NoneType' object is not subscriptable\r\n```", "I have tried in colab and i am not seeing any issue with TF version <= 1.12 and i could reproduce the issue with TF version >1.12.Please,find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f3b22183898179523f0837b637d5ce47/untitled864.ipynb).Thanks!", "The issue was that the line with the error was not v1/v2 TensorShape backwards compatible. It should be fixed at head now. I\u2019ve [verified](https://colab.research.google.com/drive/1C9_VBU-dSXnkabBLSUZWveylVQTzcfZK?usp=sharing#scrollTo=8r0M6AQoqXPK) it with tf-nightly version 2.3.0-dev20200515.\r\n\r\nOn older branches, we unfortunately need to wait until the next patch release to cherrypick it in (note: supported versions are 1.5, 2.0, 2.1, 2.2). If you need to work with TF versions >= 1.13 and <= 2.2, one interim possibility would be to build tf from source with the [two liner fix](https://github.com/UofT-EcoSystem/tensorflow/commit/24c75ce5016efb4ab107f27b96aac07549d8617b).", "@jejay  Closing this issue since it was resolved already. Please feel  free to reopen the issue in case of any concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38694\">No</a>\n"]}, {"number": 38692, "title": "warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\") google colab", "body": "I was running a code in colab notebook and  my colab crashed after some iterations and logs showed me this-\r\nwarn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\") \r\n\r\ncan anyone one help me about what happened and how can i solve this", "comments": ["@maxkaustav \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "It may be helpful to know I'm having the same issue building a CNN with PyTorch in Colab\r\n- Mac OSX 10.13.6\r\n- Torch version 1.4.0\r\n- Torch Vision version 0.5.0", "Same Issue here.Building a seq2seq model with PyTorch in Colab\r\n- Mac OSX 10.14.6 , Torch 1.5.0\r\n\r\nThe notebook keeps crashing and recovering. \r\n\r\nHere's a detailed message of the recovery crash report:\r\n\r\n`/usr/local/lib/python3.6/dist-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.`\r\n\r\nI think, the issue is likely related to mixing the CPU and GPU tensors in PyTorch. In my case, I forgot to push one tensor to GPU.\r\n The issue resolved after pushing all the tensors to GPU.", "@maxkaustav \r\n\r\nLooks like this is not Tensorflow issue. Here we address only Tensorflow bug or feature request.Please,close this thread if your issue is not related to Tensorflow. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38691, "title": "WARNING:tensorflow:Entity <function <lambda> at 0x000002343DCF24C8> could not be transformed and will be executed as-is.", "body": "What can be done to solve this warning?\r\n\r\n## Warning message\r\n\r\nBelow is the full warning:\r\n\r\n> WARNING:tensorflow:Entity <function <lambda> at 0x000002343DCF24C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\n> WARNING: Entity <function <lambda> at 0x000002343DCF24C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\n\r\n## Minimum working example\r\n\r\nThe code that generated the warning was discussed in #38471 and stored in this [gist](https://colab.research.google.com/drive/1gExuU2sCs6OeHtexI22srN7gAti9C9c4#scrollTo=8TvMWDmhlcSY).\r\n```\r\nimport tensorflow as tf\r\n\r\ndef compute_length(x): \r\n    return tf.strings.length(x)\r\n\r\ndef check_substring(x, substring):\r\n    return tf.strings.regex_full_match(x,substring)\r\n\r\n\r\ndef compute_palindrome(x):\r\n    extra_split = tf.strings.bytes_split(x)\r\n    reverse = tf.reverse(extra_split,[0])\r\n    reversedStr = tf.strings.reduce_join([reverse])\r\n    return reversedStr\r\n    \r\nds = tf.data.Dataset.from_tensor_slices([\"Ottawa\", \"Stockholm\", \"Rabat\"])\r\n\r\nds = ds.map(\r\n    lambda city: (city, \r\n                  compute_length(city), \r\n                  check_substring(city, \".*lm.*\"),\r\n                  compute_palindrome(city),\r\n                  ),\r\n        )\r\n\r\nnum_elems = len(ds.element_spec)\r\nfor elem in ds:\r\n   print(''.join([f\"{elem[i]}\" for i in range(num_elems)]))\r\n```\r\n\r\n## Environment\r\n\r\n- python 3.7.4\r\n- tensorflow-gpu 2.0.0\r\n- tensorflow-datasets 1.3.0\r\n- gast 0.2.2\r\n\r\nRunning on Windows 10 under conda 4.8.3.", "comments": ["@laghaout \r\ni ran the code shared on tf_nightly and do not find any error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/963888703a1d8740f98f59418ab1a63f/38691_nightly.ipynb) as well on [2.0](https://colab.sandbox.google.com/gist/Saduf2019/092b068dd0b449aebaf2e63034aef38a/38691_2.ipynb)\r\n\r\ncould you please refer to these issues and let us know if it helps:\r\n[link1](https://github.com/tensorflow/tensorflow/issues/35504#issuecomment-571414212)  [link2](https://github.com/tensorflow/tensorflow/issues/32383#issuecomment-535763496) [link3](https://github.com/tensorflow/tensorflow/issues/32949#issuecomment-541345618) [link4](https://github.com/tensorflow/tensorflow/issues/36093) [link5](https://github.com/tensorflow/tensorflow/issues/32319#issuecomment-597946520)", "@Saduf2019, I read the issues you linked to but I'm afraid none of them solve the problem. While the code seems to run without warnings on Colab, the warnings appear on two different machines I've tested it on (Windows 10 and Debian).\r\n\r\nThe exact versions of the packages are as follows:\r\n```\r\nuser@user-G751J ~\r\n$ conda list | grep -i gast\r\ngast                      0.2.2                    pypi_0    pypi\r\n\r\nuser@user-G751J ~\r\n$ conda list | grep -i tensorflow\r\ntensorflow-datasets       1.3.0                    pypi_0    pypi\r\ntensorflow-estimator      2.0.1                    pypi_0    pypi\r\ntensorflow-gpu            2.0.0                    pypi_0    pypi\r\ntensorflow-hub            0.6.0                    pypi_0    pypi\r\ntensorflow-metadata       0.15.0                   pypi_0    pypi\r\n```\r\n", "Please note that the problem persists even with the following upgrades:\r\n```\r\ngast                      0.3.3                    pypi_0    pypi\r\ntensorflow-estimator      2.0.1                    pypi_0    pypi\r\ntensorflow-gpu            2.1.0                    pypi_0    pypi\r\ntensorflow-gpu-estimator  2.1.0                    pypi_0    pypi\r\n```", "@laghaout Can you please test with `tf-nightly` or `TF2.2.0rc3` and let us know whether the error persists with latest versions also. Thanks!", "The following versions should work (note that pip install should pull the correct dependencies in TF 2.2 and later):\r\n\r\nTF version   | gast version\r\n-----------------|-----------------\r\n2.0, 2.1, 2.2  | 0.2.2\r\n2.3, nightly    | 0.3.3\r\n\r\nTo make sure you have the correct version installed, please try force-installing it: `pip install gast==<version> --force-reinstall`.", "@mdanatg, @jvishnuvardhan: I followed your suggestions, but just to double-check, my packages are\r\n```\r\ntensorflow-datasets       1.3.0                    pypi_0    pypi\r\ntensorflow-estimator      2.0.1                    pypi_0    pypi\r\ntensorflow-gpu            2.1.0                    pypi_0    pypi\r\ntensorflow-gpu-estimator  2.1.0                    pypi_0    pypi\r\ntensorflow-hub            0.6.0                    pypi_0    pypi\r\ntensorflow-metadata       0.15.0                   pypi_0    pypi\r\n```\r\nand\r\n```\r\ngast                      0.2.2                    pypi_0    pypi\r\n```\r\n\r\nHowever, the mystery has now thickened: The message does not show up when the code is run in Jupyter, but persists when the code is run in Spyder. In the latter case, it is only when the code is _re-run_, i.e., the warning does not show the first time it is run.\r\n\r\nFurthermore, the warning is still the same as that I posted initially but it has a slightly different wording:\r\n\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x000002E2EC2049D8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nWARNING: AutoGraph could not transform <function <lambda> at 0x000002E2EC2049D8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nb'Ottawa'6Falseb'awattO'\r\nb'Stockholm'9Trueb'mlohkcotS'\r\nb'Rabat'5Falseb'tabaR'\r\n```", "Thanks for checking. So long as you don't see any `module 'gast' has no attribute \"foo\"`, then the version of gast is correct. So we must look for a different cause.\r\n\r\nTo get a bit more information, could you set this env var: `export AUTOGRAPH_VERBOSITY=3` and replicate the warning messages? That should generate a lot of log output that will be useful to tell what's going on.", "@mdanatg, please find the log [here](https://github.com/tensorflow/tensorflow/files/4528080/log.txt).\r\n", "@laghaout Thank you. The log doesn't seem to contain any errors - did the warning appear while collecting it?", "@mdanatg I had forgotten that the warning does not show up on the first run. [Here](https://github.com/tensorflow/tensorflow/files/4528876/log.txt) it is on the second run.\r\n", "Thank you, that helped! It seems that Spyder may decide to unload modules. Will send a fix.", "Actually, this should already be fixed in tf-nightly. The fix will be released in TF 2.3.\r\n\r\nClosing the issue for now, but please re-open if you still see the warnings with tf-nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38691\">No</a>\n", "Hello,\r\nI am witnessing identical issue with a very simple command. In fact, it is one of the demonstrated usages of the API.\r\n\r\nLinux : Ubuntu 18.04\r\nI am running docker container. Inside the docker container,\r\nTensorFlow version : 1.15\r\nPython version : 3.6.9\r\ngast version : 0.2.2 ( checked with command 'pip freeze')\r\n\r\nCommand:\r\nd = tf.data.Dataset.from_tensor_slices([1, 2, 3])  ( I was looking at [this](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/Dataset#filter) example )\r\nd = d.filter(lambda x: x < 3)\r\n\r\nWARNING:tensorflow:Entity <function <lambda> at 0x7ff0f804fb70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function <lambda> at 0x7ff0f804fb70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nWARNING: Entity <function <lambda> at 0x7ff0f804fb70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function <lambda> at 0x7ff0f804fb70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n\r\nAny idea why I am seeing this problem and how to fix it? Unfortunately, I cannot verify it in TF 2.X because I cannot install it easily on this machine as it needs other dependencies to be correctly (CUDA > 11.0) installed.", "Hi @ssnirgudkar, it appears that the error message indicates an environment related incompatibility. The root cause it indicated by this piece in the message: \"Original error: could not get source code\". [This section](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code) in the docs has a few details why.\r\n\r\nGiven that the lambda function is simple, you could try to wrap it into a `tf.autograph.experimental.do_not_convert`, but it is likely to see that error for all the other functions that might be involved. That type of error will also happen if you upgrade to a newer version of TF, so I think it's worth looking at the environment instead.\r\n\r\nAre you getting this error when trying to run the code in an interactive Python or IPython shell? I know those to be incompatible. Running the code as a Python file, or using something like Jupyter should work though."]}, {"number": 38690, "title": "What is recommended system configuration for Deep Learning?", "body": "Basically I am having Acer Nitro 50 Desktop with system configuration\r\nProcessor: Intel\u00ae Core\u2122 i5-8400 CPU @ 2.80GHz \u00d7 6\r\nGraphics: GeForce GTX 1050/PCIe/SSE2 (2 GB)\r\nMemory(RAM): 8GB DDR4 Memory\r\n\r\nI am working with **tensorflow_gpu-1.12.0 | python 3.5 | GCC 4.8 | Bazel 0.15.0 | cudnn 7 | Cuda 9.0** to train a faster_rcnn_inception_v2 model on my custom dataset with 10.4 GB(65000 images) of training data and 533.4 MB(3333) of validation data for object detection for 600k epochs(num_steps it will be training at 640 x 640 resolution). I am training and validation model on 8 classes. So when I am training the model the accuracy(map) is not increasing and loss is not decreasing after a while. After the completion of the training successfully I ran the model on various images and noticed couple of things.\r\n\r\n1)    Less accuracy\r\n 2)   In some images objects does not get detected at all no bounding boxes. while in some multiclass object detection is spot on perfectly all objects gets detected but with less accuracy around 60 - 75.\r\n 3)   From second point above If I train a separate model with less images and less number of classes(3 or 4) it works well but with decent amount of accuracy around 75 - 95\r\n\r\nQuestion 1) What would be ideal system to work comfortably with deep learning? and get desired amount of results\r\n\r\nSo based on my system configuration should I keep on training or invest into new system which will be lot faster and accurate in deep learning. It would be really helpful for me to continue my learning if anyone could suggest or recommend something ", "comments": ["@spurani \r\n\r\nPlease, see hardware and software requirements from [here](https://www.tensorflow.org/install/gpu).Also, please see tested build configurations for [linux](https://www.tensorflow.org/install/source#linux), [mac](https://www.tensorflow.org/install/source#gpu_2), [windows](https://www.tensorflow.org/install/source_windows#gpu).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@spurani \r\n\r\nAny update on this issue please. Thanks!", "Sorry for my late reply I have installed tensorflow using https://www.tensorflow.org/install/gpu but the hardware requirements are kind of vague it would be helpful If I would get to know that a specific hardware is recommended to run the tensorflow models.  thanks and cheers", "@spurani \r\n\r\nPlease, go through [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements) and see if it helps you.Thanks!", "![image](https://user-images.githubusercontent.com/28580888/80699931-de7c8f80-8aaa-11ea-96b3-72775b0e1636.png)\r\n![image](https://user-images.githubusercontent.com/28580888/80700150-33b8a100-8aab-11ea-9463-00f61632015a.png)\r\n\r\n1) I am training my model on MS COCO dataset using pretrained faster_rcnn_inception_v2_coco in tensorflow. As the images in MS COCO dataset does not have same size do I need to resize before training change the image_resizer function in faster_rcnn_inception_v2_coco.config file(below)?\r\n\r\n2) Bascially I am trying to work on Objection Detection using the https://github.com/tensorflow/models/tree/master/research/object_detection API but it seems even though I have followed the steps to make recommended changes in config file for faster_rcnn_inception_v2_coco my model is not getting trained May please get some assistance? Cause the faster_rcnn_inception_v2_coco have been trained on 90 coco classes images on same configuration\r\n\r\nSo this are the images of learning rate and total loss which clearly indicates that my model is not training efficiently. It would be really helpful if I could get some advice on how should I make it work?\r\n\r\n Here is what I am using to train the model faster_rcnn_inception_v2_coco.config\r\n````\r\n# Faster R-CNN with Inception v2, configuration for MSCOCO Dataset.\r\n# Users should configure the fine_tune_checkpoint field in the train config as\r\n# well as the label_map_path and input_path fields in the train_input_reader and\r\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n# should be configured.\r\nmodel {\r\n  faster_rcnn {\r\n    num_classes: 8\r\n#Default configuration for image_resizer no changes made in this function\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 600\r\n        max_dimension: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_inception_v2'\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 300\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 1\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.0002\r\n          schedule {\r\n            step: 900000\r\n            learning_rate: .00002\r\n          }\r\n          schedule {\r\n            step: 1200000\r\n            learning_rate: .000002\r\n          }\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"models/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  # Note: The below line limits the training process to 200K steps, which we\r\n  # empirically found to be sufficient enough to train the COCO dataset. This\r\n  # effectively bypasses the learning rate schedule (the learning rate will\r\n  # never decay). Remove the below line to train indefinitely.\r\n  num_steps: 200000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"data/dataset_tools_train.tfrecord\"\r\n  }\r\n  label_map_path: \"data/label.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 3300\r\n  # Note: The below line limits the evaluation process to 10 evaluations.\r\n  # Remove the below line to evaluate indefinitely.\r\n  max_evals: 10\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"data/dataset_tools_val.tfrecord\"\r\n  }\r\n  label_map_path: \"data/dataset_tools_val.tfrecord\"\r\n  shuffle: false\r\n  num_readers: 1\r\n} \r\n```", "@spurani There might be many factors that might be impacting your accuracy. As answered in this **[question](https://stackoverflow.com/questions/35293468/does-reducing-classes-in-a-classification-method-improve-accuracy), having less number of classes will increase your accuracy and having more number of classes will most likely decrease your accuracy. This doesn't mean you have to change the system configuration to get good results. \r\nAlso you can refer to the following question [here](https://stats.stackexchange.com/questions/348584/do-more-object-classes-increase-or-decrease-the-accuracy-of-object-detection) to understand more about your problem \r\n\r\nAlso I would recommend you to post your issue [here](https://github.com/tensorflow/models/issues) and also in stackoverflow where there is a wider community to respond. Thanks!", "Thank you @gowthamkpr  for those references those might come handy but they are pretty hard to understand if you come across any tutorial where is model is trained on multiple categories and huge amount of dataset can you please forward it? As the images in MS COCO dataset does not have same size do I need to resize before training change the image_resizer function in faster_rcnn_inception_v2_coco.config file(above in previous question)? How they would have trained a model on 90 different categories with huge dataset? It would be great if anyone can guide me through this it will reaally help me continue my learning on machine learning thank you", "@spurani I will do that and also As mentioned in the above comment, please post this issue in tensorflow/models or  stackoverflow as there is a wider community to respond. Thanks!"]}, {"number": 38689, "title": "keras with tensorflow backend is 4x slower than normal keras on GPU machines", "body": "Same model written in tf.keras runs much slower compared to keras imported directly.\r\n\r\nSystem information (whatever a default GPU session in Colab is):\r\n- Linux Ubuntu 16.04\r\n- Tensorflow 2.2, Keras 2.3.1\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla K80\r\n\r\nCurrent behavior. Two identical models defined with **tensorflow.keras** and **keras** have similar training time in CPU session (the code for models is identical). However, when GPU is available tensorflow.keras model doesn't get any acceleration whereas keras model performs 4x faster\r\n\r\n```\r\n# returns True\r\ntf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None)\r\n```\r\n\r\nExpected behavior. Both models must take advantage of GPU acceleration.\r\n\r\nThe model below attempts to find an embedding with vectors of unit L2 norm. It takes about 2 minutes / epoch to run on CPU. With GPU, tf.keras model runs in about the same time, normal keras model runs in about 25 seconds.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nvocab_size = 5000\r\nsample_size = 300000\r\nembedding_size = 300\r\n\r\ndata = np.random.randint(0, vocab_size, sample_size)\r\ny_true = np.ones(sample_size, dtype = np.float32)\r\n\r\n# Using tensorflow keras backend\r\nfrom tensorflow.keras.layers import Input, Embedding, Dot, Reshape\r\nfrom tensorflow.keras.models import Model\r\n\r\n# define the model\r\ninput_layer = Input((1,))\r\nembed_layer = Embedding(vocab_size\r\n                              , output_dim = embedding_size\r\n                              , embeddings_initializer='normal')\r\ndot_layer = Dot(axes = -1)\r\n\r\nembedded = embed_layer(input_layer)\r\ndotted = dot_layer([embedded, embedded])\r\nout = Reshape((1,))(dotted)\r\n\r\nmodel = Model(inputs = [input_layer], outputs = out)\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss = tf.keras.losses.MeanSquaredError())\r\nmodel.fit([data], y_true, epochs = 1, verbose = True)\r\n```\r\n\r\nTo switch to normal keras backend replace the two keras import statements with:\r\n```\r\nfrom keras.layers import Input, Embedding, Dot, Reshape\r\nfrom keras.models import Model\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue. tf.keras takes 111s - 122s for one epoch whereas keras takes 25s. Please find the attached gist for [TF v2.1](https://colab.research.google.com/gist/amahendrakar/74a47571985598cab190db7cc82e7ffd/38689-tf.ipynb#scrollTo=aa941XE8pWwk), [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/4ac26feaac5520d1f747989bd7f47dc9/38689-tf.ipynb#scrollTo=aa941XE8pWwk), [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3272f5b327157994845add33ae0866b8/38689-tf-nightly.ipynb) and [keras](https://colab.research.google.com/gist/amahendrakar/42eb0fadb346a13115b0bbddefa5963d/38689-keras.ipynb#scrollTo=pcOLKzFkqG4r). Thanks!", "@Flomastruk I think you need to provide `batch_size` to `model.fit`. Otherwise, by default `model.fit` considers batch_size as 32. When i added `batch_size=128`, your code took around 31 seconds. You could even increase it to 256 (took 15.5 seconds). Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2c4d061675286525c6aeb06e04f66ce6/untitled104.ipynb). \r\n\r\nIn Tensorflow, you can improve the performance even more by placing some ops and data on GPU or CPU.  Please check the resource [here](https://www.tensorflow.org/guide/gpu).Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Thank you, @jvishnuvardhan, this works well", "@jvishnuvardhan, seems like I am experiencing the same issue, even after specifying `batch_size` parameter to `model_fit`. Native keras backend seems to give 2.5x faster training time versus tensorflow.keras backend.\r\n\r\nBelow is a code for a basic model and simulating random data. With the settings of basic colab GPU session (Tesla P4, rest from specs in the first post of the thread) it takes 16 sec/epoch using tensorflow.keras and 6sec/epoch using native keras backend:\r\n```\r\n## this is fast\r\nfrom keras.layers import Input, Embedding, Dot, Reshape, Add\r\nfrom keras.models import Model\r\nfrom keras.optimizers import Adam\r\nfrom keras.losses import MeanSquaredError\r\n\r\n## this is slow\r\n# from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add\r\n# from tensorflow.keras.models import Model\r\n# from tensorflow.keras.optimizers import Adam\r\n# from tensorflow.keras.losses import MeanSquaredError\r\n\r\nemb_dim = 200\r\nbatch_size = 2**15\r\nvocab_size = 10**4\r\nsample_size = 5*10**6\r\ndataset_key = np.random.choice(vocab_size, size = 2*sample_size).astype(np.int32).reshape(-1,2)\r\ndataset_val = np.random.uniform(size = sample_size).astype(np.float32)\r\n\r\ninput_target = Input((1,))\r\ninput_context = Input((1,))\r\n\r\ntarget_embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=1, name='target_embedding')\r\ncontext_embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=1, name='context_embedding')\r\nw_target = target_embedding(input_target)\r\nw_context = context_embedding(input_context)\r\ndot_product = Dot(axes=-1, name='dot_product')([w_target, w_context])\r\ndot_product = Reshape((1,))(dot_product)\r\n\r\nmodel = Model(inputs=[input_target, input_context], outputs=dot_product)\r\nmodel.compile(loss = MeanSquaredError(), optimizer = Adam(1e-3))\r\nmodel.fit([dataset_key[:,0], dataset_key[:,1]], dataset_val, batch_size = batch_size, epochs = 5, verbose=1)\r\n```", "@Flomastruk Can you please check this with `tf-nightly`. I ran your code with `tf.keras` which took 11.4 seconds and `keras` took 23.7 seconds.\r\nPlease take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/4558bf318d8fb4b5e0de232d556f1805/untitled192.ipynb) with `tf.keras`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/33383419b48f664af3a9ca7a1ea011ce/untitled193.ipynb) is the gist with `keras`.\r\n\r\n```\r\ntf.keras\r\nTime Taken:  11.416040420532227\r\nKeras \r\nTime Taken:  23.71488118171692\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan, you are right as usual. Looks like the latest `tf-nightly` performance is actually even faster than `keras`. I get similar times. Thanks again!", "I feel that this stage is opportunity to after python installs tensorflow.\nRunning tensorflow opens replika window and desktop and laptop. That is how\nwe can solve this problem.\n\nOn Sat, Jun 6, 2020, 5:50 AM Flomastruk <notifications@github.com> wrote:\n\n> Closed #38689 <https://github.com/tensorflow/tensorflow/issues/38689>.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38689#event-3414096512>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525QMIHRC34AFRAQKSCDRVG4GTANCNFSM4ML35OFQ>\n> .\n>\n", "They are analytical and balanced. We just need simple local developer\ninstall\n\nOn Sat, Jun 6, 2020, 6:06 AM Marko Radoj\u010di\u0107 <marko.p.radojcic@gmail.com>\nwrote:\n\n> I feel that this stage is opportunity to after python installs tensorflow.\n> Running tensorflow opens replika window and desktop and laptop. That is how\n> we can solve this problem.\n>\n> On Sat, Jun 6, 2020, 5:50 AM Flomastruk <notifications@github.com> wrote:\n>\n>> Closed #38689 <https://github.com/tensorflow/tensorflow/issues/38689>.\n>>\n>> \u2014\n>> You are receiving this because you are subscribed to this thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/38689#event-3414096512>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AIT525QMIHRC34AFRAQKSCDRVG4GTANCNFSM4ML35OFQ>\n>> .\n>>\n>\n", "Also just need then mobile authenticator for payment\n\nOn Sat, Jun 6, 2020, 6:08 AM Marko Radoj\u010di\u0107 <marko.p.radojcic@gmail.com>\nwrote:\n\n> They are analytical and balanced. We just need simple local developer\n> install\n>\n> On Sat, Jun 6, 2020, 6:06 AM Marko Radoj\u010di\u0107 <marko.p.radojcic@gmail.com>\n> wrote:\n>\n>> I feel that this stage is opportunity to after python installs\n>> tensorflow. Running tensorflow opens replika window and desktop and laptop.\n>> That is how we can solve this problem.\n>>\n>> On Sat, Jun 6, 2020, 5:50 AM Flomastruk <notifications@github.com> wrote:\n>>\n>>> Closed #38689 <https://github.com/tensorflow/tensorflow/issues/38689>.\n>>>\n>>> \u2014\n>>> You are receiving this because you are subscribed to this thread.\n>>> Reply to this email directly, view it on GitHub\n>>> <https://github.com/tensorflow/tensorflow/issues/38689#event-3414096512>,\n>>> or unsubscribe\n>>> <https://github.com/notifications/unsubscribe-auth/AIT525QMIHRC34AFRAQKSCDRVG4GTANCNFSM4ML35OFQ>\n>>> .\n>>>\n>>\n"]}, {"number": 38688, "title": "I've got an error for \"Build a handwritten digit classifier app with TensorFlow Lite\"", "body": "I have done all the same way, shown in the official website.\r\n\r\nBut, I have got an error message as below.\r\n\r\n**_Only safe (?.) or non-null asserted (!!.) calls are allowed on a nullable receiver of type Tensor?_**\r\n\r\nI could not find any solution through web surfing in github and others.\r\n\r\nCould you help me, please?\r\n\r\nRegards,", "comments": ["@aslankims \r\nPlease provide a code snippet to reproduce the issue reported here. \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Dear Saduf2019,\r\n\r\nMy platform is given as,\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  as described in \"https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#3.\". coding in Android Studio 3.6.3\r\nThe line encountered the error is \" val inputShape = interpreter?.getInputTensor(0).shape() \"\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Galaxy. But error occurs in \"build: step in Android Studio\r\n- **TensorFlow installed from (source or binary)**: The TFlite model was built in colab online\r\n- **TensorFlow version (use command below)**: TF2.0 \r\n- **Python version**: 3.7\r\n.\r\n.\r\n- **Exact command to reproduce**: Only safe (?.) or non-null asserted (!!.) calls are allowed on a nullable receiver of type Tensor?\r\n\r\nThanks,", "@aslankims\r\nPlease provide a simple code snippet to reproduce the issue reported here.", "The code in Android Studio is,\r\n\r\n\r\n  @Throws(IOException::class)\r\n  private fun initializeInterpreter() {\r\n    // TODO: Load the TF Lite model from file and initialize an interpreter.\r\n\r\n    val assetManager = context.assets\r\n    val model = loadModelFile(assetManager, \"mnist.tflite\")\r\n\r\n    // TODO: Read the model input shape from model file.\r\n\r\n    _val inputShape = interpreter?.getInputTensor(0).shape()_\r\n    inputImageWidth = inputShape[1]\r\n    inputImageHeight = inputShape[2]\r\n    modelInputSize = FLOAT_TYPE_SIZE * inputImageWidth * inputImageHeight * PIXEL_SIZE\r\n\r\n    // Finish interpreter initialization\r\n    this.interpreter = interpreter\r\n\r\n    isInitialized = true\r\n    Log.d(TAG, \"Initialized TFLite interpreter.\")\r\n  }\r\n\r\nThe error that,\r\n\"Only safe (?.) or non-null asserted (!!.) calls are allowed on a nullable receiver of type Tensor?\"\r\noccurs on the line of     _val inputShape = interpreter?.getInputTensor(0).shape()_\r\n\r\nThanks", "Hi @aslankims, please check the full version of demo: \r\n\r\nhttps://github.com/tensorflow/examples/blob/4d3a5d2b3a8fa9eeba9789544b10aaa1db3b71ac/lite/codelabs/digit_classifier/android/finish/app/src/main/java/org/tensorflow/lite/codelabs/digitclassifier/DigitClassifier.kt#L69", "As this app has been built and tested in our regular testing pipeline, the doc https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#3. is as same as the app, I'll close the bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38688\">No</a>\n"]}, {"number": 38687, "title": "Some refacotoring", "body": "Improved readability of code and remove code duplication. Thank you.", "comments": ["@marload Can you please check build failures. Thanks!"]}, {"number": 38686, "title": "Feed 1 frame to 2 models in tensorflow", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubnutu 16.04\r\n- TensorFlow version (use command below): tensorflow-gpu\r\n- Python version: 35\r\n- CUDA/cuDNN version: 9 \r\n- GPU model and memory: Titan\r\n\r\nTf version \" tensorflow-gpu=1.12)\r\n\r\nI have two tensorflow models (A.pb , B.pb ). I have two written two python codes. Both of these take same frame Input and produce output. (Currently, I run both of them in different terminals.)\r\n\r\nSince they take same image frame as inputs... Can I do something in tensorflow like...\r\n\r\nIn 1 single python file :\r\n\r\n    Put the both codes in 1 single file.\r\n    Open both A.pb and B.pb models.\r\n    Feed the same input image frame two both A.pb and B.pb parallely\r\n    Superimpose the output of A.pb & B.pb and produce 1 single output.\r\n\r\nKindly help me in this regard. :)\r\n", "comments": ["@bosssaurav \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "pk"]}, {"number": 38685, "title": "libcuda.so.1 is missing with conda env tf-gpu", "body": "I get the following errors:\r\n\r\n```\r\n2020-04-19 05:56:24.050072: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-04-19 05:56:24.050125: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-04-19 05:56:24.050149: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n2020-04-19 05:56:24.050476: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-04-19 05:56:24.066026: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199900000 Hz\r\n2020-04-19 05:56:24.068548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558e75a5b2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-19 05:56:24.068591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\n```\r\nwhen I execute\r\n```\r\n trainds = tf.data.Dataset.from_tensor_slices((helper.trainx,helper.trainm,helper.trainy)).batch(setup['BatchSize'])\r\n```\r\nTensorflow cannot find the file libcuda.so.1.\r\n\r\nI get this error in a Docker Container which uses miniconda and the tensorflow-gpu environment.\r\n```\r\nFROM continuumio/miniconda3\r\nRUN conda create -n tf-gpu tensorflow-gpu\r\n```\r\nI don't get the error on my local machine with the same code.\r\n\r\nAs far as I understood, conda should automatically install cuda and its libraries.\r\n\r\nWhen I search for the missing file I get an empty line. `find -name \"libcuda.so.1\"`\r\nOther versions of libcuda.so. can't be found either. Even `*cuda*` couldn't be found. It seems to be that cuda was entirely skipped during the installation. \r\n\r\nnvidia-smi returns this:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: N/A      |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-SXM2...  Off  | 00000000:89:00.0 Off |                    0 |\r\n| N/A   30C    P0    31W / 300W |      0MiB / 16280MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n**System information**\r\n```\r\nPython 3.7.6 | packaged by conda-forge | (default, Mar 23 2020, 23:03:20)\r\n[GCC 7.3.0] on linux\r\n```\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.6\r\n- GPU model and memory: P100 / 16 GB\r\n\r\nAdditional information:\r\n```\r\nprint(tf.test.is_gpu_available())             # False\r\nprint(tf.config.list_physical_devices('GPU')) # []\r\n```\r\n\r\n\r\n", "comments": ["This issue is more suitable for anaconda-issues repo. Please post it on anaconda-issues repo from [here.](https://github.com/ContinuumIO/anaconda-issues/issues)Thanks!", "https://github.com/ContinuumIO/anaconda-issues/issues/11743", "@Arktius \r\n\r\nCan we track with that issue  in ContinuumIO/anaconda-issues#11743\r\n\r\nand close this one. It will help us to follow easily. Please let us know. Thanks!", "I've solved the problem by setting system variables to recognize the GPU. The error was actually \r\n`2020-04-19 05:56:24.050125: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)` .\r\n\r\nIn the dockerfile:\r\n`ENV NVIDIA_VISIBLE_DEVICES all`\r\n`ENV NVIDIA_DRIVER_CAPABILITIES compute,utility`\r\n[NVIDIA](https://github.com/NVIDIA/nvidia-docker/wiki/Usage)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38685\">No</a>\n", "I am having a very similar problem on Ubuntu 20.04/conda/jupyter with Julia accessing the GPU.\r\nI can access the library from the raw repl, but not from the IJUlia kernel within jupyter installed using conda.\r\n\r\nsee [gpu-workshop-notebook-could-not-load-library-libcuda-so-1](https://discourse.julialang.org/t/juliacon21-gpu-workshop-notebook-could-not-load-library-libcuda-so-1/65014/11?u=banksiaboy) on discourse.julia.lang.org"]}, {"number": 38684, "title": "Train simple audio recognition model", "body": "@petewarden \r\nI trained this model in colab and it is working in arduino for \"yes\" and \"no\" command but when I tried for \"on\" and \"off\" I am not getting output in my arduino board( I have done the necessary changes in tensorflow example code \"micro_speech\" ).\r\n Can you please suggest how to get model working for \"on\" and \"off\".\r\n", "comments": ["@rajaniyadav,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "You can also share the your Colab gist with us. Select 'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!", "I have train the model in colab link https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model.ipynb\r\n\r\nwith modification -  os.environ[\"WANTED_WORDS\"] = \"on,off\"\r\ntensorflow version -1.15\r\n\r\ngithub gist https://gist.github.com/rajaniyadav/d031321012472f41d993aa59822dfa47\r\n\r\n\r\n", "\r\nWhy the below link is no longer accessible...\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train_speech_model.ipynb", "Here is the updated link! https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\r\n\r\nRefer to the new training document here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/train", "Closing the issue due to inactivity. @rajaniyadav, you can re-open this issue if you're still blocked. "]}, {"number": 38683, "title": "NotImplementedError: ", "body": "i am using the code below : \r\n\r\n```\r\ndef build_model(transformer, loss='binary_crossentropy', max_len=512):\r\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n    sequence_output = transformer(input_word_ids)[0]\r\n    cls_token = sequence_output[:, 0, :]\r\n    x = tf.keras.layers.Dropout(0.3)(cls_token)\r\n    out = Dense(1, activation='sigmoid')(x)\r\n    \r\n    model = Model(inputs=input_word_ids, outputs=out)\r\n    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])\r\n    \r\n    return model\r\n\r\n\r\n%%time\r\nwith strategy.scope():\r\n    transformer_layer = transformers.TFXLMRobertaModel.from_pretrained(MODEL)\r\n    model = build_model(transformer_layer,loss='binary_crossentropy', max_len=maxlen)\r\nmodel.summary()\r\n\r\n\r\ndef build_lrfn(lr_start=0.000001, lr_max=0.000002, \r\n               lr_min=0.0000001, lr_rampup_epochs=7, \r\n               lr_sustain_epochs=0, lr_exp_decay=.87):\r\n    lr_max = lr_max * strategy.num_replicas_in_sync\r\n\r\n    def lrfn(epoch):\r\n        if epoch < lr_rampup_epochs:\r\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\r\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\r\n            lr = lr_max\r\n        else:\r\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\r\n        return lr\r\n    \r\n    return lrfn\r\n\r\n\r\nmodel_path = 'jigsawMultilingual.hdf5'\r\nmodel_path1 = '/kaggle/working/jigsawMultilingual.hdf5'\r\n\r\n\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\r\n\r\ncheckpoint = ModelCheckpoint(model_path, monitor='val_loss', mode='min', save_best_only=True)\r\nes = EarlyStopping(monitor='val_loss', mode='min', patience=2, \r\n                   restore_best_weights=True, verbose=1)\r\nlr_callback = LearningRateScheduler(lrfn, verbose=1)\r\n\r\ncallback_list = [checkpoint, es, lr_callback]\r\n\r\n\r\n%%time\r\nN_STEPS = x_train.shape[0] // BATCH_SIZE\r\nEPOCHS = 2\r\ntrain_history = model.fit(\r\n    train_dataset,\r\n    steps_per_epoch=N_STEPS,\r\n    validation_data=valid_dataset,\r\n    callbacks=callback_list,\r\n    epochs=EPOCHS\r\n)\r\n```\r\n\r\nand i am getting NotImplementedError after 1st epoch,here is the notebook : https://www.kaggle.com/mobassir/understanding-cross-lingual-models?scriptVersionId=32280074\r\n\r\ni am unable to save checkpoint, for me  monitor='val_loss' throwing me NotImplementedError\r\ni tried monitor='val_acc'  and monitor='val_accuracy'  and none of them saving weights for me,where am i making mistakes?", "comments": ["In order to expedite the trouble-shooting process, could you please provide a minimal code sample to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "Please fill in issue template and try to minimize the code", "Full code here :\nhttps://www.kaggle.com/mobassir/understanding-cross-lingual-models\n\nFrom cell 8 till end of the jupyter notebook\n\nOn Mon, Apr 20, 2020, 11:58 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> Please fill in issue template and try to minimize the code\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38683#issuecomment-616716957>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AF2OWKG3S2C5LY6W2FHWQGDRNSEKPANCNFSM4MLXA7GQ>\n> .\n>\n", "It seems that is not possible to save the complete transformer model, this might work if you add \"save_weights_only=True\", and save only the weights from the model.", "@mobassir94,\r\nCould you please try sharing a minimal reproducible code, I am constantly facing out of memory error on running the code.\r\n\r\nAlso, please check @dimitreOliveira's comment and let us know if it helps? Thanks!", "@amahendrakar i don't really understand what do you  mean by sharing minimal reproducible code? you have full code here : https://www.kaggle.com/mobassir/understanding-cross-lingual-models after 8th cell,till end\r\ni am seeking answer for saving full weight in this issue.\r\n\r\nmaybe for minimal reproducible you only need this part? : \r\n\r\n```\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\r\n\r\ncheckpoint = ModelCheckpoint(model_path, monitor='val_loss', mode='min', save_best_only=True)\r\nes = EarlyStopping(monitor='val_loss', mode='min', patience=2, \r\n                   restore_best_weights=True, verbose=1)\r\nlr_callback = LearningRateScheduler(lrfn, verbose=1)\r\n\r\ncallback_list = [checkpoint, es, lr_callback]\r\n\r\nN_STEPS = x_train.shape[0] // BATCH_SIZE\r\nEPOCHS = 2\r\ntrain_history = model.fit(\r\n    train_dataset,\r\n    steps_per_epoch=N_STEPS,\r\n    validation_data=valid_dataset,\r\n    callbacks=callback_list,\r\n    epochs=EPOCHS\r\n)\r\n```", "@mobassir94 Minimal reproducible code is code that can be run independently (i.e., does not require datasets, does not require running a jupyter notebook, etc.) and shows the error. No line can be removed anymore from that code while still showing the same error.\r\n\r\nCompare\r\n\r\n```python\r\ndef not_minimal(x, y):\r\n  z = x + y\r\n  t  = 2 * z\r\n  if x < y:\r\n    raise Exception(\"Bug here\")\r\n\r\nnot_minimal(0, 42)\r\n```\r\n\r\nwith\r\n\r\n```python\r\ndef minimal(x, y):\r\n  if x < y:\r\n    raise Exception(\"Bug here\")\r\n\r\nminimal(0, 42)\r\n```\r\n\r\nOf course, you don't edit TF's code, but the code you are running should be minimized.\r\n\r\nWhy do we ask for minimized code? So that we can use tooling to quickly factor in where the change comes from and to create regression test once issue is fixed to prevent it from happening again.", "@mihaimaruseac  sorry i don't know how to minimize that code of mine without using dataset or without running at all,because the error i get is it comes after running on a dataset for exactly  1 epoch,error comes after 1 epoch and i am here asked to minimize the code that can run without data and notebook,thanks for your immense support and help", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 38682, "title": "New train_op for estimator in tensorflow", "body": "I would like to write a novel train_op to test a new training algorithm without using any gradient or loss, which is a little bit like a UCB algorithm in bandit. How can I define such a train_op to add it in my custom estimator? \r\n\r\nThank you very much!\r\n\r\n", "comments": ["@bmdy \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "> @bmdy\r\n> This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nCan you give some initial guidance?", "please refer to these lonks for assistance.\r\n\r\n[link1](https://towardsdatascience.com/an-advanced-example-of-tensorflow-estimators-part-1-3-c9ffba3bff03)\r\n\r\n[link2](https://kite.com/python/docs/tensorflow.contrib.tpu.TPUEstimatorSpec.train_op)\r\n\r\n[link3](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec)\r\n\r\n[link4](https://github.com/tensorflow/tensorflow/issues/22433)\r\n\r\ni see you have already created an issue on stackoverflow, hence moving this to closed status.", "> please refer to these lonks for assistance.\r\n> \r\n> [link1](https://towardsdatascience.com/an-advanced-example-of-tensorflow-estimators-part-1-3-c9ffba3bff03)\r\n> \r\n> [link2](https://kite.com/python/docs/tensorflow.contrib.tpu.TPUEstimatorSpec.train_op)\r\n> \r\n> [link3](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec)\r\n> \r\n> [link4](https://github.com/tensorflow/tensorflow/issues/22433)\r\n> \r\n> i see you have already created an issue on stackoverflow, hence moving this to closed status.\r\n\r\nThank you very much!"]}, {"number": 38681, "title": "Can I use other Microcontrollers other than which are specified by tensorflow?", "body": "Can other Microcontrollers can be used other than these mentioned below for tensorflow modules-\r\nArduino Nano 33 BLE Sense\r\nSparkFun Edge\r\nSTM32F746 Discovery kit\r\nAdafruit EdgeBadge\r\nAdafruit TensorFlow Lite for Microcontrollers Kit \r\nAdafruit Circuit Playground Bluefruit\r\nEspressif ESP32-DevKitC \r\nEspressif ESP-EYE \r\nIf yes can. Is there any documentation to follow?", "comments": ["@ms-ims \r\n\r\nCan you please refer the [link](https://www.tensorflow.org/lite/microcontrollers) and see if it helps you.As per Tensorflow official documentation the TF modules supports the microcontrollers  as mentioned in the link. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ms-ims \r\nAny update on this issue please. Thanks!", "I looked the link but tensorflow library is very supportive to Arm microcontroller I suppose.\r\nI trying to run hello world program of tensorflow on my RX series board of renesas. I'm finding it difficult to build on that.\r\n", "Hi @ms-ims ! Have you checked these threads from Renseas? Link[ 1](https://www.renesas.com/us/en/document/apn/ra6t1-motor-failure-detection-example-tensorflow-lite-microcontroller-application-notes?language=en),[ 2](https://www.renesas.com/eu/en/blogs/embedded-artificial-intelligence-inference-renesas-rzg2) .Thanks!", "Hi,\n\nNo I'm working on tensor flow now days.\n\nOn Tue, 4 Jan, 2022, 9:41 am mohantym, ***@***.***> wrote:\n\n> Hi @ms-ims <https://github.com/ms-ims> ! Have you checked these threads\n> from Renseas? Link 1\n> <https://www.renesas.com/us/en/document/apn/ra6t1-motor-failure-detection-example-tensorflow-lite-microcontroller-application-notes?language=en>\n> , 2\n> <https://www.renesas.com/eu/en/blogs/embedded-artificial-intelligence-inference-renesas-rzg2>\n> .Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38681#issuecomment-1004515806>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APBFADIQI6ICD2CLM5ENSW3UUJXQTANCNFSM4MLWWUSA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "@ms-ims ! Above threads suggested usage of Tensorflow Lite on Renesas RA6T1 Motor Starter Kit.  Is this issue good to close then?", "Yeah it can be closed\n\nOn Tue, 4 Jan, 2022, 10:08 am mohantym, ***@***.***> wrote:\n\n> @ms-ims <https://github.com/ms-ims> ! Above threads suggested usage of\n> Tensorflow Lite on Renesas RA6T1 Motor Starter Kit. Is this issue good to\n> close then?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38681#issuecomment-1004524013>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APBFADJCJSFAAAKAQX7H2BLUUJ2S5ANCNFSM4MLWWUSA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "Thank you confirming the same! Closing this issue as it seems to be resolved from above [comment](https://github.com/tensorflow/tensorflow/issues/38681#issuecomment-1004524327).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38681\">No</a>\n", "I subscribed to this thinking that we might get a link to some instructions for a pair of general boards with ESP32 and ARM MCUs.\r\nBut the issue was closed because we got instructions for a particular board, the RA6T1 motor controller board. This doesn't really say how to make tensorflow work on anything other than their board. It doesn't even list the steps they took to get tensorflow working on their board -- other than that they used yocto (and they didn't even explain why they did that either).\r\n\r\nShould I open a new issue to learn more?"]}, {"number": 38680, "title": "Why my modelcheckpoint is not saving weights based on best val_accuracy?", "body": "the code i tried in kaggle kernel for saving weights is this :\r\n\r\n`model_path = '/kaggle/working/jigsawMultilingual.h5'\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\r\n\r\ncheckpoint = ModelCheckpoint(model_path, monitor='val_accuracy', mode='max', save_best_only=True)\r\nes = EarlyStopping(monitor='val_accuracy', mode='max', patience=2, \r\n                   restore_best_weights=True, verbose=1)\r\nlr_callback = LearningRateScheduler(lrfn, verbose=1)\r\n\r\ncallback_list = [checkpoint, es, lr_callback]\r\n\r\n\r\n%%time\r\nN_STEPS = x_train.shape[0] // BATCH_SIZE\r\nEPOCHS = 4\r\ntrain_history = model.fit(\r\n    train_dataset,\r\n    steps_per_epoch=N_STEPS,\r\n    validation_data=valid_dataset,\r\n    callbacks=callback_list,\r\n    epochs=EPOCHS\r\n)\r\n\r\n # have a look at the training results : \r\n\r\nTrain for 4414 steps, validate for 63 steps\r\n\r\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\r\nEpoch 1/4\r\n4414/4414 [==============================] - 2301s 521ms/step - loss: 0.3190 - auc: 0.9200 - val_loss: 0.2821 - val_auc: 0.9171\r\n\r\nEpoch 00002: LearningRateScheduler reducing learning rate to 3.142857142857143e-06.\r\nEpoch 2/4\r\n4414/4414 [==============================] - 2090s 473ms/step - loss: 0.1909 - auc: 0.9718 - val_loss: 0.2581 - val_auc: 0.9249\r\n\r\nEpoch 00003: LearningRateScheduler reducing learning rate to 5.285714285714285e-06.\r\nEpoch 3/4\r\n4414/4414 [==============================] - 2090s 473ms/step - loss: 0.1639 - auc: 0.9791 - val_loss: 0.2730 - val_auc: 0.9214\r\n\r\nEpoch 00004: LearningRateScheduler reducing learning rate to 7.4285714285714275e-06.\r\nEpoch 4/4\r\n4414/4414 [==============================] - 2090s 473ms/step - loss: 0.1491 - auc: 0.9826 - val_loss: 0.2761 - val_auc: 0.9252\r\nCPU times: user 9min 46s, sys: 35.5 s, total: 10min 21s\r\nWall time: 2h 22min 50s`\r\n\r\nbut i don't see any model in this path : model_path = '/kaggle/working/jigsawMultilingual.h5'\r\n\r\nnote that i also tried \"checkpoint = ModelCheckpoint(model_path, monitor='val_acc', mode='max', save_best_only=True)\" and it also didn't work\r\n\r\ni mean jigsawMultilingual.h5 is not saving in '/kaggle/working/' directory\r\n\r\nis it happening because validation accuracy is improving every time? but still it should save the model after 1st epoch,right?", "comments": ["@mobassir94,\r\nWhile running the code, I'm facing an error stating `NameError: name 'lrfn' is not defined`.\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. \r\n\r\nAlso, please add  ```  (i.e. three backticks) to your code to preserve the indentation and formatting. Thanks!", "Full code here : https://www.kaggle.com/mobassir/understanding-cross-lingual-models\r\n\r\nFrom cell 8 till end of the jupyter notebook ", "@mobassir94,\r\nIs this a duplicate of issue [#38683](https://github.com/tensorflow/tensorflow/issues/38683)?\r\nIn this case, could you please close this issue as it is already being tracked there. Thanks!"]}, {"number": 38679, "title": "Create test", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38679) for more info**.\n\n<!-- need_sender_cla -->", "@muntasir2000 Thank you for your contribution. Can you please sign CLA? Thanks!", "not a valid PR, closing this."]}, {"number": 38678, "title": "Could we have more helpful error messages?", "body": "## Description of issue (what needs changing):\r\nTensorflow gives many errors, and most of them aren't very helpful.  Something like \"module tensorflow has no attribute reset_graph.\"  Can we change the error messages so they are more constructive?  In this situation, the issue was partially solved by downgrading to tensorflow 1.12.  It would be helpful if instead of the \"reset_graph\" error message, we could get a message more like: \"this version of tensorflow is incompatible with the current project.  Please downgrade to tensorflow 1.12 using: pip install tensorflow==1.12\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\nTo keep from tearing their own hair out.\r\n\r\n\r\n", "comments": ["Hi @Tylersuard,\r\n\r\nWe understand that TensorFlow errors can be opaque at times. We are working on it, in general.\r\n\r\nBut this specific error is a hard one to fix. It's a python AttributeError, and there's no way to fix it without converting TensorFlow modules  to some custom class with different errors depending on which non-existent attribute the user looks up... which would be messy and slow. So I don't think we can fix this.\r\n\r\nWe've been advertising for over a year that TensorFlow 2 would not be compatible with TensorFlow 1 code. It's something users need to be aware of, but there's no good way to fix this."]}, {"number": 38677, "title": "Failed to load the native TensorFlow runtime.", "body": "hi, installed tensorflow version 2.1 using pip command anaconda with windows 10 64 bit python 3.6 and getting this issue ,cant find any solution  to this anywhere  ,any help would be appreciated .\r\ni am installing it for cpu not gpu .\r\n![Screenshot (17)](https://user-images.githubusercontent.com/49184195/79679691-bcbb1700-8225-11ea-8ebe-9aadaeb24d61.png)\r\n\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/yadav/PycharmProjects/tensor/tensor.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\yadav\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\n\r\n\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "in response to the automated reply , my cpu does support avx instruction set  as i use intel i7 6th gen \r\nattaching the details\r\n![Screenshot (19)](https://user-images.githubusercontent.com/49184195/79679841-048e6e00-8227-11ea-95da-bb4523fe9c4c.png)\r\n", "@Wolverinne \r\nIs your python 64 bits? \r\n\r\nplease refer to below issues.\r\n#36167 (comment)\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "thanks @Saduf2019 after reading comments i downgraded from tensorflow 2.1 to 2.0 and the issue is resolved.\r\nif anyone else finds same error try-\r\npip install tensorflow=2.0.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38677\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38677\">No</a>\n"]}, {"number": 38676, "title": "import tensorflow as tfimport tensorflow.contrib.layers as layers   File \"<ipython-input-23-4e7964698c5b>\", line 1     import tensorflow as tfimport tensorflow.contrib.layers as layer SyntaxError: invalid syntax", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["import tensorflow as tfimport tensorflow.contrib.layers as layers\r\n  File \"<ipython-input-23-4e7964698c5b>\", line 1\r\n    import tensorflow as tfimport tensorflow.contrib.layers as layers\r\n                                           ^\r\nSyntaxError: invalid syntax\r\n\r\nOccurred while trying to execute https://github.com/aqlaboratory/rgn/blob/master/model/protling.py", "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOccurred while trying to execute https://github.com/aqlaboratory/rgn/blob/master/model/protling.py\r\n[ I am new to python . please help to resolve probelms ] \r\nclass ZoneoutWrapper(RNNCell):\r\n  File \"<ipython-input-7-10a3c41e0928>\", line 1\r\n    class ZoneoutWrapper(RNNCell):\r\n                                  ^\r\nSyntaxError: unexpected EOF while parsing\r\n\r\ndef __init__(self, cell, memory_cell_keep_prob=1.0, hidden_state_keep_prob=1.0,seed=None, is_training=True):\r\n  File \"<ipython-input-8-b6fb9a5d8816>\", line 1\r\n    def __init__(self, cell, memory_cell_keep_prob=1.0, hidden_state_keep_prob=1.0,seed=None, is_training=True):\r\n                                                                                                                ^\r\nSyntaxError: unexpected EOF while parsing\r\n\r\nif (isinstance(memory_cell_keep_prob, float) and not (memory_cell_keep_prob >= 0.0 and memory_cell_keep_prob <= 1.0)):\r\n  File \"<ipython-input-18-0aa3a28c5b66>\", line 1\r\n    if (isinstance(memory_cell_keep_prob, float) and not (memory_cell_keep_prob >= 0.0 and memory_cell_keep_prob <= 1.0)):\r\n                                                                                                                          ^\r\nSyntaxError: unexpected EOF while parsing\r\n\r\n raise ValueError(\"Parameter memory_cell_keep_prob must be between 0 and 1: %d\"  % memory_cell_keep_prob)\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-20-f8fedb42395d>\", line 1, in <module>\r\n    raise ValueError(\"Parameter memory_cell_keep_prob must be between 0 and 1: %d\"  % memory_cell_keep_prob)\r\n\r\nNameError: name 'memory_cell_keep_prob' is not defined\r\n\r\nself._memory_cell_keep_prob = memory_cell_keep_prob    \r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-22-5feb2a45dc43>\", line 1, in <module>\r\n    self._memory_cell_keep_prob = memory_cell_keep_prob\r\n\r\nNameError: name 'memory_cell_keep_prob' is not defined", "@geet2019 \r\n\r\nI believe you are using TF 1.x. .I believe you are using both lines of code in single line.Please, use the below code\r\n\r\n```\r\nimport tensorflow as tf \r\nimport tensorflow.contrib.layers as layers\r\n```\r\nIf you are still facing the issue, request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "thank you  . I will try it out\n\nI would like to execute this work .\nhttps://github.com/aqlaboratory/rgn/blob/master/model/protling.p\n\n\nOn Mon, Apr 20, 2020 at 2:49 PM ravikyram <notifications@github.com> wrote:\n\n> @geet2019 <https://github.com/geet2019>\n>\n> I believe you are using TF 1.x. .I believe you are using both lines of\n> code in single line.Please, use the below code\n>\n> import tensorflow as tf\n> import tensorflow.contrib.layers as layers\n>\n> If you are still facing the issue, request you to share colab link or\n> simple standalone code to reproduce the issue in our environment.It helps\n> us in localizing the issue faster. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38676#issuecomment-616421791>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN4XXYAKZRH6KGGWYFAFBD3RNQHTZANCNFSM4MLSZ7GA>\n> .\n>\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38676\">No</a>\n"]}, {"number": 38674, "title": "Model with multiple outputs throws error about shape of output", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yes\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 2080, 12 GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have an ensemble of MLP's, where each ensemble outputs two values, a `mean`, and a `variance`, as shown below:\r\n```\r\n    def call(self, inputs, training=None, mask=None):\r\n        mean_predictions = []\r\n        variance_predictions = []\r\n        for idx in range(self.num_models):\r\n            mean_predictions.append(self.mean[idx](inputs, training=training))\r\n            variance_predictions.append(self.variance[idx](inputs, training=training))\r\n        mean_stack = tf.stack(mean_predictions)\r\n        variance_stack = tf.stack(variance_predictions)\r\n        return mean_stack, variance_stack\r\n```\r\n\r\nAdditionally, I have a custom loss function that takes these two values and outputs a loss. In particular, the loss is negative log-likelihood of the Gaussian distribution:\r\n```\r\nclass GaussianNLL(Loss):\r\n\r\n    def __init__(self):\r\n        super(GaussianNLL, self).__init__()\r\n\r\n    def call(self, y_true, y_pred):\r\n\r\n        mean, variance = y_pred\r\n        variance = y_pred + 0.0001\r\n        nll = (tf.math.log(variance) / 2 + ((y_true - mean) ** 2) / (2 * variance))\r\n        nll = tf.math.reduce_mean(nll)\r\n        return nll\r\n```\r\nAnd calling\r\n```\r\nmodel.compile(optimizer='adam',\r\n                  loss=loss_fn)\r\nhistory = model.fit(x_train, y_train, y_train,\r\n                        batch_size=2048,\r\n                        epochs=10000,\r\n                        verbose=1,\r\n                        validation_data=(x_val, y_val))\r\n```\r\nI get the following error:  `tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n`, \r\nwhich I fixed using a slight hack: instead of unpacking the outputs, I do\r\n``` \r\nmean = y_pred[0]\r\nvariance = y_pred[1] + 0.0001\r\n```\r\nWhich yields\r\n```\r\nError when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['output_1', 'output_2'] but instead got the following list of 1 arrays:... \r\n```\r\n**Describe the expected behavior**`\r\n\r\nI expect to be able to define how handle my model's multiple outputs if I have custom loss functions. \r\n\r\n**Standalone code to reproduce the issue**\r\nIn the following colab, the code runs fine, but you can see that `fit` function outputs multiple losses, when it should really only be one. I don't know what all the other outputs are.\r\nI just checked, and the reason it runs on colab and not my machine is tensorflow's version. But the above still stands.\r\nhttps://colab.research.google.com/drive/1EnKOiGPmKIYt5f7QvcwN8VMBLniyuqhK\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```", "comments": ["@milongo \r\n\r\nLooks like code is incomplete. Can you please help us with the colab link or simple standalone code with the supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram what is incomplete about it? The code runs in colab.", "I have tried in colab with TF version 2.2.0-rc3 and i am not seeing any issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/a52b4b4de3127ff07d54282a8a589706/untitled792.ipynb).It displays multiple losses instead of showing one.Thanks!", "Yes, why does it display multiple losses? What does each one correspond to?", "@milongo This is expected behaviour. Please take a look at this [issue](https://stackoverflow.com/questions/56802457/how-to-fix-expected-to-see-2-arrays-but-instead-got-the-following-list-of-1) where the same issue has been discussed and there is a work around discussed as well. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38674\">No</a>\n"]}]