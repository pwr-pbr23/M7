[{"number": 50882, "title": "Update gast mirror URL", "body": "Updated the mirrored URL for gast_archive\r\n\r\nFix for #50777", "comments": ["@mihaimaruseac @lamberta @MarkDaoust \r\n\r\nShould we update the min. bazel version for v2.4 here https://www.tensorflow.org/install/source ? See https://github.com/tensorflow/tensorflow/issues/50966\r\n\r\nThanks @Saduf2019 \r\n\r\n", ">  min. bazel version for v2.4 here\r\n\r\nI don't understand the question.", "Never mind - it says 3.1, so 4.1 or 3.99 should do the job \ud83d\udc4d "]}, {"number": 50881, "title": "model.evaluate() does not yield the same accuracy as computing it manually using a for-loop", "body": "Apologies in advance if this is the wrong place for this discussion. I suspect the `model.evaluate()` behavior I identify below is a feature, not a bug, but would like to better understand it if so.\r\n\r\nAfter following the transfer learning tutorial on [Tensorflow's site][1], I have a question about how `model.evaluate()` works in comparison to calculating accuracy by hand.\r\n\r\nAt the very end, after fine-tuning, in the Evaluation and prediction section, we use `model.evaluate()` to calculate the accuracy on the test set as follows:\r\n```\r\nloss, accuracy = model.evaluate(test_dataset)\r\nprint('Test accuracy :', accuracy)\r\n6/6 [==============================] - 2s 217ms/step - loss: 0.0516 - accuracy: 0.9740\r\nTest accuracy : 0.9739583134651184\r\n```\r\n\r\nNext, we generate predictions manually from one batch of images from the test set as part of a visualization exercise:\r\n```\r\n# Apply a sigmoid since our model returns logits\r\npredictions = tf.nn.sigmoid(predictions)\r\npredictions = tf.where(predictions < 0.5, 0, 1)\r\n```\r\nHowever, it's also possible to extend this functionality to calculate predictions across the entire test set and compare them to the actual values to yield an average accuracy:\r\n```\r\nall_acc=tf.zeros([], tf.int32) #initialize array to hold all accuracy indicators (single element)\r\nfor image_batch, label_batch in test_dataset.as_numpy_iterator():\r\n    predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\r\n    predictions = tf.nn.sigmoid(predictions) #apply sigmoid activation function to transform logits to [0,1]\r\n    predictions = tf.where(predictions < 0.5, 0, 1) #round down or up accordingly since it's a binary classifier\r\n    accuracy = tf.where(tf.equal(predictions,label_batch),1,0) #correct is 1 and incorrect is 0\r\n    all_acc = tf.experimental.numpy.append(all_acc, accuracy)\r\nall_acc = all_acc[1:]  #drop first placeholder element\r\navg_acc = tf.reduce_mean(tf.dtypes.cast(all_acc, tf.float16)) \r\nprint('My Accuracy:', avg_acc.numpy()) \r\nMy Accuracy: 0.974\r\n```\r\n\r\nNow, if `model.evaluate()` generates predictions by applying a sigmoid to the logit model outputs and using a threshold of 0.5 like the tutorial suggests, my manually-calculated accuracy should equal the accuracy output of Tensorflow's `model.evaluate()` function. This is indeed the case for the tutorial. My Accuracy: 0.974 = accuracy from `model.evaluate()` function. However, when I try this same code with a model trained using the same convolutional base as the tutorial, but different Gabor images (not cats & dogs like the tutorial), my accuracy no longer equals the `model.evaluate()` accuracy:\r\n```\r\ncurrent_set = set17 #define set to process. \r\nall_acc=tf.zeros([], tf.float64) #initialize array to hold all accuracy indicators (single element)\r\nloss, acc = model.evaluate(current_set) #now test the model's performance on the test set\r\nfor image_batch, label_batch in current_set.as_numpy_iterator():\r\n    predictions = model.predict_on_batch(image_batch).flatten() #run batch through model and return logits\r\n    predictions = tf.nn.sigmoid(predictions) #apply sigmoid activation function to transform logits to [0,1]\r\n    predictions = tf.where(predictions < 0.5, 0, 1) #round down or up accordingly since it's a binary classifier\r\n    accuracy = tf.where(tf.equal(predictions,label_batch),1,0) #correct is 1 and incorrect is 0\r\n    all_acc = tf.experimental.numpy.append(all_acc, accuracy)\r\nall_acc = all_acc[1:]  #drop first placeholder element\r\navg_acc = tf.reduce_mean(all_acc)\r\nprint('My Accuracy:', avg_acc.numpy()) \r\nprint('Tf Accuracy:', acc) \r\nMy Accuracy: 0.832\r\nTf Accuracy: 0.675000011920929\r\n```\r\nDoes anyone know why there would be a discrepancy? Does the model.evaluate() *not* use a sigmoid? Or does it use a different threshold than 0.5? Or perhaps it's something else I'm not considering? Please note, my new model was trained using Gabor images, which are different than the cats and dogs from the tutorial, but the code was the same. \r\n\r\nThank you in advance for any insight!\r\n\r\n\r\n  [1]: https://www.tensorflow.org/tutorials/images/transfer_learning\r\n\r\n\r\n", "comments": ["@thor4 ,\r\n\r\nThis question is better asked on [TensorFlow Forum](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there.", "Thanks, I have posted it as you suggested: https://discuss.tensorflow.org/t/model-evaluate-does-not-yield-the-same-accuracy-as-computing-it-manually-using-a-for-loop/3214\r\n\r\nHopefully someone somewhere will have some insight.", "I'm re-opening this again for your review as it may be a bug. There have been no responses on the Tensorflow Forum post and it's been a month now. Please advise.", "@thor4 ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "thanks, I am working on a jupyter nodebook and will provide soon", "Thanks for your patience. Here is a Jupyter notebook with the model and dataset which duplicates the accuracy calculation discrepancy: https://colab.research.google.com/github/thor4/neuralnets/blob/master/projects/1-CNN/model_evaluate.ipynb\r\n\r\nPlease let me know of questions/issues. Thank you for your help.", "@Saduf2019 ,\r\nI was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/e8b505e356c58733bc9f2c2d90daeaa5/2-5model_evaluate.ipynb), [v2.6](https://colab.research.google.com/gist/tilakrayal/a961871712b2465b4bb6c5b9031b0900/2-6-model_evaluate.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/1ea6aa835c043ffdf216d96c4c0961d9/nightly-model_evaluate.ipynb).Please find the gist of it here.", "For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. This is exactly what you are trying to calculate manually.\r\nBut this is not the case always.Instead of batch wise accuracy or it keeps track of the all True positives and False positives and returns the overall precision, which is called `streaming metric `or `stateful metric` as it is updated batch after batch.\r\nConsider the below simple example.\r\n```\r\n>>> precision = keras.metrics.Precision()\r\n>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\r\n<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\r\n>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\r\n<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\r\n```\r\nIn the above example by passing predictions and labels of first batch and second batch, you can see the it returns 0.8 for first batch and 0.5 for second batch, in second batch instead of returning precision for only second batch it actually returns the overall precision.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50880, "title": "ModuleNotFoundError: No module named 'TensorFlow'  File \"<string>\", line 1, in <module>", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): $ pip install --upgrade TensorFlow\r\n- TensorFlow version: tensorflow 2.5.0\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?:      venv\r\n- using the following sequence \r\n- $ python3 -m venv venv\r\n$ source venv/bin/activate\r\n$ pip install --upgrade pip\r\n$ pip install --upgrade TensorFlow\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)\r\n- CUDA/cuDNN version: /usr/local/cuda-11.0 installed using  https://github.com/tensorflow/tensorflow/issues/45930\r\n- \r\n-  cuda had a problem. I fixed using  \r\n- 1) find / -name 'libcudart.so.11.0'\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0\r\n2) edit /etc/profile\r\n$ sudo vim /etc/profile\r\n3) append path to \"LD_LIBRARY_PATH\" in profile file\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.0/targets/x86_64-linux/lib\r\n4) make environment file work\r\nsource /etc/profile\r\n-\r\n- GPU model and memory:\r\n- Intel HD Graphics 4400\r\nHP proBook 430 G2 HSTNN-C84C product:F6N65AV  \r\ncpu i5-4210U 1.7GHz \r\nmemory 4G main \r\n64bit OS\r\n-\r\n-\r\n**Describe the problem**\r\n@hp430:~/my_tensorflow$ python -c 'import TensorFlow as tf; print(tf.__version__)'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'TensorFlow'\r\n-\r\nAny help really helps. Thanks.\r\n-\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n$python3 -v\r\n$ sudo apt install python3-venv python3-dev\r\n$ mkdir my_tensorflow \r\n$ cd my_tensorflow \r\n$ python3 -m venv venv\r\n$ source venv/bin/activate\r\n$ pip install --upgrade pip\r\n$ pip install --upgrade TensorFlow\r\n$ python -c 'import TensorFlow as tf; print(tf.__version__)'\r\n-\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n(venv) @hp430:~/my_tensorflow$ pip debug --verbose\r\nWARNING: This command is only meant for debugging. Do not use this with automation for parsing and getting \r\nthese details, since the output and options of this command may change without notice.\r\npip version: pip 21.1.3 from /home/benh/my_tensorflow/venv/lib/python3.8/site-packages/pip (python 3.8)\r\nsys.version: 3.8.10 (default, Jun  2 2021, 10:49:15)\r\n[GCC 9.4.0]\r\nsys.executable: /home/benh/my_tensorflow/venv/bin/python3\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: UTF-8\r\nsys.platform: linux\r\nsys.implementation:\r\n  name: cpython\r\n'cert' config value: Not specified\r\nREQUESTS_CA_BUNDLE: None\r\nCURL_CA_BUNDLE: None\r\npip._vendor.certifi.where(): /home/benh/my_tensorflow/venv/lib/python3.8/site-packages/pip/_vendor/certifi/cacert.pem\r\npip._vendor.DEBUNDLED: False\r\nvendored library versions:\r\n  appdirs==1.4.4\r\n  CacheControl==0.12.6\r\n  colorama==0.4.4\r\n  distlib==0.3.1\r\n  distro==1.5.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  html5lib==1.1\r\n  msgpack==1.0.2 (Unable to locate actual module version, using vendor.txt specified version)\r\n  packaging==20.9\r\n  pep517==0.10.0\r\n  progress==1.5\r\n  pyparsing==2.4.7\r\n  requests==2.25.1\r\n  certifi==2020.12.05\r\n  chardet==4.0.0\r\n  idna==3.1\r\n  urllib3==1.26.4\r\n  resolvelib==0.7.0\r\n  setuptools==44.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  six==1.15.0\r\n  tenacity==7.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  toml==0.10.2\r\n  webencodings==0.5.1 (Unable to locate actual module version, using vendor.txt specified version)\r\nCompatible tags: 600\r\n  cp38-cp38-manylinux_2_31_x86_64\r\n  cp38-cp38-manylinux_2_30_x86_64\r\n  cp38-cp38-manylinux_2_29_x86_64\r\n  cp38-cp38-manylinux_2_28_x86_64\r\n  cp38-cp38-manylinux_2_27_x86_64\r\n  cp38-cp38-manylinux_2_26_x86_64\r\n  cp38-cp38-manylinux_2_25_x86_64\r\n  cp38-cp38-manylinux_2_24_x86_64\r\n  cp38-cp38-manylinux_2_23_x86_64\r\n  cp38-cp38-manylinux_2_22_x86_64\r\n  cp38-cp38-manylinux_2_21_x86_64\r\n  cp38-cp38-manylinux_2_20_x86_64\r\n  cp38-cp38-manylinux_2_19_x86_64\r\n  cp38-cp38-manylinux_2_18_x86_64\r\n  cp38-cp38-manylinux_2_17_x86_64\r\n  cp38-cp38-manylinux2014_x86_64\r\n  cp38-cp38-manylinux_2_16_x86_64\r\n  cp38-cp38-manylinux_2_15_x86_64\r\n  cp38-cp38-manylinux_2_14_x86_64\r\n  cp38-cp38-manylinux_2_13_x86_64\r\n  cp38-cp38-manylinux_2_12_x86_64\r\n  cp38-cp38-manylinux2010_x86_64\r\n  cp38-cp38-manylinux_2_11_x86_64\r\n  cp38-cp38-manylinux_2_10_x86_64\r\n  cp38-cp38-manylinux_2_9_x86_64\r\n  cp38-cp38-manylinux_2_8_x86_64\r\n  cp38-cp38-manylinux_2_7_x86_64\r\n  cp38-cp38-manylinux_2_6_x86_64\r\n  cp38-cp38-manylinux_2_5_x86_64\r\n  cp38-cp38-manylinux1_x86_64\r\n  cp38-cp38-linux_x86_64\r\n  cp38-abi3-manylinux_2_31_x86_64\r\n  cp38-abi3-manylinux_2_30_x86_64\r\n  cp38-abi3-manylinux_2_29_x86_64\r\n  cp38-abi3-manylinux_2_28_x86_64\r\n  cp38-abi3-manylinux_2_27_x86_64\r\n  cp38-abi3-manylinux_2_26_x86_64\r\n  cp38-abi3-manylinux_2_25_x86_64\r\n  cp38-abi3-manylinux_2_24_x86_64\r\n  cp38-abi3-manylinux_2_23_x86_64\r\n  cp38-abi3-manylinux_2_22_x86_64\r\n  cp38-abi3-manylinux_2_21_x86_64\r\n  cp38-abi3-manylinux_2_20_x86_64\r\n  cp38-abi3-manylinux_2_19_x86_64\r\n  cp38-abi3-manylinux_2_18_x86_64\r\n  cp38-abi3-manylinux_2_17_x86_64\r\n  cp38-abi3-manylinux2014_x86_64\r\n  cp38-abi3-manylinux_2_16_x86_64\r\n  cp38-abi3-manylinux_2_15_x86_64\r\n  cp38-abi3-manylinux_2_14_x86_64\r\n  cp38-abi3-manylinux_2_13_x86_64\r\n  cp38-abi3-manylinux_2_12_x86_64\r\n  cp38-abi3-manylinux2010_x86_64\r\n  cp38-abi3-manylinux_2_11_x86_64\r\n  cp38-abi3-manylinux_2_10_x86_64\r\n  cp38-abi3-manylinux_2_9_x86_64\r\n  cp38-abi3-manylinux_2_8_x86_64\r\n  cp38-abi3-manylinux_2_7_x86_64\r\n  cp38-abi3-manylinux_2_6_x86_64\r\n  cp38-abi3-manylinux_2_5_x86_64\r\n  cp38-abi3-manylinux1_x86_64\r\n  cp38-abi3-linux_x86_64\r\n  cp38-none-manylinux_2_31_x86_64\r\n  cp38-none-manylinux_2_30_x86_64\r\n  cp38-none-manylinux_2_29_x86_64\r\n  cp38-none-manylinux_2_28_x86_64\r\n  cp38-none-manylinux_2_27_x86_64\r\n  cp38-none-manylinux_2_26_x86_64\r\n  cp38-none-manylinux_2_25_x86_64\r\n  cp38-none-manylinux_2_24_x86_64\r\n  cp38-none-manylinux_2_23_x86_64\r\n  cp38-none-manylinux_2_22_x86_64\r\n  cp38-none-manylinux_2_21_x86_64\r\n  cp38-none-manylinux_2_20_x86_64\r\n  cp38-none-manylinux_2_19_x86_64\r\n  cp38-none-manylinux_2_18_x86_64\r\n  cp38-none-manylinux_2_17_x86_64\r\n  cp38-none-manylinux2014_x86_64\r\n  cp38-none-manylinux_2_16_x86_64\r\n  cp38-none-manylinux_2_15_x86_64\r\n  cp38-none-manylinux_2_14_x86_64\r\n  cp38-none-manylinux_2_13_x86_64\r\n  cp38-none-manylinux_2_12_x86_64\r\n  cp38-none-manylinux2010_x86_64\r\n  cp38-none-manylinux_2_11_x86_64\r\n  cp38-none-manylinux_2_10_x86_64\r\n  cp38-none-manylinux_2_9_x86_64\r\n  cp38-none-manylinux_2_8_x86_64\r\n  cp38-none-manylinux_2_7_x86_64\r\n  cp38-none-manylinux_2_6_x86_64\r\n  cp38-none-manylinux_2_5_x86_64\r\n  cp38-none-manylinux1_x86_64\r\n  cp38-none-linux_x86_64\r\n  cp37-abi3-manylinux_2_31_x86_64\r\n  cp37-abi3-manylinux_2_30_x86_64\r\n  cp37-abi3-manylinux_2_29_x86_64\r\n  cp37-abi3-manylinux_2_28_x86_64\r\n  cp37-abi3-manylinux_2_27_x86_64\r\n  cp37-abi3-manylinux_2_26_x86_64\r\n  cp37-abi3-manylinux_2_25_x86_64\r\n  cp37-abi3-manylinux_2_24_x86_64\r\n  cp37-abi3-manylinux_2_23_x86_64\r\n  cp37-abi3-manylinux_2_22_x86_64\r\n  cp37-abi3-manylinux_2_21_x86_64\r\n  cp37-abi3-manylinux_2_20_x86_64\r\n  cp37-abi3-manylinux_2_19_x86_64\r\n  cp37-abi3-manylinux_2_18_x86_64\r\n  cp37-abi3-manylinux_2_17_x86_64\r\n  cp37-abi3-manylinux2014_x86_64\r\n  cp37-abi3-manylinux_2_16_x86_64\r\n  cp37-abi3-manylinux_2_15_x86_64\r\n  cp37-abi3-manylinux_2_14_x86_64\r\n  cp37-abi3-manylinux_2_13_x86_64\r\n  cp37-abi3-manylinux_2_12_x86_64\r\n  cp37-abi3-manylinux2010_x86_64\r\n  cp37-abi3-manylinux_2_11_x86_64\r\n  cp37-abi3-manylinux_2_10_x86_64\r\n  cp37-abi3-manylinux_2_9_x86_64\r\n  cp37-abi3-manylinux_2_8_x86_64\r\n  cp37-abi3-manylinux_2_7_x86_64\r\n  cp37-abi3-manylinux_2_6_x86_64\r\n  cp37-abi3-manylinux_2_5_x86_64\r\n  cp37-abi3-manylinux1_x86_64\r\n  cp37-abi3-linux_x86_64\r\n  cp36-abi3-manylinux_2_31_x86_64\r\n  cp36-abi3-manylinux_2_30_x86_64\r\n  cp36-abi3-manylinux_2_29_x86_64\r\n  cp36-abi3-manylinux_2_28_x86_64\r\n  cp36-abi3-manylinux_2_27_x86_64\r\n  cp36-abi3-manylinux_2_26_x86_64\r\n  cp36-abi3-manylinux_2_25_x86_64\r\n  cp36-abi3-manylinux_2_24_x86_64\r\n  cp36-abi3-manylinux_2_23_x86_64\r\n  cp36-abi3-manylinux_2_22_x86_64\r\n  cp36-abi3-manylinux_2_21_x86_64\r\n  cp36-abi3-manylinux_2_20_x86_64\r\n  cp36-abi3-manylinux_2_19_x86_64\r\n  cp36-abi3-manylinux_2_18_x86_64\r\n  cp36-abi3-manylinux_2_17_x86_64\r\n  cp36-abi3-manylinux2014_x86_64\r\n  cp36-abi3-manylinux_2_16_x86_64\r\n  cp36-abi3-manylinux_2_15_x86_64\r\n  cp36-abi3-manylinux_2_14_x86_64\r\n  cp36-abi3-manylinux_2_13_x86_64\r\n  cp36-abi3-manylinux_2_12_x86_64\r\n  cp36-abi3-manylinux2010_x86_64\r\n  cp36-abi3-manylinux_2_11_x86_64\r\n  cp36-abi3-manylinux_2_10_x86_64\r\n  cp36-abi3-manylinux_2_9_x86_64\r\n  cp36-abi3-manylinux_2_8_x86_64\r\n  cp36-abi3-manylinux_2_7_x86_64\r\n  cp36-abi3-manylinux_2_6_x86_64\r\n  cp36-abi3-manylinux_2_5_x86_64\r\n  cp36-abi3-manylinux1_x86_64\r\n  cp36-abi3-linux_x86_64\r\n  cp35-abi3-manylinux_2_31_x86_64\r\n  cp35-abi3-manylinux_2_30_x86_64\r\n  cp35-abi3-manylinux_2_29_x86_64\r\n  cp35-abi3-manylinux_2_28_x86_64\r\n  cp35-abi3-manylinux_2_27_x86_64\r\n  cp35-abi3-manylinux_2_26_x86_64\r\n  cp35-abi3-manylinux_2_25_x86_64\r\n  cp35-abi3-manylinux_2_24_x86_64\r\n  cp35-abi3-manylinux_2_23_x86_64\r\n  cp35-abi3-manylinux_2_22_x86_64\r\n  cp35-abi3-manylinux_2_21_x86_64\r\n  cp35-abi3-manylinux_2_20_x86_64\r\n  cp35-abi3-manylinux_2_19_x86_64\r\n  cp35-abi3-manylinux_2_18_x86_64\r\n  cp35-abi3-manylinux_2_17_x86_64\r\n  cp35-abi3-manylinux2014_x86_64\r\n  cp35-abi3-manylinux_2_16_x86_64\r\n  cp35-abi3-manylinux_2_15_x86_64\r\n  cp35-abi3-manylinux_2_14_x86_64\r\n  cp35-abi3-manylinux_2_13_x86_64\r\n  cp35-abi3-manylinux_2_12_x86_64\r\n  cp35-abi3-manylinux2010_x86_64\r\n  cp35-abi3-manylinux_2_11_x86_64\r\n  cp35-abi3-manylinux_2_10_x86_64\r\n  cp35-abi3-manylinux_2_9_x86_64\r\n  cp35-abi3-manylinux_2_8_x86_64\r\n  cp35-abi3-manylinux_2_7_x86_64\r\n  cp35-abi3-manylinux_2_6_x86_64\r\n  cp35-abi3-manylinux_2_5_x86_64\r\n  cp35-abi3-manylinux1_x86_64\r\n  cp35-abi3-linux_x86_64\r\n  cp34-abi3-manylinux_2_31_x86_64\r\n  cp34-abi3-manylinux_2_30_x86_64\r\n  cp34-abi3-manylinux_2_29_x86_64\r\n  cp34-abi3-manylinux_2_28_x86_64\r\n  cp34-abi3-manylinux_2_27_x86_64\r\n  cp34-abi3-manylinux_2_26_x86_64\r\n  cp34-abi3-manylinux_2_25_x86_64\r\n  cp34-abi3-manylinux_2_24_x86_64\r\n  cp34-abi3-manylinux_2_23_x86_64\r\n  cp34-abi3-manylinux_2_22_x86_64\r\n  cp34-abi3-manylinux_2_21_x86_64\r\n  cp34-abi3-manylinux_2_20_x86_64\r\n  cp34-abi3-manylinux_2_19_x86_64\r\n  cp34-abi3-manylinux_2_18_x86_64\r\n  cp34-abi3-manylinux_2_17_x86_64\r\n  cp34-abi3-manylinux2014_x86_64\r\n  cp34-abi3-manylinux_2_16_x86_64\r\n  cp34-abi3-manylinux_2_15_x86_64\r\n  cp34-abi3-manylinux_2_14_x86_64\r\n  cp34-abi3-manylinux_2_13_x86_64\r\n  cp34-abi3-manylinux_2_12_x86_64\r\n  cp34-abi3-manylinux2010_x86_64\r\n  cp34-abi3-manylinux_2_11_x86_64\r\n  cp34-abi3-manylinux_2_10_x86_64\r\n  cp34-abi3-manylinux_2_9_x86_64\r\n  cp34-abi3-manylinux_2_8_x86_64\r\n  cp34-abi3-manylinux_2_7_x86_64\r\n  cp34-abi3-manylinux_2_6_x86_64\r\n  cp34-abi3-manylinux_2_5_x86_64\r\n  cp34-abi3-manylinux1_x86_64\r\n  cp34-abi3-linux_x86_64\r\n  cp33-abi3-manylinux_2_31_x86_64\r\n  cp33-abi3-manylinux_2_30_x86_64\r\n  cp33-abi3-manylinux_2_29_x86_64\r\n  cp33-abi3-manylinux_2_28_x86_64\r\n  cp33-abi3-manylinux_2_27_x86_64\r\n  cp33-abi3-manylinux_2_26_x86_64\r\n  cp33-abi3-manylinux_2_25_x86_64\r\n  cp33-abi3-manylinux_2_24_x86_64\r\n  cp33-abi3-manylinux_2_23_x86_64\r\n  cp33-abi3-manylinux_2_22_x86_64\r\n  cp33-abi3-manylinux_2_21_x86_64\r\n  cp33-abi3-manylinux_2_20_x86_64\r\n  cp33-abi3-manylinux_2_19_x86_64\r\n  cp33-abi3-manylinux_2_18_x86_64\r\n  cp33-abi3-manylinux_2_17_x86_64\r\n  cp33-abi3-manylinux2014_x86_64\r\n  cp33-abi3-manylinux_2_16_x86_64\r\n  cp33-abi3-manylinux_2_15_x86_64\r\n  cp33-abi3-manylinux_2_14_x86_64\r\n  cp33-abi3-manylinux_2_13_x86_64\r\n  cp33-abi3-manylinux_2_12_x86_64\r\n  cp33-abi3-manylinux2010_x86_64\r\n  cp33-abi3-manylinux_2_11_x86_64\r\n  cp33-abi3-manylinux_2_10_x86_64\r\n  cp33-abi3-manylinux_2_9_x86_64\r\n  cp33-abi3-manylinux_2_8_x86_64\r\n  cp33-abi3-manylinux_2_7_x86_64\r\n  cp33-abi3-manylinux_2_6_x86_64\r\n  cp33-abi3-manylinux_2_5_x86_64\r\n  cp33-abi3-manylinux1_x86_64\r\n  cp33-abi3-linux_x86_64\r\n  cp32-abi3-manylinux_2_31_x86_64\r\n  cp32-abi3-manylinux_2_30_x86_64\r\n  cp32-abi3-manylinux_2_29_x86_64\r\n  cp32-abi3-manylinux_2_28_x86_64\r\n  cp32-abi3-manylinux_2_27_x86_64\r\n  cp32-abi3-manylinux_2_26_x86_64\r\n  cp32-abi3-manylinux_2_25_x86_64\r\n  cp32-abi3-manylinux_2_24_x86_64\r\n  cp32-abi3-manylinux_2_23_x86_64\r\n  cp32-abi3-manylinux_2_22_x86_64\r\n  cp32-abi3-manylinux_2_21_x86_64\r\n  cp32-abi3-manylinux_2_20_x86_64\r\n  cp32-abi3-manylinux_2_19_x86_64\r\n  cp32-abi3-manylinux_2_18_x86_64\r\n  cp32-abi3-manylinux_2_17_x86_64\r\n  cp32-abi3-manylinux2014_x86_64\r\n  cp32-abi3-manylinux_2_16_x86_64\r\n  cp32-abi3-manylinux_2_15_x86_64\r\n  cp32-abi3-manylinux_2_14_x86_64\r\n  cp32-abi3-manylinux_2_13_x86_64\r\n  cp32-abi3-manylinux_2_12_x86_64\r\n  cp32-abi3-manylinux2010_x86_64\r\n  cp32-abi3-manylinux_2_11_x86_64\r\n  cp32-abi3-manylinux_2_10_x86_64\r\n  cp32-abi3-manylinux_2_9_x86_64\r\n  cp32-abi3-manylinux_2_8_x86_64\r\n  cp32-abi3-manylinux_2_7_x86_64\r\n  cp32-abi3-manylinux_2_6_x86_64\r\n  cp32-abi3-manylinux_2_5_x86_64\r\n  cp32-abi3-manylinux1_x86_64\r\n  cp32-abi3-linux_x86_64\r\n  py38-none-manylinux_2_31_x86_64\r\n  py38-none-manylinux_2_30_x86_64\r\n  py38-none-manylinux_2_29_x86_64\r\n  py38-none-manylinux_2_28_x86_64\r\n  py38-none-manylinux_2_27_x86_64\r\n  py38-none-manylinux_2_26_x86_64\r\n  py38-none-manylinux_2_25_x86_64\r\n  py38-none-manylinux_2_24_x86_64\r\n  py38-none-manylinux_2_23_x86_64\r\n  py38-none-manylinux_2_22_x86_64\r\n  py38-none-manylinux_2_21_x86_64\r\n  py38-none-manylinux_2_20_x86_64\r\n  py38-none-manylinux_2_19_x86_64\r\n  py38-none-manylinux_2_18_x86_64\r\n  py38-none-manylinux_2_17_x86_64\r\n  py38-none-manylinux2014_x86_64\r\n  py38-none-manylinux_2_16_x86_64\r\n  py38-none-manylinux_2_15_x86_64\r\n  py38-none-manylinux_2_14_x86_64\r\n  py38-none-manylinux_2_13_x86_64\r\n  py38-none-manylinux_2_12_x86_64\r\n  py38-none-manylinux2010_x86_64\r\n  py38-none-manylinux_2_11_x86_64\r\n  py38-none-manylinux_2_10_x86_64\r\n  py38-none-manylinux_2_9_x86_64\r\n  py38-none-manylinux_2_8_x86_64\r\n  py38-none-manylinux_2_7_x86_64\r\n  py38-none-manylinux_2_6_x86_64\r\n  py38-none-manylinux_2_5_x86_64\r\n  py38-none-manylinux1_x86_64\r\n  py38-none-linux_x86_64\r\n  py3-none-manylinux_2_31_x86_64\r\n  py3-none-manylinux_2_30_x86_64\r\n  py3-none-manylinux_2_29_x86_64\r\n  py3-none-manylinux_2_28_x86_64\r\n  py3-none-manylinux_2_27_x86_64\r\n  py3-none-manylinux_2_26_x86_64\r\n  py3-none-manylinux_2_25_x86_64\r\n  py3-none-manylinux_2_24_x86_64\r\n  py3-none-manylinux_2_23_x86_64\r\n  py3-none-manylinux_2_22_x86_64\r\n  py3-none-manylinux_2_21_x86_64\r\n  py3-none-manylinux_2_20_x86_64\r\n  py3-none-manylinux_2_19_x86_64\r\n  py3-none-manylinux_2_18_x86_64\r\n  py3-none-manylinux_2_17_x86_64\r\n  py3-none-manylinux2014_x86_64\r\n  py3-none-manylinux_2_16_x86_64\r\n  py3-none-manylinux_2_15_x86_64\r\n  py3-none-manylinux_2_14_x86_64\r\n  py3-none-manylinux_2_13_x86_64\r\n  py3-none-manylinux_2_12_x86_64\r\n  py3-none-manylinux2010_x86_64\r\n  py3-none-manylinux_2_11_x86_64\r\n  py3-none-manylinux_2_10_x86_64\r\n  py3-none-manylinux_2_9_x86_64\r\n  py3-none-manylinux_2_8_x86_64\r\n  py3-none-manylinux_2_7_x86_64\r\n  py3-none-manylinux_2_6_x86_64\r\n  py3-none-manylinux_2_5_x86_64\r\n  py3-none-manylinux1_x86_64\r\n  py3-none-linux_x86_64\r\n  py37-none-manylinux_2_31_x86_64\r\n  py37-none-manylinux_2_30_x86_64\r\n  py37-none-manylinux_2_29_x86_64\r\n  py37-none-manylinux_2_28_x86_64\r\n  py37-none-manylinux_2_27_x86_64\r\n  py37-none-manylinux_2_26_x86_64\r\n  py37-none-manylinux_2_25_x86_64\r\n  py37-none-manylinux_2_24_x86_64\r\n  py37-none-manylinux_2_23_x86_64\r\n  py37-none-manylinux_2_22_x86_64\r\n  py37-none-manylinux_2_21_x86_64\r\n  py37-none-manylinux_2_20_x86_64\r\n  py37-none-manylinux_2_19_x86_64\r\n  py37-none-manylinux_2_18_x86_64\r\n  py37-none-manylinux_2_17_x86_64\r\n  py37-none-manylinux2014_x86_64\r\n  py37-none-manylinux_2_16_x86_64\r\n  py37-none-manylinux_2_15_x86_64\r\n  py37-none-manylinux_2_14_x86_64\r\n  py37-none-manylinux_2_13_x86_64\r\n  py37-none-manylinux_2_12_x86_64\r\n  py37-none-manylinux2010_x86_64\r\n  py37-none-manylinux_2_11_x86_64\r\n  py37-none-manylinux_2_10_x86_64\r\n  py37-none-manylinux_2_9_x86_64\r\n  py37-none-manylinux_2_8_x86_64\r\n  py37-none-manylinux_2_7_x86_64\r\n  py37-none-manylinux_2_6_x86_64\r\n  py37-none-manylinux_2_5_x86_64\r\n  py37-none-manylinux1_x86_64\r\n  py37-none-linux_x86_64\r\n  py36-none-manylinux_2_31_x86_64\r\n  py36-none-manylinux_2_30_x86_64\r\n  py36-none-manylinux_2_29_x86_64\r\n  py36-none-manylinux_2_28_x86_64\r\n  py36-none-manylinux_2_27_x86_64\r\n  py36-none-manylinux_2_26_x86_64\r\n  py36-none-manylinux_2_25_x86_64\r\n  py36-none-manylinux_2_24_x86_64\r\n  py36-none-manylinux_2_23_x86_64\r\n  py36-none-manylinux_2_22_x86_64\r\n  py36-none-manylinux_2_21_x86_64\r\n  py36-none-manylinux_2_20_x86_64\r\n  py36-none-manylinux_2_19_x86_64\r\n  py36-none-manylinux_2_18_x86_64\r\n  py36-none-manylinux_2_17_x86_64\r\n  py36-none-manylinux2014_x86_64\r\n  py36-none-manylinux_2_16_x86_64\r\n  py36-none-manylinux_2_15_x86_64\r\n  py36-none-manylinux_2_14_x86_64\r\n  py36-none-manylinux_2_13_x86_64\r\n  py36-none-manylinux_2_12_x86_64\r\n  py36-none-manylinux2010_x86_64\r\n  py36-none-manylinux_2_11_x86_64\r\n  py36-none-manylinux_2_10_x86_64\r\n  py36-none-manylinux_2_9_x86_64\r\n  py36-none-manylinux_2_8_x86_64\r\n  py36-none-manylinux_2_7_x86_64\r\n  py36-none-manylinux_2_6_x86_64\r\n  py36-none-manylinux_2_5_x86_64\r\n  py36-none-manylinux1_x86_64\r\n  py36-none-linux_x86_64\r\n  py35-none-manylinux_2_31_x86_64\r\n  py35-none-manylinux_2_30_x86_64\r\n  py35-none-manylinux_2_29_x86_64\r\n  py35-none-manylinux_2_28_x86_64\r\n  py35-none-manylinux_2_27_x86_64\r\n  py35-none-manylinux_2_26_x86_64\r\n  py35-none-manylinux_2_25_x86_64\r\n  py35-none-manylinux_2_24_x86_64\r\n  py35-none-manylinux_2_23_x86_64\r\n  py35-none-manylinux_2_22_x86_64\r\n  py35-none-manylinux_2_21_x86_64\r\n  py35-none-manylinux_2_20_x86_64\r\n  py35-none-manylinux_2_19_x86_64\r\n  py35-none-manylinux_2_18_x86_64\r\n  py35-none-manylinux_2_17_x86_64\r\n  py35-none-manylinux2014_x86_64\r\n  py35-none-manylinux_2_16_x86_64\r\n  py35-none-manylinux_2_15_x86_64\r\n  py35-none-manylinux_2_14_x86_64\r\n  py35-none-manylinux_2_13_x86_64\r\n  py35-none-manylinux_2_12_x86_64\r\n  py35-none-manylinux2010_x86_64\r\n  py35-none-manylinux_2_11_x86_64\r\n  py35-none-manylinux_2_10_x86_64\r\n  py35-none-manylinux_2_9_x86_64\r\n  py35-none-manylinux_2_8_x86_64\r\n  py35-none-manylinux_2_7_x86_64\r\n  py35-none-manylinux_2_6_x86_64\r\n  py35-none-manylinux_2_5_x86_64\r\n  py35-none-manylinux1_x86_64\r\n  py35-none-linux_x86_64\r\n  py34-none-manylinux_2_31_x86_64\r\n  py34-none-manylinux_2_30_x86_64\r\n  py34-none-manylinux_2_29_x86_64\r\n  py34-none-manylinux_2_28_x86_64\r\n  py34-none-manylinux_2_27_x86_64\r\n  py34-none-manylinux_2_26_x86_64\r\n  py34-none-manylinux_2_25_x86_64\r\n  py34-none-manylinux_2_24_x86_64\r\n  py34-none-manylinux_2_23_x86_64\r\n  py34-none-manylinux_2_22_x86_64\r\n  py34-none-manylinux_2_21_x86_64\r\n  py34-none-manylinux_2_20_x86_64\r\n  py34-none-manylinux_2_19_x86_64\r\n  py34-none-manylinux_2_18_x86_64\r\n  py34-none-manylinux_2_17_x86_64\r\n  py34-none-manylinux2014_x86_64\r\n  py34-none-manylinux_2_16_x86_64\r\n  py34-none-manylinux_2_15_x86_64\r\n  py34-none-manylinux_2_14_x86_64\r\n  py34-none-manylinux_2_13_x86_64\r\n  py34-none-manylinux_2_12_x86_64\r\n  py34-none-manylinux2010_x86_64\r\n  py34-none-manylinux_2_11_x86_64\r\n  py34-none-manylinux_2_10_x86_64\r\n  py34-none-manylinux_2_9_x86_64\r\n  py34-none-manylinux_2_8_x86_64\r\n  py34-none-manylinux_2_7_x86_64\r\n  py34-none-manylinux_2_6_x86_64\r\n  py34-none-manylinux_2_5_x86_64\r\n  py34-none-manylinux1_x86_64\r\n  py34-none-linux_x86_64\r\n  py33-none-manylinux_2_31_x86_64\r\n  py33-none-manylinux_2_30_x86_64\r\n  py33-none-manylinux_2_29_x86_64\r\n  py33-none-manylinux_2_28_x86_64\r\n  py33-none-manylinux_2_27_x86_64\r\n  py33-none-manylinux_2_26_x86_64\r\n  py33-none-manylinux_2_25_x86_64\r\n  py33-none-manylinux_2_24_x86_64\r\n  py33-none-manylinux_2_23_x86_64\r\n  py33-none-manylinux_2_22_x86_64\r\n  py33-none-manylinux_2_21_x86_64\r\n  py33-none-manylinux_2_20_x86_64\r\n  py33-none-manylinux_2_19_x86_64\r\n  py33-none-manylinux_2_18_x86_64\r\n  py33-none-manylinux_2_17_x86_64\r\n  py33-none-manylinux2014_x86_64\r\n  py33-none-manylinux_2_16_x86_64\r\n  py33-none-manylinux_2_15_x86_64\r\n  py33-none-manylinux_2_14_x86_64\r\n  py33-none-manylinux_2_13_x86_64\r\n  py33-none-manylinux_2_12_x86_64\r\n  py33-none-manylinux2010_x86_64\r\n  py33-none-manylinux_2_11_x86_64\r\n  py33-none-manylinux_2_10_x86_64\r\n  py33-none-manylinux_2_9_x86_64\r\n  py33-none-manylinux_2_8_x86_64\r\n  py33-none-manylinux_2_7_x86_64\r\n  py33-none-manylinux_2_6_x86_64\r\n  py33-none-manylinux_2_5_x86_64\r\n  py33-none-manylinux1_x86_64\r\n  py33-none-linux_x86_64\r\n  py32-none-manylinux_2_31_x86_64\r\n  py32-none-manylinux_2_30_x86_64\r\n  py32-none-manylinux_2_29_x86_64\r\n  py32-none-manylinux_2_28_x86_64\r\n  py32-none-manylinux_2_27_x86_64\r\n  py32-none-manylinux_2_26_x86_64\r\n  py32-none-manylinux_2_25_x86_64\r\n  py32-none-manylinux_2_24_x86_64\r\n  py32-none-manylinux_2_23_x86_64\r\n  py32-none-manylinux_2_22_x86_64\r\n  py32-none-manylinux_2_21_x86_64\r\n  py32-none-manylinux_2_20_x86_64\r\n  py32-none-manylinux_2_19_x86_64\r\n  py32-none-manylinux_2_18_x86_64\r\n  py32-none-manylinux_2_17_x86_64\r\n  py32-none-manylinux2014_x86_64\r\n  py32-none-manylinux_2_16_x86_64\r\n  py32-none-manylinux_2_15_x86_64\r\n  py32-none-manylinux_2_14_x86_64\r\n  py32-none-manylinux_2_13_x86_64\r\n  py32-none-manylinux_2_12_x86_64\r\n  py32-none-manylinux2010_x86_64\r\n  py32-none-manylinux_2_11_x86_64\r\n  py32-none-manylinux_2_10_x86_64\r\n  py32-none-manylinux_2_9_x86_64\r\n  py32-none-manylinux_2_8_x86_64\r\n  py32-none-manylinux_2_7_x86_64\r\n  py32-none-manylinux_2_6_x86_64\r\n  py32-none-manylinux_2_5_x86_64\r\n  py32-none-manylinux1_x86_64\r\n  py32-none-linux_x86_64\r\n  py31-none-manylinux_2_31_x86_64\r\n  py31-none-manylinux_2_30_x86_64\r\n  py31-none-manylinux_2_29_x86_64\r\n  py31-none-manylinux_2_28_x86_64\r\n  py31-none-manylinux_2_27_x86_64\r\n  py31-none-manylinux_2_26_x86_64\r\n  py31-none-manylinux_2_25_x86_64\r\n  py31-none-manylinux_2_24_x86_64\r\n  py31-none-manylinux_2_23_x86_64\r\n  py31-none-manylinux_2_22_x86_64\r\n  py31-none-manylinux_2_21_x86_64\r\n  py31-none-manylinux_2_20_x86_64\r\n  py31-none-manylinux_2_19_x86_64\r\n  py31-none-manylinux_2_18_x86_64\r\n  py31-none-manylinux_2_17_x86_64\r\n  py31-none-manylinux2014_x86_64\r\n  py31-none-manylinux_2_16_x86_64\r\n  py31-none-manylinux_2_15_x86_64\r\n  py31-none-manylinux_2_14_x86_64\r\n  py31-none-manylinux_2_13_x86_64\r\n  py31-none-manylinux_2_12_x86_64\r\n  py31-none-manylinux2010_x86_64\r\n  py31-none-manylinux_2_11_x86_64\r\n  py31-none-manylinux_2_10_x86_64\r\n  py31-none-manylinux_2_9_x86_64\r\n  py31-none-manylinux_2_8_x86_64\r\n  py31-none-manylinux_2_7_x86_64\r\n  py31-none-manylinux_2_6_x86_64\r\n  py31-none-manylinux_2_5_x86_64\r\n  py31-none-manylinux1_x86_64\r\n  py31-none-linux_x86_64\r\n  py30-none-manylinux_2_31_x86_64\r\n  py30-none-manylinux_2_30_x86_64\r\n  py30-none-manylinux_2_29_x86_64\r\n  py30-none-manylinux_2_28_x86_64\r\n  py30-none-manylinux_2_27_x86_64\r\n  py30-none-manylinux_2_26_x86_64\r\n  py30-none-manylinux_2_25_x86_64\r\n  py30-none-manylinux_2_24_x86_64\r\n  py30-none-manylinux_2_23_x86_64\r\n  py30-none-manylinux_2_22_x86_64\r\n  py30-none-manylinux_2_21_x86_64\r\n  py30-none-manylinux_2_20_x86_64\r\n  py30-none-manylinux_2_19_x86_64\r\n  py30-none-manylinux_2_18_x86_64\r\n  py30-none-manylinux_2_17_x86_64\r\n  py30-none-manylinux2014_x86_64\r\n  py30-none-manylinux_2_16_x86_64\r\n  py30-none-manylinux_2_15_x86_64\r\n  py30-none-manylinux_2_14_x86_64\r\n  py30-none-manylinux_2_13_x86_64\r\n  py30-none-manylinux_2_12_x86_64\r\n  py30-none-manylinux2010_x86_64\r\n  py30-none-manylinux_2_11_x86_64\r\n  py30-none-manylinux_2_10_x86_64\r\n  py30-none-manylinux_2_9_x86_64\r\n  py30-none-manylinux_2_8_x86_64\r\n  py30-none-manylinux_2_7_x86_64\r\n  py30-none-manylinux_2_6_x86_64\r\n  py30-none-manylinux_2_5_x86_64\r\n  py30-none-manylinux1_x86_64\r\n  py30-none-linux_x86_64\r\n  cp38-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n==================\r\n(venv) @hp430:~/my_tensorflow$ python -m pip --version and python -m pip install --upgrade pip && python \r\n-m pip install -vvv tensorflow && python -c \"import tensorflow\"\r\npip 21.1.3 from /home/benh/my_tensorflow/venv/lib/python3.8/site-packages/pip (python 3.8)\r\nUsing pip 21.1.3 from /home/benh/my_tensorflow/venv/lib/python3.8/site-packages/pip (python 3.8)\r\nNon-user install because user site-packages disabled\r\nCreated temporary directory: /tmp/pip-ephem-wheel-cache-xo6vdpue\r\nCreated temporary directory: /tmp/pip-req-tracker-5wc69qjc\r\nInitialized build tracking at /tmp/pip-req-tracker-5wc69qjc\r\nCreated build tracker: /tmp/pip-req-tracker-5wc69qjc\r\nEntered build tracker: /tmp/pip-req-tracker-5wc69qjc\r\nCreated temporary directory: /tmp/pip-install-__l3fu1x\r\nRequirement already satisfied: tensorflow in ./venv/lib/python3.8/site-packages (2.5.0)\r\nRequirement already satisfied: absl-py~=0.10 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.13.0)\r\nRequirement already satisfied: google-pasta~=0.2 in ./venv/lib/python3.8/site-packages (from tensorflow) \r\n(0.2.0)\r\nRequirement already satisfied: gast==0.4.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.4.0)\r\nRequirement already satisfied: typing-extensions~=3.7.4 in ./venv/lib/python3.8/site-packages (from \r\ntensorflow) (3.7.4.3)\r\nRequirement already satisfied: six~=1.15.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.15.0)\r\nRequirement already satisfied: h5py~=3.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.1.0)\r\nRequirement already satisfied: wrapt~=1.12.1 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12.1)\r\nRequirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in ./venv/lib/python3.8/site-packages \r\n(from tensorflow) (2.5.0)\r\nRequirement already satisfied: flatbuffers~=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow) \r\n(1.12)\r\nRequirement already satisfied: termcolor~=1.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied: grpcio~=1.34.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.34.1)\r\nRequirement already satisfied: wheel~=0.35 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.36.2)\r\nRequirement already satisfied: opt-einsum~=3.3.0 in ./venv/lib/python3.8/site-packages (from tensorflow) \r\n(3.3.0)\r\nRequirement already satisfied: protobuf>=3.9.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.17.3)\r\nRequirement already satisfied: astunparse~=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorflow) \r\n(1.6.3)\r\nRequirement already satisfied: tensorboard~=2.5 in ./venv/lib/python3.8/site-packages (from tensorflow) \r\n(2.5.0)\r\nRequirement already satisfied: numpy~=1.19.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.19.5)\r\nRequirement already satisfied: keras-preprocessing~=1.1.2 in ./venv/lib/python3.8/site-packages (from \r\ntensorflow) (1.1.2)\r\nRequirement already satisfied: keras-nightly~=2.5.0.dev in ./venv/lib/python3.8/site-packages (from tensorflow) \r\n(2.5.0.dev2021032900)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (0.4.4)\r\nRequirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (2.26.0)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (1.33.1)\r\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venv/lib/python3.8/site-packages \r\n(from tensorboard~=2.5->tensorflow) (0.6.1)\r\nRequirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (2.0.1)\r\nRequirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (57.4.0)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (1.8.0)\r\nRequirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.8/site-packages (from \r\ntensorboard~=2.5->tensorflow) (3.3.4)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.8/site-packages (from google-\r\nauth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-\r\nauth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-\r\nauth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.8/site-packages (from google-\r\nauth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-\r\nmodules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from \r\nrequests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from \r\nrequests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.6)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from \r\nrequests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.0.3)\r\nRequirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0-\r\n>tensorboard~=2.5->tensorflow) (3.2)\r\nRequirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-\r\noauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\r\nCreated temporary directory: /tmp/pip-unpack-kcbizxz6\r\nRemoved build tracker: '/tmp/pip-req-tracker-5wc69qjc'\r\n2021-07-21 11:16:29.094300: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully \r\nopened dynamic library libcudart.so.11.0\r\n(venv)  @hp430:~/my_tensorflow$\r\n", "comments": ["pip show tensorflow to check install\r\npython test.py to run test\r\n\r\n\r\n\r\nwith python3\r\n\r\npip3 show tensorflow to check install\r\n\r\npython3 test.py to run test\r\n\r\nIf its shows error, this means , Tensorflow hasn't been install\r\n\r\ntest.py\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nc = np.array([[3.,4], [5.,6], [6.,7]])\r\nstep = tf.reduce_mean(c, 1)                                                                                 \r\nwith tf.Session() as sess:\r\n    print(sess.run(step))", "(venv) @hp430:~/my_tensorflow$ pip show tensorflow \r\nName: tensorflow\r\nVersion: 2.5.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/park/my_tensorflow/venv/lib/python3.8/site-packages\r\nRequires: wheel, protobuf, google-pasta, keras-preprocessing, tensorboard, numpy, wrapt, termcolor, grpcio, absl-py, h5py, gast, typing-extensions, flatbuffers, keras-nightly, opt-einsum, astunparse, six, tensorflow-estimator\r\nRequired-by: \r\n\r\n(venv) @hp430:~/my_tensorflow$ pip3 show tensorflow\r\nName: tensorflow\r\nVersion: 2.5.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/park/my_tensorflow/venv/lib/python3.8/site-packages\r\nRequires: six, gast, grpcio, numpy, tensorboard, keras-nightly, termcolor, flatbuffers, absl-py, wheel, protobuf, wrapt, opt-einsum, keras-preprocessing, tensorflow-estimator, google-pasta, h5py, astunparse, typing-extensions\r\nRequired-by: \r\n\r\n\r\n\r\n(venv) park@hp430:~/my_tensorflow$ cat test.py\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nc = np.array([[3.,4], [5.,6], [6.,7]])\r\nstep = tf.reduce_mean(c, 1)\r\nwith tf.Session() as sess:print(sess.run(step))\r\n(venv) park@hp430:~/my_tensorflow$  python3 test.py \r\n2021-07-21 23:27:14.671800: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-21 23:27:15.722597: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.0/targets/x86_64-linux/lib\r\n2021-07-21 23:27:15.722637: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-07-21 23:27:15.722659: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hp430): /proc/driver/nvidia/version does not exist\r\n2021-07-21 23:27:15.722971: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 6, in <module>\r\n    with tf.Session() as sess:print(sess.run(step))\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\n\r\n\r\n", "https://heads0rtai1s.github.io/2021/02/25/gpu-setup-r-python-ubuntu/\r\nafter following above instructions, I got new errors.\r\n\r\n(venv) park@hp430: ~ / my _ tensorflow$ python3 test.py \r\n2021-07-22 00:35:06.643593: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-22 00:35:08.842257: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-07-22 00:35:08.958604: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-07-22 00:35:08.958676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hp430): /proc/driver/nvidia/version does not exist\r\n2021-07-22 00:35:08.960287: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 6, in <module>\r\n    with tf.Session() as sess:print(sess.run(step))\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\n(venv) park@hp430: ~ / my _tensorflow$  ", "@jhparkwife ,\r\n\r\nPlease try with cuda 11.2 for TF 2.5 and let us know if you are facing same error.Please take a look at [https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu).\r\nAlso have a look at this [link](https://github.com/tensorflow/tensorflow/issues/34658#issuecomment-559511245) for **AttributeError: module 'tensorflow' has no attribute 'Session'**.Thanks!", "k@hp430:~/my_tensorflow$ python -c 'import TensorFlow as tf; print(tf.__version__)'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'TensorFlow'\r\n--- I guess my hp430 does not have any NVIDIA gpu. I guess I need to downgrade my installs (linux & TF) according to my hp 430. Thanks for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50880\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50880\">No</a>\n", "I guess I need to downgrade my installs (linux & TF) according to my hp 430. Thanks for your help.", "@jhparkwife ,\r\n\r\nGlad the suggestion worked for you,Thanks!"]}, {"number": 50879, "title": "2.6rc2 cherry-pick request: Fix issue where disabling TensorFloat-32 had no effect.", "body": "This fixes a bug where disabling TensorFloat-32 with `tf.config.experimental.enable_tensor_float_32_execution(False)` had no effect.", "comments": []}, {"number": 50878, "title": "[CherryPick2.6]Fix segfault on string tensors with mismatched dimensions", "body": "PR #50508: [Go] Fix segfault on string tensors with mismatched dimensions\r\n\r\nPiperOrigin-RevId: 384557722", "comments": []}, {"number": 50877, "title": "Backport 'Make element_shape_except_first_dim as a local variable instead of in\u2026' to r2.4", "body": "Commit 603d88d fixes a data race bug in TensorListConcat op, which is incoporated into v2.5.0 and later versions, but not backported to 2.4.x versions. This PR is directly performed using\r\n\r\n`git cherry-pick 603d88d72c1f739cc46fb250edfc9924d9517b17`", "comments": []}, {"number": 50876, "title": "[ROCm] Proper fix for XLA unit test seg fault issue.", "body": "This was root caused to be a divergence between the CUDA and HIP\r\nconv_canonicalizer implementation.", "comments": ["/cc @cheshire @jurahul \r\n\r\nI found the root cause of the seg fault with the original change in gpu_compiler.cc.\r\n\r\nThis PR fixes the seg fault and restores that original change.\r\n"]}, {"number": 50875, "title": "Support per-channel quantized INT8 weights unpacking in XNNPACK delegate", "body": "This MR expands INT8 weights unpacking for FP32 inference in XNNPACK delegate support added in [the previous MR](https://github.com/tensorflow/tensorflow/pull/48751) for per-channel dynamic range quantized model. Previous changes have only supported per-tensor quantization mode, which is obsolete in recent TensorFlow releases.\r\nI have not added proper testing yet, because I'd like to get some suggestions from a maintainer, i.e. the best way to organize such testing while keeping the codebase clean (specifically, how to change testers). Thank you in advance!", "comments": ["@multiverse-tf Could you review this please?", "@dev0x13  Can you please resolve conflicts? Thanks!", "@gbaned Done", "@multiverse-tf Could you review this please?", "It's been more than a month since I've created this MR. Should I close it now due to the lack of review?", "> It's been more than a month since I've created this MR. Should I close it now due to the lack of review?\r\n\r\nSorry for not catching this review request earlier. In the past month, I think we've also added native per-channel quantized INT8 op support in XNNPACK. @Maratyszcza, could you shed more light on the support? Thx", "@multiverse-tf XNNPACK does not support per-channel dynamic range quantization. This MR adds the support.", "Closing due to the lack of response from maintainers.", "@Maratyszcza could you help with this review? Thx!", "@multiverse-tf @Maratyszcza I added test cases for channel-wise INT8 quantization, so this MR is finalized and only review is required for now.\r\nA little reminder on the MR purpose: it adds support for per-channel (channel-wise) dynamic-range INT8 quantized models delegation to XNNPACK.", "@multiverse-tf @Maratyszcza Could you review this please? Thanks in advance!", "@multiverse-tf @Maratyszcza Could you review this please? This MR is ready to be merged, only review is needed.", "@Maratyszcza Thank you for the review! All the changes are made.", "@Maratyszcza Thank you! All the suggested changes are made.", "LGTM. @gbaned could you merge?", "I noticed that the PR was rolled back right after merge. Is there something wrong with it? ", "`transpose_conv_test` failed under sanitizers with `UndefinedBehaviorSanitizer: float-cast-overflow tensorflow/lite/delegates/xnnpack/test_util.cc:32:28`", "@Maratyszcza Fixed this in [my fork](https://github.com/dev0x13/tensorflow/commit/bc81bf3c74c47bb07aea6c2d2dbb1887d83d71a9), but it seems that this PR cannot be reopened. What do I need to do in order to submit this fix to make my changes merged again? Thank you in advance!", "@Maratyszcza I am really sorry for bothering you, but could you clarify what I need to do in order to submit this fix to make my changes merged again?", "`prelu_test` fails under asan too, with the same error", "@Maratyszcza [Fixed](https://github.com/dev0x13/tensorflow/commit/efa0a06559b2f980fc967a9dde8338997aadb8ac)", "Re-landed in a6d352f031f272972da0e6271de07e8b40cac74a", "Thank you!"]}, {"number": 50874, "title": "Allow use of non-system compiler to build mlir_generated tests", "body": "Allow setting of LD_LIBRARY_PATH to reach execution environment of tf_to_kernel so it can load correct version of libstdc++ and so avoid build failure when built with non-system gcc\r\nFixes #50873 ", "comments": ["@elfringham  Can you please resolve conflicts? Thanks!", "This appears to have been made moot by the merge of #44549"]}, {"number": 50873, "title": "MLIR tests fail to build with non-system gcc and later glibc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RTHEL 8.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\nRunning unit tests //tensorflow/core/kernels/... fails to build on a system with a later gcc and glibc as LD_LIBRARY_PATH is ignored so incorrect glibc is attempted to be used when executing tf_to_kernel binary.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n$ export PATH=/usr/local/bin:$PATH\r\n$ export LD_LIBRARY_PATH=/usr/local/lib64:/usr/local/lib\r\n$ bazel test --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=noaws --config=nogcp --config=nonccl --verbose_failures //tensorflow/core/kernels/...\r\n\r\n\r\n**Any other info / logs**\r\n================================================================================\r\nERROR: /home/builder/1/tensorflow_build/tensorflow-2.5.0/tensorflow/core/kernels/mlir_generated/BUILD:1071:19: compile tensorflow/core/kernels/mlir_generated/rsqrt_cpu_f64_f64_kernel_generator_kernel.o failed (Exit 1): tf_to_kernel failed: error executing command \r\n  (cd /home/builder/.cache/bazel/_bazel_builder/d307c59d5864ee3d13b4b70232d700ac/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes=256' '--arch=' '--input=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/rsqrt_cpu_f64_f64.mlir' '--output=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/rsqrt_cpu_f64_f64_kernel_generator_kernel.o' '--enable_ftz=False' '--cpu_codegen=True')\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /home/builder/.cache/bazel/_bazel_builder/d307c59d5864ee3d13b4b70232d700ac/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/../../../../../_solib_k8/_U_S_Stensorflow_Scompiler_Smlir_Stools_Skernel_Ugen_Ctf_Uto_Ukernel___Utensorflow/libtensorflow_framework.so.2)\r\nINFO: Elapsed time: 6.145s, Critical Path: 2.14s\r\nINFO: 10 processes: 4 internal, 6 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Similar to issue #46549 which was fixed by PR #46550", "Fixed by the merge of #44549", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50873\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50873\">No</a>\n"]}, {"number": 50871, "title": "AttributeError: type object 'IteratorBase' has no attribute 'from_structure'", "body": "\uff54\uff45\uff4e\uff53\uff4f\uff52\uff46\uff4c\uff4f\uff57\u3000\uff12\uff0e\uff15\r\n```\r\n\r\n`\r\nimport tensorflow as tf\r\n\r\ntraining_dataset = tf.data.Dataset.range(100).map(\r\n    lambda x: x + tf.random.uniform([], -10, 10, tf.int64))\r\nvalidation_dataset = tf.data.Dataset.range(50)\r\n\r\n# A reinitializable iterator is defined by its structure. We could use the\r\n# `output_types` and `output_shapes` properties of either `training_dataset`\r\n# or `validation_dataset` here, because they are compatible.\r\niterator = tf.data.Iterator.from_structure(training_dataset.output_types,\r\n                                           training_dataset.output_shapes)\r\nnext_element = iterator.get_next()\r\n\r\ntraining_init_op = iterator.make_initializer(training_dataset)\r\nvalidation_init_op = iterator.make_initializer(validation_dataset)\r\n\r\n# Run 20 epochs in which the training dataset is traversed, followed by the\r\n# validation dataset.\r\nfor _ in range(20):\r\n    # Initialize an iterator over the training dataset.\r\n    sess.run(training_init_op)\r\n    for _ in range(100):\r\n        sess.run(next_element)\r\n\r\n    # Initialize an iterator over the validation dataset.\r\n    sess.run(validation_init_op)\r\n    for _ in range(50):\r\n        sess.run(next_element)`\r\n```", "comments": ["Please tell me how to revise this piece of code and let it run with tf 2.5? The sooner the better, thanks a million.", "@saikumarchalla @ry @jmhodges @eggie5 ", "@saikumarchalla Is there any update?", "@Saduf2019 Any progress yet", "@Saduf2019 Any progress yet", "\uff20Saduf2019 Don't you think this is an important issue that should be fixed immediately. I've issued this for over three weeks, why there's no reply at all.", "Hi! @sjtusmartboy !Was able to resolve the issue with TF v2.5 ,  I changed the code using migration document . find the[ gist](https://colab.research.google.com/gist/mohantym/a8b6faa09656636c48c3ff84f096beab/50871.ipynb#scrollTo=gzq4U3SpmlQT) here Thanks!", "@mohantym thanks for your reply. If we're using eager mode, then how do we use tf.data.Iterator", "Hi @sjtusmartboy ,\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade code base to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50871\">No</a>\n"]}, {"number": 50870, "title": "Clarity issues in the mixed-precision execution guide", "body": "## URL(s) with the issue:\r\n\r\nIn particular this point: https://github.com/tensorflow/docs/blame/master/site/en/guide/mixed_precision.ipynb#L817\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nGiven linked summary point:\r\n\r\n>   If your model ends in softmax, make sure it is float32. And regardless of what your model ends in, make sure the output is float32.\r\n\r\nHow do exactly those sentences make sense together?\r\n\r\nIf \"output\" is a result of an arbitrary calculation then why mention specific softmax at all?\r\nIf \"output\" is a variable then it was mentioned before that all variables should be float32, and pointing at the end specifically doesn't make much sense either.\r\n\r\nI find this part of the summary very confusing.\r\n\r\nAs a side note, if there was any reference to explain why despite float16 calculation it still makes sense to store the\r\noutput in float32 variables that would also be very useful. I.e., why it helps with numeric precision exactly?. I mean, low-resolution operation presumably results in a low-resolution result, therefore how is it not wasteful to store such result in a higher resolution \"container\"? Clearly, it is not wasteful because \"everyone\" does it.", "comments": ["@Antymon ,\r\n\r\nCan you please take a look at this [link](https://www.tensorflow.org/guide/mixed_precision) for information on mixed-precision.It helps.Thanks", "@tilakrayal Sorry, but I don't follow the suggestion - the issue is precisely on this piece of text, it quotes pieces of it.", "Maybe the reason is that they want the _input_ to the softmax to be float32 because of the loss of precision associated with exponentiation. (I don't work here, I was just looking for someone to answer my own issue because mine isn't being answered.)", "@Antymon \r\nThe answer in the notebook, as it says calculating loss  with float 16 precision may result in numeric loss.\r\nThis is not a bug or feature request, for any further queries you may open this issue in tf discussion forum as there is a larger community there. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50870\">No</a>\n"]}, {"number": 50869, "title": "copies data from gpu to cpu", "body": "I learned that the GPU ops' running code is in` /lite/delegates/gpu/cl/api.cc`.The code for Run() is as follows:\r\n```\r\nabsl::Status Run() override {\r\n#ifdef CL_DELEGATE_ALLOW_GL\r\n    if (gl_interop_fabric_) {\r\n      RETURN_IF_ERROR(gl_interop_fabric_->Start());\r\n    }\r\n#endif\r\n    for (const auto& input : inputs_) {\r\n      RETURN_IF_ERROR(input->CopyFromExternalObject());\r\n    }\r\n\r\n    RETURN_IF_ERROR(RunWithoutExternalBufferCopy());\r\n\r\n    bool has_async_copies = false;\r\n    for (const auto& output : outputs_) {\r\n      RETURN_IF_ERROR(output->CopyToExternalObject());\r\n      if (output->def().external_def.object_def.object_type ==\r\n          ObjectType::CPU_MEMORY) {\r\n        has_async_copies = true;\r\n      }ng \r\n    }\r\n#ifdef CL_DELEGATE_ALLOW_GL\r\n    if (gl_interop_fabric_) {\r\n      RETURN_IF_ERROR(gl_interop_fabric_->Finish());\r\n    }\r\n#endif\r\n    if (has_async_copies) {\r\n      RETURN_IF_ERROR(queue_->WaitForCompletion());\r\n    }\r\n    return absl::OkStatus();\r\n  }\r\n```\r\nI found that the outputs_ has only one element,and the elapsed time of CopyToExternalObject() depends on the number of ops running on the GPU.In other words,the more ops are deployed on GPU,the longer the time will be.\r\nIt doesn't make sense.Normally,we only need to copy the last op's data.I wonder why this is the case.\r\nThanks in advance!", "comments": ["@xmy19029 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the TensorFlow version you are using.Thanks!", "@tilakrayal Hi,the version is 2.4.2.\r\nI got all the code from tensorflow.All I did is measuring the elapsed time of CopyToExternalObject(),which is in /tensorflow/lite/delegates/gpu/cl/api.cc.\r\nThanks for your reply!", "@xmy19029 ,\r\n\r\nIn order to reproduce the issue,can you please provide the exact link for the code or colab gist.It helps to debug the issue in our environment.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50868, "title": "[CherryPick2.6]Prevent infinite loop in strided slice implementation", "body": "Add test to prevent regression\n\nPiperOrigin-RevId: 385190237\nChange-Id: Ie274ffc8e3f1cc5f50760bcebc2242ca824fe85c", "comments": []}, {"number": 50867, "title": "[CherryPick2.6]Prevent a division by 0 in average ops.", "body": "PiperOrigin-RevId: 385184660\nChange-Id: I7affd4554f9b336fca29ac68f633232c094d0bd3", "comments": []}, {"number": 50866, "title": "[CherryPick2.6]Prevent a division by 0 in division ops.", "body": "PiperOrigin-RevId: 385223169\nChange-Id: Ia4228960b5d2aa44480385f74bdd70d21a3613c3", "comments": []}, {"number": 50865, "title": "[CHerryPick2.6]Fix a null pointer exception caused by branching on uninitialized data.", "body": "This is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.\n\nPiperOrigin-RevId: 385173491\nChange-Id: I8fc476c4b274fdb21ba741caa0fbc6d1b8840663", "comments": []}, {"number": 50864, "title": "[CherryPick2.6]Fix a null pointer exception caused by branching on uninitialized data.", "body": "This is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.\n\nPiperOrigin-RevId: 385168337\nChange-Id: I28661e4f12ba1c92cfeae23d22a3fb2df2a2c6a4", "comments": []}, {"number": 50863, "title": "[CherryPick2.6]Fix a null pointer exception caused by branching on uninitialized data.", "body": "This is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.\n\nPiperOrigin-RevId: 385163909\nChange-Id: I2beb8d50649b6542db224c163033fbcbaa49314f", "comments": []}, {"number": 50862, "title": "[CherryPick:2.6]Fix a null pointer exception in SVDF", "body": "This is due to not checking that `GetVariableInput` returns non-null tensor.\n\nAlso fix a potential null pointer exception in `GetVariableInput`.\n\nPiperOrigin-RevId: 385160147\nChange-Id: Iadf3f0705b036a9014d27caa5a8bbd91f4c4c401", "comments": []}, {"number": 50861, "title": "[CherryPick2.6]Prevent division by 0 in `fully_connected.cc`", "body": "PiperOrigin-RevId: 385137282\nChange-Id: If201e69b6e0048f0be001330b4b977e2b46db2cb", "comments": []}, {"number": 50860, "title": "Prediction depends on Batch size", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: https://colab.research.google.com/drive/1moHCzJlvoc4KXdOFpwQTqBaQLJ9paeHi?usp=sharing\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n-   **TensorFlow version (use command below)**: TF 2.5.1\r\n-   **Python version**: 3.6\r\n-   **GPU model and memory**: NVIDIA 3080 10GB\r\n\r\n### Describe the problem\r\n\r\nI have trained a model for semantic segmentation in Tensorflow. I want to run the predictions of a set of 64 new images (they are greyscale images, therefore the number of channels is 1). For this, I wrote two methods and compare the results (I initially expected that they would provide the same value):\r\n\r\nMethod A: Load all the images into a 4D array (NB_IMAGES, ROWS, COLS, 1). I run the prediction in the entire 4D array, i.e., all images at the same time (Batch size=64).\r\n\r\nMethod B: I run the predictions in each image separately. Each image is loaded as a 4D array of the shape (1, ROWS, COLS, 1). Batch size=1.\r\n\r\nAfter running it in colab and displaying the prediction of the third class for the first image: \r\n\r\n```\r\nMethod A:\r\n[[0. 0. 0. ... 0. 0. 0.]\r\n [0. 0. 0. ... 0. 0. 0.]\r\n [0. 0. 0. ... 0. 0. 0.]\r\n ...\r\n [0. 0. 0. ... 0. 0. 0.]\r\n [0. 0. 0. ... 0. 0. 0.]\r\n [0. 0. 0. ... 0. 0. 0.]]\r\n\r\nMethod B\r\n[[0.02932691 0.01693235 0.0095239  ... 0.33940026 0.14944448 0.1266723 ]\r\n [0.00729698 0.00919143 0.00475383 ... 0.21612257 0.05683496 0.05206718]\r\n [0.00214194 0.00178528 0.0010777  ... 0.35763222 0.11783648 0.05261206]\r\n ...\r\n [0.00742953 0.00188467 0.00282917 ... 0.04309301 0.01660612 0.01606987]\r\n [0.01856694 0.00265247 0.00360173 ... 0.03587637 0.01677576 0.02269481]\r\n [0.04196882 0.01845963 0.01436812 ... 0.07170304 0.09968089 0.16050124]]\r\n```\r\n\r\nHowever, as you can see in the colab attached, thre predictions differ greatly. \r\nI'm really lost about these differences in the predictions, considering that the model is the same, no batch_normalization was used and the data set is also the same. Does anybody know or have any experience regarding the effect of batch size and prediction?\r\n\r\n### Source code / logs\r\nhttps://colab.research.google.com/drive/1moHCzJlvoc4KXdOFpwQTqBaQLJ9paeHi?usp=sharing\r\n", "comments": ["@pedrogalher \r\n\r\nIt looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](https://github.com/keras-team/keras/issues) repository instead. As previously [announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) all future development of Keras is expected to happen in the keras-team/keras repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50860\">No</a>\n"]}, {"number": 50859, "title": "Bug with optimizer_v2.OptimizerV2.set_weights method ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n None\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux (Ubuntu/Debina)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- NA\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\n- Python version: 2.4.1 & 2.4.x\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n11.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n2.4.1\r\n\r\n**Describe the current behavior**\r\nI am seeing an issue in setting Adam optimizer weights in Optimizer v2 implementation. Looking at the code here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/adam.py#L154-L160, it seems to be able to set the weights the current optimizer must have been initialized and set the right size in terms of array length. If this assertion is correct setting weights `on_train_begin` or prior to the first gradient calculation is a bit too soon to set optimizers weights. \r\nIn earlier versions, this was not a problem, and optimizer set_weights use to work at `on_train_begin`.  \r\nI suspect this is a bug in how slots and weights are internally structured.\r\n\r\nCode to reproduce the issue is here: \r\nhttps://gist.github.com/suneeta-mall/b6086a6b252075f48936a5f80f040b77\r\nTo run try:\r\n```\r\npython debug_optimizer_issue.py\r\n```\r\n& then try again:\r\n```\r\npython debug_optimizer_issue.py --start_epoch 2\r\n```\r\n\r\nThe first run finishes and the second one, where its suppose to start from epoch 2,  fails with error as:\r\n```\r\nValueError: You called `set_weights(weights)` on optimizer Adam with a  weight list of length 9, but the optimizer was expecting 0 weights. Provided weights: [120, array([[ 0.0000000e+00,  0.0000000e+00,  0.0...\r\n```\r\n\r\nI looked around in Adam V2 code to see if there are any other APIs that can help with creating slots etc but I didn't see any that does not require knowledge of internal implementation details (I think). \r\n\r\nAs a crazy thought, I use array extension on actual weights to bypass the bug in set_weights like this:\r\n```\r\nmodel.optimizer.weights.extend(_ref_model.optimizer.get_weights())\r\n```\r\nThis lets past the training regime, but the next model save errors into:\r\n```\r\nAttributeError: 'numpy.int64' object has no attribute 'name'\r\n```\r\n\r\nWorth noting, the same problem exists with SGD, which is met with the following related error:\r\n```\r\nValueError: You called `set_weights(weights)` on optimizer SGD with a  weight list of length 1, but the optimizer was expecting 0 weights. Provided weights: [120]...\r\n```\r\n\r\n*Please note*: This code is written for TF2, and will not work as in TF1. Also, in earlier versions of TF 2.1 support for loading h5 weights in distributed scope was not there. In that case, a callback was necessary to set the weights, for more see here https://github.com/tensorflow/tensorflow/issues/30850. \r\n\r\n\r\nAlso worth noting that, I appreciate that the instead of setting weights, I can just load the model like following:\r\n```\r\n        if args.start_epoch:\r\n            model = load_model(f\"model_{args.start_epoch}.h5\")\r\n            # deduced_epoch = int(_ref_model.optimizer.iterations.numpy() // (len(train_images) / args.batch_size))\r\n            # if deduced_epoch != args.start_epoch:\r\n            #     raise ValueError()\r\n            # model.set_weights(_ref_model.get_weights())\r\n            # model.optimizer.set_weights(_ref_model.optimizer.get_weights())\r\n```\r\nand that works fine. But for the use case I have I need to set the weights and optimizer weights explicitly. \r\n\r\n**Describe the expected behavior**\r\nI expect the set_weights to work naturally. \r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@suneeta-mall \r\n\r\nCould you please share the colab gist with all the dependencies to analyse the issue better.Thanks", "@UsharaniPagadala  https://colab.research.google.com/drive/1x03x1trJQcOXG07otYnLyyFjCJSxjUG-?usp=sharing ", "@suneeta-mall \r\n\r\nCould you please refer the similar issues [#7229](https://github.com/keras-team/keras/issues/7229 ), [#30850](https://github.com/tensorflow/tensorflow/issues/30850) and also refer this [optimizer_v2.py Line 989](https://github.com/tensorflow/tensorflow/blob/bd6b557c02a5cc1d094a7bb180b9779121a58520/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L989) and let us know if it helps.Thanks", "Hi @UsharaniPagadala,\r\nThanks for looking into this issue. IMU, reference of https://github.com/keras-team/keras/issues/7229 is not directly related here as I don't have a problem in parameter weight setting. \r\n\r\nThe problem is specific to optimizers' weights. I am aware of the length check code in V2 Optimizer here https://github.com/tensorflow/tensorflow/blob/bd6b557c02a5cc1d094a7bb180b9779121a58520/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L988 that you have linked. I am also aware that in V2 the initialization does not happen until the first step is made. \r\n\r\nHowever, I am unclear what you are suggesting. Are you saying this is by design or suggesting a solution? \r\n### This cant be by design\r\nTo be able to cleanly restart a training, optimizers' weights need to be loaded before the first step and not after. So, IMU, the optimizers.set_weights() call should be able to handle the loading of weights beyond current weight length & source weight length equality restriction.  Whether that's done implicitly in set_weights call or another alternative simple initialization API is provided so that caller can initialize optimizer.param capacity to hold incoming weights. Otherwise, there is no way to restart training using optimizer weights.\r\nStressing to the point that this use to work as expected in V1 optimizers. \r\n\r\n### Possible workaround \r\nMy apologies if I missed but I am not seeing a workaround suggested here. Like I said earlier, setting after a step has been made is a tad bit too late to set optimizers. I also don't see callbacks that may be of use in this case but did not find any. \r\n ", "@suneeta-mall \r\n\r\nI was able to reproduce the code shared in tf2.5 with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/45e8e99b7870e1f81b115d88c7f72ed3/tf-bug-50859-v2optimizer.ipynb) and let us know if it helps.Thanks", "@UsharaniPagadala That's excellent .. thanks for testing it for now .. any chance we backport the fix to 2.4? \r\n ", "@suneeta-mall \r\n\r\nWe don't frequently backport the fix to the previous versions. Recommend you to upgrade to the latest stable Version.Thanks\r\n\r\n", "@suneeta-mall \r\nCould you please move this to closed status as it is resolved.Thanks", "@UsharaniPagadala I disagree that its fixed in 2.5. I tried and I saw the same issue on 2.5 then had a look at your notebook and can see you missed running the actual step tat restarts the training .. \r\nsee here https://colab.research.google.com/gist/UsharaniPagadala/45e8e99b7870e1f81b115d88c7f72ed3/tf-bug-50859-v2optimizer.ipynb#scrollTo=YXqWUOIpg44F\r\n\r\nYou are missing this last step `do_train(2, batch_size)` that also fails on 2.5", "@suneeta-mall \r\nCould you refer to [this issue](https://stackoverflow.com/questions/64183141/valueerror-you-called-set-weightsweights-on-optimizer-rmsprop-with-a-weight) with similar error and let us know.", "@Saduf2019 Are you talking about `load_model` with compile=True? I am not sure if thats a solution as the author itself says it works and it also does not? Nevertheless, my usecase is slightly different so I need a way to set optimizer weights. \r\n\r\nbtw I dont see any `_make_train_function` function. \r\n\r\nDo you not agree that this is a bug in V2 Optimizer that needs fixing? ", "I am transferring your issue to keras-team/keras repo. We can continue discussion on above thread.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50859\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50859\">No</a>\n", "Seems the issue in keras-team/keras repo https://github.com/keras-team/keras/issues/15298 is closed without fix. Is there a plan to fix this behavior?"]}, {"number": 50858, "title": "[Cherrypick:2.6]Disallow dims input of 0 in tf.raw_ops.UnravelIndex", "body": "PiperOrigin-RevId: 384284198\nChange-Id: Ia1804ef1aec57b4d857ea507e6891bcccde18e9b", "comments": []}, {"number": 50856, "title": "bug", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 50855, "title": "Fix numerical stability issue in binary search", "body": "Binary search algorithm implementation can result in an overflow caused by a variable that holds the index of the middle element of the input array. If the number of elements is very large, the operation left + right for calculating the middle index has a risk of overflow. Please refer the following link for more information: https://en.wikipedia.org/wiki/Binary_search_algorithm#Implementation_issues.\r\nmid = left + (right - left + 1)/2 is mathematically equivalent to mid = (left + right +1)/2, but more numerically stable.\r\nmid = left + (right - left + 1)/2 = left + right/2 - left/2 + \u00bd = left/2 + right/2 + \u00bd = (left + right +1)/2.\r\nThis fix follows the implementation in PyTorch.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50855) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n"]}, {"number": 50854, "title": "Gradient not working with tf.math.angle", "body": "I tried to create my own loss function that uses `tf.math.angle` in it. However, when I use this custom loss I get `None` gradients.\r\nI created a minimum working example (link [here](https://colab.research.google.com/drive/1SkomH1STPtHX6GfB8RBDRuu-pK5VkTyk?usp=sharing)).\r\n\r\nThe expected behavior for this example would be the gradient to be zero as the loss is zero so there is no minimizing it.\r\nAnother note is that the loss would be zero for every real number which is what we are going to have using Tensorflow.\r\n\r\nMy motivation is because I work with complex numbers as you can see [here](https://github.com/NEGU93/cvnn). So this loss function will make sense on the complex domain as not every tensor will have the same angle.\r\nThis issue is related to [this](https://github.com/NEGU93/cvnn/issues/12).\r\n\r\n", "comments": ["@Saduf2019 \r\n\r\nI  was able to replicate the issue reported here. Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/7925e19240055a24668f4a29de195583/copy-of-loss_with_angle.ipynb).Thanks\r\n\r\n", "Hi @Saduf2019, @NEGU93:\r\n\r\nI tried out the code in @UsharaniPagadala's gist--it seems like we're computing `tf.math.angle` of `tf.float32` instances, which requires that an `array_ops.where` operation be done (and it's not useful because the results are always zero anyway). This disconnects the gradient, resulting in None `grads`. \r\n\r\nIf you replace this line:\r\n`angle_diff = tf.math.angle(y_true) - tf.math.angle(y_pred)`\r\nwith this (which we don't want, but for illustration):\r\n`angle_diff = tf.math.angle(tf.complex(y_true, 0.)) - tf.math.angle(tf.complex(y_true, 0))`\r\nwe get tensor gradients.\r\n\r\nFor complex number applications, I assume we have to explicitly declare the `dtype` of the input so that we don't run into this problem? I'm a new contributor, so let me know if I have this right. Thanks", "@MaanasArora  I tried the change you suggested (using `tf.math.angle(tf.complex(y, 0.))`) in the gist but still have the same error... Can you provide an example?", "@NEGU93 With this code?\r\n\r\n```python\r\ndef mean_squared_angle_err(y_true, y_pred):\r\n    angle_diff = angle_diff = tf.math.angle(tf.complex(y_true, 0.)) - tf.math.angle(tf.complex(y_pred, 0.))\r\n    angle_diff = ((angle_diff + np.pi) % (2*np.pi)) - np.pi # [-pi:pi]\r\n    return tf.math.reduce_mean(angle_diff**2)\r\n```\r\n\r\nSorry, this 'worked' for me but will not return valid losses, I just provided it for illustration.", "I am not sure I understand, you said that if I replaced with the line `angle_diff = tf.math.angle(tf.complex(y_true, 0.)) - tf.math.angle(tf.complex(y_true, 0))` we should have gradients but I didn't. Is there something else that should be changed for it to work? Why is it not working for me?\r\n\r\nI also have this [link](https://colab.research.google.com/drive/1EABU8xXjHhnTK8JJe4qNDyYnHVxlATbP?usp=sharing) where I work with complex numbers and didn't work. This was the origin actually of my question which I cleared for giving the example in this issue. \r\n\r\nRegards,", "Sorry, I edited a few typos in the code above. Could you try again?", "If it's the point after the zero I corrected that when I tried it not to have the error about the int32. Still, I keep having the None gradients.", "Here is the [gist](https://colab.research.google.com/gist/NEGU93/af072c1b50530ad4e868348d09b0f5ea/copy-of-loss_with_angle.ipynb) with the changes. You can see the gradients are NAN.\r\n\r\nI believe the gradients should be zero, as the complex numbers have the same angle, right?", "I just meant that the gradients become non-None python values--if you want not NAN numpy values you can replace the 0s by 1s. The angle for a complex number with a zero imaginary component is NAN. This is not a real solution.", "Yes, I changed it to 1 and worked!\r\n\r\nThe angle with a zero imaginary part is NAN? That is a weird behavior. I wonder the reason for that but I guess that is for another issue.", "As for the code in the complex number gist, the model outputs are `float32` instances which explains why their output returns nans. I assume the CVNN library might be using floats to construct complex numbers, in which case you will need to deconstruct the complex number and call `tf.complex`. That's why I suspect the issue lies outside of tensorflow. This illustration will not return valid losses, so it will not really work in code.", "Well, that is a definition, however, atan(b/a) = atan(0/a) = 0 (as says [wolphram](https://www.wolframalpha.com/input/?i=atan%280%29)) and not Nan, the problem would be a zero real part (wich is also bad).\r\nAlso, using that definition will have the problem of -a-ib (third quadrant) will have the angle of a+ib (first quadrant) as -a/-b = a/b right?.\r\n\r\nI would expect a complex number with a zero imaginary part (a+i0) would be on the real axis, therefore, angle = 0.\r\n\r\nMoreover, I tried just computing:\r\n`tf.math.angle(tf.complex(tf.ones((1,1)), 0.))` and it does return `0` and not Nan. \r\n\r\nThe problem was indeed solved as you said but I still fail to understand it.", "Ups, stupid me, I thought inp was ones and not zeros, of course a 0+i0 would be a problem. I got it now! Thank you very much! \r\n\r\nI will close the issue then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50854\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50854\">No</a>\n", "Sorry, opened it again, if I do `tf.math.angle(tf.zeros((1, 1), dtype=np.complex64))` it gives me 0 and not Nan, so apparently, TF has chosen to make 0+i0 angle to be zero (sounds ok and actually a good choice), and therefore the issue it is still not explained for me.", "Sorry, since I am not a member of this repository, my understanding might not be definitive. \r\n\r\nThe `None` gradients resulting from a disconnected graph, caused by `float` inputs to `tf.math.angle`, would be an issue because the loss will not compute. What I can gather is that `nan` numpy values for gradients are acceptable when those gradients are _differentials_ of zero values, and this shouldn't cause an issue when actual complex numbers are inserted.", "No need to go to the repository, that is my job once this is cleared. My question is with the original gist.\r\nWe have the following remarks:\r\n\r\n1. Using `angle_diff = tf.math.angle(tf.complex(y_true, 0.)) - tf.math.angle(tf.complex(y_pred, 0.))` gives nan gradients.\r\n2. Using `angle_diff = tf.math.angle(tf.complex(y_true, 1.)) - tf.math.angle(tf.complex(y_pred, 1.))` appears to work.\r\n\r\nThe reason behind this is still unknown to me. even if `y_true` and/or `y_pred` is zero, tensorflow appears to return a `0` angle and not `Nan` as one can see by calling `tf.math.angle(tf.zeros((1, 1), dtype=np.complex64))`. So why would I have nan gradients? \r\nWhy does `0+i0` break the gradients if tensorflow appears to interpret the angle of `0+i0` to equal `0`?", "Shall I close this and open another issue asking why the gradient of `0+i0` is `nan` or we continue the chat in this one? I still don't get why\r\n\r\n> `nan` numpy values for gradients are acceptable when those gradients are _differentials_ of zero values, and this shouldn't cause an issue when actual complex numbers are inserted.\r\n\r\n", "@NEGU93 \r\nTf.math angle is configured such that it will return 0 instead of nan, the operation must support gradients which is not possible if the value becomes nan or inf.", "Sorry for this. But I still don't get it.\n\n1. Gradients do not work if operation returns nan of inf\n2. This case is coded to return 0, so no nan nor inf.\n\nThen what is the problem?", "@NEGU93 \r\nThat is what it is supposed to do tf.math.angle must support gradients and since gradients are not defined for inf and nan. Kindly move this to closed status and open an issue on tf discussion group for any further queries as this is not a bug or feature request and there is a larger community there to respond. Thanks!", "Ok. I will do that. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50854\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50854\">No</a>\n"]}, {"number": 50852, "title": "Merge pull request #1 from tensorflow/master", "body": "update", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50852) for more info**.\n\n<!-- need_sender_cla -->", "Closing this PR due to no files changed. Thanks!"]}, {"number": 50851, "title": "[MLIR][TF] Canonicalize tf.DivNoNan and tf.MulNoNan", "body": "Added and used a template to canonicalize tf.DivNoNan and tf.MulNoNan.\r\nWhen the divisor/multiplier of tf.DivNoNan/tf.MulNoNan, respectively is\r\na constant with all the elements in its tensor (with float/complex type)\r\nbeing zero, tf.DivNoNan/tf.MulNoNan is replaced with its\r\ndivisor/multiplier, respectively. Further, when the divisor/multiplier\r\nof tf.DivNoNan/tf.MulNoNan, respectively is a constant with all the\r\nelements in its tensor (with float/complex type) being non-zero,\r\ntf.DivNoNan/tf.MulNoNan is replaced with tf.Div/tf.Mul, respectively.\r\nRelevant test cases are added as well.\r\n\r\nSigned-off-by: Srishti Srivastava <srishti.srivastava@polymagelabs.com>", "comments": ["@joker-eph, pinging you for reviewing this. Thanks!", "I am not sure it merged the most recent version of the code, you likely should double check this.", "> I am not sure it merged the most recent version of the code, you likely should double check this.\r\n\r\nIt did merge the most recent version!"]}, {"number": 50850, "title": "GazeML upgrade problem TF 1.4.0 to TF 2.5 (tf.contrib.layers.batch_norm)", "body": "ubnutu 20.04\r\nTensorflow 2.5\r\n\r\nThis is an open source GazeML from github. I am trying to modify this code to tensorflow 2.5 but have some problem in GazeML/src/models/elg.py \r\n\r\n  def _apply_bn(self, tensor):\r\n        return tf.contrib.layers.batch_norm(\r\n            tensor,\r\n            scale=True,\r\n            center=True,\r\n            is_training=self.use_batch_statistics,\r\n            trainable=True,\r\n            data_format=self._data_format,\r\n            updates_collections=None,\r\n        )\r\n\r\nAttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'\r\n\r\ni tried \r\ndef _apply_bn(self, tensor):\r\n        return tf.keras.layers.LayerNormalization(\r\n            tensor,\r\n            scale=True,\r\n            center=True,\r\n            is_training=self.use_batch_statistics,\r\n            trainable=True,\r\n            data_format=self._data_format,\r\n            updates_collections=None,\r\n        )\r\n\r\n File \"/home/user/GazeML/src/models/elg.py\", line 178, in _apply_bn\r\n    return tf.keras.layers.LayerNormalization( #tf.contrib.layers.batch_norm(\r\n  File \"/home/user/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization.py\", line 1119, in __init__\r\n    super(LayerNormalization, self).__init__(**kwargs)\r\n  File \"/home/user/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 522, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/user/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\", line 159, in __init__\r\n    generic_utils.validate_kwargs(kwargs, allowed_kwargs)\r\n  File \"/home/user/.local/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 1137, in validate_kwargs\r\n    raise TypeError(error_message, kwarg)\r\nTypeError: ('Keyword argument not understood:', 'is_training')\r\nException ignored in: <function BaseModel.__del__ at 0x7f3a09e553a0>\r\nTraceback (most recent call last):\r\n  File \"/home/user/GazeML/src/core/model.py\", line 110, in __del__\r\n  File \"/home/user/GazeML/src/core/data_source.py\", line 157, in cleanup\r\n  File \"/home/user/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\r\n  File \"/home/user/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1114, in _run\r\nRuntimeError: Attempted to use a closed Session.\r\n\r\nHow do I modify this code?\r\nThank you guys!\r\n", "comments": ["@louis960126 \r\n\r\nCan you please take a look at this [comment](https://stackoverflow.com/questions/55870127/module-tensorflow-has-no-attribute-contrib) with similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]