[{"number": 47327, "title": "Add some more notes for running a binary with renode.", "body": "The additional notes were helpful when I was working on https://github.com/tensorflow/tensorflow/pull/47276", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47326, "title": "Unable to convert mT5 model to tflite (tensorflow.GraphDef exceeds maximum protobuf size of 2GB)", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library : 2.3.1\r\n\r\n### 2. Code\r\n```\r\nimport transformers\r\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, TFAutoModelForSeq2SeqLM\r\nimport tensorflow as tf\r\n\r\nmodel_name = \"google/mt5-base\"\r\nconfig = AutoConfig.from_pretrained(\r\n    model_name\r\n)\r\ntokenizer = AutoTokenizer.from_pretrained(\r\n    model_name\r\n)\r\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(\r\n    model_name,\r\n    from_pt=True,\r\n    config=config\r\n)\r\n\r\ninput_spec = tf.TensorSpec([1, 64], tf.int16)\r\nmodel._set_inputs(input_spec, training=False)\r\nprint(model.inputs)\r\nprint(model.outputs)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.float32\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"mt5_base_en_to_fa.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n### 3. Error Message:\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-78-e62147be515c> in <module>\r\n----> 1 tflite_model = converter.convert()\r\n      2 open(\"mt5_base_en_to_fa.tflite\", \"wb\").write(tflite_model)\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    807     frozen_func, graph_def = (\r\n    808         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n--> 809             self._funcs[0], lower_control_flow=False))\r\n    810 \r\n    811     input_tensors = [\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n   1107 \r\n   1108   frozen_func = _construct_concrete_function(func, output_graph_def,\r\n-> 1109                                              converted_input_indices)\r\n   1110   return frozen_func, output_graph_def\r\n   1111 \r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)\r\n    999   new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n   1000                                                    new_input_names,\r\n-> 1001                                                    new_output_names)\r\n   1002 \r\n   1003   # Manually propagate shape for input tensors where the shape is not correctly\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)\r\n    648     importer.import_graph_def(graph_def, name=\"\")\r\n    649 \r\n--> 650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n    651   import_graph = wrapped_import.graph\r\n    652   return wrapped_import.prune(\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n    626           signature=signature,\r\n    627           add_control_dependencies=False,\r\n--> 628           collections={}),\r\n    629       variable_holder=holder,\r\n    630       signature=signature)\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)\r\n     85 \r\n     86   def __call__(self, *args, **kwargs):\r\n---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n     88 \r\n     89   def call_with_variable_creator_scope(self, fn):\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)\r\n     91     def wrapped(*args, **kwargs):\r\n     92       with variable_scope.variable_creator_scope(self.variable_creator_scope):\r\n---> 93         return fn(*args, **kwargs)\r\n     94 \r\n     95     return wrapped\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()\r\n    646 \r\n    647   def _imports_graph_def():\r\n--> 648     importer.import_graph_def(graph_def, name=\"\")\r\n    649 \r\n    650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)\r\n    403       return_elements=return_elements,\r\n    404       name=name,\r\n--> 405       producer_op_list=producer_op_list)\r\n    406 \r\n    407 \r\n\r\n~/.pyenv/versions/3.7.2/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    492   # _ProcessNewOps.\r\n    493   with graph._mutation_lock():  # pylint: disable=protected-access\r\n--> 494     with c_api_util.tf_buffer(graph_def.SerializeToString()) as serialized:\r\n    495       try:\r\n    496         results = c_api.TF_GraphImportGraphDefWithResults(\r\n\r\nValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 2330463933\r\n```\r\n\r\n### 4. Note:\r\n1. I realized that the suuport for models larger than 2GB is added to ONNX converter ([here](https://github.com/onnx/tensorflow-onnx/pull/1090) ). I was wondering if any similar fix is added for tflite converter as well? If not, how can I overcome this error?\r\n2. I have been to convert \"google/mt5-small\" model to tflite, but not the mt5-base as it is a larger model.\r\n\r\nThanks.", "comments": ["Hi @Arman-IMRSV\r\n\r\nIs it possible to follow this workaround in https://github.com/tensorflow/tensorflow/issues/45041#issuecomment-731268801 ?", "@abattery \r\nI do not believe if it is possible to apply that solution here, as I am loading a pretrained model. Do you have any idea how I can apply that to this?", "@abattery \r\nDo you have any suggested solution? I also had a look at converting ONNX to TFLite. It did not work either. ", "Do you think it is possible to follow the above suggestion in the conversion code from ONNX to TF?", "@abattery\r\nI do not believe so. Again, in ONNX, we have the frozen graph. I do not think if we can apply that solution to this. Is there any other way around?", "@TomWildenhain-Microsoft\r\nIs there a way to add the support for large models to TFLite conversion, similar to what you did for ONNX conversion in [this PR](https://github.com/TomWildenhain-Microsoft)?\r\n", "@abattery\r\nCould you please let me know who I should request help from? Who has been in TFlite implementation to help, please?\r\n", "The way we did this with the onnx converter is a bit of a hack, but it would probably work here.  Just curious, why do you want to convert this model to tflite?  Tflite is designed to run on low-power devices so I'm surprised you'd want to run such a huge model on it.", "@TomWildenhain-Microsoft\r\nI intend to use mt5-base one mobile device for multi-lingual translation task. I know it's pretty big, but I believe I should be able to fit it on a mobile device after compression.\r\n\r\nSo you have any idea how to apply that hack here as well?", "The ONNX-TFLite converter actually creates the corresponding TensorFlow graph from the given original model. Could you file a feature request towards the ONNX-TFLite converter in order to pull out of weights separately and use a saved model format for the weight serialization instead of inlining weights in the operator definitions in a single protobuf file like https://github.com/tensorflow/tensorflow/issues/45041#issuecomment-731268801?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47326\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47326\">No</a>\n"]}, {"number": 47325, "title": "Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string", "body": "Opening a new bug per @nikitamaia's suggestion. See discussion in #28007, in which she confirmed the bug. Bug only presents when using GPU, not CPU.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 2.4.1/8\r\n- GPU model and memory: GeForce GTX 1050 Ti computeCapability: 6.1, 3.95GiB\r\n\r\n**Describe the current behavior**\r\nError during function call for compiled function:\r\n`Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string`\r\n\r\n**Describe the expected behavior**\r\nCompiled function call should succeed like uncompiled function call.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef _get_char_flags(char_tensor):\r\n    char = char_tensor.numpy().decode()\r\n    return char.isalpha(), char.isspace()\r\n\r\n\r\n@tf.function  # <--------------- Runs fine with this line commented out.\r\ndef tf_split_text(text):\r\n    tf.assert_rank(text, 0)\r\n    tf.debugging.assert_type(text, tf.string)\r\n\r\n    chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\r\n    is_alpha, is_space = tf.map_fn(lambda char: tf.py_function(_get_char_flags, char, Tout=[tf.bool, tf.bool]),\r\n                                   (chars,), dtype=tf.bool, parallel_iterations=True,\r\n                                   fn_output_signature=[tf.bool, tf.bool])\r\n\r\n    is_alpha = tf.concat([is_alpha, [False]], axis=0)\r\n    is_space = tf.concat([is_space, [True]], axis=0)\r\n\r\n    is_special = ~(is_alpha | is_space)\r\n    is_non_alpha = ~is_alpha\r\n    is_non_space = ~is_space\r\n\r\n    was_special = tf.concat([[False], is_special[:-1]], axis=0)\r\n    was_non_alpha = tf.concat([[True], is_non_alpha[:-1]], axis=0)\r\n    was_non_space = tf.concat([[False], is_non_space[:-1]], axis=0)\r\n\r\n    any_to_special = is_special\r\n    non_alpha_to_non_space = was_non_alpha & is_non_space\r\n    token_start_flags = any_to_special | non_alpha_to_non_space\r\n    token_start_indices = tf.where(token_start_flags)[:, 0]\r\n\r\n    special_to_any = was_special\r\n    non_space_to_non_alpha = was_non_space & is_non_alpha\r\n    token_end_flags = special_to_any | non_space_to_non_alpha\r\n    token_end_indices = tf.where(token_end_flags)[:, 0]\r\n\r\n    tf.debugging.assert_equal(tf.size(token_start_indices), tf.size(token_end_indices))\r\n\r\n    preceding_space = tf.concat([[False], is_space[:-1]], axis=0)\r\n\r\n    tokens = tf.strings.substr(text, token_start_indices, token_end_indices - token_start_indices, unit='UTF8_CHAR')\r\n    has_preceding_space = tf.gather(preceding_space, token_start_indices)\r\n\r\n    tf.assert_rank(has_preceding_space, 1)\r\n    tf.assert_rank(tokens, 1)\r\n    tf.assert_equal(tf.reduce_sum(tf.map_fn(tf.strings.length, tokens, fn_output_signature=tf.int32)) +\r\n                    tf.reduce_sum(tf.cast(has_preceding_space, tf.int32)),\r\n                    tf.strings.length(text))\r\n    return has_preceding_space, tokens\r\n\r\n\r\ntext = tf.constant('hi there', tf.string)\r\npreceding_spaces, words = tf_split_text(text)\r\nprint(words)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nTraceback:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/hosford42/PycharmProjects/ImageParser/error.py\", line 56, in <module>\r\n    preceding_spaces, words = tf_split_text(text)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 894, in _call\r\n    return self._concrete_stateful_fn._call_flat(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\t [[{{node map_1/TensorArrayUnstack/TensorListFromTensor/_96}}]]\r\n\t [[map_1/while/loop_body_control/_61/_107]]\r\n  (1) Invalid argument:  2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\t [[{{node map_1/TensorArrayUnstack/TensorListFromTensor/_96}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_tf_split_text_265]\r\n\r\nFunction call stack:\r\ntf_split_text -> tf_split_text\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/c035c29422e11b84b99577232d3c160a/47325-2-3.ipynb) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/d6c26cc365d808a42006dfaab1f7a673/47325.ipynb). Whereas with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/345b439ec9d5c7971140885ac9e163ba/47325-tf-nightly.ipynb), Colab doesn't detect GPU. \r\n\r\nPlease check the linked gist for reference. Thanks!", "This prints `tf.Tensor([b'hi' b'there'], shape=(2,), dtype=string)` on tf-nightly, so I'm assuming this is already fixed.\r\n\r\n@hosford42 I'm closing the issue for now, but please reopen if you find that it is not fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47325\">No</a>\n", "@sanjoy, was that when using the GPU? It doesn't occur with CPU. See @amahendrakar's earlier comment:\r\n\r\n> Whereas with TF-nightly, Colab doesn't detect GPU.", "> @sanjoy, was that when using the GPU? It doesn't occur with CPU. See @amahendrakar's earlier comment:\r\n\r\nThis was on my local machine and TF was able to use the GPU.  Can you confirm that you're able to reproduce the problem with tf-nightly?", "I can't install tf-nightly right now. I may be able to get back to you on this in a few days, though. Thanks, all, for looking at this.", "hi @geetachavan1, \r\n\r\nI saw you mark this issue Done in TF2.5.0. \r\nBut I can reproduce it in [TF2.5.0 Colab](https://colab.research.google.com/drive/1p-mEPrW24R7WvcvFa6IZxdA-wqke96RX?usp=sharing).  This issue will be reproduced in TF2.5.0-GPU, But not in TF2.5.0-CPU.\r\nPlease Help. \r\n", "I just tried this in colab with TF2.7 and I do not get the error. I believe this is fixed."]}, {"number": 47324, "title": "Using GridSearchCV in a regression with Keras:  TypeError: cannot pickle '_thread.RLock' object", "body": "I'm trying to use GridSearchCV in a regression with a Keras neural network.\r\nThe data I'm using is the Boston Housing Price dataset, which was loaded directly from keras `boston_housing.load_data()`. The following is a code snippet of I'm trying to do.\r\n\r\n    def build_model():\r\n        model=models.Sequential()\r\n        model.add(layers.Dense(64,activation=\"relu\",\r\n                              input_shape=(train_data_norm.shape[1],)))\r\n        model.add(layers.Dense(64,activation=\"relu\"))\r\n        model.add(layers.Dense(1))\r\n        model.compile(optimizer='rmsprop',loss=\"mse\",metrics=[\"mae\"])\r\n        return model \r\n    \r\n    from sklearn.model_selection import GridSearchCV\r\n    \r\n    from keras.wrappers.scikit_learn import KerasRegressor\r\n    \r\n    model=KerasRegressor(build_fn=build_model(),epochs=30)\r\n    \r\n    param_grid = {\"epochs\":list(range(1,51))}\r\n    \r\n    grid_model=GridSearchCV(model,param_grid,cv=4)\r\n    \r\n    grid_model.fit(train_data_norm, train_targets)\r\n\r\nAnd I get the following error message: \r\n\r\n    TypeError Traceback (most recent call last)\r\n    <ipython-input-185-3fa5fc34b6b0> in <module>\r\n          9 grid_model=GridSearchCV(model,param_grid,cv=4)\r\n         10 \r\n    ---> 11 grid_model.fit(train_data_norm, train_targets)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\r\n         70                           FutureWarning)\r\n         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})\r\n    ---> 72         return f(**kwargs)\r\n         73     return inner_f\r\n         74 \r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self, X, y, groups, **fit_params)\r\n        679         n_splits = cv.get_n_splits(X, y, groups)\r\n        680 \r\n    --> 681         base_estimator = clone(self.estimator)\r\n        682 \r\n        683         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\r\n         70                           FutureWarning)\r\n         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})\r\n    ---> 72         return f(**kwargs)\r\n         73     return inner_f\r\n         74 \r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\base.py in clone(estimator, safe)\r\n         85     new_object_params = estimator.get_params(deep=False)\r\n         86     for name, param in new_object_params.items():\r\n    ---> 87         new_object_params[name] = clone(param, safe=False)\r\n         88     new_object = klass(**new_object_params)\r\n         89     params_set = new_object.get_params(deep=False)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\r\n         70                           FutureWarning)\r\n         71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})\r\n    ---> 72         return f(**kwargs)\r\n         73     return inner_f\r\n         74 \r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\base.py in clone(estimator, safe)\r\n         69     elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\r\n         70         if not safe:\r\n    ---> 71             return copy.deepcopy(estimator)\r\n         72         else:\r\n         73             if isinstance(estimator, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        170                     y = x\r\n        171                 else:\r\n    --> 172                     y = _reconstruct(x, memo, *rv)\r\n        173 \r\n        174     # If is its own copy, don't memoize.\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n        268     if state is not None:\r\n        269         if deep:\r\n    --> 270             state = deepcopy(state, memo)\r\n        271         if hasattr(y, '__setstate__'):\r\n        272             y.__setstate__(state)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n        228     memo[id(x)] = y\r\n        229     for key, value in x.items():\r\n    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n        231     return y\r\n        232 d[dict] = _deepcopy_dict\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n        203     append = y.append\r\n        204     for a in x:\r\n    --> 205         append(deepcopy(a, memo))\r\n        206     return y\r\n        207 d[list] = _deepcopy_list\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        170                     y = x\r\n        171                 else:\r\n    --> 172                     y = _reconstruct(x, memo, *rv)\r\n        173 \r\n        174     # If is its own copy, don't memoize.\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n        268     if state is not None:\r\n        269         if deep:\r\n    --> 270             state = deepcopy(state, memo)\r\n        271         if hasattr(y, '__setstate__'):\r\n        272             y.__setstate__(state)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n        228     memo[id(x)] = y\r\n        229     for key, value in x.items():\r\n    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n        231     return y\r\n        232 d[dict] = _deepcopy_dict\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n        203     append = y.append\r\n        204     for a in x:\r\n    --> 205         append(deepcopy(a, memo))\r\n        206     return y\r\n        207 d[list] = _deepcopy_list\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        170                     y = x\r\n        171                 else:\r\n    --> 172                     y = _reconstruct(x, memo, *rv)\r\n        173 \r\n        174     # If is its own copy, don't memoize.\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n        268     if state is not None:\r\n        269         if deep:\r\n    --> 270             state = deepcopy(state, memo)\r\n        271         if hasattr(y, '__setstate__'):\r\n        272             y.__setstate__(state)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n        228     memo[id(x)] = y\r\n        229     for key, value in x.items():\r\n    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n        231     return y\r\n        232 d[dict] = _deepcopy_dict\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        170                     y = x\r\n        171                 else:\r\n    --> 172                     y = _reconstruct(x, memo, *rv)\r\n        173 \r\n        174     # If is its own copy, don't memoize.\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n        268     if state is not None:\r\n        269         if deep:\r\n    --> 270             state = deepcopy(state, memo)\r\n        271         if hasattr(y, '__setstate__'):\r\n        272             y.__setstate__(state)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n        228     memo[id(x)] = y\r\n        229     for key, value in x.items():\r\n    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n        231     return y\r\n        232 d[dict] = _deepcopy_dict\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        170                     y = x\r\n        171                 else:\r\n    --> 172                     y = _reconstruct(x, memo, *rv)\r\n        173 \r\n        174     # If is its own copy, don't memoize.\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n        268     if state is not None:\r\n        269         if deep:\r\n    --> 270             state = deepcopy(state, memo)\r\n        271         if hasattr(y, '__setstate__'):\r\n        272             y.__setstate__(state)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n        228     memo[id(x)] = y\r\n        229     for key, value in x.items():\r\n    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n        231     return y\r\n        232 d[dict] = _deepcopy_dict\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        170                     y = x\r\n        171                 else:\r\n    --> 172                     y = _reconstruct(x, memo, *rv)\r\n        173 \r\n        174     # If is its own copy, don't memoize.\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n        268     if state is not None:\r\n        269         if deep:\r\n    --> 270             state = deepcopy(state, memo)\r\n        271         if hasattr(y, '__setstate__'):\r\n        272             y.__setstate__(state)\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        144     copier = _deepcopy_dispatch.get(cls)\r\n        145     if copier is not None:\r\n    --> 146         y = copier(x, memo)\r\n        147     else:\r\n        148         if issubclass(cls, type):\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in _deepcopy_dict(x, memo, deepcopy)\r\n        228     memo[id(x)] = y\r\n        229     for key, value in x.items():\r\n    --> 230         y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n        231     return y\r\n        232 d[dict] = _deepcopy_dict\r\n    \r\n    ~\\anaconda3\\envs\\PythonCPU\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n        159                     reductor = getattr(x, \"__reduce_ex__\", None)\r\n        160                     if reductor is not None:\r\n    --> 161                         rv = reductor(4)\r\n        162                     else:\r\n        163                         reductor = getattr(x, \"__reduce__\", None)\r\n    \r\n    TypeError: cannot pickle '_thread.RLock' object\r\n\r\n\r\n", "comments": ["@ anoldmaninthesea \r\n\r\nI ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a29602b436740e214e5416bac43023d6/untitled549.ipynb).\r\nYou may also refer to similar issues: #33204, [link](https://github.com/tensorflow/tensorflow/issues/21485), [link1](https://stackoverflow.com/questions/44144584/typeerror-cant-pickle-thread-lock-objects)", "@Saduf2019  your error comes from not importing keras models and layers.  I solved my problem by removing the `( )` in the `KerasRegressor(build_fn=build_model(),epochs=30)`, by rewriting it into `KerasRegressor(build_fn=build_model,epochs=30)`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47324\">No</a>\n"]}, {"number": 47323, "title": "Getting negative loss function in Autoregressive", "body": "Hi\r\n\r\nI'm using Autoregressive for density estimation, the same example that is written in this page:\r\n\r\nhttps://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/AutoregressiveNetwork?version=nightly\r\n\r\nHowever, when I increase the dimension (event_shape) of the data that I want to find its density, from two to four for example, I start getting negative loss function!\r\n\r\nCan anyone help me with this issue.\r\nThanks\r\n\r\n", "comments": ["@Hakim777 This is more related to TF probability. There is a separate repository for probability [here](https://github.com/tensorflow/probability/issues). \r\n\r\nPlease post this issue in that repo and close here. Experts in that repo will resolve your issue. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47323\">No</a>\n"]}, {"number": 47322, "title": "Value error with DELF", "body": "I use the following code to compute delf,\r\n\r\n```\r\nimport argparse\r\nfrom glob import glob\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\nfrom tqdm import tqdm\r\n\r\ndisable_eager_execution()\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\nclass DeepDELF:\r\n\r\n    def __init__(self, input_path):\r\n        ops.reset_default_graph()\r\n\r\n        m = hub.Module('https://tfhub.dev/google/delf/1')\r\n\r\n        # The module operates on a single image at a time, so define a placeholder to\r\n        # feed an arbitrary image in.\r\n        self.image_placeholder = tf.compat.v1.placeholder(\r\n            tf.float32, shape=(None, None, 3), name='input_image')\r\n\r\n        module_inputs = {\r\n            'image': self.image_placeholder,\r\n            'score_threshold': 100.0,\r\n            'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],\r\n            'max_feature_num': 1000,\r\n        }\r\n\r\n        self.module_outputs = m(module_inputs, as_dict=True)\r\n        self.image_tf = self.image_input_fn(glob(input_path + '/*'))\r\n        self.path_list = glob(input_path + '/*')\r\n\r\n    def extract(self):\r\n        with tf.compat.v1.train.MonitoredSession() as sess:\r\n            results_dict = {}  # Stores the locations and their descriptors for each image\r\n            for image_path in tqdm(self.path_list):\r\n                image = sess.run(self.image_tf)\r\n                print('Extracting locations and descriptors from %s' % image_path)\r\n                results_dict[image_path] = sess.run(\r\n                    [self.module_outputs['locations'], self.module_outputs['descriptors']],\r\n                    feed_dict={self.image_placeholder: image})\r\n            return results_dict\r\n\r\n    def image_input_fn(self, image_files):\r\n        filename_queue = tf.compat.v1.train.string_input_producer(\r\n            image_files, shuffle=False)\r\n        reader = tf.compat.v1.WholeFileReader()\r\n        _, value = reader.read(filename_queue)\r\n        image_tf = tf.image.decode_jpeg(value, channels=3)\r\n        return tf.image.convert_image_dtype(image_tf, tf.float32)\r\n\r\n\r\ndef main(args):\r\n    path = args['input_path']\r\n    extrator = None\r\n    extractor = DeepDELF(path)\r\n    results_dict = extractor.extract()\r\n    results_dict2 = extractor.extract()\r\n    print(\"Shape feature: \", results_dict.keys())\r\n    print(\"Shape feature 2: \", results_dict2.keys())\r\n\r\n\r\ndef args_parser():\r\n    parser = argparse.ArgumentParser(description=\"Methods extract image.\")\r\n    parser.add_argument('-i', '--input_path', \r\n                        help=\"The path of the input image.\")\r\n    return vars(parser.parse_args())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    args = args_parser()\r\n    # End default optional arguments\r\n    # Print info arguments\r\n    print(\"Extract feature from image.\".upper().center(100))\r\n    print(str(\"-\" * 63).center(100))\r\n    print(\"|{:<30}:\\n|{:<30}|\".format(\"Image path\", args['input_path']).center(100))\r\n    print(str(\"-\" * 63).center(100))\r\n\r\n    main(args)\r\n\r\n```\r\n\r\nUnfortunately, it throws the following error\r\n\r\n```\r\n    raise ValueError(not_null_err)\r\nValueError: string_input_producer requires a non-null input tensor\r\n```\r\n\r\nHow can I fix this?", "comments": ["@Zumbalamambo,\r\nTensorFlow Hub issues are tracked in tensorflow/hub repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/hub/issues/new) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 47321, "title": "Keras fit with generators not executing in the main thread when setting workers=0", "body": "Can be reproduced on colab with TensorFlow (`v2.4.1-0-g85c8b2a817f 2.4.1`)\r\n\r\n**Describe the current behavior**\r\n\r\nGenerator code is executed in other threads when using `model.fit`.\r\n\r\nFrom the docs, it looks like that setting workers=0 would execute the generator code in the main thread.\r\n\r\n> workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. **If 0, will execute the generator on the main thread**.\r\n\r\nThis doesn't seem to work though as only the first iterations seems to be executed in the main thread.\r\n\r\nFor example:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\nmodel.compile(loss = \"mse\", optimizer = \"adam\")\r\n\r\ndef gen ():\r\n  for i in range(100):\r\n    print(threading.current_thread())\r\n    yield (tf.random.normal(shape=(100,1)), tf.random.normal(shape = (100,)))\r\n\r\nmodel.fit(gen(), epochs = 1, workers = 0, verbose = 0, steps_per_epoch = 3, max_queue_size=0)\r\n```\r\n\r\nI get:\r\n\r\n```\r\n<_MainThread(MainThread, started 140516450817920)>\r\n<_DummyThread(Dummy-4, started daemon 140514717599488)>\r\n<_DummyThread(Dummy-5, started daemon 140514709206784)>\r\n<tensorflow.python.keras.callbacks.History at 0x7fcc1d7223c8>\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd expect that all calls from the generator to be executed in the main thread. This is problematic because in my use case the generator is not thread safe and crashes the program when executed from other threads. \r\n\r\nNote: I have opened a [SO question](https://stackoverflow.com/questions/66320198/keras-fit-with-generator-function-always-execute-in-the-main-thread) but I feel this behavior should be treated as a bug.\r\n", "comments": ["i am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/821b3a7d1b9bfe7978d4f48410592fbf/untitled.ipynb).", "Hi,\r\n\r\nThanks for reporting this issue. The documentation appears to be outdated: the generator would only execute on the main thread in graph-mode TensorFlow (v1.x). We'll update the documentation.\r\n\r\nIn your case, there's a simple trick you can use to get the old behavior without reverting to TF 1.x: you can call `tf.compat.v1.disable_eager_execution()` at the start of your program. This will use the old behavior.\r\n\r\nThe root cause of the issue is that in 2.x Keras converts your generator to a `tf.data.Dataset` before training (this enables Keras to use a unified training path for all types of inputs). Converting a generator to a dataset results in the use of threading, as you can see in the following example:\r\n\r\n```python\r\ndef gen ():\r\n  for i in range(100):\r\n    print(threading.current_thread())\r\n    yield (tf.random.normal(shape=(100, 1)), tf.random.normal(shape=(100, 1)))\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_signature=(tf.TensorSpec(shape=(100, 1), dtype=tf.float32), tf.TensorSpec(shape=(100, 1), dtype=tf.float32))).batch(1)\r\nfor _ in ds.take(1):\r\n  pass\r\n```\r\n\r\nWhich will print:\r\n\r\n```\r\n<_DummyThread(Dummy-5, started daemon 140597821310720)>\r\n\r\n```\r\n\r\nThis problem cannot be fixed in the Keras codebase without creating an exception to our unified training code path (i.e. *not* converting generators to Datasets), which would be highly impractical.\r\n\r\nIn the future, two things you could do if you don't want threads involved in your generator training, would be 1. write your own training loop based on `model.train_on_batch()` (where you iterate on your generator yourself), or 2. write your own low-level training loop [as per this guide](https://keras.io/guides/writing_a_training_loop_from_scratch/).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47321\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47321\">No</a>\n", "Thanks very much for your answer @fchollet ! I'll try the eager execution trick.\r\nI wonder if you think we could have a codepath in tf.data that allows executing the generator in a thread safe manner. \r\n\r\nI would write a custom training loop if this was for an applied project, but in the **Keras for R** project we would need to support all callbacks, and etc for the generator codepath and it's probably too much work.", "> I wonder if you think we could have a codepath in tf.data that allows executing the generator in a thread safe manner.\r\n\r\nI think it is a reasonable ask. It could be an option in `tf.data.Dataset.from_generator`. @jsimsa would be the best contact point. Consider creating a new issue (referencing this one) to track it.", "It is not clear how feasible this is for the following reasons:\r\n\r\n1) `tf.data.Dataset.from_generator` uses the `py_func` TensorFlow op for invoking the user-defined function because tf.data implementation needs to transfer execution between C++ and Python and `py_func` provides a robust mechanism for this. `py_func` implementation uses a separate thread to run the user-provided function.\r\n\r\n2) tf.data input pipeline can contain downstream asynchrony, which means that the thread executing the `from_generator` C++ kernel can be different from the main thread irrespective of how is the `from_generator` C++ kernel is implemented (and what options it is executed with).\r\n\r\nI would be happy to review external proposals / PRs for how to achieve it, but it is unlikely that the tf.data team will develop this functionality themselves. From my perspective this is a niche scenario, which would require non-trivial engineering effort to address.", "Thanks very much @jsimsa ! This makes sense to me! I don't think it's worth the effort. I think there might still exist other workarounds in the R side like using an event loop and scheduling the generator call to the main thread."]}, {"number": 47320, "title": "Prefer generator expressions over list comprehensions", "body": "This PR replaces list comprehensions that are only used as input to `any()` or `all()` with generator expressions. This removes the need to instantiate unnecessary lists if the expression is only consumed as an iterator and in the case of any allows the loop to potentially exit early which can improve performance for long iterations.\r\n\r\nMost of the changes are not in a hot code path so this won't noticeably improve performance but since the change don't hurt readability I think they are still useful to include.", "comments": []}, {"number": 47319, "title": "TPU with bfloat16 does not support tf.image.resize with nearest", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.9\r\n- GPU model and memory: TPUv3\r\n\r\n\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/3354448/108751857-b4d2ae80-757d-11eb-8d7f-467085464eca.png)\r\n", "comments": ["@edwardyehuang \r\n\r\nPlease, fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47319\">No</a>\n"]}, {"number": 47318, "title": "[tf.data] refactor serialization tests based on CheckpointTestBase in kernel_tests", "body": "This PR is a pre-requisite for https://github.com/tensorflow/tensorflow/pull/47314 where serializability tests are being cleaned up and moved to `kernel_tests`.\r\n\r\n- Renamed `DatasetSerializationTestBase` to `CheckpointTestBase` to be consistent with the future test cases.\r\n- Refactored all serialization tests to use the new `checkpoint_test_base` target.\r\n- modified targets in BUILD files.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa I tried to run the serialization tests once before refactoring them with `checkpoint_test_base`. \r\n\r\nWhen I run the following, \r\n```\r\n$ bazel test --runs_per_test=1 //tensorflow/python/data/experimental/kernel_tests/serialization:auto_shard_dataset_serialization_test\r\n```\r\nI get\r\n```\r\nWARNING: All specified test targets were excluded by filters\r\nINFO: Analyzed 0 targets (0 packages loaded, 0 targets configured).\r\nINFO: Found 0 test targets...\r\nINFO: Elapsed time: 0.276s, Critical Path: 0.00s\r\nINFO: 1 process: 1 internal.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\n```\r\n\r\nIs there any issue with the current test targets or am I missing something?\r\n\r\nUPDATE: The tests are being filtered out even after the changes.", "You see the tests not being executed because they are all marked as \"no_oss\" (e.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/serialization/BUILD#L39). If you remove the tag, the test should execute. ", "@kvignesh1420 can you please check sanity build failures ?", "@rthadur I have fixed the ci sanity issues.", "@jsimsa any update on this PR? Anything I can help with? Will continue with https://github.com/tensorflow/tensorflow/pull/47314 once this gets merged.", "@kvignesh1420 there are internal projects that import the serialization module this PR moves and I will need to address the conflicts before your PR is merged, I hope to take care of that today", "Thanks @jsimsa"]}, {"number": 47317, "title": "Add cmake build for tflite label_image example.", "body": "Add label_image build for cmake.", "comments": ["@mihaimaruseac\r\nCould you please review this pr? The cmake build for label_image example. \r\n", "@terryheo could you review this PR?", "@JerryShih Thanks for the contribution.\r\nCan we still build benchmark tool under the tflite cmake build directory?\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#step_6_build_tensorflow_lite_benchmark_tool", "> @JerryShih Thanks for the contribution.\r\n> Can we still build benchmark tool under the tflite cmake build directory?\r\n> \r\n> https://www.tensorflow.org/lite/guide/build_cmake#step_6_build_tensorflow_lite_benchmark_tool\r\n\r\n@terryheo \r\nYes, we could still use the same target name to build that benchmark tool.\r\n\r\nbenchmark tool\r\n```\r\ncmd:\r\ncmake --build . -j -t benchmark_model\r\n\r\nlog:\r\nScanning dependencies of target benchmark_model\r\n...\r\n[100%] Building CXX object tools/benchmark/CMakeFiles/benchmark_model.dir/__/__/profiling/profile_summarizer.cc.o\r\n[100%] Linking CXX executable benchmark_model\r\n[100%] Built target benchmark_model\r\n\r\nbinary path:\r\n/tensorflow/tensorflow/lite/build$ ls tools/benchmark/\r\nbenchmark_model  CMakeFiles  cmake_install.cmake  Makefile\r\n```\r\n\r\nlabel_image example\r\n```\r\ncmd:\r\ncmake --build . -j -t label_image\r\n\r\nlog:\r\nScanning dependencies of target label_image\r\n...\r\n[100%] Linking CXX executable label_image\r\n[100%] Built target label_image\r\n\r\nbinary path:\r\n/tensorflow/tensorflow/lite/build$ ls examples/label_image/\r\nCMakeFiles  cmake_install.cmake  label_image  Makefile\r\n```"]}, {"number": 47316, "title": "BUG: gfile.rmtree is raises a NotFound error for ram:// filesystems", "body": "Tested via tf-nightly on 2020-02-22 (YYYY-MM-DD).\r\n\r\nCode to reproduce:\r\n\r\n```python\r\nfrom tensorflow.io import gfile\r\n\r\ngfile.mkdir(\"ram://deletethisdir\")\r\ngfile.rmtree(\"ram://deletethisdir\")  # raises a NotFoundError\r\n```\r\n\r\nColab notebook: https://colab.research.google.com/drive/13bWmaJ40aeQKd2pUfNtnfwaaUP1M_IVL?authuser=1#", "comments": ["ccing @mihaimaruseac ", "Was able to reproduce the issue with TF v2.4 and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/31f6ff9f7152b2cd2ea8321d9e736a30/47316.ipynb#scrollTo=MMP-3L1yxwgF). \r\n\r\nHowever, I did not face any errors while running the code with [TF v2.3.2](https://colab.research.google.com/gist/amahendrakar/897a5c4b12bdea9fd44233a0792c748f/47316-2-3.ipynb). Please check the linked gist for reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47316\">No</a>\n"]}, {"number": 47315, "title": "[MLIR] Add concatenateOp lowering from lmhlo to Affine.", "body": "Lowering of `concatenateOp` is added from lmhlo to Affine. The lowering\r\nhas been added as a part of `lhlo-legalize-to-affine` pass.\r\n\r\nSigned-off-by: Prashant Kumar <prashantk@polymagelabs.com>", "comments": ["@pashu123 Can you please check @joker-eph's comments and keep us posted ? Thanks!", "@pashu123  Can you please check @joker-eph's comments and keep us posted ? Thanks!"]}, {"number": 47314, "title": "[tf.data] move map, map_and_batch serialization tests to kernel_tests", "body": "This PR moves the serialization tests for `map` and `map_and_batch` datasets to the respective files in `kernel_tests` and `experimental/kernel_tests`:\r\n\r\nA part of a larger cleanup as discussed in point 4 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\n\r\ncc: @jsimsa ", "comments": ["@kvignesh1420 Can you please check @jsimsa's comments and keep us posted ? Thanks!", "@gbaned As per the discussion, the review comments have been addressed in a separate PR https://github.com/tensorflow/tensorflow/pull/47318. Once it gets merged I can continue with this one. \r\ncc: @jsimsa ", "@jsimsa as https://github.com/tensorflow/tensorflow/pull/47318 was merged, I have made the changes accordingly.", "I suggest we introduce / adjust `shard_count` for the modified test targets so that the modified tests do not take extra long to run.", "@jsimsa I tried sharding the datasets and running the tests but couldn't find any major test time reduction. This is probably due to the small size of the datasets being used for checkpointing. However, for reference, I have added sharded dataset checkpointing in `def testNumParallelBatches()` of `class MapAndBatchDatasetCheckpointTest`\r\n\r\n```\r\nmap_and_batch_test: before sharding ~ 20 sec, after sharding ~18 sec\r\n```"]}, {"number": 47313, "title": "BUG: gfile.walk return full paths instead of filenames for ram:// filesystems", "body": "Tested via tf-nightly on 2020-02-22 (YYYY-MM-DD).\r\n\r\nCode to reproduce:\r\n\r\n```python3\r\nfrom tensorflow.io import gfile\r\n\r\ngfile.mkdir(\"ram://testdir\")\r\nwith gfile.GFile(\"ram://testdir/file.txt\", \"w\") as f:\r\n    f.write(\"test\")\r\n\r\nprint(list(gfile.walk(\"ram://testdir\")))\r\n```\r\n\r\nThe third element is `['ram://testdir/file.txt']`, but it should be just `['file.txt']`\r\n\r\nColab notebook: https://colab.research.google.com/drive/13bWmaJ40aeQKd2pUfNtnfwaaUP1M_IVL?authuser=1#", "comments": ["ccing @mihaimaruseac ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47313\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47313\">No</a>\n"]}, {"number": 47312, "title": "CUDA 11.2/TF 2.5.0 Failed to load libcudnn.so.8", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0-dev20210218\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: NVIDIA GeForce RTX 2060 Super\r\n\r\n\r\n\r\nI have gone through every installation guide, installed/reinstalled TF many times, as well as cuda and cudnn. But everytime i run:\r\n```\r\n\r\nimport tensorflow as tf\r\ntf.config.list_physical_devices()\r\n```\r\nI get:\r\n```\r\n\r\n2021-02-22 10:08:07.383300: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-22 10:08:11.985232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-22 10:08:12.014912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-22 10:08:12.015228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1770] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.695GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-22 10:08:12.015246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-22 10:08:12.016727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-22 10:08:12.016757: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-22 10:08:12.017340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-02-22 10:08:12.032485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-02-22 10:08:12.050575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-02-22 10:08:12.051062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-02-22 10:08:12.051213: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n2021-02-22 10:08:12.051239: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1803] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n\r\nOut[3]: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n\r\n```\r\nI have three cuda folders in /usr/local/, cuda, cuda-11 and cuda-11.2 and they all have the 'libcudnn.so.8' file.\r\n", "comments": ["UPDATE:\r\n\r\n```\r\n2021-02-23 11:16:32.720482: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-02-23 11:16:32.739296: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3600000000 Hz\r\nEpoch 1/100\r\n/home/anthony/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/sort_pooling/map/while/gradients/model/sort_pooling/map/while/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/sort_pooling/map/while/gradients/model/sort_pooling/map/while/GatherV2_grad/Reshape:0\", shape=(None, None), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/sort_pooling/map/while/gradients/model/sort_pooling/map/while/GatherV2_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  warnings.warn(\r\n2021-02-23 11:16:33.495332: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-23 11:16:33.762835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-02-23 11:16:33.776293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-23 11:16:37.745284: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 8100\r\n2021-02-23 11:16:43.938366: W tensorflow/stream_executor/gpu/asm_compiler.cc:64] Running ptxas --version returned 256\r\n2021-02-23 11:16:44.048594: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n```", "Please take a look at this related [thread](https://github.com/tensorflow/tensorflow/issues/40036#issuecomment-774353012)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47312\">No</a>\n"]}, {"number": 47311, "title": "TypeError: Cannot convert a symbolic Keras input/output to a numpy array.", "body": "I dont quite understand why i'm getting this error. i came back to this project after updating some things and now my code wont work. Below is the code for my model. Any idea how I can avoid this? \r\n\r\n```\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\n\r\nLEARNING_RATE = 1e-4\r\nHIDDEN_SIZE = 32\r\nCLIPPING = 0.2\r\nLOSS = 1e-5\r\n\r\n\r\n# PPO loss function\r\ndef PPO_loss(advantage, old_prediction):\r\n    def loss(y_true, y_pred):\r\n        prob = y_true * y_pred\r\n        old_prob = y_true * old_prediction\r\n        r = prob/(old_prob + 1e-10)\r\n\r\n        return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - CLIPPING, max_value=1 + CLIPPING) * advantage) + LOSS * -(prob * K.log(prob + 1e-10)))\r\n\r\n    return loss\r\n\r\n\r\nclass PPO:\r\n    def __init__(self, statesize, num_intruders, actionsize, valuesize):\r\n        self.statesize = statesize\r\n        self.num_intruders = num_intruders\r\n        self.actionsize = 5\r\n        self.valuesize = valuesize\r\n\r\n        self.model = self.__build_linear__()\r\n\r\n    def __build_linear__(self):\r\n        # Input of the aircraft of focus\r\n        _input = keras.layers.Input(\r\n            shape=(self.statesize,), name='input_state')\r\n\r\n        # This is the input for the n_closest aircraft\r\n        _input_context = keras.layers.Input(\r\n            shape=(self.num_intruders, 7), name='input_context')\r\n\r\n        # Empty layer\r\n        empty = keras.layers.Input(shape=(HIDDEN_SIZE,), name='empty')\r\n\r\n        # Input for advantages\r\n        advantage = keras.layers.Input(shape=(1,), name=\"advantage\")\r\n\r\n        # Input old prediction\r\n        old_prediction = keras.layers.Input(\r\n            shape=(self.actionsize,), name='old_predictions')\r\n\r\n        # Flatten the context layer (As context is passed as an n*m tensor)\r\n        flatten_context = keras.layers.Flatten()(_input_context)\r\n\r\n        # Hidden Layers\r\n\r\n        # 1st hidden applies to the context only\r\n        h1 = keras.layers.Dense(\r\n            HIDDEN_SIZE, activation='relu')(flatten_context)\r\n\r\n        # Combine the input and the context\r\n        combine = keras.layers.concatenate([_input, h1], axis=1)\r\n\r\n        # Hidden layers 2 & 3 apply to all inputs\r\n        h2 = keras.layers.Dense(256, activation='relu')(combine)\r\n        h3 = keras.layers.Dense(256, activation='relu')(h2)\r\n\r\n        # Output layer\r\n        out = keras.layers.Dense(self.actionsize+1, activation=None)(h3)\r\n\r\n        # Policy and value layer processing\r\n        policy = keras.layers.Lambda(\r\n            lambda x: x[:, :self.actionsize], output_shape=(self.actionsize,))(out)\r\n        value = keras.layers.Lambda(\r\n            lambda x: x[:, self.actionsize:], output_shape=(self.valuesize,))(out)\r\n\r\n        # Policy and value outputs\r\n        policy_out = keras.layers.Activation(\r\n            'softmax', name='policy_out')(policy)\r\n        value_out = keras.layers.Activation(\r\n            'linear', name='value_out')(value)\r\n\r\n        # Optimizer\r\n        optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)\r\n\r\n        # Produce the model\r\n        model = keras.models.Model(inputs=[\r\n                                   _input, _input_context, empty, advantage, old_prediction], outputs=[policy_out, value_out])\r\n\r\n        self.estimator = keras.models.Model(\r\n            inputs=[_input, _input_context, empty], outputs=[policy_out, value_out])\r\n\r\n        # Compile the model\r\n\r\n        model.compile(optimizer=optimizer, loss={'policy_out': PPO_loss(\r\n            advantage=advantage, old_prediction=old_prediction), 'value_out': 'mse'})\r\n\r\n        print(model.summary())\r\n        return model\r\n\r\n```\r\n", "comments": ["@Ellislee1,\r\nIn the given code snippet you have defined the class and its methods but are not calling them anywhere. \r\n\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "@Ellislee1 try creating a virtual env (or cloning the one you use if you do so) and downgrade numpy to v1.19.3\r\n\r\nseems to be a compatibility issue between numpy 1.20+ and tf #47263 ", "The main issue here is that you are using a custom loss callback that takes an argument `advantage` (from your data generator, most likely numpy arrays).\r\nIn Tensorflow 2 eager execution, the `advantage` argument will be numpy, whereas `y_true`, `y_pred` are symbolic. The way to solve this is to turn off eager execution\r\n\r\n```\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n```\r\n\r\nSee similar [stackoverflow issue](https://stackoverflow.com/questions/65366442/cannot-convert-a-symbolic-keras-input-output-to-a-numpy-array-typeerror-when-usi)", "@Ellislee1 Looks like this is a duplicate issue https://github.com/tensorflow/tensorflow/issues/47263\r\n\r\nPlease note that `Tensorflow 2.4` requires numpy version to be >=1.19.2 and < 1.20. \r\ncc @mihaimaruseac \r\n\r\nPlease downgrade numpy version from 1.20 to the versions mentioned above and test the code. Please let us know how it progresses. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you after adjusting the numpy versions mentioned above. Thanks!", "Can confirm that this bug triggers also on Python 3.9 and numpy 1.20 (which are stock versions supplied with Arch linux). As such, the only way to get tensorflow working on Arch now is to use conda to get parallel python interpreter going. \r\nAlso you can use code below to reproduce easier:\r\n```\r\n    import tensorflow as tf\r\n    x = tf.keras.layers.LSTM(5)\r\n    y = tf.keras.layers.LSTM(5) \r\n    print(x(y))\r\n```", "@alexpyattaev Even Python3.9 is not supported. \r\nThe following versions are supported\r\n\r\n```\r\n        'Programming Language :: Python :: 3.6',\r\n        'Programming Language :: Python :: 3.7',\r\n        'Programming Language :: Python :: 3.8',\r\n```\r\n\r\nPlease check the supported packages and their version listed [here](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/tools/pip_package/setup.py#L84). Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am closing this issue. We will follow the progress of supporting numpy2.x in the future versions of TF through https://github.com/tensorflow/tensorflow/issues/47268 or any feature request issue (@ghylander may create one). Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47311\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47311\">No</a>\n", "Moved to #47691", "I'm also having this issue.\r\nI'm using:\r\nnumpy version 1.19.5\r\npython version 3.7\r\ntensorflow version 2.4.1 \r\n\r\nRunning:\r\n        from tensorflow.python.framework.ops import disable_eager_execution\r\n       disable_eager_execution()\r\ndid not work.", "Reopening as it seems it is not caused by the numpy version", "To my knowladge, it might be due to the eager execution, I may be wrong however.", "I encounter the same problem here. It seems that we cannot run the custom loss function based model training even after disable the eager execution.", "Encountering same issue here, I tried with to replace all numpy component to tf in my custom loss function. Still having same error. \r\n\r\nTried disable_eager_execution() as well, but encounter another Keras backend with numpy issue when init random weight in some layer, which this issue doesn't appear previously. \r\n\r\nFYI, I was actually trying to upgrade my project from TF1 to TF2, that weight initialize issue doesn't appear while i was using TF1.\r\n\r\nTF version 2.4.1\r\nNumpy 1.19.5", "@AndyTsangChun I also encountered the issue when trying to upgrade TF1 code\r\n\r\n> \r\n> \r\n> Encountering same issue here, I tried with to replace all numpy component to tf in my custom loss function. Still having same error.\r\n> \r\n> Tried disable_eager_execution() as well, but encounter another Keras backend with numpy issue when init random weight in some layer, which this issue doesn't appear previously.\r\n> \r\n> FYI, I was actually trying to upgrade my project from TF1 to TF2, that weight initialize issue doesn't appear while i was using TF1.\r\n> \r\n> TF version 2.4.1\r\n> Numpy 1.19.5\r\n\r\n", "@Ellislee1 Can you please share a standalone code to reproduce the issue? Your code is not complete at the moment. \r\n\r\nIt would be great If anyone can share a simple standalone code to reproduce the issue? Thanks!", "One can evoke this error by running the following in a Colab notebook with a GPU runtime:\r\n\r\n```python\r\nfrom keras import applications\r\n\r\n# build the VGG16 network\r\nmodel = applications.VGG16(include_top=False, weights='imagenet')\r\n\r\n# get the symbolic outputs of each \"key\" layer (we gave them unique names).\r\nlayer_dict = dict([(layer.name, layer) for layer in model.layers])\r\n\r\nlayer_name = 'block5_conv3'\r\nfilter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer\r\n\r\n# build a loss function that maximizes the activation\r\n# of the nth filter of the layer considered\r\nlayer_output = layer_dict[layer_name].output\r\nloss = K.mean(layer_output[:, :, :, filter_index])\r\n\r\n# compute the gradient of the input picture wrt this loss\r\ngrads = K.gradients(loss, input_img)[0]\r\n```\r\n\r\n(The above is from @fchollet's [delightful guide to visualizing convnet filters](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html).) Versions in my Python 3.7.10 notebook:\r\n\r\n```bash\r\nnumpy==1.19.5\r\ntensorflow==2.4.1\r\nKeras==2.4.3\r\n```", "could not reproduce\r\n\r\nhttps://colab.research.google.com/drive/1tSjH0ZlgT549pfUInaRmckZ_6mkHmIub?usp=sharing\r\n\r\nexecuted with no errors\r\n\r\nnumpy: 1.19.5\r\ntf: 2.4.3\r\n\r\n(also fixed missing keras backend import, disabled eager execution, and added a http link to an image)\r\n\r\nthe file collab notebook had restricted access, should be fine now", "@ghylander Thank you for your notebook link! I also cannot reproduce this problem this morning. My sole offering is a single [screenshot](https://postimg.cc/hhBrSrV1) from the notebook that originally triggered this issue yesterday. If this error springs again I will follow up...", "Please close this issue If this was resolved for you all with new `TF` version. Thanks!", "> @AndyTsangChun I also encountered the issue when trying to upgrade TF1 code\r\n> \r\n> > Encountering same issue here, I tried with to replace all numpy component to tf in my custom loss function. Still having same error.\r\n> > Tried disable_eager_execution() as well, but encounter another Keras backend with numpy issue when init random weight in some layer, which this issue doesn't appear previously.\r\n> > FYI, I was actually trying to upgrade my project from TF1 to TF2, that weight initialize issue doesn't appear while i was using TF1.\r\n> > TF version 2.4.1\r\n> > Numpy 1.19.5\r\n\r\nStill not resolved, but i found out the root cause was from keras.Input\r\n\r\nIn my loss function, I used the keras.Input tensor for some calculation. Still finding a way to convert the keras tensor to tf.op.tensor", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@AndyTsangChun The solution I am using so far is to NOT specify loss when compiling model with `comile()`, while specify custom `train_step()` with your custom loss inside it.\r\n\r\nRemember to give a dict when calling `fit()` in the case below so that you can use dict to get what you need (in this case `output` and `labels_err` from dict)\r\n\r\nFor example: \r\n```python\r\nfrom tensorflow.python.keras.engine import data_adapter\r\n\r\nclass CustomModel(keras.Model):\r\n    def train_step(self, data):\r\n        data = data_adapter.expand_1d(data)\r\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n    \r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            # mse_robust is a custom_loss function with four arguments where two of the arguments are from model output (i.e. the model has two outputs/heads)\r\n            loss = mse_robust(y['output'], y_pred[0], y_pred[1], x['labels_err'])\r\n    \r\n        # apply gradient here\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\nmodel = CustomModel(.....).compile(optimizer=your_optimizer_here, loss=None)\r\nmodel.fit({'input': ...., 'labels_err': ....}, {'output': .....})\r\n```", "@henrysky  \r\n\r\n> The solution I am using so far is to NOT specify loss when compiling model with comile(), while specify custom train_step() with your custom loss inside it. Remember to give a dict when calling fit() in the case below so that you can use dict to get what you need (in this case output and labels_err from dict)\r\n\r\n\r\nI am facing a similar issue and was interested in trying the solution that you proposed. \r\nI tried to implement the custom model like you have mentioned but get the following error: \r\n NotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\r\n \r\n tf version : 2.4.1. \r\n \r\n Sorry if I am being a noob, but if you could point me to some sample of this I will really appreciate. Thanks. ", "@harshad-dh you can read the official tutorial about sub-classing keras model (https://www.tensorflow.org/guide/keras/functional#when_to_use_the_functional_api)", "Hi,\r\nI just added \"`del model`\" before instantiating my model which in this case is:\r\n\r\n```\r\ndel model\r\nmodel = build_model(h, w, channels, actions)\r\n```\r\n\r\nAnd it resolved my issue.\r\n\r\nDo give it a shot. I understand it sounds a bit silly, but it worked for me.\r\n\r\nWhat do we have to lose. :) .", "I am also suffering the same issue when giving custom loss or metrics (eg., accuracy) in compile().\r\nIn the custom loss and accuracy, I used mask when calculating mean without padded parts like:\r\n\r\n this is my custom accuracy\r\n\r\n```\r\n acc = categorical_accuracy(y_true, y_pred)  # shape=(B,T)\r\n mask = tf.cast(self.mask, dtype=tf.float32)  # shape=(B,T)\r\n acc = tf.reduce_sum(acc * mask, axis=-1) / tf.reduce_sum(mask, axis=-1)  # shape=(B,)\r\n```\r\n\r\nThe last two lines causes the TypeError (with eager execution).\r\nHowever, when I change the two lines to this one line:\r\n\r\n```\r\nacc = tf.reduce_mean(acc, axis=-1)  # shape=(B,)\r\n```\r\n\r\nThe TypeError disappears.\r\n\r\nI cannot understand why the two lines causes the TypeError. \r\nAnyone can explain or comment?\r\n\r\nI am using conda environment and some related versions are:\r\nTF=2.4.1\r\nnumpy=1.19.2\r\nkeras=2.4.3\r\n", "> I am also suffering the same issue when giving custom loss or metrics (eg., accuracy) in compile().\r\n> In the custom loss and accuracy, I used mask when calculating mean without padded parts like:\r\n> \r\n> this is my custom accuracy\r\n> \r\n> ```\r\n>  acc = categorical_accuracy(y_true, y_pred)  # shape=(B,T)\r\n>  mask = tf.cast(self.mask, dtype=tf.float32)  # shape=(B,T)\r\n>  acc = tf.reduce_sum(acc * mask, axis=-1) / tf.reduce_sum(mask, axis=-1)  # shape=(B,)\r\n> ```\r\n> \r\n> The last two lines causes the TypeError (with eager execution).\r\n> However, when I change the two lines to this one line:\r\n> \r\n> ```\r\n> acc = tf.reduce_mean(acc, axis=-1)  # shape=(B,)\r\n> ```\r\n> \r\n> The TypeError disappears.\r\n> \r\n> I cannot understand why the two lines causes the TypeError.\r\n> Anyone can explain or comment?\r\n> \r\n> I am using conda environment and some related versions are:\r\n> TF=2.4.1\r\n> numpy=1.19.2\r\n> keras=2.4.3\r\n\r\nIt seemed to be solved in my case. \r\nIn my case, I have got the last layer's call() method to return mask and used the mask in the custom loss and accuracy methods. By the way, the mask in the last layer has type of tf.Tensor but after returning it by calling the last layer, the mask's type automatically changed into KerasTensor (this happens in case of Functional Model construction) and this caused the error when calculating (multiplying or dividing) with other Tensor objects in the custom loss and accuracy method.\r\nSo, I used the last layer's mask directly by introducing and accessing last_layer.mask attribute like:\r\n\r\n```\r\n # in the last layer's call() method\r\n def call(self, inputs, mask=None):\r\n     self.mask = mask\r\n      ....\r\n```\r\n\r\n```\r\n# in the custom accuracy method\r\ndef my_accuracy(self):\r\n  def accuracy(y_true, y_pred):\r\n     acc = categorical_accuracy(y_true, y_pred)  # shape=(B,T)\r\n     mask = tf.cast(self.last_layer.mask, dtype=tf.float32)  # shape=(B,T)\r\n     acc = tf.reduce_sum(acc * mask, axis=-1) / tf.reduce_sum(mask, axis=-1)  # shape=(B,)\r\n     return acc\r\n  return accuracy\r\n```\r\nThis could resolve the error.\r\n", "> Hi,\r\n> I just added \"`del model`\" before instantiating my model which in this case is:\r\n> \r\n> ```\r\n> del model\r\n> model = build_model(h, w, channels, actions)\r\n> ```\r\n> \r\n> And it resolved my issue.\r\n> \r\n> Do give it a shot. I understand it sounds a bit silly, but it worked for me.\r\n> \r\n> What do we have to lose. :) .\r\n\r\nThis worked for me! Thank you @abhishekvenkat764", "> > @AndyTsangChun I also encountered the issue when trying to upgrade TF1 code\r\n> > > Encountering same issue here, I tried with to replace all numpy component to tf in my custom loss function. Still having same error.\r\n> > > Tried disable_eager_execution() as well, but encounter another Keras backend with numpy issue when init random weight in some layer, which this issue doesn't appear previously.\r\n> > > FYI, I was actually trying to upgrade my project from TF1 to TF2, that weight initialize issue doesn't appear while i was using TF1.\r\n> > > TF version 2.4.1\r\n> > > Numpy 1.19.5\r\n> \r\n> Still not resolved, but i found out the root cause was from keras.Input\r\n> \r\n> In my loss function, I used the keras.Input tensor for some calculation. Still finding a way to convert the keras tensor to tf.op.tensor\r\n\r\nUse ```keras.Input.type_spec``` ", "Downgrading TF and TF Addons did the trick for me. TF 2.1", "Yes. But that's a temporary solution.\n\nOn Fri, May 7, 2021, 8:32 PM Joao Paulo Leite ***@***.***>\nwrote:\n\n> Downgrading TF and TF Addons did the trick for me. TF 2.1\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-834490352>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KF5ZQH7EHGDWLNN5GLTMP6H3ANCNFSM4YASNBHQ>\n> .\n>\n", "I am also getting similar errors as filed under this issue: [**Custom loss function is not working**](https://github.com/tensorflow/tensorflow/issues/43650)\r\n\r\nI have found that the custom loss function works with the **TensorFlow v1.15.0** but doesn't work with **TensorFlow v2.3.0 & 2.5.0** (both tested)\r\n\r\n**But it starts working when eager execution is disabled using:**\r\n```python\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n```\r\n\r\nI think this comments might be helpful:\r\n- [**Complete Report on Errors**](https://github.com/tensorflow/tensorflow/issues/43650#issuecomment-850797985)\r\n- [**Complete Report on Causes**](https://github.com/tensorflow/tensorflow/issues/43650#issuecomment-850850455)", "I have the same issue even without any custom loss function.\r\nTF=2.5.0\r\nnumpy=1.20.3 (the same with 1.19.5)\r\n\r\nhttps://colab.research.google.com/gist/msveshnikov/d71c465998bc3dd3230d894eb4508736/twitter.ipynb?hl=ru\r\n", "Thank you for feedback! But it was working with tf - 2.4.0 and works on Kaggle runtime for example. I attached example notebook to first comment", "@msveshnikov I will check the error but, the notebook you provided requires the dataset.", "Sorry, this is kaggle competition dataset. Attached.\r\n[train.csv](https://github.com/tensorflow/tensorflow/files/6565957/train.csv)\r\n\r\n[test.csv](https://github.com/tensorflow/tensorflow/files/6565956/test.csv)\r\n", "Hi @msveshnikov,\r\n\r\nThe current error is solved by importing the `keras` as `tensorflow.keras`, but after this I encountered another error, given below:\r\n```\r\nInvalidArgumentError:  assertion failed: [Input values must be in the range 0 <= values < num_tokens with num_tokens=222]\r\n\t [[node model_1/lambda_1/category_encoding/Assert/Assert (defined at <ipython-input-19-0b1a3e8de16f>:5) ]] [Op:__inference_train_function_22110]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nThe reference notebook, you can check what all changes I have made:\r\nhttps://colab.research.google.com/drive/1oj50_5L6cRamLe61AD9wdGbUWG-D0VdX", "Can you provide a Colab with issue?\n\nOn Mon, May 31, 2021 at 8:21 AM Dhyey Thumar ***@***.***>\nwrote:\n\n> Hi @msveshnikov <https://github.com/msveshnikov>, The current error is\n> solved if you use TF v2.3.0 but there is another error, given as follows:\n>\n> InvalidArgumentError:  Matrix size-incompatible: In[0]: [1,10223], In[1]: [10221,500]\n> \t [[node functional_1/dense_1/Relu (defined at <ipython-input-21-5a471be0655d>:6) ]] [Op:__inference_train_function_42495]\n>\n> Function call stack:\n> train_function\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-851130321>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KBVN7CNGWXLSHEKTCTTQL2SFANCNFSM4YASNBHQ>\n> .\n>\n", "@s4sarath the comment you have quoted is modified & it contains the colab link, & also the current error is solved but a new error (possibly unrelated to this issue) is generated.", "Hi @dhyeythumar !\r\nThanks, this fixed my problem! Now I can compute on 2.5.\r\nSo remaining error is just fixed by \r\n`encoder = preprocessing.CategoryEncoding(output_mode=\"binary\", num_tokens=len(vocab)+2)`\r\n\r\nFor evereyone who has original bug after migration TF 2.4 => 2.5, you have to change imports like this:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Flatten, Dropout, Input, Lambda, concatenate\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\nfrom keras.layers.experimental.preprocessing import TextVectorization\r\n```", "> Hi @dhyeythumar !\r\n> Thanks, this fixed my problem! Now I can compute on 2.5.\r\n> So remaining error is just fixed by\r\n> `encoder = preprocessing.CategoryEncoding(output_mode=\"binary\", num_tokens=len(vocab)+2)`\r\n> \r\n> For evereyone who has original bug after migration TF 2.4 => 2.5, you have to change imports like this:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow import keras\r\n> from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Flatten, Dropout, Input, Lambda, concatenate\r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.callbacks import EarlyStopping\r\n> from tensorflow.keras.layers.experimental import preprocessing\r\n> from keras.layers.experimental.preprocessing import TextVectorization\r\n> ```\r\n\r\nThankyou, This solved the error. but got another one.\r\n \"raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\r\nTypeError: Could not build a TypeSpec for <KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'tf.math.l2_normalize')> with type KerasTensor\"\r\n\r\nDo you anyhoe know how to solve this. I am a Noob. These things are new to me.", "You can get type spec from keras tensor.\n\nOn Fri, 11 Jun, 2021, 6:31 pm Akashmanoj369, ***@***.***>\nwrote:\n\n> Hi @dhyeythumar <https://github.com/dhyeythumar> !\n> Thanks, this fixed my problem! Now I can compute on 2.5.\n> So remaining error is just fixed by\n> encoder = preprocessing.CategoryEncoding(output_mode=\"binary\",\n> num_tokens=len(vocab)+2)\n>\n> For evereyone who has original bug after migration TF 2.4 => 2.5, you have\n> to change imports like this:\n>\n> import tensorflow as tf\n> from tensorflow import keras\n> from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Flatten, Dropout, Input, Lambda, concatenate\n> from tensorflow.keras.models import Model\n> from tensorflow.keras.callbacks import EarlyStopping\n> from tensorflow.keras.layers.experimental import preprocessing\n> from keras.layers.experimental.preprocessing import TextVectorization\n>\n> Thankyou, This solved the error. but got another one.\n> \"raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n> TypeError: Could not build a TypeSpec for <KerasTensor: shape=(None, 4096)\n> dtype=float32 (created by layer 'tf.math.l2_normalize')> with type\n> KerasTensor\"\n>\n> Do you anyhoe know how to solve this. I am a Noob. These things are new to\n> me.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-859565212>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KFFL5TPT4JZUZJ6GE3TSICLFANCNFSM4YASNBHQ>\n> .\n>\n", "I also have the same issue when definining a custom loss function. `disable_eager_execution()` solves the issue but raises a new one:\r\n` FailedPreconditionError: Could not find variable training/Adam/beta_1. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training/Adam/beta_1/class tensorflow::Var does not exist.\r\n\t [[{{node training/Adam/Identity_1/ReadVariableOp}}]]`\r\n\r\nDisabling the eager execution seems to break other things, as models which previously worked are also broken after disabling it.", "I recommend not to use mode.fit. it's custom loss is not great. I wrote a\ncustom trainer. You want to try that ?\n\nOn Tue, 15 Jun, 2021, 3:59 am jeroenvermunt, ***@***.***>\nwrote:\n\n> I also have the same issue when definining a custom loss function.\n> disable_eager_execution() solves the issue but raises a new one:\n> FailedPreconditionError: Could not find variable training/Adam/beta_1.\n> This could mean that the variable has been deleted. In TF1, it can also\n> mean the variable is uninitialized. Debug info: container=localhost,\n> status=Not found: Resource localhost/training/Adam/beta_1/class\n> tensorflow::Var does not exist. [[{{node\n> training/Adam/Identity_1/ReadVariableOp}}]]\n>\n> Disabling the eager execution seems to break other things, as models which\n> previously are also broken after disabling it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861035911>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KEAN34OSVK3C4UA2DLTSZ7FRANCNFSM4YASNBHQ>\n> .\n>\n", "> I recommend not to use mode.fit. it's custom loss is not great. I wrote a custom trainer. You want to try that ?\r\n\r\nSure! you think it would help?\r\n", "I think it helps.\nThe reason I wrote a wrapper is that ```model.fit```  does not support dict\ninputs and outputs. Internally it flattens everything.\n\nOn Tue, Jun 15, 2021 at 1:07 PM jeroenvermunt ***@***.***>\nwrote:\n\n> I recommend not to use mode.fit. it's custom loss is not great. I wrote a\n> custom trainer. You want to try that ?\n>\n> Sure! you think it would help?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861261358>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KBDU6VOC3LCQFH6XETTS37LHANCNFSM4YASNBHQ>\n> .\n>\n", "Can you share with me a colab of sample code you are trying. No data, just\ndummy data + code. I can give it a shot.\n\nOn Tue, Jun 15, 2021 at 1:22 PM sarath r nair ***@***.***> wrote:\n\n> I think it helps.\n> The reason I wrote a wrapper is that ```model.fit```  does not support\n> dict inputs and outputs. Internally it flattens everything.\n>\n> On Tue, Jun 15, 2021 at 1:07 PM jeroenvermunt ***@***.***>\n> wrote:\n>\n>> I recommend not to use mode.fit. it's custom loss is not great. I wrote a\n>> custom trainer. You want to try that ?\n>>\n>> Sure! you think it would help?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861261358>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ACRE6KBDU6VOC3LCQFH6XETTS37LHANCNFSM4YASNBHQ>\n>> .\n>>\n>\n", "Here is an example of my code with some dummy data:\r\n\r\nhttps://colab.research.google.com/drive/16EG873BZV_6BP-uuWDQvPp0QmRnWt5xh?usp=sharing\r\n\r\nI'm a bit inexperience so the reasoning of the model might make no sense, but it does give the same error as the OP", "Asking for permission.\n\nOn Tue, Jun 15, 2021 at 2:28 PM jeroenvermunt ***@***.***>\nwrote:\n\n> Here is an example of my code with some dummy data:\n>\n>\n> https://colab.research.google.com/drive/16EG873BZV_6BP-uuWDQvPp0QmRnWt5xh?usp=sharing\n>\n> I'm a bit inexperience so the reasoning of the model might make no sense,\n> but it does give the same error as the OP\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861319360>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KB2Q65YWBKTCH3REI3TS4IZVANCNFSM4YASNBHQ>\n> .\n>\n", "I have updated the colab.\n\nCapaciity_loss is wrong. You are passing model output spec there and i did\nnot understand why?\nI have modified the loss, have a look.\nIt's working now, but please check is it correct.\n\nOn Tue, Jun 15, 2021 at 2:43 PM sarath r nair ***@***.***> wrote:\n\n> Asking for permission.\n>\n> On Tue, Jun 15, 2021 at 2:28 PM jeroenvermunt ***@***.***>\n> wrote:\n>\n>> Here is an example of my code with some dummy data:\n>>\n>>\n>> https://colab.research.google.com/drive/16EG873BZV_6BP-uuWDQvPp0QmRnWt5xh?usp=sharing\n>>\n>> I'm a bit inexperience so the reasoning of the model might make no sense,\n>> but it does give the same error as the OP\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861319360>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ACRE6KB2Q65YWBKTCH3REI3TS4IZVANCNFSM4YASNBHQ>\n>> .\n>>\n>\n", "I pass the output because I want to combine both outputs into a single loss function, the y_pred only represents one out of the two outputs right?\r\n\r\n> [\u2026](#)\r\n> I have updated the colab. Capaciity_loss is wrong. You are passing model output spec there and i did not understand why? I have modified the loss, have a look. It's working now, but please check is it correct.\r\n\r\n\r\n", "Oh. You mean you want to take average loss from both outputs. I am not sure\nabout that.\n\nTry using a custom loop trainer. It's hardly a few lines of code. very easy.\n\nOn Tue, Jun 15, 2021 at 5:38 PM jeroenvermunt ***@***.***>\nwrote:\n\n> I pass the output because I want to combine both outputs into a single\n> loss function, the y_pred only represents one out of the two outputs right?\n>\n> \u2026 <#m_-8096883610156409773_>\n> I have updated the colab. Capaciity_loss is wrong. You are passing model\n> output spec there and i did not understand why? I have modified the loss,\n> have a look. It's working now, but please check is it correct.\n>\n> On Tue, Jun 15, 2021 at 2:43 PM sarath r nair *@*.*> wrote: Asking for\n> permission. On Tue, Jun 15, 2021 at 2:28 PM jeroenvermunt @.*> wrote: >\n> Here is an example of my code with some dummy data: > > >\n> https://colab.research.google.com/drive/16EG873BZV_6BP-uuWDQvPp0QmRnWt5xh?usp=sharing\n> > > I'm a bit inexperience so the reasoning of the model might make no\n> sense, > but it does give the same error as the OP > > \u2014 > You are\n> receiving this because you were mentioned. > Reply to this email directly,\n> view it on GitHub > <#47311 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861319360>>,\n> > or unsubscribe >\n> https://github.com/notifications/unsubscribe-auth/ACRE6KB2Q65YWBKTCH3REI3TS4IZVANCNFSM4YASNBHQ\n> > . >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-861443389>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KD4W4CFUSLFN4TXEPLTS47DZANCNFSM4YASNBHQ>\n> .\n>\n", "When I run:\r\n`from tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()`\r\nI get this error:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Conv3DBackpropInputOpV2 only supports NDHWC on the CPU.`\r\nAny ideas? Here's my versions:\r\n\r\n1. Python 3.6.7\r\n2. tensorflow-gpu 2.4.0\r\n3. numpy 1.19.5\r\n\r\nI should **also** mention, I installed and set up this [package](https://github.com/cybertronai/gradient-checkpointing) which I think might have caused this issue, which involved running the following:\r\n`pip install tf-nightly-gpu toposort networkx pytest\r\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/local/cuda/extras/CUPTI/lib64\"\r\n` \r\n\r\nThen I uninstalled the packages when I got the error:\r\n\r\n`pip uninstall tf-nightly-gpu toposort networkx pytest\r\n`", "The error does not seem related to the import statement you mention. And neither of them seem relevant to this issue", "> I am also getting similar errors as filed under this issue: [**Custom loss function is not working**](https://github.com/tensorflow/tensorflow/issues/43650)\r\n> \r\n> I have found that the custom loss function works with the **TensorFlow v1.15.0** but doesn't work with **TensorFlow v2.3.0 & 2.5.0** (both tested)\r\n> \r\n> **But it starts working when eager execution is disabled using:**\r\n> \r\n> ```python\r\n> from tensorflow.python.framework.ops import disable_eager_execution\r\n> disable_eager_execution()\r\n> ```\r\n> \r\n> I think this comments might be helpful:\r\n> \r\n> * [**Complete Report on Errors**](https://github.com/tensorflow/tensorflow/issues/43650#issuecomment-850797985)\r\n> * [**Complete Report on Causes**](https://github.com/tensorflow/tensorflow/issues/43650#issuecomment-850850455)\r\n\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\nthis did the job for me\r\nThanks", "This is one of the solutions if you use tf2.x  and you dont want to close tf eager_execution.\r\nConvert your loss function to a loss layer, and make the parameters advantage and old_prediction as Input layer.\r\nfor example,\r\n    class PPO_loss_layer(tensorflow.keras.layers.Layer):\r\n          def call(self,y_true, y_pred,advantage, old_prediction):\r\n                     ....\r\n    y_true = Input(...)\r\n    advantage= Input(...)\r\n    old_prediction= Input(...)\r\n    loss_layer = PPO_loss_layer()(y_true, y_pred,advantage, old_prediction)\r\n    model = Model(inputs=[y_true,advantage,old_prediction],outputs=loss_layer )", "> This is one of the solutions if you use tf2.x and you dont want to close tf eager_execution.\r\n> Convert your loss function to a loss layer, and make the parameters advantage and old_prediction as Input layer.\r\n> for example,\r\n> class PPO_loss_layer(tensorflow.keras.layers.Layer):\r\n> def call(self,y_true, y_pred,advantage, old_prediction):\r\n> ....\r\n> y_true = Input(...)\r\n> advantage= Input(...)\r\n> old_prediction= Input(...)\r\n> loss_layer = PPO_loss_layer()(y_true, y_pred,advantage, old_prediction)\r\n> model = Model(inputs=[y_true,advantage,old_prediction],outputs=loss_layer )\r\n\r\n@rcx986635 Could you maybe upload a more detailed example? :D I'm a bit new to this topic and I don't know if i fully understand your setup for this solution :)", "> > Hi,\r\n> > I just added \"`del model`\" before instantiating my model which in this case is:\r\n> > ```\r\n> > del model\r\n> > model = build_model(h, w, channels, actions)\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > And it resolved my issue.\r\n> > Do give it a shot. I understand it sounds a bit silly, but it worked for me.\r\n> > What do we have to lose. :) .\r\n> \r\n> This worked for me! Thank you @abhishekvenkat764\r\n\r\n Glad it worked!\r\nI am still perplexed as to why it works though. :D haha", "For some strange reason re-formulating the import statements fixed it for me. \r\nThis is really confusing.", "```\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n```\r\n\r\n\r\nThis worked for me", "Hi @Ellislee1! I was able to  resolve this issue in TF 2.6 by changing the syntax for your model class here.\r\n`class PPO: `to \r\n`class PPO():`  and have taken dummy values for model instantiation. Attaching [Gist ](https://colab.research.google.com/gist/mohantym/5fea442804e4a904b91e4db01b86c19b/github_47311.ipynb#scrollTo=lihaZKoANawN)for reference . Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47311\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47311\">No</a>\n", "> The main issue here is that you are using a custom loss callback that takes an argument `advantage` (from your data generator, most likely numpy arrays). In Tensorflow 2 eager execution, the `advantage` argument will be numpy, whereas `y_true`, `y_pred` are symbolic. The way to solve this is to turn off eager execution\r\n> \r\n> ```\r\n> from tensorflow.python.framework.ops import disable_eager_execution\r\n> disable_eager_execution()\r\n> ```\r\n> \r\n> See similar [stackoverflow issue](https://stackoverflow.com/questions/65366442/cannot-convert-a-symbolic-keras-input-output-to-a-numpy-array-typeerror-when-usi)\r\n\r\nyou save a lot of people\r\n", "The \"[add_loss](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_loss)\" method stated in [this answer](https://stackoverflow.com/a/68275764) seems to solve my problem.\r\nBelow is my code cited from [here](https://github.com/ChintanTrivedi/rl-bot-football/issues/2#issuecomment-969929463), and hope it can help\ud83d\ude42\r\n```py\r\ndef ppo_loss(y_true, y_pred, oldpolicy_probs, advantages, rewards, values):\r\n    newpolicy_probs = y_pred\r\n    ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\r\n    p1 = ratio * advantages\r\n    p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\r\n    actor_loss = -K.mean(K.minimum(p1, p2))\r\n    critic_loss = K.mean(K.square(rewards - values))\r\n    total_loss = critic_discount * critic_loss + actor_loss - entropy_beta * K.mean(\r\n        -(newpolicy_probs * K.log(newpolicy_probs + 1e-10)))\r\n    return total_loss\r\n\r\ndef get_model_actor(input_dims, output_dims):\r\n    state_input = Input(shape=input_dims)\r\n    oldpolicy_probs = Input(shape=(1, output_dims,))\r\n    advantages = Input(shape=(1, 1,))\r\n    rewards = Input(shape=(1, 1,))\r\n    values = Input(shape=(1, 1,))\r\n\r\n    n_actions = output_dims\r\n    feature_extractor = MobileNetV2(\r\n        input_shape=(*MOBILENET_IMG_SIZE, 3),\r\n        weights='imagenet', include_top=False)\r\n    for layer in feature_extractor.layers:\r\n        layer.trainable = False\r\n    x = Flatten(name='flatten')(feature_extractor(state_input))\r\n    x = Dense(1024, activation='relu', name='fc1')(x)\r\n    out_actions = Dense(n_actions, activation='sigmoid')(x)\r\n    model_actor = Model(\r\n        inputs=[state_input, oldpolicy_probs, advantages, rewards, values],\r\n        outputs=[out_actions])\r\n    # ==================================================\r\n    # ==================================================\r\n    model_actor.add_loss(ppo_loss(\r\n        y_true=None,\r\n        y_pred=out_actions,\r\n        oldpolicy_probs=oldpolicy_probs,\r\n        advantages=advantages,\r\n        rewards=rewards,\r\n        values=values))\r\n    model_actor.compile(optimizer=Adam(lr=1e-4))\r\n    # ==================================================\r\n    # ==================================================\r\n    return model_actor\r\n```", "@gruentee\r\n> For some strange reason re-formulating the import statements fixed it for me. This is really confusing.\r\n\r\nCould you please explain what you mean by \"re-formulating\" the imports?\r\n", "@mohantym @rinchenlama0075 @Ellislee1 \r\nI am too trying to convert a python notebook from TF1 to TF2 (in Google Colab). Particularly this one:\r\nhttps://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb\r\nI am facing the same error when it comes to training the model.\r\nSo far these are the changes I've made:\r\n#### 1. I have changed the imports to:\r\n`from tensorflow.keras.models import Sequential, Model, load_model`\r\n`from tensorflow.keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda, Concatenate, UpSampling2D, ReLU, LeakyReLU `\r\n`from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard`\r\n`from tensorflow.keras.optimizers import SGD, Adam, RMSprop`\r\nso as to not use keras imports.\r\n#### 2. I have also added some tf.compat.v1.*\r\nto tf1 functions that don't exist in tf2, namely:\r\n`tf.compat.v1.to_float` and\r\n`tf.compat.v1.assign_add`\r\n#### 3. I've tried to use\r\n`from tensorflow.python.framework.ops import disable_eager_execution disable_eager_execution()`\r\nwhich gives me a different error:\r\n`NotFoundError: 2 root error(s) found.\r\n  (0) NOT_FOUND: Resource localhost/loss_1/hack_layer_loss/Variable/N10tensorflow3VarE does not exist.\r\n\t [[{{node loss_1/hack_layer_loss/AssignAddVariableOp}}]]\r\n\t [[loss_1/mul/_933]]\r\n  (1) NOT_FOUND: Resource localhost/loss_1/hack_layer_loss/Variable/N10tensorflow3VarE does not exist.\r\n\t [[{{node loss_1/hack_layer_loss/AssignAddVariableOp}}]]\r\n0 successful operations.\r\n0 derived errors ignored.`\r\n\r\nAlso downgrading Tensorflow to 2.x < 2.7 does not help. I still get the same errors\r\nThe `del model` hack also didn't work.\r\n\r\nHas anyone had any luck with fixing this?\r\nAny help is tremendously appreciated.", "Guys, problem is mostly with the version of numpy. Aa per my experience.\n\nUninstall all numpy versions, PIL, and tensorflow, then install tensorflow\nagain. It will install all appropriate versions. Worked for me.\n\nOn Sat, 8 Jan, 2022, 6:08 am eirini5th, ***@***.***> wrote:\n\n> I am too trying to convert a python notebook from TF1 to TF2 (in Google\n> Colab). Particularly this one:\n>\n> https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb\n> I am facing the same error when it comes to training the model.\n> So far these are the changes I've made:\n> 1. I have changed the imports to:\n>\n> from tensorflow.keras.models import Sequential, Model, load_model\n> from tensorflow.keras.layers import Reshape, Activation, Conv2D, Input,\n> MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda, Concatenate,\n> UpSampling2D, ReLU, LeakyReLU\n> from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,\n> TensorBoard\n> from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n> so as to not use keras imports.\n> 2. I have also added some tf.compat.v1.*\n>\n> to tf1 functions that don't exist in tf2, namely:\n> tf.compat.v1.to_float and\n> tf.compat.v1.assign_add\n> 3. I've tried to use\n>\n> from tensorflow.python.framework.ops import disable_eager_execution\n> disable_eager_execution()\n> which gives me a different error:\n> NotFoundError: 2 root error(s) found. (0) NOT_FOUND: Resource\n> localhost/loss_1/hack_layer_loss/Variable/N10tensorflow3VarE does not\n> exist. [[{{node loss_1/hack_layer_loss/AssignAddVariableOp}}]]\n> [[loss_1/mul/_933]] (1) NOT_FOUND: Resource\n> localhost/loss_1/hack_layer_loss/Variable/N10tensorflow3VarE does not\n> exist. [[{{node loss_1/hack_layer_loss/AssignAddVariableOp}}]] 0 successful\n> operations. 0 derived errors ignored.\n>\n> Also downgrading Tensorflow to 2.x < 2.7 does not help. I still get the\n> same errors\n> The del model hack also didn't work.\n>\n> Any help is tremendously appreciated.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47311#issuecomment-1007842923>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACRE6KFJ55S5G7FDE3ZOB7LUU6BPZANCNFSM4YASNBHQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "@s4sarath nope didn't work"]}, {"number": 47310, "title": "Update audio recognition tutorial link", "body": "Updated audio recognition tutorial link.\r\n\r\nFixes issue [#47305](https://github.com/tensorflow/tensorflow/issues/47305)", "comments": []}, {"number": 47309, "title": "How to fix 'Tidx' type in TensorFlowOpLayer (Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6.1\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: RTX3070 8GB\r\n\r\nAfter upgrading to a newer version of TensorFlow (1.14 -> 2.4.0) in my project based on MaskRCNN I get an error when trying read model with model_from_json (tf.keras.model.model_from_json):\r\n```ValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean/Mean' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>```\r\n", "comments": ["Traceback :\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1853, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean/Mean' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Project\\load_model.py\", line 111, in read_model\r\n    model = model_from_json(file, custom_objects=custom_objects)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 131, in model_from_json\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 177, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 358, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 669, in from_config\r\n    config, custom_objects)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1285, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1233, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 786, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 3169, in call\r\n    return self._make_op(inputs)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 3191, in _make_op\r\n    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1856, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean/Mean' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n```", "@kiflowb777 \r\nPlease share simple stand alone code for us to replicate the issue reported or a  colab gist.\r\nAs per the error reported, can you please ensure that the original input data types match with the data types of your new inputs.\r\nCasting keras input to float64 can help resolve this error.\r\nAlso refer to similar issue: #32375", "Ok, I will try to prepare a simple code. \r\nFor now, I have found where the error is occurring:\r\n```\r\n...\r\nloss = tf.reduce_mean(input_tensor=layer.output, keepdims=True)  # here the 'tf_op_layer_Mean/Mean' is created\r\nmodel.add_loss(loss)\r\n....\r\n```\r\nAnd after fit and save model:\r\n```\r\n...\r\nmodel = model_from_json(file, custom_objects=custom_objects)  # here the error with 'tf_op_layer_Mean/Mean' is throw.\r\n```\r\n\r\nAny ideas why index 'Tidx' in function  tf.reduce_mean/tf_op_layer_Mean has values of type float instead of int?", "I noticed that **Tidx** corresponds to the **axis** parameter in\r\n```tf.math.reduce_mean(input_tensor, axis=None, keepdims=False, name=None)```\r\n\r\nWhen axis is None, its value is determined by the rank of the tensor, but in MaskRCNN, the rank of all tensors corresponding to losses is zero:\r\n\r\n```\r\nlayer: name=rpn_class_loss, type=int32, rank=Tensor(\"Rank:0\", shape=(), dtype=int32)\r\nlayer: name=rpn_bbox_loss, type=float32, rank=Tensor(\"Rank_2:0\", shape=(), dtype=int32)\r\nlayer: name=mrcnn_class_loss, type=int32, rank=Tensor(\"Rank_4:0\", shape=(), dtype=int32)\r\nlayer: name=mrcnn_bbox_loss, type=float32, rank=Tensor(\"Rank_6:0\", shape=(), dtype=int32)\r\nlayer: name=mrcnn_mask_loss, type=float32, rank=Tensor(\"Rank_8:0\", shape=(), dtype=int32)\r\n```\r\n", "But if I manually set the axis to zero then I get: \r\n```\r\nValueError: Invalid reduction dimension 0 for input with 0 dimensions. for '{{node Mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=true](rpn_class_loss/cond/Merge, Mean/reduction_indices)' with input shapes: [], [] and with computed input tensors: input[1] = <0>.\r\n```\r\n\r\nTraceback :\r\n```\r\n  File \"D:\\Project\\load_model.py\", line 64, in compile_model_network\r\n    loss = (tf.math.reduce_mean(input_tensor=layer.output, keepdims=True, axis=0) * loss_weights.get(name, 1.))\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2372, in reduce_mean\r\n    name=name))\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5781, in mean\r\n    name=name)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 750, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3536, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2016, in __init__\r\n    control_input_ops, op_def)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1856, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Invalid reduction dimension 0 for input with 0 dimensions. for '{{node Mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=true](rpn_class_loss/cond/Merge, Mean/reduction_indices)' with input shapes: [], [] and with computed input tensors: input[1] = <0>.\r\n```\r\n", "Maybe the problem is that MaskRCNN loss layer is a Lambda type:\r\n```\r\n<tensorflow.python.keras.layers.core.Lambda object at 0x0000012404B51D30>\r\n```\r\n", "I noticed that if we disable 'v2 behavior' with ```tf.compat.v1.disable_v2_behavior()``` the layer created with ```tf.math.reduce_mean``` in the ```model.summary()``` is named:\r\n```\r\n...\r\ntf_op_layer_Mean (TensorFlowOpL [()] \r\n...\r\n```\r\n\r\nbut if we enable 'v2 behaviour' the layer is named:\r\n```\r\n...\r\ntf.math.reduce_mean (TFOpLam (\r\n...\r\n```\r\n", "@kiflowb777\r\nPlease share simple stand alone code for us to replicate the issue reported or a colab gist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Simple code to replicate issue:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import model_from_json\r\nfrom tensorflow.keras.layers import Layer\r\nfrom tensorflow.keras import Input, Model\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n# tf.config.run_functions_eagerly(False)\r\n\r\n\r\ndef loss_function(inputs):\r\n    return tf.keras.backend.switch(tf.size(input=inputs) > 0, tf.keras.backend.mean(inputs), tf.constant(0.0))\r\n\r\n\r\nclass RpnClassLoss(Layer):\r\n    def __init__(self, name=\"loss_function\", **kwargs):\r\n        super(RpnClassLoss, self).__init__(name=name, **kwargs)\r\n\r\n    def call(self, inputs):\r\n        loss = loss_function(inputs)\r\n        return loss\r\n\r\n\r\nclass Output(Layer):\r\n    def __init__(self, **kwargs):\r\n        super(Output, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        return inputs\r\n\r\n\r\ninputs = Input(shape=(3,))\r\noutput = Output()(inputs)\r\n# loss = tf.convert_to_tensor(loss_function(inputs))\r\nloss = RpnClassLoss()(inputs)\r\noutputs = [output, loss]\r\nmodel = Model(inputs, outputs)\r\n\r\nloss = tf.reduce_mean(loss, keepdims=True)\r\nmodel.add_loss(loss)\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer=\"adam\", loss=[None] * len(model.outputs))\r\nmodel.fit(np.random.random((2, 3)))\r\n\r\nmodel_json = model.to_json()\r\nwith open(\"model.json\", \"w\") as json_file:\r\n    json_file.write(model_json)\r\nmodel.save_weights(\"model.h5\")\r\n\r\njson_file = open('model.json', 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nprint(\"Try model_from_json\")\r\nloaded_model = model_from_json(loaded_model_json, custom_objects={\"Output\": Output, \"RpnClassLoss\": RpnClassLoss})  # ValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1'\r\nprint(\"model_from_json ok\")\r\n\r\n```", "https://gist.github.com/kiflowb777/c14349437f0a08d5331eb48a30950bc4", "I tried moving the .add_loss() method inside the custom layer RpnClassLoss:\r\nfrom:\r\n```python\r\nloss = tf.reduce_mean(loss, keepdims=True)\r\nmodel.add_loss(loss)\r\n```\r\nto\r\n```python\r\nclass RpnClassLoss(Layer):\r\n    def call(self, inputs):\r\n        loss = loss_function(inputs)\r\n        loss = tf.reduce_mean(loss, keepdims=True)\r\n        self.add_loss(loss)\r\n        return loss\r\n```\r\n\r\n but then I get:\r\n```batch\r\nTraceback (most recent call last):\r\n  File \"D:\\project\\model.py\", line 229, in read_model_for_training\r\n    model = model_from_json(loaded_model_json, custom_objects=custom_objects)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 131, in model_from_json\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 177, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 358, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 669, in from_config\r\n    config, custom_objects)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1285, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1233, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\nUnboundLocalError: local variable 'kwargs' referenced before assignment\r\n \r\n```", "I am able to replicate the issue from the github gist, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8362e6c0dbf85a130f17c1a8272f9d12/untitled.ipynb)", "I have checked all versions of the TensorFlow (1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.3.2, 2.4.0rc0, 2.4.0, 2.4.1) and on each of them I am able to reproduce the problem.\r\n\r\nhttps://gist.github.com/kiflowb777/374cfb87fd17ef615ef6126e576e65d7", "On the version 1.14.0 there is a problem with model.to_json(). \r\n\r\n```batch\r\nTypeError: ('Not JSON Serializable:', b'\\n\\x03mul\\x12\\x03Mul\\x1a\\x07input_1\\x1a\\x05mul/y*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n```\r\n\r\nMore info: https://github.com/tensorflow/tensorflow/issues/47696", "I tried the sample code with Python 3.7 and 3.8. On each of them I'm able to reproduce the issue:\r\n\r\nPython 3.7:\r\n```batch\r\n2021-03-12 13:03:47.148575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 13:03:50.488120: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-12 13:03:50.488980: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-12 13:03:50.533928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-12 13:03:50.533989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 13:03:50.542407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-12 13:03:50.542434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-12 13:03:50.546530: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-12 13:03:50.548001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-12 13:03:50.556211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-12 13:03:50.559888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-12 13:03:50.560891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-12 13:03:50.561010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-12 13:03:50.561389: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-12 13:03:50.561710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-12 13:03:50.561730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 13:03:50.561739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-12 13:03:50.561766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-12 13:03:50.561774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-12 13:03:50.561781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-12 13:03:50.561788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-12 13:03:50.561796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-12 13:03:50.561802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-12 13:03:50.561844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-12 13:03:51.147306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-12 13:03:51.147360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-12 13:03:51.147453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-12 13:03:51.147703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6589 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-12 13:03:51.148460: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-12 13:03:51.149454: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 3)]          0                                            \r\n__________________________________________________________________________________________________\r\ntf_op_layer_mul (TensorFlowOpLa [(None, 3)]          0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mean (TensorFlowOpL [()]                 0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mean_1 (TensorFlowO [()]                 0           tf_op_layer_Mean[0][0]           \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mean_2 (TensorFlowO [()]                 0           tf_op_layer_Mean_1[0][0]         \r\n__________________________________________________________________________________________________\r\nadd_loss (AddLoss)              ()                   0           tf_op_layer_Mean_2[0][0]         \r\n==================================================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\nTrain on 2 samples\r\n\r\n2/2 [==============================] - ETA: 0s - loss: 0.5957\r\n2/2 [==============================] - 0s 10ms/sample - loss: 0.5957\r\nTry model_from_json\r\nTraceback (most recent call last):\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1853, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"json_test.py\", line 36, in <module>\r\n    loaded_model = model_from_json(loaded_model_json)  # ValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1'\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 131, in model_from_json\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 177, in deserialize\r\n    printable_module_name='layer')\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 358, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 669, in from_config\r\n    config, custom_objects)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1285, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1233, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 786, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 3169, in call\r\n    return self._make_op(inputs)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 3191, in _make_op\r\n    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1856, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n```\r\n\r\nPython 3.8:\r\n```batch\r\n2021-03-12 12:58:42.163756: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 12:58:45.610755: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-12 12:58:45.611820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-12 12:58:45.655594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-12 12:58:45.655650: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 12:58:45.663501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-12 12:58:45.663530: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-12 12:58:45.667875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-12 12:58:45.669401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-12 12:58:45.677569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-12 12:58:45.681111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-12 12:58:45.681996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-12 12:58:45.682149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-12 12:58:45.682675: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-12 12:58:45.683045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-12 12:58:45.683064: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 12:58:45.683073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-12 12:58:45.683081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-12 12:58:45.683088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-12 12:58:45.683095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-12 12:58:45.683102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-12 12:58:45.683119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-12 12:58:45.683125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-12 12:58:45.683191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-12 12:58:46.251940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-12 12:58:46.251996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-12 12:58:46.252109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-12 12:58:46.252347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6589 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-12 12:58:46.253043: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-12 12:58:46.254111: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 3)]          0                                            \r\n__________________________________________________________________________________________________\r\ntf_op_layer_mul (TensorFlowOpLa [(None, 3)]          0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mean (TensorFlowOpL [()]                 0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mean_1 (TensorFlowO [()]                 0           tf_op_layer_Mean[0][0]           \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mean_2 (TensorFlowO [()]                 0           tf_op_layer_Mean_1[0][0]         \r\n__________________________________________________________________________________________________\r\nadd_loss (AddLoss)              ()                   0           tf_op_layer_Mean_2[0][0]         \r\n==================================================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\nTrain on 2 samples\r\n\r\n2/2 [==============================] - ETA: 0s - loss: 0.3721\r\n2/2 [==============================] - 0s 10ms/sample - loss: 0.3721\r\nTry model_from_json\r\nTraceback (most recent call last):\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1853, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"json_test.py\", line 36, in <module>\r\n    loaded_model = model_from_json(loaded_model_json)  # ValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1'\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py\", line 131, in model_from_json\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 173, in deserialize\r\n    return generic_utils.deserialize_keras_object(\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 354, in deserialize_keras_object\r\n    return cls.from_config(\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 668, in from_config\r\n    input_tensors, output_tensors, created_layers = reconstruct_from_config(\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1285, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 1233, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 786, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 3169, in call\r\n    return self._make_op(inputs)\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 3191, in _make_op\r\n    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1856, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n```", "I wrote a shorter code to reproduce the issue:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import model_from_json\r\nfrom tensorflow.keras import Input, Model\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ninputs = Input(shape=(3,))\r\noutput = inputs * 2\r\noutput_loss = tf.keras.backend.mean(inputs)\r\noutputs = [output, output_loss]\r\nmodel = Model(inputs, outputs)\r\n\r\nloss = tf.reduce_mean((output_loss * 0.9))\r\nmodel.add_loss(loss)\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer=\"adam\", loss=[None] * len(model.outputs))\r\nmodel.fit(np.random.random((2, 3)))\r\n\r\nmodel_json = model.to_json()\r\njson_filename = \"model.json\"\r\nwith open(json_filename, \"w\") as json_file:\r\n    json_file.write(model_json)\r\nmodel.save_weights(\"model.h5\")\r\n\r\njson_file = open(json_filename, 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\n\r\nloaded_model = model_from_json(loaded_model_json)  # ValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean_1/Mean_1'\r\nprint(\"model_from_json ok\")\r\n```", "On version 2.5.0 of the TensorFlow I am still able to replicate the error.", "`Input` will go to create a `LambdaLayer` again.\r\n\r\nTry to use  `tf.keras.Input`", "Was able to replicate this issue in TF 2.6.0-dev20210527 ,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/7ea54b7baba812fee361ce0561d0cd8a/untitled13.ipynb#scrollTo=PA0JdhGTgy8j)...Thanks ! ", "@sushreebarsa @kiflowb777  Can you try to run it without `tf.compat.v1.disable_eager_execution()`?", "Was able to reproduce this issue in TF 2.6.0-dev20210527 without any error,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/c7c451c324116f0648a6c8487a1832a5/untitled22.ipynb#scrollTo=lf7v9AqSAmHX)..Thanks !", "@bhack \r\nThe code to reproduce the issue works without ```tf.compat.v1.disable_eager_execution()```.\r\n\r\nBut I have a large project based on a MaskRCNN that requires the use of a ```tf.compat.v1.disable_eager_execution()```, without it a training does not work on TensorFlow 2.x\r\n\r\nPlease look:\r\nhttps://github.com/matterport/Mask_RCNN", "@kiflowb777 As It seems to me an edge use case in the meantime can you try to use our official MaskRCNN as it is directly compatible with TF2.x:\r\n\r\nhttps://github.com/tensorflow/models/tree/master/official/vision/detection", "When you run the code in eager execution, you can notice the better error message which says `ValueError: Target data is missing. Your model was compiled with loss=ListWrapper([None, None]), and therefore expects target data to be provided in `fit()`.`\r\n[Attached](https://colab.sandbox.google.com/gist/sachinprasadhs/451ce96ceb832b3086f8b95ff8e93734/47309.ipynb) Gist for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47309\">No</a>\n"]}, {"number": 47308, "title": "Need a easy way to let different weights have different LR in Keras.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes, may be\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAt nowtimes, if I want to train model and use defferent LR for weights, I need to give up `Model.Fit()`.\r\n\r\n\r\n**Will this change the current api? How?**\r\nYes, probably:\r\n- Layer.add_weight() may have extra param \"Init_LR\"\r\n- Weights may add attribute \u201cInit_LR\u201d\r\n- Some others.\r\n\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who want to use this.\r\nDeveloper who also use PyTorch.", "comments": ["It's easy to implement it with custom training loop.\r\nJust set ```gradient = gradient * lr / base_lr``` for different variables.", "```\r\nimport numpy\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.eager import backprop\r\nfrom tensorflow.python.keras.engine import data_adapter\r\nclass DoubleBiasLRModel(keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n        super(DoubleBiasLRModel, self).__init__(**kwargs)\r\n        self.LR_list=[]\r\n        for weights in self.trainable_variables:\r\n            if \"bias\" in weights.name or \"beta\" in weights.name:\r\n                self.LR_list.append(2.)\r\n            else:\r\n                self.LR_list.append(1.)\r\n        self.LR_list=np.array(self.LR_list)\r\n\r\n    def train_step(self, data):\r\n        data = data_adapter.expand_1d(data)\r\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n        with backprop.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\r\n\r\n        trainable_vars = self.trainable_variables\r\n        gradients = tape.gradient(loss, trainable_vars) * self.LR_list\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n        return {m.name: m.result() for m in self.metrics}\r\n```\r\nThen def model with `DoubleBiasLRModel(xxx)` instead of ` keras.Model(xxx)`\r\nI use this code to solve my need. I wish we can have more elegant and simple ", "The responses above look good. In Keras we provide basic components and leave room for customization, and the different lr scenario is an example of customization."]}, {"number": 47307, "title": "Building v2.3.1 from sources with avx512 support on a system without avx512", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 2.7.15\r\n- Installed using virtualenv? pip? conda?: build from sources\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: without cuda\r\n- GPU model and memory: without GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am able to build and run TF 2.3.1 using `-msse3`. I am now trying to build it for `-march=skylake-avx512` on a system which is not `skylake-avx512`. As the build system does not support avx512 instruction sets so during the build phase it crashes with error like\r\n\r\n```\r\nERROR: tensorflow-2.3.1/tensorflow/compiler/mlir/tensorflow/BUILD:171:7: Generating code from table: transforms/canonicalize.td //tensorflow\r\n/compiler/mlir/tensorflow:tensorflow_canonicalize_inc_gen__gen_rewriters_genrule failed (Illegal instruction): bash failed: error executing command\r\n```\r\n\r\nI think this is because many internal tools e.g.  `bazel-out/k8-opt/bin/tensorflow/compiler/mlir/xla/operator_writer_gen` were build with `-march=skylake-avx512` flags.  I was wondering if there is a way to build these internal mlir tools with `-march=native` only\r\n\r\n", "comments": ["For these the tools are built with host compiler options, I'm not sure where/how you are setting the march file. Normally the tools are build with different options than the general bazel run so that this isn't needed. Try passing different arch via --host_copt/--host-cxxopt for the tools", "@jpienaar , I set the following env before calling bazel\r\n```\r\nexport CC_OPT_FLAGS=\"-march=skylake-avx512\"\r\n```\r\nand looks like these options are then passed to all tools. I am trying --host_copt/--host-cxxopt now (thanks for the suggestion).", "looks like `--host_*opt` is not properly used for building mlir-tblgen \r\n\r\n```\r\nbazel --batch --output_user_root ../build build -s --verbose_failures --host_copt=-msse3 --host_cxxopt=-msse3 --copt=-march=skylake-avx512 --config=opt --cxxopt=-std=c++17 -j 16 --config=noaws --config=nogcp --config=nohdfs --config=nonccl --distinct_host_configuration=false @llvm-project//mlir:mlir-tblgen\r\n....\r\n...\r\n/build/muz/avx/w/slc7_amd64_gcc900/external/gcc/9.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/MlirTableGenMain/mlir-tblgen.d '-frandom-seed=bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/MlirTableGenMain/mlir-tblgen.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/zlib/include -isystem bazel-out/k8-opt/bin/external/zlib/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=skylake-avx512' -Wno-sign-compare '-std=c++14' '-std=c++17' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/llvm-project/mlir/tools/mlir-tblgen/mlir-tblgen.cpp -o bazel-out/k8-opt/bin/external/llvm-project/mlir/_objs/MlirTableGenMain/mlir-tblgen.o\r\n```\r\n\r\nthis causes the build to fail due to ` Illegal instruction`\r\n```\r\n/bin/bash: line 1: 16797 Illegal instruction     bazel-out/k8-opt/bin/external/llvm-project/mlir/mlir-tblgen -gen-struct-attr-decls tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/lhlo_gpu_ops_structs.td -Ibazel-out/k8-opt/bin -I external/llvm-project/mlir/include -I external/org_tensorflow -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/org_tensorflow -Itensorflow/compiler/mlir/hlo/include -Iexternal/org_tensorflow/tensorflow/compiler/mlir/hlo/include -Ibazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/include -I $(dirname tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/lhlo_gpu_ops_structs.td) -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/lhlo_gpu_ops_structs.h.inc\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n```", "That is different: you are building the tool with that option, then you are basically hitting a LLVM bug where avx512 isn't supported, but you are explicitly requesting to build the binary. The host flags come to play where you are building a target that uses mlir-tblgen as a tool (e.g., by way of gentbl rule). Also try passing in option via --cxopt=\"-march=skylake-avx512 flag to bazel vs environment variable.", "ah my bad, `--distinct_host_configuration=false ` was the issue, sorry for the noise. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47307\">No</a>\n"]}, {"number": 47306, "title": "TopKV2 InvalidArgumentError: k must be scalar", "body": "Running on Google Colab\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n\r\n### Current behaviour\r\n\r\nI am trying to use tf.raw_ops.TopKV2 to get the top k values of my output, with k varying per row.\r\n\r\nAs stated in the documentation k expects \"A Tensor of type int32. 0-D.  Number of top elements to look for along the last dimension (along each row for matrices).\" \r\n\r\nHowever, it gives me InvalidArgumentError: k must be scalar, got shape [2] [Op:TopKV2]\r\n\r\n### Code\r\n\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput = np.array([[0,1,2], [5,4,3]])\r\nk = np.array([1,2])\r\n\r\ntf.raw_ops.TopKV2(input=input, k=k)\r\n```\r\nTrace:\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n<ipython-input-11-245902c47f71> in <module>()\r\n      5 k = np.array([1,2])\r\n      6 \r\n----> 7 tf.raw_ops.TopKV2(input=input, k=k)\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_export.py in wrapper(*args, **kwargs)\r\n    402           'Please pass these args as kwargs instead.'\r\n    403           .format(f=f.__name__, kwargs=f_argspec.args))\r\n--> 404     return f(**kwargs)\r\n    405 \r\n    406   return tf_decorator.make_decorator(f, wrapper, decorator_argspec=f_argspec)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py in top_kv2(input, k, sorted, name)\r\n  11426       return _result\r\n  11427     except _core._NotOkStatusException as e:\r\n> 11428       _ops.raise_from_not_ok_status(e, name)\r\n  11429     except _core._FallbackException:\r\n  11430       pass\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6860   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n   6864 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: k must be scalar, got shape [2] [Op:TopKV2]\r\n```\r\n### Expected behaviour\r\n\r\nI expect topKV2 to accept a list/array/tensor for the k parameter. Returning value 2, index 2 for the first row and values 5 4, indices 0 1 for the second row.\r\n\r\n### Extra\r\n\r\nAdditionally, the documentation example use of TopKV2 is\r\n```\r\ntf.raw_ops.TopKV2(\r\n    input, k, sorted=True, name=None\r\n)\r\n```\r\n\r\nHowever, providing the input and k arguments directly gives the following error:\r\ntop_kv2 only takes keyword args (possible keys: ['input', 'k', 'sorted', 'name']). Please pass these args as kwargs instead.\r\n\r\n\r\n", "comments": ["@robinderat \r\nK is a scalar ex: k=3 but you are using a vector. \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n", "@Saduf2019 \r\nI know I am using a vector. That is what I want. Please note I am using TopKV2 not TopK. \r\nFor TopK documentation says:\r\nk: An int that is >= 0. Number of top elements to look for along the last dimension (along each row for matrices). \r\n\r\nFor TopKV2 documentation says:\r\nk: A Tensor of type int32. 0-D. Number of top elements to look for along the last dimension (along each row for matrices). \r\n\r\nTherefore, I should be able to use a vector/tensor, right?\r\n", "Value of `k` can be 0-D tensor as per the docs. With `k=np.ndim(np.array([1,2]))` we get 1-D tensor.\r\n```python\r\nimport numpy as np\r\nnp.ndim(2)\r\n==> 0\r\nnp.ndim(np.array([1,2]))\r\n==> 1\r\n```\r\nPerhaps we can update docs to state that k expects a scalar tensor (0-D).\r\n", "Ah yes, of course. My bad. Thanks for explanation!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47306\">No</a>\n"]}, {"number": 47305, "title": "tutorial URL outdated, should probably be \"www.tensorflow.org/tutorials/audio/simple_audio\"", "body": "The url \r\n`https://www.tensorflow.org/tutorials/audio_recognition`\r\nin line 21 of `tensorflow/tensorflow/examples/speech_commands/train.py` is outdated and produces an 404 error. \r\n\r\nIt should probably be:\r\n`https://www.tensorflow.org/tutorials/audio/simple_audio`\r\n", "comments": ["@OlafEichstaedt,\r\nThank you for reporting the issue.\r\n\r\nThe requested change is being tracked in PR [#47310](https://github.com/tensorflow/tensorflow/pull/47310). The link will be updated once the PR is merged. Thanks!"]}, {"number": 47304, "title": "Remove incorrect test in batch_to_space_nd.cc and space_to_batch_nd.cc", "body": "@tensorflow/micro\r\n\r\nEdit: Removes incorrect tests in the prepare function after discussion with @njeffrie.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> I think this check may be incorrect overall. I've run into an issue with both >= and <=, so removing it altogether seems reasonable to me. It's also not present in the TFLite version of the kernel (I was getting too \"clever\", it seems).\r\n\r\nOk thanks, I have removed it now from batch_to_space_nd.cc and space_to_batch_nd.cc.", "Looks like one of the batch_so_space and space_to_batch tests is failing since it was evaluating the check we are removing. Please remove that test and try to run `bazel run tensorflow/lite/micro/kernels:space_to_batch_nd_test` and `bazel run tensorflow/lite/micro/kernels:batch_to_space_nd_test`\r\n\r\nSorry for missing this in the first pass of reviews.", "> Looks like one of the batch_so_space and space_to_batch tests is failing since it was evaluating the check we are removing. Please remove that test and try to run `bazel run tensorflow/lite/micro/kernels:space_to_batch_nd_test` and `bazel run tensorflow/lite/micro/kernels:batch_to_space_nd_test`\r\n> \r\n> Sorry for missing this in the first pass of reviews.\r\n\r\nDone! I also removed the `// TODO(b/158102673): workaround for not having fatal test assertions.`, was that correct?", "Ahh, I meant more to remove the whole third test case, without removing the equality check in the validate method. Removing the test case starting with `TF_LITE_MICRO_TEST(BatchToSpaceInvalidOutputDimensionShouldFail) {` and the corresponding test case in SpaceToBatch. Apologies for the confusion.", "Sorry that it needed so many iterations - now it should be fine \ud83d\ude48"]}, {"number": 47303, "title": "Add compatability for h5py 3", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe are developing some software that relies heavily on both H5Py and TensorFlow and require the latest release of each. In terms of functionality the code will run however if the user installs both separately but the setup.py will throw errors reading conflicts in dependencies.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nThose who wish to have a python module that relies on H5Py 3 and TensorFlow\r\n\r\n**Any Other info.**\r\n", "comments": ["Can you detail the errors?", "Sorry I should have been a little more clear. It isn't an issue with any tensor flow functionality it is simply when I run pip install on my package. In the setup.py I have both h5py>3 and tensorflow>2 and so pip fails to run because these requirements conflict with one another. I have no issues with actual tf operations failing. ", "Ok, so this then needs to wait until we are confident that there are no big bugs if we bump our `h5py` dependency.", "Okay no problem. Is there anything I can do to help this? Or is there a timeline on that process? If not we can close this issue.", "Let's try sending a PR with the bound updated and see what breaks.", "Is there any update on this?", "Can this ticket be closed?\r\nMultiple sources indicate, that `tensorflow` is compatible to and built with support for `h5py~=3.1` since December 2020:\r\n* tensorflow/tools/ci_build/release/requirements_common.txt\r\n* tensorflow/tools/pip_package/setup.py ([c0dbdfb7](https://github.com/tensorflow/tensorflow/commit/c0dbdfb718de6481b00d7ff17a0ff763cf7a7544#diff-f526feeafa1000c4773410bdc5417c4022cb2c7b686ae658b629beb541ae9112))", "yes it appears to be working. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47303\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47303\">No</a>\n"]}, {"number": 47302, "title": "Hide subprocess windows for windows os", "body": "Unlike Linux-based operating systems, Windows will create a new console for a child process if the parent is a GUI application.\r\n\r\nThis causes console window flashes when `ptxas` is called if the process that loaded `tensorflow.dll` is running in GUI mode.\r\n\r\nThis PR makes children's windows hidden by default. \r\n\r\nAs tensorflow does not start any GUI programs, it should not cause any bad effect.\r\n\r\nConsole mode is not affected.", "comments": []}, {"number": 47301, "title": "`saved_model_cli show` report a error when apply it to a bert fine-tuned model", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Ubuntu 16.04.2 LTS\r\n- TensorFlow installed from binary \r\n- TensorFlow version v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.7.9\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Quadro P5000 16G\r\n\r\n**Describe the current behavior**\r\n\r\nI am fine-tune a bert model for text classification, and there are preprocess procedure inside the model. The pre-trained model and preprocess model are all from tfhub so it should be fine, and code written by me is all about read file, build model with a few dense layers, fit, evaluate, predict and save.\r\n\r\nAfter all these, I want to use `saved_model_cli` before serving like introduced in [this tutorial](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#save_your_model)\r\n\r\nWhen using command\r\n\r\n```bash\r\n$ saved_model_cli show --dir training_checkpoints_sm/ --all\r\n2021-02-22 11:38:35.613708: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['text'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1)\r\n        name: serving_default_text:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['classifier'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: StatefulPartitionedCall_2:0\r\n  Method name is: tensorflow/serving/predict\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3929, in _get_op_def\r\n    return self._op_def_cache[type]\r\nKeyError: 'RegexSplitWithOffsets'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 890, in load_internal\r\n    ckpt_options, filters)\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 132, in __init__\r\n    meta_graph.graph_def.library))\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 340, in load_function_def_library\r\n    func_graph = function_def_lib.function_def_to_graph(copy)\r\n  File \"/home/kingsoft/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/function_def_to_graph.py\", line 59, in function_def_to_graph\r\n    fdef, input_shapes)\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/function_def_to_graph.py\", line 220, in function_def_to_graph_def\r\n    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3934, in _get_op_def\r\n    buf)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'RegexSplitWithOffsets' in binary running on k8s-w-10-13-84-8. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/user/anaconda3/envs/gang/bin/saved_model_cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1192, in main\r\n    args.func(args)\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 719, in show\r\n    _show_all(args.dir)\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 307, in _show_all\r\n    _show_defined_functions(saved_model_dir)\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 187, in _show_defined_functions\r\n    trackable_object = load.load(saved_model_dir)\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 859, in load\r\n    return load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"/home/user/anaconda3/envs/gang/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 893, in load_internal\r\n    str(err) + \"\\n If trying to load on a different device from the \"\r\nFileNotFoundError: Op type not registered 'RegexSplitWithOffsets' in binary running on k8s-w-10-13-84-8. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.\r\n```\r\n\r\nThis error `KeyError: 'RegexSplitWithOffsets'` really looks like when I forget to import `tensorflow_text` in my code.\r\n\r\n```python\r\nimport tensorflow_text as text  # Registers the ops.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behavior is I can import tensorflow_text when using this tool `saved_model_cli`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\ndef build_classifier_model():\r\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\r\n    preprocessor = hub.KerasLayer(\r\n        \"https://tfhub.dev/tensorflow/bert_zh_preprocess/3\", name=\"preprocessor\")\r\n    encoder_inputs = preprocessor(text_input)\r\n    encoder = hub.KerasLayer(\r\n        \"https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/3\",\r\n        trainable=True,\r\n        name=\"BERT_encoder\")\r\n    outputs = encoder(encoder_inputs)\r\n    net = outputs['pooled_output']\r\n    net = tf.keras.layers.Dropout(0.1)(net)\r\n    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\r\n    return tf.keras.Model(text_input, net)\r\n\r\n...\r\n\r\nhistory = classifier_model.fit(x=train_dataset,\r\n                               epochs=epochs,\r\n                               callbacks=[checkpoint_callback, early_stopping],\r\n                               validation_data=val_dataset,\r\n                               class_weight=class_weight)\r\n\r\n...\r\n\r\nclassifier_model.save(checkpoint_dir + \"_sm\")\r\n\r\n...\r\n\r\n```\r\n\r\nCan I successfully use `saved_model_cli`? or skip this part in that tutorial maybe?\r\n", "comments": ["@HarborZeng \r\nThe code provided is not complete  hence it would be difficult for us to pinpoint the issue. Please share complete stand alone code to replicate the issue or a colab gist with the error reported.? That will allow us to determine the source of the issue easily. \r\nThanks!", "@Saduf2019 Hey here is a colab example: <https://colab.research.google.com/drive/1wW7p79YsSJDy9fvED7OnJ6mk37zlvFW8?usp=sharing>", "I am able to replicate the issue reported on [tf 2.3](https://colab.research.google.com/gist/Saduf2019/8b70737e15bd5c19b50bf54dfc4b08aa/untitled555.ipynb), tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a25ebe2bc7f8b06644bce4a7e97799d4/untitled548.ipynb).\r\nThanks!", "Was able reproduce this issue in TF v2.5,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/a8da6e7b2920ac0a70757900b74b12ff/untitled14.ipynb)..Thanks !", "Have you tried the solution mentioned [here](https://github.com/tensorflow/hub/issues/732). Thanks!", "> Have you tried the solution mentioned [here](https://github.com/tensorflow/hub/issues/732). Thanks!\r\n\r\nIt's more like a binary executable bug rather than my python code import-related problem.", "But it does not matter anymore because it has been so many months passed..........", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47301\">No</a>\n"]}, {"number": 47300, "title": "Remove unnecessary bool conversion in basic test.", "body": "Add tie breaking test for axis=1.", "comments": ["Any updates ?", "Any updates please ?"]}, {"number": 47299, "title": "TFLM / example: micro_speech / mbed compile -m DISCO_F746NG -t GCC_ARM collect2: error: ld returned 1 exit status", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: TensorFlow/Lite/Micro\r\n- Python version: Python 2.7.17 and Python3 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip, and no virtualenv, no conda\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- mbed version: 1.10.5\r\n- Target platform: DISCO_F746NG\r\n\r\n**Describe the problem**\r\nTensorFlow Lite Micro cross compile for micro speech example at DISCO_F746NG platform. It was 100% completed in compile, but report link error.\r\nI follow the advice from [issue 46721](https://github.com/tensorflow/tensorflow/issues/46721) and modify the tensorflow/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc as the following, see also [link](https://github.com/marconi1964/tensorflow/blob/example/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc)\r\n\r\n`# Settings for the Discovery STM32F746NG board.`\r\n`ifeq ($(TARGET),$(filter $(TARGET),mbed))`\r\n\r\n`  micro_speech_MBED_PROJECT_FILES += \\`\r\n`    AUDIO_DISCO_F746NG.lib \\`\r\n`    BSP_DISCO_F746NG.lib \\`\r\n`    SDRAM_DISCO_F746NG.lib \\`\r\n`    LCD_DISCO_F746NG.lib`\r\n`  MICRO_SPEECH_SRCS += \\`\r\n`\ttensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc \\`\r\n`\ttensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc`\r\n\r\n`endif`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe procedures are\r\n`$ cd tensorflow`\r\n`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed generate_micro_speech_mbed_project`\r\n`$ mbed config root .`\r\n`$ mbed deploy`\r\n`$ mbed compile -m DISCO_F746NG -t GCC_ARM`\r\ncollect2: error: ld returned 1 exit status\r\n\r\n[mbed] ERROR: \"/usr/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed\"\r\n       Command: \"/usr/bin/python -u /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n\r\nBelow is the complete log of last command with -v flag.\r\n\r\n**Any other info / logs**\r\n`ubuntu@ubuntu:~/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed$ /usr/bin/python -u /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM -v`\r\n\r\n[Warning] @,: Compiler version mismatch: Have 7.3.1; expected version >= 9.0.0 and < 10.0.0\r\nBuilding project mbed (DISCO_F746NG, GCC_ARM)\r\nScan: mbed\r\nMacros: -DDEVICE_CRC=1 -DTARGET_STM32F746xG -DDEVICE_I2C_ASYNCH=1 -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_I2CSLAVE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_MPU=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTARGET_N25Q128A -DTOOLCHAIN_GCC -DDEVICE_CAN=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_QSPI=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_USBDEVICE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DHSE_VALUE=25000000 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DMBED_BUILD_TIMESTAMP=1613922146.79 -DDEVICE_RESET_REASON=1 -DDEVICE_SPI=1 -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_PORTIN=1 -DSTM32F746xx\r\nLink: mbed\r\nPreproc: arm-none-eabi-cpp -E -P ./mbed-os/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F746xG/device/TOOLCHAIN_GCC_ARM/STM32F746xG.ld -Wl,--gc-sections -Wl,--wrap,main -Wl,--wrap,_malloc_r -Wl,--wrap,_free_r -Wl,--wrap,_realloc_r -Wl,--wrap,_memalign_r -Wl,--wrap,_calloc_r -Wl,--wrap,exit -Wl,--wrap,atexit -Wl,-n -Wl,--wrap,printf -Wl,--wrap,sprintf -Wl,--wrap,snprintf -Wl,--wrap,vprintf -Wl,--wrap,vsprintf -Wl,--wrap,vsnprintf -Wl,--wrap,fprintf -Wl,--wrap,vfprintf -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -DMBED_MINIMAL_PRINTF -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -DMBED_BOOT_STACK_SIZE=4096 -DXIP_ENABLE=0 -o ./BUILD/DISCO_F746NG/GCC_ARM/.link_script.ld -DDEVICE_CRC=1 -DTARGET_STM32F746xG -DDEVICE_I2C_ASYNCH=1 -DDEVICE_EMAC=1 -D__MBED__=1 -DDEVICE_I2CSLAVE=1 -D__FPU_PRESENT=1 -DDEVICE_PORTOUT=1 -DDEVICE_PORTINOUT=1 -D__MBED_CMSIS_RTOS_CM -DTARGET_DISCO_F746NG -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F7 -DDEVICE_MPU=1 -DDEVICE_SERIAL_ASYNCH=1 -D__CMSIS_RTOS -DTARGET_N25Q128A -DTOOLCHAIN_GCC -DDEVICE_CAN=1 -DARM_MATH_CM7 -DTARGET_CORTEX_M -DTARGET_LIKE_CORTEX_M7 -DDEVICE_RTC=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_QSPI=1 -DTARGET_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DEXTRA_IDLE_STACK_REQUIRED -DDEVICE_LPTICKER=1 -DDEVICE_PWMOUT=1 -DDEVICE_SPI_ASYNCH=1 -DMBED_TICKLESS -DUSE_FULL_LL_DRIVER -DCOMPONENT_QSPIF=1 -DTARGET_CORTEX -DDEVICE_I2C=1 -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_USBDEVICE=1 -DDEVICE_STDIO_MESSAGES=1 -D__CORTEX_M7 -DTARGET_FAMILY_STM32 -DUSE_HAL_DRIVER -DTARGET_FF_ARDUINO -DHSE_VALUE=25000000 -DTARGET_RELEASE -DTARGET_STM -DTARGET_NAME=DISCO_F746NG -DDEVICE_SERIAL_FC=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_USTICKER=1 -DDEVICE_WATCHDOG=1 -DDEVICE_TRNG=1 -DTARGET_LIKE_MBED -DTARGET_RTOS_M4_M7 -DDEVICE_SLEEP=1 -DTOOLCHAIN_GCC_ARM -DMBED_BUILD_TIMESTAMP=1613922146.79 -DDEVICE_RESET_REASON=1 -DDEVICE_SPI=1 -DCOMPONENT_NSPE=1 -DDEVICE_INTERRUPTIN=1 -DDEVICE_SPISLAVE=1 -DDEVICE_ANALOGIN=1 -DDEVICE_SERIAL=1 -DDEVICE_FLASH=1 -DDEVICE_PORTIN=1 -DSTM32F746xx @./BUILD/DISCO_F746NG/GCC_ARM/.includes_d41d8cd98f00b204e9800998ecf8427e.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h\r\n[DEBUG] Return: 0\r\nLink: arm-none-eabi-gcc @./BUILD/DISCO_F746NG/GCC_ARM/.link_options.txt\r\n[DEBUG] Return: 1\r\n[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)':\r\n[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:154: multiple definition of `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)'\r\n[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:28: first defined here\r\n[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `LatestAudioTimestamp()':\r\n[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:181: multiple definition of `LatestAudioTimestamp()'\r\n[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:37: first defined here\r\n[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.o: In function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':\r\n[DEBUG] Errors: /home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc:26: multiple definition of `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)'\r\n[DEBUG] Errors: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/command_responder.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/command_responder.cc:23: first defined here\r\n[DEBUG] Errors: collect2: error: ld returned 1 exit status\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/make.py\", line 78, in wrapped_build_project\r\n    *args, **kwargs\r\n  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/build_api.py\", line 610, in build_project\r\n    res = toolchain.link_program(resources, build_path, name)\r\n  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 778, in link_program\r\n    self.link(elf, objects, libraries, lib_dirs, linker_script)\r\n  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/gcc.py\", line 357, in link\r\n    self.default_cmd(cmd)\r\n  File \"/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 830, in default_cmd\r\n    raise ToolException(stderr)\r\nToolException: BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)':\r\n/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:154: multiple definition of `GetAudioSamples(tflite::ErrorReporter*, int, int, int*, short**)'\r\nBUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:28: first defined here\r\nBUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.o: In function `LatestAudioTimestamp()':\r\n/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/audio_provider.cc:181: multiple definition of `LatestAudioTimestamp()'\r\nBUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/audio_provider.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/audio_provider.cc:37: first defined here\r\nBUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.o: In function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':\r\n/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/disco_f746ng/command_responder.cc:26: multiple definition of `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)'\r\nBUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/examples/micro_speech/command_responder.o:/home/ubuntu/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/micro_speech/mbed/./tensorflow/lite/micro/examples/micro_speech/command_responder.cc:23: first defined here\r\ncollect2: error: ld returned 1 exit status", "comments": ["Issue solved by modifying the line of tensorflow/tensorflow/lite/micro/examples/micro_speech/disco_f746ng/Makefile.inc\r\n\r\nfrom\r\n`ifeq ($(TARGET),$(filter $(TARGET),mbed))`\r\nto\r\n`ifeq ($(TARGET),$(filter $(TARGET),disco_f746ng))`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47299\">No</a>\n", "Hi, May I ask what is the code size after compilation? fx the size of .hex (binary file) and .elf (execution file)", "Compile result shows \r\nTotal static RAM memory (data + bss) : 36,408 bytes\r\nTotal Flash memory (text + data): 94,904 bytes\r\n\r\nThe bin file 'mbed.bin' size shows 135,308 bytes, under linux system; and 'mbed.elf' shows 4,756,752 bytes."]}, {"number": 47298, "title": "[TFLM] CEVA-BX1: Changed compiler to use O4 and fixed a typo in the Makefile", "body": "Fixed a couple of typos and changed optimization level to O4.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "copybara failed (which happens from time to time). reapplied the `kokoro:force-run` and `ready to pull` labels to trigger copybara again."]}]