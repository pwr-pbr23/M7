[{"number": 42006, "title": "error: 'std.constant' op requires attribute's type ('tensor<500x2450xf32>') to match op's return type ('tensor<*xf32>')", "body": "Hello,\r\nWe are currently using TFlite to quantize our trained models with FP16 and INT8. When running the experiments the unquantized baseline and FP16 quantization seem to work, but with INT8 the logfile created gives us the following error and output: \r\nerror: 'std.constant' op requires attribute's type ('tensor<500x2450xf32>') to match op's return type ('tensor<*xf32>')\r\n00000000C1C040BD00000000000000000000000000000000C1C040BDC1C0403D00000000C1C0403D000000000000000000000000C1C0403D00000000000000000000000000000000C1C040BDC1C0403D00000000C1C0403D000000000000000000000000C1C0403DC1C0403D00000000C1C0403D00000000C1C040BDC1C0403DC1C0403D00000000000000000000000000000000\u2026\r\nWe are running our experiments on the CPUs of a DGX1 and are using TF 2.3, is there anything we are missing?\r\nThank you!\r\n", "comments": ["@tabeareichmann \r\n\r\nPlease, fill [issue template.](https://github.com/tensorflow/models/issues/new/choose).\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@tabeareichmann ....Do you got the solution? Can you please tell me if resolved as I am facing the same error while doing the tflite conversion", "Facing the same issue when converting a QAT model with quantized Dense layers. Works with Conv2D layers but not Dense."]}, {"number": 42005, "title": "Tensorflow 2.0.0 error", "body": "---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\anaconda3\\envs\\iNeuron\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\anaconda3\\envs\\iNeuron\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\karti\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\karti\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\karti\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\karti\\anaconda3\\envs\\iNeuron\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\karti\\anaconda3\\envs\\iNeuron\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@KartikRayaprolu,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You you running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42005\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42005\">No</a>\n"]}, {"number": 42004, "title": "KeyError on concrete_functions while loading a model", "body": "**System information**\r\n- Have I written custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.0 / 7.6\r\n- GPU model and memory: Nvidia Quadro P1000, 4GB\r\n\r\n**Describe the current behavior**\r\nWhen loading a SavedModel object, function_deserialization.py#265 (`concrete_function_objects.append(concrete_functions[concrete_function_name])`) throws a `KeyError` exception caused from looking for a concrete function which is not in the SavedModel (was not serialised). In my case the missing functions are called\r\n1. `__inference_my_model_layer_call_fn_37929`\r\n2. `__inference_my_model_layer_call_fn_38165`\r\nAll other concrete functions are successfully located and recovered.\r\n\r\nThe call traces back to `saved_model/load.py` at line 604. Yet when I debug the values in `object_graph_proto` I can't see these missing keys, meaning they're added at a later phase.\r\nI also tried changing the original code line to\r\n```python\r\n        try:\r\n            concrete_function_objects.append(concrete_functions[concrete_function_name])\r\n        except KeyError:\r\n            print(\r\n                f'from saved_model/function_deserialization.py: couldnt find concrete function {concrete_function_name}'\r\n            )\r\n            continue\r\n        print(\r\n            f'from saved_model/function_deserialization.py: found concrete function {concrete_function_name}'\r\n        )\r\n```\r\nIt indeed shows that only these two functions are not found while all other ones are there. Also by doing that I can use the model for testing and inference, i.e. I see no limited functionality at the moment.\r\nNotice there are two similarly-named functions in `concrete_functions` called\r\n1. `__inference_my_model_layer_call_and_return_conditional_losses_35962`\r\n2. `__inference_my_model_layer_call_and_return_conditional_losses_37693`\r\n\r\nI guess there are two functions there as one originates from the training function and the other is related to the cross validation function.\r\n\r\nMy `MyModel` object extends Keras' Model class and contains a series of Models and Layers which are called in the `call` function. `MyModel` doesn't have any paramters or variables of its own. The last object in the series (an object extending `Layer`) also uses `self.add_loss` in its `call` function.\r\n\r\n**Describe the expected behavior**\r\nModel should load and work properly.\r\n\r\n**Standalone code to reproduce the issue**\r\nNot easy to do since the model structure is rather complicated.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nThe output without my workaround:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 148, in <module>\r\n    main()\r\n  File \"test.py\", line 107, in main\r\n    model = tf.keras.models.load_model(model_path)\r\n  File \"tensorflow\\python\\keras\\saving\\save.py\", line 190, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 116, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 604, in load_internal\r\n    export_dir)\r\n  File \"tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 188, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 123, in __init__\r\n    self._load_all()\r\n  File \"tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 212, in _load_all\r\n    super(KerasObjectLoader, self)._load_all()\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 134, in _load_all\r\n    self._load_nodes()\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 264, in _load_nodes\r\n    node, setter = self._recreate(proto, node_id)\r\n  File \"tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 233, in _recreate\r\n    obj, setter = super(KerasObjectLoader, self)._recreate(proto, node_id)\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 370, in _recreate\r\n    return factory[kind]()\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 359, in <lambda>\r\n    \"function\": lambda: self._recreate_function(proto.function),\r\n  File \"tensorflow\\python\\saved_model\\load.py\", line 398, in _recreate_function\r\n    proto, self._concrete_functions), setattr\r\n  File \"tensorflow\\python\\saved_model\\function_deserialization.py\", line 265, in recreate_function\r\n    concrete_function_objects.append(concrete_functions[concrete_function_name])\r\nKeyError: '__inference_my_model_layer_call_fn_37929'\r\n```\r\n", "comments": ["Can you share a very minimal/dummy but runnable code to reproduce this?", "No very easily. The model is rather complicated. Dismantling it to the level where I can provide a minimal example would take hours.", "Are these custom objects?", "Well.. yes. They are either implementations of keras.layers.Layer or, in the case of multiple layers with a shared semantic meaning, an implementation of keras's Model object, grouping layers together.\r\nFor example, imagine an encoder decoder framework for something like VAE. The encoder and decoder are `Model`s and the sampling layer is the custom layer in the middle. The entire thing is then wrapped in a MyModel `Model` class", "Have you correctly loaded custom objects? e.g.:\n\n```\nwith CustomObjectScope({'AttentionLayer': AttentionLayer}): \n    model = load_model('my_model.h5')\n```", "No\r\nWhere is this documented? \r\nWhat does it do?\r\nFor what types of objects do i need this?", "It Is documented in many places but I suggest  https://keras.io/guides/serialization_and_saving/", "The explanations and examples in this page only use the `with` scope if loading from config. Not when using the native `load` function.\r\nAlso, it refers the reader to [this](https://keras.io/guides/serialization_and_saving/save_and_serialize.ipynb#custom-objects) page which doesn't exist", "https://www.tensorflow.org/api_docs/python/tf/keras/utils/CustomObjectScope?hl=en#used-in-the-notebooks\n\n", "Ok, so this wasn't the solution, but it set me in the right direction, so many thanks \ud83d\udc4d \r\nAfter doing the following changes to the code, this now seems to work\r\n\r\n1. All `Model` implementations, except for the \"main model wrapper thingy\", were changes to `Layer` objects.\r\n2. All custom implementations (both the `Layer`s and the `Model`) were given a `get_config` function.\r\n\r\nI'm not sure which of these did the trick, but my money is on the first point.\r\n\r\nAnyway it's working now so many thanks.\r\n\r\nP.s.\r\nI still don't really like the way this works. It's not properly documented anyway. Some places just vaguely say that \"it's a good practice to implement `get_config`\", but I didn't see any proper explanations.\r\nFurthermore, there's no proper explanation on when a `Layer` should be used and when a `Model` would be better.\r\nFinally, even when trying to write the `get_config` function, no documentation tells you what it should include.", "/cc @fchollet about evaluate doc improvements margins", "@idofr To speed-up a little bit the process I suggest you to open a new documentation ticket mentioning this one at https://github.com/keras-team/keras-io", "@idofr Please update as per above comment", "What is there to update? you can see exactly that I opened a ticket in keras-io and that the ticket has the status 'open'", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42004\">No</a>\n", "@idofr Can you elaborate on your [comment ](https://github.com/tensorflow/tensorflow/issues/42004#issuecomment-669118318)as to what those exact changes were ? \r\nI'm facing a similar issue https://github.com/tensorflow/tensorflow/issues/50706 and it will be helpful. "]}, {"number": 42003, "title": "Please add Tensorflow 2.3 GPU and all subsequent versions to Conda package manager for Windows and Linux", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): No because I do not think I can\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nThis will change the current API. It will add the latest version of TensorFlow to Conda. \r\n**Who will benefit with this feature?**\r\nAnyone who wants to get started with TensorFlow GPU fast or wants to have multiple versions of TensorFlow GPU across multiple environments because when you install TensorFlow-GPU from Conda, it automatically installs CUDA and cuDNN for that specific virtual environment. \r\n**Any Other info.**\r\nAs of right now, only version 2.1 is there for windows and 2.2 for Linux as seen [https://anaconda.org/anaconda/tensorflow-gpu](here)", "comments": ["As suggested in others Conda tickets I think it is better to track this in conda github repositories.", "What do you mean by that? Sorry... I am new to this. ", "E.g. check https://github.com/tensorflow/tensorflow/issues/35754#issuecomment-635683444", "Oh ok...thanks\n\nOn Mon, Aug 3, 2020 at 11:19 AM bhack <notifications@github.com> wrote:\n\n> E.g. check #35754 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/35754#issuecomment-635683444>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/42003#issuecomment-668081090>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQHBYC5QT45MPPLOCV7XLL3R63IORANCNFSM4PTHRZGA>\n> .\n>\n", "@johncaling40 Could you close this?", "Sure"]}, {"number": 42002, "title": "tf.nn.conv2d_transpose name is overwritten", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): TF 2.2\r\n- TensorFlow version: v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.2 .76\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nconv2d_transpose output tensor name is Conv2DBackpropInput:0\r\n\r\n**Describe the expected behavior**\r\nconv2d_transpose output tensor name should be ct:0\r\n\r\n**Standalone code to reproduce the issue**\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Input, Model\r\n\r\n\r\ninputs = Input(shape=(32, 32, 3), name='net_input')\r\nw0 = tf.Variable(np.ones((4, 4, 3, 6)).astype(np.float32)/16/16, name='w0')\r\nc0 = tf.nn.conv2d(inputs, w0, 2, \"SAME\", name='conv0')\r\nr0 = tf.nn.relu(c0, name='relu0')\r\n\r\nwt = tf.Variable(np.ones((3, 3, 6, 6)).astype(np.float32)/16/16, name='wt')\r\nct = tf.nn.conv2d_transpose(r0, wt, (1, 16, 16, 6), strides=1, name='ct')\r\nrt = tf.nn.relu(ct, name='relut')\r\n\r\nwt2 = tf.Variable(np.ones((3, 3, 6, 6)).astype(np.float32)/16/16, name='wt2')\r\nct2 = tf.nn.conv2d_transpose(rt, wt2, (1, 16, 16, 6), strides=1, name='ct2')\r\nrt2 = tf.nn.relu(ct2, name='relut2')\r\n\r\nw1 = tf.Variable(np.ones((16, 16, 6, 4)).astype(np.float32)/16/16, name='w1')\r\nc1 = tf.nn.conv2d(rt2, w1, 1, \"VALID\", name='conv1')\r\nout = tf.nn.relu(c1, name='relu1')\r\n\r\nm = Model(inputs=inputs, outputs=out, name='test')\r\nprint(ct.name)", "comments": ["\r\n\r\nI have tried in colab with TF version 2.3 and was able to reproduce the issue.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/4ad8fa39d71a6e1e81857bcde4ac825b/-42002.ipynb).\r\nSeems like this issue is resolved in nightly version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/24c0a2049a897bda7e76f299de383147/-42002-nightly.ipynb). Thanks!", "i'm not sure how it's solved. if i understand correctly the output tensor name is \"tf.nn.conv2d_transpose_4/conv2d_transpose:0\", instead of \"Conv2DBackpropInput:0\".\r\nShouldn't it be \"ct:0\"?\r\nin the format: \"name:0\"?\r\nsame as: c0 = tf.nn.conv2d(inputs, w0, 2, \"SAME\", name='conv0') ==> c0.name: \"conv0:0\"", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/626795f656d916f2242877403d5c911b/42002.ipynb#scrollTo=jrXw5h5tlecY) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e26826031d04e87e297dde43b09ca243/42002-tf-nightly.ipynb). Please find the attached gist. Thanks!", "A few comments here:\r\n1. The name argument in that API is used when you are tracing a graph, and it still works as seen in the below snippet:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Input, Model\r\n\r\ninputs = Input(shape=(32, 32, 3), name='net_input')\r\n@tf.function\r\ndef foo(r0):\r\n  wt = tf.convert_to_tensor(np.ones((3, 3, 6, 6)).astype(np.float32)/16/16, name='wt')\r\n  ct = tf.nn.conv2d_transpose(r0, wt, (1, 16, 16, 6), strides=1, name='ct')\r\n  print(ct.name)\r\n\r\n  return ct\r\n\r\nct_out = foo(tf.ones((1, 16, 16, 6)))\r\n```\r\nThe name that gets printed is `ct:0`.\r\n\r\n2. Your code example should not be thought of as tracing a graph, it is constructing a Keras model. as such, the name of the intermediate value will be whatever name as been assigned to it as a result of the layers that created the intermediate value. That's why you see the generated layer name (`tf.nn.conv2d_transpose_4`) in the name string. We make zero guarantees about how the name is generated, including zero guarantees that the `name` argument in tensorflow APIs will affect it when those APIs are used directly as layers. We do not have any plans to change this at the moment.\r\n\r\n3. The Keras model you are constructing in the code snippet may actually be problematic and is definitely not tracking the tf.variables you've created outside of the model. Example warnings that get raised in the nightly (though this would happen before the nightlies too, just w/o any explicit warnings):\r\n```\r\nWARNING:tensorflow:\r\nThe following Variables were used a Lambda layer's call (tf.compat.v1.nn.conv2d_3), but\r\nare not present in its tracked objects:\r\n  <tf.Variable 'w1:0' shape=(16, 16, 6, 4) dtype=float32>\r\nIt is possible that this is intended behavior, but it is more likely\r\nan omission. This is a strong indication that this layer should be\r\nformulated as a subclassed Layer rather than a Lambda layer.\r\n```\r\n\r\n4. Due to the lack of guarantees I mentioned above, code should never be treating tensor (or intermediate keras value) names as load-bearing. If you're trying to do something like put your tensors as keys in a data structure or checking them for instance equality rather than value equality, use `tensor.ref()` instead of checking by names.\r\nIf you're trying to look at the names for debugging purposes while building a Keras model, check the layer name that gets placed in the name or check the repr of the intermediate value (which will tell you what layer produced it in the nightlies):\r\n`<KerasTensor: shape=(None, 16, 16, 6) dtype=float32 (created by layer 'tf.nn.conv2d_transpose')>`\r\n\r\nClosing the issue because it's non-actionable on our side (as we make no guarantees about how naming will work when building a Keras model), but do feel free to follow up with any other questions you may have!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42002\">No</a>\n"]}, {"number": 42001, "title": "tf.string.format is not returning Unicode characters", "body": "As also described in [this Stackoverflow post](https://stackoverflow.com/questions/59795552/tf-string-format-is-not-returning-unicode-characters), when a string passes through `tf.strings.format` unicode characters are not represented correctly.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nprint(tf.constant('\ud83d\ude0a:\ud83d\ude0a').numpy().decode('utf-8')) # output: \ud83d\ude0a:\ud83d\ude0a\r\nprint(tf.strings.format(\"\ud83d\ude0a:{}\", tf.constant('\ud83d\ude0a')).numpy().decode('utf-8')) # output: \ud83d\ude0a:\"\\\\360\\\\237\\\\230\\\\212\" \r\n```\r\n\r\nError observed in tensorflow 2.2 and 2.3", "comments": ["The problem is that the formatted tensor string is escaped:\r\n`b'\\xf0\\x9f\\x98\\x8a:\"\\\\360\\\\237\\\\230\\\\212\"'`", "Also the formatted tensor is in Octal Escape Sequence ", "Added a PR #42067 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42001\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42001\">No</a>\n"]}, {"number": 42000, "title": "Report error 'non-first' call when repeate experiment", "body": "Question: The code can successfully run when using autograph for acceleration, whereas it reports 'optimizer non-first call'  error when I repeat the experiment multiple times(10 in the sample code), which confused me.\r\n The code logic is as follows\uff1a\r\n--------------------------------------------\r\n```\r\n for i in np.arange(10):\r\n       run_flow(random_seed=i)\r\n\r\ndef run_flow(random_seed=0):\r\n      optimizer = tf.keras.optimizers.Adam()\r\n      for i in num_epoch:\r\n           train_epoch(optimizer)\r\n\r\ndef train_epoch(optimzier):\r\n      for i in num_batchs:\r\n            train_step(optimizer)\r\n\r\n@tf.function\r\ndef train_step(optimizer)\r\n      # .....\r\n      optimizer.apply_gradients(...)\r\n```\r\n----------------------------------------------\r\nClearly, the definition-optimizer  is outside the function-train_step.", "comments": ["It seems to me in the https://github.com/tensorflow/tensorflow/issues/27120 \"famliy\"", "> It seems to me in the #27120 \"famliy\"\r\n\r\nThank you. Still different. my code works well both in eager mode and autograph mode when not repeat experiments, I will paste the error details later.\r\nps\uff1a I am wandering is there have any related API in tensorflow-2 to init the runtime environment before conducting the next experiment. Currently, I have to  restart the next experiments manually after current experiment have finished, Which is boring.", "@v3551G \r\n\r\n Please, fill [issue template.](https://github.com/tensorflow/models/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @v3551G\r\n> \r\n> Please, fill [issue template.](https://github.com/tensorflow/models/issues/new/choose).\r\n> \r\n> Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\"\"\"\r\nNote: when add \"@tf.function\" in front from train_step, report error; else, successfully run two times.\r\nEnvironment; tensorflow2.x python3.x\r\nauthor: masterqkk\r\n\"\"\"\r\n\r\n\r\ndef build_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(10, input_shape=[2]))\r\n    model.add(tf.keras.layers.Dense(1))\r\n    return model\r\n\r\ndef compute_loss(batch_true, batch_pred):\r\n    losses = tf.losses.mean_squared_error(batch_true, batch_pred)\r\n    loss = tf.reduce_mean(losses)\r\n    return loss\r\n\r\n\r\n@tf.function\r\ndef train_step(model, batch_input, batch_label, optimizer):\r\n    with tf.GradientTape() as tape:\r\n        preds = model(batch_input)\r\n        loss = compute_loss(batch_label, preds)\r\n    trainable_variables = model.trainable_variables\r\n    grads = tape.gradient(loss, trainable_variables)\r\n    optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_variables))\r\n    return loss\r\n\r\n\r\ndef train_epoch(model, data_batchs, optimizer):\r\n    for (batch_input, batch_label) in data_batchs:\r\n        loss = train_step(model, batch_input, batch_label, optimizer)\r\n\r\n\r\ndef train_model(model, data_batchs, optimizer):\r\n    for i in np.arange(1):\r\n        train_epoch(model, data_batchs, optimizer)\r\n\r\n\r\ndef load_data():\r\n    np.random.seed(0)\r\n    x = np.array(np.random.random(size=(10, 2)), np.float32)\r\n    y = np.array(np.random.random(size=(10, 1)), np.float32)\r\n    data = tf.data.Dataset.from_tensor_slices((x, y))\r\n    data_batchs = data.batch(batch_size=5)\r\n    return data_batchs\r\n\r\n\r\ndef run_flow():\r\n    data_batchs = load_data()\r\n    model = build_model()\r\n    optimizer = tf.keras.optimizers.Adam()\r\n    train_model(model, data_batchs, optimizer)\r\n    tf.print('model train finished.')\r\n\r\n\r\nfor i in np.arange(2):\r\n    run_flow()\r\n```\r\n\r\nWaitting your reply.\r\n\r\n", "@v3551G I still think that you are in the perimeter of https://github.com/tensorflow/tensorflow/issues/27120. \r\nCan you try your code with the same workaround as in https://github.com/tensorflow/tensorflow/issues/27120#issuecomment-540071844\r\n", "> @v3551G I still think that you are in the perimeter of #27120.\r\n> Can you try your code with the same workaround as in [#27120 (comment)](https://github.com/tensorflow/tensorflow/issues/27120#issuecomment-540071844)\r\n\r\nThe demo for reproducing the error is pasted at my last comment\uff0c please swipe up the page. \r\n\r\n\r\n", "I saw your code. Can you try your code with that workaround? \nPersonally I tried that workaround with your code and It Is working.", "> I saw your code. Can you try your code with that workaround?\r\n> Personally I tried that workaround with your code and It Is working.\r\ndo you mean commit the \"@tf.function\"?\r\nmy code work well in eager mode whether repeating experiment or not, if repeat experiment, only work well in eager mode, while fail in autograph mode.", "```\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\"\"\"\r\nNote: when add \"@tf.function\" in front from train_step, report error; else, successfully run two times.\r\nEnvironment; tensorflow2.x python3.x\r\nauthor: masterqkk\r\n\"\"\"\r\n\r\n\r\ndef build_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(10, input_shape=[2]))\r\n    model.add(tf.keras.layers.Dense(1))\r\n    return model\r\n\r\ndef compute_loss(batch_true, batch_pred):\r\n    losses = tf.losses.mean_squared_error(batch_true, batch_pred)\r\n    loss = tf.reduce_mean(losses)\r\n    return loss\r\n\r\ndef get_apply_grad_fn():\r\n    @tf.function\r\n    def train_step(model, batch_input, batch_label, optimizer):\r\n        with tf.GradientTape() as tape:\r\n            preds = model(batch_input)\r\n            loss = compute_loss(batch_label, preds)\r\n        trainable_variables = model.trainable_variables\r\n        grads = tape.gradient(loss, trainable_variables)\r\n        optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_variables))\r\n        return loss\r\n    return train_step\r\n\r\n\r\ndef train_epoch(model, data_batchs, optimizer):\r\n    for (batch_input, batch_label) in data_batchs:\r\n        model_apply_grads = get_apply_grad_fn()\r\n        loss = model_apply_grads(model, batch_input, batch_label, optimizer)\r\n\r\n\r\ndef train_model(model, data_batchs, optimizer):\r\n    for i in np.arange(1):\r\n        train_epoch(model, data_batchs, optimizer)\r\n\r\n\r\ndef load_data():\r\n    np.random.seed(0)\r\n    x = np.array(np.random.random(size=(10, 2)), np.float32)\r\n    y = np.array(np.random.random(size=(10, 1)), np.float32)\r\n    data = tf.data.Dataset.from_tensor_slices((x, y))\r\n    data_batchs = data.batch(batch_size=5)\r\n    return data_batchs\r\n\r\n\r\ndef run_flow():\r\n    data_batchs = load_data()\r\n    model = build_model()\r\n    optimizer = tf.keras.optimizers.Adam()\r\n    model_apply_grads = get_apply_grad_fn()\r\n    train_model(model, data_batchs, optimizer)\r\n    tf.print('model train finished.')\r\n\r\n\r\nfor i in np.arange(2):\r\n    run_flow()\r\n```", "Hi @v3551G, as @bhack explained the comment in #27120 to wrap the train_step is a workaround that will allow you to run multiple experiments. However, you might notice that it can have an impact on performance. The problem of not being able to repeat the experiment is actually a known bug as I've noted in this comment [#36574 ](https://github.com/tensorflow/tensorflow/issues/36574#issuecomment-659040854). ", "Closing this issue now since this is a known issue being tracked in #36574 and there is a temporary workaround.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42000\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42000\">No</a>\n", "As you pointed out, you are creating your optimizer outside the `tf.function`; however, you are attempting to pass a new optimizer into the same trace of `tf.function` and the optimizer creates variables the first time it applies gradients. This is why you see the error: the optimizer that is instantiated in `run_flow` attempts to create variables in `train_step` when `train_step is retraced in your secod experiment.\r\n\r\nJust my two cents:\r\n\r\nSuppose you are running a batch of 10 experiments where each experiment is called with `run_flow`:\r\n```\r\nfor i in np.arange(10):\r\n       run_flow(random_seed=i)\r\n```\r\n\r\nIf you are interested in keeping experiments #1 - #10 identical:\r\n\r\n```\r\ndef build_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.Dense(10, input_shape=[2]))\r\n    model.add(tf.keras.layers.Dense(1))\r\n    return model\r\n\r\ndef compute_loss(batch_true, batch_pred):\r\n    losses = tf.losses.mean_squared_error(batch_true, batch_pred)\r\n    loss = tf.reduce_mean(losses)\r\n    return loss\r\n\r\ndef train_step(model, batch_input, batch_label, optimizer):\r\n    with tf.GradientTape() as tape:\r\n        preds = model(batch_input)\r\n        loss = compute_loss(batch_label, preds)\r\n    trainable_variables = model.trainable_variables\r\n    grads = tape.gradient(loss, trainable_variables)\r\n    optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_variables))\r\n    return loss\r\n\r\ndef train_model(model, data_batchs, optimizer):\r\n    # Create a new tf.function from train_step that will be used in this experiment only\r\n    train_step_function = tf.function(train_step)\r\n    num_epochs = 10\r\n    for i in np.arange(num_epochs):\r\n        for (batch_input, batch_label) in data_batchs:\r\n            loss = train_step_function(model, batch_input, batch_label, optimizer)\r\n\r\ndef load_data():\r\n    np.random.seed(0)\r\n    x = np.array(np.random.random(size=(10, 2)), np.float32)\r\n    y = np.array(np.random.random(size=(10, 1)), np.float32)\r\n    data = tf.data.Dataset.from_tensor_slices((x, y))\r\n    data_batchs = data.batch(batch_size=5)\r\n    return data_batchs\r\n\r\ndef run_flow():\r\n    data_batchs = load_data()\r\n    model = build_model()\r\n    optimizer = tf.keras.optimizers.Adam()\r\n    train_model(model, data_batchs, optimizer)\r\n\r\nfor i in np.arange(10):\r\n    print(\"Running experiment {}\".format(i))\r\n    run_flow()\r\n```\r\nThis solution should not have a drastic impact on performance. \r\n"]}, {"number": 41999, "title": "tf.GradientTape() doesn't work on sliced outputs", "body": "Here is a piece of code which I tried to run:\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant([[1, 2], [2, 3]], dtype=tf.float32)\r\nb = tf.constant([[1, 2], [2, 3]], dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as tape1, tf.GradientTape() as tape2:\r\n    tape1.watch(a)\r\n    tape2.watch(a)\r\n    \r\n    c = a * b\r\n\r\ngrad1 = tape1.gradient(c, a)\r\ngrad2 = tape2.gradient(c[:, 0], a)\r\nprint(grad1)\r\nprint(grad2)\r\n```\r\nAnd this is the output:\r\n```\r\ntf.Tensor(\r\n[[1. 2.]\r\n [2. 3.]], shape=(2, 2), dtype=float32)\r\nNone\r\n```\r\nAs you can see that tf.GradientTape() is not working with sliced outputs. Is there any way around to this?", "comments": ["Is it ok with the [Stackoverflow reply](https://stackoverflow.com/questions/63225910/tf-gradienttape-doesnt-work-on-sliced-outputs)?\r\nI think that you need to manipulate the tensor slicing in the `GradientTape()` context.", "Yes, I got it. I posted the same question there as well \ud83d\ude05."]}, {"number": 41998, "title": "tf.nn.conv2d_transpose name is overridden", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nIf you use this conv2d_transpose with a name, it chooses another name, with BackProp as substring\r\n**Describe the expected behavior**\r\nI want the supplied name to be the name of the convolution.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@zvika-adler\r\nPlease provide with simple indented stand alone code for us to replicate the issue faced (with all dependencies) or if posible share a colab gist with the error faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41998\">No</a>\n"]}, {"number": 41997, "title": "Need to build tensorflow version 2.1.0 from source on Windows", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n**Describe the problem**\r\nI want to build tensorflow 2.1.0 from source. Github Branch r2.1 builts tensorflow version 2.1.1 by default. May I know how to explicitly give the tensorflow version for compilation ?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@manidevi \r\n\r\nPlease follow [this guide](https://www.tensorflow.org/install/source_windows) to build tensorflow and when you checkout the branch enter the following command\r\n\r\n`git checkout r2.1.0`\r\n\r\nThanks!", "I get the following error for the above command\r\n```\r\n$ git checkout r2.1.0 \r\nerror: pathspec 'r2.1.0' did not match any file(s) known to git\r\n```\r\n ", "@manidevi you need to run `git checkout v2.1.0`", "@manidevi \r\n\r\nAs suggested by @bhack you can try with `git checkout v2.1.0`.Please, check this [gist](https://colab.research.google.com/gist/ravikyram/4206629a3953fa6ede6daf219ca89b53/untitled80.ipynb) for your reference.Thanks!\r\n`\r\n", "Got it. Thanks @ravikyram  and @bhack \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41997\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41997\">No</a>\n"]}, {"number": 41996, "title": "Tensorflow Build Failure with Bazel", "body": "**Apologies in advance for the text dump**\r\n\r\nI am attempting to compile Tensorflow in OSX 10.13.4 using Bazel based on these instructions:\r\nhttps://medium.com/xplore-ai/nvidia-egpu-macos-tensorflow-gpu-the-definitive-setup-guide-to-avoid-headaches-f40e831f26ea\r\n\r\nI attempted the build using the following command: \r\n`bazel build --config=cuda --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nHere are the versions of all relevant programs:\r\nPython: 3.6\r\nCUDA: 10.0\r\ncuDNN: 7.4\r\nTensorflow: https://github.com/zylo117/tensorflow-gpu-macosx\r\nGPU: NVIDIA GTX 980\r\nBazel Version: 0.16.1\r\n\r\nHere is the verbose error information along with some run info:\r\n\r\n5 warnings generated.\r\n\r\n```\r\nERROR: /Users/brianmoser/tensorflow-gpu-macosx/tensorflow/core/kernels/BUILD:3423:1: error while parsing .d file: /private/var/tmp/_bazel_brianmoser/9608e82147ff56f68b42fd19bee93cb0/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/core/kernels/_objs/bincount_op_gpu/bincount_op_gpu.cu.d (No such file or directory)\r\n\r\nnvcc fatal : The version ('10.0') of the host compiler ('Apple clang') is not supported\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\nINFO: Elapsed time: 1407.017s, Critical Path: 63.10s\r\n\r\nINFO: 2389 processes: 2389 local.\r\n\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAdditionally, here is the final step the build conducted:\r\n\r\n`tensorflow/stream_executor/cuda/cuda_dnn.cc:1506:19: warning: private field 'data_type_' is not used [-Wunused-private-field]\r\n  cudnnDataType_t data_type_;\r\n`\r\n\r\nFinally, here is the final relevant information:\r\n\r\n```\r\ngit rev-parse HEAD\r\n\r\n795d7c36152a1f96adca0c48f4537e500f5ad36b\r\n```\r\n```\r\n\r\nbazel version\r\n\r\nBuild label: 0.16.1\r\n\r\n```\r\nBuild target: \r\n`bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar`\r\n\r\nDoes anybody have any idea what could be the reason for this error?", "comments": ["@Bmoser-boop,\r\nPlease refer to the [official guide](https://www.tensorflow.org/install/source#macos) while building the TensorFlow package. \r\n\r\nAlso, check the [tested build configurations](https://www.tensorflow.org/install/source#cpu_2) and make sure you have the correct dependencies installed. Thanks! ", "@Bmoser-boop,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem? Also, specify the TensorFlow version you are trying to build. Thanks!", "Sure, here are is the full list of commands:\r\n\r\n```\r\nbrew install coreutils llvm\r\n\r\nbrew install cliutils/apple/libomp\r\n\r\ngit clone https://github.com/zylo117/tensorflow-gpu-macosx\r\n\r\ncd tensorflow-gpu-macosx\r\n\r\n./configure\r\n\r\nbazel build --config=cuda --config=opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nAlso, I don't think I can give an exact version of Tensorflow because this is a custom-made version of Tensorflow 18 that is designed to allow GPU access for users of Mac OSX 10.13\r\nhttps://github.com/zylo117/tensorflow-gpu-macosx\r\n\r\nMore details about the steps I have executed can be found here: https://medium.com/xplore-ai/nvidia-egpu-macos-tensorflow-gpu-the-definitive-setup-guide-to-avoid-headaches-f40e831f26ea", "Additionally, I tried downgrading to Xcode 9.4 and was able to get further, now to 4583 processes completed", "@Bmoser-boop What version of tensorflow are you using?", "I was trying to use 1.4.0", "@Bmoser-boop Can you please try using latest version of tensorflow like tensorvlow 1.14.0 or 1.15.0 instead of old version of tensorfow. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41996\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41996\">No</a>\n"]}, {"number": 41995, "title": "Don't check for attribute `is_tensor_like` in `is_tensor`", "body": "`is_tensor` implementation says that an object is Tensor if it matches at least one of three independent checks.\r\n\r\nOne of them is an object having attribute `is_tensor_like`.\r\n\r\nBut nobody seems to use it throughout the code anymore.\r\n\r\nFrom the history we can see that this check [was added specifically to support the same name property of the `DistributedValues`](https://github.com/tensorflow/tensorflow/commit/89e06304aad35bfb019a8c10f39fc1ead83e0f99) class, but again from the history we see that the property [was later removed from `DistributedValues`](https://github.com/tensorflow/tensorflow/commit/6b68396a8279e00676c75d685ada2f74398d3c08).\r\n\r\nLet's remove checking for `is_tensor_like` attribute from `is_tensor`.", "comments": ["Unfortunately, it seems that there a couple of users relying on this attribute. One is in Keras (isolated to unit tests), and the other is [tensorflow_probability](https://github.com/tensorflow/probability/blob/b3ea633bdf9b2dbbbe4fcb82b0dd805cb9f284f8/tensorflow_probability/python/util/deferred_tensor.py#L259).\r\n\r\nWe'll need to make sure those two uses are cleaned up before merging the PR.\r\n\r\n@jvdillon @tomerk ", "@jvdillon @tomerk  Can you please check @mdanatg's comments and keep us posted ? Thanks!", "To clarify there are a number of downstream users of Keras who currently rely on this behavior in their unit tests. I think if tensorflow_probability is updated we can try to update the various client code though. I'm not sure currently what the timelines for any of that would look like currently.", "@lithuak  Can you please check @tomerk's comments and keep us posted ? Thanks!", "@gbaned @mdanatg I'm not really sure what my response should be here... It's clear that one day this PR would be great to merge and it's also clear that now is not the time for that and that we can't give any estimates about that time. So what do you guys do in a situation like this? Should I close it or just leave it here and let it be open for an unpredictable amount of time?", "Agree, it's unfortunate that we cant push it right away. I think it's ok to close since we'll likely revisit this logic in the future.", "@mdanatg Thanks, Dan! Closing it for now."]}, {"number": 41994, "title": "Line104 should be \"ZerosLike\" in gradients.cc", "body": "This is from issue #41912 .\r\n\r\nLine 104 should be \r\n```\r\n absl::StrCat(\"ZerosLike\", ToId(handle_)).c_str()); \r\n```", "comments": ["@Dynmi can you please update your branch , we see some conflicts.", "> @Dynmi can you please update your branch , we see some conflicts.\r\n\r\nNo problem. But how should I update it? Could you guide me?", "you can try this \r\n\r\n````\r\ngit clone ${your_fork_url}\r\n$ cd tensorflow\r\n$ git remote add upstream ${main_tensorflow_repo_url}\r\n$ # do work, git add, git commit, everything on a branch for your repo\r\n$ git push # -u optionally\r\n$ # now syncing from upstream\r\n$ git checkout master\r\n$ git pull --rebase upstream\r\n$ git push  # now master on your fork is synced with master upstream\r\n$ git checkout -  # switch back to the branch\r\n$ git rebase master  # rebase on master without any merge commits\r\n$ git push  # now the changes on your repo and rebased on master upstream\r\n````", "> you can try this\r\n> \r\n> ```\r\n> git clone ${your_fork_url}\r\n> $ cd tensorflow\r\n> $ git remote add upstream ${main_tensorflow_repo_url}\r\n> $ # do work, git add, git commit, everything on a branch for your repo\r\n> $ git push # -u optionally\r\n> $ # now syncing from upstream\r\n> $ git checkout master\r\n> $ git pull --rebase upstream\r\n> $ git push  # now master on your fork is synced with master upstream\r\n> $ git checkout -  # switch back to the branch\r\n> $ git rebase master  # rebase on master without any merge commits\r\n> $ git push  # now the changes on your repo and rebased on master upstream\r\n> ```\r\n\r\nOk, I will do it later!", "@Dynmi Just want to check if are you still interested in fixing this?", "> @Dynmi Just want to check if are you still interested in fixing this?\r\n\r\nWell. It seems this problem has been fixed in another PR, which was just a litte earlier than my PR."]}, {"number": 41993, "title": "E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- TensorFlow installed from (source or binary): Latest\r\n- TensorFlow version (use command below): Latest\r\n- Python version: 3.8.1\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: RTX 2070\r\n\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n\r\nE tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1831): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2020-08-02 22:58:40.548649: W tensorflow/core/framework/op_kernel.cc:1772] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 19, 32, 1, 24, 32, 32] \r\n\r\n\r\n", "comments": ["@summa-code \r\nPlease provide with simple stand alone code or a colab gist with the error for us to analyse the issue.", "As LSTM is used with CUDA, it is probably this: https://github.com/tensorflow/tensorflow/issues/41630 , you can try the workaround with the environment variable TF_CUDNN_RESET_RND_GEN_STATE=1", "NOPE, it did not work, here is console output\r\n\r\nCUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1936): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n\r\nOn the notebook:\r\n\r\nInternalError:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 19, 32, 1, 24, 32, 32] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[PartitionedCall]] [Op:__inference_train_function_149648]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n", "@summa-code \r\nThis seems to be a duplicate of #41987, please confirm.", "Ah !!! Looks like it. My bad.. has been doing few things and did not track what i filed before. Yes same.", "Moving to closed status as its a duplicate of  #41987", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41993\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41993\">No</a>\n", "> TF_CUDNN_RESET_RND_GEN_STATE=1\r\n\r\nthis is help me"]}, {"number": 41992, "title": "CNN convolution neural network problem", "body": "Why do we define the weights of CNN convolutions neural networks with normal distribution\uff0cis it because of the central limit theorem\uff1f", "comments": ["@lvjinqiao \r\n\r\nCan you please go through the [link](https://www.deeplearning.ai/ai-notes/initialization/) , [link2](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79) and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41991, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime. ", "body": "I'm receiving this error message when importing keras: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow'\r\n\r\nWhen I downgrade to keras 2.3.1, I receive the following error \"ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\"  \r\n\r\nStackoverflow link: https://stackoverflow.com/questions/63209972/error-while-loading-keras-2-4-3-for-tensorflow-2-2?noredirect=1#comment111793997_63209972\r\n\r\n###update as of 8/2/2020###\r\nI've narrowed the problem down to the c:\\windows\\system32\\msvcp140_1.dll file.  The file is not read-only and everyone has full permissions on the file.  I've tried condas install and reinstalling C++ redistrib packages and still receive the same error message that the DLL can't initialize. \r\n\r\nimport struct\r\nprint(\"Python version: \" + str(8 * struct.calcsize(\"P\")))  # outputs 64\r\nprint(\"loading keras.layers...\")\r\nfrom keras.layers import Input, LSTM, Dense\r\nprint(\"loading keras.models...\")\r\nfrom keras.models import Model\r\nprint(\"loading nltk.corpus...\")\r\nfrom nltk.corpus import stopwords\r\n------------------------\r\n\r\n### System information\r\n\r\nWindows Server 2012 R12 x64 (VM)\r\npython 3.8.3 x64\r\nKeras 3.4.1 or 3.2.1\r\ntensorflow 2.2\r\nC++ redistribution\r\nNo GPU\r\n\r\n##### Error Message When Laucnhing the above Test Script ####\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dyna\r\nmic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module\r\n>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in\r\n<module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35\r\n, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, i\r\nn <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", lin\r\ne 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dyna\r\nmic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@shoother813,\r\nCould you please share the output of the `pip list` command with us?\r\n\r\nAlso, check this these similar issues [#22512](https://github.com/tensorflow/tensorflow/issues/22512), [#36167](https://github.com/tensorflow/tensorflow/issues/36167) and let us know if it works. Thanks!", "Package                Version\r\n---------------------- ---------\r\nabsl-py                0.9.0\r\nappdirs                1.4.4\r\nasgiref                3.2.10\r\nastor                  0.8.1\r\nastroid                2.4.2\r\nastunparse             1.6.3\r\nattrs                  19.3.0\r\nbcrypt                 3.1.7\r\nbeautifulsoup4         4.8.2\r\nbs4                    0.0.1\r\ncached-property        1.5.1\r\ncachetools             4.1.1\r\ncertifi                2020.6.20\r\ncffi                   1.14.1\r\nchardet                3.0.4\r\nclick                  7.1.2\r\ncolorama               0.4.3\r\ncryptography           3.0\r\ncx-Oracle              7.3.0\r\ncycler                 0.10.0\r\ndefusedxml             0.6.0\r\nDjango                 3.0.8\r\net-xmlfile             1.0.1\r\ngast                   0.3.3\r\ngoogle-auth            1.20.0\r\ngoogle-auth-oauthlib   0.4.1\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.30.0\r\nh5py                   2.10.0\r\nidna                   2.10\r\nimportlib-metadata     1.7.0\r\nisodate                0.6.0\r\njdcal                  1.4.1\r\njoblib                 0.16.0\r\nKeras                  2.4.3\r\nKeras-Applications     1.0.8\r\nKeras-Preprocessing    1.1.2\r\nlazy-object-proxy      1.4.3\r\nlxml                   4.5.2\r\nMarkdown               3.2.2\r\nnltk                   3.5\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopenpyxl               3.0.4\r\nopt-einsum             3.3.0\r\npandas                 1.1.0\r\nparamiko               2.7.1\r\npip                    20.2\r\nprotobuf               3.12.4\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\npycparser              2.20\r\nPyNaCl                 1.4.0\r\npyodbc                 4.0.30\r\npython-dateutil        2.8.1\r\npytz                   2020.1\r\nPyYAML                 5.3.1\r\nregex                  2019.11.1\r\nrequests               2.24.0\r\nrequests-oauthlib      1.3.0\r\nrequests-toolbelt      0.9.1\r\nrsa                    4.6\r\nscikit-learn           0.23.1\r\nscipy                  1.4.1\r\nsetuptools             47.1.0\r\nsix                    1.15.0\r\nsklearn                0.0\r\nsoupsieve              2.0.1\r\nsql-server.pyodbc      1.0\r\nsqlparse               0.3.1\r\ntensorboard            2.3.0\r\ntensorboard-plugin-wit 1.7.0\r\ntensorflow             2.3.0\r\ntensorflow-estimator   2.3.0\r\ntermcolor              1.1.0\r\nthreadpoolctl          2.1.0\r\ntqdm                   4.48.0\r\nurllib3                1.25.10\r\nWerkzeug               1.0.1\r\nwheel                  0.34.2\r\nwrapt                  1.12.1\r\nXlsxWriter             1.3.0\r\nzeep                   3.4.0\r\nzipp                   3.1.0\r\n", "I've also tried downgrading to python 3.7 and installed tensorflow 2.0, but still received the exact same problem.  I'm currently back to python 3.8 and tensorflow 2.3", "@shoother813,\r\nPlease create a virtual environment and then try installing TensorFlow in it, and check if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41991\">No</a>\n"]}, {"number": 41990, "title": "CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid?", "body": "\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n-   **Python version**: 3.8.2\r\n-   **CUDA/cuDNN version**: Cuda 10.1/ cuDNN 7.6.5\r\n-   **GPU model and memory**: Nvidia GTX 750Ti\r\n-   **Exact command to reproduce**: \r\n\r\n```\r\n# TensorFlow and tf.keras\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# At this step I was getting the error which I've posted below in the terminal.\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(128, activation='relu'),\r\n    keras.layers.Dense(10)\r\n])\r\n```\r\n\r\n\r\n### Describe the problem\r\nI've recently installed ubuntu 20.04 LTS and it comes with python-3.8, so I'll installed **nvidia-cuda-toolkit** and **nvidia drivers** and I can confirm they are working fine.\r\n\r\n```\r\n$ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n```\r\n```\r\n$ nvidia-smi\r\nMon Aug  3 02:56:11 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 750 Ti  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 27%   38C    P0     1W /  38W |    245MiB /  1997MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       979      G   /usr/lib/xorg/Xorg                            20MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```\r\n\r\nNow, I tried to build a small sequential model I am getting an error which says `InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid`\r\n\r\nI don't know what causing the issue. My linux ubuntu is a new installation. I have installed everything correctly.\r\n\r\n### Source code / logs\r\n```\r\n2020-08-03 02:48:40.720575: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-03 02:48:40.750630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-03 02:48:40.750735: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-03 02:48:40.791690: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-03 02:48:40.815993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-03 02:48:40.821924: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-03 02:48:40.863910: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-03 02:48:40.870559: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-03 02:48:40.945916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-03 02:48:40.947130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-03 02:48:40.979471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3311130000 Hz\r\n2020-08-03 02:48:40.980123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4aa6700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-03 02:48:40.980190: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-03 02:48:41.121266: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49375f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-03 02:48:41.121357: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 750 Ti, Compute Capability 5.0\r\n2020-08-03 02:48:41.122574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-03 02:48:41.122676: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-03 02:48:41.122762: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-03 02:48:41.122830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-03 02:48:41.122898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-03 02:48:41.122963: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-03 02:48:41.123029: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-03 02:48:41.123145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-03 02:48:41.124618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-03 02:48:41.124716: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-4-ac4dc71cdd20> in <module>\r\n----> 1 model = keras.Sequential([\r\n      2     keras.layers.Flatten(input_shape=(28, 28)),\r\n      3     keras.layers.Dense(128, activation='relu'),\r\n      4     keras.layers.Dense(10)\r\n      5 ])\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)\r\n    114     \"\"\"\r\n    115     # Skip the init in FunctionalModel since model doesn't have input/output yet\r\n--> 116     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n    117         name=name, autocast=False)\r\n    118     self.supports_masking = True\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n    306     self._steps_per_execution = None\r\n    307 \r\n--> 308     self._init_batch_counters()\r\n    309     self._base_model_initialized = True\r\n    310     _keras_api_gauge.get_cell('model').set(True)\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)\r\n    315     # `evaluate`, and `predict`.\r\n    316     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA\r\n--> 317     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    318     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n    319     self._predict_counter = variables.Variable(\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    260       return cls._variable_v1_call(*args, **kwargs)\r\n    261     elif cls is Variable:\r\n--> 262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n    242     if aggregation is None:\r\n    243       aggregation = VariableAggregation.NONE\r\n--> 244     return previous_getter(\r\n    245         initial_value=initial_value,\r\n    246         trainable=trainable,\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)\r\n    235                         shape=None):\r\n    236     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    239       previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n   2631   shape = kwargs.get(\"shape\", None)\r\n   2632 \r\n-> 2633   return resource_variable_ops.ResourceVariable(\r\n   2634       initial_value=initial_value,\r\n   2635       trainable=trainable,\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    265 \r\n    266 \r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1505       self._init_from_proto(variable_def, import_scope=import_scope)\r\n   1506     else:\r\n-> 1507       self._init_from_args(\r\n   1508           initial_value=initial_value,\r\n   1509           trainable=trainable,\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1648         with ops.get_default_graph()._attr_scope({\"_class\": attr}):\r\n   1649           with ops.name_scope(\"Initializer\"), device_context_manager(None):\r\n-> 1650             initial_value = ops.convert_to_tensor(\r\n   1651                 initial_value() if init_from_fn else initial_value,\r\n   1652                 name=\"initial_value\", dtype=dtype)\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1497 \r\n   1498     if ret is None:\r\n-> 1499       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1500 \r\n   1501     if ret is NotImplemented:\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n     50 def _default_conversion_function(value, dtype, name, as_ref):\r\n     51   del as_ref  # Unused.\r\n---> 52   return constant_op.constant(value, dtype, name=name)\r\n     53 \r\n     54 \r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    261     ValueError: if called on a symbolic tensor.\r\n    262   \"\"\"\r\n--> 263   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    264                         allow_broadcast=True)\r\n    265 \r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    273       with trace.Trace(\"tf.constant\"):\r\n    274         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 275     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    276 \r\n    277   g = ops.get_default_graph()\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    298 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    299   \"\"\"Implementation of eager constant.\"\"\"\r\n--> 300   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    301   if shape is None:\r\n    302     return t\r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     95     except AttributeError:\r\n     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n---> 97   ctx.ensure_initialized()\r\n     98   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     99 \r\n\r\n/mnt/Work/work_env/lib/python3.8/site-packages/tensorflow/python/eager/context.py in ensure_initialized(self)\r\n    537         if self._use_tfrt is not None:\r\n    538           pywrap_tfe.TFE_ContextOptionsSetTfrt(opts, self._use_tfrt)\r\n--> 539         context_handle = pywrap_tfe.TFE_NewContext(opts)\r\n    540       finally:\r\n    541         pywrap_tfe.TFE_DeleteContextOptions(opts)\r\n\r\nInternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n```\r\n", "comments": ["@abhipn \r\nI ran the code shared and do not see any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/817033c7fa7cabc1f0d5b539b8fc43b1/untitled312.ipynb).\r\n\r\nWith repect tot he error please refer to below links:\r\n #41892 [link](https://stackoverflow.com/questions/54274107/out-of-memory-running-tensorflow-with-gpu-support-in-pycharm)", "> @abhipn\r\n> I ran the code shared and do not see any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/817033c7fa7cabc1f0d5b539b8fc43b1/untitled312.ipynb).\r\n> \r\n> With repect tot he error please refer to below links:\r\n> #41892 [link](https://stackoverflow.com/questions/54274107/out-of-memory-running-tensorflow-with-gpu-support-in-pycharm)\r\n\r\nI don't think the code is the issue. I used tensorflow 2.2.0 and it's working fine. I'm only getting this issue on tensorflow-2.3.0 and It's detecting my GPU. It's showing when I run this code, `tf.config.list_physical_devices(device_type='GPU')`", "@abhipn Looks like the GPU was detected but it loads some libraries (like `libcudart.so.10.1`) are loaded from `CUDA10.1` and some other libraries (for example `libcublas.so.10`, `libcusolver.so.10` etc) are loaded from `CUDA10.0`. Please check your error trace.\r\n\r\nCan you please uninstall CUDA drivers (10.1 and 10.0), unistall TF, restart, install CUDA10.1 and finally install TF. Please let me know how to progresses. Thanks!", "@jvishnuvardhan Sure. I can check it again. There's no official documentation in TensorFlow docs about Ubuntu-20.04 - cuda toolkit installation. Can you provide a source/docs where I could follow the instruction for installing it and setting up everything for tensorflow to work? It's just that I've tried other docs/methods but always ended up with one or more issues.", "@abhipn [Here](https://tensorflow.google.cn/install/gpu) are the installation instruction on TF website. Thanks!", "@jvishnuvardhan I've installed a fresh copy of ubuntu-20.04 and followed the instruction on TF website. I installed them successfully. The issue persists. \r\n\r\n```\r\n$ python\r\nPython 3.8.2 (default, Jul 16 2020, 14:00:26) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-08-04 18:51:00.960426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> tf.test.is_gpu_available()\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-08-04 18:51:27.946470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3311065000 Hz\r\n2020-08-04 18:51:27.947440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x490abb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-04 18:51:27.947533: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-04 18:51:27.978917: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-04 18:51:28.091542: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49b3d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-04 18:51:28.091605: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 750 Ti, Compute Capability 5.0\r\n2020-08-04 18:51:28.096325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-04 18:51:28.096387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-04 18:51:28.139427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-04 18:51:28.164492: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-04 18:51:28.170704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-04 18:51:28.213560: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-04 18:51:28.221614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-04 18:51:28.299972: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-04 18:51:28.301262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-04 18:51:28.310961: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/mnt/Work/env/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/mnt/Work/env/lib/python3.8/site-packages/tensorflow/python/framework/test_util.py\", line 1563, in is_gpu_available\r\n    for local_device in device_lib.list_local_devices():\r\n  File \"/mnt/Work/env/lib/python3.8/site-packages/tensorflow/python/client/device_lib.py\", line 43, in list_local_devices\r\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\nRuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n```\r\nI have installed cuda as per the instruction in TF website. I have only installed Cuda 10.1 and I didn't install Cuda 10.0\r\n\r\n**Update**: I'm facing this similar issue even on Ubuntu 18.04 with TF 2.3.0 and Nvidia Cuda 10.1. (Python 3.6.9)\r\n\r\n```\r\n(env) $ python\r\nPython 3.6.9 (default, Jul 17 2020, 12:50:27) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-08-06 04:26:56.672204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n\r\n>>> tf.test.is_built_with_cuda()\r\nTrue\r\n\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-08-06 04:27:12.809505: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-06 04:27:12.858796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-06 04:27:12.858860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-06 04:27:12.898665: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-06 04:27:12.923237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-06 04:27:12.928622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-06 04:27:12.970054: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-06 04:27:12.977014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-06 04:27:12.984431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-06 04:27:12.985434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n\r\n>>> keras = tf.keras\r\n\r\n>>> model = keras.Sequential()\r\n2020-08-06 04:27:44.444123: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3311200000 Hz\r\n2020-08-06 04:27:44.445003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x459cdd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-06 04:27:44.445050: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-06 04:27:44.524173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3eed990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-06 04:27:44.524220: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 750 Ti, Compute Capability 5.0\r\n2020-08-06 04:27:44.524909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-06 04:27:44.524970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-06 04:27:44.525008: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-06 04:27:44.525040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-06 04:27:44.525071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-06 04:27:44.525101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-06 04:27:44.525131: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-06 04:27:44.525161: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-06 04:27:44.526070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-06 04:27:44.526127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 117, in __init__\r\n    name=name, autocast=False)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 308, in __init__\r\n    self._init_batch_counters()\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 317, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 256, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2646, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1518, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1652, in _init_from_args\r\n    name=\"initial_value\", dtype=dtype)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 539, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n```", "@abhipn we no longer ship PTX for some older GPUs, which includes the one you're using.  The easiest solution for you is probably to build the pip package from source with support for your GPU (sm_50) included.  See https://www.tensorflow.org/install/gpu#hardware_requirements.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41990\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41990\">No</a>\n", "> @abhipn we no longer ship PTX for some older GPUs, which includes the one you're using. The easiest solution for you is probably to build the pip package from source with support for your GPU (sm_50) included. See https://www.tensorflow.org/install/gpu#hardware_requirements.\r\n\r\n@sanjoy where do I have to specify \"sm_50\" when building from source? The `./configure` only asking for cuda paths and others. Also can you estimate the time it takes for building from source I have a amdfx-6100 processor with 8GB Corsair Vengeance ram? Previous it took 10 hours but it didn't complete even then. ", "Hi @abhipn,\r\n\r\nYou can enter the compute capability when the configure script says:\r\n\r\n`Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:`  (Enter 5.0, and possibly others if needed.)\r\n\r\nAs for compile time, I don't think it will build quickly (although targeting just 5.0 should cut down on the compile time); I recommend building it on a beefy GCP VM for a quicker turnaround.", "I have a Quadro M1200 which has compute capability 5 and I am still getting this same error.", "> I have a Quadro M1200 which has compute capability 1200 and I am still getting this same error.\r\n\r\nQuadro M1200 also has compute capability 5.0.", "> > I have a Quadro M1200 which has compute capability 5 and I am still getting this same error.\r\n> \r\n> Quadro M1200 also has compute capability 5.0.\r\n\r\nSorry I missed this : [Default is: 3.5,7.0]. Yes in that case we would have to recompile.\r\n", "> Sorry I missed this : [Default is: 3.5,7.0]. Yes in that case we would have to recompile.\r\n\r\nThat is the default for `./configure.py` but the pip package we ship supports 3.5, 3.7, 5.2, 6.0, 6.1, 7.0 and higher than 7.0.  I will update https://www.tensorflow.org/install/gpu to be clearer about this.", "I started having this same issue today.  Getting the same error using Keras with Nvidia RTX 2080 Super.  Ubuntu 20.4", "I have the same issue today, and find nothing to solve it, have you solve it yet?", "my tensorflow work very well.\r\nI update all the packages of my python and after that getting same issue.\r\nwhat should I do to fix this?", "My tensorflow GPU environment works very well after I re-installed whole Ubuntu18.04 for TF2.0&CUDA drivers. But today, I'm forced to upgrade the tensorflow from 2.2 to 2.3, and nothing else I've done. I found the exactly same errors:\r\n```\r\n.........\r\n2020-08-30 11:35:22.030586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-30 11:35:22.035060: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<string>\", line 9, in <module>\r\n  File \"/home/clock/Work/Scripts/Python/Learn/tf2/agents/tfEnv_test.py\", line 740, in <module>\r\n    aV=tf.Variable(3, name=\"AAAAAAAA\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 256, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2646, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1518, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1652, in _init_from_args\r\n    name=\"initial_value\", dtype=dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 539, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n\r\n```\r\nIt seems the TF2.3 doesn't support my GPU. ", "TF 2.3 doesn't work with my laptop's GPU \"GeForce GTX 960M\" which is compute capability 5.0\r\nTF 2.2 works though.\r\nI'm not compiling from source. Guess it's time to move to torch. ", "yup, exactly same problem here. i had TF 2.2 and needed to use tf.keras.preprocessing.timeseries_dataset_from_array.\r\ninstalled TF 2.3, opened successfully every dynamic library, recognized my gpu (tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce MX110 computeCapability: 5.0), and then.... InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid.\r\n\r\n", "I think it happens only TF 2.3... (not  happen in 2.2 version or 2.1) maybe for some GPUS(or some CUDA versions)", "Yep, my read is that some summer intern thought it was a good idea to not support old hardware anymore to reduce the pip binary file size and to make some metrics like tensorflow average startup time go down (TF2 is a lot slower than TF1 on those old GPUs).\r\n\r\nIn https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0\r\n\r\n> GPU\r\n> TF 2.3 includes PTX kernels only for compute capability 7.0 to reduce the TF pip binary size. Earlier releases included PTX for a variety of older compute capabilities.\r\n\r\nNow it's September, holidays should be over even though work from home is probably still in place, and a lot of people will update and discover that it screws them over one way or the other. \r\nHopefully someone at tensorflow will take the helm back, and **turn the flag back on**.\r\nIt's incredibly short sighted, to make a non-backward compatible breaking change that will prevent a significant fraction of users, to use tensorflow at all. \r\n\r\nStaying with an old tensorflow version is a no go because you can't use the latest algorithms like @elvis1020 is showing.\r\n\r\nI understand that some operation may benefit from the newer compute capabilities but it shouldn't prevent glorified matrix multiplications from running.\r\n\r\nI use my laptop as my development machine because the machines with powerful GPUs are already running. If I can't have the same version of tensorflow on development and production it's a deal braker for using tensorflow.\r\n\r\nAlso my old laptops are reused as robot brains/passive monitoring tools so if I can't run tensorflow on them they become useless, so deal breaker for using tensorflow.\r\n\r\nFor information it's kind of critical for me and will make me migrate all my code toward torch within a month if nothing is changed.\r\n", "Hi everyone,\r\n\r\nTF nightly starting from the latest nightly build should now work on GPUs with compute capability 5.0 like GeForce GTX 960M.  Please give it a try and let us know how it goes.", "Hi @sanjoy \r\nI'm not sure it's working yet. \r\nI created a new virtual-env and installed and run some tf.test_is_gpu_available\r\n```\r\npip install tf-nightly-gpu\r\npython -c \"import tensorflow as tf; tf.test.is_gpu_available(); print('version :' + tf.__version__)\"\r\n```\r\n\r\n> 2020-09-04 08:08:21.708982: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n> ....\r\n\r\n> Skipping registering GPU devices...\r\n> 2020-09-04 08:08:21.743557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-09-04 08:08:21.743589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n> 2020-09-04 08:08:21.743601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n> version :2.4.0-dev20200903\r\n\r\nThe code is failing but if's because it's running a tf 2.4 version which require cudnn-8.0 which is missing  because I have currently installed cudnn7.1 to make tf 2.3 work as recommended by : \r\n\r\nhttps://www.tensorflow.org/install/gpu?hl=fr\r\n```\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-1 \\\r\n    libcudnn7=7.6.4.38-1+cuda10.1  \\\r\n    libcudnn7-dev=7.6.4.38-1+cuda10.1\r\n```\r\n\r\nI'm not sure I'm willing to try it further because it's risking screwing my current cuda installation (it's not in a virtual env and it's quite finicky to get a stable environment without falling into DLL hell). \r\n\r\nIf you tell me that you don't intend to remove compatibility for compute-capability-5.0 I'm confident it will work again, and I can work with that for now. \r\n\r\n", "@unrealwill if you're running on linux you can use [TF docker images](https://www.tensorflow.org/install/docker) to avoid having to deal with CUDA and cuDNN versions.", "@sanjoy \r\nI gave it a try : \r\nI installed nvidia-docker following : https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\r\nIt ran fine : \r\n` docker run --gpus all --rm nvidia/cuda nvidia-smi`\r\n\r\n> Fri Sep  4 08:57:25 2020       \r\n> +-----------------------------------------------------------------------------+\r\n> | NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\r\n\r\nI then freshly pull a tensorflow nightly-gpu version and run it without removing the container\r\n```\r\n docker pull  tensorflow/tensorflow:nightly-gpu\r\n docker run --gpus all -it tensorflow/tensorflow:nightly-gpu \\\r\npython -c \"import tensorflow as tf; print('version :' + tf.__version__); tf.test.is_gpu_available();\"\r\n\r\n```\r\n\r\nIf fails with \r\n>version**:2.4.0-dev20200902**\r\n> Adding visible gpu devices: 0\r\n> 2020-09-04 09:00:14.233229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/test_util.py\", line 1567, in is_gpu_available\r\n>     for local_device in device_lib.list_local_devices():\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/device_lib.py\", line 43, in list_local_devices\r\n>     _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\n> RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n\r\nThe version is still from yesterday.\r\nI'll try it again tomorrow\r\n", "Running the above command today after a fresh docker pull makes the bug disappear\r\n\r\n> version :2.4.0-dev20200904\r\n> ...\r\n> 2020-09-05 11:58:03.649241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 1494 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n\r\nThanks\r\n\r\nAddentum : I couldn't help but notice the time is not my local time and my usual trick \r\n`docker -e TZ=Europe/Paris ...` doesn't seem to have an effect", "Hello, I am facing the same issue. I am runnin tf2.2.0 on nvidia container, i comes with cuda 11 by default, but i am not being able to recognize the gpu in the code. \r\nNow It brings me the following error (when it is trying to load mtcnn):\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory\r\n", "I have the same error with a 3070 and with same tf", "> I have the same error with a 3070 and with same tf\r\n\r\nI have recently purchased a 3090 and I have build `tensorflow==2.3.0` from source with `cuda 11.1` and `cudnn 8.0.4`, I don't have any issues. If you don't want to build from source, maybe you can try the `tensorflow 2.4.0 nightly` and see if it works?", "Adding the following:\r\n```python\r\nimport os\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n```\r\n\r\nSolved the problem for me", "Many thanks, I tried it and it works!!", "Here is my config:\r\n - Kernel: Linux 5.8.13-050813-generic\r\n - Ubuntu 20.04\r\n`nvidia-smi` header says:\r\n - NVIDIA-SMI 455.38\r\n - Driver Version: 455.38\r\n - CUDA Version: 11.1\r\nDespite CUDA not being installed (`nvcc` not found)\r\n - Docker version 19.03.13, build 4484c46d9d\r\n - `nvidia-docker2` Version: 2.5.0-1\r\n - `nvidia-container-runtime` Version: 3.4.0-1\r\n\r\nMy graphic card is a GeForce RTX 3090 with compute capability of 8.6.\r\n\r\nThe following crashes with the aformentionned error (device kernel image is invalid):\r\n```sh\r\n$ docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \\\r\n    python -c \"import tensorflow as tf; \\\r\n    print(tf.config.list_physical_devices(device_type='GPU')); \\\r\n    print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n>\r\n[...]\r\n2020-11-21 17:44:30.855123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n[...]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 539, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n```\r\nThe `tensorflow/tensorflow:latest-gpu` for me has image id `8a8486aa1902` (3.05GB).\r\n\r\nWhilst the following doesn't:\r\n```sh\r\n$ docker run --gpus all -it --rm tensorflow/tensorflow:nightly-gpu \\\r\n    python -c \"import tensorflow as tf; \\\r\n    print(tf.config.list_physical_devices(device_type='GPU')); \\\r\n    print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n>\r\n[...]\r\n2020-11-21 18:29:42.261064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n[...]\r\n2020-11-21 18:29:42.571233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22057 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:0a:00.0, compute capability: 8.6)\r\ntf.Tensor(-494.97723, shape=(), dtype=float32)\r\n```\r\n\r\nThe `tensorflow/tensorflow:nightly-gpu` for me has image id `41fb5c07d7d2` (5.54GB).\r\n\r\nI don't understand how using the nightly could fix the error. As mentionned [in the doc](https://www.tensorflow.org/install/gpu#hardware_requirements):\r\n\r\n> Note: The error message **\"Status: device kernel image is invalid\"** indicates that the TensorFlow package does not contain PTX for your architecture. You can enable compute capabilities by building TensorFlow from source.\r\n\r\nBut [Nvidia doc says](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility):\r\n\r\n> PTX code produced for some specific compute capability can always be compiled to binary code of greater or equal compute capability. Note that a binary compiled from an earlier PTX version may not make use of some hardware features. For example, a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal. As a result, the final binary may perform worse than would be possible if the binary were generated using the latest version of PTX. \r\n\r\nSo, having a very recent card, suppose `latest-gpu` is targeted at compute capability 7.0, my 8.6 should still be able to handle it, albeit without 8.x special instructions right ? But no, it throws that `Status: device kernel image is invalid` error despite all.\r\n\r\n### The solution is to use the nightly version, like suggested in #43911", "**Issue still persists:**\r\nRunning `python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"` inside virtualenv with the follwing setup:\r\n\r\n_Software System Setup_\r\nUbuntu 20.04 LTS\r\nPython 3.8.5\r\nCuda 10.1 (`nvcc --version -> V10.1.243`)\r\nCudnn 7.6.5.32\r\nNvidia Drivers 455.38 (`nvidia-smi`)\r\n\r\n_Hardware Setup_\r\nNvidia RTX 3090\r\n\r\n====\r\nTensorflow Versions tried that did not work:\r\npip3 install tensorflow-gpu -> TENSORFLOW 2.3 \r\n\r\n\r\n**Tensorflow 2.4 and above require CUDA 11 & Cudnn 8 (therefore they do not work with the setup presented in this issue!)**\r\n\r\n\r\n**[NON SOLUTION]**\r\nDowngrading to tensorflow-gpu=2.2 makes the error go away, however, it takes minutes before tensorflow manages to resolve anything. \r\n\r\n**EDIT**\r\nUpgrading CUDA 11, CUDNN 8 & to tensorflow-nightly(2.4+) seems to be the solution (per this reddit: https://www.reddit.com/r/tensorflow/comments/jbp2sh/tensorflow_with_rtx_3080_extremely_slow/g8zrhjo/ )\r\n\r\n**EDIT 2**\r\nUpgraded to Cuda11.1, CUDNN 8.0.5, tensorflow-gpu-2.4.0rc4, results in a new error:\r\n`Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory;` I.e. Again incompatability issue, going to try nightly now. \r\nBut apparently one can solve this with a soft link:\r\n`sudo ln -s /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcusolver.so.11 /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcusolver.so.10`\r\nhttps://github.com/tensorflow/tensorflow/issues/43947\r\n\r\n**EDIT 3**\r\nThe current nightly build (2.5.0) also doesn't work with the above cuda versions (it results in the same error as for 2.4.0rcx", "> Adding the following:\r\n> \r\n> ```python\r\n> import os\r\n> os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n> ```\r\n> \r\n> Solved the problem for me\r\n\r\nworked thanks a lot\r\n", "solved this error on RTX 3070 with following specs:\r\nCUDA: 11.0\r\nCUDNN: 8\r\nTensorflow: 2.4", "same issue, nvidia 980ti, 5.2 - capability, cannot load dynamic libraries ((\r\n\r\n`021-05-22 21:17:09.146132: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\r\n2021-05-22 21:17:09.167266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 980 Ti computeCapability: 5.2\r\ncoreClock: 1.19GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 313.37GiB/s\r\n2021-05-22 21:17:09.167749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found`\r\n\r\nmeanwhile cuda for pytorch is available.", "I just close the terminal and open again, it works on me.", "This could sometimes happen because of different codes accessing the gpu one after another. I suggest that you use, \r\n\r\n> `sudo fuser -v /dev/nvidia*`\r\nand kill the python or code PID using\r\n\r\n> `sudo kill -9 \"PID code here\"`.\r\n\r\nThis will free the gpu memory. ", "> @jvishnuvardhan I've installed a fresh copy of ubuntu-20.04 and followed the instruction on TF website. I installed them successfully. The issue persists.\r\n> \r\n> ```\r\n> $ python\r\n> Python 3.8.2 (default, Jul 16 2020, 14:00:26) \r\n> [GCC 9.3.0] on linux\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> >>> import tensorflow as tf\r\n> 2020-08-04 18:51:00.960426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> >>> tf.test.is_gpu_available()\r\n> WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use `tf.config.list_physical_devices('GPU')` instead.\r\n> 2020-08-04 18:51:27.946470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3311065000 Hz\r\n> 2020-08-04 18:51:27.947440: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x490abb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-04 18:51:27.947533: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-08-04 18:51:27.978917: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n> 2020-08-04 18:51:28.091542: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49b3d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-04 18:51:28.091605: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 750 Ti, Compute Capability 5.0\r\n> 2020-08-04 18:51:28.096325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\n> coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n> 2020-08-04 18:51:28.096387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-08-04 18:51:28.139427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n> 2020-08-04 18:51:28.164492: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n> 2020-08-04 18:51:28.170704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n> 2020-08-04 18:51:28.213560: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-08-04 18:51:28.221614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-08-04 18:51:28.299972: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-08-04 18:51:28.301262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-08-04 18:51:28.310961: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/mnt/Work/env/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/mnt/Work/env/lib/python3.8/site-packages/tensorflow/python/framework/test_util.py\", line 1563, in is_gpu_available\r\n>     for local_device in device_lib.list_local_devices():\r\n>   File \"/mnt/Work/env/lib/python3.8/site-packages/tensorflow/python/client/device_lib.py\", line 43, in list_local_devices\r\n>     _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\n> RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n> ```\r\n> \r\n> I have installed cuda as per the instruction in TF website. I have only installed Cuda 10.1 and I didn't install Cuda 10.0\r\n> \r\n> **Update**: I'm facing this similar issue even on Ubuntu 18.04 with TF 2.3.0 and Nvidia Cuda 10.1. (Python 3.6.9)\r\n> \r\n> ```\r\n> (env) $ python\r\n> Python 3.6.9 (default, Jul 17 2020, 12:50:27) \r\n> [GCC 8.4.0] on linux\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> >>> import tensorflow as tf\r\n> 2020-08-06 04:26:56.672204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> \r\n> >>> tf.test.is_built_with_cuda()\r\n> True\r\n> \r\n> >>> tf.config.list_physical_devices('GPU')\r\n> 2020-08-06 04:27:12.809505: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n> 2020-08-06 04:27:12.858796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\n> coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n> 2020-08-06 04:27:12.858860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-08-06 04:27:12.898665: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n> 2020-08-06 04:27:12.923237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n> 2020-08-06 04:27:12.928622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n> 2020-08-06 04:27:12.970054: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-08-06 04:27:12.977014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-08-06 04:27:12.984431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-08-06 04:27:12.985434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n> \r\n> >>> keras = tf.keras\r\n> \r\n> >>> model = keras.Sequential()\r\n> 2020-08-06 04:27:44.444123: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3311200000 Hz\r\n> 2020-08-06 04:27:44.445003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x459cdd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-06 04:27:44.445050: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n> 2020-08-06 04:27:44.524173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3eed990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n> 2020-08-06 04:27:44.524220: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 750 Ti, Compute Capability 5.0\r\n> 2020-08-06 04:27:44.524909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\n> coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\r\n> 2020-08-06 04:27:44.524970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-08-06 04:27:44.525008: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n> 2020-08-06 04:27:44.525040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n> 2020-08-06 04:27:44.525071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n> 2020-08-06 04:27:44.525101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-08-06 04:27:44.525131: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-08-06 04:27:44.525161: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-08-06 04:27:44.526070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> 2020-08-06 04:27:44.526127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n>     result = method(self, *args, **kwargs)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 117, in __init__\r\n>     name=name, autocast=False)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n>     result = method(self, *args, **kwargs)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 308, in __init__\r\n>     self._init_batch_counters()\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n>     result = method(self, *args, **kwargs)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 317, in _init_batch_counters\r\n>     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\r\n>     return cls._variable_v2_call(*args, **kwargs)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 256, in _variable_v2_call\r\n>     shape=shape)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 237, in <lambda>\r\n>     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2646, in default_variable_creator_v2\r\n>     shape=shape)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n>     return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1518, in __init__\r\n>     distribute_strategy=distribute_strategy)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1652, in _init_from_args\r\n>     name=\"initial_value\", dtype=dtype)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1499, in convert_to_tensor\r\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n>     return constant_op.constant(value, dtype, name=name)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n>     allow_broadcast=True)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\r\n>     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\r\n>     t = convert_to_eager_tensor(value, ctx, dtype)\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 97, in convert_to_eager_tensor\r\n>     ctx.ensure_initialized()\r\n>   File \"/home/abhi/env/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 539, in ensure_initialized\r\n>     context_handle = pywrap_tfe.TFE_NewContext(opts)\r\n> tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n> ```\r\n\r\nHi, I have the same problem as you, did you solve your problem?\r\n", "If it has happened once, you can just restart jupyter notebook\r\nMy run time is 2minutes... is it normal? or can I speed up?\r\n"]}, {"number": 41989, "title": "Weird dash lines on ImageProjectiveTransformV2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (Google Colab)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nI used these transform values\r\n```\r\ntransform = [\r\n             [1, 0.027, -4.905, -0.025, 1.096, 4.518, 0, 0],\r\n             [1.041, 0.01, -10.256, -0.01, 1, -0.67, 0, 0],\r\n             [1, 0, 0, 0, 1.06, -2.536, 0, 0]\r\n]\r\n```\r\nbut the resulting images got.... weird dash lines. To view the images, you can open my notebook from link on the standalone code section.\r\n\r\n**Describe the expected behavior**\r\nIt should be seamless without weird lines?\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1z6zDhE6ikQr-aYHluxvlrpOhriztOmB0?usp=sharing\r\n\r\n**Other question**\r\nhttps://github.com/tensorflow/tensorflow/blob/4910e8e8ed56af3779eaa88449631a7855d4815e/tensorflow/core/kernels/image_ops.cc#L61-L83\r\nAlso is this is only logging? It's not stopping me entering random string into `fill_mode` and `interpolation` parameters?\r\n\r\n**Speculation**\r\nMy speculation is, it seems like the code responsible for map the coordinate miss by 1 pixel? I tried to understand `image_ops` code but I don't get which one it is.", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200802`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/fc67840a4f56df46d23b8c95cbbc6069/untitled78.ipynb).Thanks!", "@Smankusors Have you tried to check coordinates calcs in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image_ops.h?", "> @Smankusors Have you tried to check coordinates calcs in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image_ops.h?\r\n\r\nyea but unfortunately I can't quite understand it. It's the `MapCoordinate` structs that's responsible right? Maybe I will try tinkering/fiddling it to understand it futher.", "Ok let me know if you will find something in the meantine. \nAs It Is c++ code I will try to build a fresh version but It Is a very time consuming job (https://github.com/tensorflow/build/issues/5)", "@Smankusors As you have already some example code can you try to add a small PR to extend or add a test in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py to let to cover you error?\r\nThanks ", "> @Smankusors As you have already some example code can you try to add a small PR to extend or add a test in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py to let to cover you error?\r\n> Thanks\r\n\r\nhuh... sure.... but currently I don't know what RGB values should be filled on that dash lines \ud83d\ude15", "/cc @WindQAQ @tanzhenyu Are we using any reference impl to compare these image ops outputs (e.g. PIL etc..)?", "@Smankusors Can I ask you how you have formed the transformation matrix in the example?", "> @Smankusors Can I ask you how you have formed the transformation matrix in the example?\r\n\r\nit's random combination of\r\n* rotation (2 degrees)\r\n* zoom (x=(0.95, 1.05), y=(0.9, 1.1)); and\r\n* translation (x=(0, 0), y=(-10, 10))\r\n\r\nwhere each of them have 50% chance", "alright I just get this compiled after encountering this issue (#42066).\r\n\r\nit seems the fix is simple, only put -1 somewhere in the code. And the dash lines is gone. \ud83d\ude32\r\n\r\nAlthough........ I don't fully understand the code yet...", "Fixed by 86276ae47c546d72ef9d3fbc21141aaebb0a9de9", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41989\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41989\">No</a>\n", "@Smankusors Please let me know if there is any further problem. Thanks \ud83d\ude03 "]}, {"number": 41988, "title": "Facing issues while importing 'network' from 'tensorflow.python.keras.engine'", "body": "\r\nOn Tensorflow 2.3.0\r\nAnaconda Environment.\r\n```\r\nfrom tf_agents.environments import suite_gym\r\n\r\n\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-8-b9cd8ac2787f> in <module>\r\n----> 1 from tf_agents.environments import suite_gym\r\n      2 \r\n      3 # env = suite_gym.load(\"Breakout-v4\")\r\n      4 # env\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\__init__.py in <module>\r\n     24 from tf_agents.environments import tf_py_environment\r\n     25 from tf_agents.environments import trajectory_replay\r\n---> 26 from tf_agents.environments import utils\r\n     27 from tf_agents.environments import wrappers\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\utils.py in <module>\r\n     23 from tf_agents.environments import tf_environment\r\n     24 from tf_agents.environments import tf_py_environment\r\n---> 25 from tf_agents.policies import random_py_policy\r\n     26 from tf_agents.specs import array_spec\r\n     27 \r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\policies\\__init__.py in <module>\r\n     16 \"\"\"Policies Module.\"\"\"\r\n     17 \r\n---> 18 from tf_agents.policies import actor_policy\r\n     19 from tf_agents.policies import boltzmann_policy\r\n     20 from tf_agents.policies import epsilon_greedy_policy\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\policies\\actor_policy.py in <module>\r\n     27 import tensorflow_probability as tfp\r\n     28 \r\n---> 29 from tf_agents.networks import network\r\n     30 from tf_agents.policies import tf_policy\r\n     31 from tf_agents.specs import tensor_spec\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\networks\\__init__.py in <module>\r\n     16 \"\"\"Networks Module.\"\"\"\r\n     17 \r\n---> 18 from tf_agents.networks import actor_distribution_network\r\n     19 from tf_agents.networks import actor_distribution_rnn_network\r\n     20 from tf_agents.networks import bias_layer\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\networks\\actor_distribution_network.py in <module>\r\n     24 import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\r\n     25 \r\n---> 26 from tf_agents.networks import categorical_projection_network\r\n     27 from tf_agents.networks import encoding_network\r\n     28 from tf_agents.networks import network\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\networks\\categorical_projection_network.py in <module>\r\n     24 import tensorflow_probability as tfp\r\n     25 \r\n---> 26 from tf_agents.networks import network\r\n     27 from tf_agents.networks import utils\r\n     28 from tf_agents.specs import distribution_spec\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\networks\\network.py in <module>\r\n     31 \r\n     32 # pylint:disable=g-direct-tensorflow-import\r\n---> 33 from tensorflow.python.keras.engine import network as keras_network  # TF internal\r\n     34 from tensorflow.python.training.tracking import base  # TF internal\r\n     35 from tensorflow.python.util import tf_decorator  # TF internal\r\n\r\nImportError: cannot import name 'network' from 'tensorflow.python.keras.engine' (C:\\Users\\asus\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\__init__.py)\r\n```\r\n\r\nTried Reinstalling TF. The issue still persists.\r\nTried running on TF 2.2, it asks me to upgrade.\r\n", "comments": ["@vybhav72954,\r\nCould you please install the `tf-agents-nightly` package. With the nightly version, I was able to import the module without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/89a833806ae0ebf6aaf34f3d6f58e3b1/41988.ipynb). Thanks!", "I think `pip install tf-agents==0.5.0` is enought\r\n\r\nUPDATE:\r\nthis was not for Tensorflow 2.3.0", "How i resolved the issue :\r\n\r\nuninstall version tf_agents 0.5.0  ===>   `pip uninstall tf-agents`\r\n\r\ninstall the nightly version 0.6.0 at the moment ==> `pip install tf-agents-nightly`", "Kindly give me a few days, as my laptop has blown up and I don't have access to another system cause of the pandemic. I'll update you by the weekend. Sorry for the inconvenience. \r\n\r\n@bhack I had tried that, but wasn't aware about the Version issue, thnks", "@vybhav72954  Yes cause TF version in colab i.e. to run @amahendrakar  gist was updated as default to 2.3.0 probably  just yesterday/today", "Thank you, it works @amahendrakar \r\nThough am asking too much, but is possible can you put some light on what was wrong here? ", "> Though am asking too much, but is possible can you put some light on what was wrong here?\r\n\r\n@vybhav72954,\r\nSeems like the issue was caused due to TensorFlow and TF-Agents version incompatibility. For more information, please take a look at this [link](https://github.com/tensorflow/agents#releases). Thanks!", "+1 looks like you need to use a newer version of TF-Agents."]}, {"number": 41987, "title": "NotFoundError:  No algorithm worked! .... convolutional model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- TensorFlow installed from (source or binary): Latest as of today\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: RTX 2070\r\n\r\nThe sample code to debug this issue is at,\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n\r\nNotFoundError:  No algorithm worked!\r\n\t [[node sequential_3/conv1d/conv1d (defined at <ipython-input-42-716049f06cb3>:12) ]] [Op:__inference_train_function_127130]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n", "comments": ["@summa-code \r\nPlease provide simple stand alone code for us to replicate the issue faced or if possible share colab gist with the error faced.", "@Saduf2019 , the link is provided with the bug. Please look above link.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41987\">No</a>\n", "Closed by mistake. It is the sample code on Tensorflow.org and it is not working with TF-NIGHTLY code.", "@summa-code \r\nCan you please provide a colab gist with the error faced for us to analyse.", "Wow... i really don't have colab, here is the link again, can you not use this ?\r\nI do it locally. This code is a sample code from Tensorflow.org and it is not working with the latest build i do locally. If it is too difficult for you guys to test your own code, i will just move to MxNet. I have a problem to solve here.\r\n\r\nThe sample code to debug this issue is at,\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb\r\n\r\nFrom Colab,\r\n```\r\nprint('Input shape:', wide_window.example[0].shape)\r\ntry:\r\n  print('Output shape:', multi_step_dense(wide_window.example[0]).shape)\r\nexcept Exception as e:\r\n  print(f'\\n{type(e).__name__}:{e}')\r\n-----------------------------------------------------\r\nerror,\r\nInput shape: (32, 24, 19)\r\nInvalidArgumentError:Matrix size-incompatible: In[0]: [32,456], In[1]: [57,32] [Op:MatMul]\r\n```\r\n\r\nThis seems to work in Colab, but not working on the code i build locally, what is it in Colab ? How do i see what version of the code is in Colab ?\r\nThis is the part of the code that is giving me issues.\r\n```\r\nhistory = compile_and_fit(conv_model, conv_window)\r\nIPython.display.clear_output()\r\nval_performance['Conv'] = conv_model.evaluate(conv_window.val)\r\nperformance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)\r\n```", "@summa-code can you please check formatting in the above message? Please use ` ``` ` before and after code blocks and errors", "Learned something with '```'\r\nLooks like this is not happening in Colab, not sure what changed. But I still have the original error... some people are reporting that this is CUDNN initialization issue. And the shortcut seems to disable GPU completely, and only runs on CPU.\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\n\r\nAnd this solution did not help either, TF_CUDNN_RESET_RND_GEN_STATE=1\r\n\r\nSo i wonder, how would we go about convolution network if GPU support is not available.", "I am giving up on these because i have many core CPU, i have a problem to work on.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41987\">No</a>\n"]}, {"number": 41986, "title": "Typo on the order number for title on guide \"Automatic Differentiation\"", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://tensorflow.google.cn/guide/autodiff\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWhen listing the reason of the \"None\" gradient (Chapter of \"Getting a gradient of None\"), the text  \"**3. Took gradients through an integer or string**\" is followed by order number \"5\", which is \"**5. Took gradients through a stateful object**\". Suggest to correct it to  \"**4. Took gradients through a stateful object**\"\r\n\r\n", "comments": ["@zsk843 Thanks for reporting the typo. We updated the doc. It is already reflecting in the TF website.\r\n\r\nPlease verify once and close the issue if this was resolved. Thanks!", "I am closing this issue as this was already resolved and TF website reflects the correction. Thanks!"]}, {"number": 41985, "title": "Posenet when change to front facing camera displays the camera preview upside down. ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Microsoft Windows 10 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: all\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nAs in the current behaviour the app displays the camera preview with upside down orientation from the front facing camera.\r\n( According to the example code provided for posenet sample app. )\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should displays the camera preview as perfect and well oriented as it shows from the back facing camera.\r\n\r\nPlease help as i have working on something important.", "comments": ["@mohitgithubprojects,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!", "@amahendrakar , Thank you for responsing,\r\n\r\n**[Actually i'm just using the example code provided by tflite of posenet in android.]**\r\n\r\n[https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android](url)\r\n\r\nThe example code displays the results using the back camera of the device but as i have mentioned i want the results using the front camera of the android device . So, i have updated the code in this class :- \r\n[https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/android/app/src/main/java/org/tensorflow/lite/examples/posenet/PosenetActivity.kt#L283](url)\r\n\r\n to do so in the line no. \" 283 \".\r\n\r\n**example code:-**   cameraDirection == CameraCharacteristics.LENS_FACING_FRONT\r\n\r\n**My updated code :-** cameraDirection == CameraCharacteristics.LENS_FACING_BACK\r\n\r\n**Results by the example code when using with the Back camera :-**  \r\n\r\nAs you can see the orientation of the preview is perfect.\r\n\r\n![Screenshot_20200803-202041 1](https://user-images.githubusercontent.com/60915290/89196081-5802ad00-d5c7-11ea-883d-fcbf3fe07109.png)\r\n\r\n**Results by the example code when using with the Front camera :-**\r\nAs you can see the orientation of the preview is Upside down. ( just by changing that one line (283) to front camera )\r\n\r\n![Screenshot_20200803-201945 1](https://user-images.githubusercontent.com/60915290/89196346-b760bd00-d5c7-11ea-9ac5-3554a9bc2c4e.png)\r\n", "@mohitgithubprojects Thanks for your issue. \r\nThe code needs to be tweaked for implementing front camera as you tried.\r\nPerhaps you may try referring this [thread](https://stackoverflow.com/questions/37103571/android-front-camera-images-are-being-saved-upside-down) for custom code implementing the front camera orientation parameters.\r\nCurrently this is [not supported](https://github.com/tensorflow/examples/blob/4fd91657a3242dca66165e68e377b2b891846c08/lite/examples/posenet/android/app/src/main/java/org/tensorflow/lite/examples/posenet/PosenetActivity.kt#L280) by the example out of the box.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41984, "title": "Merge pull request #1 from tensorflow/master", "body": "sync with main", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41984) for more info**.\n\n<!-- need_sender_cla -->", "@kk-dhara Thank you for your contribution. Can you please sign CLA? Thanks!", "This is an empty PR"]}, {"number": 41983, "title": "Bug in Saved Tensorflow model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nModel training has unexplainable impact on older models saved on disk. I saved 1 tensorflow model(let's say 'best_model.h5') onto the disk which when loaded and evaluated gave me correct results. However, we I retrained my model which I didn't save to disk and then loaded my older model (i.e. the saved model 'best_model.h5'); the model evaluation results of saved model differed from the earlier load.\r\nE.g with steps.\r\nStep 1. Took UCI data for binary sentiment classification.\r\nStep 2. Data Preparation -> Tokenize and Padded sequences of text.\r\nStep 3. Simple NN architecture with Embedding, GlobalAveragePooling1D() and output Dense layer.\r\nStep 4. Evaluated the model on Test data. Accuracy 83%\r\nStep 5. Saved the model as 'best_model.h5'\r\nStep 6. Loaded the 'best_model.h5' model and evaluated in Test Data. Accuracy 83%.\r\nStep 7. Repeated all above steps except Step 5. Step 4 gave accuracy 81% and Step 6 gave accuracy 92%.\r\n**Describe the expected behavior**\r\nIn the rerun as explained in Step 7 above, Step 6 should have maintained the accuracy of 83% instead it gave a different accuracy.\r\n**Standalone code to reproduce the issue**\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\ndf=pd.read_csv('imdb_labelled.txt', sep='\\t', header=None)\r\ndf.columns=['text', 'sentiment']\r\n\r\ntext=df.text.values\r\nlabels=df.sentiment.values\r\n\r\nnum_tokens=2000\r\nmax_len=20\r\n\r\ntokenize=Tokenizer(num_words=num_tokens, oov_token='<OOV>')\r\ntokenize.fit_on_texts(text)\r\nsentences=tokenize.texts_to_sequences(text)\r\n\r\ntext_padded = pad_sequences(sentences, maxlen=max_len, padding='post', truncating='post')\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(text_padded, labels, test_size = 0.25)\r\n\r\ninitializer1 = tf.keras.initializers.GlorotUniform(seed=37)\r\ninitializer2 = tf.keras.initializers.GlorotNormal(seed=41)\r\nmodel=tf.keras.Sequential([\r\n                           tf.keras.layers.Embedding(num_tokens, 16, input_length=max_len, embeddings_initializer=initializer1),\r\n                           tf.keras.layers.GlobalAveragePooling1D(),\r\n                           tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer2)\r\n])\r\n\r\nmodel.summary()\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\nmodel.fit(X_train, y_train, batch_size=16, epochs=30, verbose=0)\r\n\r\ny_pred=np.round(model.predict(X_test))\r\n\r\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\nprint(confusion_matrix(y_test,y_pred))\r\nprint(classification_report(y_test,y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\n\r\nModel: \"sequential_90\"\r\nLayer (type)                 Output Shape              Param \r\nembedding_91 (Embedding)     (None, 20, 16)            32000     \r\nglobal_average_pooling1d_91  (None, 16)                0         \r\ndense_90 (Dense)             (None, 1)                 17        \r\nTotal params: 32,017\r\nTrainable params: 32,017\r\nNon-trainable params: 0\r\n\r\nConfusion matrix when trained again.\r\n========================\r\n[[65 20]\r\n [25 77]]\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.72      0.76      0.74        85\r\n           1       0.79      0.75      0.77       102\r\n\r\n    accuracy                           0.76       187\r\n   macro avg       0.76      0.76      0.76       187\r\nweighted avg       0.76      0.76      0.76       187\r\n\r\n0.7593582887700535\r\n\r\n### Commented the model.save below to avoid overwrite of earlier saved model of 75.93%.\r\n'# model.save('best_model.h5')\r\nsaved_model=tf.keras.models.load_model('best_model.h5')\r\nsaved_model.summary()\r\nsm_y_pred=np.round(saved_model.predict(X_test))\r\n\r\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\nprint(confusion_matrix(y_test,sm_y_pred))\r\nprint(classification_report(y_test,sm_y_pred))\r\nprint(accuracy_score(y_test, sm_y_pred))\r\n\r\nloss, acc = saved_model.evaluate(X_test,  y_test, verbose=2)\r\nprint('Restored model, accuracy: {:5.2f}%'.format(100*acc))\r\n\r\nModel: \"sequential\"\r\nLayer (type)                 Output Shape              Param #   \r\nembedding (Embedding)        (None, 20, 16)            32000     \r\nglobal_average_pooling1d (Gl (None, 16)                0         \r\ndense (Dense)                (None, 1)                 17        \r\n\r\nTotal params: 32,017\r\nTrainable params: 32,017\r\nNon-trainable params: 0\r\n\r\nConfusion matrix of saved model.  Earlier the accuracy of saved model was 75.93% when initially saved.\r\n==================================================================\r\n[[89  7]\r\n [ 9 82]]\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.91      0.93      0.92        96\r\n           1       0.92      0.90      0.91        91\r\n\r\n    accuracy                           0.91       187\r\n   macro avg       0.91      0.91      0.91       187\r\nweighted avg       0.91      0.91      0.91       187\r\n\r\n0.9144385026737968\r\n187/187 - 0s - loss: 0.3531 - accuracy: 0.9144\r\nRestored model, accuracy: 91.44%\r\n\r\n## The saved model predictions improved magically to 91.44%.\r\n\r\nAttaching the dataset used by the code.\r\n==========================\r\n[imdb_labelled.txt](https://github.com/tensorflow/tensorflow/files/5012013/imdb_labelled.txt)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Roshan1986 \r\nplease provide complete stand alone indented code to replicate the issue or a colab link with the issue reported for us to analyse. \r\nCan you please confirm if you have changed any parameters  or data when you say retrain", "I confirm I didn't change any parameters while I re-trained. Anyhow, does changing parameters affect the model saved to disk?\r\n\r\nI have provided the code and data in the issue description. Anyways, I am also providing the colab link as well https://colab.research.google.com/drive/1jpNMISMJGLILaCkhrj1NiEsqKCYLFBd0. Hope this helps.\r\n\r\nYou will have to run the code as described in my issue description to understand the bug.", "@Roshan1986 \r\nI do not have access to the gist shared.", "If you understood the bug.. you can try in any code of yours.. this bug doesn't necessarily require my code.. I believe it is a bug which can be noticed whenever you save your model and run the way I am running the code. Let me know if it is feasible at your end.", "@Roshan1986 I have requested the access for your notebook. Please grant it. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Roshan1986 I followed your steps and I don't see any discrepancy in the evaluation results from the loaded model. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/995da4780c50e96fcf5d8c22e1a8492a/untitled21.ipynb).\r\n\r\nAs we don't have access to your code, I created one with `mnist` data. In future, try to share your code so that it will save some time for us and it will help faster resolution. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41982, "title": "I could do import tensorflow as tf with no problem, but the library keeps returning errors when used", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: installed using pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA (10.1), cuDNN(7.6.4)\r\n- GPU model and memory:\r\nNVIDIA GeForce GTX 1050 with Max-Q Design\r\n\r\n\r\n**Describe the problem**\r\nI could import tensorflow with no problem, but when I used the library, it returns errors.\r\nThese are my tf library versions\r\ntensorboard                        2.1.1\r\ntensorboard-plugin-wit             1.7.0\r\ntensorflow                         2.1.0\r\ntensorflow-estimator               2.1.0\r\ntensorflow-gpu                     2.1.0\r\ntensorflow-gpu-estimator           2.1.0\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport tensorflow as tf\r\ntf. __version__ (it is bolded because of the double underscore)\r\n\r\n**Any other info / logs**\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-a0ebd646edc3>\", line 7, in <module>\r\n    tf.__version__\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-a0ebd646edc3>\", line 7, in <module>\r\n    tf.__version__\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n---------------------------------------------------------------------------", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@PreVizsla \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nPlease, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "this is my cpu model\r\nProcessor\tIntel(R) Core(TM) i7-8550U CPU @ 1.80GHz, 2001 Mhz, 4 Core(s), 8 Logical Processor(s)\r\n\r\nIt seems that I forget to download the visual c++, I'll download it and check on the situation ASAP", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41982\">No</a>\n", "The problem is due to the lack of Visual C++ when running tf"]}, {"number": 41981, "title": "Add support for any TensorSpec-describable object to tf.data.Dataset.from_generator", "body": "This PR addresses the #35342 issue which proposes to add the support for different types of tensors to tf.data.Dataset.from_generator. It also finalizes the development effort started at PR #37400.\r\n\r\nInitially `from_generator` had two parameters: `output_types` and `output_shapes` (one mandatory and the other one optional) that allowed user to specify the dtypes and optionally shapes of the output objects, but not the types of the objects themselves thus making it impossible for a generator function to output more complicated types of objects like `tf.RaggedTensor` or `tf.SparseTensor`.\r\n\r\nWe add new parameter `output_signature` which describes the expected output of `from_generator` in terms of [TypeSpecs](https://www.tensorflow.org/api_docs/python/tf/TypeSpec). With this change a user can emit any object from a generator function they decide to pass to the `from_generator` given that such object is describable by the corresponding `TypeSpec` specification.\r\n\r\nWe leave the `output_types` and `output_shapes` parameters in place for now but mark them as deprecated with the intention to remove them in one of the upcoming TF releases and make `output_signature` the only way to specify the output for `from_generator`.\r\n", "comments": ["@edloper Thank you very much for the detailed and thorough review!", "@edloper could you please approve or follow up on the requested changes?", "@aaudiber @jsimsa Hey guys, do you still have plans to release this feature? It's complete, the requested changes are addressed. It's also almost identical to the previous PR that was merged but with the bug fixed and minor changes applied. From the comments it looks like community is still interested in having it.", "@lithuak can you please check sanity builds failures ?", "@rthadur Fixed pylint issues\r\n@edloper Could you try to re-approve please.", "@lithuak  Can you please check @jsimsa's comments and keep us posted ? Thanks!", "@jsimsa check it out )", "@lithuak  can you please check sanity build failures ?", "@rthadur @jsimsa guys, any clue why copybara fails?", "@lithuak There were a couple of internal tests which started failing now that `from_generator` checks output types more carefully. I'm working with the test owners to update their code to set output types correctly. Once the tests are fixed we can merge.", "@aaudiber Thanks Andrew, great to hear that!", "@lithuak, Please update the title and description to directly describe the changes made in the PR. That will help with writing release notes based on the commit history (PR title becomes commit title, and PR body becomes commit description).", "@aaudiber check it out"]}, {"number": 41980, "title": "Large batch size with dense layers will fail all_reduce occasionally", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n  os: Linux\r\n  os kernel version: #1 SMP Debian 4.9.210-1 (2020-01-20)\r\n  os release version: 4.9.0-12-amd64\r\n  os platform: Linux-4.9.0-12-amd64-x86_64-with-debian-9.12\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla P100-PCIE-16GB (on Google Cloud Platform)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI tried to use the SyncBatchnormalization (tf.keras.layers.experimental.SyncBatchNormalization) in my models but I found it will result in NaN in the model sometimes. Thus I decided to take a closer look. In the below code I implemented a simple SyncBatchnormalization. And I found when the batch size is very large (e.g. 262144) for dense layers (which is the case for the sub-module in my model), the `all_reduce` will occasionally result in wrong results.\r\n\r\n**Describe the expected behavior**\r\n`all_reduce` results in correct results at all times.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.distribute import distribution_strategy_context as ds\r\nfrom tensorflow.python.distribute import reduce_util\r\nfrom tensorflow.python.keras.layers import normalization\r\n\r\n\r\nclass SyncBatchNormalization(normalization.BatchNormalizationBase):\r\n    \"\"\"The SyncBatchNormalization in TF 2.2 seems causing NaN issue.\r\n    We implement this one to avoid the issue.\r\n    See https://github.com/google-research/simclr/blob/bfe07eed7f101ab51f3360100a28690e1bfbf6ec/resnet.py#L37-L85\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 axis=-1,\r\n                 momentum=0.99,\r\n                 epsilon=1e-3,\r\n                 center=True,\r\n                 scale=True,\r\n                 beta_initializer='zeros',\r\n                 gamma_initializer='ones',\r\n                 moving_mean_initializer='zeros',\r\n                 moving_variance_initializer='ones',\r\n                 beta_regularizer=None,\r\n                 gamma_regularizer=None,\r\n                 beta_constraint=None,\r\n                 gamma_constraint=None,\r\n                 renorm=False,\r\n                 renorm_clipping=None,\r\n                 renorm_momentum=0.99,\r\n                 trainable=True,\r\n                 adjustment=None,\r\n                 name=None,\r\n                 **kwargs):\r\n        # Currently we only support aggregating over the global batch size.\r\n        super(SyncBatchNormalization, self).__init__(\r\n            axis=axis,\r\n            momentum=momentum,\r\n            epsilon=epsilon,\r\n            center=center,\r\n            scale=scale,\r\n            beta_initializer=beta_initializer,\r\n            gamma_initializer=gamma_initializer,\r\n            moving_mean_initializer=moving_mean_initializer,\r\n            moving_variance_initializer=moving_variance_initializer,\r\n            beta_regularizer=beta_regularizer,\r\n            gamma_regularizer=gamma_regularizer,\r\n            beta_constraint=beta_constraint,\r\n            gamma_constraint=gamma_constraint,\r\n            renorm=renorm,\r\n            renorm_clipping=renorm_clipping,\r\n            renorm_momentum=renorm_momentum,\r\n            fused=False,\r\n            trainable=trainable,\r\n            virtual_batch_size=None,\r\n            name=name,\r\n            **kwargs)\r\n\r\n    def _calculate_mean_and_var(self, inputs, reduction_axes, keep_dims):\r\n        shard_mean, shard_variance = super(SyncBatchNormalization, self)._calculate_mean_and_var(\r\n            inputs, reduction_axes, keep_dims=keep_dims)\r\n        replica_ctx = ds.get_replica_context()\r\n        if replica_ctx:\r\n            group_mean, group_variance = replica_ctx.all_reduce(reduce_util.ReduceOp.MEAN, [shard_mean, shard_variance])\r\n            mean_distance = tf.math.squared_difference(tf.stop_gradient(group_mean), shard_mean)\r\n            group_variance += replica_ctx.all_reduce(reduce_util.ReduceOp.MEAN, mean_distance)\r\n            tf.cond(tf.reduce_mean(group_variance) > 50,\r\n                    lambda: tf.print(\r\n                        f\"\\n{self.name} id\", replica_ctx.replica_id_in_sync_group, \"/\",\r\n                        replica_ctx.num_replicas_in_sync, \"\\n\",\r\n                        \"local mean distance:\", mean_distance, \"mean local mean distance\",\r\n                        tf.reduce_mean(mean_distance), \"\\n\",\r\n                        \"group var:\", group_variance, \"mean group var:\", tf.reduce_mean(group_variance), \"\\n\",\r\n                        \"local var:\", shard_variance, \"mean local var:\", tf.reduce_mean(shard_variance), \"\\n\",\r\n                        \"group mean:\", group_mean, \"mean group mean\", tf.reduce_mean(group_mean), \"\\n\",\r\n                        \"local mean:\", shard_mean, \"mean local mean\", tf.reduce_mean(shard_mean), \"\\n\",\r\n                        \"size:\", tf.shape(shard_mean)),\r\n                    lambda: tf.no_op()\r\n                    )\r\n            return group_mean, group_variance\r\n        else:\r\n            return shard_mean, shard_variance\r\n\r\n\r\nclass Test(tf.keras.models.Model):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.mlps = []\r\n        for i in range(10):\r\n            self.mlps.append(tf.keras.Sequential([\r\n                tf.keras.layers.Dense(512),\r\n                SyncBatchNormalization(),\r\n                tf.keras.layers.ReLU(),\r\n                tf.keras.layers.Dense(256),\r\n                SyncBatchNormalization(),\r\n                tf.keras.layers.ReLU(),\r\n                tf.keras.layers.Dense(128),\r\n            ]))\r\n        self.head = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        out = []\r\n        for mlp in self.mlps:\r\n            out.append(mlp(inputs))\r\n        return self.head(tf.concat(out, axis=-1))\r\n\r\n\r\ndummy_data = np.random.random((2621440, 3)).astype(np.float32) * 6 - 3\r\ndummy_label = np.random.randint(0, 10, 2621440).astype(np.int32)\r\n# print(dummy_label.shape)\r\ndataset = tf.data.Dataset.from_tensor_slices((dummy_data, dummy_label)).batch(262144)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = Test()\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0)\r\n    )\r\n    model.fit(dataset, epochs=10000)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere I paste the outputs when the code runs on a machine with 4 P100 GPUs:\r\n```\r\n$ python test_syncbn.py\r\n2020-08-02 00:12:57.236317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-08-02 00:12:58.708082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.712891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:58.713072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.714765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:00:05.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:58.714901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.801883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties:\r\npciBusID: 0000:00:06.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:58.802048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.803362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties:\r\npciBusID: 0000:00:07.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:58.803698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-02 00:12:58.805523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-08-02 00:12:58.807244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-08-02 00:12:58.807614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-08-02 00:12:58.809516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-02 00:12:58.810620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-02 00:12:58.814611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-02 00:12:58.814734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.815664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.816579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.817486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.818413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.819344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.820235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.821133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:58.821967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-08-02 00:12:58.822388: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-08-02 00:12:58.830470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2000185000 Hz\r\n2020-08-02 00:12:58.831013: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c4178b0db0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-02 00:12:58.831039: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-02 00:12:59.256430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.329113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.360236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.367112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.368223: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c4145a2150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-02 00:12:59.368249: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2020-08-02 00:12:59.368256: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2020-08-02 00:12:59.368267: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2020-08-02 00:12:59.368293: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2020-08-02 00:12:59.371553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.372373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:59.372472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.373300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:00:05.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:59.373389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.374198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties:\r\npciBusID: 0000:00:06.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:59.374265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.375094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties:\r\npciBusID: 0000:00:07.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-08-02 00:12:59.375161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-02 00:12:59.375183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-08-02 00:12:59.375204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-08-02 00:12:59.375223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-08-02 00:12:59.375242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-02 00:12:59.375260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-02 00:12:59.375280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-02 00:12:59.375341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.376284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.377135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.378016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.378845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.379650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.380503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.381341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.382160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-08-02 00:12:59.382205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-02 00:12:59.386308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-02 00:12:59.386333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 2 3\r\n2020-08-02 00:12:59.386341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y N N\r\n2020-08-02 00:12:59.386350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N N N\r\n2020-08-02 00:12:59.386355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 2:   N N N Y\r\n2020-08-02 00:12:59.386363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 3:   N N Y N\r\n2020-08-02 00:12:59.386614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.387475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.388358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.389240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.390107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.390967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15056 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\r\n2020-08-02 00:12:59.391525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.392374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15056 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:05.0, compute capability: 6.0)\r\n2020-08-02 00:12:59.392906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.393843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15056 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:06.0, compute capability: 6.0)\r\n2020-08-02 00:12:59.394386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 00:12:59.395303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15056 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:07.0, compute capability: 6.0)\r\nEpoch 1/10000\r\n2020-08-02 00:13:52.468511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n10/10 [==============================] - 3s 335ms/step - loss: 2.7836\r\nEpoch 2/10000\r\n10/10 [==============================] - 3s 335ms/step - loss: 2.7742\r\nEpoch 3/10000\r\n10/10 [==============================] - 3s 339ms/step - loss: 2.7742\r\nEpoch 4/10000\r\n10/10 [==============================] - 3s 341ms/step - loss: 2.7742\r\nEpoch 5/10000\r\n 1/10 [==>...........................] - ETA: 0s - loss: 2.7735\r\nsync_batch_normalization_9 id 0 / 4\r\n local mean distance: [372190.406 373072.062 361857.031 ... 11281.4863 14100.6201 13597.5859] mean local mean distance 71088.7109\r\n group var: [278891.656 279530 271112.156 ... 8756.89258 10855.8447 10485.8672] mean group var: 53282.5\r\n local var: [0.216322735 0.351680636 0.654191434 ... 0.101593301 0.440946668 0.21738404] mean local var: 0.440011531\r\n group mean: [611.271729 611.215698 601.546692 ... 105.587341 118.7118 115.605087] mean group mean 91.9500732\r\n local mean: [1.1976552 0.419436097 0.00116307894 ... -0.626998782 -0.0342339203 -1.00359774] mean local mean -0.01634828\r\n size: [256]\r\n\r\nsync_batch_normalization_9 id 1 / 4\r\n local mean distance: [0.806274891 0.0985621 1.88196e-05 ... 0.223046422 0.000869709416 0.567140639] mean local mean distance 0.132760748\r\n group var: [279142.844 279803.875 271389.75 ... 8461.29 10575.9697 10198.2939] mean group var: 53316.6094\r\n local var: [0.217430741 0.351392329 0.65093118 ... 0.102492645 0.439819723 0.220061317] mean local var: 0.439691663\r\n group mean: [0.299309373 0.104648665 0.00144605222 ... -0.15742597 -0.00983027834 -0.251029134] mean group mean -0.00408996362\r\n local mean: [1.19723749 0.418594658 0.00578420889 ... -0.629703879 -0.0393211134 -1.00411654] mean local mean -0.0163598545\r\n size: [256]\r\n\r\nsync_batch_normalization_9 id 2 / 4\r\n local mean distance: [372190.406 373070.594 361849.094 ... 11281.6875 14101.8311 13597.8037] mean local mean distance 71088.5547\r\n group var: [278891.656 279530 271112.156 ... 8756.89258 10855.8447 10485.8672] mean group var: 53282.5\r\n local var: [0.214262575 0.350284219 0.650416493 ... 0.101662345 0.442076713 0.21601209] mean local var: 0.439108968\r\n group mean: [611.271729 611.215698 601.546692 ... 105.587341 118.7118 115.605087] mean group mean 91.9500732\r\n local mean: [1.19765222 0.420656025 0.00775553659 ... -0.627944 -0.0393257216 -1.00453138] mean local mean -0.0164480079\r\n size: [256]\r\n\r\nsync_batch_normalization_9 id 3 / 4\r\n local mean distance: [372189.5 373072.375 361852.312 ... 11281.6631 14100.9863 13597.001] mean local mean distance 71088.6406\r\n group var: [278891.656 279530 271112.156 ... 8756.89258 10855.8447 10485.8672] mean group var: 53282.5\r\n local var: [0.218500063 0.352682829 0.651915729 ... 0.102855965 0.440146983 0.216531143] mean local var: 0.439344555\r\n group mean: [611.271729 611.215698 601.546692 ... 105.587341 118.7118 115.605087] mean group mean 91.9500732\r\n local mean: [1.19834054 0.419162512 0.00508273114 ... -0.627833903 -0.0357722044 -1.00109076] mean local mean -0.0162223056\r\n size: [256]\r\n10/10 [==============================] - 3s 343ms/step - loss: 2.6909\r\nEpoch 6/10000\r\n10/10 [==============================] - 3s 337ms/step - loss: 2.6679\r\nEpoch 7/10000\r\n10/10 [==============================] - 3s 336ms/step - loss: 2.6742\r\n```\r\n\r\nAs you can see, the `group_mean` readouts from different replicas are different.", "comments": ["I guess #39569 might also relate to this issue.", "Thanks for the reproduction. It's very helpful in debugging in the issue.\r\n\r\nWe're now suspecting there's bug in the implementation of batching all reduces. We're still triaging the issue. As a workaround, could you try\r\n\r\n```\r\ntf.distribute.experimental.MultiWorkerMirroredStrategy(communication=\r\n    tf.distribute.experimental.CollectiveCommunication.NCCL)\r\n```\r\n\r\nMultiWorkerMirroredStrategy can work with a single host. It has a different implementation of all-reduce which doesn't appear to have this issue. \r\n\r\nor\r\n\r\n```\r\ntf.distribute.MirroredStrategy(cross_device_ops=\r\n    tf.distribute.NcclAllReduce(num_packs=0))\r\n```\r\n\r\nwhich disables batching.\r\n\r\nBoth seems work with your code snippet.", "Thanks! Let me have a try!", "I tried both but it still seems not quite working with the model I am using...", "`tf.distribute.experimental.MultiWorkerMirroredStrategy` seem still not working on the snippet. \r\nI got:\r\n```\r\n.....\r\nEpoch 43/10000\r\n 9/10 [==========================>...] - ETA: 0s - loss: 2.7582\r\nsync_batch_normalization_9 id 0 / 4\r\n local mean distance: [8.53681286e-06 1.85389661e-06 2.63737311e-05 ... 1.8964065e-05 3.21558673e-06 4.8557812e-08] mean local mean distance 7.56685677e-06\r\n group var: [1561.26294 1593.64099 1791.79395 ... 6671.23535 6986.21484 6931.66748] mean group var: 25238.8965\r\n local var: [0.619471371 0.115997538 0.56299603 ... 0.556955397 0.187242299 0.108146854] mean local var: 0.427159101\r\n group mean: [0.0513707772 -0.244486839 -0.13331385 ... 0.597594917 -0.32254 -0.545654297] mean group mean -0.00133506674\r\n local mean: [0.0542925596 -0.245848417 -0.138449386 ... 0.601949692 -0.320746779 -0.545433939] mean local mean -0.00123446435\r\n size: [256]\r\n\r\nsync_batch_normalization_9 id 1 / 4\r\n local mean distance: [4746.66357 4724.86865 5493.61768 ... 21673.2168 22961.373 22451.6543] mean local mean distance 64156.8047\r\n group var: [1561.26294 1593.64099 1791.79395 ... 6671.23535 6986.21484 6931.66748] mean group var: 25238.8965\r\n local var: [0.619366884 0.116250142 0.570473433 ... 0.555402756 0.185872182 0.107667856] mean local var: 0.428228378\r\n group mean: [68.9452591 68.494 73.9906616 ... 147.813507 151.205566 149.292618] mean group mean 205.618973\r\n local mean: [0.0492218435 -0.243678957 -0.128276825 ... 0.595238209 -0.324539959 -0.546138525] mean local mean -0.0013961466\r\n size: [256]\r\n\r\nsync_batch_normalization_9 id 2 / 4\r\n local mean distance: [1495.91516 1649.23169 1671.29077 ... 5009.50391 4982.7417 5274.5835] mean local mean distance 36797.0703\r\n group var: [1561.26294 1593.64099 1791.79395 ... 6671.23535 6986.21484 6931.66748] mean group var: 25238.8965\r\n local var: [0.615450859 0.116186649 0.56849575 ... 0.554236054 0.184639618 0.108589321] mean local var: 0.426401079\r\n group mean: [38.7277641 40.367157 40.7463303 ... 71.3742142 70.2679291 72.0813828] mean group mean 171.070541\r\n local mean: [0.0507010408 -0.243576661 -0.135094836 ... 0.596365333 -0.320611417 -0.544947863] mean local mean -0.00133211352\r\n size: [256]\r\n\r\nsync_batch_normalization_9 id 3 / 4\r\n local mean distance: [1.06306119e-08 1.27067707e-07 3.53240216e-06 ... 5.90565662e-07 2.96459302e-06 1.95916087e-07] mean local mean distance 2.34964932e-06\r\n group var: [1561.26294 1593.64099 1791.79395 ... 6671.23535 6986.21484 6931.66748] mean group var: 25238.8965\r\n local var: [0.619064808 0.1151025 0.565606236 ... 0.554088473 0.1859712 0.108047657] mean local var: 0.426624477\r\n group mean: [0.0513707772 -0.244486839 -0.13331385 ... 0.597594917 -0.32254 -0.545654297] mean group mean -0.00133506674\r\n local mean: [0.0512676723 -0.244843304 -0.131434381 ... 0.596826434 -0.324261785 -0.546096921] mean local mean -0.00137753412\r\n size: [256]\r\n.....\r\n```", "`tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce(num_packs=0))` seems not working either\r\n```\r\nsync_batch_normalization_15 id 0 / 4\r\n local mean distance: [0.036636889 0.0267491788 0.157074586 ... 0.0306813717 0.115952119 0.00129608891] mean local mean distance 0.0566041581\r\n group var: [8809.3623 10924.9756 10323.6641 ... 15140.1162 12941.5811 14948.5811] mean group var: 16489.3125\r\n local var: [0.383874357 0.17763406 1.19304097 ... 0.120828331 0.240406573 0.332836837] mean local var: 0.429641724\r\n group mean: [0.197513074 0.165621802 -0.393351614 ... -0.174160585 0.337269396 -0.0377266482] mean group mean -0.0191135146\r\n local mean: [0.388920724 0.329173565 -0.789678 ... -0.349321574 0.677786827 -0.0737278834] mean local mean -0.0380694903\r\n size: [256]\r\n\r\nsync_batch_normalization_15 id 1 / 4\r\n local mean distance: [35235.7969 43699.1094 41289.4023 ... 60559.8945 51765.0195 59792.9883] mean local mean distance 65955.3594\r\n group var: [8809.3623 10924.9756 10323.6641 ... 15140.1162 12941.5811 14948.5811] mean group var: 16489.3125\r\n local var: [0.381689548 0.177335322 1.19517827 ... 0.121255741 0.240857527 0.332744867] mean local var: 0.42955482\r\n group mean: [188.100204 209.373657 202.414642 ... -246.436234 -226.843338 -244.603348] mean group mean -95.5150909\r\n local mean: [0.388204068 0.330330819 -0.783299804 ... -0.347025782 0.675929427 -0.0772941113] mean local mean -0.0381464213\r\n size: [256]\r\n\r\nsync_batch_normalization_15 id 2 / 4\r\n local mean distance: [0.0389708 0.0276281461 0.156617671 ... 0.0299725905 0.114160225 0.00134693284] mean local mean distance 0.0563229881\r\n group var: [8809.3623 10924.9756 10323.6641 ... 15140.1162 12941.5811 14948.5811] mean group var: 16489.3125\r\n local var: [0.385531843 0.177066773 1.20327413 ... 0.119457915 0.240990862 0.334448516] mean local var: 0.429490775\r\n group mean: [0.197513074 0.165621802 -0.393351614 ... -0.174160585 0.337269396 -0.0377266482] mean group mean -0.0191135146\r\n local mean: [0.3949233 0.331838965 -0.789101124 ... -0.347286522 0.675145447 -0.0744272321] mean local mean -0.0381462201\r\n size: [256]\r\n\r\nsync_batch_normalization_15 id 3 / 4\r\n local mean distance: [0.0372947715 0.0275733843 0.152806878 ... 0.0298070945 0.114042602 0.00170057896] mean local mean distance 0.0564728938\r\n group var: [8809.3623 10924.9756 10323.6641 ... 15140.1162 12941.5811 14948.5811] mean group var: 16489.3125\r\n local var: [0.387123376 0.178325772 1.19653797 ... 0.12071833 0.239743888 0.333901644] mean local var: 0.429986924\r\n group mean: [0.197513074 0.165621802 -0.393351614 ... -0.174160585 0.337269396 -0.0377266482] mean group mean -0.0191135146\r\n local mean: [0.390631616 0.331674159 -0.784256816 ... -0.346807897 0.674971342 -0.0789647251] mean local mean -0.0383110754\r\n size: [256]\r\n```", "You're right. This affects all NCCL all-reduce implementations. Root cause is still not clear. So far I have the following observations:\r\n\r\n- This is easy to reproduce when there're enough NCCL all-reduces, and a sufficiently large batch size. \r\n- In most cases the last device that launches a all-reduce ends up with incorrect value. But it's not always the case, see https://github.com/tensorflow/tensorflow/issues/41980#issuecomment-668238029. ", "Seems like a duplicate of https://github.com/tensorflow/tensorflow/issues/41539. Incorrect results of NCCL allreduce can be reproduced since TF 1.15.", "Same issue here. I got a lower result with sync bn than normal bn or single GPU", "A bit more info: I tried to run my model with syncBN on TPUs and it seem I did not encounter similar issue, which might indicate that all_reduce on TPU is correct?", "We think the problem is either with NCCL or with Tensorflow's interaction with NCCL. TPU all-reduce uses a different implementation, which shouldn't be affected by this issue. \r\n\r\nTensorflow also has non NCCL all-reduce for GPU, although the performance is worse. You can use it by\r\n\r\n```\r\ntf.distribute.experimental.MultiWorkerMirroredStrategy(communication=\r\n    tf.distribute.experimental.CollectiveCommunication.RING)\r\n```", "@anj-s can you take a look, i also got this problem when training multi-gpu with SynBatchNorm in my framework (https://github.com/TensorSpeech/TensorFlowTTS). Seems you are the person implemented this SynBatchnorm :3", "@crccw just want to check if there are updates on this issue?", "@dathudeptrai Do you still see the issue if you use a different `tf.distribute.experimental.CollectiveCommunication` as suggested by @crccw above?\r\n```\r\ntf.distribute.experimental.MultiWorkerMirroredStrategy(communication=\r\n    tf.distribute.experimental.CollectiveCommunication.RING)\r\n```", "I tried 3 strategies (TPU, CollectiveCommunication.RING and MirroredStrategy) with Sync Bn multiple times\r\n\r\nACC : TPU > > > CollectiveCommunication.RING > MirroredStrategy", "We haven't made much progress, but it appears that it's not happening on V100. This doesn't mean it's a P100 issue, since V100 runs faster which could hide the problem.\r\n\r\nCould you try V100 if possible? If not, I suggest RING as a workaround, which is slower but uses a non-NCCL implementation.\r\n\r\n@edwardyehuang Are all hyper parameters the same with the three strategies? ", "> \r\n> \r\n> We haven't made much progress, but it appears that it's not happening on V100. This doesn't mean it's a P100 issue, since V100 runs faster which could hide the problem.\r\n> \r\n> Could you try V100 if possible? If not, I suggest RING as a workaround, which is slower but uses a non-NCCL implementation.\r\n> \r\n> @edwardyehuang Are all hyper parameters the same with the three strategies?\r\n\r\nYes. fully same training startegies.  Lower acc may caused by multiple issues/bugs.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41980\">No</a>\n", "I am not experimenting with GPUs recently, but I vaguely remember on V100 there were similar issues too?\r\nHmmm, why is this issue closed? I think it has not been resolved yet?", "We've rolled back a commit that appears to fix the issue, but we don't understand the root cause yet.\r\n\r\n@dubey Should we keep this issue open to investigate the root cause?", "Yes SG to keep the issue open to investigate the root cause.", "@YurongYou It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41980\">No</a>\n"]}, {"number": 41979, "title": "BatchNorm with Momentum=0.0 Should Return the Same Output with training=True, and then with Training=False", "body": "Python 3.6\r\nUbuntu 18.04\r\nTensorflow from Binary\r\nv2.3.0-rc2-23-gb36436b087 2.3.0\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nseq1 = tf.keras.Sequential([tf.keras.layers.Conv2D(32, 3), tf.keras.layers.BatchNormalization(axis=3, momentum=0.0), tf.keras.layers.LeakyReLU(0.01)])\r\n\r\nimport numpy as np\r\nx = np.random.randn(128,32,32,1)\r\n\r\nres1 = seq1(x, training=True)\r\nres2 = seq1(x, training=False)\r\n\r\nprint(np.linalg.norm(res1 - res2))\r\n```\r\n\r\nWhen using BatchNormalization with momentum=0.0 with training=True, the moving average of mean and variance should update to the statistics of the batch exactly. Therefore when running the model again on the same data with training=False one should expect to get the same output. \r\nBut the print shows a difference between the two of 0.005684062\r\nHelp will be appreciated.\r\nThank you\r\n\r\n\r\n\r\n", "comments": ["@benjamink85 \r\nThis is not any bug related issue or feature request for Tensorflow, hence request you to report this on [StackOverflow](https://stackoverflow.com/questions/tagged//tensorflow) as its a bigger community to help.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41979\">No</a>\n"]}, {"number": 41978, "title": "TensorFlow UnboundLocalError: local variable 'batch_outputs' referenced before assignment", "body": "I am trying to run some python3 code on databricks GPU cluster for image understanding by CNN.\r\nThe env:\r\n\r\n       TensorFlow: 2.2\r\n       python 3.7.6\r\n       keras: 2.3.0-tf\r\n       Unbuntu: 4.4\r\n\r\nThe code: \r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nbase_dir = '/Users/Downloads/cats_and_dogs_small'\r\n\r\ntrain_dir = os.path.join(base_dir, 'train')\r\nvalidation_dir = os.path.join(base_dir, 'validation')\r\ntest_dir = os.path.join(base_dir, 'test')\r\n\r\ndatagen = ImageDataGenerator(rescale=1./255)\r\nbatch_size = 20\r\n\r\ndef extract_features(directory, sample_count):\r\n    features = np.zeros(shape=(sample_count, 4, 4, 512))\r\n    labels = np.zeros(shape=(sample_count))\r\n    generator = datagen.flow_from_directory(\r\n        directory,\r\n        target_size=(150, 150),\r\n        batch_size=batch_size,\r\n        class_mode='binary')\r\n    i = 0\r\n    for inputs_batch, labels_batch in generator:\r\n        features_batch = conv_base.predict(inputs_batch) # error !\r\n        features[i * batch_size : (i + 1) * batch_size] = features_batch\r\n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\r\n        i += 1\r\n        if i * batch_size >= sample_count:\r\n            # Note that since generators yield data indefinitely in a loop,\r\n            # we must `break` after every image has been seen once.\r\n            break\r\n    return features, labels\r\n\r\ntrain_features, train_labels = extract_features(train_dir, 2000). # error here !\r\nvalidation_features, validation_labels = extract_features(validation_dir, 1000)\r\ntest_features, test_labels = extract_features(test_dir, 1000)\r\n\r\n```\r\n\r\nthe call stack:\r\n\r\n```\r\nFound 0 images belonging to 0 classes.\r\nUnboundLocalError: local variable 'batch_outputs' referenced before assignment\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<command-2528158> in <module>\r\n     33     return features, labels\r\n     34 \r\n---> 35 train_features, train_labels = extract_features(train_dir, 2000)\r\n     36 validation_features, validation_labels = extract_features(validation_dir, 1000)\r\n     37 test_features, test_labels = extract_features(test_dir, 1000)\r\n\r\n<command-2528158> in extract_features(directory, sample_count)\r\n     23     i = 0\r\n     24     for inputs_batch, labels_batch in generator:\r\n---> 25         features_batch = conv_base.predict(inputs_batch)\r\n     26         features[i * batch_size : (i + 1) * batch_size] = features_batch\r\n     27         labels[i * batch_size : (i + 1) * batch_size] = labels_batch\r\n\r\n/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     86       raise ValueError('{} is not supported in multi-worker mode.'.format(\r\n     87           method.__name__))\r\n---> 88     return method(self, *args, **kwargs)\r\n     89 \r\n     90   return tf_decorator.make_decorator(\r\n\r\n/databricks/python/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1283             callbacks.on_predict_batch_end(step, {'outputs': batch_outputs})\r\n   1284       callbacks.on_predict_end()\r\n-> 1285     all_outputs = nest.map_structure_up_to(batch_outputs, concat, outputs)\r\n   1286     return tf_utils.to_numpy_or_python_type(all_outputs)\r\n   1287 \r\n\r\nUnboundLocalError: local variable 'batch_outputs' referenced before assignment\r\n\r\n```", "comments": ["@umusa \r\n\r\nPlease, provide sample data to reproduce the issue in our environment.Thanks!", "Hi, the data is at www.kaggle.com/c/dogs-vs-cats/data, you may need to download the zip file that contain cat and dog images.\r\nThe py3 code that I am using is at https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb\r\n\r\nthanks\r\n\r\n", "@umusa \r\n\r\nI have tried in colab with TF version 2.3 and i am seeing the below error message.\r\n(`AttributeError: 'int' object has no attribute 'assign'`).\r\nPlease, find the gist [here](https://colab.research.google.com/gist/ravikyram/255eaab9af456883953ea0b3bc3e13a6/untitled231.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41978\">No</a>\n", "@umusa did u the find the solution ? I am facing the same error", "@umusa @ravikyram I have the same problem when I passed an empty input set to the predict method. A better error message really would help to avoid confusion.", "@177arc  that's correct. This error is related to passing  empty array to the predict method. "]}, {"number": 41977, "title": "Benchmark application with `use_xnnpack=true`: ModifyGraphWithDelegate is disallowed when graph is immutable.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.3.0\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI've built the benchmarking application with XNNPack enabled for my host machine (Linux, Ubuntu 18.04) using the `tensorflow/tensorflow:2.3.0` Docker image. I built the application with the following bazel command\r\n\r\n`bazel build -c opt --define tflite_with_xnnpack=true tensorflow/lite/tools/benchmark:benchmark_model`\r\n\r\nWhen running `benchmark_application` with a non-quantized test model I have, TFLite spits out the following error: \r\n\r\n`ERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.`\r\n\r\nThe benchmarking application still runs to completion, although I am not sure whether the XNNPack delegate is being used because of the above error. I noticed no difference in performance when running without `--use_xnnpack`, so don't think the delegate is being used.\r\n\r\nI've run `benchmark_application` with the following command:\r\n\r\n`./benchmark_model --graph=model.tflite --use_xnnpack=true`\r\n\r\n**Describe the expected behavior**\r\n\r\nWould expect with v2.3.0 for the XNNPack delegate to work with non-quantized tflite models. The model was generated with the TFLite model conversion tools, specifically the Keras to TFLite tools.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis may not be able to be reproduced across all platforms, however, invoking `benchmark_model` as above would be the best attempt to reproduce the issue.\r\n\r\nThe model can be found here: https://www.dropbox.com/s/dyed8ch0erghb1v/yolov4-416.tflite?dl=0\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n`uname -a` output: Linux ubuntu 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \r\n", "comments": ["Note \"bazel build -c opt --define tflite_with_xnnpack=true tensorflow/lite/tools/benchmark:benchmark_model\" will compile the binary that will apply xnnpack delegate by default. \r\n\r\nIf you omit \"--define tflite_with_xnnpack=true\" when building the benchmark tool, you could simply use \"--use_xnnpack=true\" to check the xnnpack delegate performance as you're aware of.\r\n\r\nSpeaking of the error, I think it's because we tried to apply xnnpack delegate twice in this case (i.e. the fist is triggered by setting \"--use_xnnpack=true\", and the second is triggered by applying xnnpack delegate by default (i.e. setting \"--define tflite_with_xnnpack=true\" when compiling the benchmark tool).\r\n\r\nTo tell if the xnnpack delegate has been applied, I think there should be sth. from the output that tells this.\r\n\r\nAnyway, will look into this issue further by running the attached model.\r\n", "@multiverse-tf thanks for following up.\r\n\r\n> Note \"bazel build -c opt --define tflite_with_xnnpack=true tensorflow/lite/tools/benchmark:benchmark_model\" will compile the binary that will apply xnnpack delegate by default.\r\n\r\nIf I run `./benchmark_model --graph=model.tflite`, `Use xnnpack : [0]` is printed to stdout as part of `benchmark_model`'s debug output. Since the benchmarking tool was built with `--define tflite_with_xnnpack=true`, shouldn't I expect `Use xnnpack: [1]` to be output instead? If I run `./benchmark_model --graph=model.tflite --use_xnnpack=true`, `Use xnnpack: [1]` is output to stdout, in addition to (but not limited to) the following,\r\n\r\n`INFO: Created TensorFlow Lite XNNPACK delegate for CPU.`\r\n`Applied XNNPACK delegate, and the model graph will be partially executed w/ the delegate.`\r\n", "Hi Rameen,\r\n\r\nSorry for the confusing output message. \"Use xnnpack: [..]\" is an output from the model benchmark tool itself. Currently, there's no runtime query api into TFLite interpreter w.r.t. which delegates have been applied. As a result, when the xnnpack delegate could be applied by default (i.e. via compilation flag --define=tflite_with_xnnpack=true\"), the benchmark tool doesn't know whether the xnnpack delegate has been applied or not by default by the underlying tflite interpreter.\r\n\r\nWe will provide more output messages from the tool, and hope it will further clarify the behavior.\r\n", "@multiverse-tf thanks!", "Note: Let's leave this issue open to address the case where it should be perfectly OK to apply XNNPACK multiple times to the same graph (or to apply the GPU delegate, then the XNNPACK delegate).", "@raryanpur It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "@sushreebarsa @multiverse-tf provided an explantation of this behavior above that was fine for my use case at the time and I should've closed the ticket then. It appears via @jdduke this ticket is relevant to the broader project so is being kept open. I don't currently have the bandwidth to test this for you on the latest version of TF but hopefully the details I've provided in the ticket description are enough for you or the team to test if desired.", "We addressed this issue in the latest version of TF. Now TFLite supports applying multiple delegates if the graph doesn't have dynamically-shaped tensors. I am closing this issue, thanks for providing the context @raryanpur :-)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41977\">No</a>\n"]}]