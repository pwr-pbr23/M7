[{"number": 12185, "title": "`numpy >= 1.11.0` is not sufficient (repro script included)", "body": "Our pip package includes a dependency on [`numpy >= 1.11.0`][1]. However, when `numpy` version 1.11.0 is already installed and then the user installs TensorFlow, TensorFlow fails to load with the following error:\r\n```\r\nSystemError: initialization of _pywrap_tensorflow_internal raised unreported exception\r\n```\r\nThe root cause is:\r\n```\r\nRuntimeError: module compiled against API version 0xb but this version of numpy is 0xa\r\n```\r\n\r\nThis problem occurs on **Python 3 only** (I only tested Python 3.4).\r\n\r\nThis is a regression from build 582 to build 583. I include a repro script below.\r\n```sh\r\n$ cat examine.sh\r\n#!/bin/sh\r\n# USAGE: ./examine.sh JENKINS_BUILD_ID\r\nset -eu\r\nvirtualenv=\"$(mktemp -d)\"\r\nvirtualenv \"${virtualenv}\" -p python3\r\n. \"${virtualenv}/bin/activate\"\r\npip install numpy==1.11.0\r\nbuild_id=\"$1\"\r\nNIGHTLY_URL=\"https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/${build_id}/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl\"\r\npip install \"${NIGHTLY_URL}\"\r\npython -c 'import tensorflow'\r\nprintf 'Done; everything checks out.\\n'\r\n```\r\nRemoving the `pip install numpy==1.11.0`, or subsequently removing it with `pip uninstall -y numpy==1.11.0`, or installing TensorFlow with `pip install -I \"${NIGHTLY_URL}` all fix the issue.\r\n\r\nThe change log for those two builds is 349932f4400d15d610f7b6e51923c6a60ddd186b...dff1062bbf53cd8890d8d43ea815c90ba4555ba4, but nothing in there looks particularly suspicious. Perhaps something changed on the Jenkins side?\r\n\r\nHere is the result of the repro script on builds 582 and 583:\r\n```sh\r\n$ ./examine.sh 582\r\nRunning virtualenv with interpreter /usr/bin/python3\r\nUsing base prefix '/usr'\r\nNew python executable in /tmp/tmp.IS6cENR7vo/bin/python3\r\nAlso creating executable in /tmp/tmp.IS6cENR7vo/bin/python\r\nInstalling setuptools, pip, wheel...done.\r\nCollecting numpy==1.11.0\r\n  Using cached numpy-1.11.0-cp34-cp34m-manylinux1_x86_64.whl\r\nInstalling collected packages: numpy\r\nSuccessfully installed numpy-1.11.0\r\nCollecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/582/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl\r\n  Downloading https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/582/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl (46.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46.5MB 33kB/s \r\nCollecting six>=1.10.0 (from tensorflow==1.head)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.3.0 (from tensorflow==1.head)\r\n  Using cached protobuf-3.3.0-cp34-cp34m-manylinux1_x86_64.whl\r\nRequirement already satisfied: numpy>=1.11.0 in ./tmp.IS6cENR7vo/lib/python3.4/site-packages (from tensorflow==1.head)\r\nCollecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow==1.head)\r\n  Using cached tensorflow_tensorboard-0.1.2-py3-none-any.whl\r\nRequirement already satisfied: wheel>=0.26 in ./tmp.IS6cENR7vo/lib/python3.4/site-packages (from tensorflow==1.head)\r\nRequirement already satisfied: setuptools in ./tmp.IS6cENR7vo/lib/python3.4/site-packages (from protobuf>=3.3.0->tensorflow==1.head)\r\nCollecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\n  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl\r\nCollecting markdown==2.2.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\nCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nCollecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\nInstalling collected packages: six, protobuf, werkzeug, markdown, html5lib, bleach, tensorflow-tensorboard, tensorflow\r\nSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 protobuf-3.3.0 six-1.10.0 tensorflow-1.3.0rc2 tensorflow-tensorboard-0.1.2 werkzeug-0.12.2\r\nDone; everything checks out.\r\n\r\n$ ./examine.sh 583\r\nRunning virtualenv with interpreter /usr/bin/python3\r\nUsing base prefix '/usr'\r\nNew python executable in /tmp/tmp.Iwv3luN526/bin/python3\r\nAlso creating executable in /tmp/tmp.Iwv3luN526/bin/python\r\nInstalling setuptools, pip, wheel...done.\r\nCollecting numpy==1.11.0\r\n  Using cached numpy-1.11.0-cp34-cp34m-manylinux1_x86_64.whl\r\nInstalling collected packages: numpy\r\nSuccessfully installed numpy-1.11.0\r\nCollecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/583/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl\r\n  Downloading https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/583/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl (46.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46.4MB 32kB/s \r\nRequirement already satisfied: numpy>=1.11.0 in ./tmp.Iwv3luN526/lib/python3.4/site-packages (from tensorflow==1.head)\r\nCollecting protobuf>=3.3.0 (from tensorflow==1.head)\r\n  Using cached protobuf-3.3.0-cp34-cp34m-manylinux1_x86_64.whl\r\nRequirement already satisfied: wheel>=0.26 in ./tmp.Iwv3luN526/lib/python3.4/site-packages (from tensorflow==1.head)\r\nCollecting autograd>=1.1.11 (from tensorflow==1.head)\r\nCollecting six>=1.10.0 (from tensorflow==1.head)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow==1.head)\r\n  Using cached tensorflow_tensorboard-0.1.2-py3-none-any.whl\r\nRequirement already satisfied: setuptools in ./tmp.Iwv3luN526/lib/python3.4/site-packages (from protobuf>=3.3.0->tensorflow==1.head)\r\nCollecting scipy>=0.17 (from autograd>=1.1.11->tensorflow==1.head)\r\n  Using cached scipy-0.19.1-cp34-cp34m-manylinux1_x86_64.whl\r\nCollecting future>=0.15.2 (from autograd>=1.1.11->tensorflow==1.head)\r\nCollecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\n  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl\r\nCollecting markdown==2.2.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\nCollecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\nCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.head)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nInstalling collected packages: six, protobuf, scipy, future, autograd, werkzeug, markdown, html5lib, bleach, tensorflow-tensorboard, tensorflow\r\nSuccessfully installed autograd-1.1.11 bleach-1.5.0 future-0.16.0 html5lib-0.9999999 markdown-2.2.0 protobuf-3.3.0 scipy-0.19.1 six-1.10.0 tensorflow-1.3.0rc2 tensorflow-tensorboard-0.1.2 werkzeug-0.12.2\r\nRuntimeError: module compiled against API version 0xb but this version of numpy is 0xa\r\nRuntimeError: module compiled against API version 0xb but this version of numpy is 0xa\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/tmp/tmp.Iwv3luN526/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/tmp/tmp.Iwv3luN526/lib/python3.4/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\nSystemError: initialization of _pywrap_tensorflow_internal raised unreported exception\r\n```\r\n\r\n[1]: https://github.com/tensorflow/tensorflow/blob/ed7d08eaf92cb9dbc96a09ec6e08d0a93b70a61b/tensorflow/tools/pip_package/setup.py#L35", "comments": ["@yifeif are you able to comment on this? Thanks.", "Thanks for bring this up!\r\n\r\nThis might be due to the following change that adds autograd as a pip dependency. https://github.com/tensorflow/tensorflow/blob/dff1062bbf53cd8890d8d43ea815c90ba4555ba4/tensorflow/tools/pip_package/setup.py#L39\r\nAutograd requires numpy>=1.12\r\nWilliam, could you try ping to a higher numpy version?", "To clarify: in environments where this is actually a problem, I'm not installing numpy myself. The problem is that some environments (e.g., CIs) come preinstalled with numpy. In this case, a setup script whose only contents are `pip install \"${TF_NIGHTLY_URL}\"` will fail, because `numpy==1.11.0` is already installed on the system.\r\n\r\nI included `pip install numpy==1.11.0` in my example script only to replicate an environment where numpy is already installed. You should think of `pip install numpy==1.11.0` as part of a \"`setUp`\" function, and everything after that as part of an actual test case."]}, {"number": 12184, "title": "Tensorboard AttributeError: 'SummaryMetadata' object has no attribute 'display_name'", "body": "### System information\r\n- No custom code\r\n- **OS: MaxOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Build label: 0.5.3-homebrew\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**: ./tensorboard --logdir=/Users/Pertis/Syncog-TF/SUD/logs/p1/summaries\r\n\r\n### Describe the problem\r\nTensorflow and Tensorboard built from source.\r\nTensorboard finds event files, but returns:\r\nAttributeError: 'SummaryMetadata' object has no attribute 'display_name'\r\nTensorboard installed from prebuilt binaries runs without error\r\n\r\n### Source code / logs\r\nSee stack trace below:\r\nTensorBoard 0.1.3 at http://Rocs-iMac.fios-router.home:6006 (Press CTRL+C to quit) ^C\r\n(tensorflow) Rocs-iMac:tensorboard Pertis$ ./tensorboard --logdir=/Users/Pertis/Syncog-TF/SUD/logs/p1/summaries\r\nException in thread Reloader:\r\nTraceback (most recent call last):\r\n  File \"/Users/Pertis/anaconda/envs/tensorflow/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/Users/Pertis/anaconda/envs/tensorflow/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/application.py\", line 325, in _reload_forever\r\n    reload_multiplexer(multiplexer, path_to_run)\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/application.py\", line 299, in reload_multiplexer\r\n    multiplexer.Reload()\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/event_processing/event_multiplexer.py\", line 195, in Reload\r\n    accumulator.Reload()\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/event_processing/event_accumulator.py\", line 209, in Reload\r\n    self._ProcessEvent(event)\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/backend/event_processing/event_accumulator.py\", line 355, in _ProcessEvent\r\n    value = data_compat.migrate_value(value)\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/data_compat.py\", line 53, in migrate_value\r\n    return handler(value) if handler else value\r\n  File \"/private/var/tmp/_bazel_Pertis/77b889388121c5daae674df9c3cd82a3/execroot/org_tensorflow_tensorboard/bazel-out/darwin_x86_64-py3-fastbuild/bin/tensorboard/tensorboard.runfiles/org_tensorflow_tensorboard/tensorboard/data_compat.py\", line 80, in _migrate_image_value\r\n    display_name=value.metadata.display_name or value.tag,\r\nAttributeError: 'SummaryMetadata' object has no attribute 'display_name'\r\n\r\n", "comments": ["TensorBoard has its own repo [here](https://github.com/tensorflow/tensorboard). Can you file the issue there instead?", "Thanks for the correction.", "I couldn't find this same issue in the tensorboard issues and multiple searches have lead me here instead. Could you please post the link? ", "https://github.com/tensorflow/tensorboard/issues/404\r\nThank you for reporting."]}, {"number": 12183, "title": "Enable int32 on GPU for tf.tile", "body": "This fix enabled int32 on GPU for tf.tile, to fix the following error:\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    tt = tf.tile(tf.range(4), [3])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tt))\r\n```\r\n\r\nThis fix fixes #12169.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "The failing Python 3 Linux test seems probably unrelated. It's in queue_runner and it passes all the tests but then the harness failed with segmentation fault\r\n\r\n```\r\nRan 17 tests in 0.804s\r\n\r\nOK\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 164: 14629 Segmentation fault      (core dumped) \r\n```", "@yaroslavvb yup, that's a known culprit. I'll merge this if the rest of the tests pass."]}, {"number": 12182, "title": "[XLA] Register XLA_GPU iff cuda enable", "body": "Fix https://github.com/tensorflow/tensorflow/issues/12070.\r\n\r\n@hawkinsp PTAL, Thanks!", "comments": ["@ScorpioCPH, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @tensorflower-gardener and @girving to be potential reviewers.", "Can one of the admins verify this patch?", "@hawkinsp PTAL :)", "@hawkinsp could you take another look, please?", "@hawkinsp PR updated. Could you take another look, please?\r\n\r\n:-)", "@tensorflow-jenkins Test this, please.", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please.", "FYI, this CL has caused compilation error in tensorflow's nightly builds, e.g.,\r\n\r\ntensorflow/compiler/tf2xla/xla_cpu_backend.cc: In function 'bool tensorflow::CpuOpFilter(tensorflow::KernelDef*)':\r\ntensorflow/compiler/tf2xla/xla_cpu_backend.cc:23:11: error: invalid use of incomplete type 'class tensorflow::KernelDef'\r\n   if (kdef->op() == \"RandomStandardNormal\") {\r\n           ^\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:27:0,\r\n                 from ./tensorflow/core/common_runtime/device.h:41,\r\n                 from ./tensorflow/core/common_runtime/local_device.h:19,\r\n                 from ./tensorflow/compiler/tf2xla/xla_op_registry.h:25,\r\n                 from tensorflow/compiler/tf2xla/xla_cpu_backend.cc:16:\r\n./tensorflow/core/framework/kernel_def_builder.h:27:7: error: forward declaration of 'class tensorflow::KernelDef'\r\n class KernelDef;\r\n       ^\r\ntensorflow/compiler/tf2xla/xla_cpu_backend.cc:24:9: error: invalid use of incomplete type 'class tensorflow::KernelDef'\r\n     kdef->clear_constraint();\r\n\r\nSee full example logs [here](http://ci.tensorflow.org/view/Tensorflow%20Jenkins%20Monitored%20builds/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/605/consoleFull).\r\n\r\n@ScorpioCPH Can you come up with a quick fix? ", "@caisq ok, I will fix it today.", "@ScorpioCPH already fixed it here: https://github.com/tensorflow/tensorflow/pull/12565", "@jhseu Thanks!"]}, {"number": 12181, "title": "Cuda build fail with 1.3.0", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n - issue with the same outcomes is here: https://stackoverflow.com/questions/45266594/tensorflow-with-gpu-cuda8-compile-error-shfl-up\r\n\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDebian 8.4\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nTrying to compile it from source.\r\n\r\n- **TensorFlow version (use command below)**:\r\nlatest git head\r\n\r\n- **Python version**: \r\n2.7.10\r\n\r\n- **Bazel version (if compiling from source)**:\r\n5.2\r\n\r\n- **CUDA/cuDNN version**:\r\nvarious, tried CUDA 8.0, 7.5, 7.0 and cuDNN 4.0, 5.0, 5.1\r\n\r\n- **Exact command to reproduce**:\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow-1.3.0\r\n./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.5.2- (@non-git) installed.\r\nPlease specify the location of python. [Default is /software/python/2.7.10/intel/bin/python]: \r\nFound possible Python library paths:\r\n/software/python/2.7.10/intel/lib/python2.7/site-packages\r\n/software/python27-modules/software/python/2.7.10/intel/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is /software/python/2.7.10/intel/lib/python2.7/site-packages\r\n/software/python27-modules/software/python/2.7.10/intel/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]: \r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]: \r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: \r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /software/cuda/8.0\r\n\"Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /software/cuda/8.0]:/software/cudnn/5.1\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]2.0,3.5\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n\r\nbazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nAfter some time of compiling this error is raised:\r\n>./tensorflow/core/util/cuda_kernel_helper.h(620): error: identifier \"__shfl\" is undefined\r\n>\r\n>./tensorflow/core/util/cuda_kernel_helper.h(640): error: identifier \"__shfl_up\" is undefined\r\n>\r\n>./tensorflow/core/util/cuda_kernel_helper.h(660): error: identifier \"__shfl_down\" is undefined\r\n>\r\n>./tensorflow/core/util/cuda_kernel_helper.h(680): error: identifier \"__shfl_xor\" is undefined\r\n>\r\n>4 errors detected in the compilation of \"/tmp/tmpxft_000042b4_00000000->10_resampler_ops_gpu.cu.compute_20.cpp1.ii\".\r\n>ERROR: /scratch/hanousek/tensorflow-1.3.0-rc2/tensorflow/contrib/resampler/BUILD:45:1: output >'tensorflow/contrib/resampler/_objs/python/ops/_resampler_ops_gpu/tensorflow/contrib/resampler/kernels/resampler_ops_gpu.cu.pic.o' was not created.\r\n>ERROR: /scratch/hanousek/tensorflow-1.3.0-rc2/tensorflow/contrib/resampler/BUILD:45:1: not all outputs were created or valid.\r\n\r\nMore detailed error message is here: [https://pastebin.com/RArJfN3m](url) (expire after a month)\r\n", "comments": ["Perhaps this is due to a selection of unsupported computing ability 2.0. I'd expect that this raise an error during configuration and not compilation error much more time after.\r\n\r\nWhen I selected only 3.5 (minimum is 3.0), compilation worked fine.", "As @phanousk suggested, this is due to invalid cuda compute capability.\r\nAs CUDA developer guide states, the 4 missing functions are supported in devices with compute capability 3.0 and higher.\r\nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions", "So would you please adjust the configuration phase? I would expect to raise an error there or to update the info at least.", "@yifeif Could you add this check to the configuration script?", "That would be something good to check ahead of time. I can add it to configure."]}, {"number": 12180, "title": "Adding support for s390x in calculation of cpu_frequency", "body": "PR for changing the GetCycleCounterFrequency for s390x to fix test `//tensorflow/core:platform_profile_utils_cpu_utils_test`.\r\n\r\nFacing issue similar to power reported [here](https://github.com/tensorflow/tensorflow/issues/10450#issuecomment-307097621) .\r\n\r\nOn s390x too `cpu_frequency = -1` and hence below test fails:\r\n```\r\n[ RUN      ] CpuUtilsTest.CheckCycleCounterFrequency\r\n2017-07-17 04:43:02.050579: F tensorflow/core/platform/profile_utils/cpu_utils_test.cc:57] Check failed: cpu_frequency > 0 (-1 vs. 0)\r\n```\r\n\r\nExtending the fix added for PPC raised via [PR ](https://github.com/tensorflow/tensorflow/pull/10522#issuecomment-311064869)\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@gunan, I saw a check for LE in the ppc commit and since s390x is big endian, thought of keeping the condition separate for s390x for avoiding any future conflicts. \r\nWould it be better to merge both in same if condition?", "I would still merge these for the sake of keeping code cleaner.\r\nThen we can worry about the added complexity (if needed) when adding ppc LE support.", "@gunan, thank you for  your feedback.\r\nClosing this PR; opened #12201\r\n"]}, {"number": 12179, "title": "Adding support for Big Endian in graph_constructor_test and wav_io", "body": "PR to add support for Big Endian for resolving below failures:\r\n\r\n1. //tensorflow/core:graph_graph_constructor_test\r\n2. //tensorflow/core/kernels:decode_wav_op_test\r\n3. //tensorflow/core:lib_wav_wav_io_test\r\n4. //tensorflow/core/kernels:encode_wav_op_test\r\n\r\nThe `tensor_content` in `graph_constructor_test` needs to be corrected for big endian.\r\nAlso the `ReadValue` in `wav_io.cc` needs to be shifted other way for big endian.\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12178, "title": "ValueError: At least one of the merge inputs is None", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, it is my code that does not work as expected.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.1 LTS\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource.\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.1-0-gb4957ff', '1.2.1')\r\n\r\n- **Python version**:\r\n2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n- **CUDA/cuDNN version**:\r\n5\r\n\r\n- **GPU model and memory**:\r\nGTX 1080, 8GB.\r\n\r\n- **Exact command to reproduce**:\r\nSee below.\r\n\r\n### Describe the problem\r\n\r\nThe code below crashes with the error shown below, although I see no reason why it should not work fine.\r\n\r\n### Source code / logs\r\n```\r\n$ cat test.py\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n    values = tf.get_variable(name='foo', shape=[13])\r\n    some_values = tf.boolean_mask(values, tf.cast(values, tf.bool))\r\n    \r\n    k = tf.minimum(42, tf.shape(some_values)[0])\r\n    \r\n    top_values = tf.cond(\r\n        k > 0,\r\n        lambda: tf.nn.top_k(some_values, k=k).values,\r\n        lambda: tf.zeros(shape=[0])\r\n    )\r\n\r\n    loss = tf.reduce_sum(top_values)\r\n    \r\n    optimizer = tf.train.MomentumOptimizer(1e-3, 0.9)\r\n    train_op = tf.contrib.training.create_train_op(loss, optimizer)\r\n\r\n$ python test.py\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    train_op = tf.contrib.training.create_train_op(loss, optimizer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/training/python/training/training.py\", line 439, in create_train_op\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 346, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_grad.py\", line 75, in _SwitchGrad\r\n    return merge([good_grad, zero_grad], name=\"cond_grad\")[0], None\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 396, in merge\r\n    raise ValueError(\"At least one of the merge inputs is None: %s\" % inputs)\r\nValueError: At least one of the merge inputs is None: [<tf.Tensor 'gradients/cond/TopKV2_grad/tuple/control_dependency_1:0' shape=() dtype=int32>, None]\r\n```", "comments": ["you should initialize the variable before `sess.run`. Add the line blow to your code.\r\n`sess.run(tf.global_variables_initializer())`", "On Tue, Aug 15, 2017 at 06:26:04AM +0000, nolan liu wrote:\n> you should initialize the variable before `sess.run`. Add the line\n> blow to your code.  `sess.run(tf.global_variables_initializer())`\n\nI do not do session.run(). I even do not have a session yet. It is the\ngraph construction that fails.\n\n-- \nYegor Derevenets\n", "v1.3 is ok for your code.", "> v1.3 is ok for your code.\r\n\r\nConfirmed. The bug seems to be already fixed in v1.3.0-rc2 and 1d33a59a9d554be863cfb06e9aa0ce1fa33ce9e6.\r\nThanks!\r\n"]}, {"number": 12177, "title": "InvalidArgumentError in tensorflow reduce_sum gradient when compiling from sources", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.3.0rc2\r\n- **Python version**: \r\n3.5.3\r\n- **Bazel version (if compiling from source)**:\r\ncmake 3.9.0\r\n- **CUDA/cuDNN version**:\r\ncpu only\r\n- **GPU model and memory**:\r\ncpu only\r\n- **Exact command to reproduce**:\r\n\r\nI compiled tensorflow from source according to the instructions given here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\nI activated support for AVX (that's the reason I want to compile from source). All steps work without problems. I then install the whl with pip in my python environment.\r\n\r\nNow, if I run my code with that version of tf, it crashes, but it works if I use a pre-compiled version.\r\n\r\n### Describe the problem\r\nA script that runs without problems when I use a pre-built version of tensorflow throws an error when I use the compiled version (see below for trace). I also compiled r1.2 and saw the same problem. Since the code works with the pre-built tf version (1.1), I suspect the problem is related to compiling from source.\r\n\r\n### Source code / logs\r\nError when running with the compiled tf:\r\n\r\n```\r\n  File \"test_network.py\", line 35, in <module>\r\n    network = Network(**configuration)\r\n[elided 0 identical lines from previous traceback]\r\n  File \"C:\\Users\\falke\\EclipseWorkspace\\nn\\src\\codebase\\network.py\", line 194, in __init__\r\n    self._build_graph()\r\n  File \"C:\\Users\\falke\\EclipseWorkspace\\nn\\src\\codebase\\network.py\", line 227, in _build_graph\r\n    self.graph_states = gop(self.graph_states, enc_state)\r\n  File \"C:\\Users\\falke\\EclipseWorkspace\\nn\\src\\codebase\\network.py\", line 158, in __call__\r\n    w = tf.exp(tf.reduce_sum(tf.multiply(node_labels, enc_state), axis=2, keep_dims=True))\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1279, in reduce_sum\r\n    name=name)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2727, in _sum\r\n    keep_dims=keep_dims, name=name)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 328, in apply_op\r\n    op_type_name, name, **keywords)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2624, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1210, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): indices[1] is out of range\r\n         [[Node: opt/gradients/node_label_update_1/Sum_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](opt/gradients/node_label_update_1/Sum_grad/range, opt/gradients/node_label_update_1/Sum_grad/mod, opt/gradients/node_label_update_1/Sum_grad/Shape, opt/gradients/node_label_update_1/Sum_grad/Fill)]]\r\n```\r\n\r\nThe code causing the error is:\r\n`w = tf.exp(tf.reduce_sum(tf.multiply(node_labels, enc_state), axis=2, keep_dims=True))`\r\nBoth `node_labels` and `enc_state` are 40x5x30 dim tensors. And as I already said, there are no issues at all running this code if I don't compile from source.", "comments": ["To help narrow down this issue, have you tried compiling without AVX support and seen if that works as expected?", "I just tried it, I don't see that error then, but get OutOfMemory errors instead (it looks like they happen at a later point, but I can't say that for sure).\r\n\r\n`2017-08-14 23:00:34.852292: W C:\\tf\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[189918321]` (occurs many times)", "Hm, I wonder if the OOM error is somehow happening with your AVX code but manifesting in a weird way. Are you able to provide a script we can use to repro the problem?", "The following code is enough to produce the described issue on my machine:\r\n\r\n```\r\nimport numpy as np\r\n\r\nx = tf.placeholder(tf.float32, [2,3])\r\ny = tf.placeholder(tf.float32, [2,3])\r\nW = tf.Variable(tf.random_normal([3,3], stddev=0.01))\r\n\r\nz = tf.matmul(x, W)\r\n# works when removing the reduce_sum op\r\nz = tf.reduce_sum(tf.multiply(z, y), axis=1, keep_dims=True)\r\n\r\noptimizer = tf.train.AdamOptimizer()\r\ntrain_op = optimizer.minimize(z)\r\n\r\nXdata = np.array([[1,2,3],[4,5,6]])\r\nYdata = np.array([[2,3,4],[3,4,5]])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(train_op, feed_dict={ x: Xdata, y: Ydata })\r\n    print('done')\r\n```\r\n\r\nWith TF 1.3.0rc2, compiled from source, with AVX support, I get the described error:\r\n\r\n```\r\n>python mwe.py\r\n2017-08-15 07:14:19.860434: W C:\\tf\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1300, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[1] is out of range\r\n         [[Node: gradients/Sum_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/Sum_grad/range, gradients/Sum_grad/mod, gradients/Sum_grad/Shape, gradients/Sum_grad/Fill)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mwe.py\", line 23, in <module>\r\n    sess.run(train_op, feed_dict={ x: X, y: Y })\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1118, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1315, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[1] is out of range\r\n         [[Node: gradients/Sum_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/Sum_grad/range, gradients/Sum_grad/mod, gradients/Sum_grad/Shape, gradients/Sum_grad/Fill)]]\r\n\r\nCaused by op 'gradients/Sum_grad/DynamicStitch', defined at:\r\n  File \"mwe.py\", line 15, in <module>\r\n    train_op = optimizer.minimize(z)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 336, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 407, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 542, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 348, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 542, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 56, in _SumGrad\r\n    output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2318, in reduced_shape\r\n    array_ops.fill(axes_shape, 1)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 480, in dynamic_stitch\r\n    name=name)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 328, in apply_op\r\n    op_type_name, name, **keywords)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2624, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1210, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'Sum', defined at:\r\n  File \"mwe.py\", line 12, in <module>\r\n    z = tf.reduce_sum(tf.multiply(z, y), axis=1, keep_dims=True)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1279, in reduce_sum\r\n    name=name)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2727, in _sum\r\n    keep_dims=keep_dims, name=name)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 328, in apply_op\r\n    op_type_name, name, **keywords)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2624, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Anaconda\\envs\\tfc\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1210, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): indices[1] is out of range\r\n         [[Node: gradients/Sum_grad/DynamicStitch = DynamicStitch[N=2, T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/Sum_grad/range, gradients/Sum_grad/mod, gradients/Sum_grad/Shape, gradients/Sum_grad/Fill)]]\r\n```\r\n\r\nWith TF 1.3.0rc2, compiled from source, _without_ AVX support, I get an error related to dimensionality (but not the OOM I saw running the full code):\r\n\r\n```\r\n>python mwe.py\r\n2017-08-15 07:21:07.349599: W C:\\tf\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:21:07.349726: W C:\\tf\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:21:29.352682: F C:\\tf\\tensorflow\\tensorflow\\core\\framework\\tensor_shape.cc:243] Check failed: ndims_byte() < MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor\r\n```\r\n\r\nWith TF 1.1.0, pre-compiled, it runs just fine:\r\n\r\n```\r\n>python mwe.py\r\n2017-08-15 07:22:31.147908: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.148040: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.149626: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.149752: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.149881: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.150000: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.150127: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-15 07:22:31.150252: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\ndone\r\n```\r\n\r\n", "Could you provide the exact configure command and a reproducible self contained test example?", "I compile with the following cmake command:\r\n```\r\n> cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\nMore? -DSWIG_EXECUTABLE=C:/tf/swigwin-3.0.12/swig.exe ^\r\nMore? -DPYTHON_EXECUTABLE=C:/Anaconda/envs/tfc/python.exe ^\r\nMore? -DPYTHON_LIBRARIES=C:/Anaconda/envs/tfc/libs/python35.lib ^\r\nMore? -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n```\r\nA reproducible self-contained test example is the code I provided at the beginning of my last post.\r\n", "@mrry, can you take a look at this?", "Nothing obvious springs to mind. There might be some issue in one of the Eigen kernels we use when building with MSVC.\r\n\r\nCan you narrow the reproduction down further? In particular, it would be useful to know which op produces a different *output* in the two configurations. For example, to get the immediate inputs of the failing op, you could run the following code in both versions:\r\n\r\n```python\r\nop = tf.get_default_graph().get_operation_by_name(\"opt/gradients/node_label_update_1/Sum_grad/DynamicStitch\")\r\n\r\nfor input_ in op.inputs:\r\n  print(input, sess.run(input_, feed_dict={x: Xdata, y: Ydata}))\r\n```", "Sorry, but I don't have the environment around anymore, so I cannot make more tests. I just accepted that the compiled version didn't work at some point. Anyway, thanks for looking at it again!"]}, {"number": 12176, "title": "remove unused parameter", "body": "remove unused parameter `pool` for `DirectSession::GetOrCreateExecutors`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12175, "title": "[OpenCL] Registers AvgPool and AvgPoolGrad (#105)", "body": "", "comments": ["Can one of the admins verify this patch?", "@benoitsteiner any cycles to take a look?", "@benoitsteiner ping", "Pong", "@benoitsteiner any cycles to take a look?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @benoitsteiner: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @benoitsteiner: It has been 16 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @benoitsteiner: It has been 150 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @benoitsteiner: It has been 165 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @benoitsteiner: It has been 180 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @benoitsteiner: It has been 195 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @benoitsteiner: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 210 days with no activity and the `awaiting review` label has been applied.", "Nagging Reviewer @benoitsteiner: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 225 days with no activity and the `awaiting review` label has been applied.", "I believe this PR is outdated.\r\n\r\n@Rbiessy will replace it with smaller one when it is ready."]}, {"number": 12174, "title": "R1.3", "body": "Missing **IndicesRowIterator::operator<()** is added.\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 12173, "title": "[OpenCL] Fixes double memcpy bug (#151)", "body": "As the debg CopyOp is called on a Tensor without type, we need to use\r\nthe DataType enum to get type information, and use this to pass the type\r\non to Eigen. This is a workaround Eigen's need to have a type when\r\ncalling memcpy. If the Eigen memcpy can be provided without a type\r\nrequirement, then the memcpy in sycl_util is unnecessary.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12172, "title": "[OpenCL] Registers BytesLimit Op", "body": "", "comments": ["@lukeiwanski, thanks for your PR! By analyzing the history of the files in this pull request, we identified @wujingyue to be a potential reviewer.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12171, "title": "Allow disabling of selected tests in vector ops", "body": "Swap over the test instantiate macro to allow selective disabling of these tests\r\n", "comments": ["@DavidNorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @eliben and @tensorflower-gardener to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12170, "title": "Fix unnecessary sync_memops cuda pointer attr in GDR", "body": "See the comments [here](https://github.com/tensorflow/tensorflow/pull/11392#discussion_r132026389). I've tested on my local boxes and it seems removing `CU_POINTER_ATTRIBUTE_SYNC_MEMOPS` slightly improves the training throughput while introducing no data race.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Test failure is unrelated."]}, {"number": 12169, "title": "Request for Tile operation for integer types", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0 / 5\r\n- **GPU model and memory**: TitanX 12G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI need some matrix indices computation using tf.tile function. But now it is only available for float types.\r\nSo I have to cast int into float and after computation back into int. It would be great to have \"tile\" function working with tensors with dtype of integer types.  \r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    tt = tf.tile(tf.range(4), [3])\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tt))\r\n\r\n```\r\n```\r\nCaused by op u'Tile', defined at:\r\n  File \"test_tf3.py\", line 6, in <module>\r\n    tt = tf.tile(tf.range(4), [3])\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3847, in tile\r\n    name=name)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/media/awork/home/astepochkin/drecs/repo/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Tile': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n     [[Node: Tile = Tile[T=DT_INT32, Tmultiples=DT_INT32, _device=\"/device:GPU:0\"](range, Tile/multiples)]]\r\n```", "comments": ["Added a PR #12183 for the fix."]}, {"number": 12168, "title": "*mlocate* should be denpendency", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n  docker centos 7\r\n- **TensorFlow installed from (source or binary)**:\r\n  source\r\n- **TensorFlow version (use command below)**:\r\n  r1.2\r\n- **Python version**: \r\n  2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n  0.5.3\r\n- **CUDA/cuDNN version**:\r\n  none\r\n- **GPU model and memory**:\r\n  none\r\n- **Exact command to reproduce**:\r\n\r\n    ./configure\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\n    `mlocate` command should be *claimed* as a dependency in your installation instruction\r\n", "comments": ["It looks to be added with this pull request.\r\nhttps://github.com/tensorflow/tensorflow/pull/9641\r\n\r\n@vrv @andydavis1 @zhangyaobit Could you comment on our dependence on mlocate?\r\nIf it is a hard dependency, I agree that we should document it.", "@vivek-rane Can you comment on \"mlocate\"?", "We don't need mlocate any more. The configure and build scripts have changed since that commit to not use mlocate.", "Thanks for the clarification @vivek-rane \r\n@cinqs is there a reason you believe we need `mlocate` as a dependency?", "@gunan , Hi, I don't have specific reason why `mlocate` is a must. I filed this issue only because when I compiling your source, I ran across the situation that `mlocate` is a must of dependency.\r\n\r\nSince I was compiling from `r1.2`, which in fact is the version your official API document supporting then. \r\nAnd, I was compiling the source inside a docker container. The image is `CentOS 7`\r\n\r\nIn conclusion, no particular reason. And sorry for the late response, was quite a busy summer."]}, {"number": 12167, "title": "TFRecord usage", "body": "### Describe the problem\r\nCan I get meta data of record in TFRecord file using tf queue pipeline, which type is an int or np.ndarry without a Tensor?\r\n\r\nSo I can use this meta data to tf.shuffle_batch example, and I do not want use a another Sesson to fecth from Tensors.\r\n\r\nMay this can be a Feature for tf.\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12166, "title": "build android ", "body": "ERROR: /home/wangmeng/RSTensorFlow_mobile/tensorflow/examples/android/BUILD:67:1: Building tensorflow/examples/android/libtensorflow_demo.jar (23 source files) failed: Worker process sent response with exit code: 1.\r\ntensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:365: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;\r\n    inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILE);\r\n                         ^\r\n  required: no arguments\r\n  found: AssetManager,String\r\n  reason: actual and formal argument lists differ in length\r\ntensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:544: error: cannot find symbol\r\n    inferenceInterface.feed(\r\n                      ^\r\n  symbol:   method feed(String,float[],int,int,int,int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:546: error: cannot find symbol\r\n    inferenceInterface.feed(STYLE_NODE, styleVals, NUM_STYLES);\r\n                      ^\r\n  symbol:   method feed(String,float[],int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:548: error: cannot find symbol\r\n    inferenceInterface.run(new String[] {OUTPUT_NODE}, isDebug());\r\n                      ^\r\n  symbol:   method run(String[],boolean)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/StylizeActivity.java:549: error: cannot find symbol\r\n    inferenceInterface.fetch(OUTPUT_NODE, floatValues);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:90: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;\r\n    c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                           ^\r\n  required: no arguments\r\n  found: AssetManager,String\r\n  reason: actual and formal argument lists differ in length\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:93: error: cannot find symbol\r\n    final Operation operation = c.inferenceInterface.graphOperation(outputName);\r\n                                                    ^\r\n  symbol:   method graphOperation(String)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:132: error: cannot find symbol\r\n    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);\r\n                      ^\r\n  symbol:   method feed(String,float[],int,int,int,int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:137: error: cannot find symbol\r\n    inferenceInterface.run(outputNames, logStats);\r\n                      ^\r\n  symbol:   method run(String[],boolean)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java:142: error: cannot find symbol\r\n    inferenceInterface.fetch(outputName, outputs);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:90: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;\r\n    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                           ^\r\n  required: no arguments\r\n  found: AssetManager,String\r\n  reason: actual and formal argument lists differ in length\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:221: error: cannot find symbol\r\n    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);\r\n                      ^\r\n  symbol:   method feed(String,float[],int,int,int,int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:226: error: cannot find symbol\r\n    inferenceInterface.run(outputNames, logStats);\r\n                      ^\r\n  symbol:   method run(String[],boolean)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:233: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[0], outputLocationsEncoding);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowMultiBoxDetector.java:234: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[1], outputScoresEncoding);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:68: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;\r\n    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                           ^\r\n  required: no arguments\r\n  found: AssetManager,String\r\n  reason: actual and formal argument lists differ in length\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:98: error: cannot find symbol\r\n    inferenceInterface.feed(inputName, bytePixels, 1, inputSize, inputSize, 3);\r\n                      ^\r\n  symbol:   method feed(String,byte[],int,int,int,int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:105: error: cannot find symbol\r\n    inferenceInterface.run(outputNames, logStats);\r\n                      ^\r\n  symbol:   method run(String[],boolean)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:114: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[3], numDetectionsArray);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:119: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[0], boxes);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:120: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[1], scores);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowSingleShotDetector.java:121: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[2], classes);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:107: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;\r\n    d.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                           ^\r\n  required: no arguments\r\n  found: AssetManager,String\r\n  reason: actual and formal argument lists differ in length\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:154: error: cannot find symbol\r\n    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);\r\n                      ^\r\n  symbol:   method feed(String,float[],int,int,int,int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:161: error: cannot find symbol\r\n    inferenceInterface.run(outputNames, logStats);\r\n                      ^\r\n  symbol:   method run(String[],boolean)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\ntensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java:172: error: cannot find symbol\r\n    inferenceInterface.fetch(outputNames[0], output);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 19.243s, Critical Path: 7.19s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Please add the info requested for the bug report so we can have some context how you got this, thanks.", "I use bazel 0.5.3, but the branch requires 0.4.2", "Please fill out the entire [Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).", "Closing due to lack of debugging information.\r\nIf you are still seeing this problem, please fill out all of the fields in the template we provide for new issues.", "@gunan I believe I'm having the same problem. here's my issue report, could this get reopened?\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, but the error occurs in the example `TensorFlowImageClassifier.java` file\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: android library binary (only using tensorflow on android, not in python)\r\n- **TensorFlow version (use command below)**: n/a\r\n- **Python version**: n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nthe problem appears to be in the `TensorFlowInferenceInterface` interface.\r\n\r\nerror log: \r\n```\r\n/home/ta/git/MLtest/node_modules/react-native-machine-learning/android/src/main/java/com/reactlibrary/TensorFlowImageClassifier.java:97: error: constructor TensorFlowInferenceInterface in class TensorFlowInferenceInterface cannot be applied to given types;\r\n    c.inferenceInterface = new TensorFlowInferenceInterface(assetManager, modelFilename);\r\n                           ^\r\n  required: no arguments\r\n  found: AssetManager,String\r\n  reason: actual and formal argument lists differ in length\r\n```\r\n\r\ncode in question: see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java) line 103 for the constructor in question.\r\n\r\nI can get past this by replacing the line with this code: \r\n\r\n```\r\nc.inferenceInterface = new TensorFlowInferenceInterface();\r\nc.inferenceInterface.initializeTensorFlow(assetManager, modelFilename);\r\n```\r\n\r\nbut then I'm left with these errors, which makes me think that something else is causing these problems:\r\n\r\n```\r\n/home/ta/git/MLtest/node_modules/react-native-machine-learning/android/src/main/java/com/reactlibrary/TensorFlowImageClassifier.java:101: error: cannot find symbol\r\n    final Operation operation = c.inferenceInterface.graphOperation(outputName);\r\n                                                    ^\r\n  symbol:   method graphOperation(String)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\n/home/ta/git/MLtest/node_modules/react-native-machine-learning/android/src/main/java/com/reactlibrary/TensorFlowImageClassifier.java:140: error: cannot find symbol\r\n    inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);\r\n                      ^\r\n  symbol:   method feed(String,float[],int,int,int,int)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\n/home/ta/git/MLtest/node_modules/react-native-machine-learning/android/src/main/java/com/reactlibrary/TensorFlowImageClassifier.java:145: error: cannot find symbol\r\n    inferenceInterface.run(outputNames, logStats);\r\n                      ^\r\n  symbol:   method run(String[],boolean)\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\n/home/ta/git/MLtest/node_modules/react-native-machine-learning/android/src/main/java/com/reactlibrary/TensorFlowImageClassifier.java:150: error: cannot find symbol\r\n    inferenceInterface.fetch(outputName, outputs);\r\n                      ^\r\n  symbol:   method fetch(String,float[])\r\n  location: variable inferenceInterface of type TensorFlowInferenceInterface\r\n4 errors\r\n\r\n```\r\n\r\n### Source code / logs\r\nI can include more code if necessary, but I'm attempting to use tensorflow in a react-native project so it might be difficult to provide a bare minimum example.", "@Talor-A thanks for the information.\r\nBug reopened.", "update: I noticed the `TensorFlowInterferenceInterface` class [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java) has a constructor that takes AssetManager and ModelFile as arguments, is it possible I'm using an outdated version?", "@taylor-a Yes, a version mismatch is the most likely culprit. Are you using a hardcoded version of the AAR (specified in build.gradle)?", "@andrewharp no, I manually added `libandroid_tensorflow_inference_java.jar` and the other `.so` files to my `libs/` folder, and this to `build.gradle`:\r\n```\r\nandroid {\r\n    ...\r\n    sourceSets {\r\n        main {\r\n            jniLibs.srcDirs = ['libs']\r\n        }\r\n    }\r\n}\r\ndependencies {\r\n    compile fileTree(dir: \"libs\", include: [\"*.jar\"])\r\n    compile 'com.facebook.react:react-native:+'\r\n}\r\n```\r\nIs there a way I can install the library automatically in build.gradle?", "Ok, it sounds like you grabbed an older version of libandroid_tensorflow_inference_java.jar than the demo code you're compiling against. Where did you find the file?\r\n\r\nTo use the AAR you can add this to your gradle config:\r\n```\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile 'org.tensorflow:tensorflow-android:+'\r\n}\r\n```\r\n\r\nYou can replace the + with a known compatible version number of TensorFlow to prevent future changes from causing issues (though we try to keep the Android releases backwards compatible). ", "thanks, that solved it @andrewharp . "]}, {"number": 12165, "title": "Tiny error about set CLASSPATH  in Deploy Doc", "body": "In https://www.tensorflow.org/deploy/hadoop this page,\r\nWhen introduce to set CLASSPATH,\r\n`shell CLASSPATH=$($HADOOP_HDFS_HOME}/bin/hadoop classpath --glob)`\r\nThere is an extra parenthesis after **$HADOOP_HDFS_HOME**", "comments": ["See #10293. I think the issue has been fixed in the master branch.", "Thanks for filing the issue @weberxie , and thanks for pointing out the PR @yongtang !\r\n\r\nClosing this out."]}, {"number": 12164, "title": "Cannot include '*.pb.h' files in tf_tutorials.cmake", "body": "**System information**\r\n\r\nWindows10\r\nVisualStudio 2015\r\nTensorFlow 1.3.0\r\nPython 3.5.3\r\nCMake 3.9.0\r\n\r\nI am generating C++ tensorflow 'GPU' version of _tf_tutorials_example_trainer_ example as defined in (https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake) using Cmake and MSbuild. \r\n\r\n**Describe the problem**\r\n\r\nBuild failing due to missing header files ../contrib/boosted_trees/proto/*.ph.h. \r\n\r\nThe *.proto files are present, and I can use protoc.exe to manually generate the *.ph.h files.  This removes the 'cannot include' errors.  BUT now there are linker errors when building the tf_tutorials_example_trainer.exe as it can't find the routines/structures defined in the  *.pb.h files.\r\n\r\nNOTE: *.proto files in other directories are also not expanded to *.pb.h equivalents.\r\n\r\n**Source code / logs**\r\n\r\n\"c:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\" (default target) (1) ->\r\n\"C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj\" (default target) (104) ->\r\n(ClCompile target) -> \r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/trees/decision_tree.h(19): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\learner\\common\\partitioners\\example_partitioner.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\learner\\stochastic\\handlers\\categorical-feature-column-handler.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\learner\\stochastic\\handlers\\bias-feature-column-handler.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\learner\\stochastic\\handlers\\dense-quantized-feature-column-handler.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/utils/dropout_utils.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\dropout_utils.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/learner/stochastic/stats/node-stats.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/learner.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\learner\\stochastic\\handlers\\sparse-quantized-feature-column-handler.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/trees/decision_tree.h(19): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\trees\\decision_tree.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n  C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow/contrib/boosted_trees/lib/models/multiple_additive_trees.h(21): fatal error C1083: Cannot open include file: 'tensorflow/contrib/boosted_trees/proto/tree_config.pb.h': No such file or directory (compiling source file C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\models\\multiple_additive_trees.cc) [C:\\Users\\Martin Rosevear\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n\r\n", "comments": ["@mrry can you take a look?"]}, {"number": 12163, "title": "tf.cast Illegal instruction", "body": "```\r\nimport tensorflow as tf\r\na = [0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]\r\n\r\ntf.cast(a, tf.int32)\r\n```\r\noutput:\r\n Illegal instruction\r\n\r\n```\r\nimport tensorflow as tf\r\na = [0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]\r\n\r\ntf.cast(a, tf.int32)\r\n```\r\nthis's ok.\r\n\r\nMy System Info:\r\n centos7 Linux bw-dev-xxxx-v01 3.10.0-123.el7.x86_64 #1 SMP Mon Jun 30 12:09:22 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\r\n Python 3.6.2\r\n```\r\nPython 3.6.2 (default, Aug  3 2017, 14:43:36)\r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.2.1'\r\n>>>\r\n```\r\n\r\nwhy? please help!\r\n", "comments": ["This sounds like being killed by SIGILL. That happens when binary is compiled for some architecture that your machine doesn't support. Are you using official binary release? What is the CPU you are running?", "CPU Info:\r\n```\r\nprocessor\t: 15\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 63\r\nmodel name\t: Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz\r\nstepping\t: 2\r\nmicrocode\t: 0x38\r\ncpu MHz\t\t: 2400.060\r\ncache size\t: 15360 KB\r\nphysical id\t: 15\r\nsiblings\t: 1\r\ncore id\t\t: 0\r\ncpu cores\t: 1\r\napicid\t\t: 30\r\ninitial apicid\t: 30\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes f16c rdrand hypervisor lahf_lm abm fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid\r\nbogomips\t: 4820.14\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 46 bits physical, 48 bits virtual\r\npower management:\r\n```\r\n\r\ntensorflow install:\r\n```\r\npip install -U tensorflow\r\n```", "Hm, I have access to almost identical CPU (only the microcode is a bit older) and can't reproduce this. Also, your example is strange -- in your code TF runtime is never utilized, only Python wrapper functions are called. This means the illegal instruction would be caused by the Python interpreter, or numpy's `asarray` function rather than TensorFlow. Could it be that your version of numpy is built for wrong architecture?\r\n\r\n```\r\n>>> a = [0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]\r\n>>> \r\n>>> tf.cast(a, tf.int32)\r\n<tf.Tensor 'Cast/x:0' shape=(32,) dtype=int32>\r\n>>> sess=tf.Session()\r\n2017-08-09 21:17:09.678711: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-09 21:17:09.678746: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-09 21:17:09.678764: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-09 21:17:09.678779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-09 21:17:09.678793: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n>>> sess.run(tf.cast(a, tf.int32))\r\narray([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\r\n       1, 1, 0, 0, 1, 1, 1, 0, 1], dtype=int32)\r\n>>> \r\n>>> tf.__version__\r\n'1.2.1'\r\n\r\n```\r\n\r\n```\r\nprocessor\t: 31\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 63\r\nmodel name\t: Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz\r\nstepping\t: 2\r\nmicrocode\t: 0x36\r\ncpu MHz\t\t: 1290.750\r\ncache size\t: 20480 KB\r\nphysical id\t: 1\r\nsiblings\t: 16\r\ncore id\t\t: 7\r\ncpu cores\t: 8\r\napicid\t\t: 31\r\ninitial apicid\t: 31\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 15\r\nwp\t\t: yes\r\n\r\n```", "the code core dump at tensorflow/python/framework/tensor_util.py\r\n```python\r\n      print(\"kkk\",downcasted_array, nparray)\r\n      if np.array_equal(downcasted_array, nparray):\r\n        print(\"lll\")\r\n```\r\n\r\nrun log:\r\n```python\r\nkkk [0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1] [0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1]\r\nIllegal instruction\r\n```\r\n\r\nnumpy version:\r\n```\r\n>>> import numpy as np\r\n>>> np.__version__\r\n'1.13.1'\r\n```", "@yaroslavvb thank you for you help. i find the error at numpy package.\r\n```python\r\nPython 3.6.2 (default, Aug  3 2017, 14:43:36)\r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> a=[0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]\r\n>>> b=[0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1]\r\n>>> np.array_equal(a,b)\r\nIllegal instruction\r\nYou have new mail in /var/spool/mail/root\r\n```"]}, {"number": 12161, "title": "Branch 164787644", "body": "", "comments": ["@rmlarsen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @meheffernan, @tensorflower-gardener and @jart to be potential reviewers."]}, {"number": 12160, "title": "Branch 164782742", "body": "", "comments": []}, {"number": 12159, "title": "TensorFlow Serving - error in documentation", "body": "https://www.tensorflow.org/serving/setup\r\n\r\nIt seems that TensorFlow serving installation guide has incorrect python package name for API:\r\n```\r\n$\ue0b0  pip install tensorflow-serving-api\r\nCollecting tensorflow-serving-api\r\n  Could not find a version that satisfies the requirement tensorflow-serving-api (from versions: )\r\nNo matching distribution found for tensorflow-serving-api\r\n\r\n$ pip install tensorflow-serving-client\r\nCollecting tensorflow-serving-client\r\n  Downloading tensorflow_serving_client-0.0.6-py2.py3-none-any.whl\r\nCollecting Pillow (from tensorflow-serving-client)\r\n  Downloading Pillow-4.2.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.5MB 226kB/s\r\nCollecting grpcio-tools (from tensorflow-serving-client)\r\n  Downloading grpcio_tools-1.4.0-cp36-cp36m-macosx_10_9_intel.whl (3.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.3MB 208kB/s\r\nRequirement already satisfied: tensorflow in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow-serving-client)\r\nCollecting grpcio (from tensorflow-serving-client)\r\n  Downloading grpcio-1.4.0-cp36-cp36m-macosx_10_7_intel.whl (1.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5MB 408kB/s\r\nCollecting olefile (from Pillow->tensorflow-serving-client)\r\nRequirement already satisfied: protobuf>=3.3.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from grpcio-tools->tensorflow-serving-client)\r\nRequirement already satisfied: markdown>=2.6.8 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: six>=1.10.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: html5lib==0.9999999 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: wheel>=0.26 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: bleach==1.5.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: numpy>=1.11.0 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: werkzeug>=0.11.10 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: backports.weakref==1.0rc1 in ./miniconda3/envs/rtb/lib/python3.6/site-packages (from tensorflow->tensorflow-serving-client)\r\nRequirement already satisfied: setuptools in ./miniconda3/envs/rtb/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg (from protobuf>=3.3.0->grpcio-tools->tensorflow-serving-client)\r\nInstalling collected packages: olefile, Pillow, grpcio, grpcio-tools, tensorflow-serving-client\r\nSuccessfully installed Pillow-4.2.1 grpcio-1.4.0 grpcio-tools-1.4.0 olefile-0.44 tensorflow-serving-client-0.0.6\r\n```\r\n\r\nSo, `tensorflow-serving-api` should be changed to `tensorflow-serving-client`", "comments": ["I think the package only exists for python 2.\r\n\r\n`conda env create` with a `environment.yml` like this:\r\n\r\n```\r\nname: serving\r\ndependencies:\r\n- python=2\r\n- numpy\r\n- pip\r\n- pip:\r\n  - tensorflow\r\n  - tensorflow-serving-api\r\n```", "`tensorflow-serving-api` is the right package name to use, [tensorflow-serving-client](https://pypi.python.org/pypi/tensorflow-serving-client) is not affiliated with the project and is published by another company.\r\n\r\nThe package indeed only exists for python 2, so that's likely the reason it's not finding it.", "I see, thanks! This issue can be closed then", "I am trying to install Tensor-flow serving on Windows 10 64-bit machine, where tensor-flow installation can not happen with python 2, atleast python 3.5 is required.\r\n\r\nIs there any workaround?\r\n\r\nSources: Internet"]}, {"number": 12158, "title": "Is `full_matrices=False` working for `tf.svd()`?", "body": "I am working with tensorflow svd decomposition,  and I notice that the svd results do not have any differences for singular matrix no matter I set `full_matrices=False` or to be `True`. For example, \r\n\r\n```\r\na = tf.Variable([[1.0, 0.0, 0.0], [0.0, 4.0, 0.0], [0.0, 0.0, 0.0]])  # Singular Matrix\r\ns, u, v = tf.svd(a, full_matrices=False)\r\ninit_op = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nsess.run([s, u, v])\r\n```\r\n\r\nThe results are always same no matter `full_matrices=False` or `True`. However, the online doc says \"If true, compute full-sized u and v. If false (the default), compute only the leading P singular vectors\". So I wonder if there is a bug for this argument. Thanks!\r\n", "comments": ["I think it's not a bug.\r\n\r\nNote from helpstring: `Tensor` of shape `[..., M, N]`. Let `P` be the minimum of `M` and\r\n        `N`.\"\r\n\r\n`full_matrices` determines whether you want only leading `P` vectors in your result.\r\n\r\nThe result is different if you use non-square a\r\n`a = tf.Variable([[1.0, 0.0, 0.0]])  # Singular Matrix`", "Closing as this does not appear to be a bug. @RuofanKong reopen if you still believe it is a bug."]}, {"number": 12157, "title": "Bug - Restoring a graph created by tensorflow.python.tools.optimize_for_inference has errors with RNN models", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes, below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu (1.1.0)\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n- **GPU model and memory**: Tesla K80 24GB\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nI believe I've found a bug. The freeze and optimize scripts appear to have bugs related to the proper function of RNNs. Creating a simple RNN, running the freeze script and the optimize script, and then attempting to restore and use the optimized graph creates a puzzling series of errors.\r\n\r\n1. After running freeze and optimize, the placeholder for sequence lengths has datatype `float32`, instead of `tf.int32`, even though the placeholder specifies that it is `tf.int32`. This breaks evaluating an instance of GRUCell, which expects length to be of type `tf.int32`.\r\n\r\n2. If I add a `tf.to_int32()` to coerce the sequence length placeholder to type `tf.int32`, then we obtain a different error, which appears to pertain to the internal operation of the `tf.nn.dynamic_rnn()` function.\r\n\r\n### Source code / logs\r\nThe code is divided into 2 user-created scripts (and 2 TF-provided scripts are employed along the way). The first of my scripts defines a model and saves it, and is called \"optimize_graph_minimal.py\". Then the freeze and optimize scripts are run. The second of my scripts attempts to restore the model to a new Python session, and this appears to be buggy.\r\n\r\n#### This is the code I used to create and save the graph.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.rnn as rnn\r\n\r\nclass tf_rnn_model(object):\r\n    def __init__(self, seq_length=10, num_units=2):\r\n        self.seq_length = seq_length\r\n        self.num_units = num_units\r\n        self.graph_context = tf.Graph()\r\n        self.graph_specification()\r\n\r\n    def graph_specification(self):\r\n        with self.graph_context.as_default():\r\n            self.X = tf.placeholder(dtype=tf.float32, shape=[None, self.seq_length, 2], name=\"X\")\r\n            self.X_length = tf.placeholder(dtype=tf.int32, shape=[None], name=\"X_length\")\r\n\r\n            cell = rnn.GRUCell(self.num_units)\r\n            Y, rnn_state = tf.nn.dynamic_rnn(cell=cell,\r\n                                             inputs=self.X,\r\n                                             sequence_length=self.X_length,\r\n                                             dtype=tf.float32,\r\n                                             swap_memory=False)\r\n\r\n            self.Y = tf.identity(Y, name=\"Y\")\r\n            self.saver = tf.train.Saver()\r\n\r\n        return None\r\n\r\n    def restore_optimized_graph(self, graph_def_optimized):\r\n        with tf.gfile.GFile(graph_def_optimized, 'rb') as f:\r\n            graph_def_optimized = tf.GraphDef()\r\n            graph_def_optimized.ParseFromString(f.read())\r\n\r\n        self.Y, = tf.import_graph_def(graph_def_optimized, return_elements=[\"Y:0\"])\r\n        self.X = self.graph_context.get_tensor_by_name(\"import/X:0\")\r\n        self.X_length = self.graph_context.get_tensor_by_name(\"import/X_length:0\")\r\n        tf.global_variables_initializer().run()\r\n\r\n        return None\r\n\r\nmodel = tf_rnn_model()\r\nwith tf.Session(graph=model.graph_context) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    inputs = np.arange(20).reshape([1, 10, 2])\r\n    out = sess.run(fetches=[model.Y], feed_dict={model.X: inputs, model.X_length: [10]})\r\n    print(out)\r\n\r\n    tf.train.write_graph(sess.graph_def, \".\", \"toy_graph.pb\")\r\n    model.saver.save(sess, save_path=\"toy_saved\")\r\n\r\n    print(\"These are some helpful things to know for the script.\")\r\n    print(\"saver.as_saver_def()= %s\" % model.saver.as_saver_def())\r\n```\r\n\r\n## Freeze and optimize scripts are executed here.\r\n\r\n```\r\npython -m tensorflow.python.tools.freeze_graph \\\r\n--input_graph toy_graph.pb \\\r\n--input_checkpoint toy_saved \\\r\n--output_graph graph_frozen.pb \\\r\n--output_node_names=Y \\\r\n--filename_tensor_name=save/Const:0 \\\r\n--restore_op_name=save/restore_all\r\n\r\npython -m tensorflow.python.tools.optimize_for_inference \\\r\n--input graph_frozen.pb \\\r\n--output graph_optimized.pb \\\r\n--input_names=X,X_length \\\r\n--output_names=Y\r\n```\r\n\r\n#### Attempt to restore and `run` using this script in a new Python session.\r\n\r\n```\r\n# This line just imports the model class from the previous Python script because this is a new Python session.\r\nfrom optimize_graph_minimal import tf_rnn_model\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf_rnn_model()\r\n\r\nwith tf.Session(graph=model.graph_context) as sess:\r\n    model.restore_optimized_graph(\"graph_optimized.pb\")\r\n    inputs = np.arange(20).reshape([1, 10, 2])\r\n    out = sess.run(fetches=[model.Y], feed_dict={model.X: inputs, model.X_length: [10]})\r\n    print(out)\r\n```\r\n#### The following errors are produced.\r\n\r\n1. Without explicitly coercing the sequence length placeholder using `tf.to_int32()`, we get an error indicating that the sequence length tensor is of type `float32` but must be type `int32`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_minimal/optimize_restore_graph.py\", line 14, in <module>\r\n    model.restore_optimized_graph(\"graph_optimized.pb\")\r\n  File \"optimize_graph_minimal.py\", line 44, in restore_optimized_graph\r\n    self.Y = tf.import_graph_def(graph_def_optimized, return_elements=[\"Y:0\"])\r\n  File \"python2.7/site-packages/tensorflow/python/framework/importer.py\", line 388, in import_graph_def\r\n    node, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node u'rnn/Shape_1': Input tensor 'X_length:0' Cannot convert a tensor of type float32 to an input of type int32.\r\n```\r\n\r\n2. If we change the graph specification to use an explicit coercion to int32 type\r\n`self.X_length = tf.to_int32(tf.placeholder(dtype=tf.int32, shape=[None], name=\"X_length\"))`\r\nthen we get this error instead.\r\n\r\n```\r\n2017-08-09 19:19:10.910686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)\r\nTraceback (most recent call last):\r\n  File \"optimize_restore_graph.py\", line 14, in <module>\r\n    model.restore_optimized_graph(\"graph_optimized.pb\")\r\n  File \"optimize_graph_minimal.py\", line 44, in restore_optimized_graph\r\n    self.Y = tf.import_graph_def(graph_def_optimized, return_elements=[\"Y:0\"])\r\n  File \"python2.7/site-packages/tensorflow/python/framework/importer.py\", line 362, in import_graph_def\r\n    % (input_name,)))\r\nValueError: graph_def is invalid at node u'rnn/while/gru_cell/gates/gates/concat/axis': More inputs specified ('rnn/while/Switch:1') than the op expects..\r\n```", "comments": ["We have the same error. \r\nValueError: graph_def is invalid at node u'rnn_model_20/bidirectional_rnn/fw/fw/while/fw/basic_lstm_cell/basic_lstm_cell/concat/axis':\r\nMore inputs specified ('rnn_model_20/bidirectional_rnn/fw/fw/while/Switch:1') than the op expects..\r\nHave you fixed it up?", "@Sycor4x, has anything changed since you filed this issue?\r\n\r\n@abrbrazj, can you provide additional information, as per [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new)?", "@angersson \r\nThe System Information is the same as Sycor4x.\r\nI want to move my model to the app on iPad, so I frozen and optimize my graph. But when I used the optimized model on C++ API of TnesorFlow, this issue occurred. I used `bidirectional_dynamic_rnn` in my graph, and this issue happened in the `whileloop` in` dynamic_rnn`.  And then I changed `bidirectional_dynamic_rnn` to `static_bidirectional_rnn`\uff0c it disappeared.  If I used the `graph_frozen.pb`, there is also no problem. So it might be a bug in `dynamic_rnn` and `optimize_for_inference`.", "I was able to recreate @Sycor4x's errors with the latest docker image (TF 1.4) from gcr.io/tensorflow/tensorflow, after changing `toy_saved` to `./toy_saved`. The first script had to be saved as `optimize_graph_minimal.py`.\r\n\r\n@ebrevdo, can you take a look at this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@angersson I have also confirmed this on Tensorflow 1.4.\r\n\r\nThe Error:\r\n```\r\nValueError: graph_def is invalid at node 'model/inference/Shape': Input tensor 'inputs:0' Cannot convert a tensor of type float32 to an input of type int32.\r\n```\r\n\r\nA non optimised only frozen graph works fine. Again with a RNN.", "@petewarden can you tal at this?  I don't have any experience with optimize_for_inference, but happy to consult on the RNN side.", "(can does optimize_for_inference handle while_loop?)", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I would be interested on how to perform optimize_for_inference.py on the ssd_mobilenet.\r\nI was using it like:\r\n```\r\npython -m tensorflow.python.tools.optimize_for_inference \\\r\n    --input /path/to/frozen_inference_graph.pb \\\r\n    --output /path/to/optimized_inference_graph.pb \\\r\n    --input_names=image_tensor \\\r\n    --output_names=detection_boxes,detection_scores,num_detections,detection_classes\r\n```\r\n\r\nbut got following error:\r\n```\r\nValueError: graph_def is invalid at node u'ToFloat': Input tensor\r\n'image_tensor:0' Cannot convert a tensor of type float32 to an input of type uint8.\r\n```\r\n\r\nSo obviously i did something wrong with In- and/or Output.\r\n\r\nWould be nice if someone could correct this.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I'm not sure what's going wrong here, but we do now recommend using the more up to date Graph Transform Tool instead of the optimize_for_inference script:\r\nhttps://www.tensorflow.org/mobile/prepare_models\r\n\r\nSince I don't believe the same errors apply there, closing this issue.", "I have the same error...\r\n\r\nAll my input_names will be changed to DT_FLOAT(type). \r\nI fixed it manual !\r\n\r\nby the way, optimize_for_inference still not works well, I used strip_unused instead.  "]}, {"number": 12156, "title": "tf.pow edge case failure", "body": "The tf.pow() function has an edge case which causes it to hang with no error message.\r\n\r\nIf you try to evaluate tf.pow(x,y), when x is an integer (and thus the output tensor is also an integer), while y is a negative value, tensorflow hangs trying to cast the fraction as an integer.\r\n\r\nExamples;\r\n\r\nsess.run(tf.pow([5,2],[-2,3]))\r\nsess.run(tf.pow([5],[-2]))\r\nsess.run(tf.pow(5, -2))\r\nsess.run(tf.pow(tf.constant(5), tf.constant(-2)))", "comments": ["Was able to reproduce this issue... actually if y is double, we also have a problem... I tried passing in 1.2 and that didn't work. \r\n\r\nTypeError: Expected int32 passed to parameter 'y' of op 'Pow', got [1.2] of type 'list' instead.\r\n\r\nThe documentation (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/math_ops.cc#L656) seems to suggest that y can be int32 or double etc. So either we should fix the documentation or increase the capability.\r\n", "Hi, I found the doc of `gen_math_ops._pow` is:\r\n```python\r\n    y: A `Tensor`. Must have the same type as `x`.\r\n```\r\nHence, if both `x` and `y` are double, it works well.\r\n\r\n```python\r\nIn [4]: sess.run(tf.pow(5.0, 1.2))\r\nOut[4]: 6.8986487\r\n```\r\n\r\nSo for double, there are two solution in my option:\r\n\r\n1. cast `x` or `y` automatically when dtypes are different.\r\n2. or fix the document: `y` must be the same type as `x`.\r\n\r\nI prefer to the first one, and I can work on the double issue.\r\n\r\nAs for negative value, it indeed hangs out. ", "I am interested in working on this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Added a PR #15607 to try to address the issue. It follows a similar approach as safe_mod/safe_div. Please take a look if interested.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 12155, "title": "Fix broken link in `tensorflow/go/README.md`", "body": "Fix broken link in `tensorflow/go/README.md`. This is related to #12145 (`tensorflow/java/README.md`) but is in go's README.md instead.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks for the fix.", "@tensorflow-jenkins test this please"]}]