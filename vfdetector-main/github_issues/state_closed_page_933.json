[{"number": 25456, "title": "[XLA] Don't dump the subgraph fingerprint into info log", "body": "This info should not be dumped by default.", "comments": ["@hawkinsp  Hi, could you PTAL and approve or suggest changes(if required)."]}, {"number": 25455, "title": "Add algorithmic optimizer to convert Log(Softmax(x)) to LogSoftmax(x)", "body": "This PR adds an algorithmic optimizer which converts `Log(Softmax(x))` to `LogSoftmax(x)`. [`LogSoftmax`](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/log-softmax) is numerically more stable and may be a bit faster in some cases.\r\n\r\nThis could be expanded in the future to also optimize `log(softmax(x) * y) = logsoftmax(x) + log(y)` and `log(softmax(x) / y) = logsoftmax(x) - log(y)`.", "comments": ["@rmlarsen  Could you PTAL and approve or suggest changes(if required).", "Rebased since #25300 introduced some merge conflicts.", "@hgadig Any updates on this?", "> @hgadig Any updates on this?\r\n\r\nWaiting for the review. Also, this is assigned to rthadur and he will be taking care of this PR(fyi). Thanks !", "Thanks! Sorry for long review cycle.", "> Thanks! Sorry for long review cycle.\r\n\r\nThanks @ezhulenev, you've been really helpful with getting my fixes merged."]}, {"number": 25454, "title": "multi gpu model error when trying to create model", "body": "Please see also SO question about [this](https://stackoverflow.com/questions/54440168/tf-python-keras-utils-multi-gpu-model-error-on-initializing)\r\n\r\nI am using python 3 with tensorflow and multiple gpu configuration, I try to use the [following example][1] to init the multi gpu model, I create a model, It's fine, compiling, running and training, but When I try to add this before the model compilation: \r\n\r\n    from tensorflow.python.keras.utils import multi_gpu_model\r\n    model = multi_gpu_model(model, gpus=2, cpu_merge=False)\r\n\r\nI get this error \r\n\r\n> TypeError: int() argument must be a string or a number, not\r\n> 'TensorShape'\r\n\r\nNote I am using tf with eager eval \r\n\r\nI found [this][2] reffering to use keras.utils.multi_gpu_model instead of tf.python.keras.utils.multi_gpu_model But when I do that I get this error instead: \r\n\r\nWhat am i missing here? \r\n\r\n> line 217, in multi_gpu_model\r\n>     with tf.device(x.device): AttributeError: 'DeferredTensor' object has no attribute 'device'\r\n\r\nthe code for the model is \r\n\r\n    model = Sequential()\r\n    model.add(Flatten(input_shape=(128, 128, 3)))\r\n    model.add(Dense(100, activation=\"sigmoid\"))\r\n    model.add(Dense(100, activation=\"sigmoid\"))\r\n\r\n\r\nThanks \r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model\r\n  [2]: https://github.com/keras-team/keras/issues/11408", "comments": ["update: could this be a gpu id issue? when I try to create a multi_gpu_model without specifing the gpus count with the following code: \r\n\r\n    model = multi_gpu_model(model)\r\n\r\nI get the following error: \r\n\r\n> ValueError: To call `multi_gpu_model` with `gpus=3`, we expect the\r\n> following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1',\r\n> '/gpu:2']. However this machine only has: ['/cpu:0', '/xla_cpu:0',\r\n> '/xla_gpu:0', '/gpu:0', '/gpu:1']. Try reducing `gpus`\r\n\r\nI only have 2 gpus, they are connected to pci ports # 1 and 2 (I cant change that, I don't have the proper space on the board needed to connect them to port 0), does it make any sense that when specifying 2 GPUs, tf will try to get GPU 0 and GPU 1? Can I specify otherwise? ", "@thebeancounter Can you check the detailed description and an example on keras website [here](https://keras.io/utils/). Please let me know how it progresses. I will try to run it tomorrow on my desktop with GPU. Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "It looks like Keras only sees one of the GPUs.\r\nMake sure that all GPUs are accessible.you can use device_lib with TensorFlow.\r\nYou can check all device list using following code:\r\n`from tensorflow.python.client import device_lib` \r\n`device_lib.list_local_devices()`", "> You can check all device list using following code:\r\nfrom tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices()\r\n\r\nI am getting this. it seems like only one gpu is accessible. What to do then?\r\n```\r\n>>> from tensorflow.python.client import device_lib\r\n>>> device_lib.list_local_devices()\r\n2020-07-19 03:09:14.714310: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-07-19 03:09:15.553561: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce MX250 major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.62GiB\r\n2020-07-19 03:09:15.557185: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce MX250, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 12818861576623973213\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 1506609971\r\nlocality {\r\n  bus_id: 1\r\n}\r\nincarnation: 1957363585754623730\r\nphysical_device_desc: \"device: 0, name: GeForce MX250, pci bus id: 0000:01:00.0, compute capability: 6.1\"\r\n]\r\n```"]}, {"number": 25453, "title": "Custom gradients at layer level instead of at function level", "body": "I am implementing a customized layer in tensorflow. However, I failed to figure out a way to customize gradients in layers. I would like to add a function to calculate the gradients to weights and gradients to input by myself. Is there any way to do so?\r\n\r\nThe code should look like this\r\n```\r\nclass MyDenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, num_outputs):\r\n        super(MyDenseLayer, self).__init__()\r\n        self.num_outputs = num_outputs\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_variable(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\r\n\r\n    def call(self, input):\r\n        return tf.matmul(input, self.kernel)\r\n    \r\n    # The function to calculate the gradients by myself\r\n    def _backprop(self,y_delta,x):\r\n        w_delta=some_function(y_delta,x)\r\n        x_delta=some_function2(y_delta,x)\r\n        return w_delta, x_delta\r\n````\r\n", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Feel free to reopen it if necessary."]}, {"number": 25452, "title": "Memory detected less than real value", "body": "I used tf-1.13-python. But when I ran 'tf.Session()', I found it had only 8.99GB free memory instead of 11GB, which was confirmed with 'nvidia-smi', task manager and GPU-Z.\r\n\r\nCUDA:10.0\r\nCUDNN:7.4.1\r\nSystem:win 10\r\n\r\n\r\n>>> tf.Session()\r\n2019-02-02 16:02:17.833936: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that thi\r\ns TensorFlow binary was not compiled to use: AVX2\r\n2019-02-02 16:02:18.143034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.665\r\npciBusID: 0000:26:00.0\r\ntotalMemory: 11.00GiB freeMemory: 8.99GiB\r\n2019-02-02 16:02:18.148745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-02-02 16:02:19.078669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor w\r\nith strength 1 edge matrix:\r\n2019-02-02 16:02:19.081929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-02-02 16:02:19.083860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-02-02 16:02:19.086029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:loc\r\nalhost/replica:0/task:0/device:GPU:0 with 8661 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus\r\n id: 0000:26:00.0, compute capability: 7.5)\r\n", "comments": ["@REFunction Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "> @REFunction Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!\r\n\r\n**Note** that the same problem occurred after I installed tensorflow 1.12 compiled by someone based on CUDA 10.0. So I think this problem may **not only belongs to tf-1.13rc0, but many versions on Windows platforms**. I guess perhaps Windows 10 (or more Windows versions, may be drivers) has some features to avoid that a graphics card uses too much GPU memory. As we know, there are often some programs occupying a small amount of memory, but as for me, other programs only consume almost 100MB while 2GB cannot be  thought available. So strange, right?\r\n\r\n**I am sorry for not providing details. You can get them below.**\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 1809 17763.107\r\n- TensorFlow installed from (source or binary):\"pip3 install tensorflow-gpu==1.13rc0\"\r\n- TensorFlow version:1.13rc0\r\n- Python version:3.6.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- CUDA/cuDNN version:CUDA 10.0 cnDNN 7.4.2\r\n- GPU model and memory:Can't understand but 2080ti 11GB\r\n\r\n\r\n\r\n**Describe the problem**\uff1aFinished above\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFinished above, just tf.Session()\r\n\r\n\r\n**Any other info / logs**\r\nNo.\r\n", "This issue is discussed on the NVIDIA forum and seems to be a known issue with windows.\r\nhttps://devtalk.nvidia.com/default/topic/996667/gtx-980ti-6gb-only-have-4-97gb-memory-available-with-win10-64/\r\n\r\nLet me know if that does not explain the situation."]}, {"number": 25451, "title": "Incorrect Hann window calculation in spectrogram kernel", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution:  N/A\r\n- Mobile device:  N/A\r\n- TensorFlow installed from:  N/A\r\n- TensorFlow version : master\r\n- Python version:  N/A\r\n- Bazel version:  N/A\r\n- GCC/Compiler version:  N/A\r\n- CUDA/cuDNN version:  N/A\r\n- GPU model and memory:  N/A\r\n\r\n**Describe the current behavior**\r\nIn [tensorflow/core/kernel/spectrogram.cc](https://github.com/tensorflow/tensorflow/blob/d08271d73c9f7d399d16917b47fcfda74806dd02/tensorflow/core/kernels/spectrogram.cc#L34) Hann window is calculated as blow: \r\n`(*window)[i] = 0.5 - 0.5 * cos((2 * pi * i) / window_length);`\r\n\r\n**Describe the expected behavior**\r\nAccording to definition of Hann window [https://en.wikipedia.org/wiki/Hann_function](https://en.wikipedia.org/wiki/Hann_function), which should be:\r\n`(*window)[i] = 0.5 - 0.5 * cos((2 * pi * i) / (window_length - 1));`\r\n\r\nPlease correct me if I missed anything.", "comments": ["Thanks, that's a good spot! The current behavior is inherited from some Google internal code, which we want to keep compatibility with if possible. I'm hoping the slight difference isn't causing you any issues. If it is, please add some details and reopen, thanks again for the report!", "The difference should be all right if training and inference use the same code. Thanks for clarification."]}, {"number": 25450, "title": "some issues about estimator distribute", "body": " Now i'd like to do some distribute training using estimator api.I have some questions about chief,worker,and ps host .\r\n  In code,\r\n   ```\r\n worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n      os.environ[\"TF_CONFIG\"] = json.dumps(\r\n              {\r\n                  \"cluster\": {\"chief\": chief_host, \"ps\": ps_hosts, \"worker\": worker_hosts},\r\n                  \"task\": {\"type\": FLAGS.job_name, \"index\": FLAGS.task_id},\r\n              }\r\n          )\r\n    print (\"run_config.num_ps_replicas = %d\" % run_config.num_ps_replicas)\r\n    print (\"run_config.num_worker_replicas = %d\" % run_config.num_worker_replicas)\r\n    print (\"run_config.master = %s\" % run_config.master)\r\n    print (\"run_config.task_type = %s\" % run_config.task_type)\r\n    print (\"run_config.task_id = %d\" % run_config.task_id)\r\n    print (\"run_config.is_chief = %d\" % run_config.is_chief)\r\n```\r\nAnd I found ,chief host or ps host can be missed if task_type is chief or ps. And worker host must be specified if task_type is worker.Is is reasonable that if chief host missed?\r\n`the command is python main.py  --job_name=\"chief\" --task_id=0 ,\r\n`\r\nand then the log as below:\r\n```\r\nINFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': [''], 'ps': ['']}, 'task': {'type': 'chief', 'index': 0}}\r\nrun_config.num_ps_replicas = 1\r\nrun_config.num_worker_replicas = 1\r\nrun_config.master = grpc://\r\nrun_config.task_type = chief\r\nrun_config.task_id = 0\r\nrun_config.is_chief = 1\r\nstart_step = 0\r\n```\r\nif task_type is chief,the ps and worker default num is 1?and if chief host is not specified,can it run directly?\r\n ", "comments": ["@liumilan Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 25449, "title": "Update example_proto_fast_parsing.cc", "body": "hash with a new seed when hash collision is found", "comments": ["@drpngx  Could you PTAL and approve or suggest changes(if required).", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "> I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n\r\nI'm confused this PR is ready to pull, but stalled for months. What I have missed or what I should do to continue this PR ?", "I think there were build failures that were not taken care of and then conflicts.\r\n\r\nTo continue, please reopen as a new PR, solve conflicts (if any). You can add me to reviewers and I'll try to get in as soon as it can land.", "@yann-yy thank you for your contribution , please reopen a new PR , thank you @mihaimaruseac ", "I have reopened a new PR (https://github.com/tensorflow/tensorflow/pull/29121).\r\nWhile I'm not the owner nor a contributor of tensorflow, I couldn't add reviewers to you.\r\nThank you @mihaimaruseac @rthadur "]}, {"number": 25448, "title": "Feature: modify the TF 2.0 upgrade script to convert .ipynb files. ", "body": "Currently, the [TensorFlow 2.0 upgrade script](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) only supports Python (`.py`) files, not Jupyter notebooks (`.ipynb`). This can make converting documentation frustrating and time consuming.\r\n\r\nThe purpose of this issue is to **modify the upgrade script to accept a Jupyter notebook, convert it from TensorFlow 1.x to TF 2.0, and to export out a replacement notebook**.\r\n\r\nFor more information, contact @VikramTiwari, @JerryKurata, and @lc0.", "comments": ["I guess we can move this one to WIP. \r\n\r\nMore on details \ud83d\udc49 https://twitter.com/lc0d3r/status/1092577411504455681", "![image](https://user-images.githubusercontent.com/3712347/52246764-d16af380-289b-11e9-9019-eb74134f4a86.png)", "Friendly reminder that @lc0 is amazing and http://tf2up.ml is something you should check out. :slightly_smiling_face: \r\n\r\nClosing! ", "I didn't get it, what to do exactly?"]}, {"number": 25447, "title": "Fix Windows build due to winsock2.h conflicts", "body": "Some abseil header files include `windows.h`. Without `WIN32_LEAN_AND_MEAN` macro, `windows.h` will include `winsock.h` which will conflict with other files that want to include `winsock2.h` instead.\r\n\r\nWe should be able to revert these rollbacks after this PR.\r\n\r\n- https://github.com/tensorflow/tensorflow/commit/d08271d73c9f7d399d16917b47fcfda74806dd02\r\n- https://github.com/tensorflow/tensorflow/commit/eeaca34ce368eae8cf54c053678f464fd05bcaa6", "comments": ["@perfinion Can you help me to trigger CI test so that I can check if this does fix Windows CI?", "@rongjiecomputer Just to be clear, do you want `experimental_shortened_obj_file_path` added or removed? If you want it removed, can you squash both your commits into one and rebase it on top of master once 25335 is merged into master? That will be much cleaner and less error-prone than the second commit which adds it back.", "> If you want it removed, can you squash both your commits into one and rebase it on top of master once 25335 is merged into master? That will be much cleaner and less error-prone than the second commit which adds it back.\r\n\r\nDone.", "I'll approve this once @perfinion has approved it, if needed.", "Adding `build --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN` into bazelrc file will cause the whole TensorFlow build not being able to use some of the less frequently used APIs. Can we only add `--copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN` to the targets that actually need it?\r\n\r\nFYI @gunan , this caused the failure \r\n```\r\nExecution platform: @org_tensorflow//third_party/toolchains/preconfig/win_1803:rbe_windows_1803\r\nexternal/bazel_tools/src/main/cpp/util/file_windows.cc(526): error C2065: 'FSCTL_GET_REPARSE_POINT': undeclared identifier\r\nexternal/bazel_tools/src/main/cpp/util/file_windows.cc(615): error C2065: 'FSCTL_GET_REPARSE_POINT': undeclared identifier\r\n```", "@meteorcloudy I don't think it is easy to figure out which files that need `WIN32_LEAN_AND_MEAN`. Having to add `copts` for the affected targets is also pretty tedious in Bazel.\r\n\r\nA workaround is for files that need header filtered by `WIN32_LEAN_AND_MEAN`, explicitly include them instead of relying on `windows.h` to include them.\r\n\r\nHere is a test:\r\n\r\n```cpp\r\n#define WIN32_LEAN_AND_MEAN\r\n#include <windows.h>\r\n#include <WinIoCtl.h>\r\n\r\n#ifndef FSCTL_GET_REPARSE_POINT\r\n#error \"Hello\"\r\n#else\r\n#error \"World\"\r\n#endif`\r\n```\r\n\r\n`FSCTL_GET_REPARSE_POINT` comes from `WinIoCtl.h` (https://msdn.microsoft.com/en-us/library/windows/desktop/aa364571(v=vs.85).aspx).", "@rongjiecomputer Thank you!"]}, {"number": 25446, "title": "tf.ConfigProto usage on TF 2.0", "body": "On TensorFlow 1.X, there are various important parameters set by passing `tf.ConfigProto` to `tf.Session(config=...)` or `tf.enable_eager_execution(config=...)`.  For example, to use `NCCL`, it is useful to set the visible GPUs for a session with `config.gpu_options.visible_device_list`.\r\n\r\nMy understanding is that TensorFlow 2.0 no longer has a way to set this configuration \u2014 both `tf.Session` and `tf.enable_eager_execution` are gone.  Is there an alternate way to set this config?\r\n\r\n\r\n[Related StackOverflow question](https://stackoverflow.com/questions/54485110/whats-the-equivalent-of-initializing-a-tf-session-with-tf-configproto-in-tensor)\r\nCC @girving @allenlavoie ", "comments": ["Potentially related: #25138.", "Gaurav is working on a proper replacement, it's not ready yet, for now please use compat.v1. ", "We don't know how to use `compat.v1` to do this.  If there is a way, would it be possible to answer the Stackoverflow question so that it would be possible for us to try TF 2?  The implication from @allenlavoie was that no workaround existed.", "@girving - If you use the [upgrade script](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) on your original TensorFlow model, `tf.Session(config=...)` should change to `tf.compat.v1.Session(config=...)` and work as expected.", "(and you should be able to use tf.compat.v1.ConfigProto to create the\nconfig parameter)\n", "If I want to experiment with writing \"true TF 2.0 code\", will `tf.compat.v1.enable_eager_execution(config=...)` have the desired effect?   In other words, should I expect that we'll end up with a drop-in replacement for the above command?", "No. We will split ConfigProto and replace it with a number of functions\r\nconfiguring different parts of the runtime. You can wait for that if you\r\nwant: what will definitely be true is that any future solution here will\r\nrequire only a local change.\r\n", "@samsamoa: I've landed a fix to #25138. I'm continuing to work on additional config options. Could you please share which specific options are of interest to you?", "@jaingaurav I use `config.device_count['GPU']` and `config.gpu_options.visible_device_list` currently.", "> @samsamoa: I've landed a fix to #25138. I'm continuing to work on additional config options. Could you please share which specific options are of interest to you?\r\n\r\nallow_growth appears to be an essential hack to fix some cudnn issue on rtx20xx devices.\r\n\r\nconfig.gpu_options.allow_growth = True\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)", "A number of new API were added in `tf.config` namespace to support this use case. Please let me know if there is anything we missed regarding this specific issue.", "Looks like you can do this in TF 2.0 now like this:\r\n```\r\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in gpu_devices:\r\n    tf.config.experimental.set_memory_growth(device, True)\r\n```", "Check https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth", "\r\n@opcecco you are a lifesaver. \r\nThank you. ", "> Looks like you can do this in TF 2.0 now like this:\r\n> \r\n> ```\r\n> gpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\n> for device in gpu_devices:\r\n>     tf.config.experimental.set_memory_growth(device, True)\r\n> ```\r\n\r\nI have placed this right before defining a keras Model() instance and compiling it. My taskmanager yet still shows that indepdently all video card memory is taken. What am I doing wrong?", "how do we add this in C++?", "The C++ API still uses sessions, so use the session config as you would in 1.x."]}, {"number": 25445, "title": "Library Conversion: TF Ranking", "body": "[**TensorFlow Ranking**](https://github.com/tensorflow/ranking) is a library for Learning-to-Rank (LTR) techniques on the TensorFlow platform. It contains the following components:\r\n\r\n*   Commonly used loss functions including pointwise, pairwise, and listwise losses.\r\n*   Commonly used ranking metrics like Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG).\r\n*   [Multi-item (also known as groupwise) scoring functions](https://arxiv.org/abs/1811.04415).\r\n*   [LambdaLoss](https://ai.google/research/pubs/pub47258) implementation for\r\n    direct ranking metric optimization.\r\n*   [Unbiased Learning-to-Rank](http://www.cs.cornell.edu/people/tj/publications/joachims_etal_17a.pdf)\r\n    from biased feedback data.\r\n\r\nThe objective of this issue is to migrate TensorFlow Ranking to TF 2.0, with support for Keras models and Ranking Estimator. The key objectives are:\r\n\r\n1. Removing [deprecated TensorFlow functions](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md).\r\n2. Migrate to newer version of [feature columns](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column). `tfr.feature.encode_features` is probably no longer needed.\r\n3. Shift from `tflearn` to `estimator`, and from `experiment_gn` to `train_and_evaluate`.\r\n4. Support for model definition via Keras: Keras allows for object oriented definition of models, in contrast with function closures currently used in TF Ranking.\r\n5. Support Ranking Estimator as a canned estimator.", "comments": ["@dynamicwebpaige I'm interested to work on this issue! It would be great if you could provide a starting point :)", "TF Ranking already uses Estimators and `feature_column`s", "@eggie5 Yeah, given that, any idea how can I customize the project proposal for the other points? :) ", "TensorFlow Ranking  is now fully compatible with TF 2.0 RC0. See release [notes](https://github.com/tensorflow/ranking/releases/tag/v0.1.4) for v0.1.4.", "Closing this issue, as the library is TF2.0 compatible. Ranking models using Keras are tracked in a separate [issue](https://github.com/tensorflow/ranking/issues/32)."]}, {"number": 25444, "title": "Implementation of WARP Loss or pairwise ranking losses", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nLooking to implement warp loss, attempting to implement but getting gradient errors based on ops used. This loss has been implemented elsewhere such as pytorch (https://medium.com/@gabrieltseng/intro-to-warp-loss-automatic-differentiation-and-pytorch-b6aa5083187a) but many models are already built using tensorflow.\r\n\r\n**Will this change the current api? How?**\r\nYes, I am looking to implement new loss functions for learning to rank problems.\r\n\r\n**Who will benefit with this feature?**\r\nAll users\r\n**Any Other info.**\r\n\r\nI have begun to implement this feature but run into gradient computation problems. I am looking for help implementing the loss function.\r\n", "comments": ["I think this belongs better in a downstream project. Specifcally, have you looked at tf ranking https://github.com/tensorflow/ranking ?"]}, {"number": 25443, "title": "Bug in embedding_ops.py Leads to Crash when importing Frozen Wide and Deep model/graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.12, 1.13 and master\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): GCC 6.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nAfter successfully training and exporting the trained Wide and Deep model from here:\r\nhttps://github.com/tensorflow/models/tree/master/official/wide_deep\r\nTried to freeze the exported model using freeze_graph.py. The frozen graph got generated without errors. However, when tried to load the frozen graph using the call tf.import_graph_def(graph_def), got the following error:\r\nFile \"../python2.7/site-packages/tensorflow/python/framework/importer.py\", line 430, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node import/linear/linear_model/linear_model/linear_model/age_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup was passed float from import/linear/linear_model/age_bucketized/weights/part_0:0 incompatible with expected resource.\r\n\r\nAfter inspecting the frozen graph, we found that ResourceGather Op is receiving float form Const node (which used to be VarHandleOP before freezing) but ResourceGather is expecting 'resource' data type.\r\n\r\nThe issue was resolved after changing this line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L693\r\nby removing the if statement and calling convert_to_tensor() unconditionally.\r\n\r\n**Describe the expected behavior**\r\nThe graph is expected to load and run successfully.\r\n**Code to reproduce the issue**\r\nhttps://github.com/tensorflow/models/tree/master/official/wide_deep\r\n\r\n", "comments": ["@zhenlinluo", "I believe this is a bug in freeze_graph (or possibly our loading code) as freeze_graph doesn't seem to properly handle ResourceVariables. The fix in #25501 presents a regression when dealing with ResourceVariables. \r\n\r\n@petewarden @aselle is my hunch about freeze_graph correct?", "@gargn for freeze-resource-variables work", "actually there are several issues reporting similar problem #25327, #25721, https://github.com/tensorflow/hub/issues/208, https://github.com/tensorflow/tfjs/issues/947, #21889 @alextp @gargn @suharshs \r\n ", "https://github.com/tensorflow/tensorflow/commit/cebce4a2b5e33a06a1c92772008082895568f10a should fix this issue. Please reopen if it's still an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25443\">No</a>\n", "When can we expect this in nightly or master?\r\n\r\nedit:\r\nTested this in nightly and it didn't work in my scenario", "Hi, I met the same problem for the wide and deep learning model. The redirect of the line 693 may have been changed, but according to the description of your, I've do the following changes:\r\nfrom \r\n`embedding_weights= [\r\n w if (isinstance(w, resource_variable_ops.ResourceVariable) \r\nand dtype in (None, w.dtype)) \r\nelse ops.convert_to_tensor(w, dtype=dtype) \r\nfor w in embedding_weights \r\n]`\r\nto\r\n`  embedding_weights = [\r\n      ops.convert_to_tensor(w,dtype=dtype)\r\n      for w in embedding_weights\r\n  ]\r\n`\r\nwhere the commented part is the original code.\r\nHowever it still did not work.\r\nDid I change the wrong part? Or the bug still remains?\r\n\r\n----------------------\r\nupdate\r\n\r\nSry that I should have retrain the model, as the code changed influences the model def phrase.\r\n\r\nBut after I had frozen the graph, I found there was any input  ( which should be a \"TextLineDataset\" operation ) other than \"IteratorV2\" or \"IteratorGetNext\" \r\n\r\nHow should I organize the input dataset/ single data ?", "@veqtor @CharlesKung Can you both provide reproducible examples with the error that you are getting?", "@gargn I just changed the code in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L693\r\nfrom\r\n`\r\nembedding_weights=[ w if (isinstance(w, resource_variable_ops.ResourceVariable) and dtype in (None, w.dtype)) else ops.convert_to_tensor(w, dtype=dtype) for w in embedding_weights ]\r\n`\r\nto\r\n`embedding_weights= [ ops.convert_to_tensor(w,dtype=dtype) for w in embedding_weights ]\r\n`\r\n\r\nthen freeze the model and reload it. \r\n\r\n```\r\nsaver = tf.train.import_meta_graph(export_path+\"/model.ckpt-65160.meta\",clear_devices=True)\r\nwith tf.Session(graph = tf.get_default_graph()) as sess:\r\n    input_graph_def = sess.graph.as_graph_def()\r\n    saver.restore(sess,export_path+\"/model.ckpt-65160\")\r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(sess,input_graph_def,output_nodes)\r\n    output_file = export_path+\"/frozen_model.pb\"\r\n    with open(output_file,\"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n\r\nwith open(output_file, \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    tf.import_graph_def(graph_def, input_map=None, return_elements=output_nodes)\r\n```\r\n\r\nI was confused by the input of the frozen graph as the nodes left for processing were \"IteratorV2\" and \"IteratorGetNext\". I thought it was removed by the freezing parts because the previous nodes are doing data processing.\r\nIs there any reference for the input of the frozen graph ? Like in the saved_model methods, the input should be wrapped as tf.Example and build a serving_input_receiver_fn function.\r\nThanks.\r\n\r\n", "@CharlesKung A few questions/comments:\r\n\r\n1. Can you attach the input files that are being used in your code snippet so that I can run it and reproduce it locally? Additionally, can you update your code snippet to work as a stand alone. I noticed `output_nodes` is not defined.\r\n2. Why are you changing the logic in the embedding op? In general, freeze graph works by pattern matching. If you are changing the patterns then it will no longer work.\r\n3. Iterator ops should not be removed unless you are specifying the output nodes as nodes before the Iterator ops. However, by default nothing happens to them in freeze graph.\r\n4. The input of your frozen graph should be the same as the original graph that you created. The only thing that changes in freeze graph is that Variable ops are converted to Const ops (and other similar transformations relating to resources). The structure of the graph and the op names stay the same. I would suggest loading the frozen graph using TensorBoard to examine what the inputs should be if you are uncertain.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25443\">No</a>\n"]}, {"number": 25442, "title": "dynamic_decode reports error under eager_execution ? \u201cValueError: The inequality of unknown TensorShapes is undefined.\"", "body": "- TensorFlow version (use command below): 1.2\r\n\r\n\r\nI have encountered a weird problem when transforming a usual seq2seq code into eager execution mode. After changing the placeholder input to numpy array,  by calling `tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)`\r\ngives error \u201cValueError: The inequality of unknown TensorShapes is undefined.\"\r\n\r\nWithout activating eager execution error, everything is fine.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe code requires two files from github: \r\nhttps://github.com/udacity/deep-learning/blob/master/seq2seq/data/letters_source.txt\r\nand \r\nhttps://github.com/udacity/deep-learning/tree/master/seq2seq/data/letters_target.txt\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntfe.enable_eager_execution()\r\n\r\nfrom distutils.version import LooseVersion\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\nassert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\r\nprint('TensorFlow Version: {}'.format(tf.__version__))\r\nimport numpy as np\r\nimport time\r\nimport tensorflow as tf\r\n\r\nwith open('data/letters_source.txt', 'r', encoding='utf-8') as f:\r\n    source_data = f.read()\r\n\r\nwith open('data/letters_target.txt', 'r', encoding='utf-8') as f:\r\n    target_data = f.read()\r\n\r\ndef extract_character_vocab(data):\r\n    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\r\n    set_words = list(set([character for line in data.split('\\n') for character in line]))\r\n    int_to_vocab = {idx: word for idx, word in enumerate(special_words + set_words)}\r\n    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\r\n    return int_to_vocab, vocab_to_int\r\n\r\nsource_int_to_letter, source_letter_to_int = extract_character_vocab(source_data)\r\ntarget_int_to_letter, target_letter_to_int = extract_character_vocab(target_data)\r\n\r\nsource_int = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) \r\n               for letter in line] for line in source_data.split('\\n')]\r\ntarget_int = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) \r\n               for letter in line] + [target_letter_to_int['<EOS>']] for line in target_data.split('\\n')] \r\n\r\ndef get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\r\n    for batch_i in range(0, len(sources)//batch_size):\r\n        start_i = batch_i * batch_size\r\n        sources_batch = sources[start_i:start_i + batch_size]\r\n        targets_batch = targets[start_i:start_i + batch_size]\r\n        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\r\n        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\r\n        pad_targets_lengths = []\r\n        for target in pad_targets_batch:\r\n            pad_targets_lengths.append(len(target))\r\n        pad_source_lengths = []\r\n        for source in pad_sources_batch:\r\n            pad_source_lengths.append(len(source))\r\n        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths\r\n        \r\ndef pad_sentence_batch(sentence_batch, pad_int):\r\n    max_sentence = max([len(sentence) for sentence in sentence_batch])\r\n    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\r\n\r\ndisplay_step = 50 # \u6bcf\u969450\u8f6e\u8f93\u51faloss\r\nepochs = 60\r\nbatch_size = 128\r\nrnn_size = 50\r\nnum_layers = 2\r\nencoding_embedding_size = 15\r\ndecoding_embedding_size = 15\r\nlearning_rate = 0.001\r\ncheckpoint = \"trained_model.ckpt\" \r\ntrain_source = source_int[batch_size:]\r\ntrain_target = target_int[batch_size:]\r\nvalid_source = source_int[:batch_size]\r\nvalid_target = target_int[:batch_size]\r\n(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\r\n                           source_letter_to_int['<PAD>'],target_letter_to_int['<PAD>']))\r\nsess = tf.InteractiveSession()\r\ntf.global_variables_initializer()\r\n\r\nepoch_i = 1\r\nbatch_i = 0\r\n(targets_batch, sources_batch, targets_lengths, sources_lengths) = next(get_batches(train_target, train_source, batch_size,source_letter_to_int['<PAD>'],target_letter_to_int['<PAD>']))\r\n\r\ninput_data = sources_batch\r\ntargets = targets_batch\r\nlr = learning_rate\r\ntarget_sequence_length = targets_lengths\r\nsource_sequence_length= sources_lengths\r\nmax_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\r\n\r\n#Encoder\r\nsource_vocab_size = len(source_letter_to_int)\r\ntarget_vocab_size = len(target_letter_to_int)\r\ndef get_lstm_cell(rnn_size):\r\n    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n    return lstm_cell\r\n\r\nencoder_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\r\ncell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])\r\nencoder_output, encoder_state = tf.nn.dynamic_rnn(cell, encoder_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\r\n\r\nending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\r\ndecoder_input = tf.concat([tf.fill([batch_size, 1], target_letter_to_int['<GO>']), ending], 1)\r\n\r\ntarget_vocab_size   = len(target_letter_to_int)\r\ndecoder_embeddings  = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\r\ndecoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input) \r\ndef get_decoder_cell(rnn_size):\r\n    decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n    return decoder_cell\r\ncell         = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) for _ in range(num_layers)])\r\noutput_layer = Dense(target_vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\r\n\r\nwith tf.variable_scope(\"decode\"):\r\n    training_helper  = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,sequence_length=target_sequence_length,time_major=False)\r\n    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,training_helper,encoder_state,output_layer) \r\n    training_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)\r\nwith tf.variable_scope(\"decode\", reuse=True):\r\n    start_tokens       = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\r\n    predicting_helper  = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,start_tokens,target_letter_to_int['<EOS>'])\r\n    predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,predicting_helper,encoder_state,output_layer)\r\n    predicting_decoder_output, _,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,impute_finished=True,maximum_iterations=max_target_sequence_length)\r\n\r\n```\r\n[letters_source.txt](https://github.com/tensorflow/tensorflow/files/2823741/letters_source.txt)\r\n[letters_target.txt](https://github.com/tensorflow/tensorflow/files/2823742/letters_target.txt)\r\n\r\n", "comments": ["I have same problem, and I can add that if we use decoder manually: initialize and step, all is ok. ", "@xudong2019 @MihailSalnikov Same problem. I checked the stack trace and decided that this might be a problem with the shape checking in _EagerTensorArray. \r\n```python\r\n# python_ops.tensor_array_ops _EagerTensorArray\r\n  def __init__(self,\r\n               dtype,\r\n               size=None,\r\n               dynamic_size=None,\r\n               clear_after_read=None,\r\n               tensor_array_name=None,\r\n               handle=None,\r\n               flow=None,\r\n               infer_shape=True,\r\n               element_shape=None,\r\n               colocate_with_first_write_call=True,\r\n               name=None):\r\n```\r\nYou need to modify the library file of tensorflow 1.12:\r\n```python\r\n# seq2seq.python.ops.decoder\r\ndef dynamic_decode(......):\r\n    def _create_ta(s, d):\r\n      return tensor_array_ops.TensorArray(\r\n          dtype=d,\r\n          size=0 if dynamic_size else maximum_iterations,\r\n          dynamic_size=dynamic_size,\r\n          element_shape=_shape(decoder.batch_size, s),\r\n          infer_shape=False # Add THIS LINE HERE\r\n      )\r\n```\r\n", "Does this still happen in nightly?\r\n\r\nIf it does, I'll happily accept a pull request with the workaround proposed here.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@jvishnuvardhan I also ran into this, please reopen.", "@andportnoy do you get this error on nightly or the 2.0 alpha preview? Instead of reopening this issue can you file a new one with instructions to reproduce if you do indeed get it on nightly?"]}, {"number": 25441, "title": "Feed batch of images to Tensorflow model in Golang", "body": "I have tried all the methods but did not succeed to make a batch processing of images in Go Tensorflow.\r\n\r\nI have been following this test case for the inception model. I tried many things but did not work.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/example_inception_inference_test.go#L199\r\n\r\nCould anyone help me? I really appreciate any help.", "comments": ["duplicate #25440"]}, {"number": 25440, "title": "Feeding Batch of  Images to session.Run() in Golang", "body": "I have tried all the methods but did not succeed to make a batch processing of images in Go Tensorflow. \r\n\r\nI have been following this test case for the inception model. I tried many things but did not work.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/example_inception_inference_test.go#L199\r\n\r\nCould anyone help me? I really appreciate any help.\r\n\r\nError:\r\n```\r\nEndpoint \"Placeholder:0\" fed more than once.\r\n```\r\n", "comments": ["@asimshankar  @yongtang  Can you PTAL? Thanks!", "@anilknayak : Having all the details asked for in the issue template (in particular, the steps to reproduce the problem) would be a good idea to add. That said, going off of just the error message, it seems that you're trying to feed the same placeholder node multiple times.\r\n\r\nNote that a \"batch\" is generally just a higher dimensional tensor where elements of the batch are stacked. The model in the example you linked to accepts a batch of images, however the tiny graph constructed to normalize an input expects a single image (so given a single JPEG image, it creates a tensor with shape (1, Height, Width, 3) - representing a \"batch\" of 1 image.  The `ExpandDims` operation is adding the batch dimension: https://github.com/tensorflow/tensorflow/blob/783cdae78c3fdce1de859f974dfdcafdb38d4251/tensorflow/go/example_inception_inference_test.go#L210)\r\n\r\nIf you want to feed a batch of images, you'd want to construct the tensor of pixels with shape (N, Height, Width, 3) and feed that to the model - in other words, you'd need to replace https://github.com/tensorflow/tensorflow/blob/783cdae78c3fdce1de859f974dfdcafdb38d4251/tensorflow/go/example_inception_inference_test.go#L187 with something that expects a list of images.\r\n\r\nHope that helps.\r\n\r\n(@ymodak : I no longer have permissions to close the issue or change the labels on it)", "@asimshankar \r\nExpandDims is not for batch size, this says which dimension it will expand the tensor.\r\n```\r\ninput = op.Placeholder(s, tf.String) // says input place holder as string type\r\n```\r\n\r\n```\r\n tensor, err := tf.NewTensor(string(bytes)) // tensor from image byte\r\nnormalized, err := session.Run(\r\n\t\tmap[tf.Output]*tf.Tensor{input: tensor}, // input to tensor\r\n\t\t[]tf.Output{output},\r\n\t\tnil)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n```\r\nhowever I have prepared a Map of ```map[tf.Output]*tf.Tensor{input: tensor}``` with a different scope of input placeholder still did not work and throws me an error\r\n```Endpoint \"Placeholder:0\" fed more than once.```\r\n\r\nI have tried lot of possibility, but I don't know the perfect setting for batch.", "> @asimshankar\r\n> ExpandDims is not for batch size, this says which dimension it will expand the tensor.\r\n\r\nI didn't quite understand? Why do you say that? :)\r\nAs per the [documentation of `ExpandDims`](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#ExpandDims), it adds an extra dimension to the input Tensor. In `constructGraphToNormalizeImage` it is used to convert a 3D tensor corresponding to a single image (of shape (Height, Width, NumChannels)) to a 4D tensor corresponding to a batch of images (with shape (1, Height, Width, NumChannels)).\r\n\r\nThat said though, I'm still not sure I understand the problem you're running into. In the code snippet provided above, there seems to be a single tensor being fed in the map (i.e., the feed map provided to `session.Run` has a single (key, value) pair in it). Is that the line that is throwing the error?\r\n\r\nIdeally, if you could provide a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve), we could better understand the problem you're running into.", "Below function will prepare a small graph that will be used to convert to tensor. The second method will be used to run that graph\r\n```\r\nfunc PrepareTensor() (*tf.Session, *tf.Output, *tf.Output) {\r\n\ttfScope := op.NewScope()\r\n\tinput := op.Placeholder(tfScope, tf.String)\r\n\tdecodeOp := op.DecodeJpeg(tfScope, input,\r\n\t\top.DecodeJpegChannels(NumColorChannels))\r\n\tcastFloatOp := op.Cast(tfScope, decodeOp, tf.Float)\r\n\tbatchScope := op.Const(tfScope.SubScope(\"make_batch\"), BatchSize)\r\n\tbatchOp := op.ExpandDims(tfScope, castFloatOp, batchScope)\r\n\tresizeScope := op.Const(tfScope.SubScope(\"resize\"),\r\n\t\t[]int32{InputHeight, InputWidth})\r\n\tresizeOp := op.ResizeBilinear(tfScope, batchOp, resizeScope)\r\n\tdivScope := op.Const(tfScope.SubScope(\"scale_div\"), ScaleDiv)\r\n\tdivOp := op.Div(tfScope, resizeOp, divScope)\r\n\tsubScope := op.Const(tfScope.SubScope(\"scale_sub\"), ScaleSub)\r\n\tsubOp := op.Sub(tfScope, divOp, subScope)\r\n\tmulScope := op.Const(tfScope.SubScope(\"scale_mul\"), ScaleMul)\r\n\toutput := op.Mul(tfScope, subOp, mulScope)\r\n\tgraph, err := tfScope.Finalize()\r\n\tif err != nil {\r\n\t\tfmt.Errorf(\"failed to create tensorflow graph: %v\", err)\r\n\t}\r\n\tsession, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tfmt.Errorf(\"failed to create tensorflow session: %v\", err)\r\n\t}\r\n\tfmt.Println(\"Input tensor\", input.Shape())\r\n\tfmt.Println(\"output tensor\", output.Shape())\r\n\treturn session, &input, &output\r\n}\r\n```\r\nRun this method to create a tensor of shape [1,224,224,3] from image (Mat type). This method accepts tensors Image, session, input and outptu tensor prepare in the previous method and runs the model graph.\r\n```\r\nfunc ConvertImageToTensor(img gocv.Mat, session *tf.Session, input *tf.Output,\r\n\toutput *tf.Output) (*tf.Tensor, error) {\r\n\tbytes, err := gocv.IMEncode(gocv.JPEGFileExt, img)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\t// DecodeJpeg uses a scalar String-valued tensor as input.\r\n\ttensor, err := tf.NewTensor(string(bytes))\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\ttensors, err := session.Run(\r\n\t\tmap[tf.Output]*tf.Tensor{*input: tensor},\r\n\t\t[]tf.Output{*output},\r\n\t\tnil, /*targets*/\r\n\t)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\treturn tensors[0], nil\r\n}\r\n```\r\nHere after running above two methods I will get a Tensor of shape [1,224,224,3] that can be feed to the MobileNet classifier.\r\n\r\nBelow is the code to run the MobileNet classifier\r\n\r\n```\r\nresult, err := classifier.Session.Run(\r\n\t\tmap[tf.Output]*tf.Tensor{\r\n\t\t\tinputTensor.Output(0): tensor,  // tensor is shape [1,224,224,3] as prepared from the above code\r\n\t\t},\r\n\t\t[]tf.Output{\r\n\t\t\toutputTensor1.Output(0),\r\n\t\t\toutputTensor2.Output(0),\r\n\t\t},\r\n\t\tnil, /*targets*/\r\n\t)\r\n```\r\n", "From what you've described I'm having a hard time understanding where the error is (is it when running the classifier or normalizing the image?), what this has to do with your original question of batching, what `inputTensor` is etc.\r\n\r\nPlease provide a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve) as described in https://stackoverflow.com/help/mcve to make it possible to help.\r\n\r\nFrom the various snippets of code provided, I don't see the obvious place where you'd get the `Endpoint \"Placeholder:0\" fed more than once.` error.", "Sorry for the confusing code, This would help to analyze what I am trying to do?\r\n\r\n```\r\npackage BatchProcessing\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"image\"\r\n\t\"testing\"\r\n\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n\t\"gocv.io/x/gocv\"\r\n)\r\n\r\nfunc TestBatch(t *testing.T) {\r\n\t// Starts : Graph to convert a image to [1,224,224,3] and normalizes it\r\n\ttfScope := op.NewScope()\r\n\tinput := op.Placeholder(tfScope, tf.String)\r\n\tdecodeOp := op.DecodeJpeg(tfScope, input, op.DecodeJpegChannels(3))\r\n\tcastFloatOp := op.Cast(tfScope, decodeOp, tf.Float)\r\n\tbatchScope := op.Const(tfScope.SubScope(\"make_batch\"), int32(0))\r\n\tbatchOp := op.ExpandDims(tfScope, castFloatOp, batchScope)\r\n\tresizeScope := op.Const(tfScope.SubScope(\"resize\"),\r\n\t\t[]int32{224, 224})\r\n\tresizeOp := op.ResizeBilinear(tfScope, batchOp, resizeScope)\r\n\tdivScope := op.Const(tfScope.SubScope(\"scale_div\"), float32(255))\r\n\tdivOp := op.Div(tfScope, resizeOp, divScope)\r\n\tsubScope := op.Const(tfScope.SubScope(\"scale_sub\"), float32(0.5))\r\n\tsubOp := op.Sub(tfScope, divOp, subScope)\r\n\tmulScope := op.Const(tfScope.SubScope(\"scale_mul\"), float32(2.0))\r\n\toutput := op.Mul(tfScope, subOp, mulScope)\r\n\tgraph, err := tfScope.Finalize()\r\n\tif err != nil {\r\n\t\tfmt.Errorf(\"failed to create tensorflow graph: %v\", err)\r\n\t}\r\n\tsession, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tfmt.Errorf(\"failed to create tensorflow session: %v\", err)\r\n\t}\r\n\t// Ends : Graph to convert a image to [1,224,224,3] and normalizes it\r\n\r\n\t// Starts: Reading Images\r\n\tvar images []gocv.Mat\r\n\tfor i := 0; i < 4; i++ {\r\n\t\timg := gocv.IMRead(\"./1.jpg\", gocv.IMWriteJpegQuality)\r\n\t\timages = append(images, img)\r\n\t}\r\n\tfor _, img := range images {\r\n\t\tfmt.Println(img.Rows(), img.Cols())\r\n\t}\r\n\t// Ends: Reading Images\r\n\r\n\t// Starts: Converts images to tensor\r\n\tvar tensor *tf.Tensor\r\n\t**imgeTensorMap := make(map[tf.Output]*tf.Tensor)**\r\n\tfor _, img := range images {\r\n\t\tbytes, err := gocv.IMEncode(gocv.JPEGFileExt, img)\r\n\t\tif err != nil {\r\n\t\t\tfmt.Println(\"Error\")\r\n\t\t}\r\n\t\ttensor, err = tf.NewTensor(string(bytes))\r\n\t\tif err != nil {\r\n\t\t\tfmt.Println(\"Error\")\r\n\t\t}\r\n\t\timgeTensorMap[input] = tensor\r\n\t}\r\n\t**tensors, err := session.Run(\r\n\t\timgeTensorMap,\r\n\t\t[]tf.Output{output}, \r\n\t\tnil /*targets*/)**\r\n\t// Ends: Converts images to tensor\r\n\tif err != nil {\r\n\t\tfmt.Println(\"Error\")\r\n\t}\r\n\tfmt.Println(tensors[0].Shape()) // this should be [4,224,224,3] but actual [1,224,224,3]\r\n\r\n// After that, the batch will be used by another graph to do some classification. like below\r\n// **anotherSession** is for MobileNet graph for classification which has\r\n// **input**: inputTensor\r\n// **outputs** two softmax as outputTensor1 of [2] and outputTensor2 of shape [10]\r\nresult, err := anotherSession.Run(\r\n\t\tmap[tf.Output]*tf.Tensor{\r\n\t\t\tinputTensor.Output(0): tensors[0],  // tensors[0] is shape [4,224,224,3] as per the above code\r\n\t\t},\r\n\t\t[]tf.Output{\r\n\t\t\toutputTensor1.Output(0),\r\n\t\t\toutputTensor2.Output(0),\r\n\t\t},\r\n\t\tnil, /*targets*/\r\n\t)\r\n}\r\n```\r\n", "Like we feed in Python\r\n```\r\nwith sess.as_default():\r\n            (output,prob) = sess.run([output,prob], feed_dict={input: images })\r\n```\r\nHere in ```feed_dict={input: images }``` \r\n```\r\nimages is the numpy 4D array [<batch>, <height>, <width>, <channels>]\r\nthis will give output as [<batch>][output]\r\n```", "That [full program provided above](https://github.com/tensorflow/tensorflow/issues/25440#issuecomment-461982846) helps (though, I'd say it qualifies as a \"complete\" example, but not \"minimal\" or \"verifiable\" :), I'd strongly suggest looking at guidelines in the the [link provided previously](https://stackoverflow.com/help/mcve). Doing so will make it easier for others to help).\r\n\r\nWith that said, I see the problem and it is the same as [mentioned in the first reply](https://github.com/tensorflow/tensorflow/issues/25440#issuecomment-460562260). Let me attempt to elaborate.\r\n\r\nWhat's happening in the [code you posted above](https://github.com/tensorflow/tensorflow/issues/25440#issuecomment-461982846) is that you're trying to feed multiple (`N = len(imageTensorMap)`) 4-D tensors, each with shape `[1, 224, 3]`, instead of a single tensor with shape `[N, 224, 224, 3]`.\r\n\r\nThe Go API isn't as comprehensive and to make things easier there are a few additions that could be made. However, given the current API, it is *possible* to construct the batched tensor using something like this:\r\n\r\n```go\r\nvar buf bytes.Buffer\r\nfor i, img := range images {\r\n  bytes, err := gocv.IMEncode(gocv.JPEGFileExt, img)\r\n  if err != nil {\r\n    fmt.Println(\"Error\")\r\n  }\r\n  tensor, err = tf.NewTensor(string(bytes))\r\n  if err != nil {\r\n    fmt.Println(\"Error\")\r\n  }\r\n  normalized, err := session.Run(\r\n    map[tf.Output]*Tensor: { input: tensor },\r\n    []tf.Output{output},\r\n    nil)\r\n  if _, err := normalized[0].WriteContentsTo(&buf); err != nil {\r\n    // Handle error\r\n  }\r\n}\r\n\r\nbatchShape := []int64{len(images), 224, 224, 3}\r\nbatch, err := tf.ReadTensor(tf.Float, batchShape, &buf)\r\nif err != nil {\r\n  // Handle error\r\n}\r\n\r\n// Now feed \"batch\" to the model\r\n```\r\n\r\nAnother alternative would be to do this batching in the graph by constructing a graph that packs multiple single-image tensors together in a batch using the [`Pack`](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#Pack) operation).\r\n\r\nHope that helps.", "@asimshankar Thanks a lot for the help. buffer is working. I will work on op.Pack() in graph. Nice!", "Closing this issue since its resolved. Feel free to reopen if the issue still persists. Thanks!", "@anilknayak How to use on op.Pack() in graph\uff1f", "@fuchao01  I am working on it. as soon as I get some information I will share it. Thanks for the question.", "@anilknayak or @asimshankar  I know this was a while ago, but have you (or anyone else) figured out how to use op.Pack(). I am struggling with this API. Any quick examples or tips would be extremely appreciated!", "This is what I came up with, it seems like there could be a cleaner way, but this seems to work \r\n\r\n```\r\n// returns 4D tensor of images in a batch\r\nfunc makeTensorFromImages(images []image.Image) (*tf.Tensor, error) {\r\n\r\n\ts := op.NewScope()\r\n\tinputMap := make(map[tf.Output]*tf.Tensor)\r\n\tinputs := make([]tf.Output, len(images))\r\n\r\n\tfor _, image := range images {\r\n\t\tbuf := new(bytes.Buffer)\r\n\t\terr := jpeg.Encode(buf, image, nil)\r\n\t\tif err != nil {\r\n\t\t\tlog.Fatal(err)\r\n\t\t}\r\n\r\n\t\ttensor, err := tf.NewTensor(string(buf.Bytes()))\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\r\n\t\tinput := op.Placeholder(s.SubScope(\"input_jpg\"), tf.String)\r\n\t\tdecoded := op.DecodeJpeg(s.SubScope(\"decode_jpg\"), input, op.DecodeJpegChannels(3))\r\n\t\tinputs = append(inputs, decoded)\r\n\t\tinputMap[input] = tensor\r\n\t}\r\n\r\n\toutput := op.Pack(s, inputs)\r\n\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\t// Execute that graph to decode this one image\r\n\tsession, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tdefer session.Close()\r\n\r\n\t// DecodeJpeg uses a scalar String-valued tensor as input.\r\n\tnormalized, err := session.Run(\r\n\t\tinputMap,\r\n\t\t[]tf.Output{output},\r\n\t\tnil)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\treturn normalized[0], nil\r\n}\r\n```", "```tensors``` are the images converted to tensor in other code, following is to prepare batch.  @csenn\r\n```\r\nvar buf bytes.Buffer\r\n\tfor _, tensor := range tensors {\r\n\t\tif _, err := tensor.WriteContentsTo(&buf); err != nil {\r\n\t\t\tfmt.Errorf(\"failed to write tensor to buffer: %v\", err)\r\n\t\t}\r\n\t}\r\n\tbatchShape := []int64{int64(len(tensors)), int64(InputHeight),\r\n\t\tint64(InputWidth), int64(NumColorChannels)}\r\n\tbatch, err := tf.ReadTensor(tf.Float, batchShape, &buf)\r\n\tif err != nil {\r\n\t\tfmt.Errorf( \"failed to read tensor from buffer: %v\", err)\r\n\t}\r\n```", "convert image to tensor\r\n```\r\nbytes, err := gocv.IMEncode(gocv.JPEGFileExt, img)\r\n\tif err != nil {\r\n\t\terr\r\n\t}\r\n\t// DecodeJpeg uses a scalar String-valued tensor as input.\r\n\ttensor, err := tf.NewTensor(string(bytes))\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\ttensors, err := session.Run(map[tf.Output]*tf.Tensor{*input: tensor},\r\n\t\t[]tf.Output{*output}, nil /*targets*/)\r\n\tif err != nil {\r\n\t\terr\r\n\t}\r\n```"]}, {"number": 25439, "title": "np.array(x), where x is a tuple or list of tensor(s), is unusable (eager execution)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\nb'v1.12.0-5845-g764109a352' 1.12.0\r\n- Python version:\r\n3.6.7\r\n- Bazel version (if compiling from source):\r\nInvocation ID: 42251854-036f-415c-8a52-76aac8520ea0\r\nBuild label: 0.21.0\r\nBuild time: Wed Dec 19 12:58:44 2018 (1545224324)\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version:\r\nCuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory:\r\nGTX 1060 Max Q, 6gb VRAM\r\n\r\n**Describe the current behavior**\r\nIf x is a list of any size, or a tuple of size >= 2, which contains tensors, np.array(x) is unusable as it runs exponentially longer than the tensor.numpy() function.\r\n\r\n**Describe the expected behavior**\r\nnp.array(x) should run in a reasonable amount of time. Doing a simple list comprehension with tensor.numpy() shouldn't be faster.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis shows the exponential time difference when x is a list containing a single tensor.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\ntf.enable_eager_execution()\r\n\r\nx = [tf.zeros((32, 32, 12))]\r\n\r\nstart = time.time()\r\nprint(\"temp\")\r\ntemp = np.array(x[0:1])\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\nprint(\"temp1\")\r\ntemp1 = np.array([x[0]])\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\nprint(\"temp2\")\r\ntemp2 = np.array([x[0].numpy()])\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\nprint(\"temp3\")\r\nlist = []\r\n[list.append(t.numpy()) for t in x]\r\ntemp3 = np.array(list)\r\nend = time.time()\r\nprint(end - start)\r\n\r\n```\r\n**Other info / logs**\r\nOutput:\r\ntemp\r\n1.7468986511230469\r\ntemp1\r\n1.7278409004211426\r\ntemp2\r\n0.00018978118896484375\r\ntemp3\r\n0.00024890899658203125\r\n\r\nHowever, strangely enough, the problem only occurs with tuples when the length of the tuple is 2 or more. Changing x to the line below and running the same code generates similar results.\r\n`x = (tf.zeros((32, 32, 12)), tf.zeros((32, 32, 12)))`\r\n\r\nWhy is it faster for me to use a list comprehension with t.numpy(), appending it to a list, and then run np.array(list), than it is to run np.array(x)?", "comments": ["FYI my numpy version is 1.16.0", "@akshaym this is weird, do you have time to take a look?", "Thanks for the report!\r\n\r\nBased on reading some code, numpy special cases objects that are already np.arrays when converting sequences to python (code reference: https://github.com/numpy/numpy/blob/dea85807c258ded3f75528cce2a444468de93bc1/numpy/core/src/multiarray/ctors.c#L453) and there doesn't seem much we can do on the TF side to get around this.\r\n\r\nNumpy does provide an API to allow this to be slightly faster (the tensor is iterated twice, one of which is to discover the shape - and that can be easily fixed by EagerTensor implementing numpy's array interface).\r\n\r\nThis is still not really ideal though, since when iterating through a tensor item by item, we create a bunch of temporary tensors. Your best bet is to continue to convert the individual tensors to numpy in the list, or to use something like ```tf.stack(x).numpy()``` instead.", "Thanks for looking into this! I guess I will avoid this function in the future for this use case."]}, {"number": 25438, "title": "Support max pooling ops in algorithmic optimizer", "body": "This PR adds support for max pooling operations to the algorithmic optimizer for monotonic functions.\r\n\r\nThis follows up on #25330\r\n/cc @ezhulenev", "comments": ["Thanks for the changes!", "@lgeiger please rebase your branch.", "> @lgeiger please rebase your branch.\r\n\r\nThis branch is based on #25330 which was recently reverted. I'm happy to rebase it, though it would be great to get feedback on why #25330 was reverted first.", "Rebased it \ud83d\ude80 "]}, {"number": 25437, "title": "Add example code to tf.data.Dataset.filter() documentation", "body": "", "comments": []}, {"number": 25436, "title": "[ROCM] Adding support for MIOpen", "body": "@timshen91 @whchung \r\n\r\nThis PR builds on the changes introduced in PR #25424 and adds support for the MIOpen (dnn) plugin. As with the other PR, the `--config=rocm` compile will build successfully with this PR, but full functionality still requires a few more PRs :)\r\n\r\nNote that PR #25424 is a pre-requisite for this PR.\r\n\r\nThe commits that are a part of PR #25424 are also included in this PR, but can/should be ignored when reviewing the changes. Alternatively since this PR is a superset of the two PRs (and if the size of the PR is not an issue) feel free to review + merge this PR directly! Only the last commit (7d1030f) is exclusive to this PR.\r\n", "comments": ["cc @tatatodd \r\n\r\n@chsigg are you interested in reviewing this one too?  :)", "> @chsigg are you interested in reviewing this one too? :)\r\nYes, I can take a look after #25424.\r\n", "@deven-amd a rebase might be need to help move this PR forward.", "@chsigg a gentle ping. this PR doesn't seem to bring side effects to existing test targets.", "I'm starting with #25424 which this change seems to build upon, according to the description.", "@chsigg \r\n\r\ngentle ping. I rebase this PR as well to remove the merge conflicts.", "Thanks, #25424 should land shortly, I will take a look then.", "Marking as 'ready to pull' to allow for easier review and resolving conflicts.", "@chsigg \r\n\r\nThank you for your feedback. I have addressed your review comments and pushed out a rebased commit.", "@chsigg \r\n\r\naddressed code review comments, and pushed out a rebased commit."]}, {"number": 25435, "title": "2.0 Reference Models: ResNet V1.5 (TPU with Keras)", "body": "Deep residual networks, or ResNets for short, proposed the breakthrough idea of identity mappings in order to enable training of very deep convolutional neural networks. This issue will track the creation of 1 GPU and 8 GPU TF 2.0-compatible implementations of ResNet for the ImageNet dataset.\r\n\r\nSee the following papers for more background:\r\n\r\n[1] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\r\n\r\n[2] [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/resnet).", "comments": ["is someone working on this issue ?", "Yes, it'll work soon.", "And ya could you tell me if the implementation would be similar to the GPU version ? :https://github.com/tensorflow/tensorflow/issues/25340\r\nActually I wanted to make changes to the identity block of resnet like changing the dilation rate of the 3x3 conv or something like that. So will such a parameter be added or something?  Cause I am actually working on a segmentation task using dilated convolution at this indentity block. If such a parameter is added it would be great if not would like to work on this task.", "Closing out this issue, as ResNet support has landed."]}, {"number": 25434, "title": "2.0 Reference Models: Keras Application Set (TPU)", "body": "Keras Applications is the `applications` module of the Keras deep learning library. It provides model definitions and pre-trained weights for a number of popular architectures, such as VGG16, ResNet50, Xception, MobileNet, and more.\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.\r\n\r\nFor more information on tf.keras.applications, check [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications).", "comments": ["Can these models run tf.keras.applications models run on TPU?", "@dynamicwebpaige Look like you are looking for the following feature [link1](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/applications) , [link2](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/applications/mobilenet/MobileNet) , [link3](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/applications/xception) , [link4](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/keras/applications/resnet50)\r\n\r\nIf you think the above is not the feature you are looking, please feel free to open a PR in [keras-team/keras](https://github.com/keras-team/keras/issues) repository.\r\nPlease note that Keras development moved to [keras-team/keras](https://github.com/keras-team/keras/issues) repository to focus on only keras. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 25433, "title": "2.0 Reference Models: Transformer (TPU with dist strat and Keras)", "body": "**Transformer** is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.\r\n\r\nSee the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) for more background.\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/transformer).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["I'd like to see the **Transformer** to be well integrated with TF2 because it had been proven that it's better than the RNN and CNN for many NLP tasks.\r\n\r\nHowever, instead of `migrate transformer model into the TF 2.0 Keras API`, why don't we just create a new model to work with TF2?\r\n\r\nFor example, we could create a new python module named `tensorflow_transformer` so that we could:\r\n\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow_transformer import Transformer\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Dense(...))\r\nmodel.add(Transformer(...))\r\nmodel.add(...)\r\n```\r\n\r\nIn this way, we could not only decouple the model/layers from the tensorflow/tensorflow_addons module, but also we can make the module easy to read and maintain. The best result could be encourage the community to create their own tensorflow 2.0 model modules and publish them to PyPI.\r\n\r\nThat's just my two cents, and I'm very interested with the `tensorflow.contrib.seq2seq` and this `transformers` model, will look into it in details when I got some time.", "@huan What you propose will work if it's only about an off-the-shelf solution for engineers. As a researcher, I really like the programming model provided by `seq2seq` including the concept like `Decoder` and `Helper`, because I could experiment with many fancy ways of decoding. \r\n\r\nIn fact, one can write a transformer completely in the `seq2seq` framework, and call `seq2seq.dynamic_decode` just like everything else. \r\n\r\nAgain most of the keras code I saw on seq2seq model has a less elegant programming model and is very error-pruning, but it could also be the case I am not versed in keras.", "Ideally I'd like to see a transformer model implemented as closely as possible to the \"Transformer model for language understanding\" tutorial:\r\nhttps://www.tensorflow.org/tutorials/text/transformer\r\n\r\nBut with added beam search decoding, this should ideally allow for passing in a function that generates a new output given inputs.\r\nThis would be ideal because then it would be up to the developer to pass in their function that might reuse an encoding and other forms of conditioning, perform tiling per-n_beams etc\r\n\r\nSomething else that would be nice is some sort of caching function such that, given that self-attention layers that are causal, one could cache previous layer outputs, adding a significant boost to speed.\r\nEssentially every layer becomes a sort of \"RNN-cell\" when doing inference.\r\n\r\nFurthermore, more efficient attention functions should configurable, Tensor2Tensor has a lot of them in TF1.x style code.\r\n\r\nI've begun porting over some of them to TF2 layer style code in a library, feel free to look for inspiration:\r\nhttps://github.com/veqtor/dynastes/blob/master/dynastes/layers/t2t_attention_layers.py", "Here is the code example for TF2 Transformer + DistributionStrategy (including TPU) using Customized Training loop https://github.com/tensorflow/models/blob/master/official/transformer/v2/transformer_main.py", "@dynamicwebpaige \r\nAs per this [link](https://github.com/tensorflow/models/blob/master/official/transformer/v2/transformer_main.py), can you please move this to closed status."]}, {"number": 25431, "title": "Solving problem with wrong type in seq2seq.BeamSearchDecoder label:prtype:bugfix ", "body": "This PR is solving of #25423 \r\n\r\nSolving problem with wrong data type in _beam_search_step: lengths_to_add. Numpy int64 isn't equal tf int64.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo lgty?", "Nagging Reviewer @ebrevdo, @lmthang: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25430, "title": "bazel tests r1.13rc0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu (4.15.0-44-generic)\r\n- TensorFlow installed from: Source\r\n- TensorFlow version: r1.13 (e7f2979fc7bbbd491a5c1db2268d4ee67cc46f88)\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source) : gcc-7 (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: 10.0/7.4.2.24-1+cuda10.0\r\n- GPU model and memory: K1100M 2 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nRunning the bazel tests for r1.13rc0 fails.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n#!/bin/bash\r\n\r\nCONDA_ENV=\"tf_dev_3.5\"\r\nPYTHON_VERSION=\"3.5\"\r\nWORKDIR=\"${HOME}/GitHub\"\r\nPYTHON_BIN=\"${HOME}/anaconda3/envs/${CONDA_ENV}/bin/python\"\r\nTF_VERSION=\"r1.13\"\r\n\r\nif [ ! -d \"${WORKDIR}/tensorflow\" ]; then\r\n  git clone git@github.com:tensorflow/tensorflow.git\r\n  git --git-dir \"${WORKDIR}\"/tensorflow/.git checkout ${TF_VERSION}\r\nfi\r\ngit --git-dir \"${WORKDIR}\"/tensorflow/.git pull\r\n\r\nexport PYTHON_BIN_PATH=\"${PYTHON_BIN}\"\r\nexport PYTHON_LIB_PATH=\"${HOME}/anaconda3/envs/${CONDA_ENV}/lib/python${PYTHON_VERSION}/site-packages\"\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=\"10.0\"\r\nexport CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.0\"\r\nexport TF_CUDNN_VERSION=\"7\"\r\nexport CUDNN_INSTALL_PATH=\"/usr/local/cuda-10.0\"\r\nexport TF_NEED_TENSORRT=1\r\nexport TENSORRT_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nexport TF_NCCL_VERSION=\"2.4\" # 2.4.2\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"3.0\"\r\nexport TF_CUDA_CLANG=0\r\nexport GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-7\" #\"/usr/bin/gcc\" #\"$(which gcc)\"\r\nexport TF_NEED_MPI=1\r\nexport MPI_HOME=\"/usr/lib/x86_64-linux-gnu/openmpi\"\r\nexport CC_OPT_FLAGS=\"-march=native -Wno-sign-compare\"\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\n\r\ncd \"${WORKDIR}\"/tensorflow/\r\n./configure\r\n\r\nbazel build --config=opt --config=cuda --config=mkl --config=ngraph //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package \"${WORKDIR}\"/tensorflow_pkg\r\n\r\n# ${PYTHON_BIN} -m pip uninstall tensorflow -y\r\n# ${PYTHON_BIN} -m pip install \"${WORKDIR}\"/tensorflow_pkg/tensorflow-*\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n \u2718 \ue0b0 ~/GitHub/tensorflow \ue0b0 \ue0a0 r1.13 \ue0b0 bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: e5417977-f3f0-4c98-a6a3-729508e1bc94\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nERROR: /home/erik/GitHub/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: no such package '@com_google_protobuf//': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nERROR: Analysis of target '//tensorflow/core:example_java_proto' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 30.254s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (410 packages loaded, 3273 targets configured)\r\n```\r\n\r\n\r\n\r\n```\r\n \u2718 \ue0b0 ~/GitHub/tensorflow \ue0b0 \ue0a0 r1.13 \ue0b0 bazel test --incompatible_remove_native_http_archive=false -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: cbdcdc07-17f9-45a7-bb7e-ea87aeb364e1\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:597:1: Traceback (most recent call last):\r\n\tFile \"/home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD\", line 597\r\n\t\tinternal_gen_well_known_protos_java(srcs = WELL_KNOWN_PROTOS)\r\n\tFile \"/home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/protobuf.bzl\", line 266, in internal_gen_well_known_protos_java\r\n\t\tLabel((\"%s//protobuf_java\" % REPOSITOR...))\r\n\tFile \"/home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/protobuf.bzl\", line 266, in Label\r\n\t\tREPOSITORY_NAME\r\nThe value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name). You can temporarily allow the old name by using --incompatible_package_name_is_a_function=false\r\nERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:android' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'\r\nERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'\r\nERROR: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/com_google_protobuf/BUILD:380:1: Target '@com_google_protobuf//:windows_msvc' contains an error and its package is in error and referenced by '@com_google_protobuf//:protoc'\r\nERROR: /home/erik/GitHub/tensorflow/tensorflow/core/BUILD:255:1: every rule of type proto_library implicitly depends upon the target '@com_google_protobuf//:protoc', but this target could not be found because of: Target '@com_google_protobuf//:protoc' contains an error and its package is in error\r\nERROR: Analysis of target '//tensorflow/core:example_java_proto' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 73.304s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (483 packages loaded, 17762 targets configured)\r\n```\r\n\r\n\r\n```\r\n \u2718 \ue0b0 ~/GitHub/tensorflow \ue0b0 \ue0a0 r1.13 \ue0b0 bazel test --incompatible_remove_native_http_archive=false --incompatible_package_name_is_a_function=false -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Invocation ID: 9421f3f2-53c7-4490-8dc6-675ac7d12ab2\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nDEBUG: /home/erik/.cache/bazel/_bazel_erik/28b1b744916e40d081909e8a1a165362/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/erik/GitHub/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nERROR: /home/erik/GitHub/tensorflow/tensorflow/contrib/BUILD:167:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' does not have mandatory providers: 'CcInfo'\r\nERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted\r\nINFO: Elapsed time: 24.164s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (532 packages loaded, 72385 targets configured)\r\n```\r\n\r\n\r\n", "comments": ["I'm getting the same thing on the same OS, only not using CUDA, and also hacked configure file to accept bazel 0.22.0", "John, seems to be a problem with bazel 0.21, 0.22. I assume we're simply not ready for that yet?", "More info... Tests do not work, build does though, it completes in entirety with no problems. Bazel versioning is a problem. I modified configure file to accept 0.22.0 and it built fine with it, so you can probably remove the upper boundary.", "Amit: We are using bazel 0.19.2.  Do we have any requirements to run with bazel 0.21 or 0.22?", "Not at the moment, the recommended way is still only 0.19.2.", "We have internally decided not to increase the upper limit. "]}, {"number": 25429, "title": "TF 2.0: Release binaries for Python 3.5 and 3.7.", "body": "", "comments": ["+@dynamicwebpaige is this issue for TF 2.0 preview?", "@annarev - I don't think this is a blocker for the preview; just intended to be an issue to track. Thanks for working on it! :)\r\n\r\n(cc: @martinwicke to keep me honest)", "This should also list all the other projects that are required and in what order. eg: I've been looking through tensorflow/estimator and it looks like it will be required for TF2.0 (but not 1.13?).\r\n\r\nSo far it looks like:\r\nfirst install: keras_applications, keras_preprocessing\r\nbuild and install tensorflow/tensorflow\r\nthen tensorflow/estimator, tensorboard etc\r\n\r\n", "@annarev, @goldiegadde is this done?", "@martinwicke I see wheels for [Python 3.5 and Python 3.7](https://pypi.org/project/tf-nightly-gpu-2.0-preview/#files) on PyPi.", "Why not release tensorflow with conda the same time of pip package?", "@dynamicwebpaige @annarev it seems to me this issue can be closed? Please reopen if I misunderstood.\r\n\r\n@zh794390558 the conda packages are community supported, we do not build those.", "The Python 3.7 wheels are not present in the latest nightly (20190618): https://pypi.org/project/tf-nightly-gpu-2.0-preview/2.0.0.dev20190618/#files\r\n\r\nBut they were present in 20190314 for example: https://pypi.org/project/tf-nightly-gpu-2.0-preview/2.0.0.dev20190314/#files\r\n\r\nThis leads to the following error in MacOS with python 3.7:\r\n```\r\npip install tf-nightly-2.0-preview\r\nNo matching distribution found for tf-nightly-2.0-preview\r\n```", "https://pypi.org/project/tf-nightly-gpu-2.0-preview/#files has packages for linux only for python 2.7, 3.3, 3.4, 3.5 and 3.7 - and windows only for python 3.6 - so, Linux doesn't have a package for python 3.6 (which is the version in Ubuntu LTS repos), and windows only has a package for python 3.6... ", "I checked https://pypi.org/project/tensorflow/#files so I will assume this issue went away. Feel free to open a new bug if the issue resurfaces."]}, {"number": 25428, "title": "T", "body": "", "comments": []}, {"number": 25427, "title": "Replace log(1 + x) with numerically more stable log1p(x)", "body": "This PR replaces `log(1 + x)` with `log1p(x)`. This function is more precise if x is close to zero.\r\n\r\nI'd be happy to add this as a general [algorithmic optimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc) if this would be appreciated.", "comments": ["This looks great! Thanks.  Bayesflow dir is marked for deprecation. Can you make those changes in https://github.com/tensorflow/probability ?  ", "> This looks great! Thanks. Bayesflow dir is marked for deprecation. Can you make those changes in https://github.com/tensorflow/probability?\r\n\r\n`tf_probability` actually uses this function from [`tf.contrib` in the tests](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/mcmc/sample_halton_sequence_test.py#L106). I quickly checked for usage of `log(1 + x)` but haven't found any occurrences in the codebase.\r\n\r\nWhat do you think about adding this transformation as a general [algorithmic optimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc). This way the conversion would happen automatically in most of the cases.", "This would be a great algorithmic optimizer!\n\nIt's still not perfect though because the optimizer would also have to\nrewrite the gradient graph to be more stable.\n\nOn Fri, Feb 1, 2019 at 11:52 AM Lukas Geiger <notifications@github.com>\nwrote:\n\n> This looks great! Thanks. Bayesflow dir is marked for deprecation. Can you\n> make those changes in https://github.com/tensorflow/probability?\n>\n> tf_probability actually uses this function from tf.contrib in the tests\n> <https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/mcmc/sample_halton_sequence_test.py#L106>.\n> I quickly checked for usage of log(1 + x) but haven't found any\n> occurrences in the codebase.\n>\n> What do you think about adding this transformation as a general algorithmic\n> optimizer\n> <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc>.\n> This way the conversion would happen automatically in most of the cases.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25427#issuecomment-459845901>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRxhrNZZBX1kr1R3rBcGU0b-ImO5ks5vJJsOgaJpZM4aevCt>\n> .\n>\n\n\n-- \n - Alex\n", "> This would be a great algorithmic optimizer!\r\n\r\n\ud83d\udc4dI'll take a look at it next week.\r\n\r\n > It's still not perfect though because the optimizer would also have to\r\nrewrite the gradient graph to be more stable.\r\n\r\nOK, I see. Haven't though about that.", "> This would be a great algorithmic optimizer!\r\n\r\nLooks like that's why it's already implemented \ud83d\ude06: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc#L2535-L2709", "Does this cause keras losses & metrics tests to fail?  I see errors like:\r\n\r\n```\r\nFile \"tensorflow/python/keras/losses_test.py\", line 703, in test_sample_weighted703, in test_sample_weighted\r\n    self.assertAlmostEqual(self.evaluate(loss), 4.6, 3)\r\n\r\nAssertionError: 4.5747714 != 4.6 within 3 places\r\n```", "Ah; given that this is implemented as an algorithmic optimizer it's interesting that the optimizer didn't optimize the keras losses & metrics.  @rmlarsen with this CL, the keras metrics & losses test gives different results for log1p(x) vs log(x + 1); shouldn't the optimizer catch those?\r\n\r\nMore generally; I guess this CL isn't needed if we can get the arithmetic optimizer to properly catch all use cases.", "> We may not need this PR due to grappler's arithmetic optimizer.\r\n\r\nThis PR touches parts of the code which doesn't allow for grapples's arithmetic optimization, e.g [here](https://github.com/tensorflow/tensorflow/pull/25427/files#diff-578bcb7af9dd0db757506082a5bfd816R72) or [here](https://github.com/tensorflow/tensorflow/pull/25427/files#diff-a2b2f06d09507ae234155496dec79765R199). Or am I missing something?\r\n", "If the grappler optimizer doesn't optimize those; it would be better to just improve the optimizer.\r\n\r\nAlso i don't think there's much numerical difference in the gradients of log1p and log(1+.).", "@ymodak   Please help proceeding with the next steps as I've some access issues(I'm trying to resolve).", "As @ebrevdo pointed out, the transformation `log(1 + x) --> log1p(x)` is already handled by the grappler optimizer, thus there's not much need to change this on the Python side.\r\n\r\nThe grappler optimizer currently doesn't touch the case `log(1 - x) --> log1p(-x)` but it looks like this change would need further investigation due to slight changes in the numerical properties: https://github.com/tensorflow/tensorflow/pull/25427#discussion_r253717573\r\n\r\nI have changed the PR so that it only contains those places that cannot be optimized by the grappler optimizer, because they are inside kernels or in C code that does not use `math_ops`.\r\n\r\n@hgadig This PR is now quite small and should be ready to merge."]}, {"number": 25426, "title": "Segmentation Fault with tf.io.decode_csv , numpy record_defaults and tensor input ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04.4 LTS on Windows Linux SubSystem\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n'2.0.0-preview' / \"b'v1.12.0-6503-g7cfe43a11d'\"\r\n- Python version:\r\nPython 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" **=> this doesn't work, you mean tf.version.x**\r\n\r\n**Describe the current behavior**\r\n`Segmentation fault (core dumped)`\r\n**Describe the expected behavior**\r\nprints result\r\n**Code to reproduce the issue**\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    record_defaults=np.zeros(5)    \r\n    parsed_fields = tf.io.decode_csv(tf.constant('1,2,3,4,5'), record_defaults)\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nIt's fine with either tf.constants for record defaults or plain string to decode, eg:\r\n\r\n    parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\r\n\r\n    \r\n    Numpy version: \r\n    numpy                     1.16.0          py36_blas_openblash1522bff_1000  [blas_openblas]  conda-forge\r\n\r\n", "comments": ["I can confirm that I could reproduce this issue on MacOSX 10.13.6, python 3.6.8, TF version `2.0.0-dev20190126`, git version `b'v1.12.0-6726-g5522d670af'`, installed using `pip3 install -U tf-nightly-2.0-preview`.\r\n\r\nThe `decode_csv()` function expects the `record_defaults` to be an array of tensors, so replacing `np.zeros(5)` with `[tf.constant(0.)]*5` (or even with `[0.]*5`) solves the problem, but still segmentation faults should never happen.", "I can reproduce this on a recent nightly in eager mode.  In graph mode, a shape inference check raises a safe failure exceptio.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25426\">No</a>\n"]}]