[{"number": 21329, "title": "Update README.md", "body": "Add new links and descriptions about Tensorflow. The reasons is to make it more user-friendly and provide more informations to all the users so that they can get the informations quickly and learn more faster. I hope this would help all the communities and users.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Thank you for your useful information. I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Good day sir, may I know when will it be merged?\r\nYour kind consideration is highly appreciated.\r\nThank you for your attention.", "I'm sorry for the late reply sir. I have arranged all the links alphabetically. Thank you for your attention."]}, {"number": 21328, "title": "FusedBatchNorm of TF 1.9 doesn't work fine ", "body": "After importing the frozen model [mobilenet_v2_1.0_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz), I find there are 4 outputs of FusedBatchNorm operation and I can't eval() other 3 tensors:\r\n\r\n### System information\r\n- **Ubuntu 18.04 desktop\r\n- **TensorFlow installed from (source or binary)**: binary(pip)\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7.15rc1\r\n- **CUDA/cuDNN version**: CPU only\r\n- **Exact command to reproduce**:\r\n```\r\nipdb> batch_norm = tf.get_default_graph().get_operation_by_name('MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm')                  \r\nipdb> batch_norm.outputs\r\n[<tf.Tensor 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:0' shape=(1, 112, 112, 32) dtype=float32>, <tf.Tensor 'MobilenetV2/expan\r\nded_conv/depthwise/BatchNorm/FusedBatchNorm:1' shape=(32,) dtype=float32>, <tf.Tensor 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNor\r\nm:2' shape=(32,) dtype=float32>, <tf.Tensor 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:3' shape=(32,) dtype=float32>, <tf.Tenso\r\nr 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:4' shape=(32,) dtype=float32>]                                                   \r\nipdb> batch_norm.outputs[1].eval()\r\nOptimizing fused batch norm node name: \"MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm\"                                            \r\nop: \"FusedBatchNorm\"\r\ninput: \"MobilenetV2/expanded_conv/depthwise/depthwise\"\r\ninput: \"Const_198\"                                                                                                                               \r\ninput: \"Const_37\"\r\ninput: \"Const_138\"\r\ninput: \"Const_120\"\r\ndevice: \"/job:localhost/replica:0/task:0/device:CPU:0\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"epsilon\"\r\n  value {\r\n    f: 0.001\r\n  }\r\n}\r\nattr {\r\n  key: \"is_training\"\r\n  value {\r\n    b: false\r\n  }\r\n}\r\n\r\n*** InvalidArgumentError: FetchOutputs MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:1: output index too large, must be < 1\r\n\r\n```\r\n\r\nBTW, TensorFlow 1.8 works fine.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nGPU model and memory\nMobile device", "Thanks for your response.\r\nAdditional information is as follows:\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNo, all nodes are imported from the frozen graph.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nUbuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\nNot in mobile device.\r\n- **Bazel version (if compiling from source)**: \r\n0.16.0\r\n- **GPU model and memory**:\r\n No GPU", "@marksandler2 can you take a look?", "Can you describe what are you trying to achieve? The outputs 1-3 of fused batchnorm are meant as for   tensorflow access during training,   that you normally wouldn't need to access directly.  Particulaly if mode is_training is set ot False, then these nodes are often optimized away by the optimizer. \r\n\r\nIf you want another look please make reproducible example in colab.  (you can use mobilentev2's ipynb as a strating point).", "When running the following example code, I get error message which doesn't exist in TF 1.8.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsess = tf.Session()\r\nsess.as_default()\r\n\r\nwith tf.gfile.FastGFile('mobilenet_v2_1.0_224_frozen.pb', 'rb') as f:\r\n    sess.graph.as_default()\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\ninput_data = np.random.randn(1,224,224,3)\r\n\r\ninput_data = tf.convert_to_tensor(input_data, dtype=tf.float32)\r\ntf.import_graph_def(graph_def, name=\"\", input_map={'input:0':input_data})\r\nbatch_norm = tf.get_default_graph().get_operation_by_name('MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm')\r\nout = sess.run(batch_norm.outputs[1])\r\nprint out\r\n\r\n```\r\n\r\nThe error is as follows:\r\n```\r\nOptimizing fused batch norm node name: \"MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm\"                                            \r\nop: \"FusedBatchNorm\"                                                                                                                             \r\ninput: \"MobilenetV2/expanded_conv/depthwise/depthwise\"\r\ninput: \"Const_198\"                                                                                                                               \r\ninput: \"Const_37\"\r\ninput: \"Const_138\"                                                                                                                               \r\ninput: \"Const_120\"\r\ndevice: \"/job:localhost/replica:0/task:0/device:CPU:0\"                                                                                           \r\nattr {\r\n  key: \"T\"                                                                                                                                       \r\n  value {\r\n    type: DT_FLOAT                                                                                                                               \r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"epsilon\"\r\n  value {\r\n    f: 0.001\r\n  }\r\n}\r\nattr {\r\n  key: \"is_training\"                                                                                                                             \r\n  value {\r\n    b: false\r\n  }                                     \r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    out = sess.run(batch_norm.outputs[1])\r\n  File \"/home/haifeng/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/haifeng/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/haifeng/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/haifeng/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: FetchOutputs MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm:1: output\r\nindex too large, must be < 1\r\n\r\n```", "@marksandler2 I can't access google colab due to internet limitation. Sorry for this. Please try above reproducible example", "See my comment above.\r\n\r\nThere is no second-and-third output in FusedBatchNorm operator when created in evaluation mode (which is what frozen proto contains).  \r\nSo you get out-of-range error because there is no such node,  this is working as expected. \r\n\r\nIf you want to create network in training mode you need to use the python code and restore from checkpoint. But even then it is unclear what yo uare trying to do. \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom nets.mobilenet import mobilenet_v2\r\n\r\ntf.reset_default_graph()\r\n\r\n# For simplicity we just decode jpeg inside tensorflow.\r\n# But one can provide any input obviously.\r\nfile_input = tf.placeholder(tf.string, ())\r\n\r\nimage = tf.image.decode_jpeg(tf.read_file(file_input))\r\n\r\nimages = tf.expand_dims(image, 0)\r\nimages = tf.cast(images, tf.float32) / 128.  - 1\r\nimages.set_shape((None, None, None, 3))\r\nimages = tf.image.resize_images(images, (224, 224))\r\n\r\n# Note: arg_scope is optional for inference.\r\nwith tf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=False)):\r\n  logits, endpoints = mobilenet_v2.mobilenet(images)\r\n  \r\n# Restore using exponential moving average since it produces (1.5-2%) higher \r\n# accuracy\r\nema = tf.train.ExponentialMovingAverage(0.999)\r\nvars = ema.variables_to_restore()\r\n\r\nsaver = tf.train.Saver(vars)  \r\nfrom IPython import display\r\nimport pylab\r\nfrom datasets import imagenet\r\nimport PIL\r\ndisplay.display(display.Image('panda.jpg'))\r\n\r\nwith tf.Session() as sess:\r\n  saver.restore(sess,  checkpoint)\r\n  x = endpoints['Predictions'].eval(feed_dict={file_input: 'panda.jpg'})\r\nlabel_map = imagenet.create_readable_names_for_imagenet_labels()  \r\nprint(\"Top 1 prediction: \", x.argmax(),label_map[x.argmax()], x.max())\r\n\r\n\r\n```\r\n\r\n\r\n\r\nIf you need further help, please explain what are you trying to do. ", "I want to evaluate all the output of FusedBatchNorm. The easy way to get it/them is to run the following code `sess.run(batch_norm.outputs[:]`. I'll get an error in TF 1.10 now:\r\n```\r\nipdb> out = sess.run(batch_norm.outputs[:])\r\n*** OutOfRangeError: Node 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm' (type: 'Const', num of outputs: 1) does not have output 1 \r\n```\r\nHowever,  if we see the length of `batch_norm`, it's `5`.\r\n", "There is nothing to evaluate - the frozen inference graph have  a trivial\n(constant) BatchNorm tensor.  If you want to evaluate batch-norm outputs  -\nyou need to create real training graph, not frozen graph.\n\nOn Wed, Aug 15, 2018 at 6:39 PM haifenghan <notifications@github.com> wrote:\n\n> I want to evaluate all the output of FusedBatchNorm. The easy way to get\n> it/them is to run the following code sess.run(batch_norm.outputs[:]. I'll\n> get an error in TF 1.10 now:\n>\n> ipdb> out = sess.run(batch_norm.outputs[:])\n> *** OutOfRangeError: Node 'MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm' (type: 'Const', num of outputs: 1) does not have output 1\n>\n> However, if we see the length of batch_norm, it's 5.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21328#issuecomment-413394394>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGqB2GusJudIe4E_VOUlFN7poL7ckIxTks5uRM1tgaJpZM4VryZr>\n> .\n>\n", "Interestingly even frozen graph have FusedBatchNorm that technically has 5\r\noutputs, but tensorflow refuses to evaluate them.  I didn't check the\r\ndetails, but fusedbatchnorm outptuts [1:5] are undocumented, so perhaps\r\ngraphs freezing destroys\r\nthem somehow..\r\n\r\nAnyway:  this code works fine:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom nets.mobilenet import mobilenet_v2\r\ntf.reset_default_graph()\r\n\r\n# For simplicity we just decode jpeg inside tensorflow.\r\n# But one can provide any input obviously.\r\nfile_input = tf.placeholder(tf.string, ())\r\n\r\nimage = tf.image.decode_jpeg(tf.read_file(file_input))\r\n\r\nimages = tf.expand_dims(image, 0)\r\nimages = tf.cast(images, tf.float32) / 128.  - 1\r\nimages.set_shape((None, None, None, 3))\r\nimages = tf.image.resize_images(images, (224, 224))\r\n\r\n# Note: arg_scope is optional for inference.\r\nwith\r\ntf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=False)):\r\n  logits, endpoints = mobilenet_v2.mobilenet(images)\r\n\r\n# Restore using exponential moving average since it produces (1.5-2%)\r\nhigher\r\n# accuracy\r\nema = tf.train.ExponentialMovingAverage(0.999)\r\nvars = ema.variables_to_restore()\r\n\r\nsaver = tf.train.Saver(vars)\r\ng = tf.get_default_graph()\r\nop =\r\ng.get_operation_by_name('MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm')\r\nwith tf.Session() as sess:\r\n  saver.restore(sess,  checkpoint)\r\n  x = endpoints['Predictions'].eval(feed_dict={file_input: 'panda.jpg'})\r\n  print(op.outputs[1].eval(feed_dict={file_input: 'panda.jpg'}))\r\nlabel_map = imagenet.create_readable_names_for_imagenet_labels()\r\nprint(\"Top 1 prediction: \", x.argmax(),label_map[x.argmax()], x.max())\r\n```", "Thanks @marksandler2 \r\n\r\nI tried your code and it works fine.  we can evaluate all output of the FusedBatchNorm in your code.\r\n\r\nIt seems the frozen graph has a compatibility issue with latest TensorFlow. \r\nOnly TF1.8 or earlier version can correctly evaluate outputs of the fused batchnorm in the frozen graph.\r\n", "Hi @haifenghan !\r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "Thanks for your suggestion", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 21327, "title": "Cannot benchmark mobilenet_v2_1.0_224.tflite", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac high sierra\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nGalaxy S7, Huawei Mate 10 etc.\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1dc4c5b6f5829c4b8ef7d67f41735e8a0ce9d59a\r\n- **Python version**:\r\n2.7.10\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n<copy benchmark file and model to phone>\r\nadb shell /data/local/tmp//tflite_benchmark --graph=/data/local/tmp///mobilenet_v2_1.0_224.tflite --warmup_runs=1 --num_runs=50 --input_layer=input --input_layer_shape=1,244,244,3 --num_threads=4\r\n```\r\n\r\n### Describe the problem\r\nCannot run benchmark on mobilenet_v2_1.0_224.tflite model. Other models seem to be fine. Tried 0.35_96 and 1.4_224. Both are fine.\r\n\r\n### Source code / logs\r\n```\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [4]\r\nBenchmark name: []\r\nOutput prefix: []\r\nWarmup runs: [1]\r\nGraph: [/data/local/tmp///mobilenet_v2_1.0_224.tflite]\r\nInput layers: [input]\r\nInput shapes: [1,244,244,3]\r\nUse nnapi : [0]\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model /data/local/tmp///mobilenet_v2_1.0_224.tflite\r\nresolved reporter\r\ntensorflow/contrib/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (4004 != 1001)\r\nNode number 64 (RESHAPE) failed to prepare.\r\n\r\nFailed to allocate tensors!\r\nAborted\r\n```", "comments": ["apparently, you have typos. Please use ` --input_layer_shape=1,224,224,3` instead of ` --input_layer_shape=1,244,244,3`", "Thanks @freedomtan \r\n\r\nJust to be extra clear, the proper size is 224, not 244. And you can also run the benchmark tool without specifying those parameters:\r\n\r\n```\r\n$ bazel run //tensorflow/contrib/lite/tools/benchmark:benchmark_model -- \\\r\n    --graph=`realpath ~/Downloads/mobilenet_v2*.tflite`\r\n...\r\nAverage inference timings in us: Warmup: 46891, Init: 5050, no stats: 33320.6\r\n...\r\n```"]}, {"number": 21326, "title": "model prediction works differently when running on different system configuration", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:nope\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.9\r\n- **Python version**:3.6\r\n- **Bazel version (if compiling from source)**:dont know\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:9.0\r\n- **GPU model and memory**: Geforce Gtx 1080 Ti\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflostackoverfloww as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\ni am running this neural translation application\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\r\nAnd i made some changes here to store the model as checkpoints so that everytime i dont want to again train the application it works fine in windows 10 gpu system.\r\n\r\nwhen i  run the same code in cpu system its giving different result .\r\nI checked the versions and environment everything is same.how could i solve this issue? \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This is same as the issue #21216 \r\n\r\nTry installing tf-nightly on your system (pip install -U --pre tf-nightly).", "i tried that one also but still giving different predictions and when running them on ubuntu system gives me the same problem\r\n", "Could you please tell us a little bit more about what you're running and what kind of differences you are seeing?", "![output gpu](https://user-images.githubusercontent.com/37259213/43696343-2497c324-995b-11e8-8b74-d9a0c7a85c2a.png)\r\nThe above one is the output when i am running the code on windows gpu system.\r\n\r\n[Untitled Document 1.txt](https://github.com/tensorflow/tensorflow/files/2261178/Untitled.Document.1.txt)\r\n\r\n\r\n![ubuntu](https://user-images.githubusercontent.com/37259213/43697069-2dfc148e-995f-11e8-8ba3-acd47f1713bf.png)\r\nThis is the output when running the same file on ubuntu without gpu. The same questions  but answers are different.", "For how many epochs was the model trained on in GPU and CPU confirgurations before you ran the predictions?", "10 epochs on GPU system and saving them in  checkpoint file and i runned them in CPU system(without retraining)", "> TensorFlow version (use command below): 1.9\r\n\r\nPlease try upgrading your tensorflow version to tf-nightly. I think, that should resolve the error. ", "@kishore0905 did that work for you?", "i tried with tf-nightly in both the systems but the problem still persist", "@allenlavoie can you take a look at this?", "@kishore0905 what's the `tf.__git_version__` of the nightly you tried that's not working?", "sorry for the late reply. The git version is \"b'v1.9.0-rc2-1098-g89e06304aa'\"", "The current nightly version is 1.11.xxx. You might wanna upgrade it again.\r\n\r\nOr TF 1.10 is out so you can upgrade to that.", "windows 10 Gpu congfiguration:\r\n\r\ntensorflow(GPU) version: 1.10.0\r\n\r\ngit version:  b'v1.10.0-rc1-19-g656e7a2b34\r\n\r\n\r\n\r\n![tensorflow_issue gpu](https://user-images.githubusercontent.com/37259213/44074259-8f69c656-9fb5-11e8-9209-59224315971c.png)\r\n\r\n\r\n\r\n\r\nUbuntu CPU configuration:\r\n\r\ntensorflow(CPU) version: 1.10.0\r\n\r\ngit version:   v1.10.0-0-g656e7a2b34\r\n\r\n![tf_issue cpu](https://user-images.githubusercontent.com/37259213/44073606-4a09b4a2-9fb2-11e8-8406-a94ef5aa5a5a.jpg)\r\n", "Hi, which checkpoint are you restoring from?\r\n\r\nAlso, the code for this example has been updated to checkpoint the variables of the model. Can you please try that code and tell us if the error still persists?", "Did this solve your problem?", "I have exactly the same problem, is there any solution yet?, \r\n\r\nIn my case I ran the training in colab using GPU, then tested and worked as expected. Later, I changed the session to only CPU (also in colab) and got random words. I also tested it on my local machine and got random words too.  I suppose colab has a consistent environment.\r\n\r\nIn my case I restore the latest checkpoint and I used the provided (seq2seq with attention) example as template. ", "@gaarangoa which TensorFlow versions are you using?", "I think its because of CuDNNGRU that's being used. It doesn't work in CPU mode so the weights are not getting loaded into that RNN. So when you switch to CPU mode, the weights are loaded into GRU and not CuDNNGRU because of the if-else loop in the code. Can you use GRU instead of CuDNNGRU and see if that works? ", "Hey there, \r\n\r\nLocal environment: '1.10.0'\r\ncolab only CPU: '1.10.0'\r\ncolab GPU: '1.10.0'\r\n\r\nOnly works in colab when using GPU. I also suspect is because of CuDNNGRU, in the provided example, there is a function that takes care of it (see below) depending on the environment it uses either GRU or CuDNNGRU. \r\n\r\nBased on the tf documentation, I found that GRU should have enabled: **reset_after=True,**,  **recurrent_activation='sigmoid',** But, I got the same inconsistent results. \r\n\r\ndef gru(units):\r\n    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\r\n    # the code automatically does that.\r\n    if tf.test.is_gpu_available():\r\n        return tf.keras.layers.CuDNNGRU(units,\r\n                                    return_sequences=True,\r\n                                    return_state=True,\r\n                                    recurrent_initializer='glorot_uniform')\r\n    else:\r\n        return tf.keras.layers.GRU(units,\r\n                                return_sequences=True,\r\n                                return_state=True,\r\n                                **reset_after=True,**\r\n                                **recurrent_activation='sigmoid',**\r\n                                recurrent_initializer='glorot_uniform',\r\n                                )\r\n\r\nI'm a little confused about what version is using the GRU implementation, in the documentation says the CPU compatible is v1 see https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\r\n\r\nMany thanks!!", "I modified my comment above. The problem is you train with CUDNNGRU and when you shift to CPU the weights are loaded into GRU. That's where the problem is. If you are using CPU for running predictions, I would suggest training it with GRU and removing the if-else conditional loop.", "Oh, that issue :(. We'll hopefully have checkpoint-compatible CPU/GPU RNNs eventually.", "@allenlavoie  I don't think its that. Its the CuDNNGRU and GRU being used in GPU and CPU mode. CuDNNGRU is getting trained and the weights are getting loaded into GRU which is causing the random output which makes sense. ", "Hey there, \r\nI just tested using the GRU only. Unfortunately, seems like the checkpoint restore does not work for RNNs yet. \r\n\r\nAny suggestions for a different approach to save/restore? \r\n\r\nThanks a lot!  ", "The checkpoint restore doesn't work in CPU mode?", "nope, seem it does not work\r\n\r\nThis is what I did:\r\nTest #1\r\n1.  I modified the gru function by enabling only the GRU (as you suggested)\r\n2. Run training and save checkpoint as in the notebook\r\n3. Test few examples and it worked fine\r\n4. Restart the Encoder/Decoder \r\n     _encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)_\r\n     _decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)_\r\n5. Load checkpoint\r\n6. Test few examples and the result was pretty random. \r\n\r\nTest #2\r\n1.  I modified the gru function by enabling only the GRU\r\n2. Run training and save checkpoint as in the notebook\r\n3. Test few examples and it worked fine\r\n4. Change to CPU only mode\r\n5. Restart the Encoder/Decoder \r\n6. Load checkpoint\r\n7. Test few examples and the result was pretty random. \r\n\r\nI did all tests using colab. \r\n\r\nRegards,", "I think you need to restart the kernel when doing it in colab. Colab behaves weirdly when loading the checkpoints if the kernel is not restarted. I think it retains some weights or something else is happening. \r\n\r\nCan you try restarting the kernel and then don't train the model but just load the checkpoints?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21325, "title": "wrong matrix inverse at tf?", "body": "I find tensorflow give a wrong inverse of some matrix.\r\n\r\nfor example\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nx = np.load('one.npy')\r\ny = tf.matrix_inverse(tf.constant(x))\r\n\r\nsess = tf.Session()\r\nsess.run(y)\r\n\r\nnp.linalg.inv(x)\r\n```\r\n\r\nthe `one.npy` can be downloaded from `https://kexue.fm/one.npy`\r\n\r\n\r\nI test it at tf 1.2 and tf 1.8 on ubuntu. both of them give different result from numpy and echo other.", "comments": ["I also found tf.matrix_inverse might have a bug. Did you find a solution for it?"]}, {"number": 21324, "title": "[INTEL MKL] Conv3d enhancement", "body": "Enhancement to support for  mkl conv3d op.", "comments": ["@jzhoulon Is this PR dependent on #21011 being merged?", "Does not matter anymore - #21011 got merged.\r\n", "@ezhulenev  done", "@ezhulenev can you approve this PR?  Thanks.", "@ezhulenev thanks for approving and sorry to bug you again. Another merge commit introduced merge conflicts that I took care of. The PR needs your approval again. Sorry about that.", "@gunan This PR is not passing the new clang-format check. Is it required to pass this test before merging?", "Yes, they are required internally, but Ill let @yifeif for the definitive answer.", "Yea they are required. Looks like there are good amount of clangformat errors. I patched the change internally with auto format, so you should be good."]}, {"number": 21322, "title": "Cherry pick rollback into TF 1.10 to un-break Cloud TPUs w/TF 1.10", "body": "Automated rollback of commit 590af170ca85a4921db0c28e4fa2785462bdcebd\r\n\r\nPiperOrigin-RevId: 204806075", "comments": []}, {"number": 21321, "title": "Add scope for node placement and numerical_hint function", "body": "This PR adds an nvidia_scope for users to be able to fine-tune trt conversion process and a numerical_hint function for future user defined quantization ranges in TRT.", "comments": ["@aaroey can you add @pooyadavoodi to reviewers please. I can't seem to be able to add him", "@samikama I was not able to do so either. His id doesn't show up in the reviewers list.", "It has been 49 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 21320, "title": "[object_detection] model_main.py failure: tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 100], but got 101", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:bazel release 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:cuda 9.1/ cuDNN 7.0\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:python object_detection/model_main.py --alsologtostderr --pipeline_config_path=pipeline.config --model_dir=.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am trying a object_detection mobileV2_model for my own custom dataset. It fails with the following exception \r\n```\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main.py\", line 101, in <module>\r\n    tf.app.run()\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 97, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 447, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 531, in run\r\n    return self.run_local()\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 669, in run_local\r\n    hooks=train_hooks)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1119, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1135, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1336, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 577, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1053, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1144, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1129, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1201, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 981, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 100], but got 101\r\n\t [[Node: Slice_55 = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](unstack_2:7, zeros_48, stack_55)]]\r\n\t [[Node: Loss/unstack_3/_11039 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10953_Loss/unstack_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op u'Slice_55', defined at:\r\n  File \"object_detection/model_main.py\", line 101, in <module>\r\n    tf.app.run()\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 97, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 447, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 531, in run\r\n    return self.run_local()\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 669, in run_local\r\n    hooks=train_hooks)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1119, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1132, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1107, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/local/YORK/ashwani.agarwal/models/research/object_detection/model_lib.py\", line 216, in model_fn\r\n    unpad_groundtruth_tensors=train_config.unpad_groundtruth_tensors)\r\n  File \"/home/local/YORK/ashwani.agarwal/models/research/object_detection/model_lib.py\", line 163, in unstack_batch\r\n    unpadded_tensor = tf.slice(padded_tensor, slice_begin, slice_size)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 576, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7177, in _slice\r\n    \"Slice\", input=input, begin=begin, size=size, name=name)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/home/local/YORK/ashwani.agarwal/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Expected size[0] in [0, 100], but got 101\r\n\t [[Node: Slice_55 = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](unstack_2:7, zeros_48, stack_55)]]\r\n\t [[Node: Loss/unstack_3/_11039 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10953_Loss/unstack_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nthis is the pipeline.config\r\n\r\n```\r\nmodel {\r\n  ssd {\r\n    num_classes: 8\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 192\r\n        width: 192\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_mobilenet_v2\"\r\n      depth_multiplier: 0.25\r\n      min_depth: 16\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 3.99999989895e-05\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.0299999993294\r\n          }\r\n        }\r\n        activation: RELU_6\r\n        batch_norm {\r\n          decay: 0.999700009823\r\n          center: true\r\n          scale: true\r\n          epsilon: 0.0010000000475\r\n          train: true\r\n        }\r\n      }\r\n      use_depthwise: true\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 3.99999989895e-05\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.0299999993294\r\n            }\r\n          }\r\n          activation: RELU_6\r\n          batch_norm {\r\n            decay: 0.999700009823\r\n            center: true\r\n            scale: true\r\n            epsilon: 0.0010000000475\r\n            train: true\r\n          }\r\n        }\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.800000011921\r\n        kernel_size: 3\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        use_depthwise: true\r\n      }\r\n    }\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 5\r\n        min_scale: 0.20000000298\r\n        max_scale: 0.949999988079\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.333299994469\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 9.99999993923e-09\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid {\r\n        }\r\n      }\r\n      hard_example_miner {\r\n        num_hard_examples: 3000\r\n        iou_threshold: 0.990000009537\r\n        loss_type: CLASSIFICATION\r\n        max_negatives_per_positive: 3\r\n        min_negatives_per_image: 3\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 24\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n  keep_checkpoint_every_n_hours: 3\r\n  optimizer {\r\n    rms_prop_optimizer {\r\n      learning_rate {\r\n        exponential_decay_learning_rate {\r\n          initial_learning_rate: 0.00400000018999\r\n          decay_steps: 214089\r\n          decay_factor: 0.949999988079\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.899999976158\r\n      decay: 0.899999976158\r\n      epsilon: 1.0\r\n    }\r\n  }\r\n  fine_tune_checkpoint: \"/data/model.ckpt-99096\"\r\n  num_steps: 25000\r\n  fine_tune_checkpoint_type: \"classification\"\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"/data/avo_labelmap.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"/data/ssdv2_tfrecord/train_v2_00000000.tfrecords\"\r\n  }\r\n}\r\neval_config {\r\n  num_visualizations: 30\r\n  num_examples: 7091\r\n  eval_interval_secs: 7200\r\n  save_graph: true\r\n  use_moving_averages: true\r\n  min_score_threshold: 0.289999991655\r\n  visualize_groundtruth_boxes: true\r\n  retain_original_images: true\r\n}\r\neval_input_reader {\r\n  label_map_path: \"/data/avo_labelmap.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n  tf_record_input_reader {\r\n    input_path: \"/data/ssdv2_tfrecord/test_v7_00000000.tfrecords\"\r\n  }\r\n}\r\n```\r\n", "comments": ["Any updates?", "We identified the issue. The fix will go out with our next release this week!", "I sent out the PR to fix this. A relevant FAQ question is also added to help understand this issue.", "Nagging Assignees @karmel, @pkulzc: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21319, "title": "[tf.py_func()] Check Python interpreter state before call", "body": "Attempting to acquire the GIL in a destructor when the process is terminating can fail, leading to a hang. Fixes #21277.", "comments": []}, {"number": 21318, "title": "Intel MKL DNN: Adding support of fusing Pad and Conv2D Operators in MKL DNN", "body": "**Idea of fusing Pad and Conv2d:**\r\n\r\nA = input(such as image), B = input(paddings), C= Pad = input of conv2D, \r\nD=input(filter), E = Conv2D, Z = Zeta\r\nC=Pad(A,B); E=Conv2D(C,D); Z=Zeta(E,Y)\r\nAfter layout pass (merge), the new Op will be the following:\r\n_MklPadWithConv2D(A, D, B, DMT/_0, DMT/_1, DMT/_2)\r\nHere, DMT/_X are the mkl layout inputs, corresponding to A, D and B.\r\n\r\n**Explanation of the Pad + Conv2D Fusion with Example:**\r\nInput = y\r\nx = pad(y, paddings) ; for example: x = pad ( y, {(0,0), (2=top,5=bottom), (6=left,9=right), (0,0)}\r\npaddings = { (0,0) (2,5) (6,9) (0,0)}\r\n\r\nEigen version of conv2D:\r\nZ = conv2D(x, filter, stride, dilation, \"padding\") // size of z depends on x, filter, stride, dilation, \u201cPadding\u201d, paddings\r\n\r\nMKL version of conv2D :\r\nY = MKL_Conv2d(input, filter, padding_left, padding_right, output) \r\n// padding_left = (2,6), padding_right =(5,9)\r\nPadding in conv2D = \"VALID\", or \u201cSAME\"\r\n\r\n**Restriction:**\r\nPad op changes the size of input. Then, in the conv2d, if \"SAME\" padding is assigned, then additional padding are applied. We calculate the output size based on the Padding. Currently this \"Pad and Conv2D fusion\" implementation does not support SAME padding. \r\n\r\nMKL DNN conv2D API only can accept two arrays for padding. They are: pad_left(top, left) and pad_write(bottom, right). There will be one value of each of the padding, such as top, left, bottom and right.\r\n\r\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)\r\n\r\n**How we handle the pad+conv fusion:**\r\n\r\nChanges in Layout pass:\r\n\r\nWe check, if pad and conv2D are in correct pattern, if yes, we merge them\r\nWe check if VALID padding option is selected in the conv2d op, if yes, then only we merge. We primarily support pad+conv2D fusion for \u201cVALID\u201d type of conv2D padding. This is the usual use cases, such as found in Yolo v2. If needed later, fusion with SAME padding will be supported.\r\nWe replace the merged dummy fused op by the correct fused op.\r\n\r\nChanges in Conv2d op:\r\n\r\nThe tensor format of padding in Pad operator is different than the format of padding to pass to the MKL DNN fused conv2D operator. Needed to convert the format so that proper format is passed to the MKL DNN conv2D op.\r\nIf VALID padding option in the conv2D op is passed, then there will be no padding requested from the Conv2D op. Then, when we fuse Pad and conv2D, then we calculate proper format of pad (such as the right, left, top, bottom padding). Then pass them to the MKL DNN op and execute in mkl_conv2d_ops.cc file. We calculate proper Conv2D output size based on the padding from the Pad op in mkl_conv2d_ops.h file.\r\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)\r\n\r\n**Unit tests:**\r\nMerge test: Two tests are written to make sure, 1) the Pad and Conv2D are merged within constrain and 2) Pad and Conv2D dont merge if the constrain does not meet\r\nProducing correct output: Two tests are written to make sure they produce correct output, 1) NHWC format and 2) NCHW format.\r\n", "comments": ["@penpornk Would you mind taking a look at this?", "Thank you so much for the detailed and very helpful reviews. I tried my best to address the issues. Please let me know any issues.", "Thank for the suggestion of removing the duplicate edge.\r\nI took the 1st suggestion.\r\nI think with small change we can achieve this. We can have the following :\r\nfor (const Edge* e : pred->in_edges()) {\r\nif (e->IsControlEdge()) {\r\n//Don't allow duplicate edge\r\n(*g)->AddControlEdge(e->src(), new_node, false);\r\n}\r\n}\r\nHere, if duplicate edge is found, then it will return nullptr, otherwise it will add the edge.\r\n\r\nThe addControlEdge is in :\r\n\r\ntensorflow/tensorflow/core/graph/graph.cc\r\n\r\nLine 454 in 0126748\r\n\r\n const Edge* Graph::AddControlEdge(Node* source, Node* dest, \r\nAnd the AddEdge is in :\r\n\r\ntensorflow/tensorflow/core/graph/graph.cc\r\n\r\nLine 403 in 425b62a\r\n\r\n const Edge* Graph::AddEdge(Node* source, int x, Node* dest, int y) { \r\nPlease let me know if this looks ok or not.", "> Could you please add a test case (some node pointing to Pad and Conv2D and some node that both Pad and Conv2D both point to) for this as well?\r\n\r\nSure, I have added actually two unit tests, which seemed required.\r\na) one test for one node pointing to Pad and conv2d. The test name is : (MklLayoutPassTest, NodeMerge_PadWithConv2D_Common_Input)\r\nb) The other test (MklLayoutPassTest, NodeMerge_PadWithConv2D_Common_InOutput) is common input for pad and conv2d and output from both pad and conv2d go to one node (Output2).  In this case pad and conv2d were not fused correctly, as checked and explained in https://github.com/Intel-tensorflow/tensorflow/blob/f6c9e054a042bf0f518a740380f3f96a28e8c5be/tensorflow/core/graph/mkl_layout_pass.cc#L4144. In short, if pad output is going to another node other than conv2d, then if we merge, we lost that pad output. the unit test successfully test this scenario.\r\n  ", "Currently, there are conflicts of two files with the master, because, other PRs merged recently. I will resolve this conflict soon. ", "Thank you so much for the changes! I believe we are getting close. \r\n\r\nI originally meant testing for the same incoming/outgoing control edges, but having the test for same input/output is also nice. Could you please also check for the control edges? For example, add a node X where `X:control->Pad:control` and `X:control->Conv2D:control`, and check for the output that we only have `X:control->_MklConv2D:control` appeared once. The same goes for outgoing control edges.", ">>>For example, add a node X where X:control->Pad:control and X:control->Conv2D:control, and check for the output that we only have X:control->_MklConv2D:control appeared once. The same goes for outgoing control edges.\r\n\r\nSorry for the delay. There are some conflicts with master, which needed to resolve. \r\n\r\nI have added two unit tests as you have suggested above. One is to test if control edges are added as input to pad and conv2d from one common op, then control edges are not duplicated on the merged node (MklPadWithConv2d) from the common op. This test name is Input_ControlEdge_PadWithConv2D_Positive. The other test will be for output control edge from both pad and conv2d to a common op. That test name is : Output_ControlEdge_PadWithConv2D_Positive.\r\nNeeded to do some smaller edit/add code in mkl_layout_pass.cc and mkl_layout_pass_test.cc file to support these two test. \r\n\r\nThere are minor conflict in the (mkl_conv_ops.cc file) for the name of the signature of the kernel (mkl_conv_ops) between this branch and the master due to adding padding types and if pad+conv2d fusion is enabled. However, the conflict can be resolved easily when merging with master.", "Thanks, it was great to work with you!", "Thanks for approving the PR. Just wondering, is there anything needed to merge the branch with master?", "I was waiting for the check results. Not sure why we still have some checks not completed yet. Just called force-rerun.", "Could you please look into the build failures and also run clang-format to fix the formatting errors? Thank you! ", "Sorry for the delay. Will look into it.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@penpornk  We fixed the build errors, merge conflict and clang format errors. Thanks", "There is a small issue with master which failing other testing. We are fixing those, and, then the tests will pass. So, we need wait until the fix is done in the master.", "With this new commit from agramesh1, the issues should be fixed.", "One test failed in Ubuntu contrib test is probably not related to this PR.\r\nWe cannot see the details on Ubuntu Python2 issue. Not sure, what is failing there. ", "I agree it's probably not from this PR. The tests aren't even built with --config=mkl to begin with. We are just having troubles pulling your additional changes in and I'm not sure why. Will figure this out soon."]}, {"number": 21317, "title": "bfloat16 training with GPUs", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 9.1/7.1.1\r\n- **GPU model and memory**: NVIDIA GTX 1080 Ti\r\n- **Exact command to reproduce**: shown below\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI have two issues, one could be feature request, and the other could be seeking confirmation from the Tensorflow team\r\nI am trying to execute the language model training - https://github.com/okuchaiev/f-lm\r\nthe model runs fine with the tf.float16 or tf.float32 datatypes for trainable variables.\r\nHowever, if the trainable variables are tf.bfloat16 type, I am getting the errors (shown below for two separate cases). \r\n(line 22 and 24 of https://github.com/okuchaiev/f-lm/blob/master/model_utils.py can affect the datatype of varibles. To define dtype as bfloat16, I changed tf.float16 in line 22 and 24 to tf.bfloat16)\r\n\r\nCase 1: Looks are bfloat16 not supported for moving_averages:\r\n\r\nError:\r\n\r\n File \"/home/ranjeeths/langModel/f-lm/single_lm_train.py\", line 54, in <module>\r\n    tf.app.run()\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/ranjeeths/langModel/f-lm/single_lm_train.py\", line 37, in main\r\n    run_train(dataset, hps, os.path.join(FLAGS.logdir, \"train\"), ps_device=\"/gpu:0\")\r\n  File \"/home/ranjeeths/langModel/f-lm/run_utils.py\", line 14, in run_train\r\n    model = LM(hps, \"train\", ps_device)\r\n  File \"/home/ranjeeths/langModel/f-lm/language_model.py\", line 62, in __init__\r\n    self.train_op = tf.group(*[self.train_op, ema.apply(variables_to_average)])\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 386, in apply\r\n    var.name)\r\nTypeError: The variables must be half, float, or double: model/lstm_0/lstm_cell/kernel:0\r\n\r\nTo get this error in executed the code as follows:\r\nexport CUDA_VISIBLE_DEVICES=1\r\nSECONDS=604800\r\nLOGSUFFIX=bigLSTM\r\npython /home/ranjeeths/langModel/f-lm/single_lm_train.py --logdir=/home/ranjeeths/langModel/f-lm/log/bigLSTM/$LOGSUFFIX/ --num_gpus=1 --datadir=/home/ranjeeths/langModel/lm/data/lm1b/1-billion-word-language-modeling-benchmark-r13output/ --hpconfig run_profiler=False,max_time=$SECONDS,num_steps=20,num_shards=8,num_layers=2,learning_rate=0.2,max_grad_norm=1,keep_prob=0.9,emb_size=1024,projected_size=1024,state_size=8192,num_sampled=8192,batch_size=256,float16_rnn=True,float16_non_rnn=False,loss_scale=1.0\r\n\r\n(with float16_rnn=True, float16_non_rnn=False, the lstm related variables will all be tf.bfloat16 and remaining variables will be tf.float32)\r\n\r\nCase 2: Looks like bfloat16 is not supported for 'Floor' operation (and probably many more)?\r\n\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ranjeeths/langModel/f-lm/single_lm_train.py\", line 54, in <module>\r\n    tf.app.run()\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/ranjeeths/langModel/f-lm/single_lm_train.py\", line 37, in main\r\n    run_train(dataset, hps, os.path.join(FLAGS.logdir, \"train\"), ps_device=\"/gpu:0\")\r\n  File \"/home/ranjeeths/langModel/f-lm/run_utils.py\", line 40, in run_train\r\n    with sv.managed_session(master, config=config) as sess:\r\n  File \"/usr/lib64/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1000, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 828, in stop\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 989, in managed_session\r\n    start_standard_services=start_standard_services)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 285, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Floor' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\n         [[Node: model/model/dropout/Floor = Floor[T=DT_BFLOAT16, _device=\"/gpu:0\"](model/model/dropout/add)]]\r\n\r\nCaused by op u'model/model/dropout/Floor', defined at:\r\n  File \"/home/ranjeeths/langModel/f-lm/single_lm_train.py\", line 54, in <module>\r\n    tf.app.run()\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/ranjeeths/langModel/f-lm/single_lm_train.py\", line 37, in main\r\n    run_train(dataset, hps, os.path.join(FLAGS.logdir, \"train\"), ps_device=\"/gpu:0\")\r\n  File \"/home/ranjeeths/langModel/f-lm/run_utils.py\", line 14, in run_train\r\n    model = LM(hps, \"train\", ps_device)\r\n  File \"/home/ranjeeths/langModel/f-lm/language_model.py\", line 29, in __init__\r\n    loss = self._forward(i, xs[i], ys[i])\r\n  File \"/home/ranjeeths/langModel/f-lm/language_model.py\", line 87, in _forward\r\n    x = tf.nn.dropout(x, hps.keep_prob)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 2318, in dropout\r\n    binary_tensor = math_ops.floor(random_tensor)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2811, in floor\r\n    \"Floor\", x=x, name=name)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ranjeeths/MLProj/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Floor' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n\r\n         [[Node: model/model/dropout/Floor = Floor[T=DT_BFLOAT16, _device=\"/gpu:0\"](model/model/dropout/add)]]\r\n\r\nTo get this error in executed the code as follows:\r\nexport CUDA_VISIBLE_DEVICES=1\r\nSECONDS=604800\r\nLOGSUFFIX=bigLSTM\r\npython /home/ranjeeths/langModel/f-lm/single_lm_train.py --logdir=/home/ranjeeths/langModel/f-lm/log/bigLSTM/$LOGSUFFIX/ --num_gpus=1 --datadir=/home/ranjeeths/langModel/lm/data/lm1b/1-billion-word-language-modeling-benchmark-r13output/ --hpconfig run_profiler=False,max_time=$SECONDS,num_steps=20,num_shards=8,num_layers=2,learning_rate=0.2,max_grad_norm=1,keep_prob=0.9,emb_size=1024,projected_size=1024,state_size=8192,num_sampled=8192,batch_size=256,float16_rnn=False,float16_non_rnn=True,loss_scale=1.0\r\n\r\n(with float16_rnn=False, float16_non_rnn=True, the lstm related variables will all be tf.float32 and remaining variables will be tf.bfloat16)\r\n\r\n\r\nI am not sure if bfloat16 is supported for all operations (not just the ones mentioned above), or whether these errors are due to the fact that I am not using a TPU (I am using a GPU GTX 1080 now). If I use run the above code on a TPU do you think I wont be getting any more errors?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["`bfloat16` support isn't complete for GPUs, as it's not supported natively by the devices.\r\n\r\nFor performance you'll want to use float32 or float16 for GPU execution (though float16 can be difficult to train models with).  TPUs support bfloat16 for effectively all operations (but you currently have to migrate your model to work on the TPU).", "Thank you, Russell! To run on TPUs, is it possible to migrate my model without using the TPU Estimator API (as recommended in TPU user documents)? Also, the second question is - while writing the TPU compatible model, is it possible to define trainable variables as either bfloat16 or float32 type (just like in a regular tensorflow code)? I want to test training performance using both bfloat16 and float32 types. ", "When training with bfloat16 we recommend using float32 for variables, and bfloat16 for operations.  There is a helper scope to make this easier: https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/bfloat16_scope\r\n\r\nUnfortunately, we don't have a documented API outside of TPUEstimator at the moment.  This will likely come in TF 1.11.  If you're using a Keras model, you can use `tf.contrib.tpu.keras_to_tpu_model` to convert your model.  You can also inspect  `keras_support.py` for examples of the lower-level interface.  At this time we can't support the use of it, so you're on your own in the short-term if you want to go this route.", "Thank you very much! I will keep variables in FP32 as suggested. Can you suggest how can we specify either bfloat16 or float32 type for operations?", "We normally use the bfloat16_scope to automatically adjust operations\ninside, e.g.:\n\nwith tf.contrib.tpu.bfloat16_scope():\n  x = y * z\n\nYou can also specify an explicit `dtype` on some ops, or cast the inputs to\nbfloat16 (ops preserve the type of their inputs).\n"]}, {"number": 21316, "title": "Fix typo in markdown", "body": "Fix missing `", "comments": ["This is for 1.9 branch. I dont think we plan on merging anything but major fixes to 1.9 from now on. Please submit this to master if it is still a typo there. Thanks!"]}, {"number": 21315, "title": "building tensorflow shared library in windows debug mode (cmake) /MDd", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r 1.9\r\n### Describe the problem\r\nWe get many linking error for building tensorflow shared library in windows when debug mode is selected\r\n(/MDd switch in VC++). We get many error messages like\r\n\r\n> mismatch detected for 'RuntimeLibrary': value 'MD_DynamicRelease' doesn't match value 'MDd_DynamicDebug' \r\n\r\nAfter digging into the issue, I noticed the linker is using wrong GRPC libs. \r\n\r\n`\r\ndouble_conversion\\src\\double_conversion\\double-conversion\\$(Configuration)\\double-conversion.lib\r\nzlib\\install\\lib\\zlibstaticd.lib\r\ngrpc\\src\\grpc\\Release\\grpc++_unsecure.lib\r\ngrpc\\src\\grpc\\Release\\grpc_unsecure.lib\r\ngrpc\\src\\grpc\\Release\\gpr.lib\r\nsnappy\\src\\snappy\\$(Configuration)\\snappy.lib\r\n`\r\nApparently, the cmake script is using the release version of grpc regardless of the config. I changed it manually and it was fixed. It would be nice if this fix is included in the future patches. \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Have you tried bazel? We are no longer supporting the cmake build for windows now that bazel build for windows is working. You can do bazel build -c dbg for a bazel build to do debug.", "@gunan, do you know who has tried this?", "I am not sure who would have tried this.\r\nAt one point in the past, we had the debug build working, but afaik not anymore.\r\n\r\nAlso, we are dropping the support for the cmake build. so I will mark this Community support.", "@amirhessam  - Is this still an issue with the latest tensorflow version ?", "> @amirhessam - Is this still an issue with the latest tensorflow version ?\r\n\r\nHi,\r\nI haven't tested the latest release. It may be fixed but as mentioned in the previous comments, cmake build is getting deprecated. Probably, it is not fixed.", "Hi @amirhessam ,\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21315\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21315\">No</a>\n"]}, {"number": 21314, "title": "[INTEL MKL] Fusing convolution with elementwise operators.", "body": "", "comments": ["@agramesh1 Sorry this fell through the cracks. Would you mind resolving conflicts? I'll review after that.", "Nagging Assignee @case540: It has been 48 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Temporarily closing this PR as there are a number of other PRs in progress that will cause conflicts with this. I will reopen later."]}, {"number": 21313, "title": "Feature request : sparse tensor in map_fn", "body": "Hello,\r\n\r\nI find that a class of useful ops is missing from Tensorflow : the ability to return outputs of various length while iterating over a tensor. Ideally it would support GPU.\r\n\r\nI tried to return a sparse_tensor inside map_fn but currently if fails with (TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor )\r\n\r\nIt would be useful to have a way to raster, i.e., iterate over a tensor and apply a function which can return tensor of different shapes.\r\n\r\nFor example, input tensor could be the coords of the vertices of a triangle, output for each triangle the list of pixels which would be turned on. We can then use unsorted_segment_xxx on the joint list.\r\n\r\nAnother example would be for a tensor [[start1,end1],[start2,end2],...,[startn,endn]] -> concat( [range(start1,end1), range(start2,end2),..., range(startn,endn)] )\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "You could fake-return a SparseTensor by returning the tuple of its elements (index, values, and dense_shape) but this would likely fail because tf would be incapable of properly stacking the contents.\r\n\r\ntf.map_fn wasn't really designed for the thing you want, though. It expects the shapes of the outputs to be uniform so it can stack them.\r\n\r\nI think you should try implementing the behavior you want with tf.while_loop directly (tf.map_fn is just a thin wrapper over that anyway); maybe using tf.contrib.autograph to rewrite python code into an efficient while loop.\r\n\r\nClosing as intended behavior."]}, {"number": 21312, "title": "Timeline equivalent for C++", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: bazel\r\n- **TensorFlow version (use command below)**:r1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: clang 6\r\n- **CUDA/cuDNN version**: 9/7.1\r\n- **GPU model and memory**:  1080ti / 11gb\r\n- **Exact command to reproduce**:\r\nn/a\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhat is the timeline profiler equivalent for Tensorflow C++ api ?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["You can fetch `StepStats` in C++ by fetching it from `RunMetadata` provided to `Session::Run()` after setting the appropriate `RunOptions`.\r\n\r\nTo format that information as say a chrome trace for visualizations you'll have to use a Python program that feeds the `StepStats` to a `Timeline` object.\r\n\r\nHope that helps.", "@asimshankar Is there any documentation that I could refer to?", "@dhingratul : Unfortunately, this is not well documented. I can provide some pointers though:\r\n\r\n- Python class that can generate a chrome trace format from `StepStats` is in https://github.com/tensorflow/tensorflow/blob/d4ad9c73969c45d1a224ebfc43eb645b9860216b/tensorflow/python/client/timeline.py#L346\r\n- `Session::Run` in C++ fills in the `RunMetadata` proto if provided https://github.com/tensorflow/tensorflow/blob/959f075558b33674c201367aef4bfc9c2dc116c4/tensorflow/core/public/session.h#L150\r\n- The tracing level can be set in `RunOptions` - https://github.com/tensorflow/tensorflow/blob/08db4a7b13c18a149ded2e5c023e4267364e47d6/tensorflow/core/protobuf/config.proto#L417\r\n\r\nApologies for the lack of documentation, but hopefully the pointers help.\r\nIf you'd like to contribute documentation, that would be awesome :)", "@asimshankar The pointers were of some help, however, I am still unsure how to \"write out\" the step_stats so that i can import them using a Python program calling the Timeline class you mentioned. This is what i have so far,\r\n\r\n`\r\n    tensorflow::RunOptions run_options;\r\n    run_options.set_trace_level(tensorflow::RunOptions::FULL_TRACE);\r\n    tensorflow::RunMetadata run_metadata;\r\n    //run the inference\r\n    std::vector<tensorflow::Tensor> output_tensors;\r\n    tensorflow::Status status = session_->Run(run_options, feed_dict, output_names_, {}, &output_tensors, &run_metadata);\r\n    std::ofstream fid(\"temp\");\r\n    fid<<run_metadata.step_stats();\r\n`\r\n\r\nAlso, when using run_metadata, my inference times increase by almost 2x, which is a very big problem", "Enabling tracing is expensive, so you don't want to turn it on for every step, just use it for sampling.\r\nTo export it, you export it like any protocol buffer - save the serialized version to a file/string and then load that up in Python using something like:\r\n\r\n```python\r\nfrom tensorflow.core.framework.step_stats_pb2 import StepStats\r\n\r\nstep_stats = StepStats()\r\nstep_stats.ParseFromString(<the serialized form of the proto from C++>)\r\n```\r\n\r\nHope that helps.\r\n", "@asimshankar Do I have to use boost serialization to achieve this, as I am unable to cast metadata.step_stats as char and write out as ofstream, as no conversion operator is registered. I am also unable to find the definition of metadata.step_stats(), as i can just iterate over each item and stream it out. What is the easiest way to accomplish this, given for this particular experiment, i don't care about inference times.", "Use protobuf serialization. All C++ protocol buffers (like `RunMetadata` and `StepStats` classes) have [serialization methods](https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.message_lite#serialization)", "@asimshankar Thanks for your help, for anyone's reference, this is a working version of code. You can add this snippet to a readme, wherever you find it relevant\r\n\r\nC++ snippet\r\n`    std::string outfile = \"serialized\";\r\n    run_metadata.step_stats().SerializeToString(&outfile);\r\n    std::ofstream ofs(\"Timeline\");\r\n    ofs << outfile; \r\n    ofs.close();`\r\n\r\nPython snippet\r\n`from tensorflow.core.framework.step_stats_pb2 import StepStats\r\nfrom tensorflow.python.client import timeline\r\nstep_stats = StepStats()\r\nwith open(\"Timeline\", \"rb\") as f:\r\n    step_stats.ParseFromString(f.read())\r\nx = timeline.Timeline(step_stats).generate_chrome_trace_format() \r\nwith open('data.json', 'w') as outfile:\r\n    outfile.write(x)`\r\n\r\nAlso look at this if you just need to print the trace\r\n`    tensorflow::StatSummarizer stat_summarizer(graph_def);\r\n    stat_summarizer.ProcessStepStats(run_metadata.step_stats());\r\n    stat_summarizer.PrintStepStats();\r\n    }`"]}, {"number": 21311, "title": "Share the mean and var of batch norm when Multi gpus training", "body": "I want to know if using estimator to train on multi gpus can share the mean and var of batchnorm.\r\n\r\nAs the example in cifar, it cannot deal with the batchnorm sharing between different gpu", "comments": ["Duplicate of #18222", "Closing it, duplicate issue. "]}, {"number": 21310, "title": "Getting slow video straming while testing a model and by using ROS ", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:('v1.9.0-0-g25c197e023', '1.9.0')\r\n- **GPU model and memory**:Quadro M1000M , 4 Gb \r\n     Bazel version: NA\r\n     CUDA/cuDNN version:NA\r\n     Exact command to reproduce: NA\r\n     Mobile device : NA\r\n\r\n \r\n### Describe the problem\r\nI trained a model in order to detect a hopper in a bakery using realsense camera D435 but  when i wanted to test and validate the model I wrote a program to stream the frames and to test the model but the stram was so slow so i was getting all the frames but with a big delay despite i am using my gpu and this is the code : \r\nPm: I am using also ROS for that. \r\n\r\n### Source code / logs\r\n#!/usr/bin/env python\r\nimport numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport zipfile\r\n\r\nfrom collections import defaultdict\r\nfrom io import StringIO\r\n#from matplotlib import pyplot as plt\r\n#from PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\n\r\nimport rospkg\r\nimport rospy\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\n# SET FRACTION OF GPU YOU WANT TO USE HERE\r\n#GPU_FRACTION = 0.4\r\n\r\nif tf.__version__ < '1.4.0':\r\n  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\r\n  \r\n# get an instance of RosPack with the default search paths\r\nrospack = rospkg.RosPack()\r\n# get the file path for rospy_tutorials\r\n\r\npath_to_learn_pkg = rospack.get_path('object-detection')\r\nresearch_module_path = os.path.join(path_to_learn_pkg,\"models/research\")\r\nobject_detection_module_path = os.path.join(path_to_learn_pkg,\"models/research/object_detection\")\r\nsys.path.append(object_detection_module_path)\r\n\r\n#print(sys.path)\r\n\r\nfrom object_detection.utils import ops as utils_ops\r\nfrom utils import label_map_util\r\n\r\nfrom utils import visualization_utils as vis_util\r\n\r\n# What model to download.\r\nMODEL_NAME = 'learned_model'\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\r\n\r\n#scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")\r\nfinal_path_to_ckpt = os.path.join(path_to_learn_pkg,PATH_TO_CKPT)\r\n\r\n# List of the strings that is used to add correct label for each box. In our case mira_robot\r\nPATH_TO_LABELS = os.path.join(path_to_learn_pkg, 'training/object-detection.pbtxt')\r\n\r\nNUM_CLASSES = 1\r\n\r\n\r\ndetection_graph = tf.Graph()\r\n\t\r\nwith detection_graph.as_default():\r\n  od_graph_def = tf.GraphDef()\r\n  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\r\n    serialized_graph = fid.read()\r\n    od_graph_def.ParseFromString(serialized_graph)\r\n    tf.import_graph_def(od_graph_def, name='')\r\n    \r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n\r\ndef load_image_into_numpy_array(image):\r\n  (im_width, im_height) = image.size\r\n  return np.array(image.getdata()).reshape(\r\n      (im_height, im_width, 3)).astype(np.uint8)\r\n      \r\n# Size, in inches, of the output images.\r\nIMAGE_SIZE = (12, 8)\r\n#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\ndef run_inference_for_single_image(image, graph):\r\n  with graph.as_default():\r\n\t    with tf.Session() as sess:\r\n\t      # Get handles to input and output tensors\r\n\t      ops = tf.get_default_graph().get_operations()\r\n\t      all_tensor_names = {output.name for op in ops for output in op.outputs}\r\n\t      tensor_dict = {}\r\n\t      for key in [\r\n\t\t  'num_detections', 'detection_boxes', 'detection_scores',\r\n\t\t  'detection_classes', 'detection_masks'\r\n\t      ]:\r\n\t\ttensor_name = key + ':0'\r\n\t\tif tensor_name in all_tensor_names:\r\n\t\t  tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\r\n\t\t      tensor_name)\r\n\t      if 'detection_masks' in tensor_dict:\r\n\t\t# The following processing is only for a single image\r\n\t\tdetection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\r\n\t\tdetection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\r\n\t\t# Reframing is required to translate the mask from box coordinates to image coordinates and fit the image size.\r\n\t\treal_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\r\n\t\tdetection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\r\n\t\tdetection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\r\n\t\tdetection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\r\n\t\t    detection_masks, detection_boxes, image.shape[0], image.shape[1])\r\n\t\tdetection_masks_reframed = tf.cast(\r\n\t\t    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\r\n\t\t# Follow the convention by adding back the batch dimension\r\n\t\ttensor_dict['detection_masks'] = tf.expand_dims(\r\n\t\t    detection_masks_reframed, 0)\r\n\t      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\r\n\r\n\t      # Run inference\r\n\t      output_dict = sess.run(tensor_dict,\r\n\t\t                     feed_dict={image_tensor: np.expand_dims(image, 0)})\r\n\r\n\t      # all outputs are float32 numpy arrays, so convert types as appropriate\r\n\t      output_dict['num_detections'] = int(output_dict['num_detections'][0])\r\n\t      output_dict['detection_classes'] = output_dict[\r\n\t\t  'detection_classes'][0].astype(np.uint8)\r\n\t      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\r\n\t      output_dict['detection_scores'] = output_dict['detection_scores'][0]\r\n\t      if 'detection_masks' in output_dict:\r\n\t\toutput_dict['detection_masks'] = output_dict['detection_masks'][0]\r\n  return output_dict\r\n  \r\n\r\nclass RosTensorFlow():\r\n    def __init__(self):\r\n        # Processing the variable to process only half of the frame's lower load\r\n        self._process_this_frame = True\r\n        self._cv_bridge = CvBridge()\r\n\t\r\n        self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\r\n        self._pub = rospy.Publisher('result', String, queue_size=1)\r\n        self.score_threshold = rospy.get_param('~score_threshold', 0.1)\r\n        self.use_top_k = rospy.get_param('~use_top_k', 5)\r\n        \r\n        \r\n\r\n    def callback(self, image_msg):\r\n        if (self._process_this_frame):\r\n            \r\n            image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\r\n    \r\n            # Expand dimensions since the model expects images to have shapes: [1, None, None, 3]\r\n            image_np_expanded = np.expand_dims(image_np, axis=0)\r\n            # Actual detection.\r\n            output_dict = run_inference_for_single_image(image_np, detection_graph)\r\n            # Visualization of the results of a detection.\r\n            vis_util.visualize_boxes_and_labels_on_image_array(\r\n                image_np,\r\n                output_dict['detection_boxes'],\r\n                output_dict['detection_classes'],\r\n                output_dict['detection_scores'],\r\n                category_index,\r\n                instance_masks=output_dict.get('detection_masks'),\r\n                use_normalized_coordinates=True,\r\n                line_thickness=8)\r\n            cv2.imshow(\"Image window\", image_np)\r\n            cv2.waitKey(1)\r\n        else:\r\n            pass\r\n        # We invert it\r\n        self._process_this_frame = not self._process_this_frame\r\n        \r\n        \r\n        \r\n    def main(self):\r\n        rospy.spin()\r\n\r\nif __name__ == '__main__':\r\n    rospy.init_node('search_hopper_node')\r\n    tensor = RosTensorFlow()\r\n    tensor.main()", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21309, "title": "iOS tensorflow audio recognition example (documentation)", "body": "Please provide an example, documentation for tensorflow audio recognition on iOS as you provide on Android [(example).](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/SpeechActivity.java)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code - NO\r\nOS Platform and Distribution - iOS\r\nTensorFlow installed from - [https://www.tensorflow.org](url)\r\nTensorFlow version - r1.8\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A\r\nMobile device - iOS", "@petewarden Can you take a look at this?", "We're actively working on porting the Android samples to iOS. Stay tuned!", "Targeting a release before the end of the year.", "The iOS audio sample can be found @ https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/ios. Feedback is welcome."]}, {"number": 21308, "title": "Update cwise_op_mul_2.cc", "body": "I think there's a typo.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "This PR seems like a duplicate of ...\r\nhttps://github.com/tensorflow/tensorflow/pull/21307"]}, {"number": 21307, "title": "Update cwise_op_mul_2.cc", "body": "Building for tf 1.9 for iOS and running custom network. Building with ANDROID_TYPES_FULL.\r\nSolve issue:\r\nNo OpKernel was registered to support Op 'Mul' with these attrs. Registered devices: [CPU], Registered kernels:\r\ndevice='CPU'; T in [DT_INT32]", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Please add some more context, or add an error or bug this change fixes, or why you think its a typo. Please reopen if you do. Thanks for the PR.", "Hello, @case540, it's fix this issue.\r\nBuilding for tf 1.9 for iOS and running custom network. Building with ANDROID_TYPES_FULL.\r\nNo OpKernel was registered to support Op 'Mul' with these attrs. Registered devices: [CPU], Registered kernels:\r\ndevice='CPU'; T in [DT_INT32]", "Please update your PR title/desc with this info/context. Thansk!", "CLAs look good, thanks!\n\n<!-- ok -->", "@PavelToropynya  gentle ping to check reviewer comments.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 21306, "title": "Support for NCE-loss/sampled softmax with dynamic_decode()", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: CUDA 9.1\r\n- **GPU model and memory**: TITAN Xp 12196MB * 8\r\n- **Exact command to reproduce**: NA\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nNCE loss/Sampled Softmax loss both require inputs along with the softmax-weight (used for multiplication to obtain logits). However, the current [sequence_loss()](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss) function in [dynamic_decode()](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode) function expects a loss function that works with logits, not these inputs. To provide these inputs to this custom loss function, the only method right now is to override the [cell ](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py#L137) being used, [return](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/seq2seq/python/ops/decoder.py#L265) their outputs (before passing through output layer) all the way up via its BasicDecoder. Then, this output can be used in the custom loss function.\r\n\r\nThere should be some support for an optional flag which lets one return the output state (before being passed through that matrix) as well, so that it may be passed to the custom loss function. TO compliment this, the custom loss function should also accept a loss in the format (inputs, outputs) instead if just (logits, outputs).\r\n\r\nI spent nearly a week into looking for a workaround for this and could not find anywhere, hence this issue/feature request.\r\n\r\nP.S.: If anyone has used NCE loss/sampled softmax loss with dynamic_decode() without going through all of this hassle, please let me know. There is nothing on Stack Overflow/Github/Tensorflow's examples about this (there is one example, but they used the logits as inputs to the sequence_loss's custom loss, which is wrong)", "comments": ["Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "CC @ebrevdo ", "@lmthang wdyt?", "Internally, we have implemented a sampled softmax loss for the NMT codebase https://github.com/tensorflow/nmt/blob/master/nmt/model.py#L498, but haven't pushed yet. I'll look into updating the codebase.", "The code has been updated. Check the loss here https://github.com/tensorflow/nmt/blob/master/nmt/model.py#L614 for sampled softmax", "This works! Thank you so much \ud83d\ude04 "]}, {"number": 21305, "title": "PS/Chief Nodes not terminating & Worker Nodes not accurately terminating with tf.estimator.train_and_evaluate", "body": "I'm using tf.estimator.train_and_evaluate of TF 1.9.\r\n\r\nWhen the training reaches the `max_steps` and `max_steps` is less than the dataset's size, the evaluator and the non-chief workers will terminate as expected. But, the ps nodes and the chief worker do not terminate automatically. They just stuck, and you have to kill them manually.\r\n\r\nWhen the `max_steps` is larger than the dataset's size, only one of the workers will terminate, which I guess is the one that reaches the end of the dataset. The remaining workers and ps nodes stuck.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It is.", "Can you describe your setup a bit more? How many workers / ps nodes, any sample code / minimal example you can provide so that we can reproduce and debug it?", "@rohan100jain I make a minimal example for linear regression. __When the training data is on the local device, only the ps nodes not terminate. When the training data is on the hdfs, it is as what I described above.__\r\n\r\nTo run the example locally:\r\n\r\nFirst, run `python gen_data.py` to generate data. Then, run \r\n\r\n`python model.py --ps_hosts=127.0.0.1:2222 --chief_hosts=127.0.0.1:2223 --worker_hosts=127.0.0.1:2224 --task_index=0 --job_name=ps`\r\n\r\n`python model.py --ps_hosts=127.0.0.1:2222 --chief_hosts=127.0.0.1:2223 --worker_hosts=127.0.0.1:2224 --task_index=0 --job_name=chief`\r\n\r\n`python model.py --ps_hosts=127.0.0.1:2222 --chief_hosts=127.0.0.1:2223 --worker_hosts=127.0.0.1:2224 --task_index=0 --job_name=worker`\r\n\r\n`python model.py --ps_hosts=127.0.0.1:2222 --chief_hosts=127.0.0.1:2223 --worker_hosts=127.0.0.1:2224 --task_index=0 --job_name=evaluator`\r\n\r\nthe code is as follows:\r\n\r\ngen_data.py\r\n```\r\nimport random\r\nimport tensorflow as tf\r\nimport os\r\n\r\nif __name__ == \"__main__\":\r\n    if not os.path.exists(os.path.join('.', 'data')):\r\n        os.makedirs(os.path.join('.', 'data'))\r\n    train_out = tf.python_io.TFRecordWriter('data/train.tfrecord')\r\n    for _ in range(10000):\r\n        x1 = (random.randint(1, 10000) - 5000) / 10000.0\r\n        x2 = (random.randint(1, 10000) - 5000) / 10000.0\r\n        x3 = (random.randint(1, 10000) - 5000) / 10000.0\r\n        y = 3 * x1 - 2 * x2 + 5 * x3\r\n        example = tf.train.Example(features=tf.train.Features(\r\n            feature={\r\n                'x': tf.train.Feature(float_list=tf.train.FloatList(value=[x1, x2, x3])),\r\n                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\r\n            }\r\n        ))\r\n        train_out.write(example.SerializeToString())\r\n    train_out.close()\r\n    test_out = tf.python_io.TFRecordWriter('data/test.tfrecord')\r\n    for _ in range(1000):\r\n        x1 = (random.randint(1, 10000) - 5000) / 10000.0\r\n        x2 = (random.randint(1, 10000) - 5000) / 10000.0\r\n        x3 = (random.randint(1, 10000) - 5000) / 10000.0\r\n        y = 3 * x1 - 2 * x2 + 5 * x3\r\n        example = tf.train.Example(features=tf.train.Features(\r\n            feature={\r\n                'x': tf.train.Feature(float_list=tf.train.FloatList(value=[x1, x2, x3])),\r\n                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\r\n            }\r\n        ))\r\n        test_out.write(example.SerializeToString())\r\n    test_out.close()\r\n\r\n```\r\n\r\nmodel.py\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport json\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_string(\"ps_hosts\", '', \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"chief_hosts\", '', \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"worker_hosts\", '', \"Comma-separated list of hostname:port pairs\")\r\ntf.app.flags.DEFINE_string(\"job_name\", '', \"One of 'ps', 'worker', 'chief', 'evaluator'\")\r\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\n\r\n\r\ndef input_fn(file_dir, num_epochs, num_workers=1, task_index=0):\r\n    def _parse_fn(record):\r\n        features = {\r\n            \"x\": tf.FixedLenFeature([3], tf.float32),\r\n            \"y\": tf.FixedLenFeature([1], tf.float32)\r\n        }\r\n        parsed = tf.parse_single_example(record, features)\r\n        label = parsed.pop(\"y\")\r\n        return parsed, label\r\n    dataset = tf.data.Dataset.list_files(file_dir, shuffle=False)\r\n    dataset = dataset.apply(tf.contrib.data.parallel_interleave(tf.data.TFRecordDataset, cycle_length=1))\r\n    dataset = dataset.map(_parse_fn, num_parallel_calls=32).prefetch(10)\r\n    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=4, count=num_epochs))\r\n    if num_workers > 1:\r\n        dataset = dataset.shard(num_workers, task_index)\r\n    dataset = dataset.batch(4)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    batch_features, batch_labels = iterator.get_next()\r\n    return batch_features, batch_labels\r\n\r\n\r\ndef set_dist_env():\r\n    ps_hosts = FLAGS.ps_hosts.split(',')\r\n    chief_hosts = FLAGS.chief_hosts.split(',')\r\n    worker_hosts = FLAGS.worker_hosts.split(',')\r\n    task_index = FLAGS.task_index\r\n    job_name = FLAGS.job_name\r\n    tf_config = {\r\n        'cluster': {'chief': chief_hosts, 'worker': worker_hosts, 'ps': ps_hosts},\r\n        'task': {'type': job_name, 'index': task_index}\r\n    }\r\n    print(json.dumps(tf_config))\r\n    os.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\n\r\ndef model_fn(features, labels, mode, params=None):\r\n    x = features[\"x\"]\r\n    w = tf.get_variable(name='w', dtype=tf.float32, shape=[3, 1], initializer=tf.glorot_normal_initializer())\r\n    b = tf.get_variable(name='b', dtype=tf.float32, shape=[1], initializer=tf.constant_initializer(0.0))\r\n    pred = tf.add(b, tf.matmul(x, w))\r\n    predictions = {\"prediction\": pred}\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        export_outputs = {\r\n            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs=export_outputs)\r\n    error = tf.reduce_mean(tf.square(labels - pred))\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, loss=error)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        train_op = tf.train.GradientDescentOptimizer(0.001).minimize(error, global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, loss=error, train_op=train_op)\r\n\r\n\r\ndef main(_):\r\n    tr_dir = 'data/train*'\r\n    va_dir = 'data/test*'\r\n    set_dist_env()\r\n    config = tf.estimator.RunConfig().replace(\r\n        session_config=tf.ConfigProto(device_count={'GPU': 0, 'CPU': 32}),\r\n        log_step_count_steps=10000,\r\n        save_summary_steps=10000,\r\n        save_checkpoints_steps=10000)\r\n    e = tf.estimator.Estimator(model_fn=model_fn, model_dir=\"model\", params={}, config=config)\r\n    num_workers = len(FLAGS.worker_hosts.split(\",\")) + 1\r\n    worker_index = 0\r\n    if FLAGS.job_name == \"worker\":\r\n        worker_index = int(FLAGS.task_index) + 1\r\n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(tr_dir, num_epochs=9999, num_workers=num_workers, task_index=worker_index), max_steps=200000)\r\n    feature_spec = {\r\n        \"x\": tf.FixedLenFeature([3], tf.float32),\r\n    }\r\n    serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\n    exporters = tf.estimator.LatestExporter(\"linear\", serving_input_receiver_fn)\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn=lambda: input_fn(va_dir, num_epochs=1),\r\n        steps=1000,\r\n        start_delay_secs=5,\r\n        throttle_secs=10,\r\n        exporters=exporters)\r\n    tf.estimator.train_and_evaluate(e, train_spec, eval_spec)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run()\r\n\r\n```", "@wuciawe  I didn't reproduce this problem with your code in TF r1.10.", "@weberxie Do you mean the ps, the chief, and the worker all will terminate properly when the training step reaches the max_steps?", "@wuciawe Only the ps still running when step reaches the max_steps.", "@weberxie So the problem remains.", "@wuciawe PS does not terminate is normal.", "@weberxie Interesting. How would you think the lack of automatic resource cleanup is normal for a machine learning framework in production. In a model training workflow, how to notify the workflow controller the termination of the training stage to terminate the remaining PSs and to update the trained model to online service in a programmable way?", "@ Have you killled the ps when using the estimator for the distributed tensorflow? I met the same problem,\r\nI try many ways, but it was still failed to shutdowm the ps.\r\n\r\nI view the code of the estimator. I only find the ps server is \"join()\".  but i want to close the PS\r\n\r\nthank you", "We met same issue in TF 1.12 like above described.\r\n\r\nset max_step in TrainSepc to 126800\r\n\r\n```python\r\n    model = build_estimator(build_model_column)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=FLAGS.max_step)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=6000, throttle_secs=120)\r\n    for index in range(Config.TRAIN_EPOCHS):\r\n        tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n        tf.logging.info('=' * 60)\r\n```\r\nThen after 126800 steps, the whole training stop, but `chief` still not exit\r\n```\r\nINFO:tensorflow:Saving checkpoints for 1268001 into hdfs://xxx\r\n```\r\n\r\nWe test same code in TF 1.7 and TF 1.8, chief can exit as expect, but in TF 1.10 and TF 1.12, chief didn't exit even max_step reached.", "We just tested TF 1.13 and found that chief can exit after max_step reached.", "> @weberxie Interesting. How would you think the lack of automatic resource cleanup is normal for a machine learning framework in production. In a model training workflow, how to notify the workflow controller the termination of the training stage to terminate the remaining PSs and to update the trained model to online service in a programmable way?\r\n\r\nIt is expected that the cluster resource manager would terminate PS. It could be a programmable framework like Apache Yarn or Kubernetes, or some scripts that kills PS after worker exits, or even a personnel who just does the job manually.", "> > @weberxie Interesting. How would you think the lack of automatic resource cleanup is normal for a machine learning framework in production. In a model training workflow, how to notify the workflow controller the termination of the training stage to terminate the remaining PSs and to update the trained model to online service in a programmable way?\r\n> \r\n> It is expected that the cluster resource manager would terminate PS. It could be a programmable framework like Apache Yarn or Kubernetes, or some scripts that kills PS after worker exits, or even a personnel who just does the job manually.\r\n\r\nI thought distributed tf was/would itself be that kind of framework, similar to that spark/flink can recover from failure and terminate after job finished in standalone mode without the help of YARN/K8S.\r\n\r\nPS Do the job manually is not expected for production usage.", "I\u2019m sorry, but apparently TF is not designed around the way you expected.", "We met same issue in TF 1.10 like above described, but no problem in TF 1.13", "I met the same issue in TF 1.14. \r\nI have 2 ps ,2 worker .Woker1 is finished successfully. But \"worker0\" is chief node , stuck in `Saving checkpoints for 10002 into hdfs:*****model.ckpt.`.Ps0 is the evaluate node, is finished successfully,ps1 is stuck in ` 2020-10-16 09:49:03.411776: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2222\r\n20/10/16 01:50:32 WARN util.NativeCodeLoader main: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable`\r\n\r\n", "> I met the same issue in TF 1.14.\r\n> I have 2 ps ,2 worker .Woker1 is finished successfully. But \"worker0\" is chief node , stuck in `Saving checkpoints for 10002 into hdfs:*****model.ckpt.`.Ps0 is the evaluate node, is finished successfully,ps1 is stuck in ` 2020-10-16 09:49:03.411776: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2222 20/10/16 01:50:32 WARN util.NativeCodeLoader main: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable`\r\n\r\nI try the same code in TF.1.12, it works!", "Hi There,\r\n\r\nWe are moving this issue to closed status, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.\r\nPlease open a new issue for any help you need against 2.x, and we will get you the right help.Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21305\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21305\">No</a>\n"]}, {"number": 21304, "title": "Time_Distributed Keras vs TF Keras ", "body": "Hi, \r\nthere is a Bug when using the Time_Distributed Layer from tf Keras:\r\n\r\n\r\n```\r\nfrom tensorflow.python.keras.layers import TimeDistributed,Input,Embedding,LSTM\r\ninu=Input(shape=(None,None))\r\nemb=Embedding(input_dim=100,output_dim=100)(inu)\r\nrnn=TimeDistributed(LSTM(100))(emb)\r\n\r\n```\r\nyou get the error: \r\n\r\n`as_list() is not defined on an unknown TensorShape.`\r\nIf I use the Time_Distributed from Keras directly like this, there is no error: \r\n\r\n```\r\nfrom keras.layers import TimeDistributed,Input,Embedding,LSTM\r\ninu=Input(shape=(None,None))\r\nemb=Embedding(input_dim=100,output_dim=100)(inu)\r\nrnn=TimeDistributed(LSTM(100))(emb)\r\n```\r\nWe can see the difference in how the Time_Distributed gets its shape, which is defined in line 163 in the keras definition [Keras](https://github.com/keras-team/keras/blob/master/keras/layers/wrappers.py#L114) they define a \"get_shape_tuple\" function and use that to not have to call the \"as_list\" as in TF Keras in line 164 [TF Keras](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/keras/layers/wrappers.py)\r\n\r\n\r\nHave I written custom code NO\r\nOS Platform and Distribution Ubuntu \r\nTensorFlow installed from Source\r\nTensorFlow version 1.9 \r\nBazel version NA \r\nCUDA/cuDNN version 9, 7.1\r\nGPU model and memory p100 \r\nExact command to reproduce See above\r\nMobile device NA", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@lysecret2 could you try a newer version of TF? ```pip install tf-nightly```. I'm not getting an error when running:\r\n\r\n```python\r\nfrom tensorflow.keras.layers import TimeDistributed,Input,Embedding,LSTM\r\ninu=Input(shape=(None,None))\r\nemb=Embedding(input_dim=100,output_dim=100)(inu)\r\nrnn=TimeDistributed(LSTM(100))(emb)\r\n```\r\n\r\n(Note Keras is now under `tensorflow.keras` rather than `tensorflow.python.keras`)", "You are right. The problem does not exist using tf nightly. Thanks!"]}, {"number": 21303, "title": "Memory Issue with my Tensorflow Code", "body": "------------------------\r\n\r\n### System information\r\n\r\n**Have I written custom code: Yes\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\n TensorFlow installed from binary\r\nBazel version: N/A\r\nPython version 2.7\r\nTensorFlow version 1.9.0\r\nCUDA/cuDNN version CUDA 9.0\r\nGPU model and memory:  Nvidia 860M and memory 4 GB\r\nExact command to reproduce: N/A\r\nMobile device:N/A**\r\n\r\n**The problem:**\r\nI wrote a tensorflow code DDQN network to run with gpu and I am getting an Odd Memory problem. My processes list from system monitor shows that my memory utilized by the python code remains almost constant and increases very slowly as there is a increase in replay buffer size(Its list which stores samples). But in resources it shows that my memory is increasing steeply and is not analogous with the python program. For example when I start the code the memory utilized by the system is around 3.8 GB with python code taking around 1.5 GB of memory but after sometime like an hour or so the memory utilized by the system increases to 9-10 GB with the python code utilizing around 1.8GB. \r\n\r\n### Source code \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np \t\r\nfrom PIL import Image\r\nimport time\r\nimport os, os.path\r\nimport cPickle as pickle\r\nfrom colorama import Fore, Back, Style\r\nfrom collections import deque,OrderedDict\r\nfrom blist import *\r\nimport gc\r\n#import pylab as pl\r\nimport sys\r\nimport matplotlib.pyplot as plt\r\nfrom PIL import Image, ImageDraw, ImageEnhance\r\n\r\n\r\nTraining_Dataset_1=\"Imagenet_Sequencing_filtered.pickle\"\r\n#Training_Dataset_2=\"OTB_Dataset.pickle\"\r\nTraining_Dataset_2=\"Dataset_VOT.pickle\"\r\n\r\nTest_Dataset=\"OTB_Dataset.pickle\"\r\nResults_file=\"Test_Results.pickle\"\r\n#Replay_Memory_file=\"Replay_Memory.pickle\"\r\n\r\nimport Layers\r\nimport RNN_State_update\r\nimport One_Shot_Representation\r\nimport Image_Cropping_Resizing\r\nimport Getting_Pretrained_caffenet_Parameters\r\nimport Grid_Generation\r\nimport Others_Func\r\n\r\n\r\nweights=Getting_Pretrained_caffenet_Parameters.param()\r\n\r\nNumber_RNN_Cell_Units=512\r\nNumber_of_Actions=11\r\n\r\nMini_batch=32\t\t\t\t\t\t\t\t\t#Batch Size over which gradient would be evaluated\r\nMax_episodes=200\t\t\t\t\t\t\t\t\t\t\t\r\nK_iterations=20 \t\t\t\t\t\t\t\t#Number of Times to Sample and Perform gradient update before collecting new dataset\r\nDiscount_Factor=0.99\r\nNumber_of_Epochs=1\r\nNumber_of_Training_Videos_1=1000\r\nNumber_of_Training_Videos_2=70\r\nNumber_of_Testing_videos=94\r\nBatch_Size=1\r\nReplayMemory=200000\r\nWeight_Update_Step_Size=10000\t\t\t\t\t\t#Updatation of weights from Evaluation Network to target network\r\n\r\nsess=tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True))\r\n\r\nclass CNN_RNN_Layers():\r\n\r\n# This class combines all the CNNs\r\n\r\n\tdef __init__(self, input,Batch_Size):\r\n\r\n\t\tself.input=input \t\t\t\t#T-1 and T Images with cropped using he Groundtruth Function\r\n\r\n\t\tself.input_1=input[0:Batch_Size]\r\n\t\tself.input_2=input[Batch_Size: 2*Batch_Size]\r\n\r\n\t\tself.Batch_Size=Batch_Size\r\n\r\n\r\n\t\twith tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.weights_1=weights[0]\r\n\t\t\tself.bias_1=weights[1]\r\n\r\n\t\t\tself.conv1_input_1= Layers.Convolayer(self.input_1, self.weights_1, self.bias_1, group=1, stride=(4,4) , pad_type='VALID' , name='Convolution_Layer_1_input_1')\r\n\t\t\tself.conv1_max_pooled_input_1=tf.nn.max_pool(value=self.conv1_input_1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='Conv_1_Maxpool_input_1')\r\n\t\t\tself.conv1_normalized_input_1=tf.nn.local_response_normalization(self.conv1_max_pooled_input_1,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_1_Normalization_input_1')\r\n\r\n\t\twith tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.conv1_input_2= Layers.Convolayer(self.input_2, self.weights_1, self.bias_1, group=1, stride=(4,4) , pad_type='VALID' , name='Convolution_Layer_1_input_2')\r\n\t\t\tself.conv1_max_pooled_input_2=tf.nn.max_pool(value=self.conv1_input_2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='Conv_1_Maxpool_input_2')\r\n\t\t\tself.conv1_normalized_input_2=tf.nn.local_response_normalization(self.conv1_max_pooled_input_2,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_1_Normalization_input_2')\r\n\t\t\t\r\n\t\twith tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.weights_2=weights[2]\r\n\t\t\tself.bias_2=weights[3]\r\n\r\n\t\t\tself.conv2_input_1= Layers.Convolayer(self.conv1_normalized_input_1, self.weights_2, self.bias_2, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_2_input_1')\r\n\t\t\tself.conv2_max_pooled_input_1=tf.nn.max_pool(self.conv2_input_1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_2_Maxpool_input_1')\r\n\t\t\tself.conv2_normalized_input_1=tf.nn.local_response_normalization(self.conv2_max_pooled_input_1,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_2_Normalization_input_1')\r\n\r\n\t\twith tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.conv2_input_2= Layers.Convolayer(self.conv1_normalized_input_2, self.weights_2, self.bias_2, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_2_input_2')\r\n\t\t\tself.conv2_max_pooled_input_2=tf.nn.max_pool(self.conv2_input_2, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_2_Maxpool_input_2')\r\n\t\t\tself.conv2_normalized_input_2=tf.nn.local_response_normalization(self.conv2_max_pooled_input_2,depth_radius=1,bias=1,alpha=0.0001,beta=0.75,name='Conv_2_Normalization_input_2')\r\n\r\n\t\t\t\r\n\t\twith tf.variable_scope(\"conv3\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.weights_3=weights[4]\r\n\t\t\tself.bias_3=weights[5]\r\n\r\n\t\t\tself.conv3_input_1= Layers.Convolayer(self.conv2_normalized_input_1, self.weights_3, self.bias_3, group=1, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_3_input_1')\r\n\r\n\t\twith tf.variable_scope(\"conv3\", reuse=tf.AUTO_REUSE):\t\r\n\t\t\t\r\n\t\t\tself.conv3_input_2= Layers.Convolayer(self.conv2_normalized_input_2, self.weights_3, self.bias_3, group=1, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_3_input_2')\r\n\t\t\r\n\t\twith tf.variable_scope(\"conv4\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.weights_4=weights[6]\r\n\t\t\tself.bias_4=weights[7]\r\n\r\n\t\t\tself.conv4_input_1= Layers.Convolayer(self.conv3_input_1, self.weights_4, self.bias_4, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_4_input_1')\r\n\t\twith tf.variable_scope(\"conv4\", reuse=tf.AUTO_REUSE):\r\n\t\t\t\r\n\t\t\tself.conv4_input_2= Layers.Convolayer(self.conv3_input_2, self.weights_4, self.bias_4, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_4_input_1')\r\n\t\t\t\r\n\t\twith tf.variable_scope(\"conv5\", reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\tself.weights_5=weights[8]\r\n\t\t\tself.bias_5=weights[9]\r\n\r\n\t\t\tself.conv5_input_1= Layers.Convolayer(self.conv4_input_1, self.weights_5, self.bias_5, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_5_input_1')\r\n\t\t\tself.conv5_max_pooled_input_1=tf.nn.max_pool(self.conv5_input_1, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_5_Maxpool_input_1')\r\n\r\n\t\twith tf.variable_scope(\"conv5\", reuse=tf.AUTO_REUSE):\t\r\n\r\n\t\t\tself.conv5_input_2= Layers.Convolayer(self.conv4_input_2, self.weights_5, self.bias_5, group=2, stride=(1,1) , pad_type='SAME' , name='Convolution_Layer_5_input_2')\r\n\t\t\tself.conv5_max_pooled_input_2=tf.nn.max_pool(self.conv5_input_2, ksize=[1,3,3,1], strides=[1,2,2,1],padding='VALID', name='Conv_5_Maxpool_input_2')\t\t\r\n\t\t\t\r\n\t\t#Starting the Fully Connected Layers After the CNN. Here We use the inbuilt densely connected layer.\r\n\t\tself.input_fully_connected_input_1=tf.layers.flatten(self.conv5_max_pooled_input_1, name='Flatten_input_1')\r\n\t\tself.input_fully_connected_input_2=tf.layers.flatten(self.conv5_max_pooled_input_2, name='Flatten_input_2')\r\n\r\n\t\tself.concatenated_output=tf.concat([tf.reshape(self.input_fully_connected_input_1, shape=[Batch_Size, -1]), \r\n\t\t\t\t\t\t\t\t\t\t\ttf.reshape(self.input_fully_connected_input_2, shape=[Batch_Size, -1])], axis=-1)\r\n\r\nclass fully_connected():\r\n\r\n\t\tdef __init__(self, input, scope_name,Batch_Size):\r\n\r\n\t\t\tself.input=input \r\n\r\n\t\t\twith tf.variable_scope(scope_name) as scope:\r\n\t\t\t\t\t\t\t\t\t\t\t\r\n\t\t\t\twith tf.variable_scope('First_Fully_Connected', reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\t\t\tself.fully_connected_1_output=tf.layers.dense(inputs = self.input, units=4096, activation=tf.nn.relu, name=\"Dense_Layer_2\")\r\n\r\n\t\t\t\twith tf.variable_scope('Second_Fully_Connected', reuse=tf.AUTO_REUSE):\r\n\r\n\t\t\t\t\tself.fully_connected_2_output=tf.layers.dense(inputs = self.fully_connected_1_output, units=2048, activation=tf.nn.relu, name=\"Dense_Layer_3\")\r\n\r\n\t\t\t\twith tf.variable_scope('Output_Layer', reuse=tf.AUTO_REUSE):\r\n\t\t\t\r\n\t\t\t\t\tself.dense_output= Layers.Dense_Layer(self.fully_connected_2_output, Number_of_Actions, name='Action_Output_Layer')\r\n\t\t\t\t\tself.Action_Output=tf.argmax(self.dense_output, axis=-1, name='Output_Action_as_Given_by_Deep_Networks')\r\n\r\n\t\t\t\t\tself.max_Q_Values=tf.reduce_max(self.dense_output, axis=-1)\r\n\r\n\t\t\t\t\tself.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=scope.name)\r\n\t\t\t\t\tself.trainable_vars_by_name = {var.name[len(scope.name):]: var for var in self.trainable_vars}\r\n\r\n\r\n\t\tdef Q_Values_of_Given_State_Action(self, actions_, y_targets):\r\n\r\n\t\t\tself.dense_output=self.dense_output\r\n\r\n\t\t\tactions_=tf.reshape(tf.cast(actions_, tf.int32), shape=(Mini_batch,1))\r\n\t\t\tz=tf.reshape(tf.range(tf.shape(self.dense_output)[0]), shape=(Mini_batch,1) )\r\n\r\n\t\t\tindex_=tf.concat((z,actions_), axis=-1)\r\n\r\n\t\t\tself.Q_Values_Select_Actions=tf.gather_nd(self.dense_output, index_)\r\n\r\n\t\t\t#loss_=tf.divide((tf.reduce_sum (tf.square(Q_Values_Select_Actions-y_targets))), 2)\r\n\t\t\t#loss_= y_targets-Q_Values_Select_Actions\t\t#Getting the difference between the target network and evaluation network\r\n\t\t\tloss_=tf.reduce_mean(tf.square(self.Q_Values_Select_Actions-y_targets))\r\n\r\n\t\t\treturn loss_\r\n\r\n\r\n\r\n\r\ndef Target_Values(r_t,s_t_plus_1,termination_criteria):\r\n\r\n\t#For DDQN\r\n\r\n\ts_t_plus_1=np.reshape(s_t_plus_1, newshape=(32,18432))\r\n\r\n\tOutput_Q_Values_Eval=sess.run((Eval_Network_Q_Values), feed_dict={x_Eval_Net:s_t_plus_1})\t#Getting the Q-Values from evaluation network\r\n\taction_=np.argmax(Output_Q_Values_Eval,axis=1)\t#Getting the actions array\r\n\tOutput_Q_Values_target=sess.run((Q_Values_Target), feed_dict={x_Target_Values:s_t_plus_1})\t#Getting the Q-Values from target network\r\n\r\n\tQ_values_selected=[]\r\n\tfor temp ,a in zip(range(32), action_):\r\n\r\n\t\tQ_values_selected.append(Output_Q_Values_target[temp][a])\r\n\ty_target_values=Others_Func.Y_Target(Q_values_selected, r_t,Discount_Factor)\r\n\r\n\tfor i in range(32):\r\n\t\tif(termination_criteria[i]==True):\r\n\t\t\ty_target_values[i]=r_t[i]\r\n\r\n\treturn y_target_values\r\n\r\n\r\ndef Evaluation_Network(s_t, a_t, y_Targets ):\r\n\r\n\twith tf.variable_scope('Eval_Network') as scope:\r\n\r\n\t\ts_t=np.reshape(s_t, newshape=(32, 18432))\r\n\r\n\t\ttotal_loss,_=sess.run((loss,train), feed_dict={x_Eval_Net:s_t, a_t_eval:a_t,y_Targets_eval:y_Targets})\t#Train the network and get the loss\r\n\r\n\t\treturn total_loss #Return the loss\r\n\r\n\r\ndef DQN(samples_):\r\n\t\r\n\ts_t,a_t,r_t,s_t_plus_1, termination_criteria=Others_Func.samples_seperation_generation(samples_)\r\n\r\n\tTargets=Target_Values(r_t,s_t_plus_1,termination_criteria)\r\n\tEval=Evaluation_Network(s_t,a_t, Targets)\t\t# Train the evaluation network and get the loss\r\n\r\n\tdel(s_t, a_t,r_t,s_t_plus_1)\r\n\tgc.collect()\t\r\n\treturn Eval \t#Return the loss\r\n\r\nwith tf.device('/device:GPU:0'):\r\n\r\n\tx_sampling=tf.placeholder(dtype=tf.float32,shape=(Batch_Size*2,227,227,3), name=\"x_sampling\") \t#Images resized to 224x224x3\r\n\r\n\tCNN_RNN_Layer_Sampling=CNN_RNN_Layers(x_sampling,Batch_Size)\t\t#Creating the Object for CNN-RNN Layers\r\n\tstate_eval_sampling=CNN_RNN_Layer_Sampling.concatenated_output\r\n\t\r\n\t##########################################\r\n\tx_Target_Values=tf.placeholder(dtype=tf.float32,shape=(None, 18432), name=\"x_Target_Values\") \t#Images resized to 224x224x3\r\n\tfully_connected_Target=fully_connected(x_Target_Values,'Target_Network',Mini_batch)\t#Creating the Object for CNN-RNN Layers\r\n\tQ_Values_Target=fully_connected_Target.dense_output\r\n\ttarget_max_Q_Value=fully_connected_Target.max_Q_Values\r\n\ttarget_vars = fully_connected_Target.trainable_vars_by_name\r\n\r\n\t################################################################\r\n\tx_Eval_Net=tf.placeholder(dtype=tf.float32,shape=(None,18432), name=\"x_Eval_Net\") \t#Images resized to 224x224x3\r\n\ta_t_eval=tf.placeholder(dtype=tf.int32,shape=(Mini_batch), name=\"a_t_eval\")\r\n\ty_Targets_eval=tf.placeholder(dtype=tf.float32,shape=(Mini_batch), name=\"y_Targets_eval\")\r\n\t\r\n\tfully_connected_Eval=fully_connected(x_Eval_Net,  'Eval_Network',Mini_batch)\t\t#Creating the Object for CNN-RNN Layers\r\n\tEval_Network_Q_Values=fully_connected_Eval.dense_output \t\t\t\t\t\t\t\t\t#Getting the Q-Values os the current state using the evaluation network\r\n\tEval_Network_max_Q_Value=fully_connected_Eval.max_Q_Values\r\n\teval_vars = fully_connected_Eval.trainable_vars_by_name\r\n\t\r\n\tloss=fully_connected_Eval.Q_Values_of_Given_State_Action(a_t_eval,y_Targets_eval)\r\n\toptimizer = tf.train.AdamOptimizer(learning_rate=0.0001,beta1=0.9)\r\n\ttrain = optimizer.minimize(loss, var_list=tf.trainable_variables(scope='Eval_Network'))\r\n\r\n\t#######################\r\n\r\n\tcopy_ops =[target_var.assign(eval_vars[var_name]) for var_name, target_var in target_vars.items()]\r\n\tcopy_online_to_target = tf.group(*copy_ops)\r\n\r\n\t######################\r\n\tinit_op = tf.global_variables_initializer()\r\n\tsess.run(init_op)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Oh Sorry, I have now updated it with all possible information I have. Thanks.", "@sunjeet95 is this a bug? Did your system crash? Or are you just worried about memory use but you're able to keep working? ", "@cy89  Yes my system did crash after memory reached it maximum with terminal saying Killed! ", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@asimshankar can you offer any hints, or redirect, please?", "@sunjeet95 : Could you add some color to what you're measuring, how you're measuring etc.? For example, you say \"My processes list from system monitor shows that my memory utilized by the python code remains almost constant and increases very slowly as there is a increase in replay buffer size(Its list which stores samples). But in resources it shows that my memory is increasing steeply and is not analogous with the python program\"\r\n\r\nHow are you measuring \"memory utilized by the python code\" and what does \"in resources\" refer to?\r\n\r\nAlso, the code snippet seems quite large. Can you reduce it to a smaller, more focused script that demonstrates the problem?"]}, {"number": 21302, "title": "Quantized model not working in Tensorflow Lite iOS", "body": "I am working on Tensorflow-Lite in iOS. A non quantized .pb file when converted to .tflite it works fine, but when I tried converting a working quantized model (pb) to .tflite, with command below:\r\n\r\n   ```\r\n bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n   --input_file=/pathOfQuantizedFile/xyz.pb \\\r\n   --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n   --output_file=/pathOfQuantizedFile/xyz.tflite   --inference_type=FLOAT \\\r\n   --input_type=FLOAT --input_arrays=input_node   --output_arrays=output_node    \r\n   --input_shapes=1,256,256  --allow_custom_ops\r\n```\r\n\r\nThe file gets converted to .tflite successfully but gave this following error when used in the iOS app:\r\n\r\n> Didn't find custom op for name 'RSQRT'\r\n> Didn't find custom op for name 'Dequantize'\r\n> Didn't find custom op for name 'ReorderAxes'\r\n> Registration failed.\r\n> Failed to construct interpreter\r\n\r\nTried this with both TensorFlow version - r1.8,  r1.9 and r1.10.\r\n\r\nHave I written custom code - No\r\nOS Platform and Distribution - MacOS\r\nTensorFlow - N/A\r\nTensorFlow version - r1.8\r\nBazel version - 0.11.0\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nMobile device", "How did you quantize your model? If you are just doing weight quantization, I suggest you start with your non-quantized  model and pass --quantize_weights to toco.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It looks like you are using a TF quantized graph (made with the old graph transform tool) instead of a TFLite quantized graph.\r\n\r\nMake sure you follow the quantization instructions here https://www.tensorflow.org/performance/quantization . We are working on some easier to use tools as well, for TensorFlow Lite quantization.\r\n\r\nClosing due to inactivity, please create a new issue if the above technique doesn't work."]}, {"number": 21301, "title": "After TensorFlow 1.1.0, libtensorflow_inference.so error: undefined reference to 'TF_NewGraph'", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "[symbol-r1.9.txt](https://github.com/tensorflow/tensorflow/files/2251801/symbol-r1.9.txt)\r\n[symbol-r1.10.txt](https://github.com/tensorflow/tensorflow/files/2251802/symbol-r1.10.txt)\r\n\r\n**nm -D libtensorflow_inference.so**\r\n\r\nI want to find the symbols, not in the new version.\r\n\r\n000a2848 T TF_AbortWhile\r\n000a0484 T TF_AddControlInput\r\n000a01f4 T TF_AddInput\r\n000a0208 T TF_AddInputList\r\n0009f7c8 T TF_AllocateTensor\r\n0009f9dc T TF_CloseDeprecatedSession\r\n000a17bc T TF_CloseSession\r\n000a0488 T TF_ColocateWith\r\n0009f40c T TF_DataTypeSize\r\n0009f908 T TF_DeleteBuffer\r\n0009fa74 T TF_DeleteDeprecatedSession\r\n000a27b4 T TF_DeleteGraph\r\n000a423c T TF_DeleteImportGraphDefOptions\r\n000a0144 T TF_DeleteLibraryHandle\r\n000a1854 T TF_DeletePRunHandle\r\n000a284c T TF_DeleteSession\r\n000a7b48 T TF_DeleteSessionOptions\r\n0009f420 T TF_DeleteStatus\r\n0009f808 T TF_DeleteTensor\r\n0009f870 T TF_Dim\r\n000a7bcc T TF_ExtendGraph\r\n000ad650 T TF_FinishOperation\r\n000ae77c T TF_FinishWhile\r\n000ade3c T TF_FinishWhileHelper\r\n000aa674 T TF_GetAllOpList\r\n0009f920 T TF_GetBuffer\r\n0009f550 T TF_GetCode\r\n000a0134 T TF_GetOpList\r\n000a3530 T TF_GraphGetTensorNumDims\r\n000a530c T TF_GraphGetTensorShape\r\n000a7edc T TF_GraphImportGraphDef\r\n000a7dc8 T TF_GraphImportGraphDefWithReturnOutputs\r\n000a1330 T TF_GraphNextOperation\r\n000a59f0 T TF_GraphOperationByName\r\n000a4fe8 T TF_GraphSetTensorShape\r\n000a7ce0 T TF_GraphToGraphDef", "Could you fill out the rest of the form? In particular I'm wondering how you obtained the so file, did you build it yourself and how, or did you get it from a release?"]}, {"number": 21300, "title": "core dumped when using tflite_convert", "body": "When I use tflite_convert to get a tflite model, it coredumps everytime.\r\n\r\nThe model I trained is based on MobileNetV2 + SSD\uff0c and generate a pb file using export_inference_graph.py in object_detection.\r\n\r\nhere is the log:\r\n\r\n'2018-08-01 08:43:35.857541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.863758: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\\n2018-08-01 08:43:35.864497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864710: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864731: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\\n2018-08-01 08:43:35.864827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.864890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.864940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.864991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.865011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.865049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865110: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.865191: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865271: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.865300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.865351: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.865467: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865525: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.865546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.865580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.865662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.865728: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.865762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.865838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865885: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.865906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.865942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.865974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.866025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866075: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.866097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.866132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.866212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.866280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.866312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866345: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.866394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.866461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.866496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.866579: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.866649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.866681: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Unpack\\n2018-08-01 08:43:35.866760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Where\\n2018-08-01 08:43:35.866807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: NonMaxSuppressionV2\\n2018-08-01 08:43:35.866829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ZerosLike\\n2018-08-01 08:43:35.866874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\\n2018-08-01 08:43:35.866884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Equal\\n2018-08-01 08:43:35.867413: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ResizeNearestNeighbor\\n2018-08-01 08:43:35.907700: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2041 operators, 3390 arrays (0 quantized)\\n2018-08-01 08:43:35.954623: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 2001 operators, 3308 arrays (0 quantized)\\n2018-08-01 08:43:36.014150: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2001 operators, 3308 arrays (0 quantized)\\n2018-08-01 08:43:36.095756: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \\nAborted (core dumped)\\n'\r\n\r\nAnyone can help ? Thanks a lot", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Please use export_tflite_ssd_graph.py as described in this [post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193).", "@huangxiao2008 What detection checkpoint did you start from? What platform did you try the conversion on? Please provide exact reproducible instructions.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21299, "title": "Fix shapes in comments of nmt_with_attention.ipynb", "body": "It is a bit misleading and confusing that the output shape of decoder is currently commented as `(batch_size * max_length, vocab)`. However the correct shape should be `(batch_size * 1, vocab)`, since the input x of GRU layer has shape == `(batch_size, 1, embedding_dim + hidden_size)`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@googlebot I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "It also seems confusing to me that in the `evaluate` method, `tf.multinomial(tf.exp(predictions), num_samples=1)` is used to sample a prediction from the `predictions` logits. The usage of `tf.exp` seems redundant here, since `predictions` itself is the logits output of a dense layer in decoder.", "Anyone of the reviewers I added know who wrote or knows about the RNN attention models?", "@yashk2810 ", "@SayHiRay Nice catch. Would you mind making the same change in the text generation notebook? Its in the generative examples folder. And also other notebooks?\r\n\r\ntf.exp never gave the wrong answers because it increased the logits that were already high. Anayways, thanks for catching the mistake :)", "Hi Ray, \r\n\r\n`tf.multinomial` does expect un-normalized log-probabilities.\r\nThe `Decode` model is only ever called with input batch_size of 1.\r\n\r\nSo both of these changes look correct.", "Yeah. Sounds good to me. tf.multinomial expects logits.", "@yashk2810 @MarkDaoust Thanks for reviewing and mentioning about other notebooks. I have checked the usage of `tf.multinomial` in text_generation.ipynb and image_captioning_with_attention.ipynb, and found same mistakes. I have committed the changes. Please help take a look again. Thanks for help!", "LGTM. Thanks for the fix \ud83d\ude00", "@MarkDaoust I think this needs a \"ready to pull\" label", "Yes!", "Does this get merged automatically? I think it should get merged by the copybara bot right?", "Yeah, its currently in the process of being merged. Some internal test flakes have slowed it down", "I think the internal tests are failing because an owner hasn't approved it. It happened with me before.", "Just looks like a test flake to me. Tests are re-running now", "Sounds good \ud83d\ude00", "Thanks a lot for helping with merging! \ud83d\udc4f"]}]