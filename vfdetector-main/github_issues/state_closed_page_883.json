[{"number": 26996, "title": "Keras: Bug when creating a custom layer without inputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the expected behavior**\r\nI want to implement a Keras custom layer without any input, just trainable weights. This kind of layers is very important for visualization (e.g., [Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035))\r\n\r\n**Code to reproduce the issue**\r\n \r\n    class Simple(Layer):\r\n\r\n        def __init__(self, output_dim, **kwargs):\r\n           self.output_dim = output_dim\r\n           super(Simple, self).__init__(**kwargs)\r\n\r\n        def build(self):\r\n           self.kernel = self.add_weight(name='kernel', shape=self.output_dim, initializer='uniform', trainable=True)\r\n           super(Simple, self).build()  \r\n\r\n        def call(self):\r\n           return self.kernel\r\n\r\n        def compute_output_shape(self):\r\n           return self.output_dim\r\n\r\n    X = Simple((1, 784))()\r\n\r\n**Other info / logs**\r\n\r\nI am getting an error message:\r\n\r\n`__call__() missing 1 required positional argument: 'inputs'`", "comments": ["You can just create `__call__(self, inputs=None)`, can't you?", "@nairouz , did you ever write a complete version of this layer?"]}, {"number": 26995, "title": "2.0 Reference Models: BERT (1 GPU, 8 GPU with dist strat and Keras)", "body": "[**BERT**, or Bidirectional Encoder Representations from Transformers](https://github.com/google-research/bert), is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\r\n\r\nThe research team's academic paper describes BERT in detail and provides full results on a number of tasks: https://arxiv.org/abs/1810.04805.\r\n\r\nAn example of using BERT can be found [here](https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["We are working on this internally and will publish the code before the end of May-2019 assigning to me for now.  ", "https://github.com/tensorflow/models/tree/master/official/bert\r\n\r\nThe README has not been updated yet, but I think the information below will get you started.\r\n\r\nI am only testing the GPU version but it was build for TPU so that would have been tested first.  I do not have the flags to use but I have the test cases we run nightly.  Below is what I know, I have not been going even a little deep into BERT so I don't know how to state the accuracy numbers other than what we are tracking nightly.\r\n\r\nBERT (SQUAD):  ~.903\r\nBERT (Classify):  .8603 with mrpc\r\n\r\n   * [Test Case SQUAD](https://github.com/tensorflow/models/blob/master/official/bert/benchmark/bert_squad_benchmark.py): official.bert.benchmark.bert_squad_benchmark.BertSquadBenchmarkReal.benchmark_8_gpu\r\n   * [Test Case Classify](https://github.com/tensorflow/models/blob/master/official/bert/benchmark/bert_benchmark.py): official.bert.benchmark.bert_benchmark.BertClassifyAccuracy.benchmark_8_gpu_mrpc\r\n\r\nI realize it is a bit painful but you can extract the FLAGS from the tests above. We are still working on performance but we are already exceeding the TF 1.0 for both single and multi-gpu single machine.  FP16 support will be coming soon.  We are working on SQUAD first but the gains would apply to both...in most instances.  \r\n\r\n**Warning:** The data in gs://cloud-tpu-checkpoints is public and you can pull that down.  I did not make the actual training data public because I have not taken time to find out if it can be shared directly.  It likely can, but I like to be careful.\r\n\r\nI will help if I can; another group owns this but I deal with it daily as well for now."]}, {"number": 26994, "title": "2.0 Reference Models: BERT (TPU with dist strat and Keras)", "body": "[**BERT**, or Bidirectional Encoder Representations from Transformers](https://github.com/google-research/bert), is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\r\n\r\nThe research team's academic paper describes BERT in detail and provides full results on a number of tasks: https://arxiv.org/abs/1810.04805.\r\n\r\nAn example of using BERT can be found [here](https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["@dynamicwebpaige I'd like to contribute this.", "@dynamicwebpaige \r\nvery interested in this.  how can help?", "@rok \r\nare you working on this?  i think ill start.  would you like to collaborate?  im just getting into TF2..", "Hey @icanswim! Sure, it would be a pleasure.\r\nI didn't properly start yet because TPU support for TF2 seems to be lacking right now.\r\nI gave you access to [my fork](https://github.com/rok/models) or we can work via yours. I'll ping when I have some work done.", "Hey @icanswim, I've done some more work on top of your commit and I'll do some more (tests are passing but there are still warnings).\r\n\r\nI already [opened a PR](https://github.com/tensorflow/models/pull/6817), could you please sign the CLA there so it can be reviewed? ", "Sorry we missed this.  We are working on this internally and are almost done.  Expect code to be pushed before end of May-2019.  ", "Are there any updates on this?\r\n", "https://github.com/tensorflow/models/tree/master/official/bert\r\n\r\nThe README has not been updated yet, but I think the information below will get you started.\r\n\r\nI am only testing the GPU version but it was build for TPU so that would have been tested first.  I do not have the flags to use but I have the test cases we run nightly.  Below is what I know, I have not been going even a little deep into BERT so I don't know how to state the accuracy numbers other than what we are tracking nightly.\r\n\r\nBERT (SQUAD):  ~.903\r\nBERT (Classify):  .8603 with mrpc\r\n\r\n   * [Test Case SQUAD](https://github.com/tensorflow/models/blob/master/official/bert/benchmark/bert_squad_benchmark.py): official.bert.benchmark.bert_squad_benchmark.BertSquadBenchmarkReal.benchmark_8_gpu\r\n   * [Test Case Classify](https://github.com/tensorflow/models/blob/master/official/bert/benchmark/bert_benchmark.py): official.bert.benchmark.bert_benchmark.BertClassifyAccuracy.benchmark_8_gpu_mrpc\r\n\r\nI realize it is a bit painful but you can extract the FLAGS from the tests above. We are still working on performance but we are already exceeding the TF 1.0 for both single and multi-gpu single machine.  FP16 support will be coming soon.  We are working on SQUAD first but the gains would apply to both...in most instances.  \r\n\r\n**Warning:** The data in gs://cloud-tpu-checkpoints is public and you can pull that down.  I did not make the actual training data public because I have not taken time to find out if it can be shared directly.  It likely can, but I like to be careful.\r\n\r\nI will help if I can; another group owns this but I deal with it daily as well for now.\r\n\r\n", "We will publish tutorial/colab to help go through the training process.\r\nI will first update README to explain basics like checkpoints and training data generation. (ETA: by the end of this week)\r\n", "Thank you, @saberkun! Closing out this issue - and excellent work! \ud83d\ude04 "]}, {"number": 26993, "title": "remove sentence about usage of rmsprop for RNN", "body": "There is not enough evidence for the following sentence.\r\n\r\n```\r\nThis optimizer is usually a good choice for recurrent\r\nneural networks.\r\n```", "comments": []}, {"number": 26992, "title": "Update docstring of tf.linspace.", "body": "Add braces to the documentation to avoid misunderstandings in the way the\r\nincrement is computed. Given the way that the help page is rendered and a\r\nstrict mathematical (or programmatic) understanding, one might misread the\r\nformula as\r\n`stop - (start / num) - 1`\r\nwhich would be incorrect. While it is relatively obvious what linspace\r\ndoes, the braces avoid any ambiguity.", "comments": ["closing this issue as there is no recent activity."]}, {"number": 26991, "title": "[TF2.0][FR] Skip TFRecord files with 'DataLossError: corrupted record at'", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-dev20190319\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn this version of TF and prior versions, TFRecord files get corrupted or are detected as corrupt spontaneously during training. This is a problem already heavily discussed in e.g https://github.com/tensorflow/tensorflow/issues/13463 . Unfortunately, no one came up with an solution or a script triggering the problem, yet. This makes debugging what causes the files to change(?) during training quite hard.\r\n\r\nThus, it would be nice to be able to indicate to Tensorflow that such files should be ignored and only a warning is printed instead of canceling the whole training process. \r\n\r\nAn exemplary error message:\r\n```cmd\r\nTraceback (most recent call last):\r\n[...]\r\n    obj = distributed_train(summary_writer)\r\n  File \"/home/meyerjo/code/kaggle/nuclei-detection-tf2/nucleidetection/train.py\", line 297, in distributed_train\r\n    lambda x: train_step(x, summary_writer), train_iterator)\r\n  File \"/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 470, in experimental_run\r\n    args = (input_iterator.get_next(),) if input_iterator is not None else ()\r\n  File \"/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/distribute/input_lib.py\", line 145, in get_next\r\n    self._iterators[i].get_next_as_list(new_name))\r\n  File \"/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/distribute/input_lib.py\", line 415, in get_next_as_list\r\n    data_list = self._iterator.get_next_as_optional()\r\n  File \"/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 311, in get_next_as_optional\r\n    self._device_iterators[i]))\r\n  File \"/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 689, in get_next_as_optional\r\n    output_shapes=iterator._element_structure._flat_shapes),\r\n  File \"/home/meyerjo/code/testfield-tf-nightly/tf-nightly/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1858, in iterator_get_next_as_optional\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 852968336\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]] [Op:IteratorGetNextAsOptional]\r\n\r\n```\r\n", "comments": ["Reassigning to @jsimsa for triage (but maybe `tf.data.experimental.ignore_errors()` would solve the problem here?).", "@meyerjo as per @mrry's suggestion, have you tried using [ignore_errors](https://www.tensorflow.org/api_docs/python/tf/data/experimental/ignore_errors)?", "I think that has went under my radar. As in the other issues discussed it seems to work around the error message. Thanks!\r\n\r\nMay I leave this open, as TFRecord Files still get spontaneously corrupted during training? As training should be readonly regarding those files, this seems to be a bug.", "@meyerjo thank you for your response. Unless you can provide us with an example that can be used to reproduce this issue, there is little value in keeping this issue open (as no further action will be taken).", "Ok. Then I will close the issue"]}, {"number": 26990, "title": "in nmt_with_attention,the gru layer in decoder confuse me", "body": "[nmt_with_attention file link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)\r\nIn the Decoder class, the gru layer is defined, and it is used in `call()` function, the code is:\r\n```python\r\noutput, state = self.gru(x)\r\n```\r\n\r\nMy question is, the gru cell state may not be updated? In the same batch, every timestamp use the same cell state? Is my understand correct?\r\n```python\r\noutput, state = self.gru(x,initial_state=context_vector)\r\n```\r\nI then use this code and retrained the model, there is no obvious difference. Maybe someone can help me...\r\n\r\nForgive my lame English.", "comments": ["The loss when I use context_vector as the gru's initial_state\r\n```\r\nEpoch 5 Batch 0 Loss 0.4456\r\n```\r\n\r\n=====updated in 3.24=====\r\nThe final output loss is\r\n```\r\nEpoch 10 Batch 374 Loss 0.1384\r\n```\r\n\r\nBut when I test the model, the result is worse, I don't know why...\r\nHere is some example:\r\n>The test using the code `gru(x)`\r\n>Input: <start> todavia estan en casa ? <end>\r\n>Predicted translation: we re still at home . <end> \r\n![image](https://user-images.githubusercontent.com/25661058/54873942-60b47200-4e1c-11e9-96ee-c9c0776b56c0.png)\r\n>Input: <start> trata de averiguarlo . <end>\r\n>Predicted translation: try to figure it out . <end> \r\n![image](https://user-images.githubusercontent.com/25661058/54873945-6e69f780-4e1c-11e9-8707-e89ee54c302b.png)\r\n\r\n---\r\n>The same test using the code `gru(x,context_vector)`\r\n>Input: <start> todavia estan en casa ? <end>\r\n>Predicted translation: are you at home ? <end> \r\n![image](https://user-images.githubusercontent.com/25661058/54873943-65792600-4e1c-11e9-9a80-4dcb82ad36b3.png)\r\n>Input: <start> trata de averiguarlo . <end>\r\n>Predicted translation: try to figure it out . <end> \r\n![image](https://user-images.githubusercontent.com/25661058/54873946-745fd880-4e1c-11e9-83d0-b3d84fc6fdf8.png)\r\n\r\nI'm so surprised at the results ... ", "@yashk2810 Can you PTAL? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "hi @ymodak \r\n\r\nI have the same confusion, it looks like that the Decoder is stateless.\r\n\r\n`output, state = self.gru(x)`\r\n\r\ndecoder call is called by time step, this will make decoder GRU stateless, there is no previous state passed to this time step.\r\nand after I change it to stateful:\r\n\r\n`output, state = self.gru(x, initial_state = hidden)` \r\n\r\nthe model's loss is increased compare to the stateless model\r\ncan you explain a little? thanks in advance"]}, {"number": 26989, "title": "Add fftshift, ifftshift, etc", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow is currently lacking a [fftshift](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftshift.html) operation. This has been repeatedly requested, see e.g. #23587 and [fftshift in tensorflow???????????!!!!!!!!!!!](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/tfCCukPtwI4).\r\n\r\n**Will this change the current api? How?**\r\nNo changes to current API, this is a pure addition.\r\n\r\n**Who will benefit with this feature?**\r\n* The FFT in tensorflow is currently different from e.g. numpy, see #10749, anyone in need of the numpy convention will benefit.\r\n* Anyone who wants to visualize the result of a fourier transform inside tensorflow\r\n\r\n**Any Other info.**\r\nNo", "comments": ["Thanks for filing the issue, @adler-j! Would you be wanting to add these DSP features to [`tf.signal`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal), or in `tensorflow/addons`?\r\n\r\ncc: @martinwicke ", "seems interesting @dynamicwebpaige , @adler-j  , @martinwicke   , i just implemented fftshift and ifftshift in tensorflow. \r\n[helper.py](https://gist.github.com/Gurpreetsingh9465/f76cc9e53107c29fd76515d64c294d3f)\r\nmam, now i just need to add these two lines\r\n`\r\nfrom tensorflow.python.ops.signal.helper import fftshift`\r\n`from tensorflow.python.ops.signal.helper import ifftshift\r\n`\r\nin this file [signal.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/signal/signal.py) . or any further steps require?\r\n\r\ncan you assign me this issue. \ud83d\ude04", "Are you interested in sending us a pull request to add this code, with unit tests, to tf.signal?\r\n", "yes sir #27075", "I am closing this issue as the related PR https://github.com/tensorflow/tensorflow/pull/27075 was merged already. Thanks!", "Regarding the new ops `fftshift` and `ifftshift`, currently it's impossible to use them without specifying the shape over the axes you want to shift (you get the following error: `TypeError: 'NoneType' object cannot be interpreted as an integer` when executing `axes = tuple(range(x.shape.ndims))`).\r\n\r\nI was wondering whether it was possible to implement them without having to specify entirely the shape of your input tensor. This would probably amount to a dynamic `roll` and I don't know whether that's possible (doesn't seem to be when reading [the doc](https://www.tensorflow.org/api_docs/python/tf/roll)).", "It's not clear to me either. Interested in giving it a try?\n\nOn Tue, Jul 30, 2019 at 5:33 AM Zaccharie Ramzi <notifications@github.com>\nwrote:\n\n> Regarding the new ops fftshift and ifftshift, currently it's impossible\n> to use them without specifying the shape over the axes you want to shift\n> (you get the following error: TypeError: 'NoneType' object cannot be\n> interpreted as an integer when executing axes =\n> tuple(range(x.shape.ndims))).\n>\n> I was wondering whether it was possible to implement them without having\n> to specify entirely the shape of your input tensor. This would probably\n> amount to a dynamic roll and I don't know whether that's possible.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26989?email_source=notifications&email_token=AAABHRPIW6X6UQCVWKG5BTTQCAYJ5A5CNFSM4HAGLPHKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3DZNAA#issuecomment-516396672>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLDDBE2LA6F2K43K3DQCAYJ5ANCNFSM4HAGLPHA>\n> .\n>\n\n\n-- \n - Alex\n", "Well at the moment I am a little bit busy, so it won't be possible for me in the near future (I found a workaround for my problem which is just to shift my data before and after the network).\r\nI am however interested in giving it a go at some point if noone tried before.", "Just a thought @alextp , but wouldn't this need to be implemented with eager execution? The `shift` would need to depend on the variable input shape so it can't be in a static graph I think.", "Static TF graphs can be shape-dynamic; they just cannot be dynamic on the number of tensors.", "yes yes of course (that's what I am doing right now), but if you want to do operations that depend on the shape of the tensor (unspecified beforehand), then you probably need eager execution, don't you?", "No, you can evaluate the shape dynamically (using tf.shape) and use tf.cond\nfor conditional computation if you need that.\n\nIt might be more convenient in eager mode, but it's possible without.\n\nOn Thu, Aug 1, 2019 at 9:25 AM Zaccharie Ramzi <notifications@github.com>\nwrote:\n\n> yes yes of course (that's what I am doing right now), but if you want to\n> do operations that depend on the shape of the tensor (unspecified\n> beforehand), then you probably need eager execution, don't you?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26989?email_source=notifications&email_token=AAEM57MA3VNSPFO5L26IXWDQCMFBFA5CNFSM4HAGLPHKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3LEAHQ#issuecomment-517357598>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEM57KHY522YAW5YPPXMBLQCMFBFANCNFSM4HAGLPHA>\n> .\n>\n", "Ok I see where I was mistaken.\r\n\r\nActually, the current `fftshift` and `ifftshift` work perfectly even without specifying the shape completely beforehand.\r\nThe problem appeared for me because I wanted to use those as part of a keras functional API Model, and therefore use a `Lambda` layer around them.\r\n\r\nA minimal (failing) example would be:\r\n```python\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.signal import ifftshift\r\n\r\nkspace = Input((640, None, 1), dtype='complex64')\r\nk_shifted = Lambda(ifftshift, output_shape=(640, None, 1), arguments={'axes': [0, 1]})(kspace)\r\n```\r\n\r\nwhich gives: `TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'\r\n` when executing `shift = [-int(x.shape[ax] // 2) for ax in axes]`.\r\n\r\nThat is because when called, the layer will be executed on the input which doesn't have its dimension specified yet, as opposed to a pure tensorflow case  where the shape would be specified when `ifftshift` is called.\r\nMaybe this isn't the place to ask, but is there a way to still build a keras functional model with `ifftshift`, or do I need to use the subclassing API?", "So I tried to play around and indeed, using `tf.shape(x)` instead of `x.shape` allows to use `fftshift` and `ifftshift` in a `Lambda` layer. I will make a PR for it. Should I open a corresponding issue?", "@zaccharieramzi No need to open another issue. I notice that you already opened an issue. That is sufficient for any discussion. Thanks!"]}, {"number": 26988, "title": "Refactoring comparisons invoke calls", "body": "Refactoring the duplicated source code comparison Eval functions", "comments": ["@renjie-liu The changes have modified to remove the macro dependency. Couldnt further refactor because the `comparisonFn` expect the type also when its getting passed. \r\nCould you please review the PR further.", "@shashishekhar Could you please review and approve this PR?", "Adding @jianlijianli who will be a better reviewer.\r\n", "@jianlijianli Could you PTAL and approve.", "Can one of the admins verify this patch?"]}, {"number": 26987, "title": "Installing tensorflow-gpu", "body": "Folks,\r\nIn many of the forums you see countless people strugging to install the tensorflow for GPU.\r\n\r\nIf NVIDIA is really a partner you need to create files that say libcublas.latest with a sym link to the version.\r\n\r\nIt's pretty annoying that i have to deinstall cuda 10.1, because tensorflow-gpu only works with cuda 10.0, and that others struggle because the file name is 10 vs 10.0, etc..\r\n\r\nthis part makes installing a few hour/day process. \r\n\r\nIt won't kill NVIDIA to have a few prior versions installed with the latest package.\r\n\r\nStart typing: libcublas.so.9.0... all you see is hits on file not found for libcublas.so.8, libcublas.so.8.0, libcublas.so.9, libcublas.so.9.0, libcublas.so.10.0, libcublas.so.10, etc...\r\n\r\nhow much longer are you going to let this problem last?\r\n\r\n\r\nAlso, stop being so conda focused. Look what oracle did to java once they had the world on it.\r\nstay with the open source standards. Conda is cr..p.\r\n\r\n\r\n", "comments": ["You don't have to uninstall CUDA 10.1. You only have to change the environment variables for your system or your virtualenv to the paths where CUDA 10.0 is located.\r\n\r\nThe error messages are not too beginner friendly, though. ", "Nvidia support gave me a lot of bad info.\r\n\r\nI found this to work:\r\n\r\n\r\n$wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\n$sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\n$sudo apt-key adv --fetch-keys $https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\n$sudo apt-get update\r\n$wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n$sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\n$sudo apt-get update\r\n\r\n\r\n$sudo apt-get install --no-install-recommends \\\r\n    cuda-10-0 \\\r\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\r\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\r\n\r\n\r\n$sudo apt-get update && \\\r\n        sudo apt-get install nvinfer-runtime-trt-repo-ubuntu1804-5.0.2-ga-cuda10.0 \\\r\n        && sudo apt-get update \\\r\n        && sudo apt-get install -y --no-install-recommends libnvinfer-dev=5.0.2-1+cuda10.0\r\n\r\n\r\n\r\n\r\n`\r\n$nvidia-smi\r\nThu Mar 21 12:14:00 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  On   | 00000000:A1:00.0 Off |                  N/A |\r\n| 41%   28C    P8     2W / 260W |  10634MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  On   | 00000000:C1:00.0  On |                  N/A |\r\n| 41%   35C    P5    22W / 260W |   1966MiB / 10986MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      3080      C   python3                                    10623MiB |\r\n|    1      3080      C   python3                                      155MiB |\r\n+-----------------------------------------------------------------------------+\r\n$ nvidia-smi nvlink -c -s\r\nGPU 0: GeForce RTX 2080 Ti (UUID: GPU-13e93ad2)\r\n         Link 0: 25.781 GB/s\r\n         Link 1: 25.781 GB/s\r\n         Link 0, P2P is supported: true\r\n         Link 0, Access to system memory supported: true\r\n         Link 0, P2P atomics supported: true\r\n         Link 0, System memory atomics supported: true\r\n         Link 0, SLI is supported: true\r\n         Link 0, Link is supported: false\r\n         Link 1, P2P is supported: true\r\n         Link 1, Access to system memory supported: true\r\n         Link 1, P2P atomics supported: true\r\n         Link 1, System memory atomics supported: true\r\n         Link 1, SLI is supported: true\r\n         Link 1, Link is supported: false\r\nGPU 1: GeForce RTX 2080 Ti (UUID: GPU-fecd8180)\r\n         Link 0: 25.781 GB/s\r\n         Link 1: 25.781 GB/s\r\n         Link 0, P2P is supported: true\r\n         Link 0, Access to system memory supported: true\r\n         Link 0, P2P atomics supported: true\r\n         Link 0, System memory atomics supported: true\r\n         Link 0, SLI is supported: true\r\n         Link 0, Link is supported: false\r\n         Link 1, P2P is supported: true\r\n         Link 1, Access to system memory supported: true\r\n         Link 1, P2P atomics supported: true\r\n         Link 1, System memory atomics supported: true\r\n         Link 1, SLI is supported: true\r\n         Link 1, Link is supported: false\r\n`", "We have updated the [gpu support page](https://www.tensorflow.org/install/gpu) on TF website with cuda requirements for TF version 1.13 and added linux gpu setup guide as well. Did you get a chance to take a look yet?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26987\">No</a>\n", "Above instructions (also on the TF homepage) worked for me perfectly, except for the last line, which returned with the following error:\r\n\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nStarting pkgProblemResolver with broken count: 1\r\nStarting 2 pkgProblemResolver with broken count: 1\r\nInvestigating (0) libnvinfer-dev:amd64 < none -> 5.0.2-1+cuda10.0 @un puN Ib >\r\nBroken libnvinfer-dev:amd64 Depends on libnvinfer5:amd64 < none | 5.1.2-1+cuda10.1 @un uH > (= 5.0.2-1+cuda10.0)\r\n  Considering libnvinfer5:amd64 0 as a solution to libnvinfer-dev:amd64 9999\r\n  Re-Instated cuda-license-10-1:amd64\r\n  Re-Instated libcublas10:amd64\r\n  Re-Instated cuda-cudart-10-1:amd64\r\n  Re-Instated libnvinfer5:amd64\r\nDone\r\nSome packages could not be installed. This may mean that you have\r\nrequested an impossible situation or if you are using the unstable\r\ndistribution that some required packages have not yet been created\r\nor been moved out of Incoming.\r\nThe following information may help to resolve the situation:\r\nThe following packages have unmet dependencies:\r\n libnvinfer-dev : Depends: libnvinfer5 (= 5.0.2-1+cuda10.0) but 5.1.2-1+cuda10.1 is to be installed\r\nE: Unable to correct problems, you have held broken packages.\r\n\r\nI had to state the following explicitly:\r\n\r\n$sudo apt-get install -y --no-install-recommends libnvinfer5=5.0.2-1+cuda10.0\r\n\r\nAfter that installing libnvinfer-dev=5.0.2-1+cuda10.0 also worked.\r\nThought it's worth mentioning here...", "@ymodak you should include \r\n\r\n`$sudo apt-get install -y --no-install-recommends libnvinfer5=5.0.2-1+cuda10.0` \r\n\r\ninto the [gpu support page](https://www.tensorflow.org/install/gpu)!\r\nIt took me a while to find this post.\r\nI also found another small issue which was easy to solve but maybe worth mentioning:\r\n[change of googles company name](https://medium.com/@andrea_45267/how-to-solve-the-changed-its-origin-value-from-google-inc-to-google-llc-apt-error-4675f891be1f)\r\nBesides these two small things the guide is super clear! Good work! Thanks!", "@ymodak I find a new problem, package `libnvinfer5=5.0.2-1+cuda10.0` and `libnvinfer-dev_5.0.2-1+cuda10.0_amd64.deb` are missing.\r\nI can not find these packages on http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/. \r\nHow to solve this problem? \r\nDoes tensorflow support libnvinfer-dev_5.1.2 ?", "> @ymodak you should include\r\n> \r\n> `$sudo apt-get install -y --no-install-recommends libnvinfer5=5.0.2-1+cuda10.0`\r\n> \r\n> into the [gpu support page](https://www.tensorflow.org/install/gpu)!\r\n\r\nQuite unfortunate that this fix has been posted for 4 months but Google hasn't bothered to add it to the Tensorflow GPU install instructions for Ubuntu.  Poor docs hurt adoption."]}, {"number": 26986, "title": "Replace deprecated usage of np.asscalar with np.ndarray.item()", "body": "[`numpy.asscalar()` is deprecated since version 1.16](https://github.com/numpy/numpy/blob/master/numpy/lib/type_check.py#L519-L548).\r\n\r\nThis PR replaces its usage with [`numpy.ndarray.item()`](https://www.numpy.org/devdocs/reference/generated/numpy.ndarray.item.html)", "comments": ["I am not a good reviewer for this change. Please redirect.", "@rthadur For better context, these are almost identical changes as already approved in #25261"]}, {"number": 26985, "title": "add support for aarch64 (jetson gpu local build)", "body": "Add support for building tensorflow 1.13+ with newer Bazel > 0.15.\r\n\r\nNewer bazel >0.15 add aarch64 type of cpu.\r\n[CPU.java](https://github.com/bazelbuild/bazel/blob/95d94fa1ebb5e173668f66fc24be10c24ea7dd45/src/main/java/com/google/devtools/build/lib/util/CPU.java#L28)\r\n\r\nNVIDIA's jetson platform (aarch64 platform) has a CUDA GPU. full version of tensorflow can work on it.\r\nhttps://docs.nvidia.com/deeplearning/dgx/install-tf-xavier/index.html", "comments": ["Any information for this PR status ?"]}, {"number": 26984, "title": "[TF 2.0 API Docs] tf.math modules, new endpoints, gen_math_ops.py", "body": "**System information**\r\n- TensorFlow version: 2.0 alpha\r\n- Doc links: \r\n`tf.math` module-related, including:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/log\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/less\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/less_equal\r\n... and **many more** - a lot of endpoints mentioned below affected\r\n\r\n**Describe the documentation issue**\r\n\r\nSimilar to [26530](https://github.com/tensorflow/tensorflow/issues/26530), [25802](https://github.com/tensorflow/tensorflow/issues/25802), [25846](https://github.com/tensorflow/tensorflow/issues/25846)\r\n\r\n(May be similar to [26532](https://github.com/tensorflow/tensorflow/issues/26532) but has to do with `gen_math_ops.py` not `[math_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py)`.)\r\n\r\n- **Incorrect links:**\r\nMissing links to ?non-existent `...python/ops/gen_math_ops.py`\r\n\r\nSince [1.10](https://github.com/tensorflow/tensorflow/blob/67a4cbbc7cacdbc33b2adb44f1b08b5a0dbc3186/RELEASE.md):\r\n```\r\nNew symbols have been added to the following modules: tf.debugging, tf.dtypes, tf.image, tf.io, tf.linalg, tf.manip, **tf.math**, tf.quantization, tf.strings\r\n```\r\n\r\n`...python/ops/gen_math_ops.py` has been referenced a bunch of times e.g. [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/log\r\n) but does not appear to exist or cannot be easily found. There are similar issues with examples, raises, etc for some if not all of the [following](https://github.com/tensorflow/tensorflow/blob/67a4cbbc7cacdbc33b2adb44f1b08b5a0dbc3186/RELEASE.md): \r\n```\r\nNew endpoints in tf.math module namespace: tf.math.acos, tf.math.acosh, tf.math.add, tf.math.asin, tf.math.asinh, tf.math.atan, tf.math.atan2, tf.math.atanh, tf.math.betainc, tf.math.ceil, tf.math.cos, tf.math.cosh, tf.math.digamma, tf.math.equal, tf.math.erfc, tf.math.exp, tf.math.expm1, tf.math.floor, tf.math.greater, tf.math.greater_equal, tf.math.igamma, tf.math.igammac, tf.math.invert_permutation, tf.math.less, tf.math.less_equal, tf.math.lgamma, tf.math.log, tf.math.log1p, tf.math.logical_and, tf.math.logical_not, tf.math.logical_or, tf.math.maximum, tf.math.minimum, tf.math.not_equal, tf.math.polygamma, tf.math.reciprocal, tf.math.rint, tf.math.rsqrt, tf.math.segment_max, tf.math.segment_mean, tf.math.segment_min, tf.math.segment_prod, tf.math.segment_sum, tf.math.sin, tf.math.sinh, tf.math.softplus, tf.math.softsign, tf.math.squared_difference, tf.math.tan, tf.math.unsorted_segment_max, tf.math.unsorted_segment_min, tf.math.unsorted_segment_prod, tf.math.unsorted_segment_sum, tf.math.zeta\r\n```\r\n\r\n- **Usage examples**\r\nNeeds examples for each of tf.math calculations e.g.\r\n`tf.math.log(42.)` returns\r\n```\r\n<tf.Tensor: id=5, shape=(), dtype=float32, numpy=3.7376697>\r\n```\r\n\r\n- **Raises**\r\nNot listed/defined. Obfuscated error from unexpected args (e.g. strings)\r\n\r\nE.g. `tf.math.log('hello log')` gives this long error message for a simple natural log calculation. As @dynamicwebpaige said [here](https://github.com/tensorflow/tensorflow/issues/25802) all tf.math.* operations and the operations they influence ... would experience these same obfuscated XLA errors.\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-7-208ff95958ff> in <module>()\r\n----> 1 tf.math.log('hello log')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in log(x, name)\r\n   5327       try:\r\n   5328         return log_eager_fallback(\r\n-> 5329             x, name=name, ctx=_ctx)\r\n   5330       except _core._SymbolicException:\r\n   5331         pass  # Add nodes to the TensorFlow graph.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in log_eager_fallback(x, name, ctx)\r\n   5376   _attrs = (\"T\", _attr_T)\r\n   5377   _result = _execute.execute(b\"Log\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 5378                              ctx=_ctx, name=name)\r\n   5379   _execute.record_gradient(\r\n   5380       \"Log\", _inputs_flat, _attrs, _result, name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: Could not find valid device for node.\r\nNode: {{node Log}}\r\nAll kernels registered for op Log :\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n [Op:Log]\r\n```\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nOk\r\n", "comments": ["I am not getting tf.math.log in math_ops.py", "Just a note for people working on this: if you add python doctests, please make sure to do so in Python files, not in `base_api/api_def` files as those are the APIs for other languages too.", "@8bitmp3 Looks like this was resolved. Can we close this issue? Thanks"]}, {"number": 26983, "title": "Fix misspellings of \"parameter\"", "body": "", "comments": ["@rmlarsen Could you PTAL and approve."]}, {"number": 26982, "title": "optimized RNNBatchStep source code", "body": "TODO(mirkov,raziel): replace this for-loop with a MACRO (or function)", "comments": ["@aselle Could you PTAL and approve.", "@joyalbin Could you please address the reviewer comments. Thanks!", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26981, "title": "Error file not found, when running protoc command. (use tensorflow  Objection detection api tutorial )", "body": "<\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 64 bit\r\n- \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.5.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n\r\n**Problem Description*\r\nI am trying to run \"introduction use tensorflow objection detection api tutorial\" \r\nhttps://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/  \r\n\r\n. I have installed protoc version 3.7.0 for my 64bit windows 10. I extracted the protoc file into the models-master/research directory. \r\n\r\nWhen i run the following command \r\n **protoc object_detection/protos/*.proto --python_out=** \r\n\r\ni get the following error \r\n\r\n **_### no such file or directory_** \r\n  \r\n\r\nHow do i solve this error ?\r\n\r\n", "comments": ["Is this given on the official website?\r\nPlease use official Tensorflow [Link](https://github.com/tensorflow/models/tree/master/research/object_detection) instead of third party tutorials.", "yeah the 'protoc' command was mentioned in the website, i gave you the link. ", "Did you run the ``` protoc object_detection/protos/*.proto --python_out=.``` command from the ```tensorflow/models/research/``` directory? Can you please confirm? Thanks!", "I ran it in the directory u mentioned and it run fine. But when I went on\nto run the code it could not recognize the  .proto files it had to import.\nGiving an error\nerror in \" cannot import from utils\"\n\n\nOn Fri, Mar 22, 2019, 10:35 PM ymodak <notifications@github.com> wrote:\n\n> Did you run the protoc object_detection/protos/*.proto --python_out=.\n> from command from the tensorflow/models/research/ directory? Can you\n> please confirm? Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26981#issuecomment-475710578>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuRmayObI9gO1ze0_LoUWK6GS5nYQ7AOks5vZRR8gaJpZM4cBIyJ>\n> .\n>\n", "Apologies for the delay in response. tensorflow/models repo can be a good platform to discuss tensorflow object detection api tutorial related questions. Please post on [tensorflow/models](https://github.com/tensorflow/models/issues) repo if still facing problems. Thanks!"]}, {"number": 26980, "title": "[TF2.0a0] Distribute Strategies and Summary Writer don't work together", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: python 3.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: TITAN X\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to use tf.summary at the same time as using distribute strategies. However, the call to `strategy.experimental_run` seems to end the scope of the `summary_writer`. I have initialized it properly, but it is not available to me. I have tried passing the SummaryWriter object directly as another parameter of the train_step but it does not work and fails with the error message. Somehow, the scope seems to be lost through the experimental_run call.\r\n\r\n```[ops/summary_ops_v2.py:608] ValueError: No step set via 'step' argument or tf.summary.experimental.set_step()```\r\n\r\nFor the most part, I followed this guide on how to work with DuplicateStrategies: \r\nhttps://www.tensorflow.org/alpha/tutorials/distribute/training_loops\r\n\r\n**Describe the expected behavior**\r\nBeing able to write to write to the summary from within the training loop.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef train_step(input, s_writer):\r\n  with s_writer.as_default():\r\n    tf.summary.scalar('test', 0)\r\n\r\ndef distributed_train(s_writer):\r\n  return strategy.experimental_run(\r\n    lambda x: train_step(x, s_writer), train_iterator)\r\n\r\nif __name__ == '__main__':\r\n  dataset = tf.data.Dataset.from_tensor_slices(\r\n    (tf.random.uniform([5, 100])))\r\n\r\n  strategy = tf.distribute.MirroredStrategy()\r\n  summary_writer = tf.summary.create_file_writer('/tmp/test')\r\n\r\n  with summary_writer.as_default() as summary_scope:\r\n    with strategy.scope():\r\n\r\n      train_iterator = strategy.make_dataset_iterator(dataset)\r\n      train_iterator.initialize()\r\n\r\n      tf.summary.experimental.set_step(0)\r\n      # This works\r\n      train_step(None, summary_writer)\r\n\r\n      # This fails\r\n      distributed_train(summary_writer)\r\n```\r\n", "comments": ["Thanks for reporting, we will take a look at it.", "Issue identified, fix is on the way (should be submitted today)", "Hi Johannes,\r\n\r\nCould you please test if https://github.com/tensorflow/tensorflow/commit/7bdb838313d322773908cf8368c03ec98117ad55 solves your issue?\r\n\r\nThanks!", "I guess it is already included in the most recent nightly, isn't it? It seems to be working now.", "Yeah nightly is less than one day old.\r\n\r\nGood to know it works for you :-) \r\n\r\nClosing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26980\">No</a>\n"]}, {"number": 26979, "title": "TfLite file close after usage generic fix", "body": "", "comments": ["@Dayananda-V could you please resolve the conflicts? Thanks!", "@gbaned \r\n\r\nClosing this PR due to write perssion denied, same changes will be track with new PR #28548."]}, {"number": 26978, "title": "All images were broken in Chinese document (404)", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link: https://www.tensorflow.org/guide/premade_estimators\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nAll images were broken in guide pages.\r\nFor example:\r\nhttps://www.tensorflow.org/images/tensorflow_programming_environment.png\r\nin \r\nhttps://www.tensorflow.org/guide/premade_estimators\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Everything's working fine on my browser\r\n<img width=\"1680\" alt=\"Screenshot 2019-03-21 at 5 52 04 PM\" src=\"https://user-images.githubusercontent.com/31190928/54751828-19eb3000-4c02-11e9-85ce-1813dca763e7.png\">\r\n", "```\r\ncurl 'https://www.tensorflow.org/images/tensorflow_programming_environment.png' -H 'authority: www.tensorflow.org' -H 'pragma: no-cache' -H 'cache-control: no-cache' -H 'upgrade-insecure-requests: 1' -H 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36' -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8' -H 'accept-encoding: gzip, deflate, br' -H 'accept-language: zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7' --compressed -I\r\nHTTP/1.1 200 Connection established\r\n\r\nHTTP/2 404\r\ncontent-type: text/html; charset=utf-8\r\nstrict-transport-security: max-age=31536000; includeSubdomains\r\nx-frame-options: SAMEORIGIN\r\nx-xss-protection: 0\r\nx-content-type-options: nosniff\r\nx-cloud-trace-context: fbf6907010d1ba7603f98fe89425f75e\r\ncontent-length: 38065\r\ndate: Thu, 21 Mar 2019 13:11:43 GMT\r\nserver: Google Frontend\r\nalt-svc: quic=\":443\"; ma=2592000; v=\"46,44,43,39\"\r\n\r\n\r\ncurl 'https://www.tensorflow.org/images/tensorflow_programming_environment.png' -H 'authority: www.tensorflow.org' -H 'pragma: no-cache' -H 'cache-control: no-cache' -H 'upgrade-insecure-requests: 1' -H 'user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36' -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8' -H 'accept-encoding: gzip, deflate, br'  -I\r\nHTTP/1.1 200 Connection established\r\n\r\nHTTP/2 200\r\ncontent-type: image/png\r\nstrict-transport-security: max-age=31536000; includeSubdomains\r\nx-frame-options: SAMEORIGIN\r\nx-xss-protection: 0\r\nx-content-type-options: nosniff\r\nx-cloud-trace-context: 07623ca89bcd658a37ee646a4b5fb73d\r\ncontent-length: 59845\r\ndate: Thu, 21 Mar 2019 13:15:29 GMT\r\nserver: Google Frontend\r\nalt-svc: quic=\":443\"; ma=2592000; v=\"46,44,43,39\"\r\n\r\n```\r\nMaybe it is because of the language. After removing `accept-language` it shows 200.\r\n\r\n\r\n@Ayush517 Is it still working in Chinese version?", "Had some server issues earlier that seemed to have been resolved. These both seem to work now:\r\n* https://tensorflow.google.cn/guide/premade_estimators?hl=zh-cn\r\n* https://www.tensorflow.org/guide/premade_estimators?hl=zh-cn\r\n\r\nPlease re-open if you run into this again. Thanks"]}, {"number": 26977, "title": "Added Testcase to cover shift scenario.", "body": "This is one of the TODO items.", "comments": ["@jianlijianli , thanks for the review, i have updated as per your suggestion, kindly check and approve.\r\n\r\nRegards\r\nAmit", "Hi Amit, thanks for updating the comment. It seems per_channel_multiplier, per_channel_shift as well bias are vectors of size 3 but the scale is only a scalar; that doesn't seem to be the way PopulateConvolutionQuantizationParams is meant to be used. Just curious, what kind of use case it is testing? Thanks.", "@jianlijianli , thanks for pointing this out, i have updated the code again, \r\nThis TC is basically to test the legacy quantization parameters, i mean the last if condition of the function PopulateConvolutionQuantizationParams . Hope this is ok.\r\n\r\nRegards\r\nAmit"]}, {"number": 26976, "title": "[TF 2.0 API Docs] tf.transpose", "body": "**System information**\r\n- TensorFlow version: 2.0 alpha\r\n- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/transpose\r\n\r\n**Describe the documentation issue**\r\n\r\n_Params:_ Minor thing - `tf.conj(tf.transpose(input))` should be formatted appropriately imho\r\n\r\n_Visuals:_ One would be useful\r\n\r\n_Raises:_ No raises listed\u2028/defined but would be useful for some folks\r\ne.g. Running this `tf.transpose([1, 2, 3], 262)` gives a long error because `perm` must be a vector\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-19-357e79a92891> in <module>()\r\n----> 1 tf.transpose([1, 2, 3], 262)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in transpose_v2(a, perm, conjugate, name)\r\n   1605     A transposed `Tensor`.\r\n   1606   \"\"\"\r\n-> 1607   return transpose(a=a, perm=perm, name=name, conjugate=conjugate)\r\n   1608 \r\n   1609 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in transpose(a, perm, name, conjugate)\r\n   1693           ret.set_shape(input_shape[::-1])\r\n   1694     else:\r\n-> 1695       ret = transpose_fn(a, perm, name=name)\r\n   1696     return ret\r\n   1697 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in transpose(x, perm, name)\r\n  10743       try:\r\n  10744         return transpose_eager_fallback(\r\n> 10745             x, perm, name=name, ctx=_ctx)\r\n  10746       except _core._SymbolicException:\r\n  10747         pass  # Add nodes to the TensorFlow graph.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in transpose_eager_fallback(x, perm, name, ctx)\r\n  10780   _attrs = (\"T\", _attr_T, \"Tperm\", _attr_Tperm)\r\n  10781   _result = _execute.execute(b\"Transpose\", 1, inputs=_inputs_flat,\r\n> 10782                              attrs=_attrs, ctx=_ctx, name=name)\r\n  10783   _execute.record_gradient(\r\n  10784       \"Transpose\", _inputs_flat, _attrs, _result, name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: perm must be a vector, not [] [Op:Transpose] name: transpose/\r\n```\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?**\r\nWill try\r\n", "comments": ["try tf.transpose(tensor, (axis0, axis1, ...))", "Assume you have a tensor T with shape (14, 28 ,3), now you want to transpose it to (3, 14, 28).\r\nYou can do this T = tf.transpose(T, (2, 0, 1))", "Fixed in nightly: https://www.tensorflow.org/api_docs/python/tf/transpose?version=nightly"]}, {"number": 26975, "title": "get \"virtual memory exhausted: Cannot allocate memory\" in Makefile", "body": "when installing Tensorflow models with:\r\nsudo make install     I get \"Error 1\" when making argmax_op_0\r\n\r\nre ran the make file several times after rebooting Raspberry Pi 3 B+\r\nand get same error each time.", "comments": ["Please provide following information. Thanks!\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Thanks for quick response.  All of my steps come EXACTLY from the MagicPi description of how to install tensorflow on a RaspberryPi 3B+  system.  The only unusual things I have is a 120GB SSD as my storage system.  Here are instructions:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\n\nI have inserted below answer to specific questions.\n\n \n\nI noticed that the two EXPORT steps listed were not being honored by the build so I manually caused them to be executed from the repository source: \n\ntensorflow/contrib/makefile/download_dependencies.sh\n\nsudo apt-get install -y autoconf automake libtool gcc-4.8 g++-4.8\n\ncd tensorflow/contrib/makefile/downloads/protobuf/\n\n./autogen.sh\n\n./configure\n\nmake\n\nsudo make install\n\nsudo ldconfig  # refresh shared library cache\n\ncd ../../../../..\n\nexport HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`\n\nexport TARGET_NSYNC_LIB=\"$HOST_NSYNC_LIB\"\n\n \n\n \n\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\\n OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\n\n \n\n \n\nA new \u201cenv\u201d shows both the exports as having a real path to a library now.  But when I build again (after using the \u201cclean\u201d step), \n\nI still get the \u201cvirtual memory exhausted \u201c error.  Here is \u201cclean\u201d step:\n\nmake -f tensorflow/contrib/makefile/Makefile clean\n\n \n\n \n\nFrom: ymodak <notifications@github.com> \nSent: Thursday, March 21, 2019 2:43 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: tjcooperforted <tjcooper@concentric.net>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] get \"virtual memory exhausted: Cannot allocate memory\" in Makefile (#26975)\n\n \n\nPlease provide following information. Thanks!\nSystem information\n\n*\tOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nRaspberryPi 3B+ running latest version Raspbian 4.14.98-v7+  armv71\n\n*\tMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n*\tTensorFlow installed from (source or binary):\n\nI think this is from source as listed above URL source\n\n*\tTensorFlow version:I\n\nI think this is 1.9    see web site listed above\n\n*\tPython version:\n\n3.7.3\n\n*\tInstalled using virtualenv? pip? conda?:\n\npip\n\n*\tBazel version (if compiling from source):\n*\tGCC/Compiler version (if compiling from source):\n\n4.8 as shown in the dump\n\n*\tCUDA/cuDNN version:\n*\tGPU model and memory:\n\nDescribe the problem\n\nEach time I do the make after the EXPORT it takes close to an hour and gives \u201cvirtual memory exhausted: Cannot allocate memory\u201d in Makefile at location :842 in argmax_op.o build.  Ends with error 1\n\nEach time I try a different build I use the \u201cclear\u201d step shown in the listing.\n\nProvide the exact sequence of commands / steps that you executed before running into the problem\n\nSequence listed above: error happens on the  \u201cmake\u201d step\n\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI \\\n OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\n\nI have tried BOTH with the no options for Raspberry PI and with the options show above for Raspberry Pi.  Same error.\n\nAny other info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\n\nHere is Log of last two compile steps before error and a dump of env to show what I have:\n\nLast couple of compile steps and env to show EXPORT:\n\ng++-4.8 --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -D__ANDROID_TYPES_SLIM__ -DRASPBERRY_PI -MT /home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/aggregate_ops.o -MMD -MP -MF /home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/dep//tensorflow/core/kernels/aggregate_ops.Td -I. -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/fft2d -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/double_conversion -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/absl -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/usr/local/include -c tensorflow/core/kernels/aggregate_ops.cc -o /home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/aggregate_ops.o\n\nIn file included from /home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:63:0,\n\n                 from /home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:81,\n\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n\n                 from ./tensorflow/core/kernels/aggregate_ops.h:21,\n\n                 from tensorflow/core/kernels/aggregate_ops.cc:22:\n\n/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/EventCount.h:173:56: warning: requested alignment 128 is larger than 64 [-Wattributes]\n\n     EIGEN_ALIGN_TO_BOUNDARY(128) std::atomic<uint64_t> next;\n\n                                                        ^\n\ng++-4.8 --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -D__ANDROID_TYPES_SLIM__ -DRASPBERRY_PI -MT /home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/argmax_op.o -MMD -MP -MF /home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/dep//tensorflow/core/kernels/argmax_op.Td -I. -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/fft2d -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/double_conversion -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/absl -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/usr/local/include -c tensorflow/core/kernels/argmax_op.cc -o /home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/argmax_op.o\n\nIn file included from /home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:63:0,\n\n                 from /home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:81,\n\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n\n                 from ./tensorflow/core/kernels/argmax_op.h:20,\n\n                 from tensorflow/core/kernels/argmax_op.cc:24:\n\n/home/pi/tf/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/EventCount.h:173:56: warning: requested alignment 128 is larger than 64 [-Wattributes]\n\n     EIGEN_ALIGN_TO_BOUNDARY(128) std::atomic<uint64_t> next;\n\n                                                        ^\n\nvirtual memory exhausted: Cannot allocate memory\n\ntensorflow/contrib/makefile/Makefile:842: recipe for target '/home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/argmax_op.o' failed\n\nmake: *** [/home/pi/tf/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/argmax_op.o] Error 1\n\n \n\n \n\n \n\npi@raspberrypi:~/tf/tensorflow $ env\n\nLC_ALL=en_US.UTF-8\n\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\n\n_LXSESSION_PID=508\n\nXDG_CONFIG_HOME=/home/pi/.config\n\nXDG_MENU_PREFIX=lxde-pi-\n\nLANG=en_US.UTF-8\n\nDISPLAY=:0.0\n\nOLDPWD=/home/pi/tf\n\nINFINALITY_FT_CHROMEOS_STYLE_SHARPENING_STRENGTH=0\n\nINFINALITY_FT_AUTOHINT_INCREASE_GLYPH_HEIGHTS=true\n\nINFINALITY_FT_CONTRAST=0\n\nNO_AT_BRIDGE=1\n\nXDG_VTNR=7\n\nSSH_AUTH_SOCK=/tmp/ssh-0JJwDndxHMH9/agent.508\n\nINFINALITY_FT_STEM_FITTING_STRENGTH=25\n\nINFINALITY_FT_GLOBAL_EMBOLDEN_X_VALUE=0\n\nXDG_SESSION_ID=c1\n\nXDG_GREETER_DATA_DIR=/var/lib/lightdm/data/pi\n\nUSER=pi\n\nDESKTOP_SESSION=LXDE-pi\n\nINFINALITY_FT_AUTOHINT_SNAP_STEM_HEIGHT=100\n\nINFINALITY_FT_GRAYSCALE_FILTER_STRENGTH=0\n\nINFINALITY_FT_GAMMA_CORRECTION=0 100\n\nQT_QPA_PLATFORMTHEME=qt5ct\n\nHOST_NSYNC_LIB=tensorflow/contrib/makefile/downloads/nsync/builds/default.linux.c++11/nsync.a\n\nPWD=/home/pi/tf/tensorflow\n\nHOME=/home/pi\n\nTEXTDOMAIN=Linux-PAM\n\nSSH_AGENT_PID=560\n\nXDG_SESSION_TYPE=x11\n\nXDG_DATA_DIRS=/usr/local/share:/usr/share/raspi-ui-overrides:/usr/share:/usr/share/gdm:/var/lib/menu-xdg\n\nXDG_SESSION_DESKTOP=lightdm-xsession\n\nTARGET_NSYNC_LIB=tensorflow/contrib/makefile/downloads/nsync/builds/default.linux.c++11/nsync.a\n\nINFINALITY_FT_WINDOWS_STYLE_SHARPENING_STRENGTH=10\n\nSAL_USE_VCLPLUGIN=gtk\n\nINFINALITY_FT_BRIGHTNESS=0\n\nTERM=xterm\n\nSHELL=/bin/bash\n\nXDG_SEAT_PATH=/org/freedesktop/DisplayManager/Seat0\n\nINFINALITY_FT_USE_VARIOUS_TWEAKS=true\n\nXDG_CURRENT_DESKTOP=LXDE\n\nGPG_AGENT_INFO=/run/user/1000/gnupg/S.gpg-agent:0:1\n\nSHLVL=1\n\nXDG_SEAT=seat0\n\nLANGUAGE=en_US.UTF-8\n\nINFINALITY_FT_BOLD_EMBOLDEN_Y_VALUE=0\n\nINFINALITY_FT_GLOBAL_EMBOLDEN_Y_VALUE=0\n\nINFINALITY_FT_AUTOHINT_HORIZONTAL_STEM_DARKEN_STRENGTH=10\n\nGDMSESSION=lightdm-xsession\n\nLOGNAME=pi\n\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\n\nXDG_RUNTIME_DIR=/run/user/1000\n\nXAUTHORITY=/home/pi/.Xauthority\n\nXDG_SESSION_PATH=/org/freedesktop/DisplayManager/Session0\n\nXDG_CONFIG_DIRS=/etc/xdg\n\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games\n\nINFINALITY_FT_FILTER_PARAMS=11 22 38 22 11\n\nINFINALITY_FT_USE_KNOWN_SETTINGS_ON_SELECTED_FONTS=true\n\nINFINALITY_FT_STEM_SNAPPING_SLIDING_SCALE=40\n\nINFINALITY_FT_STEM_ALIGNMENT_STRENGTH=25\n\nINFINALITY_FT_AUTOHINT_VERTICAL_STEM_DARKEN_STRENGTH=25\n\nINFINALITY_FT_BOLD_EMBOLDEN_X_VALUE=0\n\nINFINALITY_FT_FRINGE_FILTER_STRENGTH=0\n\n_=/usr/bin/env\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/26975#issuecomment-475413284> , or mute the thread <https://github.com/notifications/unsubscribe-auth/Auhpv2tgM2uuV2KvpbTpNlNfTc0c9eo7ks5vY_zVgaJpZM4cA2Ze> .  <https://github.com/notifications/beacon/Auhpv-cfKyFLK4yXdGP1Hm3PPR9oAs04ks5vY_zVgaJpZM4cA2Ze.gif> \n\n", "I forgot to mention in the description that the error always comes when building the module:\r\nargmax_op.o\r\n\r\nIf I do not \"clean\" the make setup, then another \"make\" will try to build only that module and will give the same virtual memory exhausted error.\r\n   tjcooper", "Python 3.7  is supported in TF 1.13 and above. Please switch to latest TF version and build again. Thanks!\r\n", "Thanks for update notice.\n\nI switched to a different build of my 1.9 TF and with other files that we not included I was finally able to get a successful build.  I now can do pet detection with Pi camera and all seems to be working\u2026\u2026However, the frame rate is slow.   Like .67 fps.   Is there anything for SSD that can run better?\n\n   Ted Cooper\n\n \n\nFrom: ymodak <notifications@github.com> \nSent: Monday, April 1, 2019 3:47 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: tjcooperforted <tjcooper@concentric.net>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] get \"virtual memory exhausted: Cannot allocate memory\" in Makefile (#26975)\n\n \n\nPython 3.7 is supported in TF 1.13 and above. Please switch to latest TF version and build again. Thanks!\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/26975#issuecomment-478773393> , or mute the thread <https://github.com/notifications/unsubscribe-auth/AuhpvyILDj34vHBBh2tHDVV0VdZqwgSyks5vcoxGgaJpZM4cA2Ze> .  <https://github.com/notifications/beacon/Auhpv4aybXd9TbRPSOp6daq8AE8egbUWks5vcoxGgaJpZM4cA2Ze.gif> \n\n", "Sorry I forgot to mention this.  I am still using TF 1.9 and with the alternate build script that DID NOT come from MagiPi (mirrored on GitHub), was able to build all of TF and get the SSD for animal detection, etc. working.  So the problem is NOT related to the TF 1.9 version.\r\n   Ted J Cooper", "It looks like the original problem has been solved, but there are still performance issues? The recommended approach for inference on the RPi is TensorFlow Lite, you can see a guide to building that here: https://www.tensorflow.org/lite/guide/build_rpi", "pete warden,\r\nDo not see why issue was closed.  I have followed all the suggestions and updates and still get the \"exceed virtual memory\" error.  Are you saying that I have to use TensorFlow Lite to overcome this problem?\r\n   tjcooperforted", "@tjcooperforted \r\nI wrote a script named as till:\r\n```\r\n#!/usr/bin/bash\r\nuntil $*\r\ndo\r\n  sleep 1\r\ndone\r\n```\r\nNow I run `till bazel build ..........`\r\n"]}, {"number": 26974, "title": "tf.keras.layers.Bidirectional is not equivalent to tf.nn.bidirectional_dynamic_rnn", "body": "In tf.nn.bidirectional_dynamic_rnn, we can set \"sequence_length\", which contains the actual lengths of the sequences in a batch. But there is no such setting in tf.keras.layers...\r\nSo, if each sample in a batch has a different actual length, how can I do bidirectional_dynamic_rnn in tf.keras.layers?", "comments": ["Use masking -- either directly though `tf.keras.layers.Masking`, or through \r\n`mask_zero` parameter of `tf.keras.layers.Embeddings`. As for masking, Keras has a notion of \"masked elements\", which are ignored by all subsequent layers (and also losses, so you no longer have to pass weights manually generated using `tf.sequence_mask`).", "```\r\nimport tensorflow as tf\r\n\r\noutputs = tf.concat(\r\n  values=tf.nn.bidirectional_dynamic_rnn(\r\n    cell_fw=tf.nn.rnn_cell.LSTMCell(5),\r\n    cell_bw=tf.nn.rnn_cell.LSTMCell(5),\r\n    inputs=tf.ones([2,10,5]),\r\n    sequence_length=[6,10],\r\n    dtype=tf.float32\r\n  )[0],\r\n  axis=2\r\n)\r\n```\r\n\r\nHow can I translate the above code into tf.keras.layers style?", "It seems that tf.keras.layers.RNN.call() accepts a \"mask\" argument, how can I use this argument to solve this problem? The document is not clear at all. ", "```\r\nimport tensorflow as tf\r\n\r\noutputs = tf.keras.layers.Bidirectional(\r\n  tf.keras.layers.LSTM(\r\n    units=5,\r\n    dropout=0.5,\r\n    return_sequences=True\r\n  )\r\n).apply(\r\n  inputs=tf.ones([2,10,5]),\r\n  mask=tf.expand_dims(input=tf.sequence_mask([6, 10]), axis=2),\r\n  training=True\r\n)\r\n```\r\nIs the above code correct?", "> Use masking -- either directly though `tf.keras.layers.Masking`, or through\r\n> `mask_zero` parameter of `tf.keras.layers.Embeddings`. As for masking, Keras has a notion of \"masked elements\", which are ignored by all subsequent layers (and also losses, so you no longer have to pass weights manually generated using `tf.sequence_mask`).\r\n\r\nIt seems that `tf.keras.layers.Masking` will mask the whole time-step? But what if each sample has a different actual length? ", "I would convert\r\n```python\r\noutputs = tf.concat(\r\n  values=tf.nn.bidirectional_dynamic_rnn(\r\n    cell_fw=tf.nn.rnn_cell.LSTMCell(5),\r\n    cell_bw=tf.nn.rnn_cell.LSTMCell(5),\r\n    inputs=tf.ones([2,10,5]),\r\n    sequence_length=[6,10],\r\n    dtype=tf.float32\r\n  )[0],\r\n  axis=2\r\n)\r\n```\r\nto\r\n```python\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Masking(mask_value=0.),\r\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5, return_sequences=True), merge_mode=\"concat\"),\r\n])\r\n\r\ninputs = tf.ones([2, 10, 5])\r\nmask = tf.expand_dims(tf.sequence_mask([6, 10], dtype=tf.float32), axis=-1)\r\n\r\nprint(model(inputs * mask))\r\n```\r\nOr you can use the `mask` directly using\r\n```python\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5, return_sequences=True), merge_mode=\"concat\"),\r\n])\r\n\r\ninputs = tf.ones([2, 10, 5])\r\nmask = tf.expand_dims(tf.sequence_mask([6, 10], dtype=tf.float32), axis=-1)\r\nprint(model(inputs, mask=mask))\r\n```\r\n\r\nThe `expand_dims` can usually be avoided, because inputs are usually two-dimensional, i.e., `[2, 10]`, and you expand them to `[2, 10, 5]` using an embedding layer.", "But they are still not equivalent, because tf.keras.layers does not support variable scope\r\nSee #27016", "Yes, they are not equivalent. But you can achieve masking, which seemed to me to be the main point of your issue. Personally, I like the TF 2.0/Keras way of masking (because they extend automatically to all following layers and also losses and metrics).", "I am a heavy user of \"graph + session\", maybe I should change my mind to fit into the new TF.", "About masking, any suggestion on #27010?", "Closing this issue since the original issue has been addressed. Feel free to reopen if have any further questions. Thanks!"]}, {"number": 26973, "title": "Tensorboard crashed when keras finish train in eager execution", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\ntensorboard crashed when keras finish training the model and do a record in tensorboard\r\n**Describe the expected behavior**\r\ntensorboard do the record normally\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` python\r\n  if True:\r\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss={\r\n            # use custom yolo_loss Lambda layer.\r\n            'yolo_loss': lambda y_true, y_pred: y_pred})\r\n\r\n        model.fit_generator(data_generator(files,batch_size, input_shape, anchors, num_classes),\r\n                    epochs=0, initial_epoch=0,\r\n                    steps_per_epoch=max(1, sum // batch_size),\r\n                    callbacks=[logging, checkpoint],\r\n                    validation_data=data_generator(val_files, batch_size, input_shape, anchors,num_classes,train=False),\r\n                    validation_steps=max(1, val_sum // batch_size))\r\n        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\r\n\r\n    # Unfreeze and continue training, to fine-tune.\r\n    # Train longer if the result is not good.\r\n    if True:\r\n        for i in range(len(model.layers)):\r\n            model.layers[i].trainable = True\r\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4),\r\n                      loss={'yolo_loss': lambda y_true, y_pred: y_pred})  # recompile to apply the change\r\n        print('Unfreeze all of the layers.')\r\n\r\n        model.fit_generator(data_generator(files,batch_size, input_shape, anchors, num_classes),\r\n                    epochs=1, initial_epoch=0, steps_per_epoch=max(1, sum // batch_size),\r\n                    callbacks=[logging,checkpoint, reduce_lr, early_stopping],\r\n                    validation_data=data_generator(val_files, batch_size, input_shape, anchors, num_classes,train=False),\r\n                    validation_steps=max(1, val_sum // batch_size))\r\n        model.save_weights(log_dir + 'trained_weights_final.h5')\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTraceback (most recent call last):\r\n  File \"/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/media/fangsixie/data/pycharm-2018.2.4/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/media/fangsixie/data/keras-yolo3/train.py\", line 192, in <module>\r\n    _main()\r\n  File \"/media/fangsixie/data/keras-yolo3/train.py\", line 78, in _main\r\n    validation_steps=max(1, val_sum // batch_size))\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1426, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 232, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs, mode=mode)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 251, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1159, in on_epoch_end\r\n    self._write_custom_summaries(step, logs)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1106, in _write_custom_summaries\r\n    summary_ops_v2.scalar(name, value, step=step)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 564, in scalar\r\n    return summary_writer_function(name, tensor, function, family=family)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 508, in summary_writer_function\r\n    should_record_summaries(), record, _nothing, name=\"\")\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py\", line 54, in smart_cond\r\n    return true_fn()\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 501, in record\r\n    with ops.control_dependencies([function(tag, scope)]):\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 562, in function\r\n    name=scope)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py\", line 675, in write_scalar_summary\r\n    writer, step, tag, value, name=name, ctx=_ctx)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py\", line 706, in write_scalar_summary_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/home/fangsixie/.conda/envs/tensorflow1.12/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/logdir:logs/000//N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: epoch_loss/", "comments": ["fixed"]}, {"number": 26972, "title": "Lite: Fully_connected Op bug-fix", "body": "1:> Move data type check to one central place, to improve code readability\r\n2:> 2 Bug fixed as part of above change\r\n\r\n    \r\n\r\n-  The code was prone to significant data loss, if user has given Input as int16 type which was explicitly type casted to int8/uint8, so restrict user input to int8/uint8 only.\r\n-  Null pointer access when Bias is not present at EvalShuffledQuantized()", "comments": ["@suharshs: Thanks for your valuable comments. I have handled all, please check and approve the changes, TIA!", "@suharshs : Gentle Reminder!", "@suharshs : Gentle Reminder!", "@suharshs : Thanks for your efforts in reviewing and providing valuable comments, all are addressed now, please check, Thanks!"]}, {"number": 26971, "title": "Lite: Fully_connected Op unnecessary logic removed", "body": "1:> Unnecessary check removed as it is clearly evident from code above\r\n2:> Shift negation is not required explicitly when computed, it should be binded with code where it is used, if needed\r\n3:> Confusing comment removed\r\n4:> If condition modified to imply quantized input", "comments": ["Over to @suharshs  for review."]}, {"number": 26970, "title": "Lite: Fully_connected Op code refactored", "body": "1:> TF_LITE_MACRO_DISPATCH --> remove this macro unused\r\n2:> Code refactored to accommodate multiple if() else ()\r\n3:> Error message improvised", "comments": []}, {"number": 26969, "title": "Batched matmul gives incorrect result on GPU with 32-bit precision and batch size >= 2 ** 16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): not sure\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffa\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: GeForce GTX 1080 Ti, 11178MiB\r\n\r\n**Describe the current behavior**\r\nWhen computing a batched matmul with 32-bit precision and batch size >= 2 ** 16, the first 2 ** 16 - 1 batch elements of the result are correct, and the remaining elements are arbitrary - often zero, but not always. When the `sess.run` call is made multiple times, the result is usually the same or similar. The bug only seems to occur when using a variable or placeholder.\r\n\r\n**Describe the expected behavior**\r\nEvery batch element of the result should be correct.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ns = (100000, 1, 1)\r\np = tf.placeholder(shape=s, dtype=tf.float32)\r\nx = tf.matmul(tf.ones(s), p)\r\n\r\nwith tf.Session() as sess:\r\n    r = sess.run(x, feed_dict={p: np.ones(s, dtype=np.float32)})\r\n    print(r[2**16 - 5:2**16 + 5, 0, 0])\r\n```\r\n\r\nTypical output:\r\n```\r\n[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\r\n```\r\n\r\n**Other info / logs**\r\n", "comments": ["Reproduced on:\r\n* Tensorflow 1.13.1\r\n* Cuda 10.0\r\n* Titan RTX\r\n* Installed from pip\r\n* Ubuntu 18.04.2 LTS\r\n\r\nI don't have a separate minimal reproducing example, but I have noticed that in my application, it is not the case that the prefix of the result is correct. In my application, I was doing trilinear interpolation manually on pixels of human-interpretable MRI images. Here is are several pictures of the output (viewed as normalized float 32s), which has a striking pattern -- it may be of help in diagnosing what went wrong. The problem happens only when I use  a GPU, and can be avoided if I wrap my `matmul` inside `with tf.device('cpu:0')`. Too my untrained eyes, this looks like some kind of issue where memory got deallocated too early, or overwritten, or maybe the dtype / stride size is wrong somewhere.\r\n\r\n<img width=\"317\" alt=\"Screen Shot 2019-06-07 at 3 22 17 PM\" src=\"https://user-images.githubusercontent.com/4404828/59136504-4d537600-8938-11e9-935a-a6c691391242.png\">\r\n<img width=\"322\" alt=\"Screen Shot 2019-06-07 at 3 22 10 PM\" src=\"https://user-images.githubusercontent.com/4404828/59136505-4d537600-8938-11e9-9e3c-5ac1d23b08ac.png\">\r\n<img width=\"314\" alt=\"Screen Shot 2019-06-07 at 3 22 03 PM\" src=\"https://user-images.githubusercontent.com/4404828/59136507-4d537600-8938-11e9-9f41-d22f8f7d1bec.png\">\r\n<img width=\"299\" alt=\"Screen Shot 2019-06-07 at 3 18 54 PM\" src=\"https://user-images.githubusercontent.com/4404828/59136508-4d537600-8938-11e9-8253-377d9061374d.png\">\r\n\r\n\r\n\r\n\r\n\r\n", "This is likely caused by cublasGemmBatchedEx function in the cuBLAS library. I will report this issue to NVIDIA after creating a small example outside of TensorFlow.", "I experienced the same bug and maybe this helps:\r\nIf matmul is first called with input tensors of a batch size below `2**16` **and then again** called with input tensors of a batch size `>=2**16`, all entries in the result above `2**16-1` are random memory.\r\n\r\nHere is a short [colab example](https://drive.google.com/open?id=1txY0rHNIeUkp7c38X65Hh4Cz6e_2_ZZF)\r\n\r\nThis error does not occur if matmul is initially called with input tensors of batch_size above `2**16-1`!\r\n\r\n<s>So my workaround will be initializing all matmul operations in my graph with large inputs.</s>\r\nDid not work in my main graph, maybe only works in the special case with zeros in the colab example...", "Duplicate of https://github.com/tensorflow/tensorflow/issues/31166\r\n\r\nDoesn't reproduce with 2.1.0-dev20191226", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26969\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26969\">No</a>\n"]}, {"number": 26968, "title": "TfLite floor_mod.cc compilation warning fix", "body": "", "comments": ["@Dayananda-V please resolve conflicts", "@rthadur \r\n\r\nconflicts resolved now.", "@karimnosseir can you please review this PR ", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26968) for more info**.\n\n<!-- need_author_consent -->", "Corrupt PR is replaced with #27369"]}, {"number": 26967, "title": "InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match:", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen I am trianging my model with tf.estimator and tf.data, this issue occurs:\r\n`InvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [128,64] vs. shape[1] = [127,16]`\r\nthe batch of data is 128, so the first feature' s is correct, the second wrong. \r\nDoes anyone else have the same problem? Thanks in advance.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]