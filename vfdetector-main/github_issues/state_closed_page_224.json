[{"number": 47864, "title": "micro: L2_POOL_2D PR3-5", "body": "PR steps 3 through 5 for the L2_POOL_2D operator as per Issue #47814\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47863, "title": "OOM errors running multiprocessing inference", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.65\r\n- GPU model and memory: NVIDIA GTX 1070 8 GB\r\n\r\n**Describe the current behavior**\r\nI have a set of images that I need to run an inference on. What I am trying to do is spawn 4 workers and each worker has 1/4 access to GPU's memory. I am getting OOM even if I restrict the memory to 1/8 per worker. Below is some of the code:\r\n\r\n```\r\ndef process_image(doc_paths):\r\n    model = models.Model(my_model)\r\n    for path in doc_paths:\r\n        ...do some work...\r\n        model.do_predict()\r\n        return 0 or 1\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    num_processes = 4\r\n    process_pool_executer = concurrent.futures.ProcessPoolExecutor(num_processes)\r\n\r\n    # Split documents into `num_processes` equal parts, so in our case -> 4 equal parts\r\n    for i, paths in enumerate(sub_file_paths):\r\n        print(f'==== Processing path {i} ====')\r\n        f = process_pool_executer.submit(process_image, paths)\r\n        futures.append(f)\r\n\r\n        for future in concurrent.futures.as_completed(futures):\r\n            r = future.result()\r\n            results.append(r)\r\n```\r\n\r\nWithin the `do_predict()` function, I import tensorflow and have tried both these memory allocation options:\r\n\r\n```\r\n# OPTION 1\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        tf.config.experimental.set_virtual_device_configuration(gpus[0], [\r\n            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n    except RuntimeError as e:\r\n        print(e)\r\n\r\n\r\n# OPTION 2\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\nBoth give me OOM errors. According to nvidia-smi my GPU has 8081 MiB free so allocating 1024 MB for each process should be fine. So I must be doing something or thinking of something in the wrong way. Any thoughts?\r\n\r\n\r\n**Describe the expected behavior**\r\nExpected behavior would be that I could run the above without OOM errors.\r\n\r\n\r\n**EDIT:**\r\nWhen running this script with one worker it works fine. And looking in task manager, my GPU usage does not get above 10%.\r\n\r\n**EDIT2:**\r\nI am actually using TF version 2.2 - not 2.3.", "comments": ["@mdable2 \r\nWould you want to [limit your gpu](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and try, also try on upgraded tf as in 2.4 the stable version and see if it helps.", "I have already tried limiting my gpu as seen in option #1 from above.\r\n\r\n```\r\n# OPTION 1\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        tf.config.experimental.set_virtual_device_configuration(gpus[0], [\r\n            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n    except RuntimeError as e:\r\n        print(e)\r\n```\r\nAnd I cannot upgrade my TF version as I am using the Object Detection API and the instructions stated to use TF 2.2.\r\n\r\nOr am I able to upgrade? Any other suggestions?\r\n", "@mdable2 \r\nCan you please check if the other two methods work and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47862, "title": "TensorFlow-Lite: need for ruy library", "body": "Hello\r\n\r\nFor an embedded software project, we are trying to integrate TensorFlow-Lite via buildroot (via CMake). In the CMakeLists.txt we find following option:\r\n\r\noption(TFLITE_ENABLE_RUY \"Enable experimental RUY integration\" OFF)\r\n\r\nHowever, even with this option disabled, the ruy library is still needed in order to compile TensorFlow-Lite. My question is if this is really necessary, or that with the right adaptions to the code it should in principle work without this libary. We want to minimize the amount of libraries we are taking in, also because ruy in its turn depends for example on the pyTorch cpuinfo library.\r\n\r\nWe are compiling for an ARM64 architecture.\r\n\r\nKind regards,\r\n\r\nWannes\r\n\r\n", "comments": ["Currently there is a code which always rely on RUY. So it's inevitable now. But I'll handle it later.\r\n\r\nBTW, if you're using aarch64, RUY gives performance improvement. Could you share why do you want to minimize library dependencies?", "Hello,\r\n\r\nAs we are running on an embedded system with a limited amount of memory, we want to avoid all libraries which we do not necessarily need. A performance hit is less of a problem for us.\r\n\r\nKind regards,\r\n\r\nWannes", "Acknowledged.\r\n\r\nBTW, if you're quite limited on memory, we should use TFLite Micro. ", "Thanks for the suggestion. I'll have a look into TFLite Micro. We are however running on an ARM A53, for which I do not immediately see support in the micro framework.", "It is correct that Ruy is an unconditional dependency for ARMv8 builds, but the dependency set within Ruy is not very large. For example, one can build Ruy without the cpuinfo library (but in that I don't believe the library would be able to choose optimized kernels based on  the detected platform). I think a good way to describe the situation is as follows: Ruy contains the optimized GEMM kernels used on ARMv8 that are necessary for good performance on that platform. Ruy needs cpuinfo in order to choose the proper kernels, again for good performance. So a minimal set of dependencies on ARMv8 that make TF Lite useful would include Ruy and cpuinfo. Depending on your use case, you may wish to create a stripped down build, for example  only using gemmlowp or only using eigen, but we don't maintain builds like that because the goal is a fully usable (both float and quant) library that has good performance while minimizing binary size. One option for your case is to manually reduce the set of built-in ops that TF Lite ships with by default. If you are just interested in a certain type of model with just a few ops, you could potentially significantly reduce binary size. This would of course not be something you could upstream back to TF Lite, but it might solve your immediate issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47862\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47862\">No</a>\n"]}, {"number": 47861, "title": "Cannot limit GPU memory with MultiWorkerMirroredStrategy", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from: pypi\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: Python 3.8.5\r\n- CUDA/cuDNN version: 11.1/8.0.5\r\n- GPU model and memory: RTX2080Ti\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running trainings with `OneDeviceStrategy` or `MirroredStrategy` on a single machine, it is possible to set a limit on the used GPU memory:\r\n```python\r\nlimit_mb = ...\r\ngpus = tf.config.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n            gpu,\r\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=limit_mb)])\r\nstrategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce())\r\n...\r\nwith strategy.scope():\r\n   ...\r\n   model.fit(...)\r\n```\r\nHowever running a distributed training with `MultiWorkerMirroredStrategy` on multiple machines does not work as both the `set_virtual_device_configuration` and `MultiWorkerMirroredStrategy` must run before anything else.\r\n\r\nThe code is the same with the strategy created as follows:\r\n\r\n```python\r\n    cluster_resolver = tf.distribute.cluster_resolver.SlurmClusterResolver(\r\n        jobs=None, port_base=8888, gpus_per_node=None, gpus_per_task=None,\r\n        tasks_per_node=None, auto_set_gpu=True, rpc_layer='grpc'\r\n    )\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n        communication=tf.distribute.experimental.CollectiveCommunication.NCCL,\r\n        cluster_resolver=cluster_resolver\r\n    )\r\n```\r\n\r\nCalling `set_virtual_device_configuration` before creating the `MultiWorkerMirroredStrategy` (as for the other strategies) leads to:\r\n```\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 340, in new_func\r\n  return func(*args, **kwargs)\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 253, in __init__\r\n  super(_CollectiveAllReduceStrategyExperimental,\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 185, in __init__\r\n  CollectiveAllReduceExtended(\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 327, in __init__\r\n  self._initialize_strategy(self._cluster_resolver)\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 335, in _initialize_strategy\r\n  self._initialize_multi_worker(cluster_resolver)\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 434, in _initialize_multi_worker\r\n  context.context().configure_collective_ops(\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 729, in configure_collective_ops\r\n  raise RuntimeError(\"Collective ops must be configured at program startup\")\r\n```\r\n\r\nCalling `MultiWorkerMirroredStrategy` before `set_virtual_device_configuration` leads to:\r\n```\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/framework/config.py\", line 748, in set_logical_device_configuration\r\n  context.context().set_logical_device_configuration(device, logical_devices)\r\nFile \"venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 1490, in set_logical_device_configuration\r\n  raise RuntimeError(\r\nRuntimeError: Virtual devices cannot be modified after being initialized\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt is possible to limit the GPU memory for `MultiWorkerMirroredStrategy` as for the other strategies.\r\n\r\nIf this is expected to not work by design, it would be useful to have it clearly stated in the docs.\r\n", "comments": ["@gatagat,\r\n\r\n> ```\r\n> gpus = tf.config.list_physical_devices('GPU')\r\n> if gpus:\r\n>   try:\r\n>     # Currently, memory growth needs to be the same across GPUs\r\n>     for gpu in gpus:\r\n>       tf.config.experimental.set_memory_growth(gpu, True)\r\n>     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n>   except RuntimeError as e:\r\n>     # Memory growth must be set before GPUs have been initialized\r\n>     print(e)\r\n> ```\r\n> \r\n\r\nCould you please take a look at [this](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) alternate method to limit GPU memory and check if it helps. Thanks!\r\n", "Thanks for the proposition, I am aware of that option and also that one can set it using the envvar TF_FORCE_GPU_ALLOW_GROWTH=yes, however, that does not limit the memory.", "Logical devices configuration should go before creating the strategy. Is there any other code before creating the strategy?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47861\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47861\">No</a>\n"]}, {"number": 47860, "title": "Mark header-implemented functions as inline", "body": "This makes their signature to match functions from above.\r\nIt also solves duplicate symbol errors in certain conditions.", "comments": []}, {"number": 47859, "title": "Android: TFLite GPU delegate has very little gain of performance over CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 33 Silverblue\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi redmi note 5 pro (android 10)\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:qualcom 509\r\n\r\n**Describe the current behavior**\r\nI'm trying to use TFLite on a yolo v5 neural network exported using [this repo](https://github.com/zldrobit/yolov5/tree/tf-android-tfl-detect). \r\nWhat I do is export the pretrained coco model from .pt file to .tflite in FP16 using the tf.py from repo. Then I make 50 inferences on an image in the android app and I get the avarage of the times. I take single inference time using `getLastNativeInferenceDurationNanoseconds()` function.\r\nI've made some test with CPU (4 thread and xnnpack) and GPU delegate both on FP16.  On GPU I set in `GpuDelegate.Options()`  the following two options\r\n```\r\ngpu_options.setPrecisionLossAllowed(true);\r\ngpu_options.setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_SUSTAINED_SPEED); \r\n```\r\nWhat i've seen is that there's no much time gain using GPU over CPU.\r\nOn an immage 256x256 I've got\r\n248.499 ms on CPU and 218.727 ms on GPU\r\n\r\nWith a bigger immage 800x640 the gain is a bit more but stil not a lot:\r\n1671.821 ms on CPU and 1445.127 ms on GPU\r\n\r\n(all times are the avarage time of 50 inferences)\r\n\r\n**Describe the expected behavior**\r\nWhat I expected was a greater improvement in time using GPU delegate. \r\n\r\nIs it normal or there's something I can do to improve GPU performance?\r\n\r\n", "comments": ["@srjoglekar246 could you take a look at this issue?", "@francku How large are your inputs & outputs? For GPU, copying inputs to/from the GPU takes time if the output is large as well. And note that XNNPack + 4 Threads is already a pretty fast setup :-).\r\n\r\nIt might also be the case that your full model isn't running on GPU, do you see any INFO/ERROR logs in logcat?\r\n\r\nUsing GPU also helps power efficiency if you perform inference frequently, something that isn't easily captured in existing benchmarks.", "> How large are your inputs & outputs? For GPU, copying inputs to/from the GPU takes time if the output is large as well. And note that XNNPack + 4 Threads is already a pretty fast setup :-).\r\n\r\nHi! thanks for answering. \r\nInput and output depends on the image passed as input and they can be big. The input is a buffer of dimension `1 * INPUT_SIZE_W * INPUT_SIZE_H * 3 * numBytesPerChannel` where numBytesPerChannel is 4 in FP model. Output is even bigger, is a buffer of dimension `output_box * (numClass + 5) * numBytesPerChannel` with output_box equals to \r\n`((((inputSize_w / 32) * (inputSize_h / 32)) + ((inputSize_w / 16)* (inputSize_h/16)) + ((inputSize_w / 8)* (inputSize_h / 8))) * 3)`\r\nSo this could be what is slowing me down?\r\nI've manged to try on a powerful phone a Huaweii P20 pro and CPU results are simililar to the old one but on GPU the results are better: on a 800x640 model I've got 256.892145 ms. Is there a way to know how much of this difference depends on copying data to/from GPU and how much depends on the computation power of the two GPUs? I take times using `getLastNativeInferenceDurationNanoseconds()` does this include both copying and computation?\r\n\r\n> It might also be the case that your full model isn't running on GPU, do you see any INFO/ERROR logs in logcat?\r\n\r\nI don't have any error on logcat but I tried the banchmark app on TFLite website to check if everything work on GPU and I've got these results\r\n`Explicitly applied GPU delegate, and the model graph will be completely executed by the delegate.`\r\nand then\r\n\r\n```\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t 4635.158\t 4635.158\t 99.996%\t 99.996%\t242484.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t         4635.093\t    0.179\t    0.091\t  0.004%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t 4635.158\t 4635.158\t 99.996%\t 99.996%\t242484.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t         4635.093\t    0.179\t    0.091\t  0.004%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t ModifyGraphWithDelegate\t        1\t  4635.158\t    99.996%\t    99.996%\t242484.000\t        1\r\n\t         AllocateTensors\t        1\t     0.183\t     0.004%\t   100.000%\t     0.000\t        2\r\n\r\nTimings (microseconds): count=1 curr=4635341\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t     TfLiteGpuDelegateV2\t            0.038\t  367.988\t  367.645\t100.000%\t100.000%\t     0.000\t        1\t[Identity]:347\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t     TfLiteGpuDelegateV2\t            0.038\t  367.988\t  367.645\t100.000%\t100.000%\t     0.000\t        1\t[Identity]:347\r\n\r\nNumber of nodes executed: 1\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     TfLiteGpuDelegateV2\t        1\t   367.644\t   100.000%\t   100.000%\t     0.000\t        1\r\n```\r\nSo I guess this mean that everything work on GPU, am I right?\r\n\r\n> Using GPU also helps power efficiency if you perform inference frequently, something that isn't easily captured in existing benchmarks.\r\n\r\nThanks this is something I'll keep in mind on my future considerations, I thought that GPU and CPU had the same power efficiency since they need the same time of computation. \r\nA drawback that I've noticed on GPU is that It took a bit more to allocate the delegate. In particular when I use\r\n`gpu_options.setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_SUSTAINED_SPEED); `\r\non big images (800x640 for instance) it could take a couple of minutes to initialize and in the meantime the phone is really laggy and even unlocking the screen becomes a nightmare. Is a normal behaviuor for this option?\r\n\r\nAnd just a last question is it normal that the same model is usable for a GPU delegate but when I try to use NNAPI I've got this error `java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: ModifyGraphWithDelegate is disallowed when graph is immutable.` ?  On benchamrk results seems that GPU use ModifyGraphWithDelegate but on NNAPI it says that graph is immutable.\r\n\r\nThanks \r\n\r\n", "@francku Your latency benchmarking method seems right to me. From the output, it looks like everything works on GPU. It will certainly be faster on newer phones, since the GPUs are better.\r\n\r\nThere is no way currently to measure how much time is spent copying data to/from the delegate, we can look into it. Thoughts, @multiverse-tf ?\r\n\r\n> A drawback that I've noticed on GPU is that It took a bit more to allocate the delegate. Is a normal behaviuor for this option?\r\n\r\nCurrently, yes. We are planning improvements in this area, for delegates to reduce initialization time.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47857, "title": "Are you using old TFLite binary with newer model?Registration failed.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution \r\nNAME=\"Raspbian GNU/Linux\"\r\nVERSION_ID=\"10\"\r\n\r\n- TensorFlow installation with this code : \r\necho \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\r\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\r\nsudo apt-get update\r\nsudo apt-get install python3-tflite-runtime\r\n\r\n### 2. Code to reproduce\r\n- Code\r\n\r\nimport tflite_runtime.interpreter as tflite\r\ninterpreter = tflite.Interpreter(model_path_lite) # path to lite model\r\n\r\n- Error raised\r\nTraceback (most recent call last):\r\n    interpreter = tflite.Interpreter(model_path=model_path_lite)\r\n  File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 207, in __init__\r\n    custom_op_registerers_by_func))\r\nValueError: Op builtin_code out of range: 132. Are you using old TFLite binary with newer model?Registration failed.\r\n\r\n\r\n### 5. Other infos\r\n- Conversion of keras model to .tflite model\r\nI converted the model from a keras model with this code :\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path_h5) # model to keras model\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\nwith open(model_path_lite, 'wb') as f:\r\n   f.write(tflite_model)\r\n   \r\nTo be able to convert, I had to use tf-nighlty  because the keras model contained Conv3DBackpropInputV2 operations. \r\n\r\n- Related issue\r\nI don't manage to install tf-nighlty on the raspberry pi. \r\n\r\n\r\n", "comments": ["This is an intended behavior. The model should be executed with the same TF version or higher TF version used for the TFLite model conversion. Please consider building your TensorFlow image through CMake or Bazel to use the matched version at the target device.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47857\">No</a>\n"]}, {"number": 47856, "title": "After training the model , how I can save it? or from which folder I can download my trained model?", "body": "hello\r\n\r\nafter training the model , how i can save it? or from which folder I can download my trained model?\r\n\r\nI just have trained \"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\" and use object detections API's\r\n\r\nmy folder structure shown in below image:\r\n\r\n![2021-03-17](https://user-images.githubusercontent.com/72650269/111440953-461de680-8720-11eb-87d7-940f27d963fc.png)\r\n", "comments": ["@halhwadi \r\nYou may use ```export_frozen_inference_graph.py```, ypu may refer to: [link](https://stackoverflow.com/questions/63524108/how-to-export-frozen-inference-graph-pb-from-a-checkpoint-in-tensorflow-objectde), [link1](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_load.ipynb).\r\n\r\nPlease confirm if you are using TF2 release of object detection API", "For tf2: models/research/object_detection/export_tflite_graph_tf2.py\r\nexample to refer to: [example](https://colab.research.google.com/gist/Saduf2019/db8b5cb091b59c5f45221c33de509863/example.ipynb)\r\n\r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47855, "title": "Update CMSIS kernels", "body": "@tensorflow/micro\r\n\r\nThe CMSIS kernels in `lite/micro/kernels/cmsis_nn/` need to be updated to match the latest changes in the corresponding micro kernels.\r\n\r\n", "comments": []}, {"number": 47854, "title": "training_a_model.md problem", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Cloud Platform\r\n- TensorFlow installed from (source or binary): \r\n- Tensorflow version (commit SHA if source): tensorflow-1.15\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32-CAM\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI am working on custom object detection for use in Arduino board (ESP32-CAM) \r\nESP32-CAM working stand alone (no wifi mode)\r\nI am practicing github page.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/training_a_model.md\r\n\r\n1.  PYTHONPATH problem\r\n    ---------------------------------------------------------------------------------\r\n    echo 'export PYTHONPATH=$PYTHONPATH:models/research/slim' >> ~/.bashrc\r\n    source ~/.bashrc\r\n    --- result ---------------------------------------------------------------------------------\r\n      File \"<ipython-input-30-ddee1c530575>\", line 1\r\n        export PYTHONPATH=$PYTHONPATH:models/research/slim >> ~/.bashrc\r\n                        ^\r\n    SyntaxError: invalid syntax\r\n\r\n2. python: can't open file 'models/research/slim/datasets/build_visualwakewords_data.py': [Errno 2] No such file or directory\r\n\"build_visualwakewords_data.py\" not exist in the datasets folder\r\n-------------------------------------------------------------------------------------------\r\n  ! python models/research/slim/datasets/build_visualwakewords_data.py\r\n  --logtostderr \\\r\n  --train_image_dir=coco/raw-data/train2014 \\\r\n  --val_image_dir=coco/raw-data/val2014 \\\r\n  --train_annotations_file=coco/raw-data/annotations/instances_train2014.json \\\r\n  --val_annotations_file=coco/raw-data/annotations/instances_val2014.json \\\r\n  --output_dir=coco/processed \\\r\n  --small_object_area_threshold=0.005 \\\r\n  --foreground_class_of_interest='person'\r\n\r\n3. build_visualwakewords_data.py file not exist in the datasets folder\r\n ! python models/research/slim/datasets/build_visualwakewords_data.py", "comments": ["You can use\r\n```\r\n!python models/research/slim/download_and_convert_data.py \\\r\n    --dataset_name=visualwakewords \\\r\n    --dataset_dir=\"${DATA_DIR}\"\r\n```\r\nto both download and build the dataset.", "thanks johanforslund", "The file has been and tflite micro is now moved to a new repo here https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/person_detection/training_a_model.md , I see the commands are updated too. Closing this issue. Thank you "]}, {"number": 47852, "title": "tf.train.Server session_config parameter cannot be configured by Estimator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\n\r\nsession_config=tf.ConfigProto(...)\r\nrun_config=tf.estimator.RunConfig(session_config=session_config)\r\nestimator = tf.estimator.Estimator(model_fn=..., config=run_config)\r\ntf.estimator.train_and_evaluator(estimator, train_spec, eval_spec)\r\n\r\nps/worker tf.Server is not configured by session_config\r\n\r\n**Describe the expected behavior**\r\n\r\nps/worker tf.Server can be configured by Estimator\r\n\r\nPR: https://github.com/tensorflow/estimator/pull/66\r\n", "comments": ["@candyzone \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "> @candyzone\r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nupdate @Saduf2019 ", "@candyzone \r\nWe see that you are using a very old version of tensor flow for which there is no support [for tf 1.x] any more, kindly upgrade to 2.x and let us know if you face any issues.", "@Saduf2019 \r\nTF 2.0 move Estimator to tensorflow_estimator  as an addon. It also has this problew in TF 2.0\r\nI make a PR in https://github.com/tensorflow/estimator/pull/66", "@candyzone ,\r\nRespective PR was closed due to lack of activity.Can you please provide the required information to debug the issue and also please  update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47852\">No</a>\n"]}, {"number": 47851, "title": "[ROCm] Switching to rocblas_gemm_ex for MFMA-enabled architectures", "body": "Filing the PR to upstream the same submitted by @ekuznetsov139 in the rocmfork.\r\n\r\nThis PR adds two pieces of functionality.\r\n1. `GetMFMASupport` routine (which is ROCM specific) to determine whether the underlying AMDGPU has support for mfma instructions\r\n2. Call the rocblas GEMM API that leverages the mfma instructions (when available and applicable)\r\n\r\n-----------------------------\r\n\r\n/cc @chsigg @cheshire ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47851) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47851) for more info**.\n\n<!-- need_author_consent -->", "@deven-amd Can you please sign CLA. Thanks!\r\n", "@googlebot I consent", "@chsigg @cheshire gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping"]}, {"number": 47850, "title": "Support ccache in cmake build.", "body": "Use ccache to speed up the compiling time.", "comments": ["@terryheo could you review this PR?"]}, {"number": 47848, "title": "Keras tuner - Missing validation split for model training", "body": "URL with the issue:\r\nhttps://www.tensorflow.org/tutorials/keras/keras_tuner#instantiate_the_tuner_and_perform_hypertuning\r\n\r\nDescription of issue (what needs changing):\r\n\r\nOn the second to last code cell where the hypermodel is re-instantiated and trained with the optimal number of epochs, a validation split parameter on the training set is not defined,as per best practises when fitting a model.\r\n\r\nIs the link to the source code correct?\r\n\r\nYes\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\nYes\r\n\r\nAre return values defined?\r\n\r\nYes\r\n\r\nAre you planning to also submit a pull request to fix the issue?\r\n\r\nYes, here: https://github.com/tensorflow/docs/pull/1844", "comments": ["Thank you for the pr, the issue will be closed once the pr is merged.", "Resolved through https://github.com/tensorflow/docs/pull/1844"]}, {"number": 47847, "title": "Fix incorrect CUDA version specification in Dockerfiles", "body": null, "comments": []}, {"number": 47846, "title": "Training BERT for QNLI task - Fitting the model stops after the first Epoch without crash. No warning or error given to understand why it is happening.", "body": "my code follows the tensorflow example below very closely:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu\r\n\r\nMy data is in a csv and in total 14000. It has a 'question', 'answer' and 'label' fields. labels are binary 0/1.\r\nThe questions and answers are strings. In the code below they are uploaded onto colab via pandas using read_csv and are referred to as train_df, val_df and test_df.\r\n\r\nAfter the running the code below, the model refuses to go past the first Epoch. \r\n<img width=\"1083\" alt=\"tfqnli\" src=\"https://user-images.githubusercontent.com/47080256/111368034-bf0f3680-866b-11eb-86a8-b281f53b49dc.PNG\">\r\n\r\n```\r\n!pip install -q -U tensorflow-text\r\n!pip install -q -U tf-models-official\r\n!pip install -U tfds-nightly\r\n```\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow_text as text \r\nimport tensorflow_addons as tfa\r\nfrom official.nlp import optimization\r\nimport numpy as np\r\n\r\ntf.get_logger().setLevel('ERROR')\r\n```\r\n```\r\nbert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\r\n\r\ntfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\r\ntfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\r\n\r\nbert_preprocess = hub.load(tfhub_handle_preprocess)\r\n```\r\n```\r\ndef make_bert_preprocess_model(sentence_features, seq_length=128):\r\n  \"\"\"Returns Model mapping string features to BERT inputs.\r\n\r\n  Args:\r\n    sentence_features: a list with the names of string-valued features.\r\n    seq_length: an integer that defines the sequence length of BERT inputs.\r\n\r\n  Returns:\r\n    A Keras Model that can be called on a list or dict of string Tensors\r\n    (with the order or names, resp., given by sentence_features) and\r\n    returns a dict of tensors for input to BERT.\r\n  \"\"\"\r\n\r\n  input_segments = [\r\n      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\r\n      for ft in sentence_features]\r\n\r\n  # Tokenize the text to word pieces.\r\n  bert_preprocess = hub.load(tfhub_handle_preprocess)\r\n  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\r\n  segments = [tokenizer(s) for s in input_segments]\r\n\r\n  truncated_segments = segments\r\n\r\n  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\r\n                          arguments=dict(seq_length=seq_length),\r\n                          name='packer')\r\n  model_inputs = packer(truncated_segments)\r\n  return tf.keras.Model(input_segments, model_inputs)\r\n```\r\n\r\n```\r\ndef load_dataset_from_tfds(dataset, batch_size, bert_preprocess_model):\r\n\r\n  num_examples = len(list(dataset))\r\n\r\n  dataset = dataset.batch(batch_size)\r\n  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\r\n  dataset = dataset.prefetch(1)\r\n  return dataset, num_examples\r\n```\r\n\r\n```\r\nsentence_features = ['question', 'answer']\r\nnum_classes = 2\r\n\r\nbert_preprocess_model = make_bert_preprocess_model(sentence_features)\r\n```\r\n\r\n```\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices({'idx':train_df.index.values, 'question':train_df.question.values, 'answer':train_df.answer.values,  'label':train_df.label.values})\r\n\r\nval_dataset = tf.data.Dataset.from_tensor_slices({'idx':val_df.index.values, 'question':val_df.question.values, 'answer':val_df.answer.values, 'label':val_df.label.values})\r\n\r\ntest_dataset = tf.data.Dataset.from_tensor_slices({'idx':test_df.index.values, 'question':test_df.question.values, 'answer':test_df.answer.values, 'label':test_df.label.values})\r\n```\r\n```\r\ndef build_classifier_model(num_classes, activation=None):\r\n  inputs = dict(\r\n      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_word_ids'),\r\n      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_mask'),\r\n      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_type_ids'),\r\n  )\r\n\r\n  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\r\n  net = encoder(inputs)['pooled_output']\r\n  net = tf.keras.layers.Dropout(rate=0.1)(net)\r\n  net = tf.keras.layers.Dense(num_classes, activation=activation, name='classifier')(net)\r\n  return tf.keras.Model(inputs, net, name='prediction')\r\n```\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nbatch_size = 32\r\nepochs = 5\r\ninit_lr = 2e-5\r\n\r\nwith strategy.scope():\r\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n  metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)\r\n\r\n  train_dataset_, train_data_size= load_dataset_from_tfds(train_dataset, batch_size, bert_preprocess_model)\r\n  steps_per_epoch = train_data_size // batch_size\r\n  num_train_steps = steps_per_epoch * epochs\r\n  num_warmup_steps = num_train_steps // 10\r\n\r\n  val_dataset_, val_data_size= load_dataset_from_tfds(val_dataset, batch_size, bert_preprocess_model)\r\n  validation_steps = val_data_size // batch_size\r\n\r\n  classifier_model = build_classifier_model(num_classes)\r\n\r\n\r\n  optimizer = optimization.create_optimizer(init_lr =init_lr , num_train_steps = num_train_steps, num_warmup_steps=num_warmup_steps, optimizer_type='adamw')\r\n\r\n  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\r\n\r\n  classifier_model.fit(x = train_dataset_, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data = val_dataset_, validation_steps=validation_steps )\r\n```", "comments": ["I have tried using BinaryCrossentropy instead of SparseCategoricalCrossentropy; varying learning rate (0.001, 10e-4, 2e-5); batch size (16, 32 and 64); num_warmup_steps (lowering it).\r\nI tried using a sub sample(15000 datapoints) of the 'glue/qnli' and it did the same thing when using GPU. On the otherhand when using TPU, if I used the entire sample it completed the number of Epochs fine, however when I used a subsample (15000 datapoints) it gave the error below:\r\n```\r\nException ignored in: <function CapturableResourceDeleter.__del__ at 0x7f7e0aa4e440>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py\", line 208, in __del__\r\n    self._destroy_resource()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 253, in restored_function_body\r\n    return _call_concrete_function(function, inputs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\", line 75, in _call_concrete_function\r\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\", line 116, in _call_flat\r\n    cancellation_manager)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1932, in _call_flat\r\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 589, in call\r\n    executor_type=executor_type)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/functional_ops.py\", line 1206, in partitioned_call\r\n    f.add_to_graph(graph)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 505, in add_to_graph\r\n    g._add_function(self)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3396, in _add_function\r\n    gradient)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\nEpoch 1/3\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"AdamWeightDecay/gradients/StatefulPartitionedCall:1\", shape=(None,), dtype=int32), values=Tensor(\"clip_by_global_norm/clip_by_global_norm/_0:0\", dtype=float32), dense_shape=Tensor(\"AdamWeightDecay/gradients/StatefulPartitionedCall:2\", shape=(None,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"shape. This may consume a large amount of memory.\" % value)\r\n406/406 [==============================] - 105s 92ms/step - loss: 0.6311 - accuracy: 0.7035 - val_loss: 0.5873 - val_accuracy: 0.7125\r\nEpoch 2/3\r\n  1/406 [..............................] - ETA: 4:50:46 - loss: 0.6417 - accuracy: 0.6250\r\n---------------------------------------------------------------------------\r\nOutOfRangeError                           Traceback (most recent call last)\r\n<ipython-input-34-ca8cb3b177ae> in <module>()\r\n     41   classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\r\n     42 \r\n---> 43   classifier_model.fit(x=train_dataset, validation_data=validation_dataset, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_steps=validation_steps)\r\n\r\n10 frames\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nOutOfRangeError: 9 root error(s) found.\r\n  (0) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[GroupCrossDeviceControlEdges_5/Identity_7/_511]]\r\n  (1) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[Pad_6/paddings/_288]]\r\n  (2) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[Pad_1/shape/_3/_320]]\r\n  (3) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[strided_slice_48/_392]]\r\n  (4) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n  (5) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[Pad_24/paddings/_266]]\r\n  (6) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[Pad_15/paddings/_308]]\r\n  (7) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[TPUReplicate/_compile/_17709408053356889722/_6/_368]]\r\n  (8) Out of range: {{function_node __inference_train_function_1014910}} End of sequence\r\n\t [[{{node cond_13/else/_164/cond_13/IteratorGetNext}}]]\r\n\t [[Pad_31/paddings/_316]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "@oqusous \r\nPlease share a colab gist of the error reported or provided code that is easy for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47846\">No</a>\n"]}, {"number": 47845, "title": "@amahendrakar, I did check this link (before raising the support request), some changes are made, but still, there are errors. I have attached the ipynb file. Please support.", "body": "@amahendrakar, I did check https://www.tensorflow.org/guide/migrate  (before raising the support request), some changes are made, but still, there are errors. I have attached the ipynb file. Please support.\r\n\r\n_Originally posted by @rrklearn2020 in https://github.com/tensorflow/tensorflow/issues/47742#issuecomment-797540614_", "comments": ["Please do not create a duplicated issue. It just makes an extra burden to the team who triages the github issue."]}, {"number": 47844, "title": "Fix std error when building the magic-wand demo for zephyr_vexriscv", "body": "When compiling the magic-wand demo for the zephyr_vexriscv target (with command: `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv magic_wand_bin`) there are the following errors:\r\n\r\n```\r\ntensorflow/lite/kernels/internal/reference/elu.h:31:40: error: 'expm1' is not a member of 'std'; did you mean 'exp'?\r\ntensorflow/lite/micro/kernels/elu.cc:57:33: error: 'round' is not a member of 'std'; did you mean 'round'?\r\n```\r\nThis happens on a current master branch.\r\n\r\nIt looks like declaring std global switch for `expm1` function and changing `std::round` to `TfLiteRound` in `elu.cc` solves the problem.\r\n\r\nRelated to https://github.com/tensorflow/tensorflow/issues/47622#issuecomment-794696077", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47844) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "The changes from this PR are merged with https://github.com/tensorflow/tensorflow/commit/b0da18591202c2b4cbd2322d2bd468901b16b4a3. Closing the PR.\r\n\r\nSince adding new commits to a PR and approval and merging in the internal infrastructure can happen independently we can get into a state where some of the commits from a PR are merged but not others (which is the case here).\r\n\r\nIn most cases this is not a problem."]}, {"number": 47843, "title": "<Removed>", "body": "<removed>", "comments": ["Please don't file spam issues"]}, {"number": 47842, "title": "TFLM: download a specific version of renode", "body": "As discussed in #47841, the approach of downloading a json file, parsing out and then getting the url for the renode download can result in some flakiness sure to rate limiting.\r\n\r\nDirectly using the URL for a specific renode release avoids this issue.\r\n\r\nFix for: https://github.com/tensorflow/tensorflow/issues/47841", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47841, "title": "Renode download fails from time to time", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nIn our CI we have seen that the Renode download fails sometimes. There are no clues to why. Not a big deal as it usually works when retrying.\r\n\r\nIt turns out that it is because of API rate limiting. So it would be nice to check for this and possible do a few retries.\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nCall ./tensorflow/lite/micro/tools/make/renode_download.sh until it fails.", "comments": []}, {"number": 47840, "title": "TF-TRT Enable Conv2DBackpropInput conversion in explicit batch mode", "body": "This PR updates the Conv2DBackpropInput converter to be compatible with explicit batch mode. \r\n\r\nConversion is disabled for inputs with unknown shape.\r\n\r\nTracker: #45481\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.", "comments": []}, {"number": 47839, "title": "Update CMSIS ref kernel wrappers.", "body": "Updating CMSIS kernels to match latest updates to ref kernels. This PR fixes #47855 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47837, "title": "constants.SAVED_MODEL_FILENAME_PB)) OSError: SavedModel file does not exist at: /home/pi/Downloads/tf_lite_model.h5/{saved_model.pbtxt|saved_model.pb}", "body": "I faced this problem when I wanted to import the pre-trained model to my rpi 3b+. I saw there are some similar issues but neither of them works for my case. Pls help me out!!\r\n\r\nconstants.SAVED_MODEL_FILENAME_PB))\r\nOSError: SavedModel file does not exist at: /home/pi/Downloads/tf_lite_model.h5/{saved_model.pbtxt|saved_model.pb}  \r\n\r\n^ this is the error.\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@Tshi90 \r\nplease share simple stand alone code to replicate the issue reported or a colab gist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47837\">No</a>\n"]}, {"number": 47836, "title": "issue with multi objects detection (using ssd_resnet50_v1_fpn_640x640_coco17_tpu-8)", "body": "**System information**\r\n\r\n- Colab (GPU runtime)\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to modified  this [notebook ](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb)to detect multi objects(2 objects) instead of one object, I have modified  the \"gt_boxes_list\" and \"gt_classes_list\" so both have (2,4) and (2,2) shapes respectively, however when I train the model I get the below error: \r\n*******\r\nValueError: in user code:\r\n\r\n    <ipython-input-17-9f168971fcaf>:55 train_step_fn  *\r\n        losses_dict = model.loss(prediction_dict, shapes)\r\n    /usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py:842 loss  *\r\n        (batch_cls_targets, batch_cls_weights, batch_reg_targets,\r\n    /usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py:1083 _assign_targets  *\r\n        groundtruth_weights_list)\r\n    /usr/local/lib/python3.7/dist-packages/object_detection/core/target_assigner.py:512 batch_assign  *\r\n        (cls_targets, cls_weights,\r\n    /usr/local/lib/python3.7/dist-packages/object_detection/core/target_assigner.py:177 assign  *\r\n        unmatched_shape_assert = shape_utils.assert_shape_equal(\r\n    /usr/local/lib/python3.7/dist-packages/object_detection/utils/shape_utils.py:321 assert_shape_equal  *\r\n        raise ValueError('Unequal shapes {}, {}'.format(shape_a, shape_b))\r\n\r\n    ValueError: Unequal shapes [3], [2]\r\n*****\r\n[hint] I have checked \"prediction_dict['class_predictions_with_background'] and its shape is (1, 51150, 2) instead of (1, 51150, **3**) as num_classes is 2 so prediction_dict['class_predictions_with_background'].shape[2] should be (num_classes + 1) as per the below quoted comment from \"ssd_meta_arch.py\" file:\r\n\"3) class_predictions_with_background: 3-D float tensor of shape\r\n          [batch_size, num_anchors, num_classes+1] containing class predictions\r\n          (logits) for each of the anchors.  Note that this tensor *includes*\"\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nmodel should train and predict the 2 classes with their corresponding boxes\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nplease find the link to the modified colab [notebook:](https://colab.research.google.com/drive/1l8e2LcPmnDAlf8r54ltC7kkTb70WbpyP?usp=sharing)\r\n\r\nfor training images, I used only 5 images (they are uploaded under this path:\r\n'/content/models/research/object_detection/test_images/bike'\r\n![bike_1](https://user-images.githubusercontent.com/72650269/111270399-3da6ac80-8649-11eb-9d7b-79b741159dc4.jpg)\r\n![bike_2](https://user-images.githubusercontent.com/72650269/111270415-426b6080-8649-11eb-846a-fcf8af843220.jpg)\r\n![bike_3](https://user-images.githubusercontent.com/72650269/111270436-49926e80-8649-11eb-839b-1425b7cb7fab.jpg)\r\n![bike_4](https://user-images.githubusercontent.com/72650269/111270450-4e572280-8649-11eb-8bcd-60eb20be28be.jpg)\r\n![bike_5](https://user-images.githubusercontent.com/72650269/111270468-54e59a00-8649-11eb-9c04-a62bd04a2291.jpg)\r\n\r\nyou need to change the pictures extension to \"jpg\" in image_path\r\n```\r\ntrain_image_dir = 'models/research/object_detection/test_images/bike/'\r\ntrain_images_np = []\r\nfor i in range(1, 6):\r\n  image_path = os.path.join(train_image_dir, 'bike_' + str(i) + '.jfif')\r\n```\r\n\r\nBest Regards\r\n\r\n", "comments": ["@halhwadi \r\nPlease provide access to the colab note book shared.", "Sorry find the link to the`[ shared notebook](https://colab.research.google.com/drive/1l8e2LcPmnDAlf8r54ltC7kkTb70WbpyP?usp=sharing)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47836\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47836\">No</a>\n"]}, {"number": 47835, "title": "fix datatypes in cwise and gather ops", "body": "cwise_ops_gpu_common and gather_functor kernels deduce loop variables from input template datatype. they cause overflows  and consequently result in Illegal CUDA memory access issues. Sample code to reproduce is added below. \r\n\r\n`import tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\u200b\r\nn = 13417677  # 13417676 works\r\nh = 160\r\nx = tf.compat.v1.get_variable('x', [n, h])\r\nwith tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())`", "comments": ["Have you tested the performance impact of these changes (especially the changes to the cwise ops, I'm less worried about the changes to gather)?\r\n\r\nCC @sherhut as FYI", "> Have you tested the performance impact of these changes (especially the changes to the cwise ops, I'm less worried about the changes to gather)?\r\n> \r\n> CC @sherhut as FYI\r\n\r\nFor what I was able to test, there was no effect for the LEFT and RIGHT performance. I used the random initializer (which calls mul, add, etc, internally ending up in cwise ops) over several session runs and actually `To64Bit` was 2-3% faster, (that might not be very meaningful as the variance was high and the call was in the order of few microseconds)", "> > Have you tested the performance impact of these changes (especially the changes to the cwise ops, I'm less worried about the changes to gather)?\r\n> > CC @sherhut as FYI\r\n> \r\n> For what I was able to test, there was no effect for the LEFT and RIGHT performance. I used the random initializer (which calls mul, add, etc, internally ending up in cwise ops) over several session runs and actually `To64Bit` was 2-3% faster, (that might not be very meaningful as the variance was high and the call was in the order of few microseconds)\r\n\r\nI see, thanks.  I'd still prefer merging this after the branch cut for TF 2.5 (currently planned for March 25) to reduce the risk of a subtle regression & cherry picks.  If that's OK with you can you please reply on this PR after March 25?", "> > > Have you tested the performance impact of these changes (especially the changes to the cwise ops, I'm less worried about the changes to gather)?\r\n> > > CC @sherhut as FYI\r\n> > \r\n> > \r\n> > For what I was able to test, there was no effect for the LEFT and RIGHT performance. I used the random initializer (which calls mul, add, etc, internally ending up in cwise ops) over several session runs and actually `To64Bit` was 2-3% faster, (that might not be very meaningful as the variance was high and the call was in the order of few microseconds)\r\n> \r\n> I see, thanks. I'd still prefer merging this after the branch cut for TF 2.5 (currently planned for March 25) to reduce the risk of a subtle regression & cherry picks. If that's OK with you can you please reply on this PR after March 25?\r\n\r\nSure, will remind you then.", "@kushanam Any update on this PR? Please. Thanks!", "> @kushanam Any update on this PR? Please. Thanks!\r\n\r\n@gbaned this doesn't wait for an action from me. @sanjoy proposed to postpone it to 2.6 and hence is the change request.", "> > Have you tested the performance impact of these changes (especially the changes to the cwise ops, I'm less worried about the changes to gather)?\r\n> > CC @sherhut as FYI\r\n> \r\n> For what I was able to test, there was no effect for the LEFT and RIGHT performance. I used the random initializer (which calls mul, add, etc, internally ending up in cwise ops) over several session runs and actually `To64Bit` was 2-3% faster, (that might not be very meaningful as the variance was high and the call was in the order of few microseconds)\r\n\r\nOne thing to consider here is that we are switching away from the Eigen kernels for cwise ops (and have already done so for many kernels, including mul and add). It is possible that when you tested, you were already using the MLIR generated kernels, not the Eigen kernels (if you were testing with the latest changes available on March 16th).\r\nCan you please try a kernel that is definitely using Eigen, e.g. from the cwise ops, this would be igamma, or explicitly disable MLIR generated kernels by passing the build flag --no=//tensorflow/core/kernels/mlir_generated:enable_gpu ?", "I did now some performance benchmarking on a index-heavy kernel, namely:\r\nadd with shapes\r\nlhs_shape = 10, 1, 10, 10, 100\r\nrhs_shape = 1, 10, 10, 1, 100\r\nSo lots of broadcasting involved -> many index computations.\r\nFor Eigen, with 64 bits it takes 1.9 times longer than before, going from 163 microseconds to 314 microseconds\r\nFor the MLIR generated kernel, with 64 bits it takes about 1.5 times longer than before, going from 17 microseconds to 26 microseconds.", "> I did now some performance benchmarking on a index-heavy kernel, namely:\r\n> add with shapes\r\n> lhs_shape = 10, 1, 10, 10, 100\r\n> rhs_shape = 1, 10, 10, 1, 100\r\n> So lots of broadcasting involved -> many index computations.\r\n> For Eigen, with 64 bits it takes 1.9 times longer than before, going from 163 microseconds to 314 microseconds\r\n> For the MLIR generated kernel, with 64 bits it takes about 1.5 times longer than before, going from 17 microseconds to 26 microseconds.\r\n\r\n@akuegel and @sanjoy  Thanks for the benchmarks, I could repro the results for the cwise ops. Any suggestions on how to mitigate the issue then? Unless the datatypes change, randomizing big embedding would not fit in int32.", "Unfortunately I don't have a suggestion on how to mitigate the issue. Given that the cases where it becomes slower are still faster with the MLIR generated kernels than with the Eigen kernels, maybe we can accept the slowdown?\r\n@sanjoy what do you think?", "@akuegel we could also have the kernel generator JIT compile a kernel for 64bit indices if needed.", "@akuegel, @sanjoy Any update on this PR? Please. Thanks!", "> @akuegel, @sanjoy Any update on this PR? Please. Thanks!\r\n\r\nI think the pull request as-is can be closed. It tries to adjust code that we are mostly not using anymore.\r\nThe index type choice for the MLIR generated kernels is hard-coded here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tools/kernel_gen/transforms/kernel_lowering_passes.cc#L56\r\nJIT compiling is planned (I think) for Q3, then it might be possible to fix this for MLIR generated kernels.", "@kushanam Can you please resolve conflicts? Thanks!", "> @kushanam Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned, this PR is suggested to get closed by @akuegel and the stale review has been dismissed.", "Yeah, let's close this. We still have a plan for adding support for int64 index values with the new jit mode. Essentially it would jit compile with 64 bit indexing if it sees a tensor where this would be needed."]}, {"number": 47834, "title": "error about Kernel8bitNeonDotprodOutOfOrder  occurs  when running int8 CPU inference by tflite", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: on mobile\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:  under-release yet, the platform is Qualcom SM4350\r\n-   **TensorFlow installed from (source or binary)**: on mobile\r\n-   **TensorFlow version (use command below)**: v2.0.0\r\n-   **Python version**: on mobile\r\n-   **Bazel version (if compiling from source)**: on mobile\r\n-   **GCC/Compiler version (if compiling from source)**:  on mobile\r\n-   **CUDA/cuDNN version**: on mobile\r\n-   **GPU model and memory**: on mobile\r\n-   **Exact command to reproduce**:tflite->Invoke\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nThe problem  occurs when doing stress testing for camera of mobile phone. In detail, we do cpu inference for a int8 quantilized model and the problem occurs occasionally, causing the camera crash.\r\n\r\n### Source code / logs\r\n03-16 04:58:16.798467 28412 28412 F DEBUG   : backtrace:\r\n03-16 04:58:16.798525 28412 28412 F DEBUG   :       #00 pc 00000000002247dc  /vendor/lib64/libtensorflowLite.so (ruy::Kernel8bitNeonDotprodOutOfOrder(ruy::KernelParams8bit<8, 8> const&)+1364)\r\n03-16 04:58:16.798548 28412 28412 F DEBUG   :       #01 pc 0000000000114774  /vendor/lib64/libtensorflowLite.so (void ruy::RunKernelTyped<(ruy::Path)8, signed char, signed char, signed char, ruy::BasicSpec<int, signed char> >(ruy::Tuning, ruy::PackedMatrix<signed char> const&, ruy::PackedMatrix<signed char> const&, ruy::BasicSpec<int, signed char> const&, int, int, int, int, ruy::Matrix<signed char>*)+508)\r\n03-16 04:58:16.798564 28412 28412 F DEBUG   :       #02 pc 0000000000113c74  /vendor/lib64/libtensorflowLite.so (void ruy::RunKernel<(ruy::Path)8, signed char, signed char, signed char, ruy::BasicSpec<int, signed char> >(ruy::Tuning, ruy::SidePair<ruy::PMatrix> const&, void*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::DMatrix*)+160)\r\n03-16 04:58:16.798573 28412 28412 F DEBUG   :       #03 pc 00000000002297b4  /vendor/lib64/libtensorflowLite.so\r\n03-16 04:58:16.798583 28412 28412 F DEBUG   :       #04 pc 0000000000228dc4  /vendor/lib64/libtensorflowLite.so (ruy::TrMul(ruy::TrMulParams*, ruy::Context*)+2292)\r\n03-16 04:58:16.798596 28412 28412 F DEBUG   :       #05 pc 00000000001131d0  /vendor/lib64/libtensorflowLite.so (void ruy::DispatchMul<(ruy::Path)15, signed char, signed char, signed char, ruy::BasicSpec<int, signed char> >(ruy::Matrix<signed char> const&, ruy::Matrix<signed char> const&, ruy::BasicSpec<int, signed char> const&, ruy::Context*, ruy::Matrix<signed char>*)+384)\r\n03-16 04:58:16.798610 28412 28412 F DEBUG   :       #06 pc 0000000000112410  /vendor/lib64/libtensorflowLite.so (tflite::optimized_integer_ops::ConvPerChannel(tflite::ConvParams const&, int const*, int const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, signed char*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*)+1164)\r\n03-16 04:58:16.798622 28412 28412 F DEBUG   :       #07 pc 000000000011109c  /vendor/lib64/libtensorflowLite.so (void tflite::ops::builtin::conv::EvalQuantizedPerChannel<(tflite::ops::builtin::conv::KernelType)1>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)+648)\r\n03-16 04:58:16.798636 28412 28412 F DEBUG   :       #08 pc 0000000000105ed8  /vendor/lib64/libtensorflowLite.so (TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)1>(TfLiteContext*, TfLiteNode*)+280)\r\n03-16 04:58:16.798647 28412 28412 F DEBUG   :       #09 pc 000000000023079c  /vendor/lib64/libtensorflowLite.so (tflite::Subgraph::Invoke()+740)\r\n03-16 04:58:16.798657 28412 28412 F DEBUG   :       #10 pc 00000000002342dc  /vendor/lib64/libtensorflowLite.so (tflite::Interpreter::Invoke()+32)\r\n", "comments": ["@DanielMao2015  Could you try your code with the recent TF version? TF 2.0.0 is too old to be supported.", "@abattery As you said, I will test with the recent version, and will close the issue if no bug occurs", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47834\">No</a>\n"]}, {"number": 47833, "title": "Lazy load optional modules to speedup pip package import", "body": "This PR lazy loads optional external Python modules to speedup initial import times of the TF pip package.\r\n\r\nThese changes speedup initial import by roughly **24%**:\r\n\r\nmodules | percentage of TF import\r\n---|---\r\nkubernetes | 14.5 %\r\npandas | 6.9 %\r\nscipy | 2.6 %\r\n\r\nThis partially addresses #37729, but import times are still quite slow. A large chunk is spent importing the `tf.compat` which I don't think is used a lot by the average user, so it would be great if this could be lazy loaded as well in the future.\r\n\r\n/cc @mihaimaruseac", "comments": ["Good catch, I fixed it in 818acdd9416ccbde08d2330dbe3f4070c2b92ef9", "@frankchn Thanks for approving. I fixed the pylint error in the latest commit, sorry about that.", "This PR was rolled back in 9f95b52fe19eb4738043482ddd24f0ddee334065.\r\n\r\n@gbaned @frankchn Could you give some context of why this was the case?", "Hi @lgeiger -- an internal test failed causing an auto-rollback. I am investigating this issue now.", "Hi @lgeiger, we have resolved the internal issue and the change has been re-committed as 5a737ab. Thanks!", "Awesome, thanks for taking care of re-commiting."]}, {"number": 47832, "title": "Bazel build fails downloading sqlite-amalgamation-3340100.zip", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubunto 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2\r\n- Python version: 3.8\r\n- Installed using: pip\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc-14\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the problem**\r\nBuilding from sources from git clone https://github.com/tensorflow/tensorflow.git, using bazel fails\r\n    \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCommand\r\n\r\n    bazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\nfails with error:\r\n\r\n    Error in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip, https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip] to /home/attardi/.cache/bazel/_bazel_attardi/9e2174a9a29f23641a2f691abf17e76f/external/org_sqlite/temp4950223412785952933/sqlite-amalgamation-3340100.zip: connect timed out\r\n\r\nIndeed:\r\n\r\n    https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2021/sqlite-amalgamation-3340100.zip\r\nis missing.\r\n\r\n**Any other info / logs**\r\nStrangely, the second alternative URL to download, does exists:\r\n\r\n    \"https://www.sqlite.org/2021/sqlite-amalgamation-3340100.zip\"\r\n\r\nbut bazel ignores it.\r\n", "comments": ["I downgraded to bazel 3.1.0 and switched to v2.4.1 and it worked:\r\n\r\n    sudo apt install bazel-3.1.0\r\n    git checkout tags/v2.4.1\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47832\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47832\">No</a>\n"]}, {"number": 47831, "title": "'Windows fatal exception: access violation' with tensor flow object detection", "body": "Hi, so I am trying to create a custom object detector for myself and am using this guide:https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#training-the-model. As I am new to this I have followed all instructions to a T. However when I try to train my model with \r\n`python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config \r\n`\r\nI get the following error:\r\n\r\n\r\n```\r\n2021-03-15 23:49:57.441253: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-03-15 23:50:08.636642: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 49 of 2048\r\n2021-03-15 23:50:18.699069: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 149 of 2048\r\n2021-03-15 23:50:28.835509: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 234 of 2048\r\n2021-03-15 23:50:39.771722: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 297 of 2048\r\n2021-03-15 23:50:49.312747: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 320 of 2048\r\n2021-03-15 23:50:58.806089: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 360 of 2048\r\n2021-03-15 23:51:09.810531: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 400 of 2048\r\n2021-03-15 23:51:18.791169: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 406 of 2048\r\n2021-03-15 23:51:30.375699: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 442 of 2048\r\n2021-03-15 23:51:38.994588: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 466 of 2048\r\n2021-03-15 23:51:49.395020: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 506 of 2048\r\n2021-03-15 23:51:59.317995: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 531 of 2048\r\n2021-03-15 23:52:09.465026: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 570 of 2048\r\n2021-03-15 23:52:19.124648: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 594 of 2048\r\n2021-03-15 23:52:28.983142: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 634 of 2048\r\n2021-03-15 23:52:38.615317: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 673 of 2048\r\n2021-03-15 23:52:49.003972: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 698 of 2048\r\n2021-03-15 23:52:59.077313: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 738 of 2048\r\n2021-03-15 23:53:09.490022: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 763 of 2048\r\n2021-03-15 23:53:18.937036: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 800 of 2048\r\n2021-03-15 23:53:29.407881: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 825 of 2048\r\n2021-03-15 23:53:39.302728: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 863 of 2048\r\n2021-03-15 23:53:48.757507: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 886 of 2048\r\n2021-03-15 23:53:58.636679: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 910 of 2048\r\n2021-03-15 23:54:09.077197: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 949 of 2048\r\n2021-03-15 23:54:18.762039: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:177] Filling up shuffle buffer (this may take a while): 988 of 2048\r\nWindows fatal exception: access violation\r\n\r\nThread 0x00000890 (most recent call first):\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 2573 in iterator_get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 730 in _next_internal\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 800 in get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py\", line 585 in get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1619 in get_next_as_list_static_shapes\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 663 in get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 632 in __next__\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 628 in next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 352 in load_fine_tune_checkpoint\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 580 in train_loop\r\n  File \"model_main_tf2.py\", line 104 in main\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\absl\\app.py\", line 251 in _run_main\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\absl\\app.py\", line 300 in run\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40 in run\r\n  File \"model_main_tf2.py\", line 113 in <module>\r\nWindows fatal exception: access violation\r\n\r\nThread 0x00000890 (most recent call first):\r\n  File \"C:\\Users\\agnip\\anaconda3\\eWindows fatal exception: access violation\r\n\r\nnvs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 2573 in iterator_get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 730 in _next_internal\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 800 in get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py\", line 585 in get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1619 in get_next_as_list_static_shapes\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 663 in get_next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 632 in __next__\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 628 in next\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 352 in load_fine_tune_checkpoint\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 580 in train_loop\r\n  File \"model_main_tf2.py\", line 104 in main\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\absl\\app.py\", line 251 in _run_main\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\absl\\app.py\", line 300 in run\r\n  File \"C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40 in run\r\n  File \"model_main_tf2.py\", line 113 in <module>\r\n```\r\n\r\nI can't figure out what the error is. Please help me solve this issue so I can create my custom object detector.\r\nAlso I am using Windows 10, 16GB ram and all the packages are listed below:\r\n\r\n(tensorflow4) D:\\Tensorflow\\workspace\\training_demo>conda list\r\n# packages in environment at C:\\Users\\agnip\\anaconda3\\envs\\tensorflow4:\r\n#\r\n# Name                    Version                   Build  Channel\r\nabsl-py                   0.10.0                   pypi_0    pypi\r\napache-beam               2.28.0                   pypi_0    pypi\r\nastunparse                1.6.3                    pypi_0    pypi\r\nattrs                     20.3.0                   pypi_0    pypi\r\navro-python3              1.9.2.1                  pypi_0    pypi\r\nblas                      1.0                         mkl\r\nca-certificates           2021.1.19            haa95532_1\r\ncachetools                4.2.1                    pypi_0    pypi\r\ncertifi                   2020.12.5                pypi_0    pypi\r\ncffi                      1.14.5                   pypi_0    pypi\r\nchardet                   4.0.0                    pypi_0    pypi\r\ncontextlib2               0.6.0.post1              pypi_0    pypi\r\ncrcmod                    1.7                      pypi_0    pypi\r\ncycler                    0.10.0                   pypi_0    pypi\r\ncython                    0.29.22                  pypi_0    pypi\r\ndataclasses               0.6                      pypi_0    pypi\r\ndill                      0.3.1.1                  pypi_0    pypi\r\ndm-tree                   0.1.5                    pypi_0    pypi\r\ndocopt                    0.6.2                    pypi_0    pypi\r\nfastavro                  1.3.3                    pypi_0    pypi\r\nflatbuffers               1.12                     pypi_0    pypi\r\nfuture                    0.18.2                   pypi_0    pypi\r\ngast                      0.3.3                    pypi_0    pypi\r\ngin-config                0.4.0                    pypi_0    pypi\r\ngoogle-api-core           1.26.1                   pypi_0    pypi\r\ngoogle-api-python-client  2.0.2                    pypi_0    pypi\r\ngoogle-auth               1.27.1                   pypi_0    pypi\r\ngoogle-auth-httplib2      0.1.0                    pypi_0    pypi\r\ngoogle-auth-oauthlib      0.4.3                    pypi_0    pypi\r\ngoogle-cloud-bigquery     2.11.0                   pypi_0    pypi\r\ngoogle-cloud-core         1.6.0                    pypi_0    pypi\r\ngoogle-crc32c             1.1.2                    pypi_0    pypi\r\ngoogle-pasta              0.2.0                    pypi_0    pypi\r\ngoogle-resumable-media    1.2.0                    pypi_0    pypi\r\ngoogleapis-common-protos  1.53.0                   pypi_0    pypi\r\ngrpcio                    1.32.0                   pypi_0    pypi\r\nh5py                      2.10.0                   pypi_0    pypi\r\nhdfs                      2.6.0                    pypi_0    pypi\r\nhttplib2                  0.17.4                   pypi_0    pypi\r\nidna                      2.10                     pypi_0    pypi\r\nimportlib-resources       5.1.2                    pypi_0    pypi\r\nintel-openmp              2020.2                      254\r\njoblib                    1.0.1                    pypi_0    pypi\r\nkaggle                    1.5.12                   pypi_0    pypi\r\nkeras-preprocessing       1.1.2                    pypi_0    pypi\r\nkiwisolver                1.3.1                    pypi_0    pypi\r\nlabelimg                  1.8.3                    pypi_0    pypi\r\nlvis                      0.5.3                    pypi_0    pypi\r\nlxml                      4.6.2                    pypi_0    pypi\r\nmarkdown                  3.3.4                    pypi_0    pypi\r\nmatplotlib                3.3.4                    pypi_0    pypi\r\nmkl                       2020.2                      256\r\nmkl-service               2.3.0            py38h196d8e1_0\r\nmkl_fft                   1.3.0            py38h46781fe_0\r\nmkl_random                1.1.1            py38h47e9c7a_0\r\nmock                      2.0.0                    pypi_0    pypi\r\nnumpy                     1.20.1                   pypi_0    pypi\r\noauth2client              4.1.3                    pypi_0    pypi\r\noauthlib                  3.1.0                    pypi_0    pypi\r\nobject-detection          0.1                      pypi_0    pypi\r\nopencv-python             4.5.1.48                 pypi_0    pypi\r\nopencv-python-headless    4.5.1.48                 pypi_0    pypi\r\nopenssl                   1.1.1j               h2bbff1b_0\r\nopt-einsum                3.3.0                    pypi_0    pypi\r\npackaging                 20.9                     pypi_0    pypi\r\npandas                    1.2.3            py38hf11a4ad_0\r\npbr                       5.5.1                    pypi_0    pypi\r\npillow                    8.1.2                    pypi_0    pypi\r\npip                       21.0.1           py38haa95532_0\r\npromise                   2.3                      pypi_0    pypi\r\nproto-plus                1.17.0                   pypi_0    pypi\r\nprotobuf                  3.15.6                   pypi_0    pypi\r\npsutil                    5.8.0                    pypi_0    pypi\r\npy-cpuinfo                7.0.0                    pypi_0    pypi\r\npyarrow                   2.0.0                    pypi_0    pypi\r\npyasn1                    0.4.8                    pypi_0    pypi\r\npyasn1-modules            0.2.8                    pypi_0    pypi\r\npycocotools               2.0                      pypi_0    pypi\r\npycparser                 2.20                     pypi_0    pypi\r\npydot                     1.4.2                    pypi_0    pypi\r\npymongo                   3.11.3                   pypi_0    pypi\r\npyparsing                 2.4.7                    pypi_0    pypi\r\npyqt5                     5.15.4                   pypi_0    pypi\r\npyqt5-qt5                 5.15.2                   pypi_0    pypi\r\npyqt5-sip                 12.8.1                   pypi_0    pypi\r\npython                    3.8.8                hdbf39b2_4\r\npython-dateutil           2.8.1              pyhd3eb1b0_0\r\npython-slugify            4.0.1                    pypi_0    pypi\r\npytz                      2021.1             pyhd3eb1b0_0\r\npyyaml                    5.4.1                    pypi_0    pypi\r\nrequests                  2.25.1                   pypi_0    pypi\r\nrequests-oauthlib         1.3.0                    pypi_0    pypi\r\nrsa                       4.7.2                    pypi_0    pypi\r\nscikit-learn              0.24.1                   pypi_0    pypi\r\nscipy                     1.4.1                    pypi_0    pypi\r\nsentencepiece             0.1.95                   pypi_0    pypi\r\nseqeval                   1.2.2                    pypi_0    pypi\r\nsetuptools                54.1.2                   pypi_0    pypi\r\nsix                       1.15.0           py38haa95532_0\r\nsqlite                    3.33.0               h2a8f88b_0\r\ntensorboard               2.4.1                    pypi_0    pypi\r\ntensorboard-plugin-wit    1.8.0                    pypi_0    pypi\r\ntensorflow                2.4.1                    pypi_0    pypi\r\ntensorflow-addons         0.12.1                   pypi_0    pypi\r\ntensorflow-datasets       4.2.0                    pypi_0    pypi\r\ntensorflow-estimator      2.4.0                    pypi_0    pypi\r\ntensorflow-hub            0.11.0                   pypi_0    pypi\r\ntensorflow-metadata       0.28.0                   pypi_0    pypi\r\ntensorflow-model-optimization 0.5.0                    pypi_0    pypi\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\ntext-unidecode            1.3                      pypi_0    pypi\r\ntf-models-official        2.4.0                    pypi_0    pypi\r\ntf-slim                   1.1.0                    pypi_0    pypi\r\nthreadpoolctl             2.1.0                    pypi_0    pypi\r\ntqdm                      4.59.0                   pypi_0    pypi\r\ntypeguard                 2.11.1                   pypi_0    pypi\r\ntyping-extensions         3.7.4.3                  pypi_0    pypi\r\nuritemplate               3.0.1                    pypi_0    pypi\r\nurllib3                   1.26.3                   pypi_0    pypi\r\nvc                        14.2                 h21ff451_1\r\nvs2015_runtime            14.27.29016          h5e58377_2\r\nwerkzeug                  1.0.1                    pypi_0    pypi\r\nwheel                     0.36.2                   pypi_0    pypi\r\nwincertstore              0.2                      py38_0\r\nwrapt                     1.12.1                   pypi_0    pypi\r\nzlib                      1.2.11               h62dcd97_4\r\n\r\n\r\nThanks in advance!", "comments": ["@JackRyan4444 \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47831\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47831\">No</a>\n", "I met the same question. Do you solve this?", "I got the same question too..."]}]