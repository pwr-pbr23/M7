[{"number": 31885, "title": "Enabling model training on Android ", "body": "The current support of tensorflow for Android does not include the binaries required for training. Is there a way by which we can include the required function's files ( for training basic image recognition model) in the libtensorflow-core.a?\r\nOr any other method by which we can enable training?", "comments": ["I am curious why you wanna train model in Android? the Android calculation ablity is hard to support it. @kukzile ", "@ghost Could you please have a look at the [TF Lite Examples ](https://www.tensorflow.org/lite/examples) and let us know if it helps? We see that you're using TF v1.x which is no longer supported so please try to upgrade to TF v2.6.0 and let us know the outcome ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31885\">No</a>\n"]}, {"number": 31884, "title": "declared output erros while building", "body": "How can I fix these errors with tensorflow 1.14.0 and bazel 1.24\r\n\r\n```\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/python/BUILD:3469:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/mh.naderan/tf/tensorflow-1.14.0/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: declared output 'external/local_config_cuda/cuda/cuda/include/thrust/detail/dispatch/is_trivial_copy.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: declared output 'external/local_config_cuda/cuda/cuda/include/thrust/iterator/detail/is_trivial_iterator.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: declared output 'external/local_config_cuda/cuda/cuda/include/thrust/system/detail/generic/type_traits.h' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)\r\nERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/a6a33b5ca42465d31b51832fb008b42b/external/local_config_cuda/cuda/BUILD:168:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3.481s, Critical Path: 2.85s\r\nINFO: 15 processes: 15 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["@mahmoodn ,\r\nPlease refer this [link](https://www.tensorflow.org/install/source).\r\nAlso provide the other platform details like operating system, architecture.Thanks!", "See:\r\n```\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.1 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n$ uname -a\r\nLinux 1080Ti 4.15.0-29-generic #31-Ubuntu SMP Tue Jul 17 15:39:52 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n$ gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper\r\nOFFLOAD_TARGET_NAMES=nvptx-none\r\nOFFLOAD_TARGET_DEFAULT=1\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 7.4.0-1ubuntu1~18.04.1' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \r\n$ python --version\r\nPython 2.7.15+\r\n$ bazel version\r\nBuild label: 0.24.1\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Apr 2 16:29:26 2019 (1554222566)\r\nBuild timestamp: 1554222566\r\nBuild timestamp as int: 1554222566\r\n\r\n```\r\n\r\n\r\n\r\nconfigure\r\n```\r\n$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n/home/mh.naderan/.local/lib/python2.7/site-packages/\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31884\">No</a>\n"]}, {"number": 31883, "title": "Can TF add a C++ kernel for PRelu? ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 4.8.5-16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.4, master branch, commit id is c188ffcc9e8769e5c6f767b7fb1977e54cad4040\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc 6.3\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nSome models(MTCC) uses PRelu as an activation layer, and it will be split to small ops including Relu, Sub(https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/advanced_activations.py#L141). It's hard to fuse and will impact the performance .\r\n\r\n**Describe the expected behavior**\r\nHope to have a TF C++ kernel for this op,  then it could call the kernel directly to get good cache locality on CPU or reduce memory access on GPU.\r\n\r\n**Code to reproduce the issue**\r\nNA\r\n\r\n\r\n\r\n**Other info / logs**\r\nNA\r\n\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 31882, "title": "no attribute python", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Duplicate of the issue #31881 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31882\">No</a>\n"]}, {"number": 31881, "title": "no attribute python", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@shenzhencheng ,\r\nCan you please provide the link for which the issue is being faced ?Thanks!", "Doesn't seem to have any new info beside the template", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31881\">No</a>\n"]}, {"number": 31880, "title": "Converting .pb to tflite GetOpWithOutput-Error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf-nightly 1.15.0.dev20190819\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI trained a object-detection model with my own dataset and I'm trying to convert the output to a tflite file for my mobile application\r\n\r\nMy code to convert the checkpoint to .pb:\r\npython export_tflite_ssd_graph.py \r\n--pipeline_config_path /tensorflow/models/research/object_detection/training/ssd_mobilenet_v1.config \r\n--trained_checkpoint_prefix /tensorflow/models/research/object_detection/dataset/model.ckpt-50000 \r\n--output_directory /tensorflow/models/research/object_detection/dataset/tflite --add_postprocessing_op=true\r\n\r\nMy code to convert .pb to tflite \r\n\r\n!tflite_convert --output_file /tensorflow/models/research/object_detection/dataset/tflite/tflite_graph.tflite \\\r\n                --graph_def_file /tensorflow/models/research/object_detection/dataset/tflite/tflite_graph.pb \\\r\n                --output_format TFLITE \\\r\n                --inference_type FLOAT \\\r\n                --input_arrays image_tensor \\\r\n                --input_shapes 1,300,300,3 \\\r\n                --output_arrays raw_detection_scores \\\r\n                --mean_values=128 \\\r\n                --std_dev_values=128 \\\r\n                --change_concat_input_ranges=false \\\r\n                --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\r\n                --allow_custom_ops\r\n\r\nThe error from console:\r\n2019-08-22 08:59:26.271136: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-08-22 08:59:26.275691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz\r\n2019-08-22 08:59:26.277466: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56240f206dc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-08-22 08:59:26.277520: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 515, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 511, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 199, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/lite.py\", line 989, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/convert.py\", line 412, in toco_convert_graph_def\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0822 08:59:27.571232 139982160922432 __init__.py:689] \r\n\r\n  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\r\n\r\n  Please upgrade your code to TensorFlow 2.0:\r\n    * https://www.tensorflow.org/beta/guide/migration_guide\r\n\r\n  Or install the latest stable TensorFlow 1.X release:\r\n    * `pip install -U \"tensorflow==1.*\"`\r\n\r\n  Otherwise your code may be broken by the change.\r\n\r\n  \r\n2019-08-22 08:59:27.633571: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2019-08-22 08:59:27.640703: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"raw_detection_scores\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nAborted (core dumped)\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n[tflite_graph.zip](https://github.com/tensorflow/tensorflow/files/3528754/tflite_graph.zip)\r\n\r\n\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Dear,\r\nhow to set the four parameters\r\n--mean_values\r\n--std_dev_values\r\n--default_ranges_min\r\n--default_ranges_max\r\n\r\nthx\r\nhttps://github.com/tensorflow/tensorflow/issues/32611"]}, {"number": 31879, "title": "Lite: is the \"MinimumRuntimeVersion\" list still under maintenance? ", "body": "*This is an issue \"compiled by eyes in original source\", so there is no templeted issue questions*.\r\n\r\nHi, I notice that the \"[GetMinimumRuntimeVersionForModel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/op_version.cc#L26)\" doesn't include **a complete list** of supported Ops of TFLite when I am adding new op. I am curious that if that list is still under maintenance? It seems that the MinimumRuntimeVersion is simply a tip string that won't block model to run on TFLite of any version. And [the min version searching logic](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/tflite/op_version.cc#L198) seems restricted to regex `[0-9]\\.[0-9]\\.[0-9]`, which means `1.9.0` is a larger version than `1.14.0`.\r\n\r\nAs there seems no runtime check to enforce this min version, maybe we can remove it to lease the op versioning handling? Or we can improve it and enforce it (include PRs)?\r\n\r\nPing people who may know, sorry for the spam ;)\r\n@haozha111 @jianlijianli @bjacob @aselle @suharshs ", "comments": ["Hi Jack,\r\n\r\nThanks for taking a look into this!\r\n\r\n1) When you add a new op, you also need to update the map in op_version.cc, with 'kPendingReleaseOpVersion' as a placeholder for the actual TF version.\r\n2) The compare logic is a bug. I will fix this soon. Sorry about that!\r\n3) We currently don't block model from execution based on min_version, I think that's intended. It could be in the user code that checks this version and see if the application code want to execute or not. Also notice that the version map only tracks built-in ops, and we don't have any mechanism of tracking the version of custom ops.\r\n\r\nThanks!", "Thank you for the detailed explanation! I am agree you that user code may need to check that version, but the list is not complete, not sure it can really help :)\r\n\r\nBtw, is the versioning here shall sync with [schema](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema), [kernel register](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc), [operator property in tools](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/operator_property.cc)?", "Hi, Jack,\r\n\r\nTotally agree with you that the current list should keep in sync with kernel register. Otherwise the generated min version won't be reliable. I'm currently working on this.\r\n\r\nIn the meantime, I will update the comment about this method to inform the users that this is currently experimental. Thanks!", "Thank you Haoliang! I have added version in this list once, I understand\nthat is hmm... a sort annoying work, thanks for you efforts, sincerely.\n\nCheers\n\n\nHaoliang Zhang <notifications@github.com> \u4e8e2019\u5e748\u670824\u65e5\u5468\u516d \u4e0a\u53482:13\u5199\u9053\uff1a\n\n> Hi, Jack,\n>\n> Totally agree with you that the current list should keep in sync with\n> kernel register. Otherwise the generated min version won't be reliable. I'm\n> currently working on this.\n>\n> In the meantime, I will update the comment about this method to inform the\n> users that this is currently experimental. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31879?email_source=notifications&email_token=ABFVHDLVDTG5DZKKA3H2AXDQGASF7A5CNFSM4IORQFQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5A5WVQ#issuecomment-524409686>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABFVHDMD2LBZ3UZ54IEJOX3QGASF7ANCNFSM4IORQFQA>\n> .\n>\n"]}, {"number": 31878, "title": "Revert skipping EIGEN_FORCE_INLINE change & potential fix for windows flaky build ", "body": "", "comments": ["Good catch on the directory. \r\n\r\n"]}, {"number": 31877, "title": "graph_transforms/transform_graph failed with shared backbone and multiple input node", "body": "**I try to optimize my model with tf transform_graph tool, and failed**\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=inference.pb \\\r\n--out_graph=inference_opt.pb \\\r\n--inputs='input_image_1:0,input_image2:0,input_image_3:0\u2019 \\\r\n--outputs='output_node_1:0,output_node_2:0\u2019 \\\r\n--transforms='\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  sort_by_execution_order'\r\n\r\n**Then long log print occur and never stop, something like this:**\r\n\r\n2019-08-22 10:56:11.108570: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/read/_135__cf__135 to be preserved.\r\n2019-08-22 10:56:11.108594: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/read/_136__cf__136 to be preserved.\r\n2019-08-22 10:56:11.108604: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/moving_mean/read/_137__cf__137 to be preserved.\r\n2019-08-22 10:56:11.108614: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/moving_variance/read/_138__cf__138 to be preserved.\r\n2019-08-22 10:56:11.108623: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n2019-08-22 10:56:11.120159: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/read/_140__cf__140 to be preserved.\r\n2019-08-22 10:56:11.120181: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/read/_141__cf__141 to be preserved.\r\n2019-08-22 10:56:11.120192: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/moving_mean/read/_142__cf__142 to be preserved.\r\n2019-08-22 10:56:11.120201: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/moving_variance/read/_143__cf__143 to be preserved.\r\n2019-08-22 10:56:11.120211: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n2019-08-22 10:56:11.138263: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/Conv_1/BatchNorm/beta/read/_10__cf__10 to be preserved.\r\n2019-08-22 10:56:11.138286: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/Conv_1/BatchNorm/gamma/read/_11__cf__11 to be preserved.\r\n2019-08-22 10:56:11.138294: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/Conv_1/BatchNorm/moving_mean/\r\n\r\n\r\n\r\n**And after I remove  transform option  **remove_nodes(op=Identity, op=CheckNumerics)**,\r\nIt seems succeed, but when I open optimized pb fie, bn node is still there, which not be fold.**\r\nI test other classification model,  transform_graph just woks well. But in my model, it just can not work. **By the way, my model has multiple input and shares backbone mobilenet_v2**.\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=inference.pb \\\r\n--out_graph=inference_opt.pb \\\r\n--inputs='input_image_1:0,input_image2:0,input_image_3:0\u2019 \\\r\n--outputs='output_node_1:0,output_node_2:0\u2019 \\\r\n--transforms='\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  sort_by_execution_order'", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nPlease fill the issue template. Could you update them if they are relevant in your case, or leave them as N/A? . If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version:  No GPU\r\n- GPU model and memory: No GPU\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI try to optimize my model with tf transform_graph tool, and failed\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \r\n--in_graph=inference.pb \r\n--out_graph=inference_opt.pb \r\n--inputs='input_image_1:0,input_image2:0,input_image_3:0\u2019 \r\n--outputs='output_node_1:0,output_node_2:0\u2019 \r\n--transforms='\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_batch_norms\r\nfold_old_batch_norms\r\nsort_by_execution_order'\r\n\r\nThen long log print occur and never stop, something like this:\r\n\r\n2019-08-22 10:56:11.108570: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/read/_135__cf__135 to be preserved.\r\n2019-08-22 10:56:11.108594: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/read/_136__cf__136 to be preserved.\r\n2019-08-22 10:56:11.108604: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/moving_mean/read/_137__cf__137 to be preserved.\r\n2019-08-22 10:56:11.108614: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/expand/BatchNorm/moving_variance/read/_138__cf__138 to be preserved.\r\n2019-08-22 10:56:11.108623: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n2019-08-22 10:56:11.120159: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/read/_140__cf__140 to be preserved.\r\n2019-08-22 10:56:11.120181: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected part_cnn/feature_extract/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/read/_141__cf__141 to be preserved.\r\n\r\n**Describe the expected behavior**\r\n\r\nBn should be fold into conv\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "@hwenjun18 Could you share minimum standalone code to reproduce the issue? Did you check graph transform with other tensorflow models? Thanks!", "> @hwenjun18 Could you share minimum standalone code to reproduce the issue? Did you check graph transform with other tensorflow models? Thanks!\r\n\r\nI have checked graph transform with other tensorflow models, including mobilenet_v1, mobilenet_v2 in slim, but with multiple input and a shared mobilenet_v2, it fails", "@hwenjun18 Could you add 'fold_constants(ignore_errors=ture)' to the transforms list as fold_constants is prerequisite of fold_batch_norms?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#fold_batch_norms", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31877\">No</a>\n"]}, {"number": 31876, "title": "Invalid Link for TensorFlow Keras Official Documentation", "body": "## URL with the issue:\r\nhttps://www.tensorflow.org/guide/keras\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras.ipynb\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/guide/keras.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nThe last two links are directed to 404, which means they don't exist with valid colab files or gitlab files.", "comments": ["Thanks for the report. We're in the process of moving the docs around for the TF2 RC.\r\nThose docs now live here in github:\r\n- https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/keras.ipynb\r\n- https://colab.sandbox.google.com/github/tensorflow/docs/blob/master/site/en/r1/guide/keras.ipynb\r\n\r\nBut, really, they should point to the `r2.0rc` branch to avoid this: https://github.com/tensorflow/docs/blob/r2.0rc/site/en/guide/keras.ipynb", "Links in TF1 guides and tutorials have been updated to point to the `r2.0rc`branch:\r\n- https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/guide/keras.ipynb\r\n- https://github.com/tensorflow/docs/blob/r2.0rc/site/en/guide/keras.ipynb", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31876\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31876\">No</a>\n"]}, {"number": 31875, "title": "Aanconda python 3.7  could not find tf-nightly-2.0-preview", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nAnaconda \r\npython 3.7\r\nconda 4.7.10\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n![image](https://user-images.githubusercontent.com/27112868/63479254-1bc26380-c4c0-11e9-9cb7-37e00538778d.png)\r\n\r\n![image](https://user-images.githubusercontent.com/27112868/63481925-862bd180-c4c9-11e9-9b4e-218e3db603e4.png)\r\n\r\n\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/27112868/63480959-7a8adb80-c4c6-11e9-94fe-e292af574574.png)\r\n\r\n\r\n**Problem**  \r\n_pip install  tf-nightly-2.0-preview_  did  not work\r\nhowever _pip search  tf-nightly-2.0-preview_  works\r\n\r\nFurther,    I found      _pip install tf-nightly_       works.\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/27112868/63483541-51bb1400-c4cf-11e9-87d8-9d7c103fbbc8.png)\r\n\r\n\r\ncould you solve this problems ?\r\nthanks \r\n\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@zwenju ,\r\nWhen tried installing latest TF-nightly-2.0-preview worked fine, can you kindly check.Thanks!", "I retest it. \r\nIt is not working with python 3.7\r\nIt works for python 3.6.\r\nDo you know  the reasons ?\r\nbut  tf-nightly works for python 3.7 and 3.6.\r\ntf-nightly-2.0-preview only works for python 3.6.\r\n\r\nwindows 10, anaconda python 3.7", "@zwenju ,\r\nI tried installing latest `tf-nightly-2.0-preview 2.0.0.dev20190825` version in both python3.6 and 3.7 in anaconda,worked fine for me.Thanks!", "thanks, it is not works for me.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31875\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31875\">No</a>\n"]}, {"number": 31874, "title": "Compute library for hardware acceleration ", "body": "Does google has their own compute library for hardware(arm cpu, gpu, fpga, etc.) acceleration? If google has it, where is it located at? which directory? Thanks!", "comments": ["I don't think I'm the proper assignee for this, sorry.", "Please check out XLA here: https://www.tensorflow.org/xla/\r\n\r\nIt lives in the compiler directory.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31874\">No</a>\n", "Is it the library can chose\u00a0Arm cpu/gpu to do the acceleration?\n\n\u53d1\u81ea iPhone \u7248 Yahoo \u90ae\u7bb1\n\n\n\u661f\u671f\u4e8c, \u516b\u6708 27, 2019, 00:20 \u4e8e Martin Wicke <notifications@github.com>\u5199\u9053\uff1a\n\n\nPlease check out XLA here: https://www.tensorflow.org/xla/\n\nIt lives in the compiler directory.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\n"]}, {"number": 31873, "title": "Parts of graphs are missing during tf.lite.TFLiteConverter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow version (use command below): r1.14\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version : gcc 5.4.0 \r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: Nvidia GeForce GTX TITAN \r\n\r\n\r\n\r\nThis is a little bit tricky to explain but I found that TFLiteConverter, which is shown above, drops a bunch of nodes in the tensorflow computation graph. \r\n\r\nHere is a part of my converting code.\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(latest,\r\n                input_arrays=[\"Placeholder\",\"Placeholder_1\"],\r\n                input_shapes={\"Placeholder\":[1,144000,1], \"Placeholder_1\":[1,500]},\r\n                output_arrays=[output_node_names])\r\n\r\n    # graph dump options\r\n    converter.dump_graphviz_dir=\"graph_dir\" # outputs graph visualization file (.dot)\r\n    tflite_model = converter.convert()\r\n    open(\"base_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\nThe problem part of graph can be described as follows with input and output node names.\r\nThe original part of the graph looks like\r\n```\r\n...\r\nSqueeze, Reshape_2 -> add (Add op)\r\nReshape_5, add -> add_1 (Add op)\r\n...\r\n```\r\n, but the converter gave me \r\n```\r\n...\r\nadd,Squeeze -> add_1 (Add op)\r\n...\r\n```\r\n. The paths containing Reshape_5 and Reshapes_2 are completely missing in the converted graph. The paths containing Reshape_5 and Reshape_2 start from Const op nodes, not input nodes, so that the wrong converted graph still gives me output array in the same shape, but the output value is wrong. \r\n\r\nWhen I inspected the dumped graph such as toco_AT_IMPORT.dot, toco_AFTER_TRANSFORMATIONS.dot and toco_AFTER_ALLOCATION.dot, to find out the cause.\r\nI found that toco_AT_IMPORT.dot, which is dumped at the importing the saved_model stage, already dropped the nodes. Thus I think that the problem occurred at the 'importing' stage, which reads the saved_model as GraphDef.\r\n\r\nI modified the converter code to see intermediate tflite graph dump files in *.dot, as follows:\r\n(https://github.com/tensorflow/tensorflow/blob/b431fcc28d6f905fd868d979121b8e6c80902731/tensorflow/lite/toco/import_tensorflow.cc#L2642)\r\n```\r\nstd::unique_ptr<Model> ImportTensorFlowGraphDef(\r\n    const ModelFlags& model_flags, const TensorFlowImportFlags& tf_import_flags,\r\n    const GraphDef& tf_graph) {\r\n  LogDumpGraphDef(kLogLevelModelChanged, \"AT IMPORT\", tf_graph);\r\n\r\n  GraphDef inlined_graph(tf_graph);\r\n  if (InlineAllFunctions(&inlined_graph)) {\r\n    LogDumpGraphDef(kLogLevelModelChanged, \"AFTER INLINING\", inlined_graph);\r\n  }\r\n\r\n  // Check input and output specification.\r\n  for (const auto& specified_input_array : model_flags.input_arrays()) {\r\n    CHECK(!absl::EndsWith(specified_input_array.name(), \":0\"))\r\n        << \"Unsupported explicit zero output index: \"\r\n        << specified_input_array.name();\r\n  }\r\n  for (const string& specified_output_array : model_flags.output_arrays()) {\r\n    CHECK(!absl::EndsWith(specified_output_array, \":0\"))\r\n        << \"Unsupported explicit zero output index: \" << specified_output_array;\r\n  }\r\n\r\n  Model* model = new Model;\r\n  internal::ConverterMapType converter_map;\r\n\r\n  // This is used for the TFLite \"Full Flex Mode\" conversion. All the ops are\r\n  // imported as `TensorFlowUnsupportedOperator`, and later all these ops are\r\n  // converted to TFLite Flex ops.\r\n  if (!tf_import_flags.import_all_ops_as_unsupported) {\r\n    converter_map = internal::GetTensorFlowNodeConverterMap();\r\n  } else {\r\n    converter_map = internal::GetTensorFlowNodeConverterMapForFlex();\r\n  }\r\n\r\n  for (auto node : inlined_graph.node()) {\r\n    StripZeroOutputIndexFromInputs(&node);\r\n    auto status = internal::ImportTensorFlowNode(node, tf_import_flags, model,\r\n                                                 converter_map);\r\n    CHECK(status.ok()) << status.error_message();\r\n  }\r\n  //LogDump(kLogLevelModelChanged, \"IN IMPORT 0\", *model); // fail\r\n  ResolveModelFlags(model_flags, model);\r\n\r\n  //LogDump(kLogLevelModelChanged, \"IN IMPORT 1\", *model); // fail\r\n  StripCaretFromArrayNames(model);\r\n  //LogDump(kLogLevelModelChanged, \"IN IMPORT 2\", *model); // fail\r\n  AddExtraOutputs(model);\r\n  //LogDump(kLogLevelModelChanged, \"IN IMPORT 3\", *model); // fail\r\n  FixNoMissingArray(model);\r\n  LogDump(kLogLevelModelChanged, \"IN IMPORT 4\", *model); //\r\n  FixNoOrphanedArray(model);\r\n  LogDump(kLogLevelModelChanged, \"IN IMPORT 5\", *model); //\r\n  FixOperatorOrdering(model);\r\n  LogDump(kLogLevelModelChanged, \"IN IMPORT 6\", *model); //\r\n  CheckInvariants(*model);\r\n  LogDump(kLogLevelModelChanged, \"IN IMPORT 7\", *model); //\r\n\r\n  // if rnn state arrays are constant, make them transient\r\n  for (const auto& rnn_state : model->flags.rnn_states()) {\r\n    model->GetArray(rnn_state.state_array()).buffer = nullptr;\r\n  }\r\n\r\n  return std::unique_ptr<Model>(model);\r\n}\r\n```\r\nI want to find out the exact stage that causes the problem and fix it. The code gave me toco_IN_IMPORT_4.dot, toco_IN_IMPORT_5.dot, toco_IN_IMPORT_6.dot and toco_IN_IMPORT_7.dot, however, they were all same to toco_AT_IMPORT.dot.\r\n\r\nI really need helps for this problem. Here are my questions.\r\n1. What are possible causes of this problem? Is it in my tensorflow model or the converter?\r\n2. The 'LogDumpGraphDef' function seems to dump outputs showing the graph definition, but how can I actually get its output?\r\n", "comments": ["I found some answers for my self. I'm sorry for a premature issue making.\r\n\r\nI could get some log messages by setting following environment variables in the converting code. \r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='0'\r\nos.environ['TF_CPP_MIN_VLOG_LEVEL']='2'\r\n```\r\n\r\nThis gives me some log messages that seems related to the converting.\r\n\r\nAmong them I found following part, says constant folding.\r\n\r\n```\r\n...\r\n2019-08-22 11:57:55.662626: I tensorflow/core/grappler/optimizers/constant_folding.cc:1305] Folded node:\r\nname: \"body/model/parallel_0/body/add\"\r\nop: \"Add\"\r\ninput: \"body/model/parallel_0/body/Reshape_5\"\r\ninput: \"body/model/parallel_0/body/Reshape_2\"\r\ndevice: \"/device:GPU:0\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n\r\n2019-08-22 11:57:56.049579: I tensorflow/core/grappler/optimizers/constant_folding.cc:1310] Generated constant node:\r\nname: \"ConstantFolding/body/model/parallel_0/body/add-folded\"\r\nop: \"Const\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"value\"\r\n  value {\r\n    tensor {\r\n      dtype: DT_FLOAT\r\n      tensor_shape {\r\n        dim {\r\n          size: 1\r\n        }\r\n        dim {\r\n          size: 600\r\n        }\r\n        dim {\r\n          size: 512\r\n        }\r\n      }\r\n      float_val: -0.0591003299\r\n      float_val: -0.00605344772\r\n      float_val: -0.668159962\r\n      float_val: -0.0551900864\r\n      float_val: 0.537890077\r\n      float_val: -1.09977579\r\n      float_val: 0.609119534\r\n...\r\n```\r\n\r\nI didn't fully checked the correctness of 'float_val's, but it seems that the convert removes the path along Reshape_2 and Reshape_5 into 'Const' nodes. Then it performed the addition in 'add' Node, then it made final result as another 'Const' node, then it provided it as the input to add_1.\r\n\r\nAt this stage, I'm still finding the reason why the converted tflite graph gave me the different output, however, this 'missing' nodes seems not to be the reason. Thus I close this issue myself.", "Dear,\r\nhow to set the four parameters\r\n--mean_values\r\n--std_dev_values\r\n--default_ranges_min\r\n--default_ranges_max\r\n\r\nthx\r\n#32611"]}, {"number": 31872, "title": "Compiling from source, error with finding cuda ", "body": "\r\n- OS Platform and Distribution: Centos 7\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.15\r\n- Python version: 2.7.5\r\n- Installed using virtualenv: virtualenv\r\n- Bazel version: 0.26.0\r\n- GCC/Compiler version: 7.3.0\r\n- CUDA version: CUDA 10.1\r\n-cuDNN version: 7.6.2\r\n- TensorRT: 5.1.5\r\n- GPU: Tesla T4\r\n\r\nI'm trying to install tensorflow from source and tried v1.14 and v1.15. When running ./configure, I select yes for CUDA and TensorRT support. I initially get an error saying no cuda.h is found, so I point the path to /usr/include/linux. The installation fails with the following error:\r\n\r\n`Traceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 500, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 492, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 457, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 251, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 238, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 227, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 244, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n`\r\n\r\nFull log:\r\n\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.0 installed.\r\nPlease specify the location of python. [Default is /home/mltlocal/envs/Resnet50GPU/bin/python]:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n  /home/mltlocal/envs/Resnet50GPU/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/mltlocal/envs/Resnet50GPU/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib64'\r\n        '/opt/rh/devtoolset-4/root/usr/lib64/dyninst'\r\n        '/usr'\r\n        '/usr/lib64/atlas'\r\n        '/usr/lib64/dyninst'\r\n        '/usr/lib64/mysql'\r\n        '/usr/local/cuda-10.1/targets/x86_64-linux/lib'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10.1\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.6.2\r\n\r\n\r\nPlease specify the TensorRT version you want to use. [Leave empty to  default to TensorRT 5]:\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]:\r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /usr/include/linux\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 500, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 492, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 457, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 251, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 238, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 227, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 244, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n", "comments": ["@manuprasad07 \r\nLooks like path to python is not correct. Please check your error\r\n\r\n```\r\nPlease specify the location of python. [**Default is /home/mltlocal/envs/Resnet50GPU/bin/python**]:\r\n\r\nTraceback (most recent call last):\r\nFile \"\", line 1, in \r\nAttributeError: 'module' object has no attribute 'getsitepackages'\r\nFound possible Python library paths:\r\n/home/mltlocal/envs/Resnet50GPU/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use. Default is [**/home/mltlocal/envs/Resnet50GPU/lib/python2.7/site-packages**]\r\n```\r\n\r\n1. Do you have python at this location? Default is /home/mltlocal/envs/Resnet50GPU/bin/python\r\n\r\n2. Did you install cuda related libraries and drivers as mentioned in Tensorflow website?\r\n\r\n3. Are you trying to install TF from source in virtual Environment?\r\n\r\nThanks!", "1. I have Python at this location, confirmed with which python\r\n2. I have CUDA 10.1, cuDNN 7.6, TensorRT 5.1.5 and NCCL 2.4.8 installed\r\n3. Yes, I'm trying to install from source in a virtual env.\r\n\r\nPS: It builds without issue if I don't add CUDA support", "I am having the same issue on Ubuntu 18 with Bazel 0.24.1 and CUDA 10/10.1. I've tried several TF releases, including r1.13, r1.14 and r1.15. \r\n\r\n```\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 497, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 489, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 454, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 248, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 235, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 224, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 241, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 497, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 489, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 454, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 248, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 235, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 224, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 241, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n```", "For the record, r1.12 builds just fine on the same system (I've got both CUDA 10 and 10.1 installed).", "Reassigning to @gunan as I don't have context here", "@manuprasad07  \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31872\">No</a>\n", "Hi,\r\n\r\nIs this resolved?\r\n\r\nI'm having the same issue here.\r\n\r\nUbuntu 18.04\r\nRTX 2080 ti\r\ncuda 11.1\r\ncudNN 8.0.4.30\r\nTensorRT 7.2.1.6\r\n\r\ntrying to install the default TF 2.3\r\n\r\n\r\n\r\n\r\nroot:~/tensorflow# git checkout r2.3\r\nBranch 'r2.3' set up to track remote branch 'r2.3' from 'origin'.\r\nSwitched to a new branch 'r2.3'\r\nroot:~/tensorflow# ./configure\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n/usr/lib/python3/dist-packages\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 648, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 640, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 578, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 254, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 241, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 230, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 247, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 11.1\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 8.0.4.30\r\n\r\n\r\nPlease specify the TensorRT version you want to use. [Leave empty to default to TensorRT 6]: 7.2.1.6\r\n\r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]:\r\n\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /lib,/lib/x86_64-linux-gnu,/usr,/usr/lib/x86_64-linux gnu/libfakeroot,/usr/local/cuda,/usr/local/cuda-11.1,/usr/local/cuda-11.1/targets/x86_64-linux/lib,/usr/local/cuda-11.1/targets/x86_64-linux/include,/root/TensorRT-7.2.1.6,/root/TensorRT-7.2.1.6/bin,/root/TensorRT-7.2.1.6/lib,/root/TensorRT-7.2.1.6/include\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 648, in <module>\r\n    main()\r\n  File \"third_party/gpus/find_cuda_config.py\", line 640, in main\r\n    for key, value in sorted(find_cuda_config().items()):\r\n  File \"third_party/gpus/find_cuda_config.py\", line 578, in find_cuda_config\r\n    result.update(_find_cuda_config(cuda_paths, cuda_version))\r\n  File \"third_party/gpus/find_cuda_config.py\", line 254, in _find_cuda_config\r\n    get_header_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 241, in _find_header\r\n    required_version, get_version)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 230, in _find_versioned_file\r\n    actual_version = get_version(file)\r\n  File \"third_party/gpus/find_cuda_config.py\", line 247, in get_header_version\r\n    version = int(_get_header_version(path, \"CUDA_VERSION\"))\r\nValueError: invalid literal for int() with base 10: ''\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]:\r\n\r\n"]}, {"number": 31871, "title": "TF 2.0 dev20190820 consumes more memory than dev20190504 when training keras model using train_on_batch()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190820\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2.24\r\n- GPU model and memory: Geforce RTX 2080 ti 11GB\r\n\r\n**Describe the current behavior**\r\nProgram crashes due to insufficient memory issue. But with same code, dev20190504 can train the model without any problem.\r\n\r\n**Describe the expected behavior**\r\nCan train large model using dev20190820 without memory issue, like dev20190504.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n\r\n\r\ndef conv(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal'):\r\n    c = tf.keras.layers.Conv2D(\r\n        filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=initializer, use_bias=False)(x)\r\n\r\n    return c\r\n\r\n\r\ndef conv_bn(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):\r\n    c = conv(x, filters=filters, kernel_size=kernel_size,\r\n             strides=strides, padding=padding, initializer=initializer)\r\n\r\n    c_bn = tf.keras.layers.BatchNormalization(\r\n        gamma_initializer=bn_gamma_initializer)(c)\r\n\r\n    return c_bn\r\n\r\n\r\ndef conv_bn_relu(x, filters, kernel_size, strides=(1, 1), padding='same', initializer='he_normal', bn_gamma_initializer='ones'):\r\n    c_bn = conv_bn(x, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\r\n                   initializer=initializer, bn_gamma_initializer=bn_gamma_initializer)\r\n\r\n    return tf.keras.layers.Activation('relu')(c_bn)\r\n\r\n\r\ndef conv_gap(x, output_filters, kernel_size=(1, 1)):\r\n    x = conv(x, filters=output_filters, kernel_size=kernel_size)\r\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n\r\n    return x\r\n\r\n\r\ndef my_block(x, output_filters, inter_filters):\r\n    c1 = conv_bn_relu(x, inter_filters, (1, 1))\r\n    c2 = conv_bn_relu(c1, inter_filters, (3, 3))\r\n    c3 = conv_bn(\r\n        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')\r\n\r\n    p = tf.keras.layers.add([c3, x])\r\n\r\n    return tf.keras.layers.Activation('relu')(p)\r\n\r\n\r\ndef my_block_inc(x, output_filters, inter_filters, strides1x1=(1, 1), strides2x2=(2, 2)):\r\n    c1 = conv_bn_relu(\r\n        x, inter_filters, (1, 1), strides=strides1x1)\r\n    c2 = conv_bn_relu(\r\n        c1, inter_filters, (3, 3), strides=strides2x2)\r\n    c3 = conv_bn(\r\n        c2, output_filters, (1, 1), bn_gamma_initializer='zeros')\r\n\r\n    strides = np.multiply(strides1x1, strides2x2)\r\n    s = conv_bn(\r\n        x, output_filters, (1, 1), strides=strides)  # shortcut\r\n\r\n    p = tf.keras.layers.add([c3, s])\r\n\r\n    return tf.keras.layers.Activation('relu')(p)\r\n\r\n\r\ndef repeat_blocks(x, block_delegate, count, **kwargs):\r\n    assert count >= 0\r\n\r\n    for _ in range(count):\r\n        x = block_delegate(x, **kwargs)\r\n    return x\r\n\r\n\r\n# This line makes trick!\r\ntf.keras.backend.set_learning_phase(1)\r\nshape = (299, 299, 3)\r\n\r\ninputs = tf.keras.Input(shape, dtype='float32')\r\noutputs = conv_bn_relu(inputs, 256 // 4, (7, 7), strides=(2, 2))\r\noutputs = tf.keras.layers.MaxPool2D(\r\n    (3, 3), strides=(2, 2), padding='same')(outputs)\r\noutputs = my_block_inc(outputs, 256, 256 // 4, strides2x2=(1, 1))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=2,\r\n                        output_filters=256,\r\n                        inter_filters=256 // 4)\r\n\r\noutputs = my_block_inc(outputs, 512, 512 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=7,\r\n                        output_filters=512,\r\n                        inter_filters=512 // 4)\r\n\r\noutputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=40,\r\n                        output_filters=1024,\r\n                        inter_filters=1024 // 4)\r\n\r\noutputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=16,\r\n                        output_filters=1024,\r\n                        inter_filters=1024 // 4)\r\n\r\noutputs = my_block_inc(outputs, 1024, 1024 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=16,\r\n                        output_filters=1024,\r\n                        inter_filters=1024 // 4)\r\n\r\noutputs = my_block_inc(outputs, 2048, 2048 // 4, strides2x2=(2, 2))\r\noutputs = repeat_blocks(outputs, block_delegate=my_block,\r\n                        count=6,\r\n                        output_filters=2048,\r\n                        inter_filters=2048 // 4)\r\n\r\noutputs = conv_gap(outputs, 1024)\r\noutputs = tf.keras.layers.Activation('sigmoid')(outputs)\r\nmodel = tf.keras.models.Model(\r\n    inputs=inputs, outputs=outputs, name='resnet_custom_v2')\r\n\r\noptimizer = tf.optimizers.Adam(0.001)\r\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy',\r\n              metrics=['mean_absolute_error'])\r\n\r\n\r\ndef make_batch(_):\r\n    x = np.ones((shape[0], shape[1], shape[2]), dtype='float32')\r\n    y = np.ones((1024), dtype='float32')\r\n\r\n    return (x, y)\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n    ['0'] * 40)\r\ndataset = dataset.map(lambda x: tf.py_function(\r\n    make_batch, (x,), (tf.float32, tf.float32)))\r\ndataset = dataset.batch(20)\r\n\r\nfor x, y in dataset:\r\n    step_results = model.train_on_batch(x=x, y=y)\r\n    print(f'loss={step_results[0]}')\r\n\r\nprint('press input key')\r\ninput()\r\n\r\n```\r\n", "comments": ["I have tried on Colab with TF version 2.0.0-dev20190820  and was able to reproduce the issue.I am able to train the model using 2.0.0-dev20190504 without any problem.Please, find the [gist ](https://colab.research.google.com/drive/1y4Z77hmagOig2fqVN1tnp2vOnXHdhuZg)here.Thanks!", "tensorflow-gpu==2.0rc1 still has this issue.", "@robieta @jvishnuvardhan Is there any news for this issue? I can't test latest RC release due to this problem.", "tensorflow-gpu==2.0rc2 still has this issue.", "tensorflow-gpu==2.0.0 still suffers this issue.", "@robieta @jvishnuvardhan TF 2.0.0 is released today, and this issue sill is there. Any news? or is it not a bug, but TF2.0's intended behaviour? ", "This should be fixed by https://github.com/tensorflow/tensorflow/commit/28d07261622a589b7f427756677c2178ccaca7fb. Can you check whether it resolves your issue?", "@robieta I can confirm that when I ran @KichangKim code with `tf-nightly`, code runs without any issue. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/5e8af6cf47aee62ec1596328d8fd84ba/untitled134.ipynb) is the gist. Thanks!", "I checked tf-nightly-gpu==2.1.0.dev20191112 and it worked correctly. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31871\">No</a>\n"]}, {"number": 31870, "title": "TF_CPP_MIN_LOG_LEVEL does not work with TF2.0 dev20190820", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190820\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10, cuDNN 7.6.2.24\r\n- GPU model and memory: Geforce RTX 2080 ti 11GB\r\n\r\n**Describe the current behavior**\r\nSetting TF_CPP_MIN_LOG_LEVEL does not work lateset TF 2.0. If I set TF_CPP_MIN_LOG_LEVEL to 2, sill TF shows information and warning logs (including libpng warnings)\r\n\r\nWith dev20190504, this issue does not occurred.\r\n\r\n**Describe the expected behavior**\r\nSet TF_CPP_MIN_LOG_LEVEL=2 should prevent showing information/warning logs including libpng warnings.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\n\r\na = tf.Variable(np.array([0, 1, 2]))\r\n\r\nprint(a)\r\n```\r\nYou can see many [I] log using dev20190820. But with dev20190504, no logs.", "comments": ["@KichangKim I tried on Colab with Tensorflow 2.0.0-dev20190820. Please see the colab [gist](https://colab.research.google.com/drive/12nzwoqnttwa4KFXR-xKs2MlnaDDWVghQ) and let us know the expected result. Thanks!", "@gadagashwini Thanks for testing. But gist's result differ to my local result.\r\nHere is my output:\r\n```\r\n2019-08-22 20:52:13.256379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-08-22 20:52:19.121304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-08-22 20:52:19.391093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\npciBusID: 0000:06:00.0\r\n2019-08-22 20:52:19.398066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-08-22 20:52:19.410017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-08-22 20:52:19.419415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2019-08-22 20:52:19.426158: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2019-08-22 20:52:19.450504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2019-08-22 20:52:19.501850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2019-08-22 20:52:19.545794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-08-22 20:52:19.551803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 20:52:19.555811: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-08-22 20:52:19.564630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\npciBusID: 0000:06:00.0\r\n2019-08-22 20:52:19.569926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-08-22 20:52:19.573802: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-08-22 20:52:19.578127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\r\n2019-08-22 20:52:19.582843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll\r\n2019-08-22 20:52:19.593490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll\r\n2019-08-22 20:52:19.598530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll\r\n2019-08-22 20:52:19.604157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-08-22 20:52:19.610050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 20:52:21.725773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 20:52:21.735777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-08-22 20:52:21.738297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-08-22 20:52:21.742246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8686 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\n<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([0, 1, 2])>\r\nPS C:\\Users\\kicha\\Downloads\\tensorflow-issue>\r\n```\r\nCan you try my code on local machine?", "same situation here, when i update my tf.nightly version to 2.0rc, TF_CPP_MIN_LOG_LEVEL does not work anymore.", "After a quick dig into this, it seems that the problem appear only if you change the log level after import tensorflow. \r\n```python\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\nimport tensorflow as tf\r\nimport numpy as np\r\na = tf.Variable(np.array([0, 1, 2]))\r\nprint(a)\r\n```\r\noutput:\r\n```python\r\n<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([0, 1, 2])>\r\n```\r\nif i change set the environ after import tf:\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\nimport numpy as np\r\na = tf.Variable(np.array([0, 1, 2]))\r\nprint(a)\r\n```\r\noutput:\r\n```python\r\n...\r\n2019-08-26 11:45:17.907378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-08-26 11:45:20.776347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-08-26 11:45:20.885617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\npciBusID: 0000:01:00.0\r\n2019-08-26 11:45:20.886163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\npciBusID: 0000:02:00.0\r\n2019-08-26 11:45:20.886391: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-08-26 11:45:20.887824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-08-26 11:45:20.888138: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports \r\n...\r\n```\r\nThis happens in Tensorflow 2.0rc, however Tensorflow beta0&beta1 works good no matter the order of import.\r\n\r\nMaybe in the latest code change the logger reading the TF_CPP_MIN_LOG_LEVEL once when init, and in the older version, the logger was reading TF_CPP_MIN_LOG_LEVEL on the fly. Could you please confirm if this is intend to do or a bug introduced in the latest version?\r\n@gadagashwini \r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Reading envvalues on init only seems the better approach to me, as that ensures determinism.", "This is fixed in tf-2.0 nightly build version 2.0.0-dev20190826 in the sense we see:\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\nimport numpy as np\r\na = tf.Variable(np.array([0, 1, 2]))\r\nprint(a)\r\n```\r\noutput:\r\n```python\r\n<tf.Variable 'Variable:0' shape=(3,) dtype=int64, numpy=array([0, 1, 2])>\r\n```\r\n\r\n", "@ymodak \r\n![1](https://user-images.githubusercontent.com/9452949/63738157-7b9f7c80-c8bb-11e9-90a7-6948b5efbb1a.png)\r\ntried 2.0.0-dev20190826, still not work. Maybe the platform-differ issues?\r\nI'm testing this on win10.\r\n\r\nAs for compare, the behavior of tf-beta0(the warnings about numpy is not realted to the issues we discussed here, just ignore them):\r\n![2](https://user-images.githubusercontent.com/9452949/63739026-62e49600-c8be-11e9-9da3-cddb7cfe0c65.png)\r\n\r\n\r\n\r\n\r\n\r\n", "I used command line to reproduce the behavior not an ide. Thanks for the screenshots. I will try using ide.", "I am not sure this is related. I am using `2.0.0rc0`, but it seems the same issue is in `2.0.0.a0`. My code is \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.get_logger().setLevel('ERROR')\r\nprint(\"Hi!\")\r\noutput = tf.keras.layers.Conv1D(1, 1)(tf.keras.layers.Input([1, 1]))\r\n```\r\n(I found this code in #28577, #28578).\r\n\r\nand the output includes:\r\n```\r\nHi!\r\n2019-08-28 10:57:14.647006: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n```\r\n\r\nIf that is not the same issue, I can file it separately.", "> I am not sure this is related. I am using `2.0.0rc0`, but it seems the same issue is in `2.0.0.a0`. My code is\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> tf.get_logger().setLevel('ERROR')\r\n> print(\"Hi!\")\r\n> output = tf.keras.layers.Conv1D(1, 1)(tf.keras.layers.Input([1, 1]))\r\n> ```\r\n> \r\n> (I found this code in #28577, #28578).\r\n> \r\n> and the output includes:\r\n> \r\n> ```\r\n> Hi!\r\n> 2019-08-28 10:57:14.647006: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n> ```\r\n> \r\n> If that is not the same issue, I can file it separately.\r\n\r\n@bersbersbers \r\nI think its not the same issue, tf.get_logger().setLevel('ERROR') means set your python tensorflow log level to \"ERROR\", and the evn value \"TF_CPP_MIN_LOG_LEVEL\" is to set tensorflow c++ log level.\r\nSo in my code, i usually set them both like this:\r\n\r\n```python\r\n if args.debug is True:\r\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"0\"\r\n        tf.get_logger().setLevel(\"INFO\")\r\nelse:\r\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\r\n        tf.get_logger().setLevel(\"ERROR\")\r\n```", "@ymodak any update?", "> It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nyes it is.", "There is a new RC. Can you try with that one?", "@mihaimaruseac still an issue in RC2. In case it is an IDE caused issue, i also tried on console:\r\n\r\n```\r\n# this one still print the log\r\npython -c \"import os;import tensorflow as tf;os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2';import numpy as np;print(tf.Variable(np.array([0,1,2])))\"\r\n# this one will not print the log\r\npython -c \"import os;os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2';import tensorflow as tf;import numpy as np;print(tf.Variable(np.array([0,1,2])))\"\r\n```\r\n\r\nyou could directly copy and try this on cmd in your env to see if this happen. And of course it's not a big deal if tf team did this change on purpose. I can change the habit of my code according to tensorflow's preference. But if this is not on purpose, I'm afraid it may cause other bugs.", "So I think you have to set the envvar before importing tf.", "> So I think you have to set the envvar before importing tf.\r\n\r\nYes if the team think the change was on purpose.\r\n\r\nFor my previous use case, i accept the args passed by argparse like this:\r\n\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nimport argparse\r\ndef some_tf_fn():\r\n    ...\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--log\", type=bool, required=False, default=False)\r\n    args = parser.parse_args()\r\n\r\n    if args.log:\r\n        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\r\n        tf.get_logger().setLevel(\"INFO\")\r\n    else:\r\n        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\n        tf.get_logger().setLevel(\"ERROR\")\r\n```\r\nIn tf2.0-beta1, i can do things like this, but now due to this change i should come up with a plan B. That's not a big deal, just want some confirmation from you guys since the change is not mentioned in any document. @mihaimaruseac\r\n\r\n\r\n\r\n\r\n\r\n", "I think it is some side effect on the interaction between absl and tensorflow.", "OK, fair enough, i personally have no further question. thank you very much for the patient follow up though.", "I will try to investigate this further in my 20% time.", "> So I think you have to set the envvar before importing tf.\r\n\r\nTo add one more data points:\r\nFollowing the official example at https://github.com/NVIDIA/tensorflow-determinism, the `TF_DETERMINISTIC_OPS` envvar can be set after importing `tensorflow`:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\r\n# Now build your graph and train it\r\n```\r\n", "@mihaimaruseac Thanks for investigation. So in my understand, this change is intended and will not be fixed, right? (if it is correct, I'll close this issue.)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31870\">No</a>\n", "> So in my understand, this change is intended\r\n\r\nI wonder what part of the discussion above makes you think that. @mihaimaruseac interpreted this as a side effect (not an intended effect) of some change, and has not concluded otherwise since mentioning to investigate this further.", "@bersbersbers Oh, I misread ziyigogogo's comment. It seems that not yet concluded. Reopen.", "Why the bot labeled this issue as \"awaiting response\"? It should be clearly labeled as \"awaiting tensorflow\".", "Hi. Is the issue solved?\r\nIf yes, could someone tell me how to avoid INFO and WARNING messages in output?", "> Hi. Is the issue solved?\r\n> If yes, could someone tell me how to avoid INFO and WARNING messages in output?\r\n\r\n@akhil189 It seems not fixed yet. Temporary workaround is that call `os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" ` before `import tensorflow as tf`, or manually set environment variable TF_CPP_MIN_LOG_LEVEL=2 before running your python script.\r\n```\r\n> SET TF_CPP_MIN_LOG_LEVEL=2\r\n> python your_script.py\r\n```", "@KichangKim Thank you for providing the workaround. But, I am using google colab and it's not working in that. The 'INFO' and 'WARNING' messages are still being shown in the output. ", "Hello, was there any resolution to this issue? I am having the same issue.", "This issue seem to be still open, but with [workaround](https://github.com/tensorflow/tensorflow/issues/31870#issuecomment-553269261) mentioned by @KichangKim works fine.\r\nVersion : `2.3.0`\r\n\r\nSample for verification\r\n```\r\n$ python36\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-09-08 04:26:59.370871: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-08 04:26:59.379059: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\n```\r\n$ python36\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import os\r\n>>> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\r\n>>> import tensorflow as tf\r\n2020-09-08 04:27:33.738961: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n```\r\n\r\nI know this is not an ideal env. I am using cygwin, on top of that I am calling a python binary installed for windows. Anyways, I think it does prove that the workaround for logging works.", "I tried this in TF nightly version & didn't face any issue reported ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/8f62c3d79013d639850ffd5f4415a860/untitled101.ipynb?authuser=1)....thanks !", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31870\">No</a>\n", "I am getting warnings \"WARNING:tensorflow:The following Variables were used in a Lambda layer's call\" even if I set the env var. Using tf.get_logger().setLevel('ERROR') works though. Anyone else with the same issue?"]}, {"number": 31869, "title": "Fix v2 compatibility with moving average", "body": "PiperOrigin-RevId: 264688574\r\n(cherry picked from commit dc3534c6a502d2830634f51e08de507163aac952)", "comments": []}, {"number": 31868, "title": "Fix v2 compatibility with moving average", "body": "PiperOrigin-RevId: 264688574\r\n(cherry picked from commit dc3534c6a502d2830634f51e08de507163aac952)", "comments": []}, {"number": 31867, "title": "Bug on last batch of first epoch using TPU with pretrained-model on google colab", "body": "Hi,\r\n\r\nI am trying to train a mobileNetV2 coming from tf.keras but the training stops on the last batch of the first epoch: \r\nEpoch 1/15\r\n1/2 [==============>...............] - ETA: 10s - loss: 2.7432 - acc: 0.1406\r\nDon't mind about the number of batch, adding some more  changes nothing\r\n\r\nHere's the error :\r\nCompilation failure: Asked to propagate a dynamic dimension from hlo %convert.283 = f32[8,80,80,32]{3,2,1,0} convert(f32[8,80,80,32]{3,2,1,0} %add.1), metadata={op_type=\"FusedBatchNorm\" op_name=\"bn_Conv1_2/FusedBatchNorm\"}@{}@0 to hlo %clamp.288 = f32[8,80,80,32]{3,2,1,0} clamp(f32[8,80,80,32]{3,2,1,0} %broadcast.286, f32[8,80,80,32]{3,2,1,0} %convert.283, f32[8,80,80,32]{3,2,1,0} %broadcast.287), metadata={op_type=\"Relu6\" op_name=\"Conv1_relu_2/Relu6\"}, which is not implemented.\r\n\tTPU compilation failed\r\n\t [[{{node TPUReplicateMetadata_1}}]]\r\n\r\nimage of the error for more clarity:\r\nhttps://drive.google.com/open?id=1Gkzow3mrUJPACYfjz6E7V3Efj8eO261F\r\n\r\nHere's my configuration below:\r\n\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\n\r\ntensorflow=1.14.0\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n\r\nHere's the link to my ipynb : https://drive.google.com/open?id=1w2VJAH1gIv6-tJms3qrQg8IxSoKS1aw0\r\n\r\nAny idea of what's going on ?", "comments": ["Tell me if you have question about my code.", "@amapic Please provide a standalone code to reproduce the issue. Please use any toy dataset if the data you are using is private. Thanks!", "Thanks, I made it works by using cifar10. But the problem is not really solved for me, do you have any idea of what was going on with my own data ? It's regular image.\r\nFurthermore, with cifar10, I now have this result :\r\n\r\nEpoch 1/15\r\n2/2 [==============================] - 15s 8s/step\r\n2/2 [==============================] - 15s 8s/step\r\n2/2 [==============================] - 35s 18s/step - loss: 2.7288 - acc: 0.0781 - val_loss: 2.3025 - val_acc: 0.0781\r\n\r\nWhy is there several line of the same steps ?\r\n\r\nHere's my standalone code:\r\n\r\n```\r\nfrom google.colab import drive\r\ndrive.mount('/content/drive')\r\nimport sys\r\nfrom PIL import Image\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\n\r\n(x_train, y_train), (x_test, y_test)=tf.keras.datasets.cifar10.load_data()\r\nimg_size = 160\r\nEPOCHS = 15\r\nnb_imag=160\r\n\r\nx_train, y_train=x_train[0:128], y_train[0:128]\r\nx_test, y_test=x_test[0:128], y_test[0:128]\r\nfrom skimage.transform import resize\r\n\r\nfrom sklearn import preprocessing\r\nimgs=[]\r\nimgs_test=[]\r\nfor img in x_train:\r\n    im = Image.fromarray(img)\r\n    imgs.append(resize(np.array(im), (img_size, img_size,3), anti_aliasing=False))\r\n    \r\nfor img in x_test:\r\n    im = Image.fromarray(img)\r\n    imgs_test.append(resize(np.array(im), (img_size, img_size,3), anti_aliasing=False))\r\n    \r\nIMG_ROWS, IMG_COLS = img_size, img_size\r\nimgs=np.array(imgs)\r\nimgs_test=np.array(imgs_test)\r\nlabels=y_train\r\nlabels_test=y_test\r\ninput_shape = (IMG_ROWS, IMG_COLS, 3)\r\n\r\nx_train = imgs.astype('float32')\r\nx_train /= 255\r\nx_test = imgs_test.astype('float32')\r\nx_test /= 255\r\n\r\nprint('x_train shape:', imgs.shape)\r\n\r\nNUM_CLASSES=np.unique(labels).size\r\nprint(\"NUM_CLASSES\",NUM_CLASSES)\r\n\r\n\r\ny_train = tf.keras.utils.to_categorical(labels, NUM_CLASSES)\r\ny_test = tf.keras.utils.to_categorical(labels_test, NUM_CLASSES)\r\n\r\nBATCH_SIZE = 64\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = build_model(IMG_ROWS, NUM_CLASSES, 1, 0.5, 3)\r\n  model.compile(\r\n  loss=tf.keras.losses.categorical_crossentropy,\r\n  optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05),\r\n  metrics=['accuracy'])\r\n  \r\nmodel.fit(\r\n  x_train,\r\n  y_train,\r\n  batch_size=BATCH_SIZE,\r\n  epochs=EPOCHS,\r\n  verbose=1,\r\n  validation_data=(x_test, y_test))\r\n```\r\n", "@amapic As it is working with cifar data, i am sure it is not a bug related to Tensorflow.\r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community (including us) to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. \r\n\r\nWhen you post it in Stackoverflow, please provide atleast partial data of your to find the root-cause.\r\nThanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31867\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31867\">No</a>\n"]}, {"number": 31866, "title": "error: 'is_final' is not a member of 'std'", "body": "With \r\n  bazel 0.26.1\r\n  gcc 7.4\r\n  cuda 10.1 update 2\r\n\r\nand the latest git clone of tensorflow, I hit the following error at the bazel build command\r\n\r\n```\r\nERROR: /home/mh.naderan/.cache/bazel/_bazel_mh.naderan/dacf7a124fc721f30ac789c201b3b139/external/llvm/BUILD.bazel:201:1: C++ compilation of rule '@llvm//:llvm-tblgen' failed (Exit 1)\r\nIn file included from external/llvm/include/llvm/TableGen/Record.h:27:0,\r\n                 from external/llvm/utils/TableGen/SubtargetFeatureInfo.h:13,\r\n                 from external/llvm/utils/TableGen/SubtargetFeatureInfo.cpp:9:\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h: In static member function 'static void llvm::TrailingObjects<BaseTy, TrailingTys>::verifyTrailingObjectsAssertions()':\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h:252:24: error: 'is_final' is not a member of 'std'\r\n     static_assert(std::is_final<BaseTy>(), \"BaseTy must be final.\");\r\n                        ^~~~~~~~\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h:252:24: note: suggested alternative: 'is_heap'\r\n     static_assert(std::is_final<BaseTy>(), \"BaseTy must be final.\");\r\n                        ^~~~~~~~\r\n                        is_heap\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h:252:39: error: expected primary-expression before '>' token\r\n     static_assert(std::is_final<BaseTy>(), \"BaseTy must be final.\");\r\n                                       ^\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h:252:41: error: expected primary-expression before ')' token\r\n     static_assert(std::is_final<BaseTy>(), \"BaseTy must be final.\");\r\n                                         ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 442.171s, Critical Path: 25.05s\r\nINFO: 1254 processes: 1254 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["see https://github.com/tensorflow/tensorflow/commit/09c588de14f02bc2f07e6de8fca01093f92ba4f0", "@bas-aarts I get the same failure. (Doesn't everyone? Why not?) Reverting 09c588d fixes it for me; thanks.", "OK. I fixed that. I am looking for a working version of tensorflow. I hit another error on 1.14.0 which is described in another thread.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31866\">No</a>\n", "@mahmoodn Why did you close this issue? The master branch still has this problem, at least for me.", "Re-filed as #31900, since this is definitely not fixed.", "@mahmoodn Is this duplicate of https://github.com/tensorflow/tensorflow/issues/31900? If yes, close this as the code owner is communicating with the user in that issue. Thanks!", "Yes. Mine was fixed.\r\nHowever, @plopresti still has the problem based on his last post.\r\n", "@mahmoodn I am closing this issue are it was resolved. @plopresti had opened another issue https://github.com/tensorflow/tensorflow/issues/31900. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31866\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31866\">No</a>\n"]}, {"number": 31865, "title": "Remove contrib from the 2.0 directory.", "body": "", "comments": ["Check out this pull request on ReviewNB: https://app.reviewnb.com/tensorflow/tensorflow/pull/31865 \n\n You'll be able to see notebook diffs and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "Ha. Let's hope this works. I bet there are some things in the builds that rely on some rule being present or whatnot. :)\r\n\r\nShould we do this on master after 1.15, and then pull this over as a regular cherry-pick? Or do we want to make sure this is in RC0?", "I apologize for the spam, GitHub automatically added reviewers. Deleting this PR."]}, {"number": 31864, "title": "r2.0-CherryPick: Refactor the keras TensorLikeDataAdapter (numpy array, EagerTensor, etc) to use tf.shuffle rather than np.shuffle", "body": "This cherrypick depends on: https://github.com/tensorflow/tensorflow/pull/31863", "comments": []}, {"number": 31863, "title": "Fix incompatibility between tf.data rebatching fallback & unknown batch dim (from partial batch)", "body": "", "comments": []}, {"number": 31862, "title": "Fix regression in memory consumption on arm64 devices", "body": "This avoids some of the more pathological cases where memory consumption can double with larger models.", "comments": []}, {"number": 31861, "title": "Update release notes for TensorFlow 1.15.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.15.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": ["Updated the release notes for 1.15.0.", "I added the `tf.estimator` notes, please take a look."]}, {"number": 31860, "title": "Fix incompatibility between tf.data rebatching fallback & unknown batch dim (from partial batch)", "body": "1. [tf.data] Make rebatching fallback work for datasets with unknown shapes.\r\n\r\nPiperOrigin-RevId: 264501770", "comments": ["Ah, there's a dependency i forgot to include, resulting in broken names everywhere. Stay tuned.", "Abandoning this, will create another PR.", "Please look at https://github.com/tensorflow/tensorflow/pull/31863 instead."]}, {"number": 31859, "title": "Tensorflow lite: micro_vision missing model data", "body": "On the latest commit (eb7b5e6) on master branch, the mico_vision example is still missing `person_detect.tflite` as well as `person_detect_model_data.cc`. This problem is mentioned in issues #29792 and #29524. The only resolution mentioned in these closed issues are mentions that it \"should be fixed now\" without mention to a commit that fixes it or a pointer to the files existing in a branch.\r\n\r\nIf these files do exist in some history and branch, where can I find them?\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31859\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31859\">No</a>\n"]}, {"number": 31858, "title": "Revert \"Deprecate `ModelCheckpoint.__init__`'s `load_weights_on_restart` argument and provide a warning message if used.\"", "body": "Reverts tensorflow/tensorflow#29458\r\n\r\nShouldn't have merged this into 1.14 - as it should be in master, I think it will ship with 1.15.", "comments": []}, {"number": 31857, "title": "Fixing a couple of issues with DenseFeatures", "body": "1. Fixing variable_scope to not pass in partition_info argument if its in V2 mode. This ensures compatibility with V2 initializers\r\n2. Adding a tracking_name argument to add_variable which allows for passing in a different name for tracking purposes without affecting the variable name.\r\n\r\nPiperOrigin-RevId: 264658202", "comments": []}, {"number": 31856, "title": "1. Fix incorrect steps inference when validation_split is provided in\u2026", "body": "\u2026 fit in v2 single path execution.\r\n\r\n2. Infer validation_steps/steps for a dataset when validation_steps/steps is not provided in fit/(evaluate and predict).\r\n\r\nPiperOrigin-RevId: 264508014", "comments": []}]