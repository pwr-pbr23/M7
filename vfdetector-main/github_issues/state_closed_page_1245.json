[{"number": 15803, "title": "Memory allocation improvement for `decode_libsvm`", "body": "This fix is an improvement to #14330. Previously, string split was handled through `str_util::Split`, which may incur unnecessary memory allocations. This fix uses StringPiece instead.\r\n\r\nSee comment https://github.com/tensorflow/tensorflow/pull/14330#pullrequestreview-79877956 for reference.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the review, @mrry "]}, {"number": 15802, "title": "tf.stack eats memory over time", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**:  3.5\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: 1060 + 6GB\r\n \r\n\r\n### Describe the problem\r\nI use tf.stack to stack 2 images. But the memory used by this program increase over time. I use `memory_profiler` check it. it is caused by tf.stack, here is the minimal re-produce code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport glob\r\nimport gc\r\nfrom memory_profiler import profile\r\n\r\n\r\n@profile\r\ndef function_mark():\r\n  pass\r\n\r\n@profile\r\ndef stack_images():\r\n  image_file_list = glob.glob(\"car_images/*.jpg\")\r\n  sess = tf.Session()\r\n\r\n  for _ in range(300):\r\n    # read image\r\n    image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()\r\n    image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()\r\n    # decode image\r\n    image1_decode = tf.image.decode_image(image1, channels=3)\r\n    image2_decode = tf.image.decode_image(image2, channels=3)\r\n    # stack image\r\n    image_stack = tf.stack([image1_decode, image2_decode])\r\n    # run session\r\n    r_image_stack = sess.run(image_stack)\r\n    # mark function. so I can check the memory-usage of every loop.\r\n    function_mark()\r\n    # force garbage collection, so all the un-reference variable will be freed.\r\n    del r_image_stack\r\n    gc.collect()\r\n```\r\n\r\n First, I profile it line by line, here is the result:\r\n**you can see it very clearly that line 26 take a lot of memory. My image is 800*600 and I only stack 2 image each time, so 1.3G memory consumption is not normal.**\r\n\r\n> \r\n> Line #    Mem usage    Increment   Line Contents\r\n> ================================================\r\n>     11    190.0 MiB    190.0 MiB   @profile\r\n>     12                             def stack_images():\r\n>     13    190.0 MiB      0.0 MiB     image_file_list = glob.glob(\"car_images/*.jpg\")\r\n>     14    421.6 MiB    231.7 MiB     sess = tf.Session()\r\n>     15\r\n>     16   1998.4 MiB      0.0 MiB     for _ in range(300):\r\n> \r\n>     17                                 # read image\r\n>     18   1992.5 MiB      1.5 MiB       image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()\r\n>     19   1992.5 MiB      0.0 MiB       image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()\r\n>     20                                 # decode image\r\n>     21   1992.8 MiB     77.6 MiB       image1_decode = tf.image.decode_image(image1, channels=3)\r\n>     22   1993.3 MiB    108.6 MiB       image2_decode = tf.image.decode_image(image2, channels=3)\r\n>     23                                 # stack image\r\n>     24   1993.3 MiB      0.7 MiB       image_stack = tf.stack([image1_decode, image2_decode])\r\n>     25                                 # run session\r\n>     26   1998.4 MiB   1350.4 MiB       r_image_stack = sess.run(image_stack)\r\n>     27                                 # mark function. so I can check the memory-usage of every loop.\r\n>     28   1998.4 MiB     29.0 MiB       function_mark()\r\n>     29                                 # force garbage collection, so all the un-reference variable will be freed.\r\n>     30   1998.4 MiB      0.0 MiB       del r_image_stack\r\n>     31   1998.4 MiB      8.9 MiB       gc.collect()\r\n\r\nThen **I profile it over time.** I use function `function_mark` to distinguish each loop. So we can see it very clearly that the memory usage of this program is increase over time.\r\n![figure_1](https://user-images.githubusercontent.com/5325686/34505710-d25c80b0-f061-11e7-99c6-5db2e990d9aa.png)\r\n\r\nMy question is: How should I avoid this problem. because it cause a serious performance regression.\r\n", "comments": ["This is another instance of the \"constructing new graph nodes in a loop\" memory leak, where the proximate cause is not actually the `tf.stack()` operation, but rather the implicit conversion of `image1` and `image2` to `tf.constant()` nodes, which get added to the graph over time.\r\n\r\nThe simplest solution is to construct the graph once and feed different values to it in each iteration:\r\n\r\n```python\r\n@profile\r\ndef stack_images():\r\n  image_file_list = glob.glob(\"car_images/*.jpg\")\r\n\r\n  image1 = tf.placeholder(tf.string, shape=[])\r\n  image2 = tf.placeholder(tf.string, shape=[])\r\n  # decode image\r\n  image1_decode = tf.image.decode_image(image1, channels=3)\r\n  image2_decode = tf.image.decode_image(image2, channels=3)\r\n  # stack image\r\n  image_stack = tf.stack([image1_decode, image2_decode])\r\n\r\n  sess = tf.Session()\r\n\r\n  for _ in range(300):\r\n    # read image\r\n    image1_data = tf.gfile.FastGFile(image_file_list[0], 'rb').read()\r\n    image2_data = tf.gfile.FastGFile(image_file_list[1], 'rb').read()\r\n\r\n    # run session\r\n    r_image_stack = sess.run(image_stack, feed_dict={image1: image1_data, image2: image2_data})\r\n\r\n    # mark function. so I can check the memory-usage of every loop.\r\n    function_mark()\r\n    # force garbage collection, so all the un-reference variable will be freed.\r\n    del r_image_stack\r\n    gc.collect()\r\n```", "@mrry, thanks very much. now the memory usage is constant and running time is  decrease to 2 seconds (original implementation is 130 seconds).", "In case anybody stumbles across this in the future I thought I would mention that if you get into the habit of calling `graph.finalize()` after defining the static graph, you will always be presented with an exception if nodes are added to the graph when you aren't expecting them rather than running into an exploding memory problem."]}, {"number": 15801, "title": "Remove calculation of unnecessary matrix columns in SVD gradient", "body": "The SVD gradient calculation when `compute_uv=False` currently uses the orthogonal \"U\" and \"V\" matrices returned by the SVD operation with `full_matrices=True`, but it really requires only the `full_matrices=False` versions.  This pull request makes the calculation use the `full_matrices=False` versions.\r\n\r\n@rmlarsen pointed out that this change could be made in https://github.com/tensorflow/tensorflow/pull/14259#discussion_r157067512.", "comments": ["Can one of the admins verify this patch?", "If desired, I could also make some other simplifications to the code that calculates the gradient when `compute_uv=False`:\r\n1. Remove the conditional on `use_adjoint`: the formula is really the same whether `use_adjoint` is `True` or `False`.\r\n2. Move it outside the `with ops.control_dependencies([grad_s, grad_u, grad_v])` block, perhaps to a new `with ops.control_dependencies([grad_s])` block.  (It doesn't use `grad_u` or `grad_v`.)  It's not clear to me why `ops.control_dependencies` is needed here at all, though, so I won't do this unless someone else confirms that it makes sense to do it.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@vnavkal Thanks for the contribution and sorry for the delay. It would be wonderful if you could implement the proposed improvements to the gradients in this PR as well. What do you think?", "Sure, I'll make the extra changes I suggested.", "@vnavkal Thank you! Let me know when this is ready for another look.", "I added two commits that implemented the suggestions:\r\n1. 15519c79c7b37533dd447c0abb91985b2c31a187 simplifies and consolidates the logic for the case when `compute_uv` is false.\r\n2. fc109072c64d5ef7b48c1c604d7dca3360984448 assigns `op.get_attr(\"full_matrices\")` to the local variable `full_matrices`.  I didn't create a variable for `op.get_attr(\"compute_uv\")` because it's now used only once.", "@rmlarsen this should be ready for another look whenever you get a chance.  Thanks!"]}, {"number": 15800, "title": "[bug] tf.estimator.DNNClassifier setting n_classes has no effect", "body": "### Describe the problem\r\nI was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py\", line 222) keeps kicking back the following error:\r\n\r\n**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**\r\n\r\n### Source code / logs\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntrainX = np.array([1,0,2,3])\r\nnum_classes = 4\r\nfeature_names = ['f1']\r\nfeature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\r\nclassifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, \r\n                                            n_classes=num_classes, #setting number of classes here\r\n                                            hidden_units=[10])\r\n\r\ndef input_fn():\r\n    my_int_variable = tf.get_variable(\"my_int_variable\", [1], dtype=tf.int32, initializer=tf.zeros_initializer)\r\n    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes\r\n    return {'f1':trainX},label\r\n\r\nclassifier.train(input_fn=lambda: input_fn())\r\n```\r\n\r\n``` bash\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    classifier.train(input_fn=lambda: input_fn())\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 711, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 694, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 334, in _model_fn\r\n    config=config)\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 203, in _dnn_model_fn\r\n    logits=logits)\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py\", line 493, in create_estimator_spec\r\n    features=features, mode=mode, logits=logits, labels=labels)\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py\", line 433, in create_loss\r\n    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))\r\n  File \"/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py\", line 222, in _check_labels\r\n    (expected_labels_dimension, dim1))\r\nValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.\r\n```\r\n\r\n------------------------\r\n\r\n### System information\r\nMac OSX 10.12.6\r\nPython 2.7\r\nTensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')\r\n\r\n\r\n\r\n", "comments": ["How about removing `tf.one_hot` for label?\r\n\r\n> labels: `Tensor` of shape [batch_size, 1] or [batch_size] labels of dtype `int32` or `int64` in the range `[0, n_classes)`.\r\n\r\n  ", "If I remove the one_hot for the label I get the following error:\r\n``` bash\r\nValueError: Dimensions must be equal, but are 4 and 1 for 'dnn/head/sparse_softmax_cross_entropy_loss/xentropy/xentropy' (op: 'SparseSoftmaxCrossEntropyWithLogits') with input shapes: [4,4], [1].\r\n```\r\nFor background: I have a bunch of 1 second audio data clips of people talking (47 distinct voices). It's in a binary format where the first 2 bytes are the id of the speaker and the next 16,000 bytes are the raw audio data. Perhaps I am using this estimator incorrectly. My assumption was to create one hot labels based on the id of the row and use that as the label. If this is not the intended way for this estimator to be used I will close this issue and revisit my approach. ", "I revisited my input function. Works as expected. \r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntrainX = np.array([1,0,2,3])\r\nlabelX = np.array([1,0,1,0])\r\nnum_classes = 2\r\nfeature_names = ['f1']\r\nfeature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\r\nclassifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, \r\n                                            n_classes=num_classes, #setting number of classes here\r\n                                            hidden_units=[10])\r\n\r\ndef input_fn():\r\n    def gen1(a,b):\r\n        return {'f1':a},b\r\n    dataset = tf.data.Dataset.from_tensor_slices((trainX, labelX)).map(gen1)\r\n    dataset = dataset.repeat(8)\r\n    dataset = dataset.batch(32)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    data, labels = iterator.get_next()\r\n    return data, labels\r\n\r\ndef input_fn_pred(idx):\r\n    def gen1(a,b):\r\n        return {'f1':[idx]},[idx]\r\n    dataset = tf.data.Dataset.from_tensor_slices((trainX, trainX)).map(gen1)\r\n    dataset = dataset.batch(1)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    data, labels = iterator.get_next()\r\n    return data, labels\r\n\r\nclassifier.train(input_fn=lambda: input_fn())\r\n\r\npredict_results = classifier.predict(\r\n    input_fn=lambda: input_fn_pred(2))\r\nfor prediction in predict_results:\r\n   print prediction[\"class_ids\"][0] \r\n```", "I actually ran into this problem recently. My labels shape for the dataset is `(?, 10)` by default and passed `n_classes=10` in a `LinearClassifier`, receiving a similar error.\r\n\r\nDigging a bit deeper - the error message is a bit misleading, as I noticed that `expected_labels_dimension` is hard-coded to be 1 in the `create_loss` function [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py#L675).", "I am also receiving this same error. No matter what I put for n_classes it says: \r\n\r\n`Classifier configured with n_classes=1.  Received 10.`", "Same error on my end-has this been fixed in an update?", "I have the same error. Any fix?", "For a given Labeled data(Location) how can we write a code in Tensorflow to predict Unlabeled data(Location).\r\n[data.zip](https://github.com/tensorflow/tensorflow/files/2889416/data.zip)\r\n\r\n\r\nI am new to Tensorflow and trying to write code to run and predict but not able to understand and write.Please help me if anyone has experience in Tensorflow.I have attached the data file"]}, {"number": 15799, "title": "Fixing the item_test failure.", "body": "Implementing @caisq fix for the item test. Push + CP will take quite a long time.", "comments": []}, {"number": 15798, "title": "Fix docs to recommend cuDNN 6.0, rather than the old 5.1 or non-exist\u2026", "body": "\u2026ent 6.1.\r\n\r\nAlso see #14805\r\n\r\nPiperOrigin-RevId: 177097162", "comments": []}, {"number": 15797, "title": "Change dso_loader to look for libcupti.so instead of libcupti.so.8.0", "body": "On Android, it is hard to package libcupti.so.8.0 with bazel to generate CUDA-enabled apk\r\nThe cc_library macro in bazel only looks for *.so, not *.so.*", "comments": ["Can one of the admins verify this patch?", "How does this affect non-Android builds of TF?", "Versions of dsos may be significant, especially if a user has multiple versions of cuda installed.\r\nTherefore, My inclination is to reject this PR.", "Then is there a safe way to verify if libcupti.so.8.0 exist without crash the entire program?", "We may be able to modify `GetDsoHandle` to first check the file existence, write a meaningful error message and then fail the program."]}, {"number": 15796, "title": "Fix build issues with cuda 9.1 through updating eigen.", "body": "", "comments": ["Jenkins, test this please.", "Nice!"]}, {"number": 15795, "title": "configure.py environment variables", "body": "Have I written custom code: No\r\nOS Platform and Distribution: Linux (Any?)\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.4 (Master Branch)\r\nBazel version: 0.6\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\n\r\n### System information\r\n- Building from source\r\n- Branch Master (currently d87a9fbbc5f49ec5ae8eb52c62628f0b1a0bf67f)\r\n\r\n\r\n\r\n### Describe the problem\r\nLine 1353 of configure.py needs to have int( ) wrapped around the function get_var, otherwise the string that is returned always flags positive and setting TF_SET_ANDROID_WORKSPACE=0 environment variable to avoid user interaction in building from source will never work.\r\n\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It appears to have been fixed with update 136697ecdc64b5171522fb7f89cfe51a02f0f1c1.\r\n\r\nThank you"]}, {"number": 15794, "title": "Fix kmeans gpu and upgrading TF base images to 16", "body": "", "comments": []}, {"number": 15793, "title": "Feature Request: tf.train.MonitoredTrainingSession implementation for slim.learning.train", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nThis is a feature request:\r\nIssue #6263 states that tf.train.Supervisor is to be deprecated. Since the slim.learning.train uses Supervisor internally, is it still advisable to use it?\r\nOr are there any chances to implement the slim.learning.train to use tf.train.MonitoredSession instead of Supervisor?\r\n\r\n@sguada \r\n\r\nThanks!\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated the fields to N/A.\r\nThanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "/CC @sguada. Note the Supervisor is created [here](https://github.com/tensorflow/tensorflow/blob/09ec6b2c59d05230b275a14bbc54a8890b6b1c62/tensorflow/contrib/slim/python/slim/learning.py#L721). Should we mark as contributions welcome?", "Sure", "`MonitoredTrainingSession` is having TF 1 implementation and behavior and in TF 2 it's used by `tf.compat.v1.train.MonitoredTrainingSession`, more details can be found [here](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MonitoredTrainingSession). \r\nTensorflow 2 has lot of changes and there is a possibility that your required feature is being implemented, you can upgrade the Tensorflow version and start using. \r\nFeel free to file a new issue if you need any clarification. Thanks! "]}, {"number": 15792, "title": "Portability of TensorFlow Meta graph file", "body": "OS Platform and Distribution : CentOS\r\nTensorFlow installed from : Sources\r\nTensorFlow version : 1.4\r\nBazel version : N/A\r\nCUDA/cuDNN version 8.0 (CUDA) and 6.0 (CuDNN)\r\nGPU model and memory : N/A\r\nExact command to reproduce : N/A\r\n\r\nThe `.meta` file from TensorFlow contains device information. Although I can use `clear_devices=False` to prevent device information getting logged, I beg to ask the relevance of the `.meta` file with respect to portability.\r\n\r\n 1. If I have the code for the generation of the TensorFlow graph, then I do not need the `.meta` file as per [this answer][1]. \r\n\r\n 2. What is the applicability of transferring only the `.meta` file to someone ? \r\n\r\n 3. Assuming that I train the graph with 4 GPUs, and then provide `.meta` file to someone with 8 or possibly only 1 GPU. For someone with 8 GPUs, would this not prevent him/her from actually running the graph for training/inference over 8 GPUs ? In the case of someone with only 1 GPU, what would happen to entities with device numbers 1-3 ?\r\n\r\n 4. And finally, what are the implications of point 3, when `clear_devices=True` ? \r\n\r\n  [1]: https://stackoverflow.com/a/36203288/8530591\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Anyone following this ?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "`clear_devices` just removes the `with tf.device()` assignments. \r\n\r\n@sukritiramesh Any comment on this?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A clear example would do wonders for especially new TensorFlow users. There is very little such practical information available on this on other forums as well.\r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "These questions may be better asked on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), because answers will help other users more than here. I'm leaving this issue open as a documentation request. Thanks!", "Not criticising; but just bringing to notice [the corresponding StackOverFlow question](https://stackoverflow.com/questions/48067015/portability-of-tensorflow-meta-graph-file). Also notice, the activity on that question. ", "We have a new saved-model format for TensorFlow2. I think this bug is obsolete."]}, {"number": 15791, "title": "weighted_cross_entropy_with_logits produces wrong result", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Geforce GTX 1080 Ti / 12G\r\n\r\n### Describe the problem\r\nI am trying to use tf.nn.weighted_cross_entropy_with_logits API, but I found I just can not get the right result when the weight is not 1.0 (1.0 means no weight).\r\n\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef my_binary_crossentropy_np(labels, output, weight=10.0):\r\n  \"\"\"\r\n  Weighted binary crossentropy between an output tensor \r\n  and a target tensor.\r\n  \"\"\"\r\n  # transform back to logits\r\n  epsilon = 1e-08\r\n  np.clip(output, epsilon, 1.0 - epsilon, out=output)\r\n  output = np.log(output / (1.0 - output))\r\n\r\n  # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits \r\n  # l = 1 + (q - 1) * z\r\n  # (1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))\r\n  l = 1.0 + (weight - 1.0) * labels\r\n  loss1 = np.multiply(1.0 - labels, output)\r\n  loss2 = np.multiply(l, np.log(1.0 + np.exp(-abs(output))))\r\n  loss3 = np.maximum(-output, 0)\r\n  loss = loss1 + loss2 + loss3\r\n  \r\n  return np.mean(loss)\r\n\r\n\r\ndef my_binary_crossentropy_tf(labels, output, weight=1.0):\r\n  \"\"\"\r\n  Weighted binary crossentropy between an output tensor \r\n  and a target tensor.\r\n  \"\"\"\r\n  epsilon = 1e-08\r\n  output = tf.clip_by_value(output, epsilon, 1.0 - epsilon)\r\n  output = tf.log(output / (1.0 - output))\r\n\r\n  # compute weighted loss\r\n  #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=output)\r\n  loss = tf.nn.weighted_cross_entropy_with_logits(targets=labels, logits=output, pos_weight=weight)\r\n  return tf.reduce_mean(loss)\r\n\r\n\r\n# generate random test data and random label\r\npredict = np.random.rand(10, 8)\r\n\r\nlabel = np.random.rand(10, 8)\r\nlabel[label >= 0.5] = 1\r\nlabel[label < 0.5] = 0\r\n\r\n\r\nloss1 = my_binary_crossentropy_np(label, predict, 1.0)\r\nprint('loss1 = ', loss1)\r\n\r\nloss1 = my_binary_crossentropy_np(label, predict, 10.0)\r\nprint('loss1 = ', loss1)\r\n\r\n\r\npredict_tf = tf.convert_to_tensor(predict)\r\nloss2 = my_binary_crossentropy_tf(label, predict_tf, 1.0)\r\nloss2 = tf.Session().run(loss2)\r\nprint('loss2 = ', loss2)\r\n\r\nloss2 = my_binary_crossentropy_tf(label, predict_tf, 10.0)\r\nloss2 = tf.Session().run(loss2)\r\nprint('loss2 = ', loss2)\r\n```\r\n\r\nrunning result: \r\nloss1 =  1.02193164517\r\nloss1 =  1.96332399324\r\n\r\nloss2 =  1.02193164517\r\nloss2 =  4.80529539791\r\n", "comments": ["it seems that my implementation is wrong instead of tensorflow."]}, {"number": 15790, "title": "order quantized table by value for ease of reading", "body": "", "comments": ["Can one of the admins verify this patch?", "Closing duplicate PR in favor of https://github.com/tensorflow/tensorflow/pull/15786"]}, {"number": 15789, "title": "order quantized table by value for ease of reading", "body": "", "comments": ["Can one of the admins verify this patch?", "Closing duplicate PR in favor of https://github.com/tensorflow/tensorflow/pull/15786."]}, {"number": 15788, "title": "MultiRNNCell and reuse_variables()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Geforce GTX TITAN X/12G\r\n\r\n### Describe the problem\r\n\r\n`tf.nn.dynamic_rnn()` and `tf.nn.rnn_cell.MultiRNNCell()` breaks down when add **`tf.get_variable_scope().reuse_variables()`** before defining a rnn architecture.\r\n\r\n#### Correct Situation\r\n\r\n- When there is no `tf.get_variable_scope().reuse_variables()`\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random_normal([6, 5, 100])\r\n\r\ndef build_lstm(num_units, num_layers, batch_size):\r\n    def build_cell(num_units):\r\n        return tf.contrib.rnn.LSTMCell(num_units)\r\n    \r\n    with tf.name_scope('multi_cells'):\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\r\n    init_state = cell.zero_state(batch_size, tf.float32)\r\n    \r\n    return cell, init_state\r\n\r\nlstm_cell, lstm_init_state = build_lstm(200, 2, 5)\r\nlstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\r\n```\r\nIt works fine!\r\n\r\n#### Wrong Situation\r\n\r\n- When there is `tf.get_variable_scope().reuse_variables()`\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random_normal([6, 5, 100])\r\n\r\n# add a magic sentence here\r\ntf.get_variable_scope().reuse_variables()\r\n\r\ndef build_lstm(num_units, num_layers, batch_size):\r\n    def build_cell(num_units):\r\n        return tf.contrib.rnn.LSTMCell(num_units)\r\n    \r\n    with tf.name_scope('multi_cells'):\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\r\n    init_state = cell.zero_state(batch_size, tf.float32)\r\n    \r\n    return cell, init_state\r\n\r\nlstm_cell, lstm_init_state = build_lstm(200, 2, 5)\r\nlstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\r\n```\r\nit returns:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-d333e19dbfd0> in <module>()\r\n----> 1 lstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    612         swap_memory=swap_memory,\r\n    613         sequence_length=sequence_length,\r\n--> 614         dtype=dtype)\r\n    615 \r\n    616     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    775       loop_vars=(time, output_ta, state),\r\n    776       parallel_iterations=parallel_iterations,\r\n--> 777       swap_memory=swap_memory)\r\n    778 \r\n    779   # Unpack final output if not using output tuples.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\r\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2817     return result\r\n   2818 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2638       self.Enter()\r\n   2639       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2641     finally:\r\n   2642       self.Exit()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2588         structure=original_loop_vars,\r\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2590     body_result = body(*packed_vars_for_body)\r\n   2591     if not nest.is_sequence(body_result):\r\n   2592       body_result = [body_result]\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\r\n    760           skip_conditionals=True)\r\n    761     else:\r\n--> 762       (output, new_state) = call_cell()\r\n    763 \r\n    764     # Pack state if using state tuples\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in <lambda>()\r\n    746 \r\n    747     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 748     call_cell = lambda: cell(input_t, state)\r\n    749 \r\n    750     if sequence_length is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n    181       with vs.variable_scope(vs.get_variable_scope(),\r\n    182                              custom_getter=self._rnn_get_variable):\r\n--> 183         return super(RNNCell, self).__call__(inputs, state)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\r\n   1064                                       [-1, cell.state_size])\r\n   1065           cur_state_pos += cell.state_size\r\n-> 1066         cur_inp, new_state = cell(cur_inp, cur_state)\r\n   1067         new_states.append(new_state)\r\n   1068 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n    181       with vs.variable_scope(vs.get_variable_scope(),\r\n    182                              custom_getter=self._rnn_get_variable):\r\n--> 183         return super(RNNCell, self).__call__(inputs, state)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\r\n    606               partitioned_variables.fixed_size_partitioner(\r\n    607                   self._num_unit_shards))\r\n--> 608         self._linear1 = _Linear([inputs, m_prev], 4 * self._num_units, True)\r\n    609 \r\n    610     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __init__(self, args, output_size, build_bias, bias_initializer, kernel_initializer)\r\n   1169           _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\r\n   1170           dtype=dtype,\r\n-> 1171           initializer=kernel_initializer)\r\n   1172       if build_bias:\r\n   1173         with vs.variable_scope(outer_scope) as inner_scope:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1201       partitioner=partitioner, validate_shape=validate_shape,\r\n   1202       use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1203       constraint=constraint)\r\n   1204 get_variable_or_local_docstring = (\r\n   1205     \"\"\"%s\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1090           partitioner=partitioner, validate_shape=validate_shape,\r\n   1091           use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1092           constraint=constraint)\r\n   1093 \r\n   1094   def _get_partitioned_variable(self,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n    415       if \"constraint\" in estimator_util.fn_args(custom_getter):\r\n    416         custom_getter_kwargs[\"constraint\"] = constraint\r\n--> 417       return custom_getter(**custom_getter_kwargs)\r\n    418     else:\r\n    419       return _true_getter(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in wrapped_custom_getter(getter, *args, **kwargs)\r\n   1581     return custom_getter(\r\n   1582         functools.partial(old_getter, getter),\r\n-> 1583         *args, **kwargs)\r\n   1584   return wrapped_custom_getter\r\n   1585 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 186     variable = getter(*args, **kwargs)\r\n    187     if context.in_graph_mode():\r\n    188       trainable = (variable in tf_variables.trainable_variables() or\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 186     variable = getter(*args, **kwargs)\r\n    187     if context.in_graph_mode():\r\n    188       trainable = (variable in tf_variables.trainable_variables() or\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\r\n    392           trainable=trainable, collections=collections,\r\n    393           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 394           use_resource=use_resource, constraint=constraint)\r\n    395 \r\n    396     if custom_getter is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\r\n    758       raise ValueError(\"Variable %s does not exist, or was not created with \"\r\n    759                        \"tf.get_variable(). Did you mean to set \"\r\n--> 760                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\r\n    761     if not shape.is_fully_defined() and not initializing_from_value:\r\n    762       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\r\n\r\nValueError: Variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n```\r\n\r\n### Conclusion\r\nIt seems that `tf.get_variable_scope().reuse_variables()` and `tf.nn.rnn_cell.MultiRNNCell()` are not compatiable", "comments": ["This is expected behaviour: you can reuse an exising variable. If it doesn't exist, it has to be created first.\r\nYou probably want to use `tf.AUTO_REUSE`, which will reuse a variable *if possible*:\r\n```\r\ncell = tf.nn.rnn_cell.LSTMCell(3, reuse=tf.AUTO_REUSE)\r\n```", "It works, thanks a lot!\r\nDose it mean that if one have reused a variable scope, no new variables would be created by LSTMCell or else?\r\n", "hello,I have  Error like this ,can you help me ? thanks a lot\r\n\"reuse=tf.AUTO_REUSE in VarScope?\" % name)\r\nValueError: Variable d_bn1/d_bn1_2/moments/Squeeze/ExponentialMovingAverage/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n", "Traceback (most recent call last):\r\n  File \"generate_images.py\", line 106, in <module>\r\n    main()\r\n  File \"generate_images.py\", line 64, in main\r\n    _, _, _, _, _ = gan.build_model()\r\n  File \"/home/andy/text2image/text-to-image-master/model.py\", line 40, in build_model\r\n    disc_wrong_image, disc_wrong_image_logits   = self.discriminator(t_wrong_image, t_real_caption, reuse = True)\r\n  File \"/home/andy/text2image/text-to-image-master/model.py\", line 162, in discriminator\r\n    h1 = ops.lrelu( self.d_bn1(ops.conv2d(h0, self.options['df_dim']*2, name = 'd_h1_conv'))) #16\r\n  File \"/home/andy/text2image/text-to-image-master/Utils/ops.py\", line 34, in __call__\r\n    ema_apply_op = self.ema.apply([batch_mean, batch_var])\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 401, in apply\r\n    colocate_with_primary=(var.op.type in [\"Variable\", \"VariableV2\"]))\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 174, in create_zeros_slot\r\n    colocate_with_primary=colocate_with_primary)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 151, in create_slot_with_initializer\r\n    dtype)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 67, in _create_slot_var\r\n    validate_shape=validate_shape)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\r\n    constraint=constraint)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/home/andy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 760, in _get_single_variable\r\n    \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\r\nValueError: Variable d_bn1/d_bn1_2/moments/Squeeze/ExponentialMovingAverage/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n", "use  tf.reset_default_graph() "]}, {"number": 15787, "title": "order quantized table by value for ease of reading", "body": "", "comments": ["Can one of the admins verify this patch?", "Closing duplicate PR in favor of https://github.com/tensorflow/tensorflow/pull/15786."]}, {"number": 15786, "title": "order quantized table by value for ease of reading", "body": "", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Atlas7 we are not taking PRs against release branches -- can you reopen this PR against master?"]}, {"number": 15785, "title": "Unable to find source java class: '/Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/java/src/main/java/org/tensorflow/op/core/Constant.java'", "body": "I follow the instructions on the tensorflow official website on how to import android samples. I did exactly as they said but when I try to run the app it shows the following error.\r\n\r\nError:Execution failed for task ':compileDebugJavaWithJavac'.\r\n> Unable to find source java class: '/Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/java/src/main/java/org/tensorflow/op/core/Constant.java' because it does not belong to any of the source dirs: '[/Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/src/main/java, /Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/src, /Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/build-types/debug/java, /Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/gradleBuild/generated/source/r/debug, /Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/gradleBuild/generated/source/buildConfig/debug, /Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/gradleBuild/generated/source/aidl/debug, /Users/vinay.garg/AndroidStudioProjects/tensorflow/tensorflow/examples/android/gradleBuild/generated/source/rs/debug]'\r\n\r\n\r\nThe problem is, Constants.java file is in the parent directory and not in the sample project directory. I tried to find the usage of the Constants.java file but can't find its use anywhere in the sample project. What am I missing here?", "comments": ["I had the same issue some minutes ago. I solved it by running \"Build -> Rebuild Project\" from Android Studio menu. ", "Thanks man. It worked.\r\nI feel so dumb that I did not try this before posting it as an issue.", "I am working on windows 10 and I want to make it work with my own model, how can I achieve it?", "> I am working on windows 10 and I want to make it work with my own model, how can I achieve it?\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#install-model-files-optional\r\nOptional: If you wish to place the models in your assets manually, remove all of the model_files entries from the assets list in tensorflow_demo found in the BUILD file. Then download and extract the archives yourself to the assets directory in the source tree:\r\n\r\nBASE_URL=https://storage.googleapis.com/download.tensorflow.org/models\r\nfor MODEL_ZIP in inception5h.zip ssd_mobilenet_v1_android_export.zip stylize_v1.zip\r\ndo\r\n  curl -L ${BASE_URL}/${MODEL_ZIP} -o /tmp/${MODEL_ZIP}\r\n  unzip /tmp/${MODEL_ZIP} -d tensorflow/examples/android/assets/\r\ndone\r\nThis will extract the models and their associated metadata files to the local assets/ directory.\r\n\r\nIf you are using Gradle, make sure to remove download-models.gradle reference from build.gradle after your manually download models; otherwise gradle might download models again and overwrite your models.\r\n\r\nBuild"]}, {"number": 15784, "title": "[FeatureRequest] Support PathLike objects for directory arguments", "body": "With python 3.6, [PEP 519](https://www.python.org/dev/peps/pep-0519/) and the [pathlib](https://docs.python.org/3/library/pathlib.html) module, it would be great if TensorFlow directory parameters accepted PathLike objects.\r\n\r\nFrom the [Backwards Compatibility](https://www.python.org/dev/peps/pep-0519/#backwards-compatibility) part of the documentation, a suggested implementation is:\r\n```python\r\npath.__fspath__() if hasattr(path, \"__fspath__\") else path\r\n```\r\n\r\nWith such an implementation, it seems `path` in the code above can be any file system representation of `str`, `bytes` or `pathlib.Path`. For my case, I was using/looking at the [`tf.estimator.model_dir`](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/estimator/estimator.py#L122-L126) parameter, but for consistency I assume it would also need applying in all cases where a path is accepted such as `tf.gfile`\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v1.4.0-8-gbca50da6eb 1.4.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nProblem is the `pathlib` module represents filesystem paths although is not accepted in parameters that refer to a directory or file in the TensorFlow API. The proposed feature would enable accepting these objects while still maintaining compatibility with existing `str` type paths.\r\n", "comments": []}, {"number": 15783, "title": "CUDA crashes during Cholesky_grad procedure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (see command below)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: 'unknown' 1.4.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: Cuda: 8.0.61.2; cuDNN: 6.1\r\n- **GPU model and memory**: GeForce GTX 770M (3GB); NVIDIA driver 388.71\r\n- **Bazel version**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith tf.Session() as sess:\r\n    x = tf.placeholder(tf.float64, [None, None])\r\n    f = tf.reduce_sum(tf.cholesky(x))\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(tf.gradients(f, x), {x:np.array(1.).reshape(1, 1)}))\r\n```\r\n\r\n### Describe the problem\r\nPython crashes when running the code above. It was initially encountered when trying to perform a Cholesky decomposition during sparse Gaussian process regression (see https://github.com/GPflow/GPflow/issues/602). I am out of my depth here, but the following error messages seemed interesting:\r\n* Address 0x00000000 is out of bounds\r\n* Blas GEMV launch failed\r\n* failed to run cuBLAS routine cublasDgemv_v2: CUBLAS_STATUS_EXECUTION_FAILED\r\n\r\nI have run various CUDA profiling tests and have had no issues. The cholesky decomposition example that comes with CUDA could also execute just fine. I have tried both reducing the amount of GPU memory available to tensorflow _and_ setting it grow-able, to no effect.\r\n\r\n### Source code / logs\r\nSee above for source code.\r\n\r\nstdout/stderr:\r\n```\r\n2018-01-02 18:14:34.305556: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018-01-02 18:14:35.110794: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 770M major: 3 minor: 0 memoryClockRate(GHz): 0.797\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.00GiB freeMemory: 2.50GiB\r\n2018-01-02 18:14:35.110932: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 770M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\n2018-01-02 18:14:35.290410: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 00000189FC81C670\r\n2018-01-02 18:14:35.615695: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-01-02 18:14:35.615847: F C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n2018-01-02 18:14:35.615695: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n```\r\n\r\ncuda-memcheck:\r\n```\r\n========= CUDA-MEMCHECK\r\n2018-01-02 18:15:26.061068: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018-01-02 18:15:26.872851: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 770M major: 3 minor: 0 memoryClockRate(GHz): 0.797\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.00GiB freeMemory: 2.52GiB\r\n2018-01-02 18:15:26.873028: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 770M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\n2018-01-02 18:15:27.425673: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 00000162C1EBD150\r\n2018-01-02 18:15:30.721433: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:551] failed to run cuBLAS routine cublasDgemv_v2: CUBLAS_STATUS_EXECUTION_FAILED\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMV launch failed:  m=1, n=1\r\n         [[Node: gradients/Cholesky_grad/MatMul_1 = MatMul[T=DT_DOUBLE, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/Cholesky_grad/MatrixTriangularSolve, gradients/Cholesky_grad/MatrixBandPart)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 11, in <module>\r\n    print(sess.run(tf.gradients(f, x), {x:np.array(1.).reshape(1, 1)}))\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMV launch failed:  m=1, n=1\r\n         [[Node: gradients/Cholesky_grad/MatMul_1 = MatMul[T=DT_DOUBLE, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/Cholesky_grad/MatrixTriangularSolve, gradients/Cholesky_grad/MatrixBandPart)]]\r\n\r\nCaused by op 'gradients/Cholesky_grad/MatMul_1', defined at:\r\n  File \"test2.py\", line 11, in <module>\r\n    print(sess.run(tf.gradients(f, x), {x:np.array(1.).reshape(1, 1)}))\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\linalg_grad.py\", line 77, in _CholeskyGrad\r\n    math_ops.matmul(l_inverse, middle, adjoint_a=True), l_inverse)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1891, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2436, in _mat_mul\r\n    name=name)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'Cholesky', defined at:\r\n  File \"test2.py\", line 9, in <module>\r\n    f = tf.reduce_sum(tf.cholesky(x))\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\", line 419, in cholesky\r\n    \"Cholesky\", input=input, name=name)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): Blas GEMV launch failed:  m=1, n=1\r\n         [[Node: gradients/Cholesky_grad/MatMul_1 = MatMul[T=DT_DOUBLE, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/Cholesky_grad/MatrixTriangularSolve, gradients/Cholesky_grad/MatrixBandPart)]]\r\n\r\n========= Invalid __global__ read of size 1\r\n=========     at 0x00000398 in void Eigen::internal::FullReductionKernel<int=256, int=128, Eigen::TensorEvaluator<Eigen::TensorReductionOp<Eigen::internal::SumReducer<double>, Eigen::DimensionList<__int64, unsigned __int64=2> const , Eigen::TensorGeneratorOp<tensorflow::generator::OverwriteDiagGenerator<double>, Eigen::TensorMap<Eigen::Tensor<double const , int=2, int=1, __int64>, int=16, Eigen::MakePointer> const > const , Eigen::MakePointer> const , Eigen::GpuDevice>, Eigen::internal::SumReducer<double>, __int64>(Eigen::internal::SumReducer<double>, double, __int64, Eigen::internal::SumReducer<double>::CoeffReturnType*, unsigned int*)\r\n=========     by thread (0,0,0) in block (0,0,0)\r\n=========     Address 0x00000000 is out of bounds\r\n=========     Saved host backtrace up to driver entry point at kernel launch time\r\n=========     Host Frame:C:\\WINDOWS\\SYSTEM32\\nvcuda.dll (cuTexRefSetAddress + 0x1aaaf3) [0x1b8145]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd [0x35d2]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd [0x2365]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (perftools::gputools::cuda::CUDATimer::Stop + 0x1cf760) [0x2202ba0]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (perftools::gputools::cuda::CUDATimer::Stop + 0x1d18f2) [0x2204d32]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (perftools::gputools::cuda::CUDATimer::Stop + 0x1ce985) [0x2201dc5]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (tensorflow::MatrixSetDiagOp<Eigen::GpuDevice,double>::Compute + 0x407) [0x13180d7]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (tensorflow::BaseGPUDevice::ComputeHelper + 0x4f4) [0x3aab84]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (tensorflow::BaseGPUDevice::Compute + 0x18a) [0x3aa3aa]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (tensorflow::NewLocalExecutor + 0xfdb) [0x2aefdb]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (?_Copy@?$_Func_impl@V?$_Binder@U_Unforced@std@@P8ExecutorState@?A0x4546b700@tensorflow@@EAAXUTaggedNode@345@_J@ZQEAV345@AEBU6345@AEA_J@std@@V?$allocator@H@2@X$$V@std@@EEBAPEAV?$_Func_base@X$$V@2@PEAX@Z + 0x78) [0x2b2548]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop + 0x3d9) [0x247839]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop + 0x560) [0x2479c0]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (tensorflow::WindowsFileSystem::Utf8ToWideChar + 0x175) [0x276795]\r\n=========     Host Frame:C:\\Users\\tomah\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd (tensorflow::WindowsFileSystem::Utf8ToWideChar + 0xc9) [0x2766e9]\r\n=========     Host Frame:C:\\WINDOWS\\System32\\ucrtbase.dll (iswascii + 0xa5) [0x1d885]\r\n=========     Host Frame:C:\\WINDOWS\\System32\\KERNEL32.DLL (BaseThreadInitThunk + 0x14) [0x11fe4]\r\n=========     Host Frame:C:\\WINDOWS\\SYSTEM32\\ntdll.dll (RtlUserThreadStart + 0x21) [0x6ef91]\r\n=========\r\n========= ERROR SUMMARY: 1 error\r\n```\r\n\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Updated (Bazel version = N/A).", "I'm experiencing the exact same issue under almost identical circumstances, and can reproduce the error with the test code provided by the original poster.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow installed from (source or binary): Binary (pip)\r\nTensorFlow version (use command below): 1.4.0\r\nPython version: 3.6.3\r\nCUDA/cuDNN version: Cuda: 8.0.61.2; cuDNN: 6.1\r\nGPU model and memory: GeForce GTX 1080ti (nvidia driver 390.65)\r\n\r\n\r\n```\r\n2018-01-09 22:54:40.246353: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018-01-09 22:54:40.800294: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.759\r\npciBusID: 0000:29:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.08GiB\r\n2018-01-09 22:54:40.800669: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:29:00.0, compute capability: 6.1)\r\n2018-01-09 22:54:41.520134: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 00000282A7DBB300\r\n2018-01-09 22:54:42.251706: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n2018-01-09 22:54:42.251713: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-01-09 22:54:42.252434: F C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n\r\n```\r\n  ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I cannot reproduce on Python 3.4 on Ubuntu 14.04, with a P100 on driver 387.34, with everything else being the same. \r\n\r\n/CC @rmlarsen, any ideas what the issue could be?\r\n\r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Pinging @rmlarsen. @rmlarsen, maybe mark as contributions welcome if you do not know what the issue is.\r\n\r\nAlso /CC @mrry @gunan, this issue might only occur on Windows.\r\n", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As this is a problem with cuda 6.1, I will mark this as contributions welcome.", "You seem to be using older version(1.x) of Tensorflow which is not supported anymore. Please try the latest [Tensorflow version](https://www.tensorflow.org/install/pip) and let us know if the problem still persists.\r\n`tf.cholesky` has been moved under `tf.linalg.cholesky`, refer [this](https://www.tensorflow.org/api_docs/python/tf/linalg/cholesky) documentation for more details. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 15782, "title": "[BUG REPORT] how to set TF_CONFIG,  bug ? ", "body": "------------------------\r\n\r\n### System information\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.4.0-19-ga52c8d9', '1.4.1')\r\n\r\n\r\n### Describe the problem\r\n\r\n[NORMAL CODE]\r\n```\r\n  chief_host = ['localhost:20000']\r\n  worker_hosts = ['localhost:20002']\r\n  ps_hosts = ['localhost:20001']\r\n\r\n  cluster = {'chief': chief_host,\r\n             'worker': worker_hosts,\r\n             'ps': ps_hosts}\r\n  task_type = 'chief'\r\n  task_index = 0\r\n  os.environ['TF_CONFIG'] = json.dumps(\r\n      {'cluster': cluster,\r\n       'task': {'type': task_type, 'index': task_index}})\r\n```\r\n\r\n[ NORMAL OUTPUT]\r\n```\r\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'localhost:20001'], u'chief': [u'localhost:20000'], u'worker': [u'localhost:20002']}, u'task': {u'index': 0, u'type': u'chief'}}\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': device_count {\r\n  key: \"GPU\"\r\n}\r\n, '_keep_checkpoint_max': 5, '_task_type': u'chief', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x5e30bd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://localhost:20000', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/census_model', '_save_summary_steps': 100}\r\nINFO:tensorflow:Start Tensorflow server.\r\n2018-01-02 16:31:58.374169: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nE0102 16:31:58.378815122   44355 ev_epoll1_linux.c:1051]     grpc epoll fd: 3\r\n2018-01-02 16:31:58.384682: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -> {0 -> localhost:20000}\r\n2018-01-02 16:31:58.384723: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20001}\r\n2018-01-02 16:31:58.384736: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:20002}\r\n2018-01-02 16:31:58.391874: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:20000\r\nParsing data/adult.data\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n2018-01-02 16:32:10.407610: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-01-02 16:32:10.407652: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n2018-01-02 16:32:20.407746: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-01-02 16:32:20.407766: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n2018-01-02 16:32:30.407834: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-01-02 16:32:30.407850: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n```\r\n\r\n============\r\n\r\n[BUG CODE]\r\n```\r\n  print(type(chief_host), type(worker_hosts), type(ps_hosts))\r\n  print('chief=',chief_host, 'worker=',worker_hosts, 'ps=',ps_hosts)\r\n\r\n  chief_host = chief_host\r\n  worker_hosts = worker_hosts\r\n  ps_hosts = ps_hosts\r\n\r\n  cluster = {'chief': chief_host,\r\n             'worker': worker_hosts,\r\n             'ps': ps_hosts}\r\n  task_type = 'chief' \r\n  task_index = 0 \r\n  os.environ['TF_CONFIG'] = json.dumps(\r\n      {'cluster': cluster,\r\n       'task': {'type': task_type, 'index': task_index}})\r\n```\r\n\r\n[BUG OUTPUT]\r\n```\r\n<type 'list'> <type 'list'> <type 'list'>\r\nchief= ['localhosts:20001'] worker= ['localhost:20003'] ps= ['localhost:20000']\r\n\r\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'localhost:20000'], u'chief': [u'localhosts:20001'], u'worker': [u'localhost:20003']}, u'task': {u'index': 0, u'type': u'chief'}}\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': device_count {\r\n  key: \"GPU\"\r\n}\r\n, '_keep_checkpoint_max': 5, '_task_type': u'chief', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x6264e10>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://localhosts:20001', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/census_model', '_save_summary_steps': 100}\r\nINFO:tensorflow:Start Tensorflow server.\r\n2018-01-02 16:34:09.909573: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nE0102 16:34:09.914156177   44635 ev_epoll1_linux.c:1051]     grpc epoll fd: 3\r\n2018-01-02 16:34:09.920491: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -> {0 -> localhost:20001}\r\n2018-01-02 16:34:09.920540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:20000}\r\n2018-01-02 16:34:09.920558: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:20003}\r\n2018-01-02 16:34:09.927272: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:20001\r\nParsing data/adult.data\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n```\r\n\r\nNote: [ BUG OUTPUT ] don't print \" CreateSession still waiting for response from worker \" ,  i have wait long enough !!!\r\n\r\n===========\r\nmy train and eval code\r\n```\r\n  train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(FLAGS.train_data,\r\n      None, True, FLAGS.batch_size), max_steps=30000)\r\n  eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(FLAGS.test_data, 1, False,\r\n      FLAGS.batch_size), steps=5)\r\n  tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\r\n```\r\ni can run my code on non-distribution and distribution as follows style:\r\n```\r\nTF_CONFIG='{\r\n    \"cluster\": {\r\n        \"chief\": [\"localhost:20000\"],\r\n        \"worker\": [\"localhost:20002\"],\r\n        \"ps\": [\"localhost:20001\"]\r\n    },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n}'\r\nTF_CONFIG=$TF_CONFIG python wide_deep_d.py \r\n\r\n......\r\n```\r\nbut can't set TF_CONFIG by os.environ with json.dumps correctly\r\n\r\nNeed your help, Thanks , ", "comments": []}, {"number": 15781, "title": "build&link tensorflow lite c++ library", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: g++ 5.4.0\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: g++ -std=c++11 -I...tensorflow -L. -lframework demo.cpp\r\n\r\n### Describe the problem\r\nI run 'bazel build //tensorflow/contrib/lite:framework' and get libframework.so. Then I use libframework.so in my own code, but get undefined reference error when compile with g++:\r\n/temp/ccYTZw2h.o: In function 'main':\r\ndemo.cpp:(.text+0x46): undefined reference to 'tflite::DefaultErrorReporter()'\r\ndemo.cpp:(.text+0x6a): undefined reference ro 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n......\r\n\r\nI get following lines by 'nm libframework.so | grep 'DefaultErrorReporter'':\r\n000000000001b1b0 b _ZGVZN6tflite20DefaultErrorReporterEvE14error_reporter\r\n0000000000007990 T _ZN6tflite20DefaultErrorReporterEv\r\n000000000001b1a8 b _ZZN6tflite20DefaultErrorReporterEvE14error_reporter\r\n\r\nI'm not familiar with how to use tensorflow lite. Where is the problem could be?\r\n\r\n### Source code / logs\r\n", "comments": ["@acappemin Can I know if you were able to use tensorflow lite for c++. Were you able to build libtensorflowlite_cc.so similar to libtensorflow_cc.so\r\n\r\n\r\n", "@acappemin Have you solved the problem? ", "Why is this not resolved yet?", "@ashokbugude  \r\n\r\n> Can I know if you were able to use tensorflow lite for c++. Were you able to build libtensorflowlite_cc.so similar to libtensorflow_cc.so\r\n\r\nYes.  Here is my code:\r\n\r\n #include < cstdio >\r\n  #include \"tensorflow/lite/interpreter.h\"\r\n  #include \"tensorflow/lite/kernels/register.h\"\r\n  #include \"tensorflow/lite/model.h\"\r\n  #include \"tensorflow/lite/tools/gen_op_registration.h\"\r\n\r\n\r\n  using namespace tflite;\r\n\r\n  int main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n  fprintf(stderr, \"minimal \\n\");\r\n  return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n  tflite::FlatBufferModel::BuildFromFile(filename);\r\n  return 0;\r\n  }\r\n\r\nAnd This is the error:\r\n/tmp/cc0JEa86.o: In function `main':\r\ntest.cpp:(.text+0x57): undefined reference to `tflite::DefaultErrorReporter()'\r\ntest.cpp:(.text+0x6d): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n/tmp/cc0JEa86.o: In function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':\r\ntest.cpp:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x1e): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\ncollect2: error: ld returned 1 exit status\r\n"]}, {"number": 15780, "title": "[configure] eagerly determine the truthfulness of environment variables", "body": "Eagerly determine the truthfulness of environment variables in `get_var` function.\r\n\r\nThis way we can skip checking, e.g. Android workspace setup if the user sets `TF_SET_ANDROID_WORKSPACE=0`\r\n\r\nTested manually. \r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 15779, "title": "I have simple question on tensorflow , Does tensorflow support for IONIC (Hybrid-Applications), I mean the models that we genarate through tensorflow that will support for Ionic application.", "body": "  I have simple question on tensorflow , Does tensorflow support for IONIC (Hybrid-Applications), I mean the models that we genarate through tensorflow that will support for Ionic application", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15778, "title": "Add `colors` support for `tf.image.draw_bounding_boxes`", "body": "This fix tries to address the issue raised in #15692 where it was not possible to specify the colors for boxes in `tf.image.draw_bounding_boxes`. Instead, a predefined fixed color table was used to cycle through colors.\r\n\r\nThis fix adds `colors` Input to `DrawBoundingBoxexV2` so that it is possible to specify the color. In case no color is specified, the default color table will be used.\r\n\r\nSince there is an API change, the op is labeled as V2.\r\n\r\nThis fix fixes #15692.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["The PR has been rebased to address merge conflict.", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 273 days with no activity and the `awaiting review` label has been applied.", "Looks like the api namespace systems have changed over time and there are some issues with the PR. Will close for now, and reopen once fixed."]}, {"number": 15777, "title": "libstdc++.so.6: version `CXXABI_1.3.8' not found", "body": "All of my `tf-nightly` Travis CI pipelines started failing today with following error\r\n\r\n``` ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /home/travis/virtualenv/python3.5.4/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)```\r\n\r\nExample:\r\nhttps://travis-ci.org/yaroslavvb/chain_constant_memory/builds/323851093\r\n\r\nAny ideas how to fix?\r\n", "comments": ["PS, happy new year!", "The only thing I can think of is that Tensorflow makes the following note at [Tensorflow Install from Source](https://www.tensorflow.org/install/install_sources)\r\n\r\n> **NOTE on gcc 5 or later:** the binary pip packages available on the TensorFlow website are built with gcc 4, which uses the older ABI. To make your build compatible with the older ABI, you need to add --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" to your bazel build command. ABI compatibility allows custom ops built against the TensorFlow pip package to continue to work against your built package.", "Happy New Year to you as well and pleased to meet you! I have been trying to build TF r1.4 with Cuda Toolkit 8.0 and CUDNN 5.1 from source using bazel on Windows. Have you ever had any such luck?", "Give that this broke this morning, I suspect this breakage is due to build infrastructure change that was checked in yesterday, perhaps this one -- https://github.com/tensorflow/tensorflow/commit/403642268695246aec08cf0577cd978b5f77ccb6\r\n\r\ncc @gunan ", "I have a similar issue with Travis CI starting tonight:\r\n\r\n```\r\nImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.20' not found\r\n(required by /home/travis/virtualenv/python2.7.14/lib/python2.7/site-\r\npackages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n```", "Similarly, in Debian Jessie:\r\n```\r\nImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found\r\n(required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n```", "Debian jessie is only gcc 4.9 (https://packages.debian.org/jessie/devel/gcc), so still not new enough. The build infra was switched to 16.04 in the new year which uses gcc 5.3 by default (https://packages.ubuntu.com/xenial/gcc). According to the symbol versioning (https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html) you'll need at least gcc 5.1 runtime to run it.", "Happy new year!\r\nThe issue is potentially a side effect of us moving our build machines up to ubuntu 16.\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.5/RELEASE.md\r\n\r\nWe made this decision because cuda 9 docker images using ubuntu 14 were not available, and we elected to keep all builds on the same base system image.\r\n\r\n@jhseu @martinwicke FYI", "@yaroslavvb, Travis CI don't have plans to support Ubuntu 16 (https://github.com/travis-ci/travis-ci/issues/5821), so it seems that our only recourse is to dockerize the tests (https://docs.travis-ci.com/user/docker/, e.g. https://github.com/uber/horovod/pull/136).", "The last comment on that Travis issue is over a year old... I guess that means they want you to move to docker for real. You're of course welcome to use our CI docker images. \r\n\r\nAs @gunan noted, we cannot stay with old Ubuntu since CUDA won't let us. ", "Ubuntu 14 LTE doesn't become EOL until April 2019. Are we planning to move it out of our support matrix? (See go/tf-issues-triage) What about CentOS 7, which has gcc 4.8.5? What are the benefits of this new ABI?\r\n\r\nIf changing our support matrix is the consensus here, I'd suggest we go a step further and compile prebuilt binaries with SSE4 (maybe AVX2, possibly even AVX512) since that makes TensorFlow CPU 4x faster.\r\n\r\nOtherwise, maybe it's possible to use [_GLIBCXX_USE_CXX11_ABI](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html) to support the old ABI? According to this page, the new ABI is mostly meant to resolve the concerns of the C++ standards committee.", "To further clarify, I agree with the points @martinwicke and @gunan made about CUDA being responsible for how much Ubuntu 14.04 support we can offer. I'm just hoping that doesn't rule out the possibility of TensorFlow CPU continuing to support Ubuntu 14.04.", "fwiw while nvidia doesn't provide cuda9 docker images for 14.04, it's pretty simple to install cuda9 on a base ubuntu 14.04 image (or even take the cuda8 image and install cuda9 on it).\r\nAll you have to do is install via the cuda9 `.run` file and skip the driver install.", "Technically, you are right that it is an easy task to build such images.\r\nBut we are mostly constrained by what is officially available, as there are more than just technical side to consider.", "IMHO it's too early to drop support for Ubuntu 14.04 and Docker still needs time to mature (ie, [3 months ago](https://aetros.com/blog/Machine%20Learning/14-11-2017-nvidia-docker2-released#Compare) you couldn't use GPUs properly with docker)", "@gunan I'll assign this to you to determine how to handle this issue from here.", "Just to clarify, we are not dropping support. but rather our prebuilt binaries will be built on ubuntu 16.\r\nI know it is inconvenient for many ubuntu 14 users, and I know there a great deal of ubuntu 14 users out there, but building the code on ubuntu 14 will still work.\r\n\r\nAs we use pypi as out primary distribution method for python packages, that also limits our options. For linux, we can only put one package on pypi, pip cannot tell between different CPUs, or linux versions. We currently choose that binary to be built on ubuntu 16.", "I'm pretty sure at this point it's impossible to build TF binaries on Ubuntu 16 that work on Ubuntu 14. The [dual ABI](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html) macro is able to remove a lot of the newer __cxx11 symbols, but no matter what I try, dependencies on new libc and libc++ interfaces just keep sneaking in.\r\n\r\nProbably the only way it'd be possible to maintain Ubuntu 14 support, would be if our Ubuntu 16 machines used an Ubuntu 14 docker container to build the CPU package.\r\n\r\nFor posterity's sake, here's what I did to experiment:\r\n\r\n```sh\r\n# Build TensorFlow pip package on Ubuntu 16.04\r\ndocker run --name=build -v /home/jart/filez:/filez -t -d ubuntu:16.04\r\ndocker exec -i -t build /bin/bash\r\ncd /root\r\napt-get update\r\napt-get install -y --no-install-recommends build-essential python python-dev pkg-config zip g++ zlib1g-dev unzip wget git-core python-numpy python-pip\r\nwget https://github.com/bazelbuild/bazel/releases/download/0.9.0/bazel-0.9.0-installer-linux-x86_64.sh\r\nchmod +x bazel-0.9.0-installer-linux-x86_64.sh\r\n./bazel-0.9.0-installer-linux-x86_64.sh\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\n./configure\r\nbazel build -s --copt=-fabi-version=7 --host_copt=-fabi-version=7 --copt=-D_GLIBCXX_USE_CXX11_ABI=0 --host_copt=-D_GLIBCXX_USE_CXX11_ABI=0 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /filez/tf\r\n\r\n# Test it works with Ubuntu 14.04 C++ ABI\r\ndocker run --name=user -v /usr/local/google/home/jart/filez:/filez -t -d ubuntu:14.04\r\ndocker exec -i -t user /bin/bash\r\ncd /root\r\napt-get update\r\napt-get install -y --no-install-recommends python python-virtualenv\r\nvirtualenv tf\r\ncd tf\r\nsource bin/activate\r\neasy_install -U pip\r\npip install -U /filez/tf/tensorflow-1.5.0rc0-cp27-cp27mu-linux_x86_64.whl\r\npython -c 'import tensorflow'\r\n\r\n# Cleanup\r\ndocker stop build\r\ndocker rm build\r\ndocker stop user\r\ndocker rm user\r\n```", "No, what I meant was for ubuntu 14, binaries can be built on ubuntu 14 for ubuntu 14.\r\nWe still have CI systems that have ubuntu 14 installed, and TF passes all tests ubuntu 14, too.\r\n\r\nIn the future, when some of our plans land this will be averted because we will have distribution systems that will check host OS and hardware, but at the moment pip does not offer much.", "I looked some more into this. Another option is to try and configure Bazel on Ubuntu 16 to build using older libraries.\r\n\r\nUbuntu 16 offers a debug package (`-dbg`) for `libstdc++6-4.8`, the same version that Ubuntu 14 uses (14's highest version is `libstdc++6-4.8.0.19`, but 16's is `.0.21`), as well as older versions of `gcc`. We might be able to configure the [Bazel`CROSSTOOL`](https://github.com/bazelbuild/bazel/wiki/Building-with-a-custom-toolchain#writing-the-crosstool) to compile using the older libsdc++.\r\n\r\n`apt-find` and `locate` came in handy.", "@angersson we would still have a glibc incompatibility even if we fix libstdc++.", "@jhseu That's likely correct. Although @angersson was right that writing a crosstool is the way to go. We'd just probably have to build it on Debian 8 in order to guarantee support for glibc 2.19 and higher. Thankfully Clang team maintains an apt repo so we can have Clang 6.0 on Debian 8. That way we can build portable binaries that support anything up to AVX-512.\r\n\r\nBuilding on Debian 8 should create binaries that work on at least the following distros:\r\n\r\n- Ubuntu 14+\r\n- CentOS 7+\r\n- Debian 8+\r\n- SuSE 13.2+\r\n- Mint 17.3+\r\n- Manjaro 0.8.11\r\n\r\nI sent out a PR adding such a toolchain in https://github.com/tensorflow/tensorflow/pull/16173 although we can't make any promises at this time quite yet, whether or not we'll be able to incorporate this into our release process.", "This issue should be safe to close, now that TF 1.5 is rolled out. To recap:\r\n\r\nThis problem wasn't easy to solve, due to challenging constraints imposed by vendors\u2014who shall remain nameless.\r\n\r\nWhat fixed this is @av8ramit @gunan @case540 (among others) are now putting aside added time to manually build CPU releases on Ubuntu 14.04, to maintain the compatibility commitments we made to the community.\r\n\r\nAn optional [Debian 8 Clang 6.0 toolchain](https://github.com/tensorflow/tensorflow/pull/16173) now also exists, but wasn't what solved this issue; however, it can provide flexibility for folks in the community needing to do compatible source builds\u2014particularly ones that take advantage of recent instructions sets, e.g. AVX-512, on old distros with very recent hardware.\r\n\r\nPlease note I'm not sure what the status is for tf-nightly, which might not be Ubuntu 14 compatible.", "Oh, for posterity, here are some facts we uncovered for anyone who's interested in using Debian 8 with Clang. This could potentially be useful for other people in the industry who are interested in distributing TensorFlow.\r\n\r\n----\r\n\r\nDebian 8 Jessie (lifetime 2015\u21922020) is my personal favorite. Clang team maintains an apt repo for Debian 8. The two have a surprising degree of synergy.\r\n\r\nUnlike the GCC 4.9.2 which comes included with Debian 8, Clang 6.0 is able to generate opcodes for more recent instruction sets, e.g. AVX-512. Those can make TF potentially 10x faster on CPUs like Skylake Xeons (c. 2015) assuming `-march=native` is used.\r\n\r\nIf one's goal is compatibility, `-msse3` is a safe optimization that works on nearly every x86_64 CPU in existence. For example, the optional [TF Debian 8 Clang 6 Bazel CROSSTOOL](https://github.com/tensorflow/tensorflow/pull/16173) should in theory create binaries that are compatible with:\r\n\r\n- Ubuntu 14+\r\n- CentOS 7+\r\n- Debian 8+\r\n- SuSE 13.2+\r\n- Mint 17.3+\r\n- Manjaro 0.8.11\r\n\r\nProof:\r\n\r\n```\r\njart@tomservo:/tmp/pip$ find . -name \\*.so | xargs ldd -v | grep -Po '(?:GCC|GLIBC|CXXABI)_[.\\d]+' | sort -n | sort -u\r\nCXXABI_1.3.7 (GCC 4.8.3)\r\nGCC_4.2.0\r\nGLIBC_2.18\r\njart@tomservo:/tmp/pip$ ldd tensorflow-1.5.0rc0.data/purelib/tensorflow/libtensorflow_framework.so \r\n        linux-vdso.so.1 (0x00007ffe70f80000)\r\n        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f4487259000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4486f58000)\r\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4486d3b000)\r\n        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f4486a30000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4486685000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007f448801b000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f448646f000)\r\n```\r\n\r\nSome facts we uncovered about Linux distributions:\r\n\r\n```\r\n# Linux distro versions\r\n2014 CentOS 5.11 (EOL 2019-08)    gcc4.1 libc2.5 libc++4.1.2 sqlite3.3.6\r\n2015 Wheezy 7.0 (EOL 2018-05)     gcc4.7 libc2.13 libc++4.7.2 sqlite3.7.13\r\n2014 CentOS 7 (EOL 2019-08)       gcc4.8 libc2.17 libc++4.8.5 sqlite3.7.17\r\n2014 Trusty 14.04 (EOL 2019-04)   gcc4.8 libc2.19 libc++4.8.4 sqlite3.8.2\r\n2015 Jessie 8.0 (EOL 2020-05)     gcc4.9 libc2.19 libc++4.8.4 sqlite3.8.7\r\n2016 Xenial 16.04 (EOL 2021-04)   gcc5.3 libc2.23 libc++5.4.0 sqlite3.11.0\r\n\r\n# C++ ABIs\r\nCXXABI_1.3.7 <-- max on ubuntu 14?\r\nCXXABI_1.3.8 <-- where the C++11 ABI break happened for GCC 5.x\r\nstrings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep LIBCXX\r\n```\r\n\r\nSome facts we uncovered about x86 CPUs (w.r.t. scientific computing):\r\n\r\n```\r\n# Noteworthy CPU Features\r\nSSE      128-bit vector instructions (e.g. addps http://www.felixcloutier.com/x86/ADDPS.html)\r\nSSE4.1   Makes SSE good\r\nSSE4.2   Makes SSE do strings and CRC32\r\nAVX      Makes SSE 256-bit (e.g. vaddps)\r\nAVX2     Adds features to AVX\r\nAVX-512  Makes SSE 512-bit\r\nSHA      Makes SSE do SHA1 and SHA256\r\nFMA      Fused multiply\u2013add\r\nCLMUL    Carry-less multiplication https://goo.gl/ritKJX\r\nTSX      Transactional memory instructions for threads http://www.intel.com/software/tsx\r\n\r\n# Intel CPU Line\r\n2003 P6 M           SSE SSE2\r\n2004 prescott       SSE3 SSSE3 (-march=prescott)\r\n2006 core           X64 SSE4.1 (only on 45nm variety) (-march=core2)\r\n2008 nehalem        SSE4.2 VT-x VT-d (-march=nehalem)\r\n2010 westmere       CLMUL AES (-march=westmere)\r\n2012 sandybridge    AVX TXT (-march=sandybridge)\r\n2012 ivybridge      F16C MOVBE (-march=ivybridge)\r\n2013 haswell        AVX2 TSX BMI2 FMA (-march=haswell)\r\n2014 broadwell      RDSEED ADCX PREFETCHW (-march=broadwell - works on trusty gcc4.9)\r\n2015 skylake        SGX ADX MPX AVX-512[xeon-only] (-march=skylake / -march=skylake-avx512 - needs gcc7)\r\n2018 cannonlake     AVX-512 SHA (-march=cannonlake - needs clang5)\r\n\r\n# Intel Low Power CPU Line\r\n2013 silvermont     SSE4.1 SSE4.2 VT-x (-march=silvermont)\r\n2016 goldmont       SHA (-march=goldmont - needs clang5)\r\n\r\n# AMD CPU Line\r\n2003 k8             SSE SSE2 (-march=k8)\r\n2005 k8 (Venus)     SSE3 (-march=k8-sse3)\r\n2008 barcelona      SSE4a?! (-march=barcelona)\r\n2011 bulldozer      SSE4.1 SSE4.2 CLMUL AVX AES FMA4?! (-march=bdver1)\r\n2011 piledriver     FMA (-march=bdver2)\r\n2015 excavator      AVX2 BMI2 MOVBE (-march=bdver4)\r\n\r\n# Google Compute Engine Supported CPUs\r\n# https://cloud.google.com/compute/docs/cpu-platforms\r\n2012 sandybridge 2.6gHz -march=sandybridge\r\n2012 ivybridge   2.5gHz -march=ivybridge\r\n2013 haswell     2.3gHz -march=haswell\r\n2014 broadwell   2.2gHz -march=broadwell\r\n2015 skylake     2.0gHz -march=skylake-avx512\r\n```\r\n\r\nOne possibility Debian/Ubuntu distributors could explore is:\r\n\r\n```\r\n# We might be able to say something like this in /etc/apt/sources.list\r\n# so folks can get a package for their specific microarchitecture.\r\ndeb [arch=skylake] https://apt.tensorflow.org/ stretch stable\r\n```\r\n\r\nPlease note `-msse3` or `-march=native` is not a binary dilemma. It's possible to use a single Debian 8 machine to cross-compile to `-march=whatever`.\r\n\r\nThere also appear to be sweet-spot combinations where you can build binaries that support large subsets in-between. For example, the following optimized CFLAGS have been carefully selected to represent the points in time where Intel and AMD microarchitectures came into alignment.\r\n\r\n```\r\noldcpu: Nearly all x86_64 microarchitectures\r\n  $ intersect <(march-copts core2) <(march-copts k8-sse3)\r\n  -msse3\r\ndefault: x86_64 + SSE4\r\n  $ intersect <(march-copts nehalem) <(march-copts bdver1)\r\n  -mcx16\r\n  -mpopcnt\r\n  -msahf\r\n  -msse3\r\n  -msse4\r\n  -msse4.1\r\n  -msse4.2\r\n  -mssse3\r\navx: x86_64 + AVX\r\n  $ intersect <(march-copts sandybridge) <(march-copts bdver1)\r\n  -maes\r\n  -mavx\r\n  -mavx256-split-unaligned-store\r\n  -mcx16\r\n  -mpclmul\r\n  -mpopcnt\r\n  -msahf\r\n  -msse3\r\n  -msse4\r\n  -msse4.1\r\n  -msse4.2\r\n  -mssse3\r\n  -mxsave\r\navx2: x86_64 + AVX2\r\n  $ intersect <(march-copts haswell) <(march-copts bdver4)\r\n  -maes\r\n  -mavx\r\n  -mavx2\r\n  -mbmi\r\n  -mbmi2\r\n  -mcx16\r\n  -mf16c\r\n  -mfma\r\n  -mfsgsbase\r\n  -mlzcnt\r\n  -mmovbe\r\n  -mpclmul\r\n  -mpopcnt\r\n  -mrdrnd\r\n  -msahf\r\n  -msse3\r\n  -msse4\r\n  -msse4.1\r\n  -msse4.2\r\n  -mssse3\r\n  -mxsave\r\n  -mxsaveopt\r\navx3: x86_64 + AVX-512\r\n  $ march-copts skylake-avx512\r\n  -madx\r\n  -maes\r\n  -mavx\r\n  -mavx2\r\n  -mavx512bw\r\n  -mavx512cd\r\n  -mavx512dq\r\n  -mavx512f\r\n  -mavx512vl\r\n  -mbmi\r\n  -mbmi2\r\n  -mclflushopt\r\n  -mcx16\r\n  -mf16c\r\n  -mfma\r\n  -mfsgsbase\r\n  -mhle\r\n  -mlzcnt\r\n  -mmovbe\r\n  -mpclmul\r\n  -mpku\r\n  -mpopcnt\r\n  -mprfchw\r\n  -mrdrnd\r\n  -mrdseed\r\n  -msahf\r\n  -msse3\r\n  -msse4\r\n  -msse4.1\r\n  -msse4.2\r\n  -mssse3\r\n  -mxsave\r\n  -mxsavec\r\n  -mxsaveopt\r\n  -mxsaves\r\n```\r\n\r\nSee also:\r\n\r\n- https://distrowatch.com/table.php?distribution=ubuntu\r\n- https://distrowatch.com/table.php?distribution=redhat\r\n- http://vault.centos.org/5.11/os/x86_64/CentOS/", "I get the same error while trying to run foxit pdf reader in ubuntu 17.10", "The following solution worked for me on Centos 7:\r\n\r\nUpgrade gcc version to 5.x.x, and then run the followings (depending on the gcc version, you may have a different file of libstdc++.so.6.0.25):\r\n\r\ncp /usr/local/lib64/libstdc++.so.6.0.25 /lib64\r\ncd /lib64\r\nrm -rf libstdc++.so.6\r\nln -s libstdc++.so.6.0.25 libstdc++.so.6", "I get the same error  on Centos 7", "Upgrade gcc or degrade tensorflow version.\r\n\r\n- For example, the latter solution:\r\npip uninstall tensorflow\r\nconda install tensorflow==1.12.0", "i get the same error when i `conda uninstall gcc` or install gcc new version."]}, {"number": 15776, "title": "Compiler Errors Installing Tensorflow from Source", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 SP1\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.5\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: NVIDIA Quadro K4000\r\n\r\n- **Exact command to reproduce**:\r\n\r\nbazel build -c opt %BUILD_OPTS% //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\n\r\nI have tried compiling with MSYS2 and VS2015. I am trying to get VS2015 to work.\r\n\r\n###Using VS2015 and `--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc` (among other options), I get the following error:\r\n\r\n```\r\nc:\\Users\\adam.hendry\\Downloads\\tensorflow>bazel build -c opt %BUILD_OPTS% //tens\r\norflow/tools/pip_package:build_pip_package\r\n.......................\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (6 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (48 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (81 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (93 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (94 packages\r\nloaded)\r\nWARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:\r\n in includes attribute of cc_library rule //tensorflow/core:framework_headers_li\r\nb: '../../external/nsync/public' resolves to 'external/nsync/public' not below t\r\nhe relative path of its package 'tensorflow/core'. This will be an error in the\r\nfuture. Since this rule was created by the macro 'cc_header_only_library', the e\r\nrror might have been caused by the macro implementation in C:/users/adam.hendry/\r\ndownloads/tensorflow/tensorflow/tensorflow.bzl:1138:30\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (116 packages\r\n loaded)\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (127 pack\r\nages loaded).\r\nINFO: Found 1 target...\r\nBuilding: no action\r\n[0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt\r\nINFO: From Compiling external/zlib_archive/uncompr.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzlib.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/adler32.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzclose.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/compress.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\n[134 / 1,014] Compiling external/zlib_archive/deflate.c [for host]; 1s local ...\r\n (16 actions, 14 running)\r\nINFO: From Compiling external/zlib_archive/infback.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling tensorflow/core/lib/hash/crc32c_accelerate.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/highwayhash/highwayhash/arch_specific.cc [for host\r\n]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/crc32.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/inflate.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/inftrees.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzread.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzwrite.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/zutil.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/inffast.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/trees.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/deflate.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/fft2d/fft/fftsg.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/farmhash_archive/src/farmhash.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling tensorflow/compiler/xla/executable_run_options.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\n[349 / 1,966] Compiling external/protobuf_archive/src/google/protobuf/compiler/j\r\ns/embed.cc [for host]; 1s local ... (5 actions, 3 running)\r\nINFO: From Compiling external/farmhash_archive/src/farmhash.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/compiler/js/e\r\nmbed.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling tensorflow/core/grappler/costs/robust_stats.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/lmdb/midl.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\n[643 / 3,343] Compiling external/lmdb/mdb.c; 0s local ... (23 actions, 22 runnin\r\ng)\r\nINFO: From Compiling external/zlib_archive/gzread.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/trees.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzclose.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzwrite.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/crc32.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/inffast.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/inftrees.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/compress.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/uncompr.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/zutil.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/gzlib.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/infback.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/deflate.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/inflate.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/zlib_archive/adler32.c:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/png_archive/pngtrans.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/highwayhash/highwayhash/sip_hash.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/png_archive/pngget.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nINFO: From Compiling external/png_archive/pngread.c [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nERROR: C:/users/adam.hendry/downloads/tensorflow/tensorflow/compiler/xla/service\r\n/cpu/BUILD:772:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu\r\n:custom_call_target_registry' failed (Exit 1): cl.exe failed: error executing co\r\nmmand\r\n  cd C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/execroo\r\nt/org_tensorflow\r\n  SET CUDA_COMPUTE_CAPABILITIE=None\r\n    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1\r\n    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.\r\n1\r\n    SET CUDNN_INSTALL_PATH=C:/cuda\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C\r\n:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program\r\nFiles (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\ucrt;C:\\Program Files (x86)\\Win\r\ndows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\inclu\r\nde\\10.0.16299.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299\r\n.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\\r\nProgram Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program\r\nFiles (x86)\\Windows Kits\\10\\lib\\10.0.16299.0\\ucrt\\x64;C:\\Program Files (x86)\\Win\r\ndows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\1\r\n0.0.16299.0\\um\\x64;\r\n    SET NO_WHOLE_ARCHIVE_OPTION=1\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\Com\r\nmonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studi\r\no 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program F\r\niles (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Mic\r\nrosoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual St\r\nudio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team\r\n Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\r\n\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\\r\nProgram Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDK\r\ns\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\ADAM~1.HEN\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\n    SET TF_CUDA_VERSION=9.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\ADAM~1.HEN\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c ten\r\nsorflow/compiler/xla/service/cpu/custom_call_target_registry.cc /Fobazel-out/x64\r\n_windows_msvc-py3-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/custom_call_\r\ntarget_registry/tensorflow/compiler/xla/service/cpu/custom_call_target_registry.\r\no /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPR\r\nECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /big\r\nobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 -DGEMMLOWP_ALLOW_SLO\r\nW_SCALAR_FALLBACK -w -march=native -D_GLIBCXX_USE_CXX11_ABI=0 /I. /Ibazel-out/x6\r\n4_windows_msvc-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/x64_windows_m\r\nsvc-py3-opt/genfiles/external/bazel_tools /Iexternal/bazel_tools/tools/cpp/gcc3\r\n/showIncludes /MD /O2 /DNDEBUG\r\nC:\\users\\adam.hendry\\appdata\\local\\temp\\_bazel_adam.hendry\\qoyar4dt\\execroot\\org\r\n_tensorflow\\tensorflow\\compiler\\xla\\service\\cpu\\custom_call_target_registry.cc :\r\n fatal error C1083: Cannot open compiler generated file: '': Invalid argument\r\ncl : Command line warning D9002 : ignoring unknown option '-march=native'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 25.271s, Critical Path: 3.48s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n###Using VS2015 and `--cpu=x64_windows_msys --host_cpu=x64_windows_msys` (among other options), I get this error:\r\n\r\n```\r\nc:\\Users\\adam.hendry\\Downloads\\tensorflow>bazel build -c opt %BUILD_OPTS% //tens\r\norflow/tools/pip_package:build_pip_package\r\n.......................\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (6 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (73 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (131 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (141 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (142 packages\r\n loaded)\r\nWARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:\r\n in includes attribute of cc_library rule //tensorflow/core:framework_headers_li\r\nb: '../../external/nsync/public' resolves to 'external/nsync/public' not below t\r\nhe relative path of its package 'tensorflow/core'. This will be an error in the\r\nfuture. Since this rule was created by the macro 'cc_header_only_library', the e\r\nrror might have been caused by the macro implementation in C:/users/adam.hendry/\r\ndownloads/tensorflow/tensorflow/tensorflow.bzl:1138:30\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (157 packages\r\n loaded)\r\nWARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/contrib/learn/BUIL\r\nD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflo\r\nw/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/sessio\r\nn_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/contrib/learn/BUIL\r\nD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflo\r\nw/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/sessio\r\nn_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (254 pack\r\nages loaded).\r\nINFO: Found 1 target...\r\nBuilding: no action\r\n[0 / 17] [-----] BazelWorkspaceStatusAction stable-status.txt\r\n[105 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 2s local ... (2\r\n4 actions running)\r\n[106 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 5s local ... (2\r\n3 actions running)\r\n[111 / 747] Executing genrule @local_config_cuda//cuda:cuda-lib; 10s local ... (\r\n24 actions running)\r\n[113 / 747] Executing genrule //tensorflow/core:version_info_gen [for host]; 13s\r\n local ... (23 actions running)\r\n[129 / 968] Executing genrule //tensorflow/core:version_info_gen [for host]; 20s\r\n local ... (24 actions running)\r\n[148 / 1,274] Executing genrule //tensorflow/core:version_info_gen [for host]; 3\r\n1s local ... (24 actions running)\r\n[149 / 1,275] Executing genrule //tensorflow/core:version_info_gen [for host]; 3\r\n7s local ... (24 actions running)\r\n[149 / 1,275] Executing genrule //tensorflow/core:version_info_gen [for host]; 4\r\n6s local ... (24 actions running)\r\n[155 / 1,279] Executing genrule @local_config_cuda//cuda:cuda-include [for host]\r\n; 57s local ... (23 actions running)\r\n[228 / 1,543] Executing genrule @local_config_cuda//cuda:cuda-include [for host]\r\n; 68s local ... (24 actions, 23 running)\r\n[417 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]\r\n; 83s local ... (24 actions, 23 running)\r\n[437 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]\r\n; 98s local ... (23 actions running)\r\n[458 / 2,009] Executing genrule @local_config_cuda//cuda:cuda-include [for host]\r\n; 115s local ... (24 actions, 23 running)\r\n[523 / 2,096] Executing genrule @local_config_cuda//cuda:cuda-include [for host]\r\n; 135s local ... (24 actions, 23 running)\r\nERROR: C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/exter\r\nnal/com_google_absl/absl/base/BUILD.bazel:30:1: C++ compilation of rule '@com_go\r\nogle_absl//absl/base:spinlock_wait' failed (Exit 1): gcc failed: error executing\r\n command\r\n  cd C:/users/adam.hendry/appdata/local/temp/_bazel_adam.hendry/qoyar4dt/execroo\r\nt/org_tensorflow\r\n  SET CUDA_COMPUTE_CAPABILITIE=None\r\n    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1\r\n    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.\r\n1\r\n    SET CUDNN_INSTALL_PATH=C:/cuda\r\n    SET NO_WHOLE_ARCHIVE_OPTION=1\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Program Files (x86)\\Microsoft Vi\r\nsual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Fi\r\nles (x86)\\MSBuild\\14.0\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\C\r\nommon7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN;C:\\Progra\r\nm Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Windows\\Microsoft.NE\r\nT\\Framework\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VC\r\nPackages;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Micros\r\noft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windo\r\nws Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETF\r\nX 4.6.1 Tools\\;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\;C:\\Program F\r\niles (x86)\\Microsoft Visual Studio 14.0\\VC\\;C:\\Program Files (x86)\\Microsoft Vis\r\nual Studio 14.0\\VC\\bin\\;C:\\Program Files (x86)\\Windows Kits\\10\\;C:\\Python\\Python\r\n36\\;C:\\Python\\Python36\\Scripts\\;C:\\ProgramData\\chocolatey\\bin;C:\\Program Files\\G\r\nit LFS;C:\\Program Files\\CMake\\bin;C:\\Program Files\\Java\\jdk1.8.0_152\\;C:\\Program\r\nData\\Oracle\\Java\\javapath;C:\\Windows;C:\\Windows\\System32;C:\\Windows\\System32Wbem\r\n;C:\\Windows\\System32WindowsPowerShell\\v1.0\\;C:\\bazel\\output;C:\\apache-maven-3.3.\r\n9\\;C:\\apache-maven-3.3.9\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\\r\nv9.1\\;%CUDNN_PATH%;C:\\Program Files\\ImageMagick-7.0.5-Q16\\;C:\\Program Files (x86\r\n)\\QuickTime\\QTSystem\\;C:\\Program Files (x86)\\gs\\gs9.21\\bin\\;C:\\cppan\\cppan.exe;C\r\n:\\Program Files (x86)\\Tesseract-OCR\\;C:\\Program Files\\MiKTeX 2.9\\miktex\\bin\\x64\\\r\n;C:\\ffmpeg\\bin;C:\\Users\\adam.hendry\\.dnx\\bin;C:\\Program Files\\Microsoft DNX\\Dnvm\r\n\\;C:\\Program Files\\HDF_Group\\HDF5\\1.10.1\\bin\\;C:\\Program Files (x86)\\GtkSharp\\2.\r\n12\\bin;C:\\Program Files (x86)\\gettext-iconv\\lib\\gettext;C:\\Program Files (x86)\\C\r\nommon Files\\Intel\\Shared Libraries\\redist\\ia32\\mpirt;C:\\Program Files (x86)\\Comm\r\non Files\\Intel\\Shared Libraries\\redist\\ia32\\compiler;C:\\Program Files\\Common Fil\r\nes\\Microsoft Shared\\Windows Live;C:\\Program Files (x86)\\Common Files\\Microsoft S\r\nhared\\Windows Live;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Pro\r\ngram Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\Progr\r\nam Files (x86)\\Windows Live\\Shared;C:\\Program Files\\Microsoft SQL Server\\130\\Too\r\nls\\Binn\\;C:\\Intel\\OpenCL\\sdk\\bin\\x64;C:\\Intel\\OpenCL\\sdk\\bin\\x86\\;C:\\Intel\\OpenC\r\nL\\sdk\\bin\\Pin;C:\\Intel\\OpenCL\\sdk\\bin\\GTPin;C:\\Program Files\\Git\\cmd;\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python/Python36/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\n    SET TF_CUDA_VERSION=9.1\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n  C:/msys64/usr/bin/gcc -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w -march=native -\r\nstd=gnu++0x -D_GLIBCXX_USE_CXX11_ABI=0 -MD -MF bazel-out/x64_windows_msys-py3-op\r\nt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/external/com_google\r\n_absl/absl/base/internal/spinlock_wait.d -frandom-seed=bazel-out/x64_windows_msy\r\ns-py3-opt/bin/external/com_google_absl/absl/base/_objs/spinlock_wait/external/co\r\nm_google_absl/absl/base/internal/spinlock_wait.o -D__CLANG_SUPPORT_DYN_ANNOTATIO\r\nN__ -iquote external/com_google_absl -iquote bazel-out/x64_windows_msys-py3-opt/\r\ngenfiles/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out\r\n/x64_windows_msys-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_\r\ntools/tools/cpp/gcc3 -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-decla\r\nrations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-re\r\nsult -Wvarargs -Wvla -Wwrite-strings -c external/com_google_absl/absl/base/inter\r\nnal/spinlock_wait.cc -o bazel-out/x64_windows_msys-py3-opt/bin/external/com_goog\r\nle_absl/absl/base/_objs/spinlock_wait/external/com_google_absl/absl/base/interna\r\nl/spinlock_wait.o\r\nIn file included from external/com_google_absl/absl/base/config.h:66:0,\r\n                 from external/com_google_absl/absl/base/port.h:23,\r\n                 from external/com_google_absl/absl/base/internal/spinlock_posix\r\n.inc:23,\r\n                 from external/com_google_absl/absl/base/internal/spinlock_wait.\r\ncc:27:\r\nexternal/com_google_absl/absl/base/policy_checks.h:40:2: error: #error \"Cygwin i\r\ns not supported.\"\r\n #error \"Cygwin is not supported.\"\r\n  ^~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 172.251s, Critical Path: 153.84s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n###Any help would be appreciated. I can give you more details as well.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["```\r\nC:\\users\\adam.hendry\\appdata\\local\\temp\\_bazel_adam.hendry\\qoyar4dt\\execroot\\org\r\n_tensorflow\\tensorflow\\compiler\\xla\\service\\cpu\\custom_call_target_registry.cc :\r\n fatal error C1083: Cannot open compiler generated file: '': Invalid argument\r\n```\r\n\r\nYou are hitting this error because `cl.exe` cannot open a file with path length exceeding the max path length in Windows. This is a known issue if you building with Bazel on Windows. Workaround is to set `TMP`, `TEMP` and `TMPDIR` to short path name like `C:\\tmp` and rebuild again.", "Thank you very much. I will try this fix and let you know the results.\r\n\r\n1. Is there any way we could fix this by prepending path names for Windows using \"\\\\\\\\?\\\\\" ? (see [Naming Files, Paths, and Namespaces](http://msdn.microsoft.com/en-us/library/windows/desktop/aa365247(v=vs.85).aspx)). I ran into this issue several years ago in a different project where I was doing deep searches and I could not change the path name. This extends the max length for the path from 260 characters to 32.767 wide characters.\r\n\r\n2. Can we update the error messages from Bazel to reflect the true error, i.e. that the path name is too long (instead of \"Invalid Argument\").", "I feel like something might have been done here [78b8be6](https://github.com/bazelbuild/bazel/commit/78b8be6486d4cd0e1515e5cc19b865671e01c743), but I am not sure...\r\n\r\nEDIT: I think this doesn't solve it, actually. It seems it just uses Unix path names, but does not prepend with \\\\\\\\?\\\\. You would need to do both for my fix to work.", "> Can we update the error messages from Bazel to reflect the true error, i.e. that the path name is too long (instead of \"Invalid Argument\").\r\n\r\nRoot cause is MSVC `cl.exe` cannot accept path name longer than max path length on Windows. Like you have mentioned, Bazel can accept long path name after https://github.com/bazelbuild/bazel/commit/78b8be6486d4cd0e1515e5cc19b865671e01c743.\r\n\r\nThe fix is either Bazel generating shorter path name or file a bug to Microsoft so that `cl.exe` can be made to understand long path name.\r\n\r\nI have never actually built pip package before, I have only tried to build pure C++ components like tfcompile, label_image and so on with Bazel on Windows. Maybe others can help.", "I deleted my earlier comments as I made a mistake. I had a previously installed version of tensorflow. So, I uninstalled this, cleaned bazel, and wiped the C:/tmp directory for a fresh install.\r\n\r\nI rebuilt using your suggestion, then got the error (boilerplate version):\r\n\r\n\"copts\" doesn't match this configuration\r\n\r\nI attempted to fix by editing the variable NSYNC_GENERIC_OPTS in\r\n\r\nC:/tmp/_bazel_adam.hendry/qoyar4dt/external/nsync/BUILD\r\n\r\nby adding a generic option\r\n\r\n\"//conditions:default\": [],\r\n\r\nper https://lengerrong.blogspot.com/2017/09/fix-up-configurable-attribute-copts.html.\r\n\r\nHowever, after doing this and building again, I get the following error:\r\n\r\n\r\n```\r\nC:\\Windows\\system32>cd C:\\Users\\adam.hendry\\Downloads\\tensorflow\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>python configure.py\r\nYou have bazel 0.9.0 installed.\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d\r\nefault to CUDA 9.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README\r\n.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/\r\nCUDA/v9.1]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuD\r\nNN 7.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to b\r\nuild with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.\r\ncom/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your\r\n build time and binary size. [Default is: 3.5,5.2]3.0\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"-\r\n-config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the envi\r\nronment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>bazel build -c opt %BUILD_OPTS% //tens\r\norflow/tools/pip_package:build_pip_package\r\n.........................\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (5 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (24 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (80 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (92 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (100 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (101 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (101 packages\r\n loaded)\r\nWARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:\r\n in includes attribute of cc_library rule //tensorflow/core:framework_headers_li\r\nb: '../../external/nsync/public' resolves to 'external/nsync/public' not below t\r\nhe relative path of its package 'tensorflow/core'. This will be an error in the\r\nfuture. Since this rule was created by the macro 'cc_header_only_library', the e\r\nrror might have been caused by the macro implementation in C:/users/adam.hendry/\r\ndownloads/tensorflow/tensorflow/tensorflow.bzl:1138:30\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (103 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (113 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (127 pack\r\nages loaded).\r\nINFO: Found 1 target...\r\n[0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: C:/tmp/_bazel_adam.hendry/qoyar4dt/external/com_google_absl/absl/base/BUI\r\nLD.bazel:54:1: C++ compilation of rule '@com_google_absl//absl/base:dynamic_anno\r\ntations' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/tmp/_bazel_adam.hendry/qoyar4dt/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\\r\nProgram Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Fi\r\nles (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\ucrt;C:\\Program Files (x86)\\Windo\r\nws Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\r\n\\10.0.16299.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\r\n\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.16299.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\\r\nProgram Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program\r\nFiles (x86)\\Windows Kits\\10\\lib\\10.0.16299.0\\ucrt\\x64;C:\\Program Files (x86)\\Win\r\ndows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\1\r\n0.0.16299.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\Com\r\nmonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studi\r\no 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program F\r\niles (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Mic\r\nrosoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual St\r\nudio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team\r\n Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\r\n\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\\r\nProgram Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDK\r\ns\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET TEMP=C:\\tmp\r\n    SET TMP=C:\\tmp\r\n    SET TMPDIR=C:\\tmp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c ext\r\nernal/com_google_absl/absl/base/dynamic_annotations.cc /Fobazel-out/host/bin/ext\r\nernal/com_google_absl/absl/base/_objs/dynamic_annotations/external/com_google_ab\r\nsl/absl/base/dynamic_annotations.o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_W\r\nINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDE\r\nXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd\r\n4250 /wd4996 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -w -march=native /Iexternal/c\r\nom_google_absl /Ibazel-out/host/genfiles/external/com_google_absl /Iexternal/baz\r\nel_tools /Ibazel-out/host/genfiles/external/bazel_tools /Iexternal/bazel_tools/t\r\nools/cpp/gcc3 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /showIncludes /MD /O2 /DNDEBUG\r\n-Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-\r\nstrings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla\r\n-Wwrite-strings\r\ncl : Command line error D8021 : invalid numeric argument '/Wextra'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1789.934s, Critical Path: 2.75s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n```\r\n\r\n**Please please help with any suggestions! What is the right way to fix the \"copts\" doesn't match configuration error? I fear that the fix I did only sent the compiler down the wrong path and know cl.exe is using a gcc.exe argument '/Wextra', which cl.exe obviously does not understand.**\r\n", "I just found github issue #13115.\r\n\r\nIt looks like in my clone of tensorflow r1.5 that my `tensorflow/python/eager/BUILD` and `tensorflow/workspace.bzl` files do not match the fixes that were made here!\r\n\r\nI will replace my files with these ones and see if it makes a difference.", "This did not work. I received the following error:\r\n\r\n```\r\n\r\nC:\\Windows\\system32>cd C:\\Users\\adam.hendry\\Downloads\\tensorflow\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>python configure.py\r\nYou have bazel 0.9.0 installed.\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d\r\nefault to CUDA 9.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README\r\n.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/\r\nCUDA/v9.1]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuD\r\nNN 7.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to b\r\nuild with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.\r\ncom/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your\r\n build time and binary size. [Default is: 3.5,5.2]3.0\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"-\r\n-config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the envi\r\nronment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>bazel build -c opt %BUILD_OPTS% //tens\r\norflow/tools/pip_package:build_pip_package\r\n........................\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (7 packages l\r\noaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (56 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (81 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (95 packages\r\nloaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (102 packages\r\n loaded)\r\nWARNING: C:/users/adam.hendry/downloads/tensorflow/tensorflow/core/BUILD:1807:1:\r\n in includes attribute of cc_library rule //tensorflow/core:framework_headers_li\r\nb: '../../external/nsync/public' resolves to 'external/nsync/public' not below t\r\nhe relative path of its package 'tensorflow/core'. This will be an error in the\r\nfuture. Since this rule was created by the macro 'cc_header_only_library', the e\r\nrror might have been caused by the macro implementation in C:/users/adam.hendry/\r\ndownloads/tensorflow/tensorflow/tensorflow.bzl:1138:30\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (106 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (107 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (113 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (114 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (115 packages\r\n loaded)\r\nERROR: C:/users/adam.hendry/downloads/tensorflow/tensorflow/python/BUILD:703:1:\r\nin py_library rule //tensorflow/python:framework_ops: cycle in dependency graph:\r\n\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n    //tensorflow/tools/pip_package:simple_console_for_windows\r\n    //tensorflow:tensorflow_py\r\n    //tensorflow/python:python\r\n    //tensorflow/python:confusion_matrix\r\n    //tensorflow/python:check_ops\r\n    //tensorflow/python:sparse_tensor\r\n.-> //tensorflow/python:framework_ops\r\n|   //tensorflow/python/eager:tape\r\n`-- //tensorflow/python:framework_ops\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' fai\r\nled; build aborted\r\nINFO: Elapsed time: 1310.472s\r\nFAILED: Build did NOT complete successfully (123 packages loaded)\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>\r\n```", "I did a `git clean -fdx`, which removed `.bazelrc, .tf_configure.bazelrc, tools/python_bin_path.sh, workspace.bzl`. Perhaps since the changes to workspace.bzl weren't in the final commit (mentioned 2 posts above this) that that caused the error. I'll do a fresh build...", "No dice. I get same error", "What is the value of `BUILD_OPTS`?\r\n\r\nI never have to run Bazel with `--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc`. Commands like `bazel build --config=opt //tensorflow/examples/label_image:label_image` simply works.\r\n\r\nCan you try to build from master branch?", "`BUILD_OPTS` is this:\r\n\r\n`--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --verbose_failures --experimental_ui --config=opt --config=win-cuda --incompatible_load_argument_is_label=false --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`\r\n\r\nLet me try it with just `--config=opt --config=cuda` to see if it works. It was giving me trouble before, so I will see if by some magic it now works. One second.", "Yep, same error as before. This is why I have all those ops in `BUILD_OPTS`:\r\n\r\n```\r\n\r\nC:\\Windows\\system32>cd C:\\Users\\adam.hendry\\Downloads\r\n\r\nC:\\Users\\adam.hendry\\Downloads>git clone https://github.com/tensorflow/tensorflo\r\nw\r\nCloning into 'tensorflow'...\r\nremote: Counting objects: 281937, done.\r\nremote: Compressing objects: 100% (21/21), done.\r\n^Cceiving objects:   4% (12988/281937), 3.68 MiB | 84.00 KiB/s\r\nC:\\Users\\adam.hendry\\Downloads>git clone https://github.com/tensorflow/tensorflo\r\nw\r\nCloning into 'tensorflow'...\r\nremote: Counting objects: 281937, done.\r\nremote: Compressing objects: 100% (21/21), done.\r\nremote: Total 281937 (delta 3), reused 2 (delta 0), pack-reused 281916\r\nReceiving objects: 100% (281937/281937), 138.32 MiB | 208.00 KiB/s, done.\r\nResolving deltas: 100% (220971/220971), done.\r\nChecking out files: 100% (9606/9606), done.\r\n\r\nC:\\Users\\adam.hendry\\Downloads>cd .\\tensorflow\\tensorflow\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow\\tensorflow>git checkout r1.5\r\nSwitched to a new branch 'r1.5'\r\nBranch r1.5 set up to track remote branch r1.5 from origin.\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow\\tensorflow>cd ..\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>python configure.py\r\nExtracting Bazel installation...\r\nYou have bazel 0.9.0 installed.\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to d\r\nefault to CUDA 9.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README\r\n.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/\r\nCUDA/v9.1]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuD\r\nNN 7.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to b\r\nuild with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.\r\ncom/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your\r\n build time and binary size. [Default is: 3.5,5.2]3.0\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"-\r\n-config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the envi\r\nronment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\n\r\nC:\\Users\\adam.hendry\\Downloads\\tensorflow>bazel build -c opt --config=opt --conf\r\nig=cuda //tensorflow/tools/pip_package:build_pip_package\r\n.........................\r\nLoading:\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nERROR: No default_toolchain found for cpu 'x64_windows'. Valid cpus are: [\r\n  k8,\r\n  piii,\r\n  arm,\r\n  darwin,\r\n  ppc,\r\n]\r\nINFO: Elapsed time: 139.018s\r\nFAILED: Build did NOT complete successfully (3 packages loaded)\r\n```", "I am currently building pip package locally (for the first time), [3148/3850] now, so it will probably work in my local build.\r\n\r\nI am building from master branch, that is `git checkout master` rather than `git checkout r1.5`. Can you try that?\r\n\r\nI am building without CUDA because my device dies not support it.\r\n\r\nIf I recalled it correctly, I have tried to build with `-c opt` before and it did not work.", "Are you building on Windows?", "Yes, on Windows 10 with Visual Studio 2017 15.5. By the way, my local build had just completed successfully.\r\n\r\nDo you have any reason to use Bazel instead of [CMake](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake) to build pip package?\r\nBuilding Tensorflow with Bazel on Windows is still very experimental.", "None at all. I can use CMake. I just need to build for TF 1.5, Cuda Toolkit 9.0, CUDNN 7, and Python 3.6.4 on Windows 7 SP1.", "Do you have a tutorial on building from CMake?", "Hey, I am getting this error.can anyone help I have protobuf installed yet i am getting this error.\r\n\r\nbazel build -c opt %BUILD_OPTS% //tensorflow/tools/pip_package:build_pip_package\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded)\r\nERROR: K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/tensorflow/tools/pip_package/BUILD:141:1:                                                error loading package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such pack                                               age '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 84\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 55, in                                                _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 36, in                                                _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/aakash/appdata/loc                                               al/temp/_bazel_aakash/lfb2hdsw/external/protobuf_archive -i K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflo                                               w-1.5.0-rc0/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: Timed out and referenced by '//tensorflow/tools/pip_package:build_pip_package'\r\nERROR: K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/tensorflow/tools/pip_package/BUILD:141:1:                                                error loading package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such pack                                               age '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 84\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 55, in                                                _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 36, in                                                _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/aakash/appdata/loc                                               al/temp/_bazel_aakash/lfb2hdsw/external/protobuf_archive -i K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflo                                               w-1.5.0-rc0/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: Timed out and referenced by '//tensorflow/tools/pip_package:build_pip_package'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loadin                                               g package 'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobu                                               f_archive//': Traceback (most recent call last):\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 84\r\n                _apply_patch(ctx, ctx.attr.patch_file)\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 55, in                                                _apply_patch\r\n                _execute_and_check_ret_code(ctx, cmd)\r\n        File \"K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflow-1.5.0-rc0/third_party/repo.bzl\", line 36, in                                                _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(256) when executing 'C:\\msys64\\usr\\bin\\bash.exe -c patch -p1 -d C:/users/aakash/appdata/loc                                               al/temp/_bazel_aakash/lfb2hdsw/external/protobuf_archive -i K:/classes/tensorflow/tensorflow-1.5.0-rc0/tensorflo                                               w-1.5.0-rc0/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: Timed out\r\nINFO: Elapsed time: 111.905s\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n", "@Aakashjagwani pardon my confusion, but I'm not seeing why your problem is the same as what's described in this bug. Can you please draw the connections? If not, would it be better to open a separate bug to track independently?", "@mrry any perspectives to offer on Bazel v. Cmake on Windows?", "I don't believe XLA is supported on Windows using either build system.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of activity. Please feel free to re-open if more information becomes available."]}, {"number": 15775, "title": "R1.2", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Please open a clean PR if you have contribution you'd like us to look at. Thanks!"]}, {"number": 15774, "title": "Fix invalid Markdown in docstring", "body": "There is currently invalid markdown in the docstring, which is causing the docs site to render incorrectly.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/gan/estimator/GANEstimator\r\n\r\n<img width=\"902\" alt=\"screen shot 2018-01-01 at 10 17 59 am\" src=\"https://user-images.githubusercontent.com/279498/34469987-1d2fbce6-eedd-11e7-896b-a43f65326fd0.png\">\r\n\r\nThis patch fixes the indentation on the MD code block, which should fix the issue.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}]