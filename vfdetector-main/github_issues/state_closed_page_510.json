[{"number": 38450, "title": "XLA without autoclustering?", "body": "https://www.tensorflow.org/xla/#explicit_compilation_with_tffunction seems to imply that it is possible to enable XLA for parts of the graph without involving autoclustering.\r\n\r\nI am attempting to do this for the Gelu op in BERT https://github.com/google-research/bert/blob/master/modeling.py#L264 as suggested here https://github.com/tensorflow/tensorflow/pull/37937 and the observed behavior does not match my expectation.\r\n\r\nI'm adding the tf.function tag to a function 'gelu' that contains tf.tanh and some cwise arithmetics, but not touching global jit options, etc. My expectation is to produce a number of identical clusters implementing this 'gelu', and possibly a number of clusters implementing its gradients.\r\n\r\nWhat I'm actually seeing instead, is a number of clusters that seem to be greedily constructed around 'gelu's, but are also sweeping everything in the neighborhood, up to and including GEMMs:\r\n\r\n```\r\n2020-04-11 09:17:46.357008: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:888] Assigning node gradients/AddN_149 to cluster cluster_1\r\n2020-04-11 09:17:46.372574: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1471] *** Clustering info for graph of size 14527\r\n2020-04-11 09:17:46.372603: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1472]  Built 50 clusters, size 4788 / 14527 (32.96%)\r\n2020-04-11 09:17:46.372622: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_0 114 / 14527 (0.78%)\r\n2020-04-11 09:17:46.372630: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddN: 2 instances\r\n2020-04-11 09:17:46.372636: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddV2: 4 instances\r\n2020-04-11 09:17:46.372642: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAdd: 3 instances\r\n2020-04-11 09:17:46.372648: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAddGrad: 2 instances\r\n2020-04-11 09:17:46.372654: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Const: 24 instances\r\n2020-04-11 09:17:46.372660: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Exp: 1 instances\r\n2020-04-11 09:17:46.372666: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    LogSoftmax: 2 instances\r\n2020-04-11 09:17:46.372671: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    MatMul: 5 instances\r\n2020-04-11 09:17:46.372677: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mean: 3 instances\r\n2020-04-11 09:17:46.372683: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mul: 19 instances\r\n2020-04-11 09:17:46.372688: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Neg: 4 instances\r\n2020-04-11 09:17:46.372695: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    OneHot: 2 instances\r\n2020-04-11 09:17:46.372701: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-11 09:17:46.372706: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    ReadVariableOp: 8 instances\r\n2020-04-11 09:17:46.372712: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RealDiv: 1 instances\r\n2020-04-11 09:17:46.372717: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reciprocal: 1 instances\r\n2020-04-11 09:17:46.372723: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reshape: 10 instances\r\n2020-04-11 09:17:46.372729: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Rsqrt: 1 instances\r\n2020-04-11 09:17:46.372734: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RsqrtGrad: 1 instances\r\n2020-04-11 09:17:46.372742: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    SquaredDifference: 1 instances\r\n2020-04-11 09:17:46.372750: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Squeeze: 1 instances\r\n2020-04-11 09:17:46.372759: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    StridedSlice: 1 instances\r\n2020-04-11 09:17:46.372767: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sub: 3 instances\r\n2020-04-11 09:17:46.372775: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sum: 9 instances\r\n2020-04-11 09:17:46.372783: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tanh: 1 instances\r\n2020-04-11 09:17:46.372792: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tile: 4 instances\r\n2020-04-11 09:17:46.372810: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_1 232 / 14527 (1.60%)\r\n2020-04-11 09:17:46.372840: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddN: 13 instances\r\n2020-04-11 09:17:46.372849: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BatchMatMulV2: 8 instances\r\n2020-04-11 09:17:46.372859: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAddGrad: 10 instances\r\n2020-04-11 09:17:46.372868: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Const: 50 instances\r\n2020-04-11 09:17:46.372877: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    MatMul: 23 instances\r\n2020-04-11 09:17:46.372885: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mul: 58 instances\r\n2020-04-11 09:17:46.372895: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Neg: 4 instances\r\n2020-04-11 09:17:46.372904: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-11 09:17:46.372913: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reshape: 19 instances\r\n2020-04-11 09:17:46.372927: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RsqrtGrad: 4 instances\r\n2020-04-11 09:17:46.372936: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sub: 6 instances\r\n2020-04-11 09:17:46.372945: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sum: 19 instances\r\n2020-04-11 09:17:46.372958: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tile: 8 instances\r\n2020-04-11 09:17:46.372967: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Transpose: 8 instances\r\n2020-04-11 09:17:46.372976: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    UnsortedSegmentSum: 1 instances\r\n2020-04-11 09:17:46.372995: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_10 113 / 14527 (0.78%)\r\n2020-04-11 09:17:46.373006: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    AddN: 6 instances\r\n2020-04-11 09:17:46.373016: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BatchMatMulV2: 4 instances\r\n2020-04-11 09:17:46.373024: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    BiasAddGrad: 6 instances\r\n2020-04-11 09:17:46.373032: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Const: 23 instances\r\n2020-04-11 09:17:46.373040: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    MatMul: 12 instances\r\n2020-04-11 09:17:46.373049: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Mul: 29 instances\r\n2020-04-11 09:17:46.373059: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Neg: 2 instances\r\n2020-04-11 09:17:46.373068: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-11 09:17:46.373077: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Reshape: 8 instances\r\n2020-04-11 09:17:46.373087: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    RsqrtGrad: 2 instances\r\n2020-04-11 09:17:46.373111: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sub: 3 instances\r\n2020-04-11 09:17:46.373119: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Sum: 9 instances\r\n2020-04-11 09:17:46.373128: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Tile: 4 instances\r\n2020-04-11 09:17:46.373136: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    Transpose: 4 instances\r\n2020-04-11 09:17:46.373151: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_11 113 / 14527 (0.78%)\r\n\r\n```\r\nThis presents reliability problems (if XLA is going to drag random operations from the graph into the cluster, I can never be sure that it won't end up with something it can't compile correctly) and also performance problems (instead of compiling 1 or 2 relatively short kernels, it ends up trying to compile 100 large kernels and that takes several minutes.)\r\n\r\nIs this the intended behavior, and can this be avoided?", "comments": ["I should note that I see this even if I set autoclustering policy in compiler/jit/xla_gpu_device.cc to kIfExplicitlyRequested.\r\n\r\nI think this is happening because the function MarkForCompilationPassImpl::RunEdgeContractionLoop() is executed regardless of the global JIT setting. If I comment it out, I get the exact behavior I originally expected:\r\n```\r\n2020-04-12 03:17:07.617912: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1471] *** Clustering info for graph of size 14527\r\n2020-04-12 03:17:07.617948: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1472]  Built 50 clusters, size 50 / 14527 (0.34%)\r\n2020-04-12 03:17:07.617973: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_0 1 / 14527 (0.01%)\r\n2020-04-12 03:17:07.617984: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-12 03:17:07.617992: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_1 1 / 14527 (0.01%)\r\n2020-04-12 03:17:07.618001: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-12 03:17:07.618009: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_10 1 / 14527 (0.01%)\r\n2020-04-12 03:17:07.618018: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-12 03:17:07.618026: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_11 1 / 14527 (0.01%)\r\n2020-04-12 03:17:07.618035: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-12 03:17:07.618043: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_12 1 / 14527 (0.01%)\r\n2020-04-12 03:17:07.618052: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1485]    PartitionedCall: 1 instances\r\n2020-04-12 03:17:07.618060: I tensorflow/compiler/jit/mark_for_compilation_pass.cc:1481]   cluster_13 1 / 14527 (0.01%)\r\n```\r\nPerhaps that function should be made conditional on the setting of tf_xla_auto_jit?", "@cheshire can you comment?", "Are you using the nightly build?\r\n\r\n> https://www.tensorflow.org/xla/#explicit_compilation_with_tffunction seems to imply that it is possible to enable XLA for parts of the graph without involving autoclustering.\r\n\r\nYes!\r\n\r\n> I'm adding the tf.function tag to a function 'gelu' that contains tf.tanh and some cwise arithmetics, but not touching global jit options, etc\r\n\r\nYes, that makes sense, it should be enough to add the annotation `@tf.function(experimental_compile=True)`\r\n\r\n> My expectation is to produce a number of identical clusters implementing this 'gelu', and possibly a number of clusters implementing its gradients\r\n\r\nBasically yes, though calling them \"clusters\" can cause confusion because it's a separate mechanism from autoclustering. It will note produce identical clusters during reuse, but it would have to recompile for each new shape.\r\n\r\n> What I'm actually seeing instead, is a number of clusters that seem to be greedily constructed around 'gelu's, but are also sweeping everything in the neighborhood, up to and including GEMMs:\r\n\r\n`mark_for_compilation_pass` is for autoclustering, and actually should not be doing anything at all for `tf.function`. What is the exact command you are using? Is autoclustering definitely off?\r\n\r\n> Is this the intended behavior, and can this be avoided?\r\n\r\nIt honestly sounds like the autoclustering mode is on.\r\n\r\n> I should note that I see this even if I set autoclustering policy in compiler/jit/xla_gpu_device.cc to kIfExplicitlyRequested.\r\n\r\nAre you using the XLA_GPU devices? They are deprecated and cause a lot of confusion and we are trying to remove them, but it takes time. A simple answer here is not to use XLA_GPU device and not to change any settings there.\r\n\r\n> MarkForCompilationPassImpl::RunEdgeContractionLoop() is executed regardless of the global JIT setting\r\n\r\nI think it's always executed, but it should not do anything if nothing is marked for compilation?", "I am using the rocm fork with all the latest changes from master merged in, I am not using XLA_GPU devices, and autoclustering is definitely off. \r\n\r\nHere's my proposed fix https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/929/files#diff-73244848675e057e66040c0bcef78b51 - on its own, it breaks a few unit tests, and I have to fix them by adding tf_xla_auto_jit=2  to TF_XLA_FLAGS. Any problems with it?\r\n\r\nAn alternative would be to go deeper into the code and e.g. check whether both ends of an edge have is_xla_compile_attr_true flags before contracting them.\r\n", "@ekuznetsov139 Is there a simple reproducer I could try to see this problem?\r\nIf you are willing to dive deep, could you see why `mark_for_compilation_pass` is doing anything at all? There is a [filter](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/jit/mark_for_compilation_pass.cc;l=1044?q=bool%20is_xla_compile_attr_true%20) which should only consider nodes marked with `kXlaMustCompileAttr` (those are created by `tf.function(experimental_compile=True)`) when the global JIT setting is on.", "Just noticed this https://github.com/tensorflow/tensorflow/commit/f7d337b2ba182332a8a465d105af3608535b20df\r\n\r\nThe change is dated 4/1, and I was working with a fork synchronized up to 3/30 :( Pretty sure it fixes my problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38450\">No</a>\n", "Yup, that's why I have asked whether it is latest TF."]}, {"number": 38449, "title": "ValueError: Error when checking : expected input_1 to have shape (608, 608, 3) but got array with shape (416, 416, 3)", "body": "please give me suggestions ? how to solve this error\r\n\r\nif __name__ == '__main__':\r\n    yolo = YOLO(0.6, 0.5)\r\n    file = 'data/coco_classes.txt'\r\n    all_classes = get_classes(file)\r\n \r\n    # detect images in test floder.\r\n    for (root, dirs, files) in os.walk('images/test'):\r\n        if files:\r\n\r\n            for f in files:\r\n                print(f)\r\n                \r\n                path = os.path.join(root, f)\r\n                image = cv2.imread(path)\r\n                image = np.array(image)\r\n                image = detect_image(image, yolo, all_classes)\r\n                cv2.imwrite('images/res/' + f, image)\r\n                \r\n\r\ndef process_image(img):\r\n    \"\"\"Resize, reduce and expand image.\r\n\r\n    # Argument:\r\n        img: original image.\r\n\r\n    # Returns\r\n        image: ndarray(64, 64, 3), processed image.\r\n    \"\"\"\r\n    \r\n    image = cv2.resize(img, (416, 416),\r\n                       interpolation=cv2.INTER_CUBIC)\r\n    image = np.array(image, dtype='float32')\r\n    image /= 255.\r\n    image = np.expand_dims(image, axis=0)\r\n\r\n    return image\r\n\r\nValueError: Error when checking : expected input_1 to have shape (608, 608, 3) but got array with shape (416, 416, 3)", "comments": ["@haideralimughal,  During training, YOLO takes images of size 320\u00d7320, 352\u00d7352, \u2026 and 608\u00d7608 (with a step of 32). so you need to replace line \r\n`image = cv2.resize(img, (416, 416), interpolation=cv2.INTER_CUBIC)`\r\nwith\r\n`image = cv2.resize(img, (608,608), interpolation=cv2.INTER_CUBIC)`.", "@haideralimughal \r\nplease update as per above comment ,\r\n also please let us know which tensorflow version are you facing this error in.", "@haideralimughal\r\nplease update on the above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38449\">No</a>\n"]}, {"number": 38448, "title": "ValueError: Cannot infer num from shape (None, None) when unstacking dense tensor converted from sparse tensor", "body": "Hello,\r\n\r\nIam trying to create a Keras compatible model using Tensorflow to use the CTC loss and I just found a problem : after decoding the output of the neural network using ``tf.nn.ctc_greedy_decoder``, it returns an array with a sparse tensor, and in order to be able to use GPUs and TPUs to calculate the loss with ``tf.nn.ctc_loss`` we need to convert the sparse tensor to a dense tensor, so I used ``tf.sparse.to_dense``, but when I try to get the ``logit_length`` requested by the loss function using ``tf.unstack`` I get the following error : ``ValueError: Cannot infer num from shape (None, None)``. The thing is when I use ``tf.unstack`` on a dense tensor converted from a sparse tensor I don't have any issues.\r\n\r\nHere is the part of code that raised this issue  (both ``batch_inputs`` and ``batch_targets`` are dense tensors): \r\n\r\n```\r\n@tf.function\r\ndef train_step(self, batch_inputs, batch_targets):\r\n    with tf.GradientTape(persistent = True) as tape:\r\n        # Doing a manual prediction because calling Keras's predict method inside a tensorflow function is not supported\r\n        X = batch_inputs\r\n        for layer in self.neural_network.layers:\r\n            X = layer(X)\r\n\r\n        logits = tf.transpose(X, [1, 0, 2]) \r\n        logits_sequence_length = [logit.shape[0] for logit in tf.unstack(logits)] \r\n        decoded, neg_sum_logits = self.ctc_decoder(X, logits_sequence_length) \r\n        decoded_logits = decoded[0] \r\n        dense_decoded_logits = tf.sparse.to_dense(decoded_logits) \r\n        decoded_logits_sequence_length = [decoded_logit.shape[0] for decoded_logit in tf.unstack(dense_decoded_logits)]\r\n        batch_target_sequence_length = [target.shape[0] for target in tf.unstack(batch_targets)] \r\n        batch_loss, average_batch_loss = self.calculate_loss(batch_targets, dense_decoded_logits, batch_target_sequence_length, decoded_logits_sequence_length) \r\n        with tape.stop_recording():\r\n            gradients = tape.gradient(batch_loss, self.neural_network.trainable_variables) \r\n            self.optimizer.apply_gradients(zip(gradients, self.neural_network.trainable_variables)) \r\n            decoded_logits = tf.transpose(decoded_logits, [1, 0, 2]) \r\n            sparse_batch_targets = tf.math.dense_to_sparse(batch_targets, ignore_values = 0) \r\n            batch_ler, average_batch_ler = self.calculate_ler(sparse_batch_targets, decoded_logits) \r\n            return average_batch_loss.numpy(), average_batch_ler.numpy()\r\n```\r\n\r\nMore specifically, this line :  `` decoded_logits_sequence_length = [decoded_logit.shape[0] for decoded_logit in tf.unstack(dense_decoded_logits)]``\r\n\r\nAnd this is the full error : \r\n```\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-44-88cbf7d06e0e> in <module>()\r\n      5           val_inputs = dev_spectrograms,\r\n      6           val_targets = dense_dev_targets,\r\n----> 7           val_batch_size = 32)\r\n\r\n10 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-42-c1dfc702e50f>:173 train_step  *\r\n        decoded_logits_sequence_length = [decoded_logit.shape[0] for decoded_logit in tf.unstack(dense_decoded_logits)]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1510 unstack  **\r\n        raise ValueError(\"Cannot infer num from shape %s\" % value_shape)\r\n\r\n    ValueError: Cannot infer num from shape (None, None)\r\n```\r\n\r\nAfter trying a few things, I noticed that this works perfectly if I return ``dense_decoded_logits`` and unstack it outside the function, but for some reason it won't work inside.\r\n\r\nI'm using Google Colab high-RAM runtime with GPU running Tensorflow version 2.2.0rc2", "comments": ["@Victor-Almeida \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "[Here](https://colab.research.google.com/drive/1Nz_Q7nOdPtxDwL9iTLpG_g-O_33-AzBG)", "@Victor-Almeida \r\n\r\nWill it be possible to share dataset to reproduce the issue in our environment. Thanks!", "Sure. I shared [this](https://drive.google.com/drive/folders/1bgGte_wVyaYAycBntQA8uQWmVQZqhPYH?usp=sharing) folder from my Google Drive. You don't need to download it all, just the ones referenced in the notebook.", "@Victor-Almeida \r\n\r\nI have tried on colab with TF version 2.2.0-rc2 and i am seeing different error message (`AttributeError: 'list' object has no attribute 'shape'`).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/06c8135bfdd56551f130dd2607b55f52/untitled777.ipynb). Thanks!", "That error happened because I edited the notebook after sending the link and tried to print the shape of a list, but lists don't have shape.\r\n\r\nI solved the problem by setting the shape of the tensor manually using ``tf.ensure_shape``, but it should be investigated why the sparse tensor outputted by ``ctc_greedy_decoder`` has shape ``(None, None)`` when converted to a dense tensor.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38448\">No</a>\n"]}, {"number": 38447, "title": "Adding progress bar also while evaluating the model", "body": "fixes tensorflow/addons issue [#1620](https://github.com/tensorflow/addons/issues/1620).  \r\n\r\n@gabrieldemarmiesse, rather than changing methods like `on_test_batch_begin()`, I have changed the `configure_callback()` method which is used to add `ProgBar` in train, now with this change, it will add in test too. Also, there is not requirement of model history logs while test progress bar. So, I have not added that one.  \r\n\r\n@gabrieldemarmiesse and @mihaimaruseac - Please review. ", "comments": ["Hi @ashutosh1919 this is not what I meant to say in https://github.com/tensorflow/addons/issues/1620. I really wanted to have the `tqdm` progress bar which is very specific, as a callback (note that there is already a progress bar when using `evaluate`). \r\nAlso here, you added the progress bar in any case, no matter if verbose or not, which is probably not something we want.\r\n\r\nI think the best way to solve this issue is as @gabrieldemarmiesse pointed out to implement the `on_test_batch_(begin|end)` methods.", "I believe the pull request should be made to tensorflow/addons, not tensorflow/tensorflow indeed, specificly for the tqdm callback.", "@zaccharieramzi and @gabrieldemarmiesse, working on it. Please give me sometime. Let me understand the problem. I will soon raise PR there.", "Closing this one."]}, {"number": 38446, "title": "Incorrect zero recognition", "body": "- Tensorflow 2.2.0rc2\r\n- Windows 10\r\n- Python 3.8.2\r\n\r\nI have an image with zeros that tensorflow recognizes as eights, can this be fixed somehow?\r\n\r\nImage:\r\n![Screenshot_2](https://user-images.githubusercontent.com/9198186/79034675-b17e4080-7bc0-11ea-94fd-448af68fde28.png)\r\nResult:\r\n8,88\r\n8,88\r\n", "comments": ["@Mostalk, Which model you are using to predict these results. One possible reason can be that model is not trained on such type of zero(0 with a cross line) and trained on zero like 0. ", "@Mostalk In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38445, "title": "Why does work_group_launch_order in TFLite Metal matter?", "body": "https://github.com/tensorflow/tensorflow/blob/9a9a4bfec64cec8a12d31809404ece94a4f55771/tensorflow/lite/delegates/gpu/metal/kernels/conv.cc#L59\r\n\r\nI'm a beginner of Metal and trying to understand the Metal implementation of convolution in TFLite. After having read this line of code and found all usages, I'm really confused that why the `work_group_launch_order` matters. How does this machenism work indeed? Is it related to the way GPU linearizes the 3D work group?\r\n\r\nThanks for your reply.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 38444, "title": "Add Python 2 EOL notice", "body": "Since 2.2 is the first full version without Python 2 support, I feel like it could be a good idea to note it, even though 2.1 was announced to lose Python 2 support. Since this coincides with the removal of `-py3` docker images I think it's a fitting place for both.", "comments": []}, {"number": 38443, "title": "Simple graph invoking tf.complex() doesn't work on GPU, but works on CPU", "body": "**Environment**: Windows 10, Python 3.6, Tensorflow 2.1.0-rc2\r\n\r\nThe code below demonstrates a minimal working example of the  bug.  This code results in CUDA_ERROR_LAUNCH_FAILED when run on the GPU.  But, if you run on the CPU, the code has no issues.  I suspect the problem lies in the tensor coming out of tf.complex() as if I do not use that function, the issues seems to go away.\r\n\r\nA small working example shows the error I get along with working code to reproduce on Windows 10.\r\n\r\n```\r\n2020-04-10 16:19:43.846387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-10 16:19:44.860247: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2020-04-10 16:19:44.879431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-10 16:19:45.231402: E tensorflow/stream_executor/cuda/cuda_driver.cc:948] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.231880: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.232665: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.233121: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.233532: E tensorflow/stream_executor/stream.cc:5452] Internal: Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.233951: E tensorflow/stream_executor/cuda/cuda_driver.cc:613] failed to load PTX text as a module: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.234331: E tensorflow/stream_executor/cuda/cuda_driver.cc:618] error log buffer (1024 bytes): \r\n2020-04-10 16:19:45.234634: W tensorflow/core/kernels/gpu_utils.cc:68] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to load PTX text as a module: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\r\n2020-04-10 16:19:45.235499: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.235957: I tensorflow/stream_executor/stream.cc:4963] [stream=000001EA81FD2D60,impl=000001EA92405DC0] did not memzero GPU location; source: 0000008AE2D3C858\r\n2020-04-10 16:19:45.236342: E tensorflow/stream_executor/cuda/cuda_driver.cc:613] failed to load PTX text as a module: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-04-10 16:19:45.236834: E tensorflow/stream_executor/cuda/cuda_driver.cc:618] error log buffer (1024 bytes): \r\n2020-04-10 16:19:45.237205: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: cuDNN launch failure : input shape([5,16,256,256]) filter shape([1,1,16,1])\r\n\t [[{{node model/conv2d_1/Conv2D}}]]\r\n```\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ninput_size = (256,256,1)\r\ninput_real = tf.keras.layers.Input(input_size)\r\ninput_imag = tf.keras.layers.Input(input_size)\r\n\r\n# Get input into mag and phase \r\ncpx_input = tf.keras.layers.Lambda(lambda x: tf.complex(x[0], x[1]))([input_real, input_imag])    \r\nabs_of_input = tf.math.abs(cpx_input)\r\nphase_of_input =  tf.math.angle(cpx_input) \r\n\r\n# Add some trainiable weights\r\nconv1 = tf.keras.layers.Conv2D(16, 5, padding = 'same')(abs_of_input)\r\nmask = tf.keras.layers.Conv2D(1, 1)(conv1) \r\nfiltered_freq = mask * abs_of_input\r\nreconstructedFreq_dc_centered = tf.complex(mask, 0.0) * tf.math.exp(tf.complex(0.0,1.0)*tf.complex(phase_of_input, 0.0))  # I believe this is the offending line\r\ntmp = tf.math.abs(reconstructedFreq_dc_centered)\r\n\r\nmodel = tf.keras.models.Model([input_real, input_imag], tmp)\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='SGD', loss = 'mse')\r\n\r\nx_real = np.random.randn(5, 256, 256, 1)\r\nx_imag = np.random.randn(5, 256, 256, 1)\r\n\r\nmodel.train_on_batch(x = [x_real, x_imag], y = x_real)\r\n\r\n```\r\nEDIT 1: Simplified code more.", "comments": ["@isaacgerg \r\ni ran the code shared by you on tf-nightly and do not face any errors, please find the [gist here on cpu](https://colab.sandbox.google.com/gist/Saduf2019/b8218d19502bc2f79ee135db10290ae5/untitled135.ipynb) same [on gpu](https://colab.sandbox.google.com/gist/Saduf2019/4d22b54a5a30eb3f9c4b8e2cafe5ca37/gpu.ipynb)", "@Saduf2019 \r\nI updated to tf-nightly and the bug still exists.  Can you rerun in a Windows 10 environment (that's the environment I mention in the first post of where the error occurs)?", "As mentioned in the error message \r\n\r\n> Invoking GPU asm compilation is supported on Cuda non-Windows platforms only Relying on driver to perform ptx compilation. This message will be only logged once.\r\n\r\nThis is the reason for you running into the error on windows @isaacgerg ", "@gowthamkpr Why doesnt the driver perform the ptx compilation then?  The operation is simple, a FOIL multiply of complex numbers.  ", "I think the message from redzone_allocator.cc is a red herring and that the `CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure` errors have some other root cause.  Can you please attach the full log?", "Hi @sanjoy, the fully log is in the first post. Let me know if you need anything else.", "@sanjoy Any update on this?  How can i help?", "Hi @isaacgerg,\r\n\r\nIt is quite difficult to say much from\r\n\r\n```\r\n2020-04-10 16:19:45.231402: E tensorflow/stream_executor/cuda/cuda_driver.cc:948] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n```\r\n\r\nif that's all the logs say.  Can you try running with `CUDA_LAUNCH_BLOCKING=1` set?  Maybe that will help narrow this down.", "@isaacgerg \r\nCould you please update with respect tot he above comment, or verify with later tf versions [2.4.1] if you still face the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38443\">No</a>\n"]}, {"number": 38442, "title": "PySpark Deep Learning Pipeline - ClassNotFoundException: org.tensorframes.ShapeDescription", "body": "I am getting the following exception while running Deep Learning Pipeline in Spark.\r\nVersions of \r\n\r\ntensorflow and tensorframes : 2.0.0\r\nconda : 4.8.3\r\npip : 20.0.2\r\npython: 3.6\r\n\r\nThe exception comes while calling fit method of the pipeline:\r\n\r\n    p = Pipeline(stages=[featurizer, lr])\r\n    p_model = p.fit(train_df)\r\n\r\n**Stack Trace**\r\n\r\n   ```\r\n Traceback (most recent call last):\r\n      File \"<stdin>\", line 1, in <module>\r\n      File \"/opt/spark/python/pyspark/ml/base.py\", line 132, in fit\r\n        return self._fit(dataset)\r\n      File \"/opt/spark/python/pyspark/ml/pipeline.py\", line 107, in _fit\r\n        dataset = stage.transform(dataset)\r\n      File \"/opt/spark/python/pyspark/ml/base.py\", line 173, in transform\r\n        return self._transform(dataset)\r\n      File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\r\n        return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\r\n      File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n      File \"/opt/spark/python/pyspark/sql/utils.py\", line 63, in deco\r\n        return f(*a, **kw)\r\n      File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\n    py4j.protocol.Py4JJavaError: An error occurred while calling o56.transform.\r\n    : java.lang.NoClassDefFoundError: org/tensorframes/ShapeDescription\r\n            at com.databricks.sparkdl.DeepImageFeaturizer.transform(DeepImageFeaturizer.scala:124)\r\n            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n            at java.lang.reflect.Method.invoke(Method.java:498)\r\n            at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n            at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n            at py4j.Gateway.invoke(Gateway.java:282)\r\n            at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n            at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n            at py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n            at java.lang.Thread.run(Thread.java:748)\r\n    Caused by: java.lang.ClassNotFoundException: org.tensorframes.ShapeDescription\r\n            at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n            at java.lang.ClassLoader.loadClass(ClassLoader.java:419)\r\n            at java.lang.ClassLoader.loadClass(ClassLoader.java:352)\r\n            ... 12 more\r\n\r\n```\r\nAny suggestions to solve this?", "comments": ["@tsharma82, This is not a tensorflow issue. Please try to post on respective [community](https://github.com/apache/spark/tree/master/python/pyspark).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 38441, "title": "[ROCm] Update tensorflow 2.1 release branch ROCm path to build with hipclang", "body": "This PR updates the Tensorflow 2.1 release branch to build with ROCm hipclang compiler.\r\n\r\n", "comments": ["cc @deven-amd @whchung @cheshire @chsigg @nvining-work", "Thank you for the PR. We will review and merge when we prepare a patch release from the branch."]}, {"number": 38440, "title": "[r2.2:CherryPick] Fix input size used for batch normalization.", "body": "Inputs_size (array_ops.size()) used to determine whether to use optional_get_next() API code path defaults to using int32 dtype. If input size is big enough this can lead to integer overflow and cause model to diverge.\n\nCorrect usage will be to use inputs.get_shape()[0] to get the batch size -- instead of using array_ops.size() which returns the number of elements in inputs tensor which can be arbitrarily large.\n\nPiperOrigin-RevId: 305823718\nChange-Id: Idc5660d80406fe233b162b73330c6fce4d5357b4", "comments": []}, {"number": 42799, "title": "[ko] Notebooks out of sync", "body": "Hello,\r\n\r\nPlease sync the ko notebooks to the source of truth notebooks using the nb_code_sync tool here( https://github.com/tensorflow/docs-l10n/blob/master/tools/nb_code_sync.py).\r\n\r\nCurrently, many of them are failing.", "comments": ["Adding [REVIEWERS](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/REVIEWERS): @rickiepark @cre8tor @choiuijin1125 @JKIsaacLee @NoelBird @wckim @eat-toast @jaketae \r\nPlease take a look.\r\n\r\n@yashk2810 I think the community could benefit from more explicit guidance. Which notebooks? And do you know why? It seems odd a bunch started failing since their haven't been any new merges this week.", "The builds have been failing for a long time. I am increasing the timeout for those, but for example `style_transfer` still has mentions of tensorflow.contrib.\r\n\r\n```\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n+ delete_or_move site/ko/tutorials/generative/style_transfer.ipynb\r\n```\r\n\r\nI can get you the list of notebooks that fail, but it would still be nice to sync atleast all the guides and tutorials directory since some of the code has been updated there.\r\n\r\nA notebook working is not indicative of the latest code being in them. It can continue to work due to TF's backward compatibility guarantees. ", "I will take a look at `style_transfer.ipynb` and file a PR today. @yashk2810 @lamberta Sorry if this is a basic question, but is there a systematized way of making sure that our code base is in sync? Reviewers can take a look once in a while, but I feel like doing so manually is ineffective and creates room for error.", "Yup, you can use the nb_code_sync tool.\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/tools/nb_code_sync.py\r\n", "Hi @jaketae \r\n\r\n> is there a systematized way of making sure that our code base is in sync\r\n\r\nNothing automated across the entire code base. We get notifications when a notebook breaks but, as @yashk2810 mentioned, it may still contain out-of-date code that happens to work.\r\n\r\n[nb_code_sync.py](https://github.com/tensorflow/docs-l10n/blob/master/tools/nb_code_sync.py) is useful for detecting when *code cells* are out of sync with the `en/` source. It's far from perfect, but has been used successfully by the community ([example](https://github.com/tensorflow/docs-l10n/pull/117)):\r\n\r\n```\r\n# Install deps:\r\n$ pip3 install -U absl-py\r\n\r\n$ ./tools/nb_code_sync.py --src=../docs/site/en/notebook.ipynb \\\r\n    ./site/<lang>/notebook.ipynb\r\n```\r\n\r\n(Check the help text at the top of the file for git usage to save translated comments, etc.)", "Thank you very much for the pointers! [nb_code_sync.py](https://github.com/tensorflow/docs-l10n/blob/master/tools/nb_code_sync.py) seems like a great handle little tool to better maintain the notebooks we have at the moment. I'll definitely take a look and try using it myself (perhaps starting with `style_transfer.ipynb`).", "These are the notebooks that failed:\r\n\r\nsite/ko/tutorials/images/segmentation.ipynb\r\nsite/ko/tutorials/generative/style_transfer.ipynb\r\nsite/ko/tutorials/distribute/save_and_load.ipynb\r\nsite/ko/tutorials/distribute/multi_worker_with_keras.ipynb\r\nsite/ko/tutorials/keras/save_and_load.ipynb\r\nsite/ko/swift/python_interoperability.ipynb\r\nsite/ko/guide/random_numbers.ipynb", "For the swift notebook, please save the outputs in it.", "@lamberta @yashk2810 Apologies for the delay, I got carried away by other things yesterday. I just ran the command\r\n\r\n```\r\npython ./tools/nb_code_sync.py site/ko/tutorials/generative/style_transfer.ipynb --site_root ../docs/site/en --src ../docs/site/en/tutorials/generative/style_transfer.ipynb\r\n```\r\n\r\nbut was returned with this error:\r\n\r\n```\r\nError: Notebooks must have same amount of code cells to sync between.\r\nPlease manually compare the source and destination notebooks.\r\n```\r\n\r\nSo it seems like the way to go about this would be to manually go back and forth to check if the notebooks are in sync. I can definitely do that, but before I begin engaging in some menial work, I was wondering if this would be the optimal way to navigate this issue.\r\n\r\nThank you again for the help!", "> For the swift notebook, please save the outputs in it.\r\n\r\n@rickiepark I'm not well-acquainted with how Swift notebooks are maintained. Should this be added as an instruction on README for community translators?", "Hi @jaketae \r\nYes, there is a note in README: \"S4TF \ub178\ud2b8\ubd81\uc740 \uaf2d \ucd9c\ub825 \uacb0\uacfc\uac00 \ud3ec\ud568\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4\" :)\r\nPlease check https://github.com/tensorflow/docs-l10n/tree/master/site/ko ", "@rickiepark Oops, my bad. I should have checked before I tagged you. My apologies, and thank you for the confirmation!", "Hi @jaketae \r\nYes, unfortunately the nb_code_sync tool assumes the same amount of code cells in both source and translation notebook. If they don't match, that probably indicates the translation is quite out of date.\r\nI'll think about better ways to automate this. I could make sure cell metadata is the same across both notebooks, but we haven't enforced that.", "For this issue, I send PRs tensorflow/docs-l10n#9, tensorflow/docs-l10n#197, tensorflow/docs-l10n#198, \r\n\r\nHi @jaketae, there are your commits in my PRs tensorflow/docs-l10n#197, tensorflow/docs-l10n#198\r\nSo these PRs may need your comment(@googlebot I consent.)\r\n\r\nThanks\r\n", "@rickiepark Just did, and I think all checks have passed. Thank you for letting me know! I'll also take a look at the PRs and review them if I can. ", "@rickiepark \r\n\r\nThese notebooks are failing:\r\n\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/distribute/multi_worker_with_keras.ipynb\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/text_classification_with_hub.ipynb\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/save_and_load.ipynb\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/images/segmentation.ipynb\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/swift/python_interoperability.ipynb\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/function.ipynb\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/migrate.ipynb\r\n\r\nPlease sync the code up with `en/`. The text update can be done in a separate PR so that the notebooks will start passing.", "Hi @yashk2810 , Thanks for letting me know that. :)\r\n\r\nmulti_worker_with_keras, save_and_load, python_interoperability is pending at tensorflow/docs-l10n#198 , but I check again if there are recent changes.\r\nAnd I also try to fix others ASAP! :)"]}, {"number": 38439, "title": "Allow passing custom image preprocessing config to the TFL eval tool.", "body": "At the moment the `ImageClassificationStage` of the TFLite evaluation tool ignores any existing image preprocessing parameters set on the `EvaluationStageConfig` and instead instantiates the `ImagePreprocessingStage` using hard-coded configuration.\r\n\r\nThis PR changes this behaviour to instead instantiate the `ImagePreprocessingStage` using existing image preprocessing parameters, if provided, and fall back to the old behaviour if not.", "comments": ["@AdamHillier can you please write some test cases around new changes ?", "> @AdamHillier can you please write some test cases around new changes ?\r\n\r\nThere are no existing tests for `ImageClassificationStage` at all; I'm very happy to create a test file and add tests specifically for this behaviour but I think writing tests for all of `ImageClassificationStage` is beyond the scope of this PR.\r\n\r\n*Edit*: It now makes sense to me why there are no existing tests for `ImageClassificationStage`, as it's just a wrapper that applies a `ImagePreprocessingStage` then a `TfliteInferenceStage` then (optionally) a `TopkAccuracyEvalStage`.\r\n\r\nAs these 'sub'-stage instances are private to `ImageClassificationStage` it's not easy to access them from the test, so I don't see a straightforward way to unit-test this behaviour?", "@multiverse-tf There seem to be a lot of MLIR build failures in those logs, how could they be related to this change?", "> @multiverse-tf There seem to be a lot of MLIR build failures in those logs, how could they be related to this change?\r\n\r\nSorry for the confusion. Those build failures are definitely not related to this change. It's just that TF overall have several global build tests across different platforms that will check every major component of TF to ensure a global buildable consistent state. A failure in one component, unfortunately, could block PR merges in other components. Could you check the status again tomorrow? I think the issue is going to be fixed asap.", "Looks like the build failures were fixed in b7c1d24656c61b39375635644dd3f168e9a811cb, so rebasing onto master should fix CI.", "> Looks like the build failures were fixed in [b7c1d24](https://github.com/tensorflow/tensorflow/commit/b7c1d24656c61b39375635644dd3f168e9a811cb), so rebasing onto master should fix CI.\r\n\r\nThanks, have rebased", "@multiverse-tf could we give the tests another go?"]}, {"number": 38438, "title": "shape_invariants \"None\" for all loop variables for tf 1.15.x", "body": "OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\nTensorFlow installed from : binary\r\n\r\nTensorFlow version : 1.15.0\r\n\r\nPython version: 3.7.3\r\n\r\nWhen I try below \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef tile_nd_ragged2(a, b):\r\n  # Need a sentinel, otherwise it's hard to give it the initial shape we need.\r\n  # We'll drop the sentinel at the end.\r\n  acc = tf.ragged.constant([[[]]], dtype=a.dtype)\r\n\r\n  # Work one row at a time...\r\n  for i1 in range(len(a.nested_row_lengths()[0])):    # Should be able to write `for a1, b1 in zip(a, b)` soon.\r\n    a1 = a[i1]\r\n    b1 = b[i1]\r\n    # If the components have variable length, we can't use a TensorArray anymore,\r\n    # so use a RaggedTensor instead.\r\n    acc1 = tf.ragged.constant([[]], dtype=a.dtype)\r\n    def loop_test(i2, _):\r\n      return i2 < tf.cast(a1.nested_row_lengths()[0][i1], tf.int32)\r\n    def loop_body(i2, acc1):\r\n      a2 = a1[i2]\r\n      b2 = b1[i2]\r\n      for _ in range(len(b2)):\r\n        acc1 = tf.concat([acc1, tf.expand_dims(a2, 0)], axis=0)\r\n      return i2 + 1, acc1\r\n\r\n    _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[None, tf.TensorShape([None, None])])\r\n    acc1 = acc1[1:]  # Drop the sentinel.\r\n    acc = tf.concat([acc, tf.expand_dims(acc1, 0)], axis=0)    # Add the row to the final result.\r\n\r\n  acc = acc[1:]  # Drop the sentinel.\r\n  return acc\r\n\r\n\r\nx = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])\r\ny = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])\r\nprint(x)\r\nprint(y)\r\nprint(tile_nd_ragged2(x, y))\r\n```\r\nI get below output  \r\n```\r\ninput \r\na = <tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2']]]>\r\nb = <tf.RaggedTensor [[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]]>\r\noutput = <tf.RaggedTensor [[[b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2', b'c3'], [b'c1', b'c2', b'c3']], [[b'd1'], [b'e1', b'e2'], [b'e1', b'e2'], [b'e1', b'e2']]]>\r\n```\r\n\r\nBut when i try to put `@tf.function` above the function and run exactly same way it gives below error \r\n\r\n ```\r\n File \"tf_3.py\", line 101, in <module>\r\n    print(tile_nd_ragged2(x, y))\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 449, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 392, in _initialize\r\n    *args, **kwds))\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 335, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    tf_3.py:88 tile_nd_ragged2  *\r\n        _, acc1 = tf.while_loop(loop_test, loop_body, [0, acc1],shape_invariants=[None, tf.TensorShape([None, None])])\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2675 while_loop\r\n        back_prop=back_prop)\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py:87 while_loop\r\n        list(shape_invariants), expand_composites=False)\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 map_structure\r\n        structure[0], [func(*x) for x in entries],\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py:536 <listcomp>\r\n        structure[0], [func(*x) for x in entries],\r\n    /<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:512 _shape_invariant_to_type_spec\r\n        % shape)\r\n\r\n    TypeError: Expected shape to be a TypeSpec or TensorShape, got None\r\n```\r\nOpening a feature request as per discussion in ticket - [https://github.com/tensorflow/tensorflow/issues/38375](https://github.com/tensorflow/tensorflow/issues/38375)\r\n\r\nFeature :\r\nsupport for \"None\" in shape_invariants for tf 1.15.x ", "comments": ["Was able to reproduce the issue. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/a2f01d78a141bb2d46ecf4664ba34772/38438.ipynb). Thanks!", "@gowthamkpr i would like to make this change ? if you agree can you please point me to the code ", "Note that this is already resolved at head, in TF 2.2 and tf-nightly. The only question is whether there will be a 1.16 release to include it.", "Update: since there will not be a TF 1.16 release, the only solution is to upgrade to TF 2.2 or newer.", "As mentioned above, this already works in TF 2.2.  But for either version of TensorFlow, this can probably be rewritten to be simpler & more efficient.  As far as I can tell, the basic operation of \"tile_nd_ragged2\" is:\r\n\r\n*given ragged tensors `a` and `b` with matching shapes in the first two dimensions (i.e., `a.row_lengths==b.row_lengths`), repeat each `a[i][j]` value `len(b[i][j])` times.*\r\n\r\nHere's an implementation of that without any loops:\r\n\r\n```\r\ndef tile_nd_ragged2(a, b):\r\n  indices = b.with_flat_values(b.values.value_rowids())\r\n  repeated = tf.gather(a.values, indices)\r\n  flattened_row_splits = tf.gather(repeated.values.row_splits, repeated.row_splits)\r\n  return tf.RaggedTensor.from_row_splits(repeated.values.values, flattened_row_splits)\r\n```\r\n\r\nA few other notes, in passing:\r\n\r\n* `len(a.nested_row_lengths()[0])` can be simplified to `a.nrows()`\r\n* In your original code, you can skip the sentinel if you use `acc = tf.ragged.constant([], ragged_rank=2, dtype=a.dtype)`.\r\n* In tf-nightly, you can use `tf.map_fn` with RaggedTensors.", "@edloper thank you very useful !"]}, {"number": 38437, "title": "xla_build CMakeLists.txt - multi platforms", "body": "Hello,\r\n\r\n0663149 added CMake support for the upcoming standalone XLA AOT build in TensorFlow 2.2.\r\n\r\nHowever some of the compile options defined in there seem to be for Linux only. When using cmake to build the `xla_aot_runtime_src` from the installed pip package and with the default CMakeLists.txt, I get the following behaviour:\r\n- Linux, everything works;\r\n- MacOS, build succeeds but with a warning of an unknown option (-Wno-deprecated-copy)\r\n- Windows with MSVC, it fails.\r\n\r\nIn all cases I have used tf-nightly, Python 3.7, pip, cpu only. Platform specific configs:\r\n- MacOS: cmake 3.17.0, AppleClang 11.0.3.11030032\r\n- Linux: cmake 3.10.2, GNU 7.5.0\r\n- Windows:  cmake 3.16.5, MSVC 19.25.28612.0\r\n\r\nWouldn't be better to have something along those lines?\r\n```\r\ncmake_minimum_required(VERSION 3.4.3)\r\n\r\nfile(GLOB_RECURSE TF_RUNTIME_SRC \"*.cc\")\r\nadd_library(tf_xla_runtime_objects OBJECT\r\n\t${TF_RUNTIME_SRC}\r\n)\r\n\r\ntarget_include_directories(tf_xla_runtime_objects PRIVATE ../include)\r\ntarget_compile_options(tf_xla_runtime_objects PRIVATE\r\n  -ftemplate-backtrace-limit=0\r\n  $<$<OR:$<CXX_COMPILER_ID:GNU>,$<CXX_COMPILER_ID:Clang>>:-Wno-ignored-attributes>  \r\n  $<$<CXX_COMPILER_ID:GNU>:-Wno-deprecated-copy>  \r\n  $<$<OR:$<CXX_COMPILER_ID:GNU>,$<CXX_COMPILER_ID:Clang>>:-Wno-cast-qual>  \r\n  $<$<OR:$<CXX_COMPILER_ID:GNU>,$<CXX_COMPILER_ID:Clang>>:-Wno-sign-compare>  \r\n)\r\n\r\nadd_library(tf_xla_runtime STATIC\r\n  $<TARGET_OBJECTS:tf_xla_runtime_objects>\r\n)\r\n```\r\n\r\nAlso, apart from managing the warning flags, do you have any view on shipping predefined MSVC compiler options required to make the build succeed? The build does eventually work, but the list of flags is quite lengthy and it's certainly not something that I would have dreamt of (I copied it from bazel).\r\n\r\nAs a side note, on all three platforms I have tried building the bazel target `//tensorflow/compiler/tf2xla:xla_compiled_cpu_runtime_standalone` and thanks to the fact that all options are pre-configured and managed by bazel the experience has been much smoother.\r\n\r\nBest Regards,\r\n\r\nMarco", "comments": ["@marcoadurno,\r\n\r\nHere is the [link](https://www.tensorflow.org/install/source_windows#tested_build_configurations) to find tested build configurations, which contains compatible version of MSVC and other tools while building Tensorflow from source on Windows.\r\n\r\nCan you try updating to `TF 2.6.0` using this [guide](https://www.tensorflow.org/install/source_windows) and let us know if the issue persists in newer version. Thanks!", "Sorry to jump in, but I think that the issue was related to build just the xla_aot_runtime (so not the whole Tensorflow source) with CMake (and not Bazel) in a Windows environment.\r\n\r\nAnd in this respect, I see that the CMakeLists.txt was not updated since this issue was created: https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/tools/pip_package/xla_build/CMakeLists.txt ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@battuzz,\r\n\r\nThanks for the clarification. We no longer support cmake builds and it is recommend to use bazel.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38437\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38437\">No</a>\n"]}, {"number": 38436, "title": "[2.2 cherrypick-request] Upgrade XNNPack to fix build issues on AArch64", "body": "This PR cherry picks some commits from `master` to upgrade XNNPack dependency on the 2.2 release branch.\r\nThis fixes the TFLite build issues on AArch64 as mentioned in #38400 and includes #37225 to support building on Windows. I couldn't find a workaround to this with less changes to the r2.2, or am I missing something?\r\n\r\nBuilding TFLite 2.1 on AArch64 with gcc (e.g. Raspberry Pi 4) was supported previously so it would be great if this would be the case for 2.2 as well.\r\n\r\nFixes #38400", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38436) for more info**.\n\n<!-- need_author_consent -->", "Looks like the CLA bot needs approval from @mattn since this cherry-pick request includes his fixes from #37225", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38436) for more info**.\n\n<!-- cla_yes -->", "Manually overriding CLA check since this is just a cherrypick"]}, {"number": 38435, "title": "tf.ragged - tf.tile like operation for each dimension ", "body": "**System information** \r\n\r\n     OS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\n    TensorFlow installed from : binary\r\n\r\n    TensorFlow version : 1.15.2\r\n\r\n    Python version: 3.7.3\r\n\r\n\r\nI have a1 and a2 ragged tensor \r\n\r\n`a1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'], [b'b1', b'b2', b'b3'], [b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2']]])`\r\n\r\n\r\n`b1 = tf.ragged.constant([[[b't1', b't2', b't3'], [b'u1', b'u2'], [b'v1', b'v2']], [[b'w1'], [b'x1', b'x2', b'x3']]])`\r\n\r\n\r\nI have a1 , b1 as above , i want c as below \r\n\r\n`c1 = tf.ragged.constant([[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']], [[b'd1'], [b'e1', b'e2'],[b'e1', b'e2'],[b'e1', b'e2']]])`\r\n\r\nie [b'a1', b'a2', b'a3'] is repeated equal to number of elements in  [b't1', b't2', b't3'] ie 3 so in c1 we will have [b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],\r\n\r\nsimilarly [b'b1', b'b2', b'b3'] is repeated equal to number of elements in  [b'u1', b'u2'] ie 2 so in c 1we will more have [b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],\r\n\r\nsimilarly [b'c1', b'c2','c3'] is repeated equal to number of elements in  [[b'v1', b'v2'] ie 2 so in c1 we will more have [b'c1', b'c2','c3'],[b'c1', b'c2','c3']\r\n\r\ntill here all will be past of first tensor  ie `[[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'a1', b'a2', b'a3'],[b'b1', b'b2', b'b3'],[b'b1', b'b2', b'b3'],[b'c1', b'c2','c3'],[b'c1', b'c2','c3']]` now for second ragged tensor \r\n\r\nsame process as above where [b'd1'] should be repeated 1 times as number of elements in [b'w1'] ..... \r\n\r\n i am using this to convert raw signal to feature as part of my savedModel . \r\n", "comments": ["We can closed this due to  https://github.com/tensorflow/tensorflow/issues/38375 ", "moving to closed status as per user", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38435\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38435\">No</a>\n"]}, {"number": 42798, "title": "[ru] Failing notebooks.", "body": "https://github.com/tensorflow/docs-l10n/blob/master/site/ru/guide/migrate.ipynb\r\n\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\n  import tensorflow.compat.v2 as tf\r\nexcept Exception:\r\n  pass\r\ntf.enable_v2_behavior()\r\n\r\n\r\nimport tensorflow_datasets as tfds\r\n------------------\r\n\r\n  File \"<ipython-input-2-affb36bccd73>\", line 2\r\n    except Exception:\r\n         ^\r\nSyntaxError: invalid syntax\r\n\r\nSyntaxError: invalid syntax (<ipython-input-2-affb36bccd73>, line 2)\r\n```\r\n\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ru/tutorials/keras/save_and_load.ipynb\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\nimport time\r\nsaved_model_path = \"./saved_models/{}\".format(int(time.time()))\r\n\r\ntf.keras.experimental.export_saved_model(model, saved_model_path)\r\nsaved_model_path\r\n------------------\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-20-9d5aff309515> in <module>\r\n      2 saved_model_path = \"./saved_models/{}\".format(int(time.time()))\r\n      3 \r\n----> 4 tf.keras.experimental.export_saved_model(model, saved_model_path)\r\n      5 saved_model_path\r\n\r\nAttributeError: module 'tensorflow_core.python.keras.api._v2.keras.experimental' has no attribute 'export_saved_model'\r\nAttributeError: module 'tensorflow_core.python.keras.api._v2.keras.experimental' has no attribute 'export_saved_model'\r\n```\r\n\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ru/tutorials/keras/text_classification_with_hub.ipynb\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\n# \u0420\u0430\u0437\u043e\u0431\u044c\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0432 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u0438 60% \u043d\u0430 40%, \u0438 \u0443 \u043d\u0430\u0441 \u0431\u0443\u0434\u0435\u0442 15,000 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432\r\n# \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, 10,000 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0438 25,000 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438.\r\ntrain_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\r\n\r\n(train_data, validation_data), test_data = tfds.load(\r\n    name=\"imdb_reviews\", \r\n    split=(train_validation_split, tfds.Split.TEST),\r\n    as_supervised=True)\r\n------------------\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-3-ecc6d6ac73cf> in <module>\r\n      6     name=\"imdb_reviews\",\r\n      7     split=(train_validation_split, tfds.Split.TEST),\r\n----> 8     as_supervised=True)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51     _check_required(fn, kwargs)\r\n---> 52     return fn(*args, **kwargs)\r\n     53 \r\n     54   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\n...\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_datasets/core/tfrecords_reader.py in _str_to_relative_instruction(spec)\r\n    354   res = _SUB_SPEC_RE.match(spec)\r\n    355   if not res:\r\n--> 356     raise AssertionError('Unrecognized instruction format: %s' % spec)\r\n    357   unit = '%' if res.group('from_pct') or res.group('to_pct') else 'abs'\r\n    358   return ReadInstruction(\r\n\r\nAssertionError: Unrecognized instruction format: NamedSplit('train')(tfds.percent[0:60])\r\nAssertionError: Unrecognized instruction format: NamedSplit('train')(tfds.percent[0:60])\r\n```\r\n\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ru/tutorials/load_data/text.ipynb\r\n```\r\nnbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\r\n------------------\r\ntrain_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\r\ntrain_data = train_data.padded_batch(BATCH_SIZE)\r\n\r\ntest_data = all_encoded_data.take(TAKE_SIZE)\r\ntest_data = test_data.padded_batch(BATCH_SIZE)\r\n------------------\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-be2cd799459e> in <module>\r\n      1 train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\r\n----> 2 train_data = train_data.padded_batch(BATCH_SIZE)\r\n      3 \r\n      4 test_data = all_encoded_data.take(TAKE_SIZE)\r\n      5 test_data = test_data.padded_batch(BATCH_SIZE)\r\n\r\nTypeError: padded_batch() missing 1 required positional argument: 'padded_shapes'\r\nTypeError: padded_batch() missing 1 required positional argument: 'padded_shapes'\r\n```\r\n", "comments": [">TypeError: padded_batch() missing 1 required positional argument: 'padded_shapes'\r\n\r\nTis will be fixed by 2.2, so can maybe just be ignored for now.", "Adding [REVIEWERS](https://github.com/tensorflow/docs-l10n/blob/master/site/ru/REVIEWERS): @0101011 @stabuev", "These files should be synced using our GitLocalize project: https://gitlocalize.com/tensorflow/docs-l10n\r\n\r\n* https://gitlocalize.com/repo/4592/ru/site/en-snapshot/guide/migrate.ipynb\r\n* https://gitlocalize.com/repo/4592/ru/site/en-snapshot/tutorials/keras/save_and_load.ipynb\r\n* https://gitlocalize.com/repo/4592/ru/site/en-snapshot/tutorials/keras/text_classification_with_hub.ipynb\r\n* https://gitlocalize.com/repo/4592/ru/site/en-snapshot/tutorials/load_data/text.ipynb\r\n\r\nBut this issue is kinda old and I'm working on a better notification/status system. Will close"]}, {"number": 38434, "title": "diag_part doesn't work for sub/super diagonals", "body": "**System information** \r\n- Have I written custom code: no, see https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part\r\n- OS Platform and Distribution: \r\n  - Windows 10, Anaconda Python 3.7.6, tensorflow 2.1.0\r\n  - Debian GNU/Linux 8.11 (jessie), Anaconda Python 3.7.3, tensorflow 2.0.0\r\n\r\n**Describe the current behavior**\r\nIf I ran the example code:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ninput = np.array([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 8, 7, 6]], [[5, 4, 3, 2], [1, 2, 3, 4], [5, 6, 7, 8]]])\r\nprint(tf.linalg.diag_part(input))\r\nprint(tf.linalg.diag_part(input, k=1))\r\n```\r\n***result***\r\n```\r\n[[1 6 7]\r\n [5 2 7]]\r\n[[1 6 7]\r\n [5 2 7]]\r\n```\r\n***expected***\r\n```\r\n[[1 6 7]\r\n [5 2 7]]\r\n[[2, 7, 6],\r\n [4, 3, 8]]\r\n```\r\nI think that the keyword arguments are just neglected.", "comments": ["@gaebor \r\n\r\nCan you please try with latest TF version 2.2.0-rc2 (`!pip install tensorflow==2.2-rc2`). I am not seeing any issue with 2.2.0-rc2 version. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2e70592203a49059e84011a6358a4f77/untitled768.ipynb). Thanks!", "Yeah, it works with that!", "@gaebor \r\n\r\nPlease, close this thread as your issue got resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38434\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38434\">No</a>\n"]}, {"number": 38433, "title": "No specific error when import keras", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10 Enterprise\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: \r\n```\r\n# packages in environment at C:\\ProgramData\\Anaconda3:\r\n# Name                    Version                   Build  Channel\r\ntensorflow                2.1.0                    pypi_0    pypi\r\ntensorflow-estimator      2.1.0                    pypi_0    pypi\r\n```\r\n\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nCan not import tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. use `pip install tensorflow` to install it and report success\r\n2. use `from tensorflow import keras` and run into error below\r\n\r\nI have no idea what is happening, no specific error code\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-1d80d955c0d6>\", line 5, in <module>\r\n    from tensorflow import keras\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-1d80d955c0d6>\", line 5, in <module>\r\n    from tensorflow import keras\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-1d80d955c0d6>\", line 5, in <module>\r\n    from tensorflow import keras\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3348, in run_code\r\n    self.showtraceback(running_compiled_code=True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\r\n    value, tb, tb_offset=tb_offset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1418, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1318, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1186, in structured_traceback\r\n    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\nTypeError: can only concatenate str (not \"list\") to str\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-1d80d955c0d6>\", line 5, in <module>\r\n    from tensorflow import keras\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3348, in run_code\r\n    self.showtraceback(running_compiled_code=True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\r\n    value, tb, tb_offset=tb_offset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1418, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1318, in structured_traceback\r\n    self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1186, in structured_traceback\r\n    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\nTypeError: can only concatenate str (not \"list\") to str\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'TypeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\r\n   2043                         # in the engines. This should return a list of strings.\r\n-> 2044                         stb = value._render_traceback_()\r\n   2045                     except Exception:\r\n\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self, code_obj, result, async_)\r\n   3346             if result is not None:\r\n   3347                 result.error_in_exec = sys.exc_info()[1]\r\n-> 3348             self.showtraceback(running_compiled_code=True)\r\n   3349         else:\r\n   3350             outflag = False\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\r\n   2045                     except Exception:\r\n   2046                         stb = self.InteractiveTB.structured_traceback(etype,\r\n-> 2047                                             value, tb, tb_offset=tb_offset)\r\n   2048 \r\n   2049                     self._showtraceback(etype, value, stb)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n   1416             self.tb = tb\r\n   1417         return FormattedTB.structured_traceback(\r\n-> 1418             self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n   1419 \r\n   1420 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, value, tb, tb_offset, number_of_lines_of_context)\r\n   1316             # Verbose modes need a full traceback\r\n   1317             return VerboseTB.structured_traceback(\r\n-> 1318                 self, etype, value, tb, tb_offset, number_of_lines_of_context\r\n   1319             )\r\n   1320         elif mode == 'Minimal':\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py in structured_traceback(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\r\n   1184         exception = self.get_parts_of_chained_exception(evalue)\r\n   1185         if exception:\r\n-> 1186             formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\r\n   1187             etype, evalue, etb = exception\r\n   1188         else:\r\n\r\nTypeError: can only concatenate str (not \"list\") to str\r\n```", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "install [hardware requirments(visual c++)](https://www.tensorflow.org/install/pip#hardware-requirements) solve my problem", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38433\">No</a>\n"]}, {"number": 38432, "title": "Converting retrained ssd_resnet50_v1 object detection model to TFLite for use on Coral USB Accelerator", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **tf-nightly installed through pip**\r\n- TensorFlow version (or github SHA if from source): **2.2.0.dev20200401**\r\n- **Nvidia GeForce GTX 1050 Ti Driver Version: 440.64.00**\r\n- **Cuda compilation tools, release 9.0, V9.0.176**\r\n- **CUDNN_MAJOR 7 CUDNN_MINOR 0 CUDNN_PATCHLEVEL 5**\r\n- **Edge TPU Compiler version 2.1.302470888**\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nTensorFlow Lite currently doesn't support control flow ops: Merge, Switch. \r\nWe are working on supporting control flow ops, \r\nplease see github issue at https://github.com/tensorflow/tensorflow/issues/28485. \r\nSome of the operators in the model are not supported by the \r\nstandard TensorFlow Lite runtime and are not recognized by TensorFlow. \r\nIf you have a custom implementation \r\nfor them you can disable this error with --allow_custom_ops, or by setting \r\nallow_custom_ops=True when calling tf.lite.TFLiteConverter(). \r\nHere is a list of builtin operators you \r\nare using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, \r\nGATHER, GREATER, GREATER_EQUAL,\r\nLESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, \r\nRANGE, RESHAPE, RESIZE_BILINEAR, SELECT, \r\nSHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, \r\nWHERE.Here is a list of operators for which you will need custom implementations: \r\nDecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, Substr.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\n\r\nBATCH_SIZE = 32\r\nIMG_HEIGHT = 320\r\nIMG_WIDTH = 320\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')\r\n\r\nlist_ds_Dianthus = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Dianthus''*/*/*'))\r\nlist_ds_DustyMiller = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/DustyMiller''*/*/*'))\r\nlist_ds_Pansy = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Pansy''*/*/*'))\r\nlist_all = list_ds_Dianthus.concatenate(list_ds_DustyMiller).concatenate(list_ds_Pansy)\r\n#len(list(list_all))\r\n\r\ndef get_label(file_path):\r\n  parts = tf.strings.split(file_path, '/')\r\n  class_name = tf.strings.split(parts[-3], '1')\r\n  class_name = tf.strings.split(class_name, '2')\r\n  class_name = tf.strings.split(class_name, '3')\r\n  class_name = tf.strings.split(class_name, '4')\r\n  return class_name\r\n\r\ndef decode_img(img):\r\n\timg = tf.image.decode_jpeg(img, channels=3) #color images\r\n\timg = tf.image.convert_image_dtype(img, tf.float32) \r\n\t#convert unit8 tensor to floats in the [0,1]range\r\n\treturn tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \r\n\r\ndef process_path(file_path):\r\n  label = get_label(file_path)\r\n  img = tf.io.read_file(file_path)\r\n  img = decode_img(img)\r\n  return img, label\r\n\r\nnum_calibration_steps= 1000\r\n\r\ndef representative_dataset_gen():\r\n\tfor _ in range(num_calibration_steps):\r\n\t# Get sample input data as a numpy array in a method of your choosing.\r\n\t\timage = list_all.take(1)\r\n\t\tyield [decode_img(image)]\r\n\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([])\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = False\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\n\r\nopen(\"/home/afayed/Desktop/output_tflite_graph.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n#### Model ckpt 5 used [here](https://drive.google.com/file/d/1ssqvHDMh1Knh1QU05HjBLdUmkNihi_Ks/view?usp=sharing) as a .tar.\r\n#### Frozen graph generated [here](https://drive.google.com/file/d/1dWspjmCtX_qscYH5jRFwR0Niw9HdY2Pb/view?usp=sharing)\r\n#### saved_model.pb used [here](https://drive.google.com/file/d/10vPKG_49a8Smlv_CRhI5a0XH4Iskd6gZ/view?usp=sharing)\r\n\r\n\r\n\r\n**Any other info / logs**\r\n# Detailed explanation\r\n**I am trying to use custom training data to retrain 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03' (model obtained from model zoo [here](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz) ) following [Google's Retrain an object detection model tutorial](hhttps://coral.ai/docs/edgetpu/retrain-detection/#prerequisites)**\r\n \r\n**I retrained it on custom training data using TensorFlow with GPU devel version 1.12.0-rc2 on a docker container built with this [Dockerfile](https://drive.google.com/open?id=1q7CTyAweZCRSsYcwOODkDSgQkWabyYac). \r\nI also tried retraining with TensorFlow with GPU version 1.15.2. The pipeline config file I am using is available [here](https://drive.google.com/file/d/1oGPdg5wK6tQOLVkMCcJz5f-pMLMVzqGf/view?usp=sharing). \r\nI have made sure I used a fixed_shape_resizer and included \"max_number_of_boxes: 200\" in train_input_reader and eval_input_reader and all of that. \r\nMy image dimensions as found in pipeline_v2.config are 320,320. \r\nI found the new Experimental Converter [here](https://groups.google.com/a/tensorflow.org/forum/#!topic/tflite/C7Ag0sUrLYg)**\r\n\r\n## **So, my process is as such:**\r\n\r\n**I start the docker container and start retraining from ckpt 0 that is given by downloading the model from the model zoo:**\r\n\r\n```\r\nsudo docker run --gpus all --name edgetpu-detect \\\r\n--rm -it --privileged -p 6006:6006 \\\r\n--mount type=bind,src=${DETECT_DIR},dst=/tensorflow/models/research/learn_pet \\\r\ndetect-tutorial\r\n```\r\n\r\n\r\n**For the sake of testing lets only retrain 5 steps (I've tried up to 20,000 giving the same results):**\r\n\r\n```\r\n./retrain_detection_model.sh --num_training_steps 5 --num_eval_steps 100\r\n```\r\n\r\nretrain_detection_model.sh found [here](https://drive.google.com/open?id=1mMErPY2j68cNGDS_pmh9gbdAKOAvWmLB)\r\n\r\n\r\n**I then use 'saved_model_cli.py' to get any info I need about the model. Text file generated [here]**(https://drive.google.com/open?id=1nDNSpEyjExg-_L_wpaDJaNlbVrRvfbp8):\r\n\r\n```\r\npython /tensorflow/tensorflow/python/tools/saved_model_cli.py show \r\n--dir /tensorflow/models/research/learn_pet/train/export/Servo/1586336368/ \r\n--all > /tensorflow/models/research/learn_pet/train/saved_model_cli_output.txt\r\n```\r\n\r\nI then export a frozen graph using export_tflite_ssd_graph.py (unedited) both through 1.12.0-rc2 and 1.15.2 *code shown here is using TF 1.15.2*:\r\n\r\n```\r\npython object_detection/export_tflite_ssd_graph.py \r\n--pipeline_config_path=\"/tensorflow/models/research/learn_pet/ckpt/pipeline_v2.config\" \r\n--trained_checkpoint_prefix=\"/tensorflow/models/research/learn_pet/train/model.ckpt-5\" \r\n--output_directory=\"/tensorflow/models/research/learn_pet/models\" \r\n```\r\n**And then I use the converter invocation (stated up above + its output) that causes this issue.**\r\n\r\n\r\n\r\n*I limit memory growth using*\r\n`physical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')`\r\n\r\n\r\n**Please help, I am also trying to do this with faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28 and that is where I found about the Experimental Converter working for RCNNs. My goal is to run either of these models on the Coral USB Accelerator after retraining with my own custom training data. I did not edit any scripts generating either of these models. Thanks in advanced**\r\n\r\n", "comments": ["SSD models don't convert directly to TFLite. You need an intermediate step to export a graph that uses our custom NMS op to speed up post-processing. Please take a look at [these instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38432\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38432\">No</a>\n", "@srjoglekar246 Apologies formatting typo in my original comment caused a step of my process to not show. I have updated it, but I am restating it here for your convenience *(following executed in TF 1.15.2 GPU).*\r\n**I do run:**\r\n```\r\npython object_detection/export_tflite_ssd_graph.py \r\n--pipeline_config_path=\"/tensorflow/models/research/learn_pet/ckpt/pipeline_v2.config\" \r\n--trained_checkpoint_prefix=\"/tensorflow/models/research/learn_pet/train/model.ckpt-5\" \r\n--output_directory=\"/tensorflow/models/research/learn_pet/models\" \r\n```\r\nAnd then I use the generated `tflite_graph.pb` as an input graph_def_file to TFLite converter following the Coral retrain object detection [tutorial](https://coral.ai/docs/edgetpu/retrain-detection/):\r\n```\r\ntflite_convert  --output_file=\"/tensorflow/models/research/learn_pet/models/output_tflite_graph.tflite\"   \r\n--graph_def_file=\"/tensorflow/models/research/learn_pet/models/tflite_graph.pb\"   \r\n--inference_type=QUANTIZED_UINT8  --input_arrays='normalized_input_image_tensor'   \r\n--output_arrays=\"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\" --mean_values=128 --std_dev_values=128 \r\n--input_shapes=1,320,320,3  --change_concat_input_ranges=false \r\n--allow_nudging_weights_to_use_fast_gemm_kernel=true --allow_custom_ops \r\n```\r\nAt which point I get error:\r\n```\r\nF tensorflow/lite/toco/tooling_util.cc:1728] Array FeatureExtractor/resnet_v1_50/fpn/top_down/nearest_neighbor_upsampling/stack, \r\nwhich is an input to the Pack operator producing the output array \r\nFeatureExtractor/resnet_v1_50/fpn/top_down/nearest_neighbor_upsampling/stack_1, \r\nis lacking min/max data, which is necessary for quantization. If accuracy matters, \r\neither target a non-quantized output format, or run quantized training with your model \r\nfrom a floating point checkpoint to change the input graph to contain min/max information. \r\nIf you don't care about accuracy, you can pass --default_ranges_min= and \r\n--default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted\r\n```\r\nIf I add the following to the above tflite_convert command:\r\n```\r\n--default_ranges_min=0 --default_ranges_max=6\r\n```\r\nIt works but then I get this error when using the Edgetpu Compiler for the final step to run inference on the Coral:\r\n```\r\nEdge TPU Compiler version 2.1.302470888\r\nERROR: :79 input->params.zero_point != output->params.zero_point (110 != 0)\r\nERROR: Node number 77 (PACK) failed to prepare.\r\n\r\nInternal compiler error. Aborting!\r\n```\r\n**And that is the point where I wrote the above script *(stated as standalone code to reproduce issue in original comment)***", "By the way, while following the [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) you shared, running this command *(in TF 1.12.0rc2 GPU docker image):*\r\n```\r\nroot@d97ce87cb12f:/tensorflow/models/research# bazel run -c opt //tensorflow/tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/output_tflite_graph.tflite --input_shapes=1,320,320,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n```\r\nWeirdly gives me this error:\r\n```\r\nERROR: Skipping '//tensorflow/tensorflow/contrib/lite/toco:toco': no such package \r\n'tensorflow/tensorflow/contrib/lite/toco': BUILD file not found on package path\r\n```\r\nWhich is strange since tflite_converter is working on the same docker image. I believe tflite_convert uses toco in the backend doesn't it?", "@adfayed Are you using `2.2.0.dev20200401` or `TF1.12`?. There is no `contrib` in `TF2.x`. mixing `TF1.x` and `TF2.x` will throw errors as there is a compatibility issue. Thanks!", "@jvishnuvardhan No no, I am using TF 1.15.2 GPU for all of this (retraining, generating frozen graph, and attempting conversion to TFLite). I tried the latest TF nightly for the new experimental TF Lite converter added functionality for RCNNs but with no luck. \r\n\r\nCan you please reply to my previous comment before that? That is the important one :)\r\nRunning tflite_convert as per the detect tutorial on TF1.15.2 on an SSD network (albeit ResNet instead of Mobilenet) gives me that error. I do not have min/max data even though my config file (linked above) includes the graph_rewrite options. After you answer that question please, my second question would be, can I use Faster RCNN with the new experimental TFlite converter? If so how?", "It doesn't seem to me that the model is quantized. This might be why the convertor is complaining about min/max data etc.\r\n\r\nFor float models, the params should look something like this:\r\n\r\n```\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=FLOAT \\\r\n--allow_custom_ops\r\n```", "@srjoglekar246 Yes! If I run tflite_convert with the above command (i.e inference_type FLOAT instead of QUANTIZED_INT8) it will convert, but then the EdgeTPU Compiler errs as such:\r\n```\r\nEdge TPU Compiler version 2.1.302470888\r\nInvalid model: /home/afayed/coral/detection_ag/models/output_tflite_graph.tflite\r\nModel not quantized\r\n```\r\nI am trying to make a quantized model to run on the Coral. Isn't quantized *re*-training done by adding this snippet to the pipeline.config file?\r\n```\r\ngraph_rewriter {\r\n  quantization {\r\n    delay: 0\r\n    weight_bits: 8\r\n    activation_bits: 8\r\n  }\r\n}\r\n```\r\nMy pipeline.config looks like this:\r\n```\r\n# SSD Resnet 50\r\n# Configured for MSCOCO Dataset.\r\n# Users should configure the fine_tune_checkpoint field in the train config as\r\n# well as the label_map_path and input_path fields in the train_input_reader and\r\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n# should be configured.\r\n\r\nmodel {\r\n  ssd {\r\n    num_classes: 3\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 320\r\n        width: 320\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_resnet50_v1_fpn\"\r\n      depth_multiplier: 1.0\r\n      min_depth: 16\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.000399999989895\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.0299999993294\r\n          }\r\n        }\r\n        activation: RELU_6\r\n        batch_norm {\r\n          decay: 0.996999979019\r\n          scale: true\r\n          epsilon: 0.0010000000475\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.000399999989895\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.00999999977648\r\n            }\r\n          }\r\n          activation: RELU_6\r\n          batch_norm {\r\n            decay: 0.996999979019\r\n            scale: true\r\n            epsilon: 0.0010000000475\r\n          }\r\n        }\r\n        depth: 256\r\n        num_layers_before_predictor: 4\r\n        kernel_size: 3\r\n        class_prediction_bias_init: -4.59999990463\r\n      }\r\n    }\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        scales_per_octave: 2\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.300000011921\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          gamma: 2.0\r\n          alpha: 0.25\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    encode_background_as_zeros: true\r\n    normalize_loc_loss_by_codesize: true\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 1\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n    random_pixel_value_scale {\r\n    }\r\n    random_vertical_flip {\r\n    }\r\n    random_patch_gaussian {\r\n    }\r\n    random_jitter_boxes {\r\n    }\r\n    random_adjust_brightness {\r\n    }\r\n    random_adjust_contrast {\r\n    }\r\n    random_adjust_hue {\r\n    }\r\n    random_adjust_saturation {\r\n    }\r\n  }\r\n  sync_replicas: true\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.003\r\n          schedule {\r\n            step: 900000\r\n            learning_rate: 0.00003\r\n          }\r\n          schedule {\r\n            step: 1200000\r\n            learning_rate: 0.000003\r\n          }      \r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"/tensorflow/models/research/learn_pet/ckpt/model.ckpt\"\r\n\r\n  # Note: The below line limits the training process to 200K steps, which we\r\n  # empirically found to be sufficient enough to train the COCO dataset. This\r\n  # effectively bypasses the learning rate schedule (the learning rate will\r\n  # never decay). Remove the below line to train indefinitely.\r\n\r\n  num_steps: 25000\r\n  from_detection_checkpoint: true\r\n  load_all_detection_checkpoint_vars: true\r\n  startup_delay_steps: 0.0\r\n  replicas_to_aggregate: 8\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"/tensorflow/models/research/learn_pet/ag/ag_label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"/tensorflow/models/research/learn_pet/ag/train_dataset.record\"\r\n  }\r\n  max_number_of_boxes: 200\r\n}\r\neval_config {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  num_examples: 400\r\n}\r\neval_input_reader {\r\n  label_map_path: \"/tensorflow/models/research/learn_pet/ag/ag_label_map.pbtxt\"\r\n  shuffle: true\r\n  num_readers: 1\r\n  tf_record_input_reader {\r\n    input_path: \"/tensorflow/models/research/learn_pet/ag/validate_dataset.record\"\r\n  }\r\n  max_number_of_boxes: 200\r\n}\r\ngraph_rewriter {\r\n  quantization {\r\n    delay: 0\r\n    weight_bits: 8\r\n    activation_bits: 8\r\n  }\r\n}\r\n```", "@srjoglekar246 Hello? Can you please reply? I believe my model _is quantized_.", "Does the floating point model run on CPU, maybe with our [benchmark_model tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)? (EdgeTPU may not support floating-point inference, but atleast CPU should give a sanity check)\r\n\r\nTBH, I am not very familiar with the object detection config APIs. But I am not sure that you can re-train with quantization only for fine-tuning.\r\n\r\nCould you try quantized re-training with an already-quantized model from the detection zoo, such as ssd_mobilenet_v2_quantized_coco?", "Any update about this issue? Same problem here.", "Also, looking for some help on this and retraining object detection for the Coral Dev Board.", "I believe this problems occurs when training with SIGMOID as a score converter for classes > 2. Im facing a similar issue, but it can be circumvented by using SOFTMAX. \r\n\r\n@srjoglekar246 \r\n\r\n> Could you try quantized re-training with an already-quantized model from the detection zoo, such as ssd_mobilenet_v2_quantized_coco?\r\n\r\nI did that (Or i used the ssd_mobilenet_edgetpu_coco) and thats when i faced this error when i tried to run inference witht the .tflite model. I was training with SIGMOID as the score converter for classes > 2.\r\n\r\n> RuntimeError: tensorflow/lite/kernels/kernel_util.cc:129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 108 (CONV_2D) failed to prepare.\r\n\r\nSo there must be something with the post-processing op i imagine", "@liufengdb I vaguely remember us encountering the issue @NicholaiStaalung mentions above. Do you know what the issue is?", "I was facing the same issue:\r\n```\r\nArray FeatureExtractor/MobilenetV1/fpn/top_down/nearest_neighbor_upsampling/stack, which is an input to the Pack operator producing the output array\r\nFeatureExtractor/MobilenetV1/fpn/top_down/nearest_neighbor_upsampling/stack_1, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantize\r\nd training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy\r\nexperimentation.\r\nAborted (core dumped)\r\n```\r\n\r\nIn my case the edge device didn't support nearest neighbor upsampling so I changed to bilinear upsampling and it worked fine. Made the change here:\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/models/feature_map_generators.py#L716\r\nchanged tf.image.resize_nearest_neighbor to tf.image.resize_bilinear and it worked just fine.\r\n", "> @srjoglekar246 Hello? Can you please reply? I believe my model _is quantized_.\r\n\r\nhave you solved your issue? I am facing the same :/"]}, {"number": 38431, "title": "Converting retrained ssd_resnet50_v1 object detection model to TFLite for use on Coral USB Accelerator", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **tf-nightly installed through pip**\r\n- TensorFlow version (or github SHA if from source): **2.2.0.dev20200401**\r\n- **Nvidia GeForce GTX 1050 Ti Driver Version: 440.64.00**\r\n- **Cuda compilation tools, release 9.0, V9.0.176**\r\n- **CUDNN_MAJOR 7 CUDNN_MINOR 0 CUDNN_PATCHLEVEL 5**\r\n- **Edge TPU Compiler version 2.1.302470888**\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nBATCH_SIZE = 32\r\nIMG_HEIGHT = 320\r\nIMG_WIDTH = 320\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')\r\n\r\nlist_ds_Dianthus = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Dianthus''*/*/*'))\r\nlist_ds_DustyMiller = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/DustyMiller''*/*/*'))\r\nlist_ds_Pansy = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Pansy''*/*/*'))\r\nlist_all = list_ds_Dianthus.concatenate(list_ds_DustyMiller).concatenate(list_ds_Pansy)\r\n#len(list(list_all))\r\n\r\ndef get_label(file_path):\r\n  parts = tf.strings.split(file_path, '/')\r\n  class_name = tf.strings.split(parts[-3], '1')\r\n  class_name = tf.strings.split(class_name, '2')\r\n  class_name = tf.strings.split(class_name, '3')\r\n  class_name = tf.strings.split(class_name, '4')\r\n  return class_name\r\n\r\ndef decode_img(img):\r\n\timg = tf.image.decode_jpeg(img, channels=3) #color images\r\n\timg = tf.image.convert_image_dtype(img, tf.float32) \r\n\t#convert unit8 tensor to floats in the [0,1]range\r\n\treturn tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \r\n\r\ndef process_path(file_path):\r\n  label = get_label(file_path)\r\n  img = tf.io.read_file(file_path)\r\n  img = decode_img(img)\r\n  return img, label\r\n\r\nnum_calibration_steps= 1000\r\n\r\ndef representative_dataset_gen():\r\n\tfor _ in range(num_calibration_steps):\r\n\t# Get sample input data as a numpy array in a method of your choosing.\r\n\t\timage = list_all.take(1)\r\n\t\tyield [decode_img(image)]\r\n\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([])\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = False\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\n\r\nopen(\"/home/afayed/Desktop/output_tflite_graph.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTensorFlow Lite currently doesn't support control flow ops: Merge, Switch. \r\nWe are working on supporting control flow ops, \r\nplease see github issue at https://github.com/tensorflow/tensorflow/issues/28485. \r\nSome of the operators in the model are not supported by the \r\nstandard TensorFlow Lite runtime and are not recognized by TensorFlow. \r\nIf you have a custom implementation \r\nfor them you can disable this error with --allow_custom_ops, or by setting \r\nallow_custom_ops=True when calling tf.lite.TFLiteConverter(). \r\nHere is a list of builtin operators you \r\nare using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, \r\nGATHER, GREATER, GREATER_EQUAL,\r\nLESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, \r\nRANGE, RESHAPE, RESIZE_BILINEAR, SELECT, \r\nSHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, \r\nWHERE.Here is a list of operators for which you will need custom implementations: \r\nDecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, Substr.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n#### Model ckpt 5 used [here](https://drive.google.com/file/d/1ssqvHDMh1Knh1QU05HjBLdUmkNihi_Ks/view?usp=sharing) as a .tar.\r\n#### Frozen graph generated [here](https://drive.google.com/file/d/1dWspjmCtX_qscYH5jRFwR0Niw9HdY2Pb/view?usp=sharing)\r\n#### saved_model.pb used [here](https://drive.google.com/file/d/10vPKG_49a8Smlv_CRhI5a0XH4Iskd6gZ/view?usp=sharing)\r\n\r\n**Failure details**\r\nConversion not successful... \r\n\r\n**Any other info / logs**\r\n# Detailed explanation\r\n**I am trying to use custom training data to retrain 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03' (model obtained from model zoo [here](http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz) ) following [Google's Retrain an object detection model tutorial](hhttps://coral.ai/docs/edgetpu/retrain-detection/#prerequisites)**\r\n \r\n**I retrained it on custom training data using TensorFlow with GPU devel version 1.12.0-rc2 on a docker container built with this [Dockerfile](https://drive.google.com/open?id=1q7CTyAweZCRSsYcwOODkDSgQkWabyYac). \r\nI also tried retraining with TensorFlow with GPU version 1.15.2. The pipeline config file I am using is available [here](https://drive.google.com/file/d/1oGPdg5wK6tQOLVkMCcJz5f-pMLMVzqGf/view?usp=sharing). \r\nI have made sure I used a fixed_shape_resizer and included \"max_number_of_boxes: 200\" in train_input_reader and eval_input_reader and all of that. \r\nMy image dimensions as found in pipeline_v2.config are 320,320. \r\nI found the new Experimental Converter [here](https://groups.google.com/a/tensorflow.org/forum/#!topic/tflite/C7Ag0sUrLYg)**\r\n\r\n## **So, my process is as such:**\r\n\r\n**I start the docker container and start retraining from ckpt 0 that is given by downloading the model from the model zoo:**\r\n\r\n```\r\nsudo docker run --gpus all --name edgetpu-detect \\\r\n--rm -it --privileged -p 6006:6006 \\\r\n--mount type=bind,src=${DETECT_DIR},dst=/tensorflow/models/research/learn_pet \\\r\ndetect-tutorial\r\n```\r\n\r\n\r\n**For the sake of testing lets only retrain 5 steps (I've tried up to 20,000 giving the same results):**\r\n\r\n```\r\n./retrain_detection_model.sh --num_training_steps 5 --num_eval_steps 100\r\n```\r\n\r\nretrain_detection_model.sh found [here](https://drive.google.com/open?id=1mMErPY2j68cNGDS_pmh9gbdAKOAvWmLB)\r\n\r\n\r\n**I then use 'saved_model_cli.py' to get any info I need about the model. Text file generated [here]**(https://drive.google.com/open?id=1nDNSpEyjExg-_L_wpaDJaNlbVrRvfbp8):\r\n\r\n```\r\npython /tensorflow/tensorflow/python/tools/saved_model_cli.py show --dir /tensorflow/models/research/learn_pet/train/export/Servo/1586336368/ --all > /tensorflow/models/research/learn_pet/train/saved_model_cli_output.txt```\r\n```\r\n\r\nI then export a frozen graph using export_tflite_ssd_graph.py (unedited) both through 1.12.0-rc2 and 1.15.2 *code shown here is using TF 1.15.2*:\r\n\r\n```\r\npython object_detection/export_tflite_ssd_graph.py --pipeline_config_path=\"/tensorflow/models/research/learn_pet/ckpt/pipeline_v2.config\" --trained_checkpoint_prefix=\"/tensorflow/models/research/learn_pet/train/model.ckpt-5\" --output_directory=\"/tensorflow/models/research/learn_pet/models\" \r\n```\r\n**Output:**\r\n```\r\nroot@d97ce87cb12f:/tensorflow/models/research# python object_detection/export_tflite_ssd_graph.py --pipeline_config_path=\"/tensorflow/learn_pet/ckpt/pipeline_v2.config\" --trained_checkpoint_prefix=\"/tensorflow/learn_pet/train/model.ckpt-5\" --output_directory=\"/tensorflow/learn_pet/models\"\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From object_detection/export_tflite_ssd_graph.py:143: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nWARNING:tensorflow:From object_detection/export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nW0410 12:39:17.462405 140155277027136 module_wrapper.py:139] From object_detection/export_tflite_ssd_graph.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:193: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0410 12:39:17.465477 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:193: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nW0410 12:39:17.465742 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:237: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nW0410 12:39:17.468422 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nW0410 12:39:17.471358 140155277027136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\r\n\r\nW0410 12:39:18.826324 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\r\n\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/predictors/convolutional_box_predictor.py:380: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\r\n\r\nW0410 12:39:19.233271 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/predictors/convolutional_box_predictor.py:380: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\r\n\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\nW0410 12:39:19.890158 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:52: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2020-04-10 12:39:19.891180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 12:39:19.908694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.909025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-04-10 12:39:19.909296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-10 12:39:19.910343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-10 12:39:19.911197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-10 12:39:19.911425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-10 12:39:19.912640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-10 12:39:19.913612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-10 12:39:19.916425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 12:39:19.916589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.916980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.917217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\r\n2020-04-10 12:39:19.917530: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 12:39:19.941192: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 12:39:19.941657: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efacc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 12:39:19.941674: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 12:39:19.987579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.987958: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7905220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 12:39:19.987995: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 12:39:19.988151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.988386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-04-10 12:39:19.988425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-10 12:39:19.988441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-10 12:39:19.988454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-10 12:39:19.988469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-10 12:39:19.988485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-10 12:39:19.988499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-10 12:39:19.988516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 12:39:19.988588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.988915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.989197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\r\n2020-04-10 12:39:19.989246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-10 12:39:19.989760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 12:39:19.989773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \r\n2020-04-10 12:39:19.989782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \r\n2020-04-10 12:39:19.989861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.990093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:19.990307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2888 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:267: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW0410 12:39:20.492587 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:267: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nW0410 12:39:20.495462 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/builders/graph_rewriter_builder.py:41: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.895448 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.895938 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.896350 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.896607 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.896919 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.897162 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.897400 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.897668 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.897969 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.898177 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.898397 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.898632 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.898874 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.899109 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.899347 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.899583 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.899877 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.900119 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.900363 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.900600 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.900838 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.901076 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.901296 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.901502 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.901758 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.901966 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.902171 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.902374 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.902588 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.902798 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.903002 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.903205 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.903466 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.903675 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.903881 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.904086 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.904299 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_0/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.904532 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_1/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.904742 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_2/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nI0410 12:39:20.904950 140155277027136 quantize.py:588] Skipping quantizing WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_3/Identity, because it is the output of a conv/fc followed by a identity, feeding a fused batch norm.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.949430 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.949680 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.964820 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.965119 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.980214 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.980386 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.996045 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:20.996264 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.011528 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.011718 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.027328 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.027498 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.044524 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.044679 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.061121 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.061372 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.076568 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.076805 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.092691 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.092893 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.107756 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.107955 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.123640 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.123864 140155277027136 quantize.py:201] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.206912 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.297351 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.426005 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nI0410 12:39:21.500694 140155277027136 quantize.py:166] Skipping FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/add, because its followed by an activation.\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/add\r\nI0410 12:39:22.099278 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/add\r\nI0410 12:39:22.099895 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/add\r\nI0410 12:39:22.100310 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/add\r\nI0410 12:39:22.100712 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/add\r\nI0410 12:39:22.101081 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/add\r\nI0410 12:39:22.101440 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/add\r\nI0410 12:39:22.101804 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/add\r\nI0410 12:39:22.102235 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/add\r\nI0410 12:39:22.102601 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/add\r\nI0410 12:39:22.103026 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/add\r\nI0410 12:39:22.103431 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/add\r\nI0410 12:39:22.103817 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/add\r\nI0410 12:39:22.104173 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/add\r\nI0410 12:39:22.104722 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/add\r\nI0410 12:39:22.105071 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/add\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/add\r\nI0410 12:39:22.105430 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/add\r\nINFO:tensorflow:Inserting fake quant op activation_AddV2_quant after FeatureExtractor/resnet_v1_50/fpn/top_down/add\r\nI0410 12:39:22.105675 140155277027136 quantize.py:262] Inserting fake quant op activation_AddV2_quant after FeatureExtractor/resnet_v1_50/fpn/top_down/add\r\nINFO:tensorflow:Inserting fake quant op activation_AddV2_quant after FeatureExtractor/resnet_v1_50/fpn/top_down/add_1\r\nI0410 12:39:22.111881 140155277027136 quantize.py:262] Inserting fake quant op activation_AddV2_quant after FeatureExtractor/resnet_v1_50/fpn/top_down/add_1\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/add_fold\r\nI0410 12:39:22.121784 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.122250 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.122534 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.122827 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.123043 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.123297 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.123489 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.123872 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.124067 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.124314 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.124595 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.124916 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.125163 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.125458 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.125700 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.126079 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.126330 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.126605 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.126782 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.127025 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.127182 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.127393 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.127542 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.127753 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.127903 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.128112 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.128261 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.128529 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.128677 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.128887 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.129033 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/add_fold\r\nI0410 12:39:22.129251 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/add_fold\r\nI0410 12:39:22.129395 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/fpn/top_down/smoothing_2/add_fold\r\nI0410 12:39:22.129603 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/fpn/top_down/smoothing_2/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/fpn/top_down/smoothing_1/add_fold\r\nI0410 12:39:22.129747 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/fpn/top_down/smoothing_1/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/fpn/bottom_up_block5/add_fold\r\nI0410 12:39:22.129896 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/fpn/bottom_up_block5/add_fold\r\nINFO:tensorflow:Skipping quant after FeatureExtractor/resnet_v1_50/fpn/bottom_up_block6/add_fold\r\nI0410 12:39:22.130037 140155277027136 quantize.py:299] Skipping quant after FeatureExtractor/resnet_v1_50/fpn/bottom_up_block6/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.130177 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.130314 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.130451 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.130587 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.130723 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.130858 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.130995 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.131131 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.131264 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.131427 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.131577 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.131731 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/BoxPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.131886 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.132046 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.132216 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.132382 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_1/ClassPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.132542 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.132689 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.132832 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.132973 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.133126 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.133264 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.133402 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.133538 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_2/ClassPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.133684 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.133875 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.134034 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.134214 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/BoxPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.134354 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.134491 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.134625 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.134763 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_3/ClassPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.134897 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.135031 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.135166 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.135301 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/BoxPredictionTower/conv2d_3/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_0/add_fold\r\nI0410 12:39:22.135435 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_0/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_1/add_fold\r\nI0410 12:39:22.135572 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_1/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_2/add_fold\r\nI0410 12:39:22.135709 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_2/add_fold\r\nINFO:tensorflow:Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_3/add_fold\r\nI0410 12:39:22.135844 140155277027136 quantize.py:299] Skipping quant after WeightSharedConvolutionalBoxPredictor_4/ClassPredictionTower/conv2d_3/add_fold\r\nWARNING:tensorflow:From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:292: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nW0410 12:39:22.138879 140155277027136 module_wrapper.py:139] From /tensorflow/models/research/object_detection/export_tflite_ssd_graph_lib.py:292: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0410 12:39:22.693765 140155277027136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n2020-04-10 12:39:23.471555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:23.471913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-04-10 12:39:23.471967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-04-10 12:39:23.471993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-10 12:39:23.472027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-04-10 12:39:23.472041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-04-10 12:39:23.472053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-04-10 12:39:23.472066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-04-10 12:39:23.472083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 12:39:23.472137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:23.472323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:23.472471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\r\n2020-04-10 12:39:23.472493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 12:39:23.472501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \r\n2020-04-10 12:39:23.472508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \r\n2020-04-10 12:39:23.472571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:23.472756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 12:39:23.472915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2888 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from /tensorflow/learn_pet/train/model.ckpt-5\r\nI0410 12:39:23.473652 140155277027136 saver.py:1284] Restoring parameters from /tensorflow/learn_pet/train/model.ckpt-5\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0410 12:39:25.057172 140155277027136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW0410 12:39:25.057349 140155277027136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nINFO:tensorflow:Froze 923 variables.\r\nI0410 12:39:25.800305 140155277027136 graph_util_impl.py:334] Froze 923 variables.\r\nINFO:tensorflow:Converted 923 variables to const ops.\r\nI0410 12:39:25.936274 140155277027136 graph_util_impl.py:394] Converted 923 variables to const ops.\r\n2020-04-10 12:39:26.212524: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes\r\nroot@d97ce87cb12f:/tensorflow/models/research# \r\n\r\n```\r\n\r\n**Now up until here everything goes relatively fine. However, converting this frozen graph or the resulting saved_model.pb from retraining causes issues.**\r\n\r\n### Method 1: converting a frozen using TFLite Converter (typically following the tutorial)\r\n```\r\ntflite_convert  --output_file=\"/tensorflow/models/research/learn_pet/models/output_tflite_graph.tflite\"   --graph_def_file=\"/tensorflow/models/research/learn_pet/models/tflite_graph.pb\"   --inference_type=QUANTIZED_UINT8  --input_arrays='normalized_input_image_tensor'   --output_arrays=\"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\" --mean_values=128 --std_dev_values=128 --input_shapes=1,320,320,3  --change_concat_input_ranges=false --allow_nudging_weights_to_use_fast_gemm_kernel=true --allow_custom_ops\r\n```\r\n\r\n**I get this output message:**\r\n\r\n```\r\nroot@fe9c39ea1f8e:/tensorflow/models/research# tflite_convert  --output_file=\"/tensorflow/models/research/learn_pet/models/output_tflite_graph.tflite\"   --graph_def_file=\"/tensorflow/models/research/learn_pet/models/tflite_graph.pb\"   --inference_type=QUANTIZED_UINT8  --input_arrays='normalized_input_image_tensor'   --output_arrays=\"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\" --mean_values=128 --std_dev_values=128 --input_shapes=1,320,320,3 \\--change_concat_input_ranges=false --allow_nudging_weights_to_use_fast_gemm_kernel=true --allow_custom_ops        \r\n2020-04-10 11:05:40.265936: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 11:05:40.330137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 11:05:40.330537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.11GiB\r\n2020-04-10 11:05:40.330577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2020-04-10 11:05:40.571784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 11:05:40.571846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2020-04-10 11:05:40.571855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2020-04-10 11:05:40.571999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2822 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 311, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2020-04-10 11:05:43.184443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-04-10 11:05:43.418899: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1987 operators, 2964 arrays (0 quantized)\r\n2020-04-10 11:05:43.488337: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1987 operators, 2964 arrays (0 quantized)\r\n2020-04-10 11:05:46.741187: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 360 operators, 642 arrays (1 quantized)\r\n2020-04-10 11:05:46.750968: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 360 operators, 642 arrays (1 quantized)\r\n2020-04-10 11:05:46.755164: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 234 operators, 516 arrays (1 quantized)\r\n2020-04-10 11:05:46.758687: F tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array FeatureExtractor/resnet_v1_50/fpn/top_down/nearest_neighbor_upsampling/mul, which is an input to the Reshape operator producing the output array FeatureExtractor/resnet_v1_50/fpn/top_down/nearest_neighbor_upsampling/Reshape_1, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted\r\n\r\nNone\r\nroot@fe9c39ea1f8e:/tensorflow/models/research#\r\n```\r\n\r\n\r\n**So I try adding  --default_ranges_min=0 and --default_ranges_max=6 following a Relu6 similar issue and it works, but fails at the edgetpu_compiler with:**\r\n**Code used:**\r\n```\r\nafayed@metecs-0497:~/coral/detection_ag/models$ sudo edgetpu_compiler output_tflite_graph.tflite\r\nEdge TPU Compiler version 2.1.302470888 \r\nERROR: :79 input->params.zero_point != output->params.zero_point (107 != 0)\r\nERROR: Node number 77 (PACK) failed to prepare.\r\n\r\nInternal compiler error. Aborting!\r\nafayed@metecs-0497:~/coral/detection_ag/models$\r\n```\r\n\r\n**Trying to convert the saved_model.pb route becomes a lot more interesting! This file gets generated after retraining in dir:**\r\n`/tensorflow/models/research/learn_pet/train/export/Servo/1586490086/saved_model.pb`\r\n\r\n**There are a few scripts I have found and stitched together. I will paste them each with their respective output: (The following is being run outside the docker container on my tf-nightly 2.2.0.dev20200401 pip install)**\r\n\r\n### Method 2 try 1: (converting a saved_model.pb using TFLite Converter)\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconverter.experimental_new_converter = **True or False**\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\ntflite_quant_model = converter.convert()\r\n```\r\n*I know the above used tf.float16 I was trying to get something... anything out of the converter. I might try GPU inference and drop the Coral USB Accelerator at this point with all this hassle or if its not even possible to use these more powerful models.*\r\n\r\n**Output:**\r\n\r\n```\r\nafayed@metecs-0497:~$ python3 ~/Desktop/test.py \r\n2020-04-10 04:52:09.463558: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:52:09.463687: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:52:09.463710: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 04:52:11.015862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 04:52:11.033075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.033430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:52:11.033684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:52:11.035302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:52:11.036590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:52:11.036867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:52:11.038115: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:52:11.038672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:52:11.040090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:52:11.040214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.040514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.040744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:52:11.041008: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 04:52:11.065286: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 04:52:11.065997: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xaf50e40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:52:11.066016: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 04:52:11.105688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.106075: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xaf740d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:52:11.106093: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 04:52:11.106271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.106524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:52:11.106575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:52:11.106590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:52:11.106604: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:52:11.106618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:52:11.106632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:52:11.106646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:52:11.106659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:52:11.106705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.107028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.107216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:52:11.107242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:52:11.107840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:52:11.107853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:52:11.107862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:52:11.107946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.108196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:52:11.108428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2889 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0410 04:52:11.142004 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.142286 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.142424 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.142553 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.142632 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.659588 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.659760 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.659844 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.659930 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:11.660046 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:12.539669 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:12.539836 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:12.539940 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:12.540039 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:12.540147 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:13.977087 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:13.977231 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:13.977302 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:13.977363 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:52:13.977421 139754160563968 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/Desktop/test.py\", line 6, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 418, in convert\r\n    raise ValueError(\"This converter can only convert a single \"\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\nafayed@metecs-0497:~$ \r\n```\r\n### Method 2 try 2:\r\n**Here I try the 'tf.lite.OpsSet.TFLITE_BUILTINS' and  'tf.lite.OpsSet.SELECT_TF_OPS'**\r\n```\r\nimport tensorflow as tf\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([1,320,320,3])\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Output:**\r\n```\r\nafayed@metecs-0497:~$ python3 ~/Desktop/test.py \r\n2020-04-10 04:54:40.829681: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:54:40.829826: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:54:40.829836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 04:54:42.415761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 04:54:42.433531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.433764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:54:42.433970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:54:42.435321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:54:42.436558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:54:42.436804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:54:42.438298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:54:42.438873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:54:42.440300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:54:42.440422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.440706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.440901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:54:42.441211: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 04:54:42.465540: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 04:54:42.466703: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb492f10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:54:42.466747: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 04:54:42.513942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.514292: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xb4b61a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:54:42.514310: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 04:54:42.514512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.514784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:54:42.514855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:54:42.514901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:54:42.514913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:54:42.514940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:54:42.514950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:54:42.514960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:54:42.514990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:54:42.515094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.515367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.515614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:54:42.515671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:54:42.516305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:54:42.516329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:54:42.516336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:54:42.516450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.516728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:54:42.516989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2889 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0410 04:54:42.553511 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:42.553783 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:42.553906 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:42.554021 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:42.554129 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:43.084858 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:43.085023 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:43.085146 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:43.085255 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:43.085352 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:44.004590 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:44.004742 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:44.004844 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:44.004932 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:44.004984 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:45.414238 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:45.414433 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:45.414536 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:45.414678 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:54:45.414774 139909104785152 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 634, in set_shape\r\n    unknown_shape)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 0 and 4\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/Desktop/test.py\", line 4, in <module>\r\n    concrete_func.inputs[0].set_shape([1,320,320,3])\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 637, in set_shape\r\n    raise ValueError(str(e))\r\nValueError: Shapes must be equal rank, but are 0 and 4\r\nafayed@metecs-0497:~$ \r\n```\r\n### Method 2 try 3 fixing set_shape() to an empty array:\r\n```\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')\r\n\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([])\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n*Output:*\r\n```\r\nafayed@metecs-0497:~$ python3 ~/Desktop/test.py \r\n2020-04-10 04:56:49.944995: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:56:49.945132: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:56:49.945157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 04:56:50.596387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 04:56:50.613734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.613967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:56:50.614179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:56:50.615449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:56:50.616734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:56:50.617019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:56:50.618337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:56:50.619126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:56:50.620796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:56:50.620927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.621282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.621526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:56:50.621866: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 04:56:50.645443: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 04:56:50.646094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6ca8e80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:56:50.646140: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 04:56:50.686690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.687029: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6ccc110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:56:50.687046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 04:56:50.687214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.687461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:56:50.687508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:56:50.687520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:56:50.687531: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:56:50.687541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:56:50.687551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:56:50.687561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:56:50.687571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:56:50.687658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.687918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.688156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:56:50.688198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:56:50.688912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:56:50.688923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:56:50.688947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:56:50.689028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.689310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:50.689592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0410 04:56:51.706341 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:51.706622 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:51.706747 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:51.706877 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:51.707004 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:52.241864 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:52.242060 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:52.242195 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:52.242302 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:52.242417 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:53.198438 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:53.198575 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:53.198707 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:53.198782 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:53.198846 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:54.791184 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:54.791393 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:54.791527 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:54.791616 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:54.791716 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-04-10 04:56:56.510059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:56.510283: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-10 04:56:56.510477: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-10 04:56:56.510978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:56.511179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:56:56.511212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:56:56.511225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:56:56.511237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:56:56.511248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:56:56.511259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:56:56.511269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:56:56.511281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:56:56.511320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:56.511527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:56.511755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:56:56.511774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:56:56.511780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:56:56.511784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:56:56.511879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:56.512118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:56:56.512332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-04-10 04:56:56.562991: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-10 04:56:56.563023: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-04-10 04:56:56.563028: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nW0410 04:56:56.611888 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:56.612050 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:56.612152 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:56.612239 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:56:56.612321 139958945724160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-04-10 04:57:04.038498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:04.038786: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-10 04:57:04.038887: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-10 04:57:04.039420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:04.039698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:57:04.039781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:57:04.039814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:57:04.039826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:57:04.039851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:57:04.039862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:57:04.039872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:57:04.039902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:57:04.039942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:04.040221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:04.040421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:57:04.040460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:57:04.040467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:57:04.040472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:57:04.040530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:04.040749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:04.040945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-04-10 04:57:05.077944: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-10 04:57:05.077966: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1572 nodes (-1774), 1874 edges (-1970), time = 572.089ms.\r\n2020-04-10 04:57:05.078096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1572 nodes (0), 1874 edges (0), time = 197.05ms.\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/Desktop/test.py\", line 14, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-10 04:57:06.359298: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:57:06.359375: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:57:06.359384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 04:57:07.195543: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:89] Ignored output_format.\r\n2020-04-10 04:57:07.195587: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:95] Ignored drop_control_dependency.\r\n2020-04-10 04:57:07.626685: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 04:57:07.653251: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 04:57:07.653736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd39c1ec330 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:57:07.653752: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 04:57:07.655781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 04:57:07.701689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:07.701992: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd39c2d6980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:57:07.702007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 04:57:07.702163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:07.702387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:57:07.702674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:57:07.704034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:57:07.705345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:57:07.705605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:57:07.706943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:57:07.707795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:57:07.709657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:57:07.709776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:07.710048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:07.710245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:57:07.710290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:57:07.710886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:57:07.710895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:57:07.710919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:57:07.711001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:07.711232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:57:07.711447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2250 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nloc(callsite(\"Equal\"(\"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py\":559:0) at callsite(\"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py\":528:0 at \"/home/afayed/Desktop/test.py\":6:0))): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<!tf.string>'\r\nloc(\"case/cond/is_jpeg/Equal@_functionalize_if_else_branch_5\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/is_jpeg/Equal@_functionalize_if_else_branch_3\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/is_png/Equal@_functionalize_if_else_branch_2\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/is_gif@_functionalize_if_else_branch_1\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp@_functionalize_if_else_branch_0\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/.local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py:559:7: error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<!tf.string>'\r\n      root = load_v1_in_v2.load(export_dir, tags)\r\n      ^\r\n/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py:528:3: note: called from\r\n  return load_internal(export_dir, tags)\r\n  ^\r\n/home/afayed/Desktop/test.py:6:1: note: called from\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\n^\r\n<unknown>:0: error: loc(\"case/cond/is_jpeg/Equal@_functionalize_if_else_branch_5\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/is_jpeg/Equal@_functionalize_if_else_branch_3\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/is_png/Equal@_functionalize_if_else_branch_2\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/is_gif@_functionalize_if_else_branch_1\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp@_functionalize_if_else_branch_0\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n\r\n\r\n\r\n\r\nafayed@metecs-0497:~$ \r\n```\r\n### Method 2 try 4 with a representative dataset (Experimental Conv set to True):\r\n```\r\nimport tensorflow as tf\r\nBATCH_SIZE = 32\r\nIMG_HEIGHT = 320\r\nIMG_WIDTH = 320\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')\r\n\r\ndef decode_img(img):\r\n\timg = tf.image.decode_jpeg(img, channels=3) #color images\r\n\timg = tf.image.convert_image_dtype(img, tf.float32) \r\n\t#convert unit8 tensor to floats in the [0,1]range\r\n\treturn tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \r\n\r\ndef representative_dataset_gen():\r\n\tfor _ in range(num_calibration_steps):\r\n\t# Get sample input data as a numpy array in a method of your choosing.\r\n\t\timage = list_all.take(1)\r\n\t\tyield [decode_img(image)]\r\n\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([])\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\n\r\nopen(\"/home/afayed/Desktop/output_tflite_graph.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\n**Output:**\r\n```\r\nafayed@metecs-0497:~$ python3 ~/Desktop/test.py \r\n2020-04-10 04:59:25.945004: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:59:25.945179: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:59:25.945191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 04:59:26.606701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 04:59:26.624591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.624832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:59:26.625073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:26.626365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:59:26.627619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:59:26.627865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:59:26.629198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:59:26.629971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:59:26.631630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:59:26.631723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.631985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.632169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:59:26.632490: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 04:59:26.657430: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 04:59:26.658056: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54cf5b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:59:26.658127: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 04:59:26.698387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.698734: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54f2840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:59:26.698751: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 04:59:26.698947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.699219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:59:26.699285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:26.699298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:59:26.699339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:59:26.699362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:59:26.699390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:59:26.699400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:59:26.699411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:59:26.699463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.699745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.699964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:59:26.700007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:26.700625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:59:26.700635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:59:26.700660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:59:26.700750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.701018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:26.701311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0410 04:59:27.646008 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:27.646277 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:27.646404 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:27.646530 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:27.646658 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:28.166036 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:28.166230 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:28.166350 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:28.166450 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:28.166565 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:29.039237 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:29.039395 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:29.039500 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:29.039588 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:29.039674 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:30.453517 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:30.453718 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:30.453832 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:30.453950 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:30.454065 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-04-10 04:59:32.095622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:32.095855: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-10 04:59:32.096054: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-10 04:59:32.096504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:32.096702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:59:32.096732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:32.096744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:59:32.096755: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:59:32.096765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:59:32.096774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:59:32.096784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:59:32.096794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:59:32.096834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:32.097122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:32.097353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:59:32.097391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:59:32.097416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:59:32.097422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:59:32.097561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:32.097814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:32.098006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-04-10 04:59:32.149381: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-10 04:59:32.149406: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-04-10 04:59:32.149411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nW0410 04:59:32.196574 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:32.196784 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:32.196887 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:32.196958 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 04:59:32.197010 140401577404160 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-04-10 04:59:39.254102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:39.254359: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-10 04:59:39.254457: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-10 04:59:39.254974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:39.255226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:59:39.255293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:39.255337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:59:39.255349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:59:39.255372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:59:39.255381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:59:39.255416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:59:39.255426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:59:39.255477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:39.255710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:39.255921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:59:39.255979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:59:39.255986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:59:39.256003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:59:39.256090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:39.256326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:39.256558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-04-10 04:59:40.209379: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-10 04:59:40.209410: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1572 nodes (-1774), 1874 edges (-1970), time = 522.985ms.\r\n2020-04-10 04:59:40.209481: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1572 nodes (0), 1874 edges (0), time = 186.768ms.\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/Desktop/test.py\", line 32, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-10 04:59:41.441981: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:59:41.442083: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 04:59:41.442091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 04:59:42.242273: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:89] Ignored output_format.\r\n2020-04-10 04:59:42.242303: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:95] Ignored drop_control_dependency.\r\n2020-04-10 04:59:42.652620: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 04:59:42.677332: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 04:59:42.678004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff2cc1ec790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:59:42.678035: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 04:59:42.680364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 04:59:42.720113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:42.720401: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff2cc2d6f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 04:59:42.720417: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 04:59:42.720579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:42.720802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 04:59:42.721080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:42.722453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 04:59:42.723755: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 04:59:42.724027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 04:59:42.725595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 04:59:42.726508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 04:59:42.728483: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 04:59:42.728633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:42.728920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:42.729150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 04:59:42.729198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 04:59:42.729846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 04:59:42.729856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 04:59:42.729881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 04:59:42.729971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:42.730223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 04:59:42.730515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2250 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nloc(callsite(\"Equal\"(\"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py\":559:0) at callsite(\"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py\":528:0 at \"/home/afayed/Desktop/test.py\":23:0))): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<!tf.string>'\r\nloc(\"case/cond/is_jpeg/Equal@_functionalize_if_else_branch_5\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/is_jpeg/Equal@_functionalize_if_else_branch_3\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/is_png/Equal@_functionalize_if_else_branch_2\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/is_gif@_functionalize_if_else_branch_1\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nloc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp@_functionalize_if_else_branch_0\"): error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/.local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py:559:7: error: 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<!tf.string>'\r\n      root = load_v1_in_v2.load(export_dir, tags)\r\n      ^\r\n/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/saved_model/load.py:528:3: note: called from\r\n  return load_internal(export_dir, tags)\r\n  ^\r\n/home/afayed/Desktop/test.py:23:1: note: called from\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\n^\r\n<unknown>:0: error: loc(\"case/cond/is_jpeg/Equal@_functionalize_if_else_branch_5\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/is_jpeg/Equal@_functionalize_if_else_branch_3\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/is_png/Equal@_functionalize_if_else_branch_2\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/is_gif@_functionalize_if_else_branch_1\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n<unknown>:0: error: loc(\"case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/cond_gif/is_bmp@_functionalize_if_else_branch_0\"): 'tfl.equal' op operand #0 must be tensor of 1-bit integer or 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or TFLite uint8 type values, but got 'tensor<*x!tf.string>'\r\n\r\n\r\n\r\n\r\nafayed@metecs-0497:~$ \r\n```\r\n### Method 2 try 5 with a representative dataset (Experimental Conv set to False):\r\n**And finally, this try is what headed this issue**\r\n**Code:**\r\n```\r\nimport tensorflow as tf\r\n\r\nBATCH_SIZE = 32\r\nIMG_HEIGHT = 320\r\nIMG_WIDTH = 320\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')\r\n\r\nlist_ds_Dianthus = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Dianthus''*/*/*'))\r\nlist_ds_DustyMiller = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/DustyMiller''*/*/*'))\r\nlist_ds_Pansy = tf.data.Dataset.list_files(str('/home/afayed/coral/detection_ag/ag/Pansy''*/*/*'))\r\nlist_all = list_ds_Dianthus.concatenate(list_ds_DustyMiller).concatenate(list_ds_Pansy)\r\n#len(list(list_all))\r\n\r\ndef get_label(file_path):\r\n  parts = tf.strings.split(file_path, '/')\r\n  class_name = tf.strings.split(parts[-3], '1')\r\n  class_name = tf.strings.split(class_name, '2')\r\n  class_name = tf.strings.split(class_name, '3')\r\n  class_name = tf.strings.split(class_name, '4')\r\n  return class_name\r\n\r\ndef decode_img(img):\r\n\timg = tf.image.decode_jpeg(img, channels=3) #color images\r\n\timg = tf.image.convert_image_dtype(img, tf.float32) \r\n\t#convert unit8 tensor to floats in the [0,1]range\r\n\treturn tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) \r\n\t#resize the image into 224*224 \r\n\r\ndef process_path(file_path):\r\n  label = get_label(file_path)\r\n  img = tf.io.read_file(file_path)\r\n  img = decode_img(img)\r\n  return img, label\r\n\r\nnum_calibration_steps= 1000\r\n\r\ndef representative_dataset_gen():\r\n\tfor _ in range(num_calibration_steps):\r\n\t# Get sample input data as a numpy array in a method of your choosing.\r\n\t\timage = list_all.take(1)\r\n\t\tyield [decode_img(image)]\r\n\r\nsaved_model_obj = tf.saved_model.load(export_dir='/home/afayed/coral/detection_ag/train/export/Servo/1586490086/')\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([])\r\nconverter =  tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = False\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations =  [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\n\r\nopen(\"/home/afayed/Desktop/output_tflite_graph.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\nOutput:\r\n```\r\nafayed@metecs-0497:~$ python3 ~/Desktop/Metecs\\ Work/quant_aware_training.py \r\n2020-04-10 06:06:04.479099: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 06:06:04.479217: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 06:06:04.479231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 06:06:05.141022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-04-10 06:06:05.159643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.159956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 06:06:05.160224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 06:06:05.161735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 06:06:05.163092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 06:06:05.163399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 06:06:05.164713: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 06:06:05.165613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 06:06:05.167462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 06:06:05.167624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.168101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.168436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 06:06:05.168831: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 06:06:05.193418: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2020-04-10 06:06:05.194356: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x62f8550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 06:06:05.194381: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-04-10 06:06:05.234776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.235127: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x631b7e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-04-10 06:06:05.235144: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2020-04-10 06:06:05.235310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.235556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 06:06:05.235612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 06:06:05.235650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 06:06:05.235670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 06:06:05.235706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 06:06:05.235727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 06:06:05.235750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 06:06:05.235771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 06:06:05.235826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.236058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.236253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 06:06:05.236304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 06:06:05.236934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 06:06:05.236945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 06:06:05.236969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 06:06:05.237066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.237329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:05.237577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0410 06:06:06.524962 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:06.525167 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:06.525248 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:06.525318 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:06.525382 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.102755 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.102902 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.102976 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.103037 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.103093 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.689994 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.690133 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.690204 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.690261 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:07.690316 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:09.211912 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:09.212095 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:09.212220 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:09.212320 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:09.212404 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-04-10 06:06:11.022645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:11.022909: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-10 06:06:11.023001: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-10 06:06:11.023880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:11.024138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 06:06:11.024205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 06:06:11.024249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 06:06:11.024261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 06:06:11.024283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 06:06:11.024293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 06:06:11.024324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 06:06:11.024357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 06:06:11.024423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:11.024695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:11.024925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 06:06:11.024962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 06:06:11.024988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 06:06:11.024993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 06:06:11.025169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:11.025432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:11.025671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-04-10 06:06:11.074813: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-10 06:06:11.074848: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-04-10 06:06:11.074853: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nW0410 06:06:11.121850 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:11.122043 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:11.122144 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:11.122252 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nW0410 06:06:11.122336 140508644669184 wrap_function.py:214] Unable to create a python object for variable <tf.Variable 'FeatureExtractor/resnet_v1_50/conv1/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2020-04-10 06:06:18.903261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:18.903541: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-04-10 06:06:18.903687: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-10 06:06:18.904242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:18.904630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-04-10 06:06:18.904684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-04-10 06:06:18.904706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-04-10 06:06:18.904729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-04-10 06:06:18.904750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-04-10 06:06:18.904772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-04-10 06:06:18.904794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-04-10 06:06:18.904817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-10 06:06:18.904878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:18.905197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:18.905387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-10 06:06:18.905414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-10 06:06:18.905426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-10 06:06:18.905435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-10 06:06:18.905511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:18.905903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-04-10 06:06:18.906185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 500 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-04-10 06:06:19.968587: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-04-10 06:06:19.968617: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1572 nodes (-1774), 1874 edges (-1970), time = 595.545ms.\r\n2020-04-10 06:06:19.968697: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 1572 nodes (0), 1874 edges (0), time = 203.882ms.\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/Desktop/Metecs Work/quant_aware_training.py\", line 65, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-10 06:06:21.294618: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 06:06:21.294725: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-9.0/lib64:/home/afayed/torch/install/lib:/home/afayed/kinova-demo/devel/lib:/opt/ros/kinetic/lib::/usr/local/lib:/usr/lib:/usr/aarch64-linux-gnu/lib\r\n2020-04-10 06:06:21.294733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-04-10 06:06:22.409963: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ParseSingleExample\r\n2020-04-10 06:06:22.410302: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeRaw\r\n2020-04-10 06:06:22.410346: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Substr\r\n2020-04-10 06:06:22.410366: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeJpeg\r\n2020-04-10 06:06:22.410433: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Substr\r\n2020-04-10 06:06:22.410442: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Substr\r\n2020-04-10 06:06:22.410488: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Substr\r\n2020-04-10 06:06:22.410520: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeJpeg\r\n2020-04-10 06:06:22.410540: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodePng\r\n2020-04-10 06:06:22.410627: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Substr\r\n2020-04-10 06:06:22.410660: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeGif\r\n2020-04-10 06:06:22.410673: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: DecodeBmp\r\n2020-04-10 06:06:22.412680: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV5\r\n2020-04-10 06:06:22.412718: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV5\r\n2020-04-10 06:06:22.412730: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: NonMaxSuppressionV5\r\n2020-04-10 06:06:22.412929: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2020-04-10 06:06:22.465849: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1111 operators, 2079 arrays (0 quantized)\r\n2020-04-10 06:06:22.514192: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1070 operators, 1975 arrays (0 quantized)\r\n2020-04-10 06:06:22.568968: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1070 operators, 1975 arrays (0 quantized)\r\n2020-04-10 06:06:22.789451: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 502 operators, 1067 arrays (0 quantized)\r\n2020-04-10 06:06:22.811142: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 502 operators, 1067 arrays (0 quantized)\r\n2020-04-10 06:06:22.825665: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 502 operators, 1067 arrays (0 quantized)\r\n2020-04-10 06:06:22.836283: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 502 operators, 1067 arrays (0 quantized)\r\n2020-04-10 06:06:22.867522: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2020-04-10 06:06:22.870222: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 31687618\r\n2020-04-10 06:06:22.871172: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeRaw is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871184: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871211: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeJpeg is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871217: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871222: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871230: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871236: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeJpeg is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871242: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodePng is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.871249: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeGif is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872020: W tensorflow/lite/toco/tflite/operator.cc:2024] Op NonMaxSuppressionV5 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872027: W tensorflow/lite/toco/tflite/operator.cc:2024] Op NonMaxSuppressionV5 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872031: W tensorflow/lite/toco/tflite/operator.cc:2024] Op NonMaxSuppressionV5 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872663: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeRaw is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872672: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872698: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeJpeg is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872704: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872708: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872716: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Substr is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872724: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeJpeg is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872728: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodePng is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.872748: W tensorflow/lite/toco/tflite/operator.cc:2024] Op DecodeGif is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.873543: W tensorflow/lite/toco/tflite/operator.cc:2024] Op NonMaxSuppressionV5 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.873551: W tensorflow/lite/toco/tflite/operator.cc:2024] Op NonMaxSuppressionV5 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.873556: W tensorflow/lite/toco/tflite/operator.cc:2024] Op NonMaxSuppressionV5 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2020-04-10 06:06:22.874173: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nTensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, WHERE. Here is a list of operators for which you will need custom implementations: DecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, Substr.\r\nTraceback (most recent call last):\r\n  File \"/home/afayed/.local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/afayed/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nTensorFlow Lite currently doesn't support control flow ops: Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, EQUAL, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_OR, LOGISTIC, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TOPK_V2, TRANSPOSE, UNPACK, WHERE. Here is a list of operators for which you will need custom implementations: DecodeGif, DecodeJpeg, DecodePng, DecodeRaw, NonMaxSuppressionV5, Substr.\r\n\r\n\r\n\r\nafayed@metecs-0497:~$ \r\n\r\n```\r\n*I limit memory growth using*\r\n`physical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=500)])\r\nlogical_devices = tf.config.list_logical_devices('GPU')`\r\n\r\n\r\n**Please help, I am also trying to do this with faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28 and thats where I found about the Experimental Converter working for RCNNs. My goal is to run either of these models on the Coral USB Accelerator after retraining with my own custom training data. I did not edit any scripts generating either of these models. Thanks in advanced**\r\n", "comments": ["@adfayed \r\ni have tried the very first code snippet shared by you and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/9c375191d5507bf04ae9c81f7c55b1a4/untitled136.ipynb)", "> @adfayed\r\n> i have tried the very first code snippet shared by you and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/9c375191d5507bf04ae9c81f7c55b1a4/untitled136.ipynb)\r\n\r\nPlease disregard (comment out) code lines number 6, 7, and 8. Like I mentioned they are only to limit GPU usage to 500 Mb. \r\n\r\nThe error you are getting is probably due to the use of Tensorflow 1.15 not Tensorflow-gpu or maybe a different index for you GPU card on your system.", "@adfayed \r\nwith nightly and gpu, the error faced is as per [this gist](https://colab.sandbox.google.com/gist/Saduf2019/5ce4ddd36cb0cfc023f2c683f88e29c6/38431.ipynb)", "Did you try commenting that out? That is totally beside the point. Please comment out those lines I get the same error with them commented out.", "@adfayed \r\nfrom the error shared, it says the files used by you is not shared, we will not be able to replicate unless we have all dependencies \"/home/afayed/coral/detection_ag/ag; No such file or directory [Op:MatchingFiles]\" is the error.", "That folder detection_ag/ag is all my training data, it consists of > 100Gb of different sub-folders of images and their corresponding detection bounding boxes data. \r\n\r\nMy issue is not with retraining. Retraining the model ckpt goes very smooth just as it should, I only included the inital part and the script used for retraining to provide a clear picture from start to finish if one can spot something wrong. \r\n\r\n**Quoting the issue detailed above, up until this comment:**\r\n\r\n*\"Now up until here everything goes relatively fine. However, converting this frozen graph or the resulting saved_model.pb from retraining causes issues.*\r\n\r\n***Method 1: converting a frozen using TFLite Converter (typically following the tutorial)***\"\r\n\r\nis the point where the issue opened here *(Converting retrained ssd_resnet50_v1 object detection model to TFLite for use on Coral USB Accelerator)* crops up. Converting the model ckpt into a usable format on the Coral USB Accelerator is the goal. This is why I have included, \"1. Model Ckpt trained, 2. Frozen graph, 3. saved_model.pb\", pipeline config file and Tensorflow versions I used. That should be all you need. It would be tough to share all the retraining images especially since it might have propriety repercussions and licensing issues.\r\n\r\nThanks,\r\nPlease do let me know of any other clarifications needed, I know my post is lengthy and might be all over the place. I just wanted to provide you with as much information as possible of the process start to finish.", "Hello? @jvishnuvardhan @Saduf2019 \r\nWill anyone please be of assistance?", "TFLite does not support control flow v1 ops like Switch or Merge. Please try creating a model with v2 control flow ops. And you are using a few TF ops that are not natively supported by TFLite. Please consider using a Flex delegate.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2\r\nhttps://www.tensorflow.org/lite/guide/ops_select", "Please follow [these instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) for converting an SSD model for TFLite.", "@adfayed \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38431\">No</a>\n"]}, {"number": 38430, "title": "flow_from_dataframe() doesn't shuffle data properly when splitting in subsets", "body": "**System information** \r\n- Google Colab\r\n- Tensorflow 2.2.0-rc2\r\n\r\n**Describe the current behavior**\r\nWhen `validation_split` is set to some value in `ImageDataGenerator` in order to split the data in two subsets to train and validate, and then shuffle is set to true in `flow_from_dataframe`, the shuffling occurs after this division. This makes the two subsets have different distributions, with a big impact in training stats.\r\n\r\n**Describe the expected behavior**\r\nThe data should be shuffled BEFORE the division in subsets instead of after.\r\n\r\n**Standalone code to reproduce the issue** \r\nFull code is available here: https://colab.research.google.com/drive/1gpiRZMHpp09nftzl909OG-9JJgTrhkO3 \r\n\r\n```\r\ndef check_iterator_distribution(iterator):\r\n  # Checks category distribution in the given iterator\r\n\r\n  iterator.reset()\r\n  nCat=[0,0,0,0,0,0,0,0,0] # number of images in each category (there are 9 categories)\r\n\r\n  step_size = iterator.n//iterator.batch_size\r\n\r\n  for i in range(step_size):\r\n    a,b = next(iterator)\r\n\r\n    for f in range(iterator.batch_size):\r\n      index = np.where(b[f]==1.)[0][0]\r\n\r\n      if index==0:\r\n        nCat[0]+=1\r\n      elif index==1:\r\n        nCat[1]+=1\r\n      elif index==2:\r\n        nCat[2]+=1\r\n      elif index==3:\r\n        nCat[3]+=1\r\n      elif index==4:\r\n        nCat[4]+=1\r\n      elif index==5:\r\n        nCat[5]+=1\r\n      elif index==6:\r\n        nCat[6]+=1\r\n      elif index==7:\r\n        nCat[7]+=1\r\n      elif index==8:\r\n        nCat[8]+=1\r\n\r\n  distribution = [x/iterator.n for x in nCat]\r\n\r\n  print('Number of images per category', nCat)\r\n  print('Distribution:',distribution)\r\n\r\ndatagen_train = ImageDataGenerator(preprocessing_function = efn.preprocess_input,\r\n                                   rotation_range = 40,\r\n                                   zoom_range = 0.2,\r\n                                   horizontal_flip = True,\r\n                                   vertical_flip = True,\r\n                                   validation_split=0.2\r\n                                   )\r\n\r\ntrain_iterator = datagen_train.flow_from_dataframe(\r\n    dataframe = df_train,\r\n    directory = path,\r\n    x_col = \"image\",\r\n    y_col = ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC','UNK'],\r\n    batch_size = 32,\r\n    shuffle = True,\r\n    class_mode = \"raw\",\r\n    target_size = (224,224),\r\n    subset = \"training\"\r\n)\r\n\r\nval_iterator = datagen_train.flow_from_dataframe(\r\n    dataframe = df_train,\r\n    directory = path,\r\n    x_col = \"image\",\r\n    y_col = ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC','UNK'],\r\n    batch_size = 32,\r\n    shuffle = True,\r\n    class_mode = \"raw\",\r\n    target_size = (224,224),\r\n    subset = \"validation\"\r\n)\r\n\r\ncheck_iterator_distribution(train_iterator)\r\ncheck_iterator_distribution(val_iterator)\r\n```\r\nThe distribution of both iterators is as follows:\r\n\r\n- `train_iterator`: Number of images per category [3789, 9417, 3214, 839, 1995, 214, 216, 572, 0]\r\nDistribution: [0.1869, 0.4646, 0.1585, 0.0414, 0.0984, 0.0105, 0.0106, 0.0282, 0.0]\r\n- `val_iterator`: Number of images per category [731, 3445, 108, 27, 627, 25, 37, 56, 0]\r\nDistribution: [0.1442, 0.6800, 0.0213, 0.0053, 0.1237, 0.0049, 0.0073, 0.0110, 0.0]\r\n\r\nIn contrast, when the dataframe is shuffled (through `df_train.sample(frac=1)`) before being passed to `flow_from_dataframe()`, the distribution is almost the same in both:\r\n\r\n- `train iterator`: Number of images per category [3596, 10290, 2699, 711, 2064, 192, 210, 494, 0]\r\nDistribution: [0.1774, 0.5077, 0.1331, 0.0350, 0.1018, 0.0094, 0.0103, 0.0243, 0.0]\r\n- `val_iterator`: Number of images per category [925, 2574, 621, 155, 558, 47, 43, 133, 0]\r\nDistribution: [0.1825, 0.5080, 0.1225, 0.0305, 0.1101, 0.0092, 0.0084, 0.0262, 0.0]\r\n\r\n", "comments": ["@jgciudad,\r\nThere are a few undefined variables like `efn.preprocess_input` and `path` used in the given code.\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "@amahendrakar \r\nHi! I updated the code in Colab declaring `path` and `efn.preprocess_input`. I think I'm not allowed to share the dataset, but it's composed of 25331 images of 9 different categories. I've printed a sample of the dataframe in Colab so you can see its structure. All the images referred in the dataframe are in `path`. I hope is enough with that, thanks!", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 38428, "title": "batch_jacobian incorrect", "body": "**System information** \r\n- Have I written custom code: yes\r\n- OS Platform and Distribution\r\n   - Windows 10, Anaconda Python 3.7.6, tensorflow 2.1.0\r\n   - Debian GNU/Linux 8.11 (jessie), Anaconda Python 3.7.3, tensorflow 2.0.0\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport numpy, tensorflow\r\nx = tensorflow.Variable([[1], [1]], dtype=\"float64\")\r\nwith tensorflow.GradientTape(persistent=True) as t:\r\n    with tensorflow.GradientTape() as tt:\r\n        obj = x[0]**2 + x[1]**2 + x[0]*x[1]\r\n    dx = tt.gradient(obj, x)\r\nprint(t.batch_jacobian(dx, x).numpy())\r\n```\r\noutput is \r\n```python\r\n[[[3.]]\r\n\r\n [[3.]]]\r\n```\r\nwhich is not `[jacobian(y[i], x[i]) for i in range(x.shape[0])]` as [stated here](https://www.tensorflow.org/api_docs/python/tf/GradientTape#batch_jacobian).\r\n\r\nIt is more like `[gradient(sum(y), x[i]) for i in range(x.shape[0])]`, same as gradient of gradient:\r\n```python\r\nimport numpy, tensorflow\r\nx = tensorflow.Variable([1, 1], dtype=\"float64\")\r\nwith tensorflow.GradientTape(persistent=True) as t:\r\n    with tensorflow.GradientTape() as tt:\r\n        obj = x[0]**2 + x[1]**2 + x[0]*x[1]\r\n    dx = tt.gradient(obj, x)\r\nprint(t.jacobian(dx, x).numpy())\r\nprint(t.gradient(dx, x).numpy())\r\n```\r\noutput is \r\n```python\r\n[[2. 1.]\r\n [1. 2.]]\r\n[3. 3.]\r\n```\r\nwhich is perfectly fine\r\n\r\n**Other info / logs**\r\n* f(x,y) = x^2+y^2+x y\r\n* df/dx = 2x+y\r\n* df/dy = 2y+x\r\n* ddf/dxdx = 2\r\n* ddf/dxdy = 1\r\n* ddf/dydy = 2\r\n\r\nWhat I want is the _diag(Hessian)_ without calculating full Hessian.", "comments": ["Could able to reproduce the issue with Tf2.2rc2 and Tf2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/eb4009fce6fa8027c150363d52b51ca0/untitled500.ipynb). Thanks", "As per documentation of batch_jacobian, the function requires the following:\r\n\r\n\"target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. target[i,...] should only depend on source[i,...].\"\r\n\r\nIn your case, the dependence requirement is not met.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38428\">No</a>\n", "I ran into this same issue today and thought it was a bug, but then saw @agarwal-ashish 's comment of the requirement not met (which is stated in the batch_jacobian API) and indeed it was the case for me too.\r\n\r\nThat said, I think the documentation in the \"Advanced Autodiff\" guide could be improved by mentioning this, because right now that guide says:\r\n\r\n> Caution: tf.GradientTape.batch_jacobian only verifies that the first dimension of the source and target match. It doesn't check that the gradients are actually independent. It's up to you to make sure you only use batch_jacobian where it makes sense. For example, adding a tf.keras.layers.BatchNormalization destroys the independence, since it normalizes across the batch dimension.\r\n\r\nHence, it seems from this warning that batch_jacobian doesn't care about y[i] being dependent on only x[i], and would still work as stated.\r\n\r\nThis is just a soft suggestion though, maybe it was just me who misunderstood."]}, {"number": 38427, "title": "Image Classification not working in Local System", "body": "**System information**\r\n**### - OS Platform and Distribution (MacOs Catalina):**\r\n### **- TensorFlow version:2.1.0**\r\n### **- Python version: 3.6.9**\r\n### **- Platform - docker with jupyter notebook**\r\n\r\n### **During The Image Classification, I got An error like this, \r\nIt working properly in google colab notebook**\r\n\r\n```\r\nimport os\r\nimport zipfile\r\nlocal_zip = '/tf/tmp/horse-or-human.zip'\r\nzip_ref = zipfile.ZipFile(local_zip, 'r')\r\nzip_ref.extractall('/tf/tmp/horse-or-human')\r\nzip_ref.close()\r\ntrain_horse_dir = os.path.join('/tf/tmp/horse-or-human/horses')\r\ntrain_human_dir = os.path.join('/tf/tmp/horse-or-human/humans')\r\ntrain_horse_names = os.listdir(train_horse_dir)\r\nprint(train_horse_names[:10])\r\ntrain_human_names = os.listdir(train_human_dir)\r\nprint(train_human_names[:10])\r\nprint('total training horse images:', len(os.listdir(train_horse_dir)))\r\nprint('total training human images:', len(os.listdir(train_human_dir)))\r\nget_ipython().run_line_magic('matplotlib', 'inline')\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\nnrows = 4\r\nncols = 4\r\npic_index = 0\r\n# Set up matplotlib fig, and size it to fit 4x4 pics\r\nfig = plt.gcf()\r\nfig.set_size_inches(ncols * 4, nrows * 4)\r\n\r\npic_index += 8\r\nnext_horse_pix = [os.path.join(train_horse_dir, fname) \r\n                for fname in train_horse_names[pic_index-8:pic_index]]\r\nnext_human_pix = [os.path.join(train_human_dir, fname) \r\n                for fname in train_human_names[pic_index-8:pic_index]]\r\n\r\nfor i, img_path in enumerate(next_horse_pix+next_human_pix):\r\n  # Set up subplot; subplot indices start at 1\r\n  sp = plt.subplot(nrows, ncols, i + 1)\r\n  sp.axis('Off') # Don't show axes (or gridlines)\r\n\r\n  img = mpimg.imread(img_path)\r\n  plt.imshow(img)\r\n\r\nplt.show()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\r\n    # This is the first convolution\r\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\r\n    tf.keras.layers.MaxPooling2D(2, 2),\r\n    # The second convolution\r\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # The third convolution\r\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # The fourth convolution\r\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # The fifth convolution\r\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(2,2),\r\n    # Flatten the results to feed into a DNN\r\n    tf.keras.layers.Flatten(),\r\n    # 512 neuron hidden layer\r\n    tf.keras.layers.Dense(512, activation='relu'),\r\n    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\r\n    tf.keras.layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\n\r\n\r\nmodel.summary()\r\n\r\n\r\n\r\nfrom tensorflow.keras.optimizers import RMSprop\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer=RMSprop(lr=0.001),\r\n              metrics=['accuracy'])\r\n\r\n\r\n\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\n\r\n\r\n\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\n# All images will be rescaled by 1./255\r\ntrain_datagen = ImageDataGenerator(rescale=1/255)\r\n\r\n# Flow training images in batches of 128 using train_datagen generator\r\ntrain_generator = train_datagen.flow_from_directory(\r\n        '/tf/tmp/horse-or-human/',  # This is the source directory for training images\r\n        target_size=(300, 300),  # All images will be resized to 150x150\r\n        batch_size=128,\r\n        # Since we use binary_crossentropy loss, we need binary labels\r\n        class_mode='binary')\r\n\r\n\r\n# In[18]:\r\n\r\n\r\nhistory = model.fit(train_generator,epochs=15,steps_per_epoch=8,verbose=1)\r\n```\r\n\r\n`\r\nand got an error \r\n### **ImportErrorTraceback (most recent call last)\r\n<ipython-input-18-8d2593ec774f> in <module>\r\n----> 1 history = model.fit(train_generator,epochs=15,steps_per_epoch=8,verbose=1)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    233           max_queue_size=max_queue_size,\r\n    234           workers=workers,\r\n--> 235           use_multiprocessing=use_multiprocessing)\r\n    236 \r\n    237       total_samples = _get_total_number_of_samples(training_data_adapter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    591         max_queue_size=max_queue_size,\r\n    592         workers=workers,\r\n--> 593         use_multiprocessing=use_multiprocessing)\r\n    594     val_adapter = None\r\n    595     if validation_data:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    704       max_queue_size=max_queue_size,\r\n    705       workers=workers,\r\n--> 706       use_multiprocessing=use_multiprocessing)\r\n    707 \r\n    708   return adapter\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, standardize_function, shuffle, workers, use_multiprocessing, max_queue_size, **kwargs)\r\n    950         use_multiprocessing=use_multiprocessing,\r\n    951         max_queue_size=max_queue_size,\r\n--> 952         **kwargs)\r\n    953 \r\n    954   @staticmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, standardize_function, workers, use_multiprocessing, max_queue_size, **kwargs)\r\n    745     # Since we have to know the dtype of the python generator when we build the\r\n    746     # dataset, we have to look at a batch to infer the structure.\r\n--> 747     peek, x = self._peek_and_restore(x)\r\n    748     assert_not_namedtuple(peek)\r\n    749 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/data_adapter.py in _peek_and_restore(x)\r\n    954   @staticmethod\r\n    955   def _peek_and_restore(x):\r\n--> 956     return x[0], x\r\n    957 \r\n    958   def _make_callable(self, x, workers, use_multiprocessing, max_queue_size):\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py in __getitem__(self, idx)\r\n     63         index_array = self.index_array[self.batch_size * idx:\r\n     64                                        self.batch_size * (idx + 1)]\r\n---> 65         return self._get_batches_of_transformed_samples(index_array)\r\n     66 \r\n     67     def __len__(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)\r\n    228                            color_mode=self.color_mode,\r\n    229                            target_size=self.target_size,\r\n--> 230                            interpolation=self.interpolation)\r\n    231             x = img_to_array(img, data_format=self.data_format)\r\n    232             # Pillow images should be closed after `load_img`,\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py in load_img(path, grayscale, color_mode, target_size, interpolation)\r\n    106         color_mode = 'grayscale'\r\n    107     if pil_image is None:\r\n--> 108         raise ImportError('Could not import PIL.Image. '\r\n    109                           'The use of `load_img` requires PIL.')\r\n    110     img = pil_image.open(path)\r\n\r\nImportError: Could not import PIL.Image. The use of `load_img` requires PIL.**\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["install `PIL` or equivalent package such as [`pillow`](https://pypi.org/project/Pillow/) then, e.g.,\r\n```\r\npip install pillow\r\n```", "> install `PIL` or equivalent package such as [`pillow`](https://pypi.org/project/Pillow/) then, e.g.,\r\n> \r\n> ```\r\n> pip install pillow\r\n> ```\r\n\r\nit alredy installed it show like\r\n`Requirement already satisfied: pillow in /usr/local/lib/python3.7/site-packages (5.3.0)`", "According to your messages. Your pillow is for Python 3.7 but your TesnorFlow is in Python 3.6.", "> According to your messages. Your pillow is for Python 3.7 but your TesnorFlow is in Python 3.6.\r\n\r\nok it's worked thank u ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38427\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38427\">No</a>\n", "> > According to your messages. Your pillow is for Python 3.7 but your TesnorFlow is in Python 3.6.\r\n> \r\n> ok it's worked thank u \r\n **solved by installing pillow in docker CLI**\r\n\r\n", "> > According to your messages. Your pillow is for Python 3.7 but your TesnorFlow is in Python 3.6.\r\n> \r\n> ok it's worked thank u\r\n\r\ncan u plz do me a favor by replacing my own data source instead of horse vs man", "> > > According to your messages. Your pillow is for Python 3.7 but your TesnorFlow is in Python 3.6.\r\n> > \r\n> > \r\n> > ok it's worked thank u\r\n> \r\n> can u plz do me a favor by replacing my own data source instead of horse vs man\r\n\r\nok please share repo link with your requirements "]}, {"number": 38426, "title": "Not converting pb to tflite using tflite_convert with some custom ops", "body": "**System information**\r\n- OS Platform and Distribution = Linux Ubuntu 18.04\r\n- TensorFlow installed from source\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-04-10 14:43:22.397713: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-10 14:43:22.426081: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194720000 Hz\r\n2020-04-10 14:43:22.426553: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40c04e0 executing computations on platform Host. Devices:\r\n2020-04-10 14:43:22.426614: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 504, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 500, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 904, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 373, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2020-04-10 14:43:24.007856: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-04-10 14:43:24.034443: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 500 operators, 754 arrays (0 quantized)\r\n2020-04-10 14:43:24.050893: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 500 operators, 754 arrays (0 quantized)\r\n2020-04-10 14:43:24.100022: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 64 operators, 176 arrays (0 quantized)\r\n2020-04-10 14:43:24.101499: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 64 operators, 176 arrays (0 quantized)\r\n2020-04-10 14:43:24.102515: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 64 operators, 176 arrays (0 quantized)\r\n2020-04-10 14:43:24.104827: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 11520000 bytes, theoretical optimal value: 8640000 bytes.\r\n2020-04-10 14:43:24.105140: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 2.25832 billion (note that a multiply-add is counted as 2 ops).\r\n2020-04-10 14:43:24.105533: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nTraceback (most recent call last):\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\nwhen I try to execute following command:\r\ntflite_convert   --output_file=\"/home/ashwini/Object_detection_general/models/research/logs_text_detection/frozen_pb_using_tflite_ssd_script/pb_without_custom_ops/temp.tflite\"   --graph_def_file=\"/home/ashwini/Object_detection_general/models/research/logs_text_detection/frozen_pb_using_tflite_ssd_script/pb_with_custom_ops/tflite_graph_46100.pb\"   --inference_type=FLOAT   --input_arrays=\"normalized_input_image_tensor\"   --output_arrays=\"TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\"   --mean_values=128   --std_dev_values=128   --input_shapes=1,300,300,3   --change_concat_input_ranges=false --default_ranges_min=0 and --default_ranges_max=6\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nTraceback (most recent call last):\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n\r\n", "comments": ["as said in the message you posted, you should set `allow_custom_ops=True`", "> as said in the message you posted, you should set `allow_custom_ops=True`\r\n\r\nI tried with --allow_custom_ops= True flag, and it is converting my .pb file to .tflite file also, But the issue is when I am passing that tflite file as C-byte array to the MicroInterpreter (to pass my file to tflite micro interpreter), It gives me the error about custom operation in graph which is not supported. So, I don't think here --allow_custom_flag= True operation will work. Please let me know If I am wrong.", "I want to use the tflite with microinterpreter (to work for detection models on microcontrollers)", "Then either add the `TFLite_Detection_PostProcess` to you interpreter or cut those ops when generating frozen graph (and implement them yourself).", "> Then either add the `TFLite_Detection_PostProcess` to you interpreter or cut those ops when generating frozen graph (and implement them yourself).\r\n\r\nCan you explain/elaborate how can I cut those operation from my frozen graph??? and if I can cut the graph then what will be the final input and output nodes?", "> > Then either add the `TFLite_Detection_PostProcess` to you interpreter or cut those ops when generating frozen graph (and implement them yourself).\r\n> \r\n> Can you explain/elaborate how can I cut those operation from my frozen graph??? and if I can cut the graph then what will be the final input and output nodes?\r\n\r\nMay be that can be by using graph transform tool..but seems to be unclear about that...It will be better if you elaborate how I can cut the graph and about input output nodes.", "@Ashwini-prkt  This is a micro related issue,Please post this in [micro repository](https://github.com/tensorflow/tflite-micro/issues)  and move this issue to closed status..Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38426\">No</a>\n"]}, {"number": 38425, "title": "TF2.x eager mode can not support ParameterServerStrategy now ?", "body": "TF version: latest master ,  b083ceafd48b3c8e4d9dfcc40a6b743bed7b371a\r\n\r\nBelow is a simple example using TF2.0 eager mode, and it ran successful with `MirroredStrategy`, but error with `ParameterServerStrategy`.\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os, json\r\n\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    \"cluster\": {\r\n        \"worker\": [\"localhost:12345\"],\r\n        \"ps\": [\"localhost:12346\"]\r\n    },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n})\r\n\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\r\n#strategy = tf.distribute.MirroredStrategy()\r\n\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n  return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n\r\n  model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\ncheckpoint_dir = './training_checkpoints'\r\n# Name of the checkpoint files\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\n# Function for decaying the learning rate.\r\n# You can define any decay function you need.\r\ndef decay(epoch):\r\n  if epoch < 3:\r\n    return 1e-3\r\n  elif epoch >= 3 and epoch < 7:\r\n    return 1e-4\r\n  else:\r\n    return 1e-5\r\n\r\n# Callback for printing the LR at the end of each epoch.\r\nclass PrintLR(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\r\n                                                      model.optimizer.lr.numpy()))\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\r\n                                       save_weights_only=True),\r\n    tf.keras.callbacks.LearningRateScheduler(decay),\r\n    PrintLR()\r\n]\r\n\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\neval_loss, eval_acc = model.evaluate(eval_dataset)\r\nprint('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n```\r\n\r\n**error message** \r\n\r\n```\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 116, in __init__\r\n    super(Sequential, self).__init__(name=name, autocast=False)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 199, in __init__\r\n    self._init_batch_counters()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 206, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 261, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 255, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 66, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1769, in creator_with_resource_vars\r\n    return self._create_variable(next_creator, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/distribute/parameter_server_strategy.py\", line 455, in _create_variable\r\n    with ops.device(self._variable_device):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 5183, in device\r\n    \"tf.device does not support functions when eager execution \"\r\nRuntimeError: tf.device does not support functions when eager execution is enabled.\r\n\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/aa3d84dc11d49f6d0625fcc51c0c6b66/38425.ipynb), [TF v2.2.0rc2](https://colab.research.google.com/gist/amahendrakar/81dca68cbbc79f8d295cd9f127a83039/38425-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/9c90f40aae25e7c66e6835ba28d7cb9d/38425-tf-nightly.ipynb). Please find the Gist here. Thanks!", "ParameterServerStrategy currently only works with Estimator API with eager mode disabled (this [chart](https://www.tensorflow.org/guide/distributed_training#types_of_strategies) explains what are supported now). Keras compile/fit and custom training loop support is being actively worked on (See this [RFC](https://github.com/tensorflow/community/blob/master/rfcs/20200306-single-client-parameter-server.md)).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38425\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38425\">No</a>\n"]}, {"number": 38424, "title": "UnicodeDecodeError", "body": "**System information** \r\n- csv2TFrecords\r\n- Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from: pip install tensorflow==2.2.0rc2\r\n- TensorFlow version : 2.2.0-rc2\r\n- Python version: 3.8.2\r\n - GPU model: AMD HD 7950\r\n\r\n**Traceback (most recent call last):\r\n  File \"tf.py\", line 100, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\Mostalk\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\Mostalk\\AppData\\Roaming\\Python\\Python38\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\Mostalk\\AppData\\Roaming\\Python\\Python38\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tf.py\", line 91, in main\r\n    tf_example = create_tf_example(group, path)\r\n  File \"tf.py\", line 46, in create_tf_example\r\n    encoded_jpg = fid.read()\r\n  File \"C:\\Users\\Mostalk\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 116, in read\r\n    self._preread_check()\r\n  File \"C:\\Users\\Mostalk\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 78, in _preread_check\r\n    self._read_buf = _pywrap_file_io.BufferedInputStream(\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xcd in position 123: invalid continuation byte**\r\n\r\n", "comments": ["@Mostalk, Can you provide the complete standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Mostalk, please update as per above comment", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38424\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38424\">No</a>\n"]}, {"number": 38423, "title": "Error when loading h5 model inside strategy scope", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Windows 10 and Ubuntu 16.04.6 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): **binary**\r\n- TensorFlow version (use command below): **2.1.0**\r\n- Python version: **Python 3.7.5**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: **CUDA 10.1.243, CuDNN 7.6.4**\r\n- GPU model and memory: **error with CPU and GPU**\r\n\r\n**Describe the current behavior**\r\nCannot load a model from a h5 file inside a strategy context (either CPU, GPU or mirrored) whereas load_model works without strategy context.\r\nInside a strategy context, it gives error :  \r\n\r\n> c:\\program files\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n>     439         return control_flow_ops.no_op()\r\n>     440       apply_state = self._prepare(var_list)\r\n> --> 441       return distribute_ctx.get_replica_context().merge_call(\r\n>     442           functools.partial(self._distributed_apply, apply_state=apply_state),\r\n>     443           args=(grads_and_vars,),\r\n> \r\n> AttributeError: 'NoneType' object has no attribute 'merge_call'\r\n\r\nThis issue is linked to https://github.com/tensorflow/tensorflow/issues/32359 \r\n\r\n**Describe the expected behavior**\r\nload_model call should work in both cases : with or without the use of a strategy context.\r\n\r\n**Standalone code to reproduce the issue** \r\nThe zip archive containing the h5 file of the model is attached : \r\n[dummy_model.zip](https://github.com/tensorflow/tensorflow/files/4460828/dummy_model.zip)\r\n\r\n  \r\nHere is the code to reproduce the issue : \r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel_path = \"dummy_model.h5\"\r\n\r\n##### WORKING CASE #####\r\nmodel = tf.keras.models.load_model(model_path)\r\nmodel.summary()\r\n########################\r\n\r\n##### NON WORKING CASE #####\r\nstrategy_context = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")  # not working also with \"/gpu:0\" and MirroredStrategy\r\nwith strategy_context.scope():\r\n    model = tf.keras.models.load_model(model_path)\r\n    model.summary()\r\n############################\r\n```\r\n", "comments": ["@ismael-elatifi I agree with you that it is not working with `TF2.1`. However, when I tried with recent `tf-nightly`, it is working as expected. [Here](https://colab.research.google.com/gist/jvishnuvardhan/b93d61013733e421467e1180e3576f82/loading_model_in_scope.ipynb) is a gist for your reference. Based on the gist, I guess this was resolved in recent `tf-nightly`. If you like stable version, In the near future there will be stable `TF2.0` release.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@ismael-elatifi \r\nplease update as per above comment", "@jvishnuvardhan Thanks for your reply. However the model you tried in your gist is not the same as in my issue. Can you try with the same h5 file I provided ?", "@ismael-elatifi [Here](https://colab.research.google.com/gist/jvishnuvardhan/ba1b415602fb89381256a1fb21fde977/loading_model_in_scope.ipynb) is the gist with your model. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan ok thanks, then I will wait for the TF2.2 stable version and in the meantime I won't use any context with the 2.1.\r\nSince it works with the latest nightly, I close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38423\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38423\">No</a>\n"]}, {"number": 38421, "title": "How to generate schema_generaed.h", "body": "I try to add a builtin op for tflite, could you please tell me how to generate tensorflow/lite/schema/schema_generated.h\r\nThanks for your  help!", "comments": ["most likely it's generated by running [`flatc`](https://google.github.io/flatbuffers/flatbuffers_guide_using_schema_compiler.html) of [`flatbuffer`](https://google.github.io/flatbuffers/)\r\n\r\njust curious, why TFLite [custom op](https://www.tensorflow.org/lite/guide/ops_custom) is not enough?", "I've solved this proplem. \r\n```sh\r\nbazel run \\\r\n  //tensorflow/lite/schema/builtin_ops_header:generate > \\\r\n  tensorflow/lite/builtin_ops.h\r\nmv bazel-genfiles/tensorflow/lite/schema/schema_generated.h tensorflow/lite/schema/schema_generated.h\r\n```\r\nHashTableV2\u3001LookupTableImportV2\u3001LookupTableFindV2 are not supported right now. I just try to implement a vocab_list_lookup op. When convert a model with a LookupTableImportV2 op, it missed in the converted tflite model.", "> I've solved this proplem.\r\n\r\n@lizhen2017,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "\r\n> @lizhen2017,\r\n> Is this still an issue? Please feel free to close the issue if resolved. Thanks!\r\n\r\nAny updates regarding this issue? Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]