[{"number": 54611, "title": "MultiaryUnstack forward type inference function for TensorSliceDataset", "body": "MultiaryUnstack forward type inference function for TensorSliceDataset\n\nFulltype helper for ops with semantics of unstacking multiple inputs into a container\n`<t>[T1, ..., Tn]`, that is `T1, ..., Tn -> <t>[PRODUCT[U1, ..., Un]]`\nwhere Ui is obtained from an \"unstack\" mapping T -> U. Both <t> and the\n\"unstack\" mapping are parameterized by this factory.\n\nTensorSliceDataset uses this.\n", "comments": []}, {"number": 54610, "title": "[mlir] ExecutionEngine: default enableObjectCache to false", "body": "[mlir] ExecutionEngine: default enableObjectCache to false\n", "comments": []}, {"number": 54609, "title": "[tfrt:jit] Allocate JIT memory in a named memfd", "body": "[tfrt:jit] Allocate JIT memory in a named memfd\n", "comments": []}, {"number": 54608, "title": "[XLA:CPU] Disable fast math by default (except in reductions).", "body": "[XLA:CPU] Disable fast math by default (except in reductions).\n\nFixes https://github.com/google/jax/issues/6566\n", "comments": []}, {"number": 54607, "title": "Add tf.random.experimental.index_shuffle().", "body": "Add tf.random.experimental.index_shuffle().\n\nIt maps an index in [0, maxval] to a new random index in [0, maxval]. This allows users to suffle a list {0, 1, ..., n} without materializing the full list in memory.\n\nThe operation is stateless and does not use the global random seed. The current implementation only works on CPU.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/54521 from PatriceVignola:add-inplace-device-default-registration 2e5af637d89b180031100530cdbd9b3ef34ec176\n", "comments": []}, {"number": 54606, "title": "Implement the new sample algorithm for memory timeline graph.", "body": "Implement the new sample algorithm for memory timeline graph.\n\nIn the old code, PerAllocatorMemoryProfile.memory_profile_snapshots are used in two places: (1) Display in timeline graph (2) Referenced by active allocations\n\nIn the new code, PerAllocatorMemoryProfile.memory_profile_snapshots are only referenced by active allocations, PerAllocatorMemoryProfile.memory_profile_timeline_snapshots are used to display in the timeline graph, and is created by a max box filter over the original data.\n", "comments": []}, {"number": 54605, "title": "Implement most_specific_common_supertype for DatasetSpec", "body": "Implement most_specific_common_supertype for DatasetSpec\n", "comments": []}, {"number": 54604, "title": "Add set_tpu_infeed_layout.h and set_tpu_infeed_layout.cc.", "body": "Add set_tpu_infeed_layout.h and set_tpu_infeed_layout.cc.\n", "comments": []}, {"number": 54603, "title": "Verifying the verification order [do not merge]", "body": "Verifying the verification order [do not merge]\n", "comments": []}, {"number": 54602, "title": "Implement `TfrtSession::Extend()` after session creation", "body": "Implement `TfrtSession::Extend()` after session creation\n", "comments": []}, {"number": 54601, "title": "Update RaggedTensor._to_components to add shape information to RaggedTensor.", "body": "Update RaggedTensor._to_components to add shape information to RaggedTensor.\n\nWhen a RaggedTensor is returned from tf.py_function in graph mode, its shape information is unknown and should be recovered from RaggedTensorSpec.\n", "comments": []}, {"number": 54600, "title": "[ST] Migrate from linalg.tiled_loop to gml_st.loop.", "body": "[ST] Migrate from linalg.tiled_loop to gml_st.loop.\n", "comments": []}, {"number": 54599, "title": "[TF:TRT] Elide log messages when qdq mode is off", "body": "This change removes log messages notifying users that QDQ mode is not enabled. Since this is not surprising for the majority of users, and log messages appear to notify users that QDQ mode is enabled, the \"not enabled\" log messages are superfluous.", "comments": ["@bixia1 for review when available. Thanks!"]}, {"number": 54598, "title": "[tfrt:jit] Move ReturnValueConverterBase constructor/destructor definitions to header", "body": "[tfrt:jit] Move ReturnValueConverterBase constructor/destructor definitions to header\n", "comments": []}, {"number": 54597, "title": "[tfrt:jit] Move async task runners definition to header", "body": "[tfrt:jit] Move async task runners definition to header\n", "comments": []}, {"number": 54596, "title": "[tfrt:jit] Move jitrt type definitions to header", "body": "[tfrt:jit] Move jitrt type definitions to header\n\nSome of the function calls are now inlined into the ReturnValues converter\n\nBEFORE:\n--- Running 'BM_compiled_rsqrt_f32':\nBM:BM_compiled_rsqrt_f32:Time Min(ns): 494\nBM:BM_compiled_rsqrt_f32:Time 50%(ns): 536\nBM:BM_compiled_rsqrt_f32:Time 95%(ns): 565\nBM:BM_compiled_rsqrt_f32:Time 99%(ns): 581\n\nAFTER:\n--- Running 'BM_compiled_rsqrt_f32':\nBM:BM_compiled_rsqrt_f32:Time Min(ns): 482\nBM:BM_compiled_rsqrt_f32:Time 50%(ns): 516\nBM:BM_compiled_rsqrt_f32:Time 95%(ns): 533\nBM:BM_compiled_rsqrt_f32:Time 99%(ns): 562\n", "comments": []}, {"number": 54595, "title": "[tf:jitrt] Add benchmark for returning Tensors", "body": "[tf:jitrt] Add benchmark for returning Tensors\n", "comments": []}, {"number": 54594, "title": "[tfrt:jit] Add a return value converter with statically defined conversion functions", "body": "[tfrt:jit] Add a return value converter with statically defined conversion functions\n\nThis allows return value converter to inline individual conversion functions into the converter and skip building vector of llvm::function_refs.\n\nBenchmark                       Time(ns)        CPU(ns)     Iterations\n----------------------------------------------------------------------\nBM_ReturnValueConverter               80.9           80.8       731760\nBM_StaticReturnValueConverter         75.7           76.8       729097\n", "comments": []}, {"number": 54593, "title": "[XLA:CPU] Allow reassociation in reduction computations even if fast math mode is disabled.", "body": "[XLA:CPU] Allow reassociation in reduction computations even if fast math mode is disabled.\n\n\nAfter, with fast math disabled globally (except obviously on reductions):\n```\nname                                 cpu/op\nbm_reduction_10_axis=0              3.80\u00b5s \u00b1 3%\nbm_reduction_100_axis=0             3.86\u00b5s \u00b1 4%\nbm_reduction_1000_axis=0            4.10\u00b5s \u00b1 0%\nbm_reduction_10000_axis=0           6.92\u00b5s \u00b1 2%\nbm_reduction_100000_axis=0          15.5\u00b5s \u00b1 1%\nbm_reduction_1000000_axis=0          165\u00b5s \u00b1 9%\nbm_reduction_10000000_axis=0         870\u00b5s \u00b1 9%\nbm_reduction_10x10_axis=0           3.80\u00b5s \u00b1 4%\nbm_reduction_10x10_axis=1           3.82\u00b5s \u00b1 3%\nbm_reduction_100x100_axis=0         8.76\u00b5s \u00b1 2%\nbm_reduction_100x100_axis=1         5.16\u00b5s \u00b1 3%\nbm_reduction_1000x1000_axis=0        504\u00b5s \u00b1 1%\nbm_reduction_1000x1000_axis=1        269\u00b5s \u00b1 1%\nbm_reduction_10000x10000_axis=0     7.63ms \u00b1 3%\nbm_reduction_10000x10000_axis=1     5.94ms \u00b1 8%\nbm_reduction_100000x100000_axis=0    601ms \u00b1 3%\nbm_reduction_100000x100000_axis=1    442ms \u00b1 5%\n```\n\nBefore, with fast math enabled;\n```\nname                                 cpu/op\nbm_reduction_10_axis=0              4.22\u00b5s \u00b1 8%\nbm_reduction_100_axis=0             4.56\u00b5s \u00b137%\nbm_reduction_1000_axis=0            4.37\u00b5s \u00b1 7%\nbm_reduction_10000_axis=0           7.62\u00b5s \u00b118%\nbm_reduction_100000_axis=0          15.2\u00b5s \u00b1 2%\nbm_reduction_1000000_axis=0          163\u00b5s \u00b111%\nbm_reduction_10000000_axis=0         818\u00b5s \u00b112%\nbm_reduction_10x10_axis=0           4.30\u00b5s \u00b121%\nbm_reduction_10x10_axis=1           4.14\u00b5s \u00b1 7%\nbm_reduction_100x100_axis=0         9.24\u00b5s \u00b1 3%\nbm_reduction_100x100_axis=1         5.36\u00b5s \u00b1 5%\nbm_reduction_1000x1000_axis=0        508\u00b5s \u00b1 1%\nbm_reduction_1000x1000_axis=1        261\u00b5s \u00b1 2%\nbm_reduction_10000x10000_axis=0     7.69ms \u00b1 5%\nbm_reduction_10000x10000_axis=1     5.42ms \u00b1 7%\nbm_reduction_100000x100000_axis=0    594ms \u00b1 4%\nbm_reduction_100000x100000_axis=1    425ms \u00b1 6%\n```\n\nBefore, with fast math disabled:\n```\nname                                cpu/op\nbm_reduction_10_axis=0              3.98\u00b5s \u00b1 3%\nbm_reduction_100_axis=0             4.28\u00b5s \u00b122%\nbm_reduction_1000_axis=0            4.74\u00b5s \u00b1 3%\nbm_reduction_10000_axis=0           11.0\u00b5s \u00b1 1%\nbm_reduction_100000_axis=0          56.8\u00b5s \u00b1 1%\nbm_reduction_1000000_axis=0          538\u00b5s \u00b1 1%\nbm_reduction_10000000_axis=0        2.05ms \u00b1 5%\nbm_reduction_10x10_axis=0           4.04\u00b5s \u00b1 1%\nbm_reduction_10x10_axis=1           4.01\u00b5s \u00b1 3%\nbm_reduction_100x100_axis=0         11.3\u00b5s \u00b1 2%\nbm_reduction_100x100_axis=1         9.63\u00b5s \u00b1 1%\nbm_reduction_1000x1000_axis=0        704\u00b5s \u00b1 1%\nbm_reduction_1000x1000_axis=1        696\u00b5s \u00b1 2%\nbm_reduction_10000x10000_axis=0     11.0ms \u00b1 3%\nbm_reduction_10000x10000_axis=1     10.2ms \u00b1 1%\nbm_reduction_100000x100000_axis=0    960ms \u00b1 2%\nbm_reduction_100000x100000_axis=1    675ms \u00b1 5%\n```\n", "comments": []}, {"number": 54592, "title": "Add StatusOr<bool> IsReady() to the PjRtBuffer API.", "body": "Add StatusOr<bool> IsReady() to the PjRtBuffer API.\n\nAn error is returned:\n- if the buffer was deleted.\n- if it's not supported by the backend (discouraged, but may happen).\n", "comments": []}, {"number": 54591, "title": "Use the attribute `_shape` in `ReadVariableOp` shape inference if found", "body": "This PR is a workaround to fix shape inference of `ReadVariableOp` nodes when a graph gets resource inputs and we can't infer the shapes of the variables. The nodes need to be annotated beforehand with an attribute `shape`.\r\n\r\nContext and code sample illustrating the problem in the comments.\r\n\r\nThis is only a temporary solution, ideally, shape inference should handle resource inputs (similarly to how it can get shapes of the inputs when `assume_valid_feeds == true`).", "comments": ["## Context\r\n\r\nI intend to demonstrate some issues that I encountered with shape inference while implementing support for non-frozen models in TF-TRT, and show how I temporarily worked around them.\r\n\r\nIn TF-TRT, the creation of graph segments that are convertible to TensorRT is a Grappler pass and requires shape inference, assuming valid feeds. The shapes are used to determine whether operations are compatible with TensorRT. Unknown shapes lead to nodes that can't be converted, e.g a convolution requires a static channel dimension.\r\n\r\n## Reproducer\r\n\r\nFor the reproducer, I create a tiny model and execute Grappler optimization, with aggressive constant folding. This is because aggressive constant folding assumes that the shapes of placeholders match the actual feed, as TF-TRT does. In the first step, I use the original graph def, and in the second step, I use a graph modified with a custom attribute `shape` on `ReadVariableOp` nodes. I modified the shape inference function to use that attribute if found.\r\n\r\nTo get some detailed logging, I add the node `\"add\"` to `node_names_for_logging` and set a high logging level.\r\n\r\n```python\r\nimport os\r\nos.environ[\"TF_CPP_VMODULE\"] = \"graph_properties=3,meta_optimizer=1\"\r\n\r\nfrom tensorflow.python.ops import array_ops\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.framework import convert_to_constants\r\nfrom tensorflow.python.grappler import tf_optimizer\r\nfrom tensorflow.core.protobuf import meta_graph_pb2\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nfrom tensorflow.python.training.saver import export_meta_graph\r\nfrom tensorflow.core.protobuf import config_pb2\r\n\r\n\r\nclass ToyModel(tf.Module):\r\n    def __init__(self):\r\n        self.v = tf.Variable(np.random.normal(0.0, 1.0, 64).astype(np.float32))\r\n\r\n    @tf.function\r\n    def __call__(self, x):\r\n        add = x + self.v\r\n        return array_ops.identity(add, name=\"my_out\")\r\n\r\ntoy_model = ToyModel()\r\n\r\n\r\ndef run_agressive_constfold(func, graph_def):\r\n    meta_graph = export_meta_graph(graph_def=graph_def, graph=func.graph)\r\n\r\n    # Add a collection 'train_op' so that Grappler knows the outputs.\r\n    fetch_collection = meta_graph_pb2.CollectionDef()\r\n    for array in func.inputs + func.outputs:\r\n        fetch_collection.node_list.value.append(array.name)\r\n    meta_graph.collection_def[\"train_op\"].CopyFrom(fetch_collection)\r\n\r\n    # Initialize RewriterConfig with agressive constant folding, so that the\r\n    # folding pass will execute shape inference with assume_valid_feeds=true.\r\n    config = config_pb2.ConfigProto()\r\n    rewrite_options = config.graph_options.rewrite_options\r\n    rewrite_options.constant_folding =\\\r\n        rewriter_config_pb2.RewriterConfig.AGGRESSIVE\r\n    rewrite_options.meta_optimizer_iterations = 1\r\n    return tf_optimizer.OptimizeGraph(config, meta_graph)\r\n\r\n\r\nfunc = toy_model.__call__.get_concrete_function(\r\n    tf.TensorSpec([None, 64], tf.float32))\r\n\r\n# Run Grappler with the original graph def.\r\ngraph_def = func.graph.as_graph_def()\r\nrun_agressive_constfold(func, graph_def)\r\n\r\n# Inline the graph.\r\ngraph_def = convert_to_constants._run_inline_graph_optimization(\r\n    func, True, False)\r\n# Annotate ReadVariableOp with shape.\r\nph_shape_map = {}\r\nfor ph, var in zip(func.graph.internal_captures, func.variables):\r\n    ph_shape_map[ph.name] = var.shape\r\nname_to_node = {node.name: node for node in graph_def.node}\r\nfor node in graph_def.node:\r\n    if node.op == \"ReadVariableOp\":\r\n        node_ = node\r\n        # Go up the chain of identities to find a placeholder\r\n        while name_to_node[node_.input[0]].op == \"Identity\":\r\n            node_ = name_to_node[node_.input[0]]\r\n        shape = ph_shape_map[node_.input[0] + \":0\"]\r\n        node.attr[\"shape\"].shape.CopyFrom(shape.as_proto())\r\n# Run Grappler with the annotated graph def.\r\nrun_agressive_constfold(func, graph_def)\r\n```\r\n\r\nDuring the first Grappler execution, these are some logs of the shape inference that show that the shape of the input `x` is passed correctly but the shape of the variable isn't, and shape inference for `add` is impossible:\r\n```\r\nadd [AddV2] has 2 inputs and 1 output: \r\n input [0] add/ReadVariableOp -- type: float, shape: ?, tensor:  null, tensor_as_shape: ?\r\n input [1] x -- type: float, shape: [?,64], tensor:  null, tensor_as_shape: ?\r\n output [0] -- type: float, shape: ?, tensor:  null, tensor_as_shape:  null\r\n```\r\n\r\nIn the second execution with my custom annotation and modification of the shape inference function, shapes are inferred correctly:\r\n```\r\nadd [AddV2] has 2 inputs and 1 output: \r\n input [0] add/ReadVariableOp -- type: float, shape: [64], tensor:  null, tensor_as_shape: ?\r\n input [1] x -- type: float, shape: [?,64], tensor:  null, tensor_as_shape: ?\r\n output [0] -- type: float, shape: [?,64], tensor:  null, tensor_as_shape:  null\r\n```", "@bixia1 Marked as ready for review.", "@bixia1 I fixed the clang-tidy issue.", "Let us consider the [example given above](https://github.com/tensorflow/tensorflow/pull/54591#issuecomment-1063280437), and try to build a function from the annotated GraphDef:\r\n```python\r\ncaptures = {x.name.split(\":\")[0]: y for x, y in zip(func.graph.internal_captures, func.graph.external_captures)}\r\nfunc2 = wrap_function.function_from_graph_def(\r\n          graph_def,\r\n          [x.name for x in func.inputs],\r\n          [x.name for x in func.outputs], \r\n          captures)\r\n```\r\nThis gives\r\n```\r\n2022-03-23 22:31:08.831971: E tensorflow/core/framework/node_def_util.cc:630] NodeDef mentions attribute shape which is not in the op definition: Op<name=ReadVariableOp; signature=resource:resource -> value:dtype; attr=dtype:type; is_stateful=true> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node add/ReadVariableOp}}\r\n```\r\nThe same message is displayed when we execute the function (it still executes correctly). What about calling the annotation attribute as `_shape`, to avoid this problem?", "> What about calling the annotation attribute as _shape, to avoid this problem?\r\n\r\nGood suggestion, I updated the PR.", "A question: if the tracing code inserted a `ReshapeOp` after the `ReadVariableOp`, then the issue of unknown shape would be resolved without introducing a new attribute - have you considered the alternative?", "@mdanatg Thanks for the suggestion. Actually, in the context where we are seeing this shape inference issue, we already need this custom attribute regardless: the TF-TRT converter for `ReadVariableOp` needs to create an output constant with the right shape, which we can easily get by annotating the node.\r\n\r\nI think, however, that the proposed fix is only a workaround hiding a limitation of shape inference w.r.t input resources.", "I see, thanks.\r\n\r\n> I think, however, that the proposed fix is only a workaround hiding a limitation of shape inference w.r.t input resources.\r\n\r\nI agree. FYI, on the long term, this will be resolvable once we complete the move to FullType, and the variable handle includes shape info (i.e. instead of `DT_RESOURCE`, the input handle is a `TFT_VARIABLE[<dtype>, <shape>]`).", "@Nyrio Could you please recreate this PR? I am trying to manually merge this, but got error \" WARN: Cannot migrate http://github.com/tensorflow/tensorflow/pull/54591 because the following check runs have not been passed: [cla/google]\".\r\nHere is what I was told:\r\nMihai Maruseac, 2 min\r\nHi. CLA process changed since february\r\nI think best is for author to recreate the PR\r\nI cannot seem to be able to trigger the new CLA process on the PR\r\n", "@bixia1 I see that the label `cla:yes` has been added manually and that the `cla/google` check is marked as passed.\r\nLet me know if I still need to reopen the PR or if that issue is solved.\r\n\r\nAlso, a \"Windows Bazel GPU\" build is failing. I don't seem to have access to the build logs, let me know if it's a false negative or if there is something to fix (I would be surprised considering the changes in this PR).", "@Nyrio that didn't work", "Closing due to CLA issues."]}, {"number": 54590, "title": "(lite) Add shim function tflite_shims::internal::verifyFlatBufferAndGetModel and use it from JNI code", "body": "(lite) Add shim function tflite_shims::internal::verifyFlatBufferAndGetModel and use it from JNI code\n\nWhen checking the validity of the model flatbuffer in\nthe Java API implementation, use the new shims function\ntflite::internal::VerifyFlatBufferAndGetModel, rather than\njust directly calling the FlatBuffer routines.\n", "comments": []}, {"number": 54589, "title": "[tfrt:jit] Split tf_jitrt_codegen_transpose into two files.", "body": "[tfrt:jit] Split tf_jitrt_codegen_transpose into two files.\n\nAlso we should not use CodegenStrategy for tiling, because of gml_st.loop\nmigration.\n", "comments": []}, {"number": 54588, "title": "Export the GPU compatibility list flatbuffer schema", "body": "Export the GPU compatibility list flatbuffer schema\n", "comments": []}, {"number": 54587, "title": "[mhlo:linalg] Pass pad sizes as attributes", "body": "[mhlo:linalg] Pass pad sizes as attributes\n\nNo need to wrap them in a ConstantOp.\n", "comments": []}, {"number": 54586, "title": "Add Lowering of RngGetAndUpdateStateOp to use a global memref", "body": "Add Lowering of RngGetAndUpdateStateOp to use a global memref\n", "comments": []}, {"number": 54585, "title": "Use lowering of RngGetAndUpdateStateOp for AOT compilation", "body": "Use lowering of RngGetAndUpdateStateOp for AOT compilation\n", "comments": []}, {"number": 54584, "title": "Define mhlo.rng_get_and_update_state_op to match existing HLO", "body": "Define mhlo.rng_get_and_update_state_op to match existing HLO\n\nThis instruction represents the change of the global random number generator state for rng instructions. The global state is incremented by delta and the old state is returned.\n", "comments": []}, {"number": 54583, "title": "Update test with mock.", "body": "Update test with mock.\n", "comments": []}, {"number": 54582, "title": "huggingface pre_trained model loading takes full gpu memory", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  2.8.0\r\n- Python version: 3.9.7\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): visual studio 2019\r\n- CUDA/cuDNN version: 11.6/8.3.2\r\n- GPU model and memory: RTX3090 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nload from pre-trained transformer takes full gpu memory, so oom happens after training started\r\n**Describe the expected behavior**\r\nappropriate memory allocation\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): add options for loading pre-trained model\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nwith kaggle feedback-prize data\r\nhttps://www.kaggle.com/c/feedback-prize-2021/data\r\nthis code\r\n```\r\nimport os\r\nfrom tqdm import tqdm,trange\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers,Model,Input,Sequential,metrics,regularizers,optimizers,losses,activations,callbacks\r\nfrom transformers import AutoTokenizer,AutoConfig,TFAutoModel\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\r\nfrom copy import copy,deepcopy\r\n\r\nVER=12\r\nEPOCHS=5\r\nN_SPLITS=5\r\nMAX_LEN=1024\r\nNUM_ATTENTION_HEADS=16\r\nROBERTA_LARGE_PATH='../../pretrained/transformers/roberta-large/'\r\nLRS = [1e-4, 1e-4, 1e-4, 1e-5, 1e-5,1e-5]\r\nRUN_TRAINING=False\r\n\r\ntokenizer=AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH+'auto_tokenizer')\r\nconfig=AutoConfig.from_pretrained(ROBERTA_LARGE_PATH+'auto_config')\r\n\r\nbackbone=TFAutoModel.from_pretrained('roberta-large',config=config)\r\n\r\nFEEDBACK_DATA_PATH='../../mldata/Feedback_Prize/'\r\ntrain_df=pd.read_csv(FEEDBACK_DATA_PATH+'train.csv')\r\n\r\nN_ID=train_df.id.nunique()\r\nIDS=train_df.id.unique()\r\n\r\ndiscourse_types=['Lead','Position','Evidence','Claim','Concluding Statement','Counterclaim','Rebuttal']\r\ntarget_map = {k:i for i,k in enumerate(discourse_types)}\r\n\r\ntargets=np.load(f'../kaggle/encoded/targets_{MAX_LEN}.npy',allow_pickle=False)\r\ntrain_tokens=np.load(f'../kaggle/encoded/tokens_{MAX_LEN}.npy',allow_pickle=False)\r\ntrain_attention=np.load(f'../kaggle/encoded/attention_{MAX_LEN}.npy',allow_pickle=False)\r\n\r\ntest_files=os.listdir('../../mldata/Feedback_Prize/test/')\r\nTEST_IDS=[f.replace('.txt','') for f in test_files if 'txt' in f]\r\nprint('There are',len(TEST_IDS),'test texts.')\r\ntest_tokens=np.zeros((len(TEST_IDS),MAX_LEN),dtype=np.int32)\r\ntest_attention=np.zeros((len(TEST_IDS),MAX_LEN),dtype=np.int32)\r\nfor i,id in enumerate(TEST_IDS):\r\n    name=f'../../mldata/Feedback_Prize/test/{id}.txt'\r\n    txt=open(name,'r').read()\r\n    tokens=tokenizer.encode_plus(txt,max_length=MAX_LEN,padding='max_length',truncation=True,return_offsets_mapping=True)\r\n    test_tokens[i]=tokens['input_ids']\r\n    test_attention[i]=tokens['attention_mask']\r\n\r\ndef build_model():\r\n    tokens=Input((MAX_LEN,),name='tokens',dtype=tf.int32)\r\n    attention=Input((MAX_LEN,),name='attention',dtype=tf.int32)\r\n    x=backbone(tokens,attention_mask=attention)\r\n    x1=layers.Dropout(0.1)(x[0])\r\n    x=layers.Dense(15,activation='softmax',dtype=tf.float32)(x1)\r\n    model=Model(inputs=[tokens,attention],outputs=x)\r\n    model.compile(optimizers.Adam(1e-4),losses.CategoricalCrossentropy(),[metrics.CategoricalAccuracy()])\r\n    return model\r\n\r\ndiscourse_types_ext=copy(discourse_types)\r\ndiscourse_types_ext.append('blank')\r\n\r\ndef get_preds(dataset = 'train', verbose = True, text_ids = None, preds = None):\r\n    all_predictions = []\r\n    for id_num in range(len(preds)):\r\n        if (id_num % 100 == 0) & (verbose): print(id_num, ', ', end = '')\r\n        n = text_ids[id_num]\r\n        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'\r\n        txt = open(name, 'r').read()\r\n        tokens = tokenizer.encode_plus(txt, max_length = MAX_LEN, padding = 'max_length', truncation = True, return_offsets_mapping = True)\r\n        off = tokens['offset_mapping']\r\n        w = []\r\n        blank = True\r\n        for i in range(len(txt)):\r\n            if (txt[i] != ' ') & (txt[i] != '\\n') & (blank == True):\r\n                w.append(i)\r\n                blank = False\r\n            elif (txt[i] == ' ') | (txt[i] == '\\n'):\r\n                blank = True\r\n        w.append(1e6)\r\n        word_map = -1 * np.ones(MAX_LEN, dtype = 'int32')\r\n        w_i = 0\r\n        for i in range(len(off)):\r\n            if off[i][1] == 0: continue\r\n            while off[i][0] >= w[w_i + 1]: w_i += 1\r\n            word_map[i] = int(w_i)\r\n        pred = preds[id_num,] / 2.0\r\n        i = 0\r\n        while i < MAX_LEN:\r\n            prediction = []\r\n            start = pred[i]\r\n            if start in [0, 1, 2, 3, 4, 5, 6, 7]:\r\n                prediction.append(word_map[i])\r\n                i += 1\r\n                if i >= MAX_LEN: break\r\n                while pred[i] == start + 0.5:\r\n                    if not word_map[i] in prediction: prediction.append(word_map[i])\r\n                    i += 1\r\n                    if i >= MAX_LEN: break\r\n            else: i += 1\r\n            prediction = [x for x in prediction if x != -1]\r\n            if len(prediction) > 4: all_predictions.append((n, discourse_types_ext[int(start)], ' '.join([str(x) for x in prediction])))\r\n\r\n    # MAKE DATAFRAME\r\n    df = pd.DataFrame(all_predictions)\r\n    df.columns = ['id', 'class', 'predictionstring']\r\n    return df\r\n\r\ndef calc_overlap(row):\r\n    set_pred = set(row.predictionstring_pred.split(' '))\r\n    set_gt = set(row.predictionstring_gt.split(' '))\r\n    len_gt = len(set_gt)\r\n    len_pred = len(set_pred)\r\n    inter = len(set_gt.intersection(set_pred))\r\n    overlap_1 = inter / len_gt\r\n    overlap_2 = inter / len_pred\r\n    return [overlap_1, overlap_2]\r\n\r\ndef score_feedback_comp(pred_df, gt_df):\r\n    gt_df = gt_df[['id', 'discourse_type', 'predictionstring']].reset_index(drop = True).copy()\r\n    pred_df = pred_df[['id', 'class', 'predictionstring']].reset_index(drop = True).copy()\r\n    pred_df['pred_id'] = pred_df.index\r\n    gt_df['gt_id'] = gt_df.index\r\n    joined = pred_df.merge(gt_df, left_on = ['id', 'class'], right_on = ['id', 'discourse_type'], how = 'outer', suffixes = ('_pred', '_gt'))\r\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\r\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\r\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\r\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\r\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\r\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\r\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\r\n    tp_pred_ids = joined.query('potential_TP').sort_values('max_overlap', ascending=False).groupby(['id','predictionstring_gt']).first()['pred_id'].values\r\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\r\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\r\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\r\n    TP = len(tp_pred_ids)\r\n    FP = len(fp_pred_ids)\r\n    FN = len(unmatched_gt_ids)\r\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\r\n    return my_f1_score\r\n\r\nEPOCHS=6\r\n\r\nall_scores=[]\r\noof_preds=np.zeros((N_ID,15))\r\ntest_preds=np.zeros((len(TEST_IDS),MAX_LEN,15))\r\nfor fold,(train_idx,valid_idx) in enumerate(MultilabelStratifiedKFold(n_splits=N_SPLITS,shuffle=True,random_state=777).split(train_tokens,targets[:,0,:])):\r\n    print('#'*25)\r\n    print('### FOLD %i' % (fold + 1))\r\n    print('#'*25)\r\n    \r\n    model=build_model()\r\n    \r\n    def lrfn(epoch):\r\n        return LRS[epoch]\r\n    \r\n    model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),\r\n                callbacks=[callbacks.LearningRateScheduler(lrfn,verbose=True),callbacks.ModelCheckpoint('models/%s-roberta-%i.tf'%(VER,fold),monitor='val_loss',\r\n                                                                                                        save_best_only=True,save_weights_only=True)],\r\n                epochs=EPOCHS,batch_size=8,verbose=1)\r\n    pred=model.predict([train_tokens[valid_idx],train_attention[valid_idx]],batch_size=4,verbose=1)\r\n    print(pred.shape)\r\n    print('predicting OOF...')\r\n    oof=get_preds(dataset='train',verbose=True,text_ids=IDS[valid_idx],preds=np.argmax(pred,axis=-1))\r\n    f1s=[]\r\n    CLASSES=oof['class'].unique()\r\n    for c in CLASSES:\r\n        pred_df = oof.loc[oof['class'] == c].copy()\r\n        valid = train_df.loc[train_df.id.isin(IDS[valid_idx])]\r\n        gt_df = valid.loc[valid['discourse_type'] == c].copy()\r\n        f1 = score_feedback_comp(pred_df, gt_df)\r\n        print(c, f1)\r\n        f1s.append(f1)\r\n    print()\r\n    print('Fold score: ', np.mean(f1s))\r\n    \r\n    test_preds+=model.predict([test_tokens,test_attention],batch_size=4,verbose=1)/N_SPLITS\r\n```\r\n\r\n\r\nerror\r\n```\r\n2022-02-25 20:42:31.859917: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size:\r\n2022-02-25 20:42:31.859950: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 131 Chunks of size 256 totalling 32.8KiB\r\n2022-02-25 20:42:31.859978: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\r\n2022-02-25 20:42:31.860006: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB\r\n2022-02-25 20:42:31.860033: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3840 totalling 3.8KiB\r\n2022-02-25 20:42:31.860061: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 655 Chunks of size 4096 totalling 2.56MiB\r\n2022-02-25 20:42:31.860089: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4352 totalling 4.2KiB\r\n2022-02-25 20:42:31.860117: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4864 totalling 4.8KiB\r\n2022-02-25 20:42:31.860145: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7168 totalling 7.0KiB\r\n2022-02-25 20:42:31.860173: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 66 Chunks of size 16384 totalling 1.03MiB\r\n2022-02-25 20:42:31.860201: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 19200 totalling 18.8KiB\r\n2022-02-25 20:42:31.860228: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 20480 totalling 40.0KiB\r\n2022-02-25 20:42:31.860256: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 24576 totalling 24.0KiB\r\n2022-02-25 20:42:31.860284: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 28672 totalling 28.0KiB\r\n2022-02-25 20:42:31.860311: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 32256 totalling 31.5KiB\r\n2022-02-25 20:42:31.860339: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 32 Chunks of size 32768 totalling 1.00MiB\r\n2022-02-25 20:42:31.860367: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40960 totalling 40.0KiB\r\n2022-02-25 20:42:31.860394: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 48896 totalling 47.8KiB\r\n2022-02-25 20:42:31.860422: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 61440 totalling 180.0KiB\r\n2022-02-25 20:42:31.860450: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB\r\n2022-02-25 20:42:31.860478: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 491520 totalling 480.0KiB\r\n2022-02-25 20:42:31.860506: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2105344 totalling 4.02MiB\r\n2022-02-25 20:42:31.860534: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 289 Chunks of size 4194304 totalling 1.13GiB\r\n2022-02-25 20:42:31.860562: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4578560 totalling 4.37MiB\r\n2022-02-25 20:42:31.860590: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 139 Chunks of size 16777216 totalling 2.17GiB\r\n2022-02-25 20:42:31.860618: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 20971520 totalling 20.00MiB\r\n2022-02-25 20:42:31.860646: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 21258240 totalling 20.27MiB\r\n2022-02-25 20:42:31.860674: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 25530368 totalling 24.35MiB\r\n2022-02-25 20:42:31.860702: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 27955200 totalling 26.66MiB\r\n2022-02-25 20:42:31.860730: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 29360128 totalling 28.00MiB\r\n2022-02-25 20:42:31.860758: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 89 Chunks of size 33554432 totalling 2.78GiB\r\n2022-02-25 20:42:31.860787: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 39534592 totalling 37.70MiB\r\n2022-02-25 20:42:31.860815: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 40222720 totalling 38.36MiB\r\n2022-02-25 20:42:31.860843: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 28 Chunks of size 134217728 totalling 3.50GiB\r\n2022-02-25 20:42:31.860872: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 205885440 totalling 589.04MiB\r\n2022-02-25 20:42:31.860901: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 21 Chunks of size 536870912 totalling 10.50GiB\r\n2022-02-25 20:42:31.860928: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 20.86GiB2022-02-25 20:42:31.860953: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 22729785344 memory_limit_: 22729785344 available bytes: 0 curr_region_allocation_bytes_: 45459570688\r\n2022-02-25 20:42:31.860988: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats:\r\nLimit:                     22729785344\r\nInUse:                     22400043008\r\nMaxInUse:                  22433597440\r\nNumAllocs:                        5211\r\nMaxAllocSize:                536870912\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2022-02-25 20:42:31.861075: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ***************************************************************************************************_\r\n2022-02-25 20:42:31.861123: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:681 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"D:\\repo\\robertatraining\\robertatraining.py\", line 157, in <module>\r\n    model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),\r\n  File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\r\n\r\nDetected at node 'model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul' defined at (most recent call last):\r\n    File \"D:\\repo\\robertatraining\\robertatraining.py\", line 157, in <module>\r\n      model.fit([train_tokens[train_idx],train_attention[train_idx]],targets[train_idx],validation_data=([train_tokens[valid_idx],train_attention[valid_idx]],targets[valid_idx]),\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\r\n      tmp_logs = self.train_function(iterator)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\r\n      return step_function(self, iterator)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\r\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\r\n      outputs = model.train_step(data)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\r\n      y_pred = self(x, training=True)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\r\n      return self._run_internal_graph(\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\r\n      outputs = node.layer(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 996, in call\r\n      outputs = self.roberta(\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 755, in call\r\n      encoder_outputs = self.encoder(\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 528, in call\r\n      for i, layer_module in enumerate(self.layer):\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 534, in call\r\n      layer_outputs = layer_module(\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 443, in call\r\n      self_attention_outputs = self.attention(\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 356, in call\r\n      self_outputs = self.self_attention(\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\r\n      outputs = call_fn(inputs, *args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"C:\\Users\\alanp\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\", line 284, in call\r\n      attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\r\nNode: 'model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul'\r\nOOM when allocating tensor with shape[8,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node model/tf_roberta_model/roberta/encoder/layer_._7/attention/self/MatMul}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\r\n [Op:__inference_train_function_53102]\r\n```\r\n\r\n[8,16,1024,1024] with tf.fload32 is just 512MB total, it doesn't make sense\r\n\r\n\r\njust with \"from_pretrained\", gpu allocate almost all of its memory\r\n![image](https://user-images.githubusercontent.com/4515120/155709476-168583f7-e1cc-495b-8ab3-2a218934d7ac.png)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @alanpurple ! Can you try again after reducing max_length and input size /add more pooling layers in build_model  aside constraining [memory growth](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth) in Gpu. You can also use [tf.data](https://www.tensorflow.org/guide/data) to set a pipe line to load the data in batches.  Attaching relevant [thread](https://stackoverflow.com/questions/63452525/oom-when-allocating-tensor-with-shape1-48-48-1024-and-type-float-on-joblocal) for reference.Thanks!", "@mohantym \r\nsame with tf.data.Dataset\r\n```\r\ntrain_ds=tf.data.Dataset.from_tensor_slices(((train_tokens[train_idx],train_attention[train_idx]),targets[train_idx])).batch(14)\r\n        valid_ds=tf.data.Dataset.from_tensor_slices(((train_tokens[valid_idx],train_attention[valid_idx]),targets[valid_idx])).batch(4)\r\n        model.fit(train_ds,validation_data=valid_ds,\r\n                  callbacks=[callbacks.LearningRateScheduler(lrfn,verbose=True),callbacks.ModelCheckpoint('models/%s-roberta-%i.tf'%(VER,fold),monitor='val_loss',\r\n                                                                                                          save_best_only=True,save_weights_only=True)],\r\n                  epochs=EPOCHS,verbose=1)\r\n        pred=model.predict(valid_ds,verbose=1)\r\n```\r\n\r\n\r\nof course no oom with max_len 256 and batch size 8.\r\n\r\n\r\nso is this not a bug? I don't understand since 14*16*1024*1024 with float32 is just under 1GB and RTX3090 has 24GB of memory", "Hi @alanpurple ! Its not only about one Tensor . It is iterating for all tensors with 1GB memory size while in model.fit() operation.  Thank you for confirmation though. Please move this issue to closed status if it helped. ", "@mohantym \r\nso almost all 24GB of GPU memory being taken right after load pretrained BERT-LARGE is normal?\r\n\r\nIn that case, this is should be closed", "Loading the model should not cause OOM issue. Can you switch to Cuda 11.2 and CuDNN 8.1 and let us know the difference ? Thanks!", "@mohantym \r\n\r\nI think I did have some misconcept, as soon as the program load cuda context, it takes almost  all of gpu's memory, it's normal I guess....\r\n\r\nsorry for misunderstanding( of cudnn )", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54582\">No</a>\n"]}]