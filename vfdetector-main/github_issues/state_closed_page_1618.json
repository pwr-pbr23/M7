[{"number": 4371, "title": "\"HOST_CFG\" is not defined on bazel clean", "body": "During the ./configure step of my build from source, I encounter a bazel error when trying to download \n\n**The main error seems to be**\n\n```\n\nERROR: /home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/protobuf.bzl:91:19: name 'HOST_CFG' is not defined.\n\n```\n\nBuilding with CUDA 8.0, cuDNN 5.1.5, GTX 1060\n\nFull stack:\n\n```\n~/tensorflow ~/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /usr/lib/python2.7/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5\nPlease specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 6.1\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/protobuf.bzl:91:19: name 'HOST_CFG' is not defined.\nERROR: package contains errors: tensorflow/contrib/metrics.\nERROR: error loading package 'tensorflow/contrib/metrics': Extension 'protobuf.bzl' has errors.\nConfiguration finished\n```\n", "comments": ["I'm getting a similar error, below.\n\nI have a GeForce GTX 1080 (hence the 6.1 compute capability), CUDA 8.0.27.1 (the 8.0 RC with patch), cuDNN 5.1.5-1.\n\n```\n$ ./configure\n~/src/tensorflow ~/src/tensorflow\nPlease specify the location of python. [Default is /home/xpe/anaconda3/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /home/xpe/anaconda3/lib/python3.5/site-packages\nPlease input the desired Python library path to use.  Default is [/home/xpe/anaconda3/lib/python3.5/site-packages]\n\n/home/xpe/anaconda3/lib/python3.5/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nInvalid path to cuDNN  toolkit. Neither of the following two files can be found:\n/usr/local/cuda-8.0/lib64/libcudnn.so\n/usr/local/cuda-8.0/libcudnn.so\n\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]:  \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 6.1\nExtracting Bazel installation...\n.\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.\nERROR: /home/xpe/.cache/bazel/_bazel_xpe/641dfc594d651643762f515addd22118/external/protobuf/protobuf.bzl:91:19: name 'HOST_CFG' is not defined.\nERROR: package contains errors: tensorflow/tools/proto_text.\nERROR: error loading package 'tensorflow/tools/proto_text': Extension 'protobuf.bzl' has errors.\nConfiguration finished\n```\n", "I build bazel from source. @kamal94 I would expect that you did as well.\n\nAs an early guess, I think I may have built from master of bazel and it may have a few bugs. (For example, if you look at https://www.bazel.io/versions/master/docs/skylark/lib/globals.html, you'll see that HOST_CFG is not listed as a global, but it was before: https://webcache.googleusercontent.com/search?q=cache:0KuaX69EkvcJ:https://www.bazel.io/versions/master/docs/skylark/lib/globals.html+&cd=3&hl=en&ct=clnk&gl=us)\n\nHere is what may be the source of the line 91 protobuf error:\n\nhttps://github.com/bazelbuild/bazel/blob/8d9fc9aef82d82076cfea2db010a3036d871027e/third_party/protobuf/3.0.0/protobuf.bzl#L91\n", "I see in the google-cache version of the docs I linked above that HOST_CFG got deprecated:\n\n> ConfigurationTransition\n> \n> Deprecated. Use string \"host\" instead. Specifies a transition to the host configuration.\n", "I made a PR for Bazel to change HOST_CFG to \"host\", here: https://github.com/bazelbuild/bazel/pull/1769\n", "Closing this out as it appears resolved. Please re-open if the PR for Bazel mentioned by @xpe does not address this issue.\n", "@xpe I tried installing bazel from source from your repository and it still didn't work, same error. I think it might have something to do with bazel fetching protobuf from github.\n\nHowever, this is really confusing because protobuf [fixed their cfg flags](https://github.com/google/protobuf/pull/2100), and they don't have any HOST_CFG in their code base anymore. So I don't see why bazel would get a version of protobuf.bzl that has HOST_CFG in it. Maybe it's not pulling from the most recent github version?\n\nTo make this clearer, look at the file where the error occurs, for me it is:\n /home/kamal/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/protobuf.bzl\nfor you it seems to be (at least in your most recent posted error message):\n/home/xpe/.cache/bazel/_bazel_xpe/641dfc594d651643762f515addd22118/external/protobuf/protobuf.bzl\n\nThis file contains the cfg = HOST_CFG, while the [protobuf.bzl](https://github.com/google/protobuf/blob/master/protobuf.bzl) in the head of protobuf contains \"host\"\n\nAm i getting something wrong? Why isn't bazel downloading from master/head? if it says it's cloning it.\n", "@kamal94 @xpe I had the same problem as Kamal about 10 hours ago, even with a Bazel patch similar to @xpe's PR.\n\nI have just pulled TF (02a8d07) and Bazel (467e716), applied again the Bazel patch, everything is fine now.\n", "@ic  I still couldn't get it to work. \nWhat i did:\n1. git clone this repository \n2. git clone bazel\n3. replace all \"HOST_CFG\" with \"host\" in the entire directory\n4. ./compile the bazel directory\n5.removed the old bazel binary from /usr/bin\n6. copied the new bazel binary in output/ to /usr/bin\n\nI still get the error, and bazel still attempts to clone the protobuf github repository and install from there. \n\nAm I missing something?\n\nWhile compiling bazel, it seemed to also have compiled protobuf, but I don't see any other binary output in /output. Perhaps there are some binaries for protobuf that I could also copy to bin?\n", "> 1. replace all \"HOST_CFG\" with \"host\" in the entire directory\n\nMy changes are basically the same as the PR by @xpe. Have you changed anything else?\n\nPerhaps need for a `bazel clean` before trying to recompile.\n\nIn the compilation log, protobuf` is also cloned and built from source here. Note I have succeeded on Darwin only---not tried yet on Linux.\n\nJust to be sure: Do you use the exact same commits as the ones I suggested, both for Bazel and TensorFlow?\n", "@andydavis1 This is still an issue. Even the bazel fix didn't do it for me. For some reason, it is till cloning an old version of Bazel evertyime it runs `bazel fetch //...` in the .config file.\n\nit **should** be pulling [this file](https://github.com/google/protobuf/blob/master/protobuf.bzl) but it is not! It is pulling a similar (older) file with flag set to HOST_CFG.\n\nIs there a way to change this so that bazel clones the most recent (and therefore non-deprecated) version of protobuf?\n\nThe only manual solution I could come up with was running the config file, and when bazel crashed because of the above error, i manually changed the file at the specified cache location (for me this was /home/_myname_/.cache/bazel/_bazel_kamal/f9ae4eca457b390bb2ebe780caca64e0/external/protobuf/protobuf.bzl) and replaced all occurrences of HOST_CFG to \"host\".\n\nThe installation runs well after that until I run into #4312\n", "@kamal94 Thanks for your feedback and updates. I'm hopeful that L\u00e1szl\u00f3 Csomor's changes move it in the right direction. I'll be doing another installation soon, also, and will keep an eye on it.\n", "@xpe I was able to trace the error.\nIt was in the [bazel workplace installation file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L101) and it pulled an older archive of protobuf. This old version had HOST_CFG instead of \"host\".\n\nTo fix this, I replaced the lines\n\n```\nnative.http_archive(\n    name = \"protobuf\",\n    url = \"http://github.com/google/protobuf/archive/v3.0.2.tar.gz\",\n    sha256 = \"b700647e11556b643ccddffd1f41d8cb7704ed02090af54cc517d44d912d11c1\",\n    strip_prefix = \"protobuf-3.0.2\",\n  )\n```\n\nwith \n\n```\n  native.git_repository(\n      name = \"protobuf\",\n      remote = \"https://github.com/google/protobuf/\",\n      commit = \"a289d43\"\n  )\n```\n", "@kamal94 Good find. \n\nMy quick, general commentary: Looking at the file, it would appear that the convention is to pin specific versions for reproducibility. Keeping them up-to-date sounds like classic dependency management pain. I don't know bazel well, but I hope it would have tooling to automatically check for newer versions.\n\nA recommendation, if possible: In this kind of situation, pointing to a particular artifact or git hash works. However, if possible, using a human-readable tag name with a version is probably more maintainable. Perhaps others more [learned](https://en.oxforddictionaries.com/usage/learnt-vs-learned) with bazel could weigh in.\n", "same problem , thank you @kamal94 \n", "I installed Bazel from the macOS installer \r\n\r\n`./bazel-0.5.2-installer-darwin-x86_64.sh --user`\r\n\r\nand I have the same problem when trying to build something (a fork of the Magenta repo, in my case): \r\n\r\n`ERROR: /private/var/tmp/_bazel_epnichols/3e6515dd3746b8d7b7b2bef4294514b6/external/protobuf/protobuf.bzl:91:19: name 'HOST_CFG' is not defined.`\r\n\r\nA bit more info:\r\n```\r\nbash bazel version\r\n\r\nBuild label: 0.5.2\r\nBuild target: bazel-out/darwin_x86_64-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 27 13:17:02 2017 (1498569422)\r\nBuild timestamp: 1498569422\r\nBuild timestamp as int: 1498569422\r\n```\r\n\r\nIs this actually broken in the prebuilt installer, or is it just me?"]}, {"number": 4370, "title": "[Test]Fix the array type bug", "body": "Fix the `PS_ARRAY` and `WORKER_ARRAY`  bug. They should be arrays, but in fact they are strings. The problem is the wrong way for assigning bash array. \nThe log info before fixing:\n\n```\n2 worker process(es) running in parallel...\nWorker 0: \n  WORKER HOST: localhost:2223 localhost:2224 \n  log file: /tmp/worker0.log\nWorker 1: \n  WORKER HOST: \n  log file: /tmp/worker1.log\n```\n\nAfter:\n\n```\n2 worker process(es) running in parallel...\nWorker 0: \n  WORKER HOST: localhost:2223\n  log file: /tmp/worker0.log\nWorker 1: \n  WORKER HOST: localhost:2224\n  log file: /tmp/worker1.log\n```\n", "comments": ["Can one of the admins verify this patch?\n", "@DjangoPeng, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq to be a potential reviewer\n", "@tensorflow-jenkins Test this please.\n", "@caisq Please check and merge\n", "Can one of the admins verify this patch?\n", "A mistake of branch! Reopen a new PR [#4464](https://github.com/tensorflow/tensorflow/pull/4464)\n"]}, {"number": 4369, "title": "Change protobuf::uint64 to protobuf_uint64 in tensor_coding.cc", "body": "", "comments": ["@andrewharp, thanks for your PR! By analyzing the annotation information on this pull request, we identified @mrry and @tensorflower-gardener to be potential reviewers\n", "@tensorflow-jenkins Test this please.\n", "Change LGTM (though FYI this has already been fixed internally, by @danmane I think).\n", "@mrry Saw that, but it's giving me build errors internally -- I think the sync script is confused because the issue was fixed as part of the previous pull CL and it's trying to turn those _'s back into ::'s.\n"]}, {"number": 4368, "title": "Building issues", "body": "```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR: The specified --crosstool_top '@local_config_cuda//crosstool:CROSSTOOL' is not a valid cc_toolchain_suite rule.\n```\n", "comments": ["I'm having the same issue, compiling on CentOS 7 using Cuda 7.5. If I compile without `--config=cuda` it builds and I can run it in CPU, but of course, I'd like to be able to run on GPU.\n\nAny idea on what the problem could be?\n", "Could it be that Tensorflow relies on an older version of bazel to build for GPU (i.e. passing `--config=cuda`)? I say this because in [this bazel changelog](https://github.com/bazelbuild/bazel/blob/master/CHANGELOG.md). In version 0.2.3 (2016-05-10), there's an entry that reads:\n\n> The key for the map to cc_toolchain_suite.toolchains is now a string of the form \"cpu|compiler\" (previously, it was just \"cpu\").\n\nI then decided to compile bazel 0.2.2 (I was previously using 0.3.1) from source, and was finally able to successfully build with `--config=cuda`.\n\nI guess something should be changed somwhere in tensorflow to use the new key format, so it can build with the latest bazel.\n", "Same issue here. Using an older version of bazel works for r0.1.0, but I need a more recent version (we have multiple versions of gcc in our systems, and the most recent one is not in `/usr/local/bin`), which unfortunately requires bazel 0.3.1.\n\nAny solution?\n", "Having this problem with both 0.2.3 and 0.3.1.  Gonna try 0.2.2, but this is kinda bad, right?  Anyone have an explanation of why?  This is the first time I've used bazel.\n", "Can you try again with TensorFlow at HEAD? #4285 made the CUDA CROSSTOOL file a proper `cc_toolchain_suite`.\n", "This is still an issue for r0.10.\n", "I am using r0.10, when building tf with `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer` after configuring. \n\nIt throws an error.\n![image](https://cloud.githubusercontent.com/assets/3538629/20136438/c180776c-a6af-11e6-9a8f-258726a8e9d2.png)\n", "Is this still a problem at master or 0.12?\r\n", "Closing due to inactivity.\r\nAll our continuous builds and my local tests are clear of this problem. Please file a new issue if the problem persists.", "I am facing the same issue. Does someone know how to fix it ?", "Is it possible you forgot to run `./configure` before bazel build?", "I meet the same issue. I built bazel( 0.4.3) from a old bazel ( 0.4.0).  Then ./configure on Tensorflow, then use 0.4.3 bazel to compile. Got the issue ,\r\nERROR: The specified --crosstool_top '//third_party/gpus/crosstool:crosstool' is not a valid cc_toolchain_suite rule.", "Any bazel version older than 0.4.2 will not work, as there has been some backwards incompatible changes to our build files. Not sure what you mean with you built bazel 0.4.3 from 0.4.0.", "Hello I have the issue with my configuration since I updated cud from 7.5 to 8.0.\r\nMy tensorflow version is r0.10,  cudnn version is 5 and hazel is 0.3.2.\r\n\r\nAny idea how to solve this issue ? ", "Using a lower version of bazel can solve this problem (e.g bazel 0.2.2)"]}, {"number": 4367, "title": "Strange internal error after switching from python 3 to python 2", "body": "I'm trying to port my code from python 3 to 2. However, I'm getting a strange error when running with python 2: `InternalError: Unsupported feed type`. This happens after trying to `sess.run` with a particular feed dict. What's odd is that I can run the following just fine:\n\n```\nfor k, v in feed_dict.items():\n    sess.run(k, {k:v})\n```\n\nThat is, the individual arrays and tensors in the feed dict are all fine. I apologize for not being able to come up with a short example that reproduces the error, but perhaps someone might have some idea of what's going on?\n", "comments": ["We need a reproducible test case (and other information requested in the template) that demonstrate the issue. Short of that, you might try asking this question on StackOverflow using tag 'tensorflow' to see if others might have a quick answer for you.\n"]}, {"number": 4366, "title": "Fixed DeepDream example for Python 3.5", "body": "The DeepDream example currently fails on Python 3.5 with\n\n```\n  <ipython-input-4-abee32f79a5a> in show_graph(graph_def, max_const_size)\n     35     if hasattr(graph_def, 'as_graph_def'):\n     36         graph_def = graph_def.as_graph_def()\n---> 37     strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n     38     code = \"\"\"\n     39         <script>\n\n<ipython-input-4-abee32f79a5a> in strip_consts(graph_def, max_const_size)\n     18             size = len(tensor.tensor_content)\n     19             if size > max_const_size:\n---> 20                 tensor.tensor_content = (\"<stripped %d bytes>\"%size )\n     21     return strip_def\n     22 \n\nTypeError: '<stripped 37632 bytes>' has type <class 'str'>, but expected one of: (<class 'bytes'>,)\n```\n\nThis fix converts the string to bytes before calling strip_consts.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins Test this please.\n"]}, {"number": 4365, "title": "Build tensorflow from source with CUDA failed", "body": "I am trying to build tensorflow in Ubuntu 14.04 but failed. My steps:\n1. git rev-parse HEAD\n   c715c3102df1556fc0ce88fc987440a3c80e5380\n2. bazel version\n   Warning: ignoring _JAVA_OPTIONS in environment.\n   Another command is running (pid = 13101).  Waiting for it to complete...\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Thu Jan 01 00:00:00 1970 (0)\n   Build timestamp: Thu Jan 01 00:00:00 1970 (0)\n   Build timestamp as int: 0\n3. ./configure \n   Please specify the location of python. [Default is /usr/bin/python]: \n   Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\n   No Google Cloud Platform support will be enabled for TensorFlow\n   Do you wish to build TensorFlow with GPU support? [y/N] y\n   GPU support will be enabled for TensorFlow\n   Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \n   Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\n   Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \n   Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5\n   Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \n   Please specify a list of comma-separated Cuda compute capabilities you want to build with.\n   You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\n   Please note that each additional compute capability significantly increases your build time and binary size.\n   [Default is: \"3.5,5.2\"]: \n   Warning: ignoring _JAVA_OPTIONS in environment.\n   ..\n   INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n   Warning: ignoring _JAVA_OPTIONS in environment.\n   .\n   WARNING: /home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/boringssl_git/WORKSPACE:1: Workspace name in /home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/boringssl_git/WORKSPACE (@boringssl) does not match the name given in the repository's definition (@boringssl_git); this will cause a build error in future versions.\n   INFO: All external dependencies fetched successfully.\n   Configuration finished\n4. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n   external/zlib_archive/zlib-1.2.8/gzread.c:591:5: warning: implicit declaration of function 'close' [-Wimplicit-function-declaration]\n    ret = close(state->fd);\n    ^\n   ERROR: missing input file '@local_config_cuda//cuda:lib64/libcudnn.so.5':/home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/local_config_cuda/cuda/lib64/libcudnn.so.5 (Permission denied).\n   ERROR: /home/yu/.cache/bazel/_bazel_yu/520915e94204a8ab6d3b139e9ec5cf67/external/re2/BUILD:9:1: C++ compilation of rule '@re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\n   ERROR: /home/yu/workspace/tensorflow/tensorflow/cc/BUILD:199:1: //tensorflow/cc:tutorials_example_trainer: missing input file '@local_config_cuda//cuda:lib64/libcudnn.so.5'.\n   Target //tensorflow/cc:tutorials_example_trainer failed to build\n   Use --verbose_failures to see the command lines of failed build steps.\n   ERROR: /home/yu/workspace/tensorflow/tensorflow/cc/BUILD:199:1 1 input file(s) do not exist.\n\nThe libcudnn.so.5 exist and I can build and run other cudnn code. How can I fix the error?\n", "comments": ["Try actually repeating the default `/usr/local/cuda` when it asks you for the library path during configure. You may also have to / want to fix your library path, i. e. `export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:$LD_LIBRARY_PATH`\n", "I was able to reproduce this problem locally with cuda8.0, cudnn5.1 on ubuntu 16.04\nFor me, it actually also complained about my header files not being there (permission denied)\nUntil we get to the bottom of this, I was able to create a workaround by making my /usr/local/cuda world read/writeable\n$ sudo chmod 777 -R /usr/local/cuda\nWith this, compilation was OK with my cuda libraries.\nI suspect this is a bazel issue?\n@damienmg Why does bazel want my cuda headers and .so files to be writeable.\n", "Sorry just realized I haven't answered that bug. It should definitely not require it.\n\nMaybe @davidzchen has some clues about that. I have no idea what is going wrong there.\n", "Wait is that on OS X ? With Bazel HEAD?\n", "Disregard last message, it is not with OS X. Though know the Bazel version would be great.\n", "For completeness to repro:\n- A full repro to get the environment (I still haven't installed cudnn on my machine, sorry for that). Maybe a docker container?\n- What does `ls -l $(bazel info output_base)/external/local_config_cuda/cuda/lib64/libcudnn.so.5` returns after the failure?\n", "I ran into the problem with bazel 0.3.1\nUnfortunately, we completely missed this with docker, because in a docker container we log in as root, and everything is read/writable for us.\n\nI will uninstall and reinstall cuda8 and cudnn5 to see if I can go back to the broken state of things for me.\n", "Ok I need to try it out I guess on latest ubuntu? Will do as soon as I can free some bandwidth.\n", "Saw this issue on 14.04\nTrying to reproduce by breaking my environment, will try to come up with detailed instructions.\n", "@apollos I ran into this issue before, but now I cannot reproduce it on my machine.\nIf you sync to head, can you still see the problem?\n\nIf not, can we close this issue?\n", "Sorry for my late. I update the bazel and then the issue fixed. \n"]}, {"number": 4364, "title": "Fix to allow compilation for Android API 19", "body": "This is a (very) small change to allow compiling TensorFlow for Android API 19. Without this change, you get `tensorflow/core/platform/posix/net.cc:44:51: error: 'errno' was not declared in this scope` when compiling the example Android project with the `WORKSPACE` `android_ndk_repository` parameter `api_level` set to 19. This change resolves that error and allows compilation for this target.\n", "comments": ["@steverichey, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ebrevdo to be a potential reviewer\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "LGTM.\n"]}, {"number": 4363, "title": "tensorflow:nightly-devel-gpu is broken", "body": "Commit [\"Fix nightly docker builds\"](https://github.com/tensorflow/tensorflow/commit/414a5ddd47bba68661eaac08d5b6b7c92c38b808) broke the nightly builds :)\n\n```\n$ nvidia-docker run --rm -ti tensorflow/tensorflow:nightly-devel-gpu nvidia-smi\nNVIDIA-SMI couldn't find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system.\nPlease also try adding directory that contains libnvidia-ml.so to your system PATH.\n```\n\nYou shouldn't override the existing value of `LD_LIBRARY_PATH`, we use it to point to the path where we mount the driver libraries mounted by nvidia-docker, see [here](https://github.com/NVIDIA/nvidia-docker/blob/master/ubuntu-14.04/cuda/7.5/runtime/Dockerfile#L38)\n\ncc @3XX0 @caisq \n", "comments": ["@caisq could you please take a look at this. Perhaps set LD_LIBRARY_PATH to also include the previous LD_LIBRARY_PATH i.e.  maybe\n\n```\nENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:${LD_LIBRARY_PATH }\n```\n", "Was fixed by 60bb54e3111daf5a57008c97db05a91a93101fd7, closing.\n"]}, {"number": 4362, "title": "CUDA 8.0 Failed to configure", "body": "Dear everyone, I recently met this problem when I try to configure this problem, I am using CUDA8.0, Bazel 0.3 and Ubuntu 14.04.\nERROR: /home/haixi/tensorflow-master/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):\n    File \"/home/haixi/tensorflow-master/tensorflow/tensorflow.bzl\", line 562\n        rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n    File \"/home/haixi/tensorflow-master/tensorflow/tensorflow.bzl\", line 568, in rule\n        attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\n\nHope that you can help me !\n", "comments": ["I tried to update the bazel and the mistake still happened....\n", "Can you post how you configured tensorflow?\n", "This error was caused by https://github.com/tensorflow/tensorflow/commit/7bcdcbbf60fc08346fd8016270a0563f4b51362b which broke old versions of Bazel. The simple fix is to just upgrade to the latest Bazel release.\n", "I also encountered this problem, could you tell me how to solve it ?\n", "Try upgrading to Bazel 0.3.1.\n"]}, {"number": 4361, "title": "Update tf.contrib.layers.batch_norm() docs", "body": "_Tensorflow version that I use : 0.10 (pip package)_\n\n---\n\nI took heavy use of _[tf.contrib.layers.batch_norm()](https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100)_ the last weeks. \n\nAfter facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:\n- https://github.com/tensorflow/tensorflow/issues/1122\n- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n\nI would suggest to do following improvements to make it more clear:\n\n**1) Update example in doc-string:**\n\nThe example tells in case we use _update_collections_ on its defaults, we have to include this:\n\n```\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.group(update_ops)\n    total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n```\n\nBut this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:\n\n```\nfrom tensorflow.python import control_flow_ops\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.tuple(update_ops)\n    total_loss = control_flow_ops.with_dependencies(updates, total_loss)\n```\n\nAs a side question, why do we apply it to the _total_loss_, and not to the train_op directly, as described in the doc-string text. Added a dependency to _total_loss_ works, but grouping it with the _train_op_ would make the example more clear in my opinion, because we do batch-statistic updates only during training.\n\n**2) _UPDATE_OPS_ in combination with reuse varscope:**\n\nThis is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to _UPDATE_OPS_ nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?\nOr is it required to filter the update-ops after collecting them with `update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)`, so that each one is executed just once?\n\nTo sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:\n\n```\nif not reuse:\n    # Collect the updates to be computed later.\n    ops.add_to_collections(updates_collections, update_moving_mean)\n    ops.add_to_collections(updates_collections, update_moving_variance)\n```\n\nIn my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling `tf.get_collection(tf.GraphKeys.UPDATE_OPS)`. As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.\n\n**3) Handling of _is_training_ parameter:**\n\nI have seen a lot of examples people doing something like this in their code to handle the _is_training_ parameter:\n\n```\ndef batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=True)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=False)\n    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return bn\n```\n\nAs far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.\n\n**4) Usage on Multi-GPU configuration**\n\na) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).\n\nb) When I use _tf.contrib.batch_norm()_ within a multi-GPU system, I get an error like this:\n\n```\nInvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': \nCould not satisfy explicit device specification '/device:GPU:1' because no supported kernel \nfor GPU devices is available.\n...\n```\n\nHence, to we have to wrap evey _batch_norm()_ call with _tf.device(\"/cpu:0\")_? I guess this might have bad impacts on performance, right?\n\nThanks!\n\n_PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know..._\n", "comments": ["Agree, I believe there is bug in batch_norm.\n", "With bug in batch_norm, which point's of my list do you actually mean? And could you propose any workaround?\n", "Dont know why, I cannot do multi-gpu training when batch_norm moving_avg is applied, but when I update my tf to master version and update my cuda,cudnn, the problem go away.\n", "@shlens Could you take a look at this? Thanks.\n", "@bsautermeister would you have a suggested edit on the docstring that would make the layer more clear?\n\n@argman@, it sounds like your error is fixed, correct?\n", "@shlens , yes, I just update tf to the newest\n", "Is reuse=True working? Whenever I'm trying 'reuse=True' I get errors like - \"Variable norm0/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\" I'm following the docstring and providing the 'scope' too. As far as I understand, when a variable is to be created using tf.get_variable() and reused, first, it has to be created and then its reuse is to be enabled by using - tf.get_variable_scope().reuse_variables().\nWithout \"reuse=True\" in 'tf.contrib.layers.batch_norm()', I think, the right moving mean and variances will not be restored.\nI'm using twnsorflow version 0.11\n\nPlease inform me if this is not the right place to raise this issue. I got to it from https://github.com/tensorflow/tensorflow/issues/1122\n", "I have the same issue as @dasabir when trying to reuse a batch_norm layer within a variable scope.\n", "For (2), I agree with @bsautermeister because as I believe adding dependences on `train_op` looks sound. For some reasons, one may compute loss value (i.e. forward-prop) for validation datapoints; but with dependences on `loss` batch-normalization statistics are also taken from validation set.\n\nFor (3), do we need to share the BN parameters for `bn_train` and `bn_inference`? (in the original code different BN variables like beta, gamma are present for those two)\n\n``` diff\n def batch_norm_layer(x, train_phase, scope_bn):\n   bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n-  updates_collections=None, is_training=True)\n+  updates_collections=None, is_training=True, scope=scope_bn)\n   bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n-  updates_collections=None, is_training=False)\n+  updates_collections=None, is_training=False, scope=scope_bn, reuse=True)\n   bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n   return bn\n```\n\nNOTE: I simply ignored the invalid moving average/variance update in the code for simplicity.\n", "@dasabir and @jfsantos I had same issue. But by speficying the scope_name for batch_norm, the issue was fixed. Under a scope with reusable=True, `tf.contrib.layers.batch_norm(x)` will always create new norm_variables and make them reusable which gives you the error. One thing you can do it is to specify the norm_scope name like this `tf.contrib.layers.batch_norm(x, scope=\"name\")`. When you reuse this norm layer, just do `tf.contrib.layers.batch_norm(x, scope=\"name\", reuse=True)` or use `tf.contrib.layers.batch_norm(x, scope=\"name\")` under a reusable scope. Hope this is helpful.\n", "I noticed that the docs haven't been updated yet. Would it be useful if the docs instead said:\r\n```python\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(total_loss)\r\n```\r\n\r\nAs for proper reuse across multiple data streams, it looks like a shareable version is still [in the works](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/normalization.py#L200). \r\n\r\nAs an aside, to the best of my understanding, the notion of a shareable BN layer should be treated with some care. Depending on the use-case, I think there should be an option to distinguish sharing of the moving averages from the sharing of the beta/gamma parameters [as noted here](https://arxiv.org/pdf/1603.09025v4.pdf).", "Is this still a problem with `tf.nn.batch_norm`?", "Closing due to lack of recent activity. Please update the issue if it persists and we will reopen.", "When you use batch normalization  across multi gpus, how to update variance?", "I solve the problem of reusing batch_normalization by specifying reuse=False when first creating bn(I use slim, but it's same to tf.layers.batch_normalization):\r\n```\r\nscope = tf.get_variable_scope()\r\nbn1 = slim.batch_norm(input1, decay=0.9, reuse=False, scope=scope, is_training=is_training)\r\nbn2 = slim.batch_norm(input2, decay=0.9, reuse=True, scope=scope, is_training=is_training)\r\n```\r\nYou have to specify reuse=False at your first time to create parameters in batch normalization. Or you will get the error info:\r\nVariable cnn/block1/conv1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n", "I obey @wjiangcmu 's advice, it works.\r\nthe code:\r\n33         self.is_training = tf.placeholder(tf.bool, name='MODE')\r\n// first use:\r\n 94         self.img_bn1 = tf.cond(self.is_training,\r\n 95                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = False),\r\n 96                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True))\r\n\r\n// add update_ops before second ruse, and filter out unrelated update_ops(unrelated moving mean and variance)\r\n126         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n127         print('update_ops')\r\n128         for key in update_ops:\r\n129             print(key)\r\n131         i2t_update_extra_ops = [elem for elem in update_ops if 'text_feature/attention' not in elem.name]\r\n\r\n// second use:\r\n132         self.img_neg_bn1 = batch_norm(self.img_neg_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True)\r\n\r\n// weight update and dependent extra_ops(moving mean and variance)\r\n242         self.i2t_optimizer = tf.train.GradientDescentOptimizer(learning_rate )\r\n243         i2t_update_grads = self.i2t_optimizer.minimize(self.i2t_loss)\r\n244 \r\n245         i2t_train_ops = [i2t_update_grads] + i2t_update_extra_ops\r\n246         self.i2t_updates = tf.group(*i2t_train_ops)\r\n\r\nin addition,  in order to update each batch_norm only once, according to @bsautermeister 's \"UPDATE_OPS in combination with reuse varscope\", I add the update_ops before the second use each batch_norm, and filter out unrelated update_ops.\r\n\r\nHope this will be helpful for others."]}, {"number": 4360, "title": "Branch 133020769", "body": "Pushing internal changes.\n", "comments": ["@tensorflow-jenkins Test this please.\n", "@tensorflow-jenkins Test this please.\n"]}, {"number": 4359, "title": "Feature Request: plug-in support for new devices", "body": "We need to be able to support using new devices in TensorFlow without requiring editing the TensorFlow binary source code.\n\nThis requires a few changes.  Among many others:\n- The ability to dynamically register an implementation / factories for alternative devices\n- New or better APIs for allowing OpKernel's to use arbitrary device resources (e.g., right now we assume either the use of StreamExecutor or EigenDevice implementations, but that's obviously not general).\n", "comments": ["Few questions.\n\n## About Feature Request\n\nBy dynamically, I assume you are referring to at compile time, but self-contained code changes. If you mean at runtime via dynamic module loading, I don't know off hand how to do this, but I think we can figure it out.\n\nHow do you wish to handle the kernel code? Should I plan to have new hardware supported by developers modifying the `tensorflow/core/kernels/*` and registering the new hardware? Or do you want this also separated. Although this currently is handled with macros and registration systems in the C++, but I ask from a build perspective. We could make a new folder that contains all the code for extra devices and contains all the build scripts over there. It this is done, I can write template files that have all the basic structural code (many projects do this for configuration scripts) that a user could basically rename and be ready to go.\n\n## About TensorFlow\n\n[TensorFlow Device Contexts, Streams, and Context Switching](http://stackoverflow.com/questions/39481453/tensorflow-device-contexts-streams-and-context-switching)\n", "Yes, I do mean self-contained code changes: a user should be able to use an existing binary install of TensorFlow that doesn't contain the custom device code, but then call a function to \"load a device\" and then another function to \"load the kernels\" for that device.\n\nThe latter is already possible for existing devices via the tf.load_op_library() mechanism, so theoretically something similar could be done for a new tf.load_device().\n\nI'll answer your other question on SO, but I don't think the answer there will be instructive for new devices.  Every device has its own execution model, and so the way the device code is written needs to take into account the execution model.  On CPU, the ThreadPoolDevice is just a device that uses Eigen's multi-threaded CPU support to implement operations.  On GPU, we have to manage streams and execution order carefully because GPU devices are asynchronous, not synchronous.\n\nIf you told us more about the execution model of your hardware, we might be able to suggest something more relevant -- it's likely that copying the CPU or the GPU way of doing things is not the right solution.\n", "I need to talk with my supervisor before giving too many details. [Here](https://www.knupath.com/products/hermosa-processors/) is the public info, but as a high level understanding, the chip is basically a cluster computer reduced to fit on a single chip. The many DSPs need to work together to do anything useful.\n\nI didn't know about the operation interface! Its pretty awesome and I definitely think that is what I want to build.\n\nIt would seem that at a minimum, a developer would need to write an Allocator, Device, DeviceFactory, and DeviceContext. This would give a non-functional device because there are no kernels registered to it. As I was developing, I noticed that some kernels seemed to be core functions like ConstOp, AssignOp, NoOp, etc. that are needed for other things to work. It would seem that a user wouldn't want to code these explicitly as they are kind of obvious and redundant. Do you think these can/should be automatically built into the framework so that every device at least has these working out of the box?\n", "Yes, those four pieces are the minimal requirements just to get the basics working, and then you'd have to register kernels for every op that was supported.\n\nYou're right that some ops are probably trivially implementable as long as some of the basics above are implemented.  We'd have to think about how to 'auto register kernels' for all devices.  However, for things like NoOp, it shouldn't be too hard: it would just be an include and a registration like in https://github.com/tensorflow/tensorflow/blob/5df4c71c86b28c2a4dd746bd67f00fc0281bd24f/tensorflow/core/kernels/no_op.cc#L21\n", "How do I add headers to be installed into the TensorFlow include path? And how about for the .proto files? I need to \"compile\" them to headers and have them installed\n\nI need several framework and common_runtime headers to compile device code. I tried to look through the Bazel files, but couldn't find anything obvious.\n\nEDIT:\n\nI tried a workaround of just setting my include path to the root of the Git repo, but found that things like `#include \"unsupported/Eigen/CXX11/Tensor\"` are being included. These would seem to be unnecessary for a \"general\" device. What should I do about this? I figure my options are: just include them (what I will do for the mean time), or write stripped down headers that contain only the minimal necessary to construct a device. IMO the second option is nicer for long term, but certainly more annoying to do.\n\nEDIT 2:\n\nAlso, the Eigen files don't appear to have header guards and cause recurrent includes\n", "FYI. I got a fake device working with contexts, factories, kernels and all, so I will begin to try to make it self contained. I posted various questions to StackOverflow and things. Those are more educational. The library problem above is more important for the purposes of this issue.\n\nThanks a bunch!\n", "1) At the moment, unsupported/Eigen/CXX11/Tensor  is required by the framework -- it's how you can map our tensor.h Tensor into typed and shaped tensors.\n\n2) Adding @keveman about how to add TF headers to include path.  I thought https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#setting-up-tensorflow-for-development might be relevant, but I'm not sure.\n", "I got all the code down to one file with no changes to the main code base except the below. I list my solution ideas, but they involve modification of the main code so I wanted to make sure its okay.\n\n```\ndiff --git a/tensorflow/core/util/device_name_utils.cc b/tensorflow/core/util/device_name_utils.cc\nindex 5816dbd..50309ed 100644\n--- a/tensorflow/core/util/device_name_utils.cc\n+++ b/tensorflow/core/util/device_name_utils.cc\n@@ -162,6 +162,16 @@ bool DeviceNameUtils::ParseFullName(StringPiece fullname, ParsedName* p) {\n       }\n       progress = true;\n     }\n+    if (str_util::ConsumePrefix(&fullname, \"/kpu:\") ||\n+        str_util::ConsumePrefix(&fullname, \"/KPU:\")) {\n+      p->has_type = true;\n+      p->type = \"KPU\";  // Treat '/kpu:..' as uppercase '/device:KPU:...'\n+      p->has_id = !str_util::ConsumePrefix(&fullname, \"*\");\n+      if (p->has_id && !ConsumeNumber(&fullname, &p->id)) {\n+        return false;\n+      }\n+      progress = true;\n+    }\n\n     if (!progress) {\n       return false;\n```\n\nTo get rid of this one, I was thinking of using the [existing registration](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/device_factory.cc#L44), but that means exposing this function. I could just make the underlying `std::unordered_map` a private/protected member of the DeviceFactory class and then make DeviceNameUtils a friend. Or just expose the function handle in the header.\n\n```\ndiff --git a/tensorflow/python/framework/device.py b/tensorflow/python/framework/device.py\nindex 8f5125d..505c2a9 100644\n--- a/tensorflow/python/framework/device.py\n+++ b/tensorflow/python/framework/device.py\n@@ -155,7 +155,9 @@ class DeviceSpec(object):\n         elif ly == 2 and y[0] == \"task\":\n           self.task = y[1]\n         elif ((ly == 1 or ly == 2) and\n-              ((y[0].upper() == \"GPU\") or (y[0].upper() == \"CPU\"))):\n+              ((y[0].upper() == \"GPU\") or \n+               (y[0].upper() == \"CPU\") or\n+               (y[0].upper() == \"KPU\"))):\n           if self.device_type is not None:\n             raise ValueError(\"Cannot specify multiple device types: %s\" % spec)\n           self.device_type = y[0].upper()\n```\n\nIf there is Python access to the same interface as above, then I can just use that. I am a fan of less configuration.\n", "I think if you name your device \"/device:KPU:0\" instead of just \"/KPU:0\", it should work without any additional edits.  \"/cpu:0\" and \"/gpu:0\" were shortcuts before we realized it was helpful to have the \"/device\" qualifier like we do for the other parts of a full device name.  Can you give that a try and let me know?\n", "Yup that worked!\n\n```\nDevice mapping:\n/job:localhost/replica:0/task:0/device:FPU:0 -> physical description\n/job:localhost/replica:0/task:0/device:FPU:1 -> physical description\n/job:localhost/replica:0/task:0/device:FPU:2 -> physical description\n/job:localhost/replica:0/task:0/device:FPU:3 -> physical description\n```\n\nOnce I figure out how to get those includes and compile it outside of TF, I will submit a pull request within documentation a la [\"Adding an Op\"](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html)\n\nAfter that. I would like to discuss how to best support my particular hardware. My company probably wants this to be private as I will need to tell you details, so would you be willing to do it over email and not as a GitHub Issue?\n", "Adding @petewarden\n", "@keveman \n\nI have been manually adding headers to my include path and I need,\n\n```\ntensorflow/core/framework/device_attributes.pb_text.h\ntensorflow/core/framework/op_segment.h\ntensorflow/core/framework/resource_mgr.h\ntensorflow/core/framework/tensor.pb_text.h\ntensorflow/core/framework/type_index.h\ntensorflow/core/graph/edgeset.h\ntensorflow/core/graph/graph.h\ntensorflow/core/graph/types.h\ntensorflow/core/arena.h\ntensorflow/core/lib/gtl/int_type.h\ntensorflow/core/lib/gtl/iterator_range.h\ntensorflow/core/public/session_options.h\n```\n\nManually adding symlinks to for all those above headers tothe TensorFlow source, I still get this error,\n\n```\ng++ -std=c++11 -shared fpu_device.cc -o fpu.so -fPIC -I /usr/local/lib/python2.7/dist-packages/tensorflow/include\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/common_runtime/local_device.h:19:0,\n                 from fpu_device.cc:10:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/common_runtime/device.h: In member function \u2018std::string tensorflow::Device::DebugString() const\u2019:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/common_runtime/device.h:139:74: error: \u2018ProtoDebugString\u2019 was not declared in this scope\n   string DebugString() const { return ProtoDebugString(device_attributes_); }\n                                                                          ^\nfpu_device.cc: In member function \u2018virtual tensorflow::Status tensorflow::FPUDevice::MakeTensorFromProto(const tensorflow::TensorProto&, tensorflow::AllocatorAttributes, tensorflow::Tensor*)\u2019:\nfpu_device.cc:84:41: error: \u2018ProtoDebugString\u2019 was not declared in this scope\n            ProtoDebugString(tensor_proto));\n```\n", "ProtoDebugString() is generated by tensorflow/tools/proto_text, which is a bit like protoc but for a minimal footprint class instead of a full protobuf one. \n", "Thanks. Do you know how I can get it to be recognized when I am compiling it into an external module?\n\nI am currently running \n\n```\ng++ -std=c++11 -shared fpu_device.cc -o fpu.so -fPIC -I /usr/local/lib/python2.7/dist-packages/tensorflow/include\n```\n", "Hello.  I think I should introduce myself.  I am working for Graphcore, a UK startup making a graph processor / accelerator.  It seems that I am traveling the same path as Aidan.\n\nI have a vaguely functioning device, although I was stuck on the 'having to edit the device_name_utils.cc and device.py' problem.   It would be a very useful thing to be able to build a dynamically linked device/kernels module indeed.  I have isolated all of my device/kernel code into the third_party branch.  Does this make sense?\n\nMy device depends on an external library, and so I have also added some code to the workspace.bzl file.  With only one workspace for the whole of tensorflow, there doesn't seem to be a way around this.  Is there is something that I have missed about bazel that would allow me to break the need for that?\n\nCheers\n", "I saw the slashdot conversation about [DeviceContexts](http://stackoverflow.com/questions/39399890/tensorflow-device-contexts?noredirect=1&lq=1).\n\nCan you clarify the difference between Devices and DeviceContexts.  Why would I not store device specific information in my Device class?  For instance, handles to device specific structures, mapped memory, etc?\n", "@DavidNorman If you look back a [couple posts ago](https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-247488042) I had the same problem. The trick is calling you device something like [\"device:FPU\"](https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-247488613). In my pull request, [I do this](https://github.com/tensorflow/tensorflow/pull/4520/files#diff-7c3aa428ac49c5f21774505a5985d051R103)\n\nAlso, @DavidNorman were you able to get your code to compile as a .so file? I have trouble with the includes. Or are you compiling into the TensorFlow main body\n", "@DavidNorman with regards to the Device Contexts, I forget were I heard it, but I think @vrv told me that GPUs have multiple contexts which handle different parts of computation like memory access, pushing kernels onto the device, etc.\n\nAlso, if you could put both your questions on StackOverflow, I will answer them there for better accessibility in the future. eg. [My question about compilation](http://stackoverflow.com/questions/39643820/adding-more-files-to-the-tensorflow-include-path)\n", "@aidan-plenert-macdonald thanks for the info  I do not have a separate .so file at the moment (or a .dylib as it would be on my Mac).   I have a few more hurdles to get past before I try to make that happen i think.  The requirement for an external library means I have to modify the workspace.bzl, so I'm not sure that there is too much point unless I can figure out how to avoid that.  \n\nHave posted [this question](http://stackoverflow.com/questions/39656677/tensorflow-device-vs-devicecontext) on slashdot\n", "@vrv I was wondering if you have any further information about adding includes. We just need those to be done.\n", "I really don't know :(  Going to assign this one to @keveman (I'll be OOO soon for a week).\n", "Got it!! Simple one liner in a BUILD file.\n\n[Bottom of the page](https://github.com/tensorflow/tensorflow/pull/4520/files?diff=unified)\n", "@DavidNorman \nYou were looking at the difference between DeviceContexts and Devices. I am debating between having Device represent actual hardware with several DeviceContext's that control virtual compute nodes (my device can allocate compute resources dynamically) which would each handle a vertex in my TF compute graph (I will use node to indicate hardware and vertex for TF operations). My concern is that TF doesn't appear to support binding Allocators to DeviceContexts which would be needed so I can make sure allocation for each TF vertex happens within my virtual compute node. On the other hand if I have Devices be my virtual compute nodes and bind each one to a TF vertex, then I can't dynamically size them as I believe the creation of devices in the DeviceFactories happens before the TF compute graph is assembled.\n\nCan I ask how you are assigning compute resources to nodes in the TF compute graph? Are you using Device to control hardware or is this done with contexts? Do you have similar problems with allocation? Do you know anything about TF's ability to do automatic resource allocation? I believe TF can auto-assign devices to operations.\n\n@vrv Is there any way to bind Allocators to specific device contexts?\n", "@aidan-plenert-macdonald: Typically an Allocator manages the memory for an entire device.\n\nIn the GPU case, there is one allocator for each GPU device, but a GPU device has hundreds to thousands of \"cores\", each which has access to the global memory that the allocator is responsible for allocating, and programs are responsible for setting properly.  CUDA has ways in its programming model to additionally have local fine-grained sharing of memory among cores, and that is specific to the cuda programming language, and we don't touch that really, since it's not part of the global memory pool.\n\nI suspect that your device is really more like many loosely coupled cores with message passing, which is a model we haven't really optimized for.  We would normally treat each 'core' as a separate device, and then your memcopy primitives (device to device) would use whatever high-performance inter-node communication primitives you wanted.\n\nAlternatively, if you'd rather treat your entire processor as a single \"device\", it might still be possible: allocator.h has an AllocatorAttributes structure, which is a 64-bit opaque value blob.  The top 8 bits of that structure we've made device specific.\n\nOpKernel has a GetAllocator() function that takes allocator attributes, so it might be possible for you to have the DeviceContext contain information that an OpKernel can use to set the appropriate bits of the AllocatorAttributes, and then you'd implement DeviceBase::GetAllocator() to return a different allocator based on the top 8 bits of the AllocatorAttributes.  If you used all 8 bits as an allocator id, you'd be able to address 256 different allocators in a single device.\n\nWithout knowing too much about your device, I'm not sure which approach is better, but those might help you make progress.\n", "@vrv I like the 8 bit option. I may run into problems with the 256 limit later, but I think for now this is a really good option.\n\nWhat would I have to change to make sure that those 8 bits get set appropriately? I tracked GetAllocator to the Executor class where it appears that I could modify the AllocatorAttributes and the Executor has virtual functions which make it appear as though I could replace it, but I don't see any registration for the ExecutorImpl.\n\nNow if I were to just have a TF Device per virtual compute node with a single allocator each. I would like to have a couple device numbers/config to specify the physical hardware and then the virtual compute nodes within it. the Distribute TF seems to use more numbers,\n\n``` python\ntf.train.ClusterSpec({\n    \"worker\": [\n        \"worker0.example.com:2222\", \n        \"worker1.example.com:2222\",\n        \"worker2.example.com:2222\"\n    ],\n    \"ps\": [\n        \"ps0.example.com:2222\",\n        \"ps1.example.com:2222\"\n    ]})\n```\n\nInternal to my device, I could treat it as a cluster computer. Is the distributed computing very extensible so I could rework the internals?\n", "We could change the 8 bits to a larger number of bits before 1.0 I guess.  Or perhaps one day maybe extend the value to be larger than 32 bits in a backwards compatible way.\n\nThe OpKernel::allocate_output() function can set the bits of the AllocatorAttributes however it wants.  I was thinking, but not sure if it works off hand:\n\n```\nFPUDeviceContext* fdc = static_cast<FPUDeviceContext*>(context->device_context());\nint32 device_id = fdc->device_id();\nAllocatorAttribute attr;\nattr.value |= (device_id && 0x000000FF) << 24; \n\nTensor* output;\ncontext->allocate_output(0, shape, &output, attr);\n```\n\nOr something like that.\n", "@vrv Do you know the answer to [this](http://stackoverflow.com/questions/39797095/tensorflow-custom-allocator-and-accessing-data-from-tensor)?\n", "Sorry, was on vacation for the past week -- just answered.\n", "I wrote a TensorFlow framework for a new device. I then wrote a Matix Multiplication that does nothing but print something when the constructor and Compute functions are called. The code looks like,\n\n```\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/common_runtime/dma_helper.h\"\n\nnamespace tensorflow {\n\n  class MatMulOp : public OpKernel {\n  public:\n    explicit MatMulOp(OpKernelConstruction* context) : OpKernel(context) {\n      KPU_DEBUG(\"Init\");\n    }\n\n    void Compute(OpKernelContext* context) override {\n      KPU_DEBUG(\"Compute\");\n    }\n  };\n\n  REGISTER_KERNEL_BUILDER(Name(\"MatMul\").Device(DEVICE_KPU), MatMulOp); \n}\n```\n\nUnfortunately, this code isn't being run during a simple test program taken directly from TensorFlow,\n\n```\nimport tensorflow as tf\nmod = tf.load_op_library('kpu.so')\nwith tf.device('/device:KPU:0'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n  c = tf.matmul(a, b)\n  # Creates a session with log_device_placement set to True.\n\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n# Runs the op.\nprint sess.run(c)\n```\n\nOddly enough, the NoOp code is being run (I remove the print statements, but it was printing things out),\n\n```\nnamespace tensorflow {\n\n  class NoOp : public OpKernel {\n  public:\n    explicit NoOp(OpKernelConstruction* context) : OpKernel(context) { }\n    void Compute(OpKernelContext* context) override { }\n    bool IsExpensive() override { return false; }\n  };\n\n  REGISTER_KERNEL_BUILDER(Name(\"NoOp\").Device(DEVICE_KPU), NoOp);\n\n}\n```\n", "I think in this case, our optimizations for constant folding are foiling you :).   (Because c can be computed statically, we pre-compute it on the first run of the graph on CPU, and then there's nothing to run on your device).\n\n1) You can disable this by disabling optimizations in ConfigProto.GraphOptions.\n\n2) You can redefine your graph to have the matmul take in two 'variables', in which case it's not constant foldable anymore.\n", "I think I will make them variables because I need to do that anyway. How do I fix,\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'b': Could not satisfy explicit device specification '/device:KPU:0' because no supported kernel for KPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nIdentity: CPU \nAssign: CPU \nVariable: KPU CPU \n     [[Node: b = Variable[container=\"\", dtype=DT_FLOAT, shape=[10,10], shared_name=\"\", _device=\"/device:KPU:0\"]()]]\n\n```\n", "You have to register kernels for a lot of standard ops on your KPU device.\n\n(NoOp, Variable, Assign, Identity, etc).  I think the generic implementations are registerable via header files like variable_op.h etc -- look for the GPU kernel registrations to get a sense of what you probably want to do for your KPU.  It's likely that most of these do not require a custom implementation on your device.\n", "Perhaps I should rephrase my question. I have kind of been shooting in the dark when registering the kernels. Is there a way to get the exact name of the kernel I am missing so I can implement it over?\n", "```\nIdentity: CPU \nAssign: CPU \nVariable: KPU CPU \n```\n\nThis is saying that these three groups of nodes needed to colocated on a single device, but the only common device implementation was CPU, so these nodes could not be placed on KPU.\n\nIf you register \"Identity\" and \"Assign\" on KPU, these three nodes could then be colocated together on KPU.\n\n(Also, you'll want to register them for at least DT_FLOAT, since that's what you're currently using).\n", "The master branch of TF recently switched to Protobuf 3.1.0 and my companies drivers use 3.0.0. I am now having trouble compiling. is there a good clean way to make backward compatibility work out? Can I force a Protobuf roll back with TF?\n", "You could locally change the workspace file to point to the earlier version of protobuf, but I suspect we moved to 3.1.0 for a good reason, so it's not something we could accept.  Might also be worth asking the protobuf team at github.com/google/protobuf ... \n", "My team fixed it on our side. Now I have reason to believe that TF is messing with the pointers I am returning from my custom allocator which are actually pointers to structures that control the hardware. It seems that the pointers are getting used within [Buffer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.cc#L87) and then the Buffer tries to free them as if they were a float pointer which causes a SegFault. Is this something that might happen? How can I fix it?\n", "@vrv \n\nI am getting segfaults from the destructors where the program terminates (below is a stacktrace).\n\n```\nProgram received signal SIGSEGV, Segmentation fault.\n0x0000000000000000 in ?? ()\n(gdb) backtrace\n#0  0x0000000000000000 in ?? ()\n#1  0x00007fffe0524e94 in tensorflow::(anonymous namespace)::Buffer<float>::~Buffer() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fffe0524f41 in tensorflow::(anonymous namespace)::Buffer<float>::~Buffer() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fffe0525fd6 in tensorflow::Tensor::~Tensor() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fffe0162514 in tensorflow::VariableOp::Var::~Var() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fffe051a474 in tensorflow::ResourceMgr::Clear() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007fffe051a501 in tensorflow::ResourceMgr::~ResourceMgr() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007fffe04505a0 in tensorflow::Device::~Device() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#8  0x00007fffe894e6d1 in tensorflow::KPUDevice::~KPUDevice (this=0xe65ee0, __in_chrg=<optimized out>) at kpu/device.cc:49\n#9  0x00007fffe894e700 in tensorflow::KPUDevice::~KPUDevice (this=0xe65ee0, __in_chrg=<optimized out>) at kpu/device.cc:52\n#10 0x00007fffe045284e in tensorflow::DeviceMgr::~DeviceMgr() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#11 0x00007fffe035b087 in tensorflow::DirectSession::~DirectSession() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#12 0x00007fffe035b1d1 in tensorflow::DirectSession::~DirectSession() () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#13 0x00007fffe04401b7 in TF_DeleteSession () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#14 0x00007fffdf16a90b in _wrap_TF_DeleteSession () from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#15 0x000000000049968d in PyEval_EvalFrameEx ()\n```\n\nI am using the default VariableOp by just importing the header and registering it to my device. I am unsure what I am doing wrong that would cause it to segfault. I believe it is trying to free the pointers to my tensor data incorrectly, but I am neither sure of that, nor is it clear how to fix it.\n\n``` c++\n#include \"kpu/kpu.h\"\n#include \"tensorflow/core/kernels/variable_ops.h\"\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\n#define REGISTER_KPU_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                               \\\n      Name(\"Variable\").Device(DEVICE_KPU).TypeConstraint<type>(\"dtype\"), \\\n      VariableOp);                                                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"TemporaryVariable\")                      \\\n                              .Device(DEVICE_KPU)                        \\\n                              .TypeConstraint<type>(\"dtype\"),            \\\n                          TemporaryVariableOp);                          \\\n  REGISTER_KERNEL_BUILDER(Name(\"DestroyTemporaryVariable\")               \\\n                              .Device(DEVICE_KPU)                        \\\n                              .TypeConstraint<type>(\"T\"),                \\\n                          DestroyTemporaryVariableOp);                   \\\n  REGISTER_KERNEL_BUILDER(Name(\"IsVariableInitialized\")                  \\\n                              .Device(DEVICE_KPU)                        \\\n                              .TypeConstraint<type>(\"dtype\")             \\\n                              .HostMemory(\"is_initialized\"),             \\\n                          IsVariableInitializedOp);\n\n  TF_CALL_GPU_NUMBER_TYPES(REGISTER_KPU_KERNELS);\n\n}  // namespace tensorflow \n```\n\nCould you help me understand what is happening?\n", "I'm not entirely sure without being able to see the code, unfortunately.  The destructor of Tensors should just call the Deallocate method of your custom Allocator -- you might be able to trace that code to figure out what's going on?\n", "Fixed it. I was calling `delete kpu_allocator_` in my Device destructor while I think TF still had reference pointers floating around. Took way too long to solve.\n", "@aidan-plenert-macdonald do you have any interesting updates so far?\n", "I have been working more on coding for my hardware. I have the TensorFlow stuff working. I just want to get my hardware more TF kernels than the basics and then make a couple more changes to my TF code to clean it up a bit more. Once I get that all cleaned up, I can start working on the documentation for this. I just want to work out all the bugs and oddities first.\n", "@vrv I got things working and am at a good rebase/pausing point.\n\nEverything works without changing TF beyond installing more header files (simple couple lines in Bazel). Let me know what goals you have on your side. I can write documentation, examples, or just be a resource for other people trying to extend TF to new hardware. A couple pieces of interesting hardware are becoming popular/relevant: Intel Phi's, TrueNorth, SyNAPSE, etc. If people want to add these to TF, then I could help them get over the learning curve, as a community that knows how to get this done is probably better than documentation.\n", "Awesome, we're really glad to hear that!\n\nThere are a couple of audiences that would be interested in general, and you are right that having documentation might go stale right now.  There's also [XLA](https://github.com/tensorflow/tensorflow/blob/47ef3c5eca776a2d10249f987cca406577389db5/tensorflow/g3doc/resources/xla_prerelease.md) which is relevant to some but not all hardware developers.\n\nMostly, I just wanted validation that we didn't have to add code to core TF to add new devices, and it seems like (pending your PR to add those missing files), it should work.  We'd be happy to have documentation or an example to show the minimal set of things that device implementors might need to think about when adding new devices.\n\nMaybe it makes sense to start out with a blog post or something describing what you had to do, (so it's discoverable), and then we can see what makes sense to have either on GitHub or in the codebase?\n", "Hi Vijay,\r\n\r\nI'm taking over for Aidan in adapting TensorFlow for the Knureon.  I am currently working on some automated scripts that will help our work in sync with the tip of development on TensorFlow.\r\n\r\nI am currently trying to figure out how to get my local TensorFlow build to run validation tests to so that I can be confident that the code snapshot I pulled from github is consistent with itself before merging it into our own work.  Unfortunately Bazel is still a little new to me, so I am having trouble understanding the calls I need to make to get Bazel to run all tests in a class (we want to exclude the GPU tests for now), and not simply run line-by-line tests as described in your \"Adding a New Op\" tutorial.  Is there a syntax similar to LLVM's 'lit' tool that allows for a class or regexp of test names to be run?\r\n\r\n-Rich", "I don't know llvm's lit tool, but you can do\r\n```\r\nbazel test //tensorflow/python/... \r\n```\r\nto build all targets under //tensorflow/python\r\n\r\nIf you don't run with --config=cuda, you will not run any tests that require a GPU (our code *should* automatically skip tests that require a GPU to run).\r\n\r\nIf you want to run a specific target, you can do:\r\n```\r\nbazel test //tensorflow/python:constant_op_test\r\n```\r\nor something like that.\r\n\r\n\r\n\r\n", "Hi @vrv , will the current way (mainly writing customized kernels) of adding new devices changed after the introduce of XLA compiler?", "And more specifically, will XLA also target for mobile devices? E.g. with customized DNN acceleration chips.\r\n\r\nQuote from XLA doc:\r\n> **Improved portability**. The compiler-based framework is designed to target different back-end hardware, including a variety of CPUs, GPUs, and custom accelerator hardware such as TPUs. The CPU and GPU back-ends currently use LLVM, while the internal Google TPU back-end (which will not be open-sourced at this time) uses custom code generation. The goal for this and other accelerators is that it should be relatively easy to write a new back-end for novel hardware, at which point a large fraction of TensorFlow programs will run unmodified on that hardware. This is in contrast with the approach of specializing individual monolithic Ops for new hardware, which requires TensorFlow programs to be rewritten to make use of those Ops.\r\n\r\nFor a non CPU/GPU device, should the device provider need to write a custom code generator like TPU? I'm wondering whether writing this generator is as simple as adding a Device now mentioned above? Just by adding these [atomic ops](https://www.tensorflow.org/versions/master/resources/xla_prerelease#operation_semantics) defined in XLA document, or more work involving LLVM backend stuff?", "@prb12 @michaelisard can you comment more? I wouldn't say writing a device today is that simple, considering there are 300+ ops to implement.  XLA should make that job a lot easier.", "I think this depends a lot on the capabilities of the hardware device you are planning to support.\r\nThere are several scenarios which I can imagine:\r\n\r\n- If the hardware is a 'one-trick-pony' accelerator (e.g. it provides a small number of fixed functions) then it may be sensible to expose those from a traditional TF device.  e.g. register new device-specific kernels for existing ops such as MatMul, or if the hardware does some very strange things then add custom ops and ideally some sort of GraphOptimizationPass which uses device-specific knowledge to rewrite existing TF programs to use your ops where it makes sense.\r\n\r\n- The above approach probably works quite well if you're wanting to add some sort of black-box library implementing eg. something like MKL.  You don't want to break portability of people's TF programs, but you want to rewrite them to use custom low-level functionality at runtime.\r\n\r\n- For an accelerator which provides a powerful set of generic computation support it is much easier to implement the small number of primitives required by XLA than the many 100s of ops registered in TensorFlow.  The TF2XLA bridge automatically identifies parts of graphs to hand to XLA and converts the TF ops into XLA primitives.  The set of ops which are currently translatable is enough to handle complex models like inception.    \r\n\r\n- Implementing XLA support for a processor supported by LLVM is not very difficult.  For something stranger you will need to write your own XLA backend.  This is strictly easier than implementing the equivalent TF ops, but you will need some compiler expertise.\r\n\r\nOne caveat with XLA as it currently stands, is that we don't have a way for devices to implement a consistent subset of XLA primitives.  At the moment it's basically 'all or nothing'.  However, I think this will evolve over time. ", "We are hoping to implement as many TF operations as we can for the Knureon.  To the extent we have a 'trick' we are focusing on sparse matrices because we believe we can show a significant performance improvement.\r\n\r\nIs there a white paper available on XLA?  Is the source on a branch?  I don't see sparse matrices mentioned in  the prerelease.\r\n\r\nAlso, is there a target release date for XLA?  Our own schedule may require us to support traditional TF kernels for our first release, at least.", "@prb12 Thanks for the explanations, and it makes my understanding much clear.", "Thank you all for your help.  I asked this question to Vijay in an email, but I think I'd like to post it here too.\r\n\r\n---------\r\n\r\nI've been trying to understand how backpropagation is implemented in tensorflow models.  I'm just asking you to let me know if I have any major misunderstandings about the nature of TensorFlow here.  Thank you again for your efforts to help me out.\r\n\r\nI'm basing this interpretation on the graph generated when I add a FileWriter to the mnist_softmax.py tutorial example (attached here, also with a relu activation added as an experiment).\r\n\r\nIt looks like the act of placing an optimizer such as a GradientDescentOptimizer into a computation flow creates a new subgraph that mirrors the original graph, except that\r\n1. Data flows through the nodes in reverse order and\r\n2. The nodes themselves are replaced with their gradients.\r\n\r\nOne training run, therefore will consist of a forward pass through the computation steps as described in the python file, culminating in the loss calculation.  If we are using minibatches, this is done a number of times.  Then, a second pass backwards through the gradients of cross entropy will cause the Variable objects to be updated with trained values and the run() command will complete.\r\n\r\nSome questions:\r\n1.  I see that I can replace the \"relu\" activation with \"atan\" and have the model run without crashing.  Why does this work?  I don't see any atan gradient function under the kernels/ directory.\r\n2.  During the course of a run() command, do the weight & bias variables really get overwritten by backpropagation, or does TF keep around the original values and store the adjusted ones in new reserved memory?\r\n3.  GradientDescentOptimizer() is called with a \"learning rate\" '0.5'.  Is this argument only used to scale the original loss function, or is it passed through all the way back through each layer of the network?\r\n\r\n\r\n[mnist_softmax.py.zip](https://github.com/tensorflow/tensorflow/files/671609/mnist_softmax.py.zip)\r\n", "@RichDubielzig  It helps to think of different layers of TensorFlow -- there's the client side which creates computational graph, and TensorFlow runtime side which executes it. If your hardware provides implementations for all ops required by TensorFlow runtime, you wouldn't need to know about the client-side. However I can see how you would want to know about typical things done by the client to make sure the hardware performs well for them.\r\n\r\nA typical implementation of training step constructs gradient computation using `tf.gradients` or related, and updates weight variable `var` using `var.assign_add(learning_rate*gradient)` or related.\r\n\r\n`tf.gradients([a],[b])` is a \"client-side\" operation that adds nodes to the graph that give gradient of a with respect to b using reverse mode AD algorithm. In a sense it's not part of core TensorFlow since it's not accessible to people using TensorFlow from other languages. Also, we've been finding that graph created by `tf.gradients` is a memory hog, and a modified version is sometimes needed to make models fit.\r\n\r\nHere's what a typical graph you get from `tf.gradients` looks like\r\n![ad-graphs1](https://cloud.githubusercontent.com/assets/23068/21486407/f172bc40-cb68-11e6-9b47-f87e85a99293.png)\r\n\r\nNodes `b_i` compute left product of a backprop vector with jacobian evaluated at `a_i`, so you can see that each node has two inputs. This is the core step of \"reverse-mode automatic differentiation\" referred to as `l_op` in Theano or `grad` or `gradient` in TensorFlow. It can correspond to a single op implementing computation in C++, or it can be implemented as a several TensorFlow ops wired together. For instance, `grad(a,b)` of `tf.square` is implemented as `mul(a, mul(2, b))` rather than a fused `SquareGrad` op.\r\n\r\nFrom graph you can see that to compute it you need enough memory to store `a1/a2/a3`. If that's too much, one could use a modified implementation below which saves memory at the cost of recomputing `a2`\r\n\r\n![ad-graphs2](https://cloud.githubusercontent.com/assets/23068/21486412/0c68954c-cb69-11e6-8467-22c402c529c2.png)\r\n\r\nRegarding questions\r\n\r\n1. You can see the graph you get as follows\r\n\r\n```\r\na = tf.placeholder(tf.float32)\r\nb = tf.atan(a)\r\ntf.gradients([b],[a])\r\nprint(tf.get_default_graph().as_graph_def())\r\n\r\n```\r\nIt seems to be implementing `atanh(z)` grad using `1/(1-z^2)` formula.\r\n\r\n2. `ApplyGradientDescent(var, alpha, delta)` will overwrite the value of var\r\nwith new weight values.\r\n\r\n3. GradientDescentOptimizer use translates to `ApplyGradientDescent(var, 0.5, gradient)` , so `0.5` is only introduced at the last step", "Thank you Yaroslav for your answers, this was very helpful.", "Hello,\r\n\r\nI am currently working my way through understanding the backend of the XLA framework.  I currently have two questions:\r\n\r\n1.  How do you debug the TensorFlow library?  For example, I wanted to try breaking on this debug message:\r\n\r\n`I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>`\r\n\r\nI tried \r\n```\r\ngdb --args python3 mnist_softmax_xla.py\r\nset follow-fork-mode child\r\nb service.cc:187\r\nr\r\n```\r\n\r\nBut the breakpoint was skipped over.\r\n\r\n2.  How does the JIT compiler \"link in\" on-core runtime libraries?  In order to take advantage of XAL, we'd need to have some point at which we link in a bit of code to handle general setup and teardown of a core, as well as communication back to the host.  We also have some TF-specific memory-handling code that would need to go in as well.  At what point do these libraries get added to the HLO? (We can provide the libraries in IR format)", "I *think* I've answered my second question.  Looking at compiler/xla/service/gpu/llvm_gpu_backend, I see the function LinkLibdeviceIfNecessary, which takes a directory path as a direct argument and the bitcode filename to link from a configuration argument.  It then does a bitcode-level linkage (which is our preference) against the runtime library and passes the result back to CompileModuleToPtx.\r\n\r\nSo it looks like our simplest approach is to create a .bc runtime library and reuse the GPU compilation code to compile to our own ISA.\r\n\r\nA follow-up question: I don't see anything in the GPU linker or compiler that checks for maximum code size.  Is there any mechanism to guarantee or check that a requested compilation block will return a kernel under a given number of instructions?", "For question 1 http://stackoverflow.com/questions/33746071/tensorflow-core-debug-missing-debug-symbols might help", "That looked promising, but apparently the built tensorflow object is the same as what got installed by pip.  \r\n\r\n```\r\nrdubielzig@swpl-000224:~/.cache/bazel/_bazel_rdubielzig/460f4b3af716bd4767655e290c45766f/execroot/tensorflow-knureon/bazel-out/local-py3-dbg/bin/tensorflow/python$ md5sum ~/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so \r\n982e011106ed013c21638157659b34e5  /home/rdubielzig/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\nrdubielzig@swpl-000224:~/.cache/bazel/_bazel_rdubielzig/460f4b3af716bd4767655e290c45766f/execroot/tensorflow-knureon/bazel-out/local-py3-dbg/bin/tensorflow/python$ md5sum _pywrap_tensorflow.so\r\n982e011106ed013c21638157659b34e5  _pywrap_tensorflow.so\r\n```", "@RichDubielzig perhaps @eliben might be able to help or point you at somebody who can help regarding your 'maximum code size' question.", "@RichDubielzig not sure I understand the maximum code size requirement. Care to elaborate? Where in the API would you expect to see this?", "I would expect to see code size considerations taken in generation of kernel code from LLVM IR.  The instuction memory for individual cores in Knureon is quite small, and it looks like XLA could JIT kernel code  that would overrun program memory.  Is there a mechanism to break up a block of IR into smaller pieces?", "@RichDubielzig this functionality is not supported by the current CPU/GPU backends; however if you'd plan to add a custom backend for your architecture you could implement this.", "@eliben Do you mean implement it in the main TensorFlow, or just in our library? Can you recommend a good place that it might fit into nicely?", "I'm afraid I'm having trouble understanding how to handle synchronization and asynchronous kernel resources when developing in TensorFlow using TensorFlow objects.  Here is my problem:\r\n\r\nThe knureon system can be thought of as a large pool of completely independent cores which can be assigned operations at any time.  So, for example, given the data flow below:\r\n\r\nA: m = a * b\r\nB: n = c *d\r\nC: o = e * f\r\n\r\nIf I have 128 available cores, then I might request A to run on 64 cores, B to run on another 64 cores, and then I would need to wait for one of the first two operations to complete before I can C on 64 cores.  To this end, I have created a little launch queue object that can hold computation requests until compute resources are available, then run.  In pseudocode, my naive OpKernel Compute() implementation is below.  Note that this employes a Knureon-specific command queue object which can be used both to launch operations and to pend on running ones.  Multiple queues are allowed to exist and run in parallel on a single Knureon system.\r\n\r\n```\r\nCompute( context  input):\r\n  Get input tensors A & B from context\r\n  Also get a Knureon command queue handle Q from the DeviceContext object.\r\n  Figure out the dimensions of the tensors\r\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\r\n  Queue the tuple (f(),Q) in the launch queue.\r\n  We can now block on Q until f() completes.\r\n\r\n(in launch queue, in a separate thread:)\r\n  Wait for resources to run f()\r\n  dispatch f() using the queue Q on the Knureon complex.\r\n```\r\n\r\nThe problem should be apparent:  I don't want TensorFlow to sit around waiting on Q if there are other operations that can be deployed right away on available resources.  The alternative *appears* to be to use an asynchronous opkernel:\r\n\r\n```\r\nComputeAsync( context  input, DoneCallback done):\r\n  Get input tensors A & B from context\r\n  Also get a Knureon command queue handle Q from the DeviceContext object.\r\n  Figure out the dimensions of the tensors\r\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\r\n  Queue the tuple (f(), Q, done) in the launch queue.\r\n  Return immediately\r\n\r\n(in launch queue, in a separate thread:)\r\n  Wait for resources to run f()\r\n  run f() using Q on the Knureon complex and spawn a task that will wait on Q.  This task will call the ComputeAsync done() call back when f() is complete.\r\n```\r\nBut I am not sure if this is the right approach, as it seems to be reserved for high-latency operations such as receiving over a network.  I've been over and through the code, and unfortunately this has only increased my confusion.  I see that the GPU uses a StreamExecutor, but the presence of all the ThenXxxx() functions in the Stream prototype makes me suspect that it is not what I want.\r\n\r\nI have also noticed that OpKernel Compute() methods can be called in parallel from concurrent Executor threads.  So do I even need to sweat about parallel execution at all? When an OpKernel's Compute() method is invoked, am I guaranteed that there will be no benefit to running asynchronously because all the other OpKernels managed by my threads' Executor have data dependencies on my operation?\r\n\r\nThank you in advance and my apologies for rambling.  I've had to spend a few days figuring out how to phrase this question in a coherent manner, and I'm not sure I've met my goal, so if you need anything clarified please let me know.\r\n\r\n", "Hi! As many others I am really interested in defining my own devices. \r\n\r\nI read this thread as well as some others. Is there any documentation available to implement new devices (either in the TensorFlow source code or as a separate module)?\r\n\r\n@RichDubielzig  and @aidan-plenert-macdonald  were working on a guide, but at the moment I could only see this:\r\nhttps://github.com/knuedge/tensorflow/blob/36e0cdf04f294bfd51931d4f78e291590ed0d3ec/tensorflow/g3doc/hardware/adding_support/index.md\r\n\r\nIn there anything more recent (targeting the release 1.0)?\r\n\r\nThank you!", "@CUinNYC indeed, that doc is a great attempt to show it's done, and that's the basic idea of how the scaffolding works, but the implementation details for every new hardware device means it takes care to figure out exactly how to implement the device code.\r\n\r\nWe're working with a few external folks such as those in this thread to provide some early guidance as to how it's done, and at some point we'll have some nice examples to point at (beyond say, our CPU and GPU implementations).\r\n\r\n@RichDubielzig: the dataflow model of TF execution means that any operations that can be run at a given time will be run by the executor (assuming there are enough inter-op-parallelism threads to run them).  So yes, it is possible to have multiple \"matmul\" nodes running at once if both can be run at the same time.\r\n\r\nOn CPU, we use Eigen for a lot of the computation, and each op has a shared threadpool on which to execute -- so those threads will typically always be busy if there's lots of op work to do (though I admit, it probably won't be super efficient to context switch all the time, but I digress).\r\n\r\nOn GPU, we use GPU streams to enqueue asyncrhronous ops: execution of an OpKernel just queues the work to be done, and we use streams to enforce execution order at the actual device. \r\n\r\n@hawkinsp might have more to say here.", "@vrv I really appreciate the initial documentation. Thanks to that, I successfully created a \"fake CPU\" (really easy indeed). It required minor changes because of the TensorFlow code evolution (r1.0). So having an external-module approach would be more portable. Let me know if you need a help or feedback comments on a preliminary documentation. \r\n\r\nIn particular, I am interested in how to allocate memory for accelerators. My goal is to reduce as much as possible memory copies.\r\n\r\nIf anybody has comments or examples I will really appreciate that. ", "Just following up on my question:  I am seeing results with this approach:  \r\n\r\n1.  Define my operation as an AsyncOpKernel.  Kernel pseudocode:\r\n```\r\nComputeAsync( context  input, DoneCallback done):\r\n  Get input tensors A & B from context\r\n  Also get a Knureon command queue handle Q from the DeviceContext object.\r\n  Figure out the dimensions of the tensors\r\n  Assemble a call f(A,B,dims) to my Knureon matrix multipllier algorithm\r\n  Queue the tuple (f(), Q, done) in the launch queue.\r\n  Return immediately\r\n```\r\n\r\n2. In launch queue, in a separate thread:\r\n```\r\n(as part of init routine, we have created our own dedicated ThreadPool, which includes a thread to run this wait/spawn loop)\r\n\r\n  Wait for resources R to run f()\r\n  schedule a task to run f() on R resources using Q and calling done().\r\n```\r\n\r\n3. In the task scheduled above:\r\n```\r\n  deploy f() to R and kick it off.\r\n  wait on Q to finish\r\n  release R back to the Knureon environment\r\n  call the AsyncOpKernel done() callback\r\n  exit\r\n```", "Asked a new question on StackOverflow about the ConstTensor, which doesn't map to the DMAHelper::base() function:\r\n\r\nhttp://stackoverflow.com/questions/42707600/tensorflow-how-to-get-pointer-to-data-contents-of-consttensor", "Posted another question on StackOverflow regarding my confusion with DeviceContext  and when it needs to be used.  I am revisiting the issue because it turns out we have a smaller limit than I thought on open queues with the system and I'm wondering if I should\r\n\r\n1. redesign with the idea of mapping one DeviceContext per queue\r\n2. keep with the current design of one DeviceContext per OpKernelContext and manage queues in a separate pool as they are needed.\r\n\r\nhttp://stackoverflow.com/questions/42869340/tensorflow-why-does-a-gpu-device-only-have-one-device-context-and-does-it-rea", "Another issue we have run into in attempting to debug XLA:  It doesn't seem like we are able to exercise any backend compiler when running in a debugger.  The issue is here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/9194", "@RichDubielzig I've been told that XLA-related development discussion has been moved to https://groups.google.com/forum/#!forum/xla-dev  -- you might get more help / feedback there now.", "Asked a new question related to this on stackoverflow.  Will also post to the xla-dev forum\r\n\r\nhttps://stackoverflow.com/questions/44271775/can-tensorflow-xla-be-supported-by-plug-ins", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 4358, "title": "Wrong example script in the docs for preprocessing data", "body": "I was reading the [docs for preprocessing](https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#preprocessing), which is a small paragraph linking to the [CIFAR-10 network](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10.py) as an example. However, that script does not perform any preprocessing. Do we have a better example illustrating preprocessing steps like data normalization, distorting images, etc?\n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\"\n", "@andydavis1 Are you sure? I'm mainly pointing at a \"bug\" in the docs. I'm not asking for anything, my question was just to suggest a way of improving the docs.\n", "Ah ok. I took that last sentence you wrote \" Do we have a better example illustrating preprocessing steps like data normalization, distorting images, etc?\". As a general question.\n", "The folder holding the file referenced in the docs has, e.g. https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_input.py, which does contain a `distorted_inputs` routine, is this what you're looking for? If so, then the easiest fix is just to change the link in the docs to point to the cifar10_input.py file instead of the cifar10.py file that imports it (line 47)\n", "Thanks. Yes, I think that would be suitable. Do you want to create a pull request? Otherwise, I can do that.\n", "You can go ahead.\n"]}, {"number": 4357, "title": "FileIO.read signature differs from Python stdlib File.read interface", "body": "`FileIO` does not quack quite like a File object; its `read` method should take an optional `size` argument. This discrepancy makes `GFile` not a drop-in replacement for `File`, which makes it hard to port code between Google and the outside world.\n\n`File.read` API doc: https://docs.python.org/2/library/stdtypes.html#file.read\nCurrent `FileIO.read`: https://github.com/tensorflow/tensorflow/blob/915b02917db21de5a0ae304a067aedb0b5dd759d/tensorflow/python/lib/io/file_io.py#L98\n", "comments": ["FileIO.read() now supports a size argument. Closing bug now.\n", "I added tell() and seek() and read(n) in the following commits:\nhttps://github.com/tensorflow/tensorflow/commit/63a7a30e6bd091f87be1de2305c6d882d68ba6a8\nhttps://github.com/tensorflow/tensorflow/commit/cb07991bd77490055eea4139230743f4573a8318\n\nI have not yet tested that this satisfies all of the python File API, but we're slowly getting there.\n\nRather, I know it doesn't implement all of the File API yet, but tell() and seek() and read(n) were pretty important, and we can add more functionality as users need them.\n"]}, {"number": 4356, "title": "Playground is not in https", "body": "It should be as the rest of tensorflow website is in https\n", "comments": ["@LouisCAD , how did you get to the playground?\n", "@xmbrst By following the link advertised in [this I/O 16 talk](https://www.youtube.com/watch?v=3dXQxSI3XDY), which points to [playground.tensorflow.org](http://playground.tensorflow.org/)\n", "Closing, and looping internally. Thanks for reporting!"]}, {"number": 4355, "title": "Clicking on Image in Tensorboard Should Open in New Window", "body": "Right now the image may be squashed and hard to read:\n![image](https://cloud.githubusercontent.com/assets/51059/18482149/0324d058-79ad-11e6-83a4-d782b523065d.png)\n\nThe current solution is to copy the image URL and manually open in a new tab.\n", "comments": ["Is this still current?", "there is an inline maximization option (which causes a reflow) but no open in new tab", "What browser?", "Chrome.", "@cancan101 Why do you need open in new tab? Why not just use the \"true size\" and \"maximize image\" buttons?", "Closing since we didn't hear a response. Feel free to reopen at https://github.com/tensorflow/tensorboard/issues"]}, {"number": 4354, "title": "Tensorboard should Update Browser Location on Tab Changes", "body": "![image](https://cloud.githubusercontent.com/assets/51059/18482024/88fb96e0-79ac-11e6-838e-3b2a6094b16a.png)\n\nRight now changing tabs does not change the location which means browser refresh, etc loses the current tab.\n", "comments": ["Is this still current?", "Yes, the location still does not change on navigation.", "Sorry to be petulant, but you tried with the latest version? Which browser/architecture?", "i am using the newest docker image: `gcr.io/tensorflow/tensorflow:latest-devel-gpu` and Chrome.", "Yeah, this is a regression in the pip package. It still works inside Google. ", "This should be fixed now. Please reopen on our new repo at tensorflow/tensorboard if it reoccurs."]}, {"number": 4353, "title": "confusion_matrix should use cast rather than convert to tensor for size", "body": "This does not work:\n\n```\n>>> cm = tf.contrib.metrics.confusion_matrix(\n...     [1,2,3], [4,5,5], num_classes=tf.constant(3, dtype=tf.int32))\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/confusion_matrix_ops.py\", line 84, in confusion_matrix\n    indices=indices, values=values, shape=shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 981, in __init__\n    shape = convert_to_tensor(shape, name=\"shape\", dtype=dtypes.int64)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 628, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 571, in _TensorTensorConversionFunction\n    % (dtype.name, t.dtype.name, str(t)))\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"confusion_matrix/pack:0\", shape=(2,), dtype=int32)'\n```\n\nwhere as this does:\n\n```\ntf.contrib.metrics.confusion_matrix(\n    [1,2,3], [4,5,5], num_classes=tf.constant(3, dtype=tf.int64))\n```\n\nThis seems related: https://github.com/tensorflow/tensorflow/pull/2187\n\nMachine:\n\n```\n bazel version\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nExtracting Bazel installation...\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n\n```\n git rev-parse HEAD\n58b37cf745c6c30d44878ddf5987c4283ed12c3c\n```\n\n```\nls -l /usr/lib/x86_64-linux-gnu/libcud*\nlrwxrwxrwx 1 root root       29 Aug 12 05:11 /usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so\nlrwxrwxrwx 1 root root       17 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn.so.5 -> libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 60696704 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\nlrwxrwxrwx 1 root root       32 Aug 12 05:11 /usr/lib/x86_64-linux-gnu/libcudnn_static.a -> /etc/alternatives/libcudnn_stlib\n-rw-r--r-- 1 root root 59715990 Jun 10 13:07 /usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n```\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n```\n", "comments": ["Looks like a fix is on its way...\n", "Closing, assuming that the fix was committed. Feel free to open again if the problem persists in more recent versions."]}, {"number": 4352, "title": " ./configure command generates  warnings", "body": "Hi ,\nWhen i execute ./configure in tensorflow directory  the output contains the following warnings:\n( Environment is  ubuntu16.04+cuda8+cudnnV5.1+bazel-0.3.0) \n\nWARNING: /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/protobuf/WORKSPACE:1: Workspace name in /home/syj/.cache/bazel/_bazel_syj/47b860f752d3ee48e8d208f528a56395/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\n\nIs this warning serious? How to avoid this ?\n\nThanks \n", "comments": ["ERROR: /home/syj/tensorflow/tensorflow/contrib/session_bundle/BUILD:237:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/syj/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature_lite'.\n", "I have some similar configuration errors that don't seem to prevent compilation.  I assume they are Android or some other operating system what is bleeding through on the configuration:\n\nLinux: Ubuntu 16.04\nCompiling with GPU support\nCommit: 91a70cbf1c627117b70a3d2dd4c612779369e293\n\nHere is the full set of errors during ./configure:\n\n> INFO: Waiting for response from Bazel server (pid 4080)...\n> ERROR: /home/greg/tensorflow/tensorflow/core/kernels/BUILD:2207:1: no such target '//tensorflow/core:android_proto_lib_no_rtti_lite_runtime': target 'android_proto_lib_no_rtti_lite_runtime' not declared in package 'tensorflow/core' defined by /home/greg/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/core/kernels:android_tensorflow_kernels_no_rtti_lite_runtime'.\n> ERROR: /home/greg/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:android_lib_lite': target 'android_lib_lite' not declared in package 'tensorflow/core' defined by /home/greg/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\n> ERROR: /home/greg/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/greg/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\n> ERROR: /home/greg/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/greg/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:session_bundle'.\n> ERROR: /home/greg/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/greg/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\n> ERROR: /home/greg/tensorflow/tensorflow/contrib/session_bundle/BUILD:213:1: no such target '//tensorflow/core:meta_graph_portable_proto': target 'meta_graph_portable_proto' not declared in package 'tensorflow/core' defined by /home/greg/tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/contrib/session_bundle:signature'.\n> ERROR: Evaluation of query \"deps((//... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\n> Configuration finished\n\nI did compile the PIP package with no problems after getting these configuration errors.\n", "I updated the bazel and confirmed the current version is 0.3.1(using command: \"which bazel\" and \"bazel version\"respectively )\nand I run the configure process as @gustavla suggested:\n\ngit clone https://github.com/tensorflow/tensorflow\ncd tensorflow\ngit show -R 7bcdcbb | git apply\n./configure\nBut the following errors appeared and configured process failed\n\n/home/syj/tensorflow/tensorflow/contrib/session_bundle/BUILD:107:1no such target '//tensorflow/core:android_lib_lite':target 'android_lib_lite'not declared in package.....\n\n/home/syj/tensorflow/tensorflow/core/BUILD:899:1no such package '@zlib_archive//':Error downloading from http://zlib.net/zlib-1.2.8.tar.gz to ......\n\nconfiguration of query \"deps((//...union @bazel_tools//tools/jdk:toolchain))\"failed:errors were encountered while computing transitive closure.\n\nI am appreciated if anyone can tell me how to solve this problem.\n\nThank you\n", "The errors regarding android_lib_lite and meta_graph_portable_proto should be fixed by f66b491, which was pushed today in #4360.\n", "@szsongyj Given @andrewharp comment, is this still an issue?\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 4351, "title": "Changes in default value", "body": "", "comments": ["@ajayraop3495, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @DjangoPeng to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 4350, "title": "saver problem", "body": "```\nsaver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n```\n\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1253, in save\n    {self.saver_def.filename_tensor_name: checkpoint_file})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\n    raise type(e)(node_def, op, message)\nResourceExhaustedError: ./model/model-49.tempstate5787507096018461664\n         [[Node: save/save = SaveSlices[T=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, \nDT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Co\nnst_0, save/save/tensor_names, save/save/shapes_and_slices, Wemb, att_W/_2905, att_b/_2907, decode_lstm_W/_2909, decode_lstm_b/_2911, decode_wo\nrd_W/_2913, decode_word_b/_2915, hidden_att_W/_2917, image_att_W/_2919, image_encode_W/_2921, init_hidden_W/_2923, init_hidden_b/_2925, init_me\nmory_W/_2927, init_memory_b/_2929, lstm_U/_2931, lstm_W/_2933, lstm_b/_2935, pre_att_b/_2937)]]\nCaused by op u'save/save', defined at:\n  File \"model_tensorflow.py\", line 345, in <module>\n    train(pretrained_model_path=None)\n  File \"model_tensorflow.py\", line 261, in train  \n    saver = tf.train.Saver(max_to_keep=50)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 966, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 990, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 614, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 294, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 240, in save_op\n    tensor_slices=tensor_slices)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 181, in _save\n    tensors, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 438, in _save_slices\n    data=data, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 710, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2334, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1253, in __init__\n    self._traceback = _extract_stack()\n", "comments": ["Is it possible that you've run out of disk space?\n", "yes, it's out of space\n", "My disk is not out-of-space but I'm also seeing the same error.\n"]}, {"number": 4349, "title": "kernel version 367.44.0 does not match DSO version 352.99.0", "body": "I can't seem to load GPUs.  After a first install of CUDA, I didn't have this error, but tensorflow couldn't find any GPUs (despite nvidia-smi finding one).  After a second install based on these instructions: http://tech.marksblogg.com/tensorflow-nvidia-gtx-1080.html I had the following error.\n\nThanks for your help!\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n-rw-r--r-- 1 root root 189170 Sep 12 23:50 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 12 23:50 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Sep 12 23:50 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Sep 12 23:50 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Sep 12 23:50 /usr/local/cuda/lib/libcudart_static.a\n\nCompiled from:\nhttp://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n   I tensorflow/stream_executor/cuda/cuda_dnn.cc:2259] Unable to load cuDNN DSO\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n   0.10.0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n tf.Session()\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: XXX\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: XXX\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 352.99.0\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:356] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.44  Wed Aug 17 22:24:07 PDT 2016\nGCC version:  gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1~14.04.1) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: 367.44.0\nE tensorflow/stream_executor/cuda/cuda_diagnostics.cc:296] kernel version 367.44.0 does not match DSO version 352.99.0 -- cannot find working devices in this configuration\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n", "comments": ["After removing the NVIDIA drivers with apt remove --purge nvidia*`  and rebooting, I only got a blank screen upon restart. \r\n\r\nLuckily I was still able to log in via a terminal and [install the latest stable NVIDIA drivers](http://askubuntu.com/questions/744419/blank-screen-after-nvidia-install).\r\n\r\nBut probably there is a better way then just throwing all your drivers away.", "Also try rebooting, duh! I was having the same problem after updating packages but I found a solution on stackoverflow. IT WORKED! Before you try a heavy intervention, try restarting, it worked for me \ud83d\udc4d ", "@milangritta Genius !", "I had the same issue, but surprisingly rebooting the desktop worked for me! Before that I was not able to get any information even through 'nvidia-smi'. Now it's working without error !", "Restart solved the issue.", "> Also try rebooting, duh! I was having the same problem after updating packages but I found a solution on stackoverflow. IT WORKED! Before you try a heavy intervention, try restarting, it worked for me \ud83d\udc4d\r\n\r\nDO NOTHING AND JUST RESTART YOUR COMPUTER WILL FIX IT !!!!! THANKS!!", "Rebooting not works for me", "Here to confirm that rebooting work. Try it before anything else.", "Can someone help me please with this issue?\r\nThank you\r\n\r\n$ nvidia-smi\r\nFailed to initialize NVML: Driver/library version mismatch\r\n\r\n2021-10-16 14:56:09.420644: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\r\n2021-10-16 14:56:09.420676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Primer\r\n2021-10-16 14:56:09.420700: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Primer\r\n2021-10-16 14:56:09.420758: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\r\n2021-10-16 14:56:09.420782: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\r\n2021-10-16 14:56:09.420791: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.57.2 -- cannot find working devices in this configuration\r\n\r\n$ uname -a\r\nLinux Primer 5.4.0-88-generic #99-Ubuntu SMP Thu Sep 23 17:29:00 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ nvidia-settings --help\r\n\r\nnvidia-settings:  version 470.57.02\r\n  The NVIDIA X Server Settings tool.\r\n\r\n$ nvidia-smi\r\nFailed to initialize NVML: Driver/library version mismatch\r\n\r\n$ sudo prime-select query\r\nnvidia\r\n\r\n$ cat /proc/driver/nvidia/version\r\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  460.91.03  Fri Jul  2 06:04:10 UTC 2021\r\nGCC version:  gcc version 9.3.0 (Ubuntu 9.3.0-17ubuntu1~20.04)\r\n\r\n$ sudo lspci -v | grep VGA\r\n02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])\r\n03:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])\r\n04:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1) (prog-if 00 [VGA controller])\r\n\r\n$ lspci | grep -i nvidia\r\n02:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n02:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n03:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n03:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n04:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n04:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n\r\n$ dpkg -l | grep -i nvidia\r\nii  cuda-nsight-compute-11-2                        11.2.0-1   amd64        NVIDIA Nsight Compute\r\nii  cuda-nsight-compute-11-4                        11.4.2-1   amd64        NVIDIA Nsight Compute\r\nii  cuda-nsight-systems-11-2                        11.2.0-1   amd64        NVIDIA Nsight Systems\r\nii  cuda-nsight-systems-11-4                        11.4.2-1   amd64        NVIDIA Nsight Systems\r\nii  cuda-nvtx-11-2                                  11.2.67-1   amd64        NVIDIA Tools Extension\r\nii  cuda-nvtx-11-4                                  11.4.120-1   amd64        NVIDIA Tools Extension\r\nii  libaccinj64-10.1:amd64                          10.1.243-3   amd64        NVIDIA ACCINJ Library (64-bit)\r\nii  libcublas10:amd64                               10.1.243-3   amd64        NVIDIA cuBLAS Library\r\nii  libcublaslt10:amd64                             10.1.243-3   amd64        NVIDIA cuBLASLt Library\r\nii  libcudart10.1:amd64                             10.1.243-3   amd64        NVIDIA CUDA Runtime Library\r\nii  libcufft10:amd64                                10.1.243-3   amd64        NVIDIA cuFFT Library\r\nii  libcufftw10:amd64                               10.1.243-3   amd64        NVIDIA cuFFTW Library\r\nii  libcuinj64-10.1:amd64                           10.1.243-3   amd64        NVIDIA CUINJ Library (64-bit)\r\nii  libcupti-dev:amd64                              10.1.243-3   amd64        NVIDIA CUDA Profiler Tools Interface development files\r\nii  libcupti-doc                                    10.1.243-3   all          NVIDIA CUDA Profiler Tools Interface documentation\r\nii  libcupti10.1:amd64                              10.1.243-3   amd64        NVIDIA CUDA Profiler Tools Interface runtime library\r\nii  libcurand10:amd64                               10.1.243-3   amd64        NVIDIA cuRAND Library\r\nii  libcusolver10:amd64                             10.1.243-3   amd64        NVIDIA cuSOLVER Library\r\nii  libcusolvermg10:amd64                           10.1.243-3   amd64        NVIDIA cuSOLVERmg Library\r\nii  libcusparse10:amd64                             10.1.243-3   amd64        NVIDIA cuSPARSE Library\r\nii  libnppc10:amd64                                 10.1.243-3   amd64        NVIDIA Performance Primitives core runtime library\r\nii  libnppial10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Arithmetic and Logic\r\nii  libnppicc10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Color Conversion\r\nii  libnppicom10:amd64                              10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Compression\r\nii  libnppidei10:amd64                              10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Data Exchange and Initialization\r\nii  libnppif10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Filters\r\nii  libnppig10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Geometry transforms\r\nii  libnppim10:amd64                                10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Morphological operations\r\nii  libnppist10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Statistics\r\nii  libnppisu10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Support\r\nii  libnppitc10:amd64                               10.1.243-3   amd64        NVIDIA Performance Primitives lib for Image Threshold and Compare\r\nii  libnpps10:amd64                                 10.1.243-3   amd64        NVIDIA Performance Primitives for signal processing runtime library\r\nii  libnvgraph10:amd64                              10.1.243-3   amd64        NVIDIA Graph Analytics library (nvGRAPH)\r\nii  libnvidia-cfg1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA binary OpenGL/GLX configuration library\r\nii  libnvidia-common-460                            460.27.04-0ubuntu1   all          Shared files used by the NVIDIA libraries\r\nii  libnvidia-common-470                            470.57.02-0ubuntu1   all          Shared files used by the NVIDIA libraries\r\nii  libnvidia-compute-418:amd64                     430.50-0ubuntu3   amd64        Transitional package for libnvidia-compute-430\r\nii  libnvidia-compute-430:amd64                     470.57.02-0ubuntu1   amd64        Transitional package for libnvidia-compute-470\r\nrc  libnvidia-compute-450:amd64                     450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA libcompute package\r\nrc  libnvidia-compute-460:amd64                     460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA libcompute package\r\nii  libnvidia-compute-470:amd64                     470.57.02-0ubuntu1   amd64        NVIDIA libcompute package\r\nii  libnvidia-decode-470:amd64                      470.57.02-0ubuntu1   amd64        NVIDIA Video Decoding runtime libraries\r\nii  libnvidia-encode-470:amd64                      470.57.02-0ubuntu1   amd64        NVENC Video Encoding runtime library\r\nii  libnvidia-extra-470:amd64                       470.57.02-0ubuntu1   amd64        Extra libraries for the NVIDIA driver\r\nii  libnvidia-fbc1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA OpenGL-based Framebuffer Capture runtime library\r\nii  libnvidia-gl-470:amd64                          470.57.02-0ubuntu1   amd64        NVIDIA OpenGL/GLX/EGL/GLES GLVND libraries and Vulkan ICD\r\nii  libnvidia-ifr1-470:amd64                        470.57.02-0ubuntu1   amd64        NVIDIA OpenGL-based Inband Frame Readback runtime library\r\nii  libnvidia-ml-dev                                10.1.243-3   amd64        NVIDIA Management Library (NVML) development files\r\nii  libnvjpeg10:amd64                               10.1.243-3   amd64        NVIDIA JPEG library (nvJPEG)\r\nii  libnvrtc10.1:amd64                              10.1.243-3   amd64        CUDA Runtime Compilation (NVIDIA NVRTC Library)\r\nii  libnvtoolsext1:amd64                            10.1.243-3   amd64        NVIDIA Tools Extension Library\r\nii  libnvvm3:amd64                                  10.1.243-3   amd64        NVIDIA NVVM Library\r\nii  nsight-compute                                  10.1.243-3   amd64        NVIDIA Nsight Compute\r\nii  nsight-compute-2020.3.0                         2020.3.0.18-1   amd64        NVIDIA Nsight Compute\r\nii  nsight-compute-2021.2.2                         2021.2.2.1-1   amd64        NVIDIA Nsight Compute\r\nrc  nvidia-compute-utils-450                        450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA compute utilities\r\nrc  nvidia-compute-utils-460                        460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA compute utilities\r\nii  nvidia-compute-utils-470                        470.57.02-0ubuntu1   amd64        NVIDIA compute utilities\r\nii  nvidia-cuda-dev                                 10.1.243-3   amd64        NVIDIA CUDA development files\r\nii  nvidia-cuda-doc                                 10.1.243-3   all          NVIDIA CUDA and OpenCL documentation\r\nii  nvidia-cuda-gdb                                 10.1.243-3   amd64        NVIDIA CUDA Debugger (GDB)\r\nii  nvidia-cuda-toolkit                             10.1.243-3   amd64        NVIDIA CUDA development toolkit\r\nrc  nvidia-dkms-450                                 450.102.04-0ubuntu0.20.04.1   amd64        NVIDIA DKMS package\r\nrc  nvidia-dkms-460                                 460.91.03-0ubuntu0.20.04.1   amd64        NVIDIA DKMS package\r\nii  nvidia-dkms-470                                 470.57.02-0ubuntu1   amd64        NVIDIA DKMS package\r\nii  nvidia-driver-470                               470.57.02-0ubuntu1   amd64        NVIDIA driver metapackage\r\nrc  nvidia-kernel-common-450                        450.102.04-0ubuntu0.20.04.1   amd64        Shared files used with the kernel module\r\nrc  nvidia-kernel-common-460                        460.91.03-0ubuntu0.20.04.1   amd64        Shared files used with the kernel module\r\nii  nvidia-kernel-common-470                        470.57.02-0ubuntu1   amd64        Shared files used with the kernel module\r\nii  nvidia-kernel-source-470                        470.57.02-0ubuntu1   amd64        NVIDIA kernel source package\r\nii  nvidia-modprobe                                 470.57.02-0ubuntu1   amd64        Load the NVIDIA kernel driver and create device files\r\nii  nvidia-prime                                    0.8.14   all          Tools to enable NVIDIA's Prime\r\nii  nvidia-profiler                                 10.1.243-3   amd64        NVIDIA Profiler for CUDA and OpenCL\r\nii  nvidia-settings                                 470.57.02-0ubuntu1   amd64        Tool for configuring the NVIDIA graphics driver\r\nii  nvidia-utils-470                                470.57.02-0ubuntu1   amd64        NVIDIA driver support binaries\r\nii  nvidia-visual-profiler                          10.1.243-3   amd64        NVIDIA Visual Profiler for CUDA and OpenCL\r\nii  nvtop                                           1.0.0-1ubuntu2   amd64        Interactive NVIDIA GPU process monitor\r\nii  screen-resolution-extra                         0.18build1   all          Extension for the nvidia-settings control panel\r\nii  xserver-xorg-video-nvidia-470                   470.57.02-0ubuntu1   amd64        NVIDIA binary Xorg driver\r\n"]}, {"number": 4348, "title": "bazel can not compile Tensorflow with GCC 6.2.1", "body": "**When I launch** \n\"bazel build  -c opt --jobs=10 tensorflow/tools/pip_package:build_pip_package\"\n**to compile tensorflow from source ,it crashed at:**\n\"external/com_googlesource_code_re2/BUILD:11:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-canonical-system-headers ... (remaining 32 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nexternal/com_googlesource_code_re2/re2/dfa.cc: In constructor 're2::DFA::State::State()':\nexternal/com_googlesource_code_re2/re2/dfa.cc:95:10: error: unknown array size in delete...\n\"\n**Here are some relative infos**:\n 1)bazel label: 0.3.1- (@non-git)\n 2)gcc (GCC) 6.2.1 20160830\n 3)tensorflow is the latest master branch\nCan you tell me how to configure bazel with clang(not cross compile) ? Thanks.\n", "comments": ["https://github.com/google/re2/issues/102  or https://gcc.gnu.org/bugzilla/show_bug.cgi?id=70932\n", "Looks related to a gcc bug. Assigning an owner from our side to track this until the gcc bug is fixed.\n", "This was fixed in https://github.com/tensorflow/tensorflow/commit/7ea4e2f"]}, {"number": 4347, "title": "Branch 132956583", "body": "Pushing internal changes.\n", "comments": ["@tensorflow-jenkins Test this please.\n", "Closing in hopes new PR https://github.com/tensorflow/tensorflow/pull/4360 fixes issues.\n"]}, {"number": 4346, "title": "Android version of TensorFlow is missing dequantize operation", "body": "### Environment info\n\nOperating System: macOS Sierra, building for Android API 19\n### Minimal reproducible example\n\nAny graph that uses either of these operations. Using the quantization tool at `tensorflow/contrib/quantization/tools:quantize_graph` will add quantize and dequantize ops to a graph, which will subsequently not work on Android.\n### What other attempted solutions have you tried?\n\nBuilding with the `makefile` module.\n", "comments": ["To unblock while we figure out a cleaner solution, can you try adding `\"//third_party/tensorflow/contrib/quantization/kernels:android_ops\"` to the srcs for //tensorflow/core:android_tensorflow_lib?\n", "I added `\"//tensorflow/contrib/quantization/kernels:android_ops\"` to the list of sources for `android_tensorflow_lib` but I got an error `public/gemmlowp.h: No such file or directory` on an `#include`. I'm looking into a fix.\n", "Not sure how to ensure that the `gemmlowp` library is included. Any help is appreciated.\n", "If you also add `//third_party/gemmlowp:eight_bit_int_gemm_sources` to the srcs list it should be able to find the missing files.\n", "I tried that but Bazel wasn't able to find that package. I did add `//tensorflow/contrib/quantization:cc_ops` to the `deps` for `android_tensorflow_lib` but then I get `external/highwayhash/BUILD:17:1: C++ compilation of rule '@highwayhash//:sip_hash' failed`. There are some other errors related to that module as well. I'm not sure if I'm including the wrong thing, or if there's some other dependency I'm missing.\n", "Only TF targets that start with \"android_\" will build for Android, so `//tensorflow/contrib/quantization:cc_ops` is not expected to work here.\n\nDid you clone with `--recurse-submodules`? Alternatively try `tensorflow/contrib/makefile/download_dependencies.sh`\n", "Okay, I had been using a downloaded ZIP, or `git clone` to get TensorFlow, but using clone with `recurse-submodules` seemed to do the trick, so maybe that was the problem all along. Thanks!\n", "Sorry -- this fixed the issue with batch normalization, but the dequantize operation is still missing. I forgot that I had removed the quantize step from my training process.\n", "Ah, sorry; just to make this more fun, I think you may also need to add `tensorflow/contrib/quantization:android_ops` to your srcs, which should actually register the op.\n", "With this changeset in `tensorflow/core/BUILD`, I'm still having the same issue.\n\n```\n@@ -712,7 +712,9 @@ cc_library(\n # binary size (by packaging a reduced operator set) is a concern.\n cc_library(\n     name = \"android_tensorflow_lib\",\n-    srcs = if_android([\":android_op_registrations_and_gradients\"]),\n+    srcs = if_android([\":android_op_registrations_and_gradients\",\n+    \"//tensorflow/contrib/quantization:android_ops\",\n+    \"//tensorflow/contrib/quantization/kernels:android_ops\"]),\n     copts = tf_copts(),\n     linkopts = [\"-lz\"],\n     tags = [\n@@ -725,6 +727,7 @@ cc_library(\n         \":protos_cc\",\n         \"//tensorflow/core/kernels:android_tensorflow_kernels\",\n         \"//third_party/eigen3\",\n+        \"//tensorflow/contrib/makefile/downloads/gemmlowp\",\n     ],\n     alwayslink = 1,\n )\n```\n", "@steverichey I think you need to add `//third_party/gemmlowp:eight_bit_int_gemm_sources` to the srcs and remove `//tensorflow/contrib/makefile/downloads/gemmlowp` from the deps.\n", "With that change I get `ERROR: ../tensorflow/tensorflow/core/BUILD:713:1: no such package 'third_party/gemmlowp': BUILD file not found on package path and referenced by '//tensorflow/core:android_tensorflow_lib'.`\n\nIf it helps, I seem to have gemmlowp in `bazel-bin/external/gemmlowp` and `bazel-tensorflow/external/gemmlowp` but I have no idea how to tell Bazel to find it there.\n", "Sorry, my mistake -- that should have been `@gemmlowp//:eight_bit_int_gemm_sources` instead.\n", "```\nIn file included from tensorflow/contrib/quantization/kernels/quantized_bias_add_op.cc:18:0:\n./tensorflow/contrib/quantization/kernels/quantization_utils.h:28:29: fatal error: public/gemmlowp.h: No such file or directory\n #include \"public/gemmlowp.h\"\n```\n\n\ud83d\ude2d \n", "Ok, looks like we just need to add an extra include dir. I just tested by changing copts to:\n    `copts = tf_copts() + [\"-Iexternal/gemmlowp\"],`\nand can confirm it builds. Let's just hope it works for you at runtime :)\n", "This does indeed fix the problem at build and runtime. This reduces the size of my frozen graph definition file to 25% of its previous value, but I'm not seeing the performance improvement I would expect at runtime. Running on a Nexus 6 with Nougat, running a session with the same input takes twice as long. Is this expected?\n", "Hi @steverichey @andrewharp , I need your help,Thx. I follow these steps, but Still Error.\n\nERROR: /home/wx/Downloads/tensorflow/tensorflow/core/BUILD:722:12: in deps attribute of cc_library rule //tensorflow/core:android_tensorflow_lib: filegroup rule '@gemmlowp//:eight_bit_int_gemm_sources' is misplaced here (expected cc_inc_library, cc_library or objc_library).\nERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.\n\nMy BUILD File:\n\n```\ncc_library(\n    name = \"android_tensorflow_lib\",\n    srcs = if_android([\":android_op_registrations_and_gradients\",\n    \"//tensorflow/contrib/quantization:android_ops\",\n    \"//tensorflow/contrib/quantization/kernels:android_ops\"]),\n    copts = tf_copts() + [\"-Iexternal/gemmlowp\"],\n    linkopts = [\"-lz\"],\n    tags = [\n        \"manual\",\n        \"notap\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":android_tensorflow_lib_lite\",\n    \":protos_cc\",\n        \"//tensorflow/core/kernels:android_tensorflow_kernels\",\n        \"//third_party/eigen3\",\n    \"@gemmlowp//:eight_bit_int_gemm_sources\",\n    ],\n    alwayslink = 1,\n)\n```\n", "@shisu250 \"@gemmlowp//:eight_bit_int_gemm_sources\" is a source filegroup, not a cc_library target, so it needs to go in the srcs list instead.\n", "@steverichey QuantizedConv2D is probably the culprit -- I believe the fast pathways may not be enabled yet due to padding or range issues. You can find similar reports in #2807.\n", "@andrewharp I'll keep an eye on that issue then. Should I close this? Should I open a pull request with the required changes, or will the TensorFlow team address them in another way?\n", "@steverichey We'll be moving the quantization code into core/ sometime soon, at which point these changes will no longer be necessary.\n", "@andrewharp Thanks for all your help!\n", "@andrewharp Thanks for your help, It worked.\n\nMy BUILD (tensorflow/tensorflow/core/BUILD) File is:\n\n```\ncc_library(\n    name = \"android_tensorflow_lib\",\n    srcs = if_android([\":android_op_registrations_and_gradients\",\n    \"//tensorflow/contrib/quantization:android_ops\",\n    \"//tensorflow/contrib/quantization/kernels:android_ops\",\n    \"@gemmlowp//:eight_bit_int_gemm_sources\"]),\n    copts = tf_copts() + [\"-Iexternal/gemmlowp\"],\n    linkopts = [\"-lz\"],\n    tags = [\n        \"manual\",\n        \"notap\",\n    ],\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":android_tensorflow_lib_lite\",\n        \":protos_cc\",\n        \"//tensorflow/core/kernels:android_tensorflow_kernels\",\n        \"//third_party/eigen3\",\n    ],\n    alwayslink = 1,\n)\n```\n\nNow,It worked.\n", "Hi. @andrewharp   After dequantize. Android become very slow. You mean We haven't solve this problem In Android ?  But in I0S is very fast when I do after dequantize. the IOS issue is #3619 . what can i do in android.  don't use dequantize until  wait for performance problem be solve? Please give me a suggestion, thanks.\n", "@petewarden Do we know why exactly dequantize is fast on iOS but not on Android?\n", "No, I'm afraid I haven't been able to look into it. @cwhipkey did some vectorization using Eigen that might be related? It's a high priority to get this sorted out, since it comes up a lot!\n", "Hi. @andrewharp @petewarden   What time can solve android dequantize performance problem ? If solved, can you inform me ? Thx.\n"]}, {"number": 4345, "title": "Add code to close session", "body": "Hi all, I read the tensorflow tutorial and find that we must close session when finishing using it. However, when I review the code in [cifar10 tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/image/cifar10), I find that in some files,  there is no `sess.close()` line. I don't know whether it's a function of `tf.app` utilities, so I add the session closing code and make you know about this issue.  Hope this can be helpful.\n", "comments": ["Can one of the admins verify this patch?\n", "@vra, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @benoitsteiner to be potential reviewers\n", "@vra Thanks for the commit! My assumption though is that session.close() will be called automatically by the __del__ method when session goes out of scope, so functionally the logic here would be identical.  @shlens can you confirm? \n", "Correct, the Session is closed when it goes out of scope so this change is not necessary.\n", "Okay I got it. Thanks for your explanation. I will close this PR.\n"]}, {"number": 4344, "title": "Merge release branch r0.10 back into master.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @martinwicke and @keveman to be potential reviewers\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "This is a merge, CLA good to ignore.\n"]}, {"number": 4343, "title": "Commit that breaks Bazel 0.3.0 support", "body": "If I try to configure the lastest master branch (db45268db0203fcdb0e48e3e558b01244424c0a6) using Bazel 0.3.0, I get:\n\n```\nERROR: [...]/tensorflow/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):\n        File \"[...]/tensorflow/tensorflow/tensorflow.bzl\", line 562\n                rule(attrs = {\"srcs\": attr.label_list...\"), <3 more arguments>)}, <2 more arguments>)\n        File \"[...]/tensorflow/tensorflow/tensorflow.bzl\", line 568, in rule\n                attr.label_list(cfg = \"data\", allow_files = True)\nexpected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.\n```\n\nUsing 0.3.1 avoids this and reverting the offending commit 7bcdcbbf60fc08346fd8016270a0563f4b51362b also fixes it.\n### Environment info\n\nSetup: CentOS, Bazel 0.3.0, CUDA 7.5, CuDNN 5.1, Tensorflow master (db45268db0203fcdb0e48e3e558b01244424c0a6)\n\nI run `configure` and set it up for GPU support. I see the error above if I use bazel 0.3.0.\n### Fix\n\nUpgrade to 0.3.1, obviously. However, since 0.3.0 is still officially supported, I thought this deserves to be an issue.\n\nAnother fix is to revert the commit 7bcdcbbf60fc08346fd8016270a0563f4b51362b. Note, I didn't have to revert to its parent; undoing that commit alone is enough:\n\n```\ngit show -R 7bcdcbbf | git apply\n```\n", "comments": ["Sorry, this is a duplicate of #4319. I'm closing this.\n"]}, {"number": 4342, "title": "Invert gradient op", "body": "This one should be pretty simple. It would be nice to have an op that acts as the identity function in feedforward, but during backpropagation it propagates the negative of the gradients. This is useful when joining two datasets as it allows for the suppression of features that distinguish between domains. Its referred to as a gradient reversal layer in [http://arxiv.org/pdf/1505.07818v4.pdf](http://arxiv.org/pdf/1505.07818v4.pdf)\n", "comments": ["You can do it by using stop_gradient --\nhttp://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182#36480182\n\nOn Mon, Sep 12, 2016 at 3:58 PM, alexatknit notifications@github.com\nwrote:\n\n> This one should be pretty simple. It would be nice to have an op that acts\n> as the identity function in feedforward, but during backpropagation it\n> propagates the negative of the gradients. This is useful when joining two\n> datasets as it allows for the suppression of features that distinguish\n> between domains. Its referred to as a gradient reversal layer in\n> http://arxiv.org/pdf/1505.07818v4.pdf\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4342, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHODc8-bx34C_YTaS3JOlNbSBK5-Mks5qpdkRgaJpZM4J7Hwx\n> .\n", "Nice!\n", "Thanks Yaroslav. Closing this out.\n"]}]