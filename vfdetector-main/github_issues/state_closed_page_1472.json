[{"number": 8784, "title": "fixes bazel/skylark handling of grep terminal colors when checking cudnn version", "body": "Fixes #8702\r\nThe issue was that on some systems, the environment or terminal are set up to use `grep` in a way that produces color-related formatting. They cannot be circumvented easily by command line manipulations. These \"silent\" color-formatting characters then go on to cause difficult-to-diagnose issues with the version checking logic in bazel/skylark.\r\n\r\nThe updated skylark logic would address this issue.", "comments": ["Can one of the admins verify this patch?", "When checking my terminal, it looks like the color in the output of grep comes from the hardcoded flag in the alias:\r\n```\r\n$ alias grep\r\nalias grep='grep --color=auto'\r\n```\r\n\r\nCould you try modifying the command a few lines above to `\\grep` instead of `grep` to disable the alias, instead of this change?", "@gunan thank you for your input. I agree that it is the better fix. I force-pushed on to my PR branch with the updated change.", "Jenkins, test this please.\r\n\r\n@liuyipei Thanks for the change.\r\nDoes the fix (as it is right now) work for you?", "@gunan yes it did.  Thanks!", "@jart @davidzchen Since we simplified the change, I went ahead and approved it.\r\nSorry for the extra emails you received."]}, {"number": 8783, "title": "Fix uninitialized local variable", "body": "Error is: \r\n`FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input/input_producer/input_producer/fraction_of_32_full/limit_epochs/epochs`\r\n\r\nThere exists a discussion about the problem and resolution [here](https://github.com/tensorflow/tensorflow/issues/3168).", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Please make sure you signed the CLA and reply to this thread as instructed by googlebot. Thanks!", "The CLA for this PR is related to [this one](https://github.com/tensorflow/tensorflow/pull/8758).", "Sounds good, we might have to wait a little bit then. Thanks for your patience.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Woohoo! \r\nJenkins, test this please.", "Checking if that's a transient failure on mac with syncreplicas.\r\n\r\nJenkins, test this please.", "Ignoring unrelated failure."]}, {"number": 8782, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension must be 2 but is 3 for 'transpose_1' (op: 'Transpose') with input shapes: [32,256], [3].", "body": "I am running the carpedm20/attentive-reader-tensorflow at https://github.com/carpedm20/attentive-reader-tensorflow. This is a Tensorflow implementation of Google DeepMind's Teaching Machines to Read and Comprehend.\r\n\r\n\r\nWhen I run the command on my terminal:  python main.py --dataset cnn\r\n\r\nI get the following error. Can some one suggest how do I fix it?\r\n\r\n{'batch_size': 32,\r\n 'checkpoint_dir': 'checkpoint',\r\n 'data_dir': 'data',\r\n 'dataset': 'cnn',\r\n 'decay': 0.95,\r\n 'epoch': 25,\r\n 'forward_only': False,\r\n 'learning_rate': 5e-05,\r\n 'model': 'LSTM',\r\n 'momentum': 0.9,\r\n 'vocab_size': 100000}\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n [*] Building Deep LSTM...\r\ninitial state is \r\nTensor(\"zeros:0\", shape=(32, 768), dtype=float32, device=/device:CPU:0)\r\n [*] Loading vocab from data/cnn/cnn.vocab100000 ...\r\n [*] Loading vocab finished.\r\nPrinting self.emb\r\nTensor(\"emb/read:0\", shape=(264560, 256), dtype=float32, device=/device:CPU:0)\r\nprinting self.inputs\r\nTensor(\"Placeholder:0\", shape=(32, 1000), dtype=int32, device=/device:CPU:0)\r\nprinting embed_inputs\r\nTensor(\"embedding_lookup:0\", shape=(1000, 32, 256), dtype=float32, device=/device:CPU:0)\r\nTraceback (most recent call last):\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 670, in _call_cpp_shape_fn_impl\r\n    status)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension must be 2 but is 3 for 'transpose_1' (op: 'Transpose') with input shapes: [32,256], [3].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 53, in <module>\r\n    tf.app.run()\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 48, in main\r\n    FLAGS.data_dir, FLAGS.dataset)\r\n  File \"/Users/skreddy/TensorFlow/PycharmProjects/Teaching_Machines_to_Read and_Comprehend/deep_lstm.py\", line 105, in train\r\n    self.prepare_model(data_dir, dataset_name, vocab_size)\r\n  File \"/Users/skreddy/TensorFlow/PycharmProjects/Teaching_Machines_to_Read and_Comprehend/deep_lstm.py\", line 76, in prepare_model\r\n    initial_state=self.initial_state)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 489, in dynamic_rnn\r\n    for input_ in flat_input)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 489, in <genexpr>\r\n    for input_ in flat_input)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1288, in transpose\r\n    ret = gen_array_ops.transpose(a, perm, name=name)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3841, in transpose\r\n    result = _op_def_lib.apply_op(\"Transpose\", x=x, perm=perm, name=name)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2397, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1757, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1707, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/Users/skreddy/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 675, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Dimension must be 2 but is 3 for 'transpose_1' (op: 'Transpose') with input shapes: [32,256], [3].\r\n", "comments": ["We cannot support models provided by others. Please file an issue on that repository directly."]}, {"number": 8781, "title": "fixed broken references to summarize_graph", "body": "\"summarize_graph.cc\" doesn't exist (anymore),\r\nupdated references to \"summarize_graph_main.cc\"", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8780, "title": "Cannot import tensorflow in python ", "body": "I get the following error when I try to import tensorflow in  python \r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\nPython Version:-\r\nPython 2.7.10 (default, Jul 30 2016, 19:40:32) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nTensor Flow Version :- \r\nName: tensorflow\r\nVersion: 1.0.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: wheel, protobuf, numpy, six, mock\r\n\r\nProtobuf\r\nName: protobuf\r\nVersion: 3.2.0\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: New BSD License\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: setuptools, six\r\n\r\nNumpy \r\nName: numpy\r\nVersion: 1.12.1\r\nSummary: NumPy: array processing for numbers, strings, records, and objects.\r\nHome-page: http://www.numpy.org\r\nAuthor: NumPy Developers\r\nAuthor-email: numpy-discussion@scipy.org\r\nLicense: BSD\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: \r\n\r\nworking on MAC sierra . Please let me know what could be the issue. ", "comments": ["Some common issues are mentioned at the end of our installation guide:\r\nhttps://www.tensorflow.org/install/install_mac#common_installation_problems\r\n\r\nI am pretty certain that this is not a bug in TF, it is an issue in your setup.\r\nTherefore, I recommend asking in StackOverflow if the installation guide does not help resolving your problem.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8779, "title": "Contrintuitive get_variable dtype checks", "body": "When reusing variables get_variable checks if dtypes are consistent, but it seems to compare the wrong dtypes.\r\n\r\nExample:\r\n```\r\na = tf.get_variable('a', initializer=tf.zeros(2, tf.int32))\r\nwith tf.variable_scope('', reuse=True):\r\n  tf.get_variable('a', initializer=tf.zeros(2, tf.int32)) # raises ValueError\r\n  tf.get_variable('a', initializer=tf.zeros(2, tf.int16)) # raises ValueError\r\n\r\nb = tf.get_variable('b', initializer=tf.zeros(2))\r\nwith tf.variable_scope('', reuse=True):\r\n  tf.get_variable('b', initializer=tf.zeros(2, tf.int32)) # works\r\n```\r\nHere the TF will compare dtype of existing variable `a` (tf.int32) against the dtype of the next get_variable call, which is tf.float32 (which is the default value for dtype argument of get_variable).\r\n\r\nThe resulting situations where get_variable raises ValueError seems counterintuitive.", "comments": ["Hmm, it looks like maybe we should simply allow to reuse without specifying dtype? Or is there something else you'd suggest? Looks like we should loosen some checks, but I'm not exactly sure yet which - do you have a suggestion?", "I think these checks are useful against copypast errors.\r\n\r\nOne of the ways to go is to change default value of the argument `dtype` from `tf.float32` to `None`. If `dtype` is `None` and `initializer` is provided, check the `dtype` of the existing variable against the `initializer` (not the `dtype` argument).\r\n\r\nIn this case the situation should be as follows:\r\n```\r\ntf.get_variable('a', initializer=tf.zeros(2, tf.int32))\r\nwith tf.variable_scope('', reuse=True):\r\n  tf.get_variable('a', initializer=tf.zeros(2, tf.int32)) # works\r\n  tf.get_variable('a', dtype=tf.int32) # works\r\n  tf.get_variable('a') # works, because nor dtype nor initializer are provided, nothing to compare against.\r\n  tf.get_variable('a', initializer=tf.zeros(2, tf.int16)) # raises ValueError\r\n  tf.get_variable('a', dtype=tf.float32) # raises ValueError\r\n\r\ntf.get_variable('b', dtype=tf.int16, initializer=tf.zeros(2, tf.int32)) # raises ValueError\r\n```\r\n\r\nAnd we can apply the same principle to shape checks as well.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8778, "title": "Getting started: anaconda and tensorflow", "body": "Hi,\r\n\r\nwhen you install tensorflow from anaconda, it does not provide any models as they are (probably) not part of the pip package. Could you update the documentation so it's states that you need to download the models manually?\r\n\r\nThanks!\r\n", "comments": ["Which models are you referring to that require manual downloads? And where are you installing from. If you are installing via conda install (and downloading from a conda repository) then that conda copy of Tensorflow is not maintained by the Tensorflow team. It's community run. \r\nFor the normal pip version I dont think i've had a problem. However, note that a lot of the models are downloaded via scripts separately to save on space for core Tensorflow.", "@jubjamie is correct that all we have about anaconda is community supported.\r\nIf you are referring to a tutorial in `www.tensorflow.org`, please feel free to send a PR, as all the documentation there is in our repository.\r\nIf this is a tutorial in google developers sites, please share the link so that we can see which tutorial is causing the confusion.\r\nIf you are talking about another tutorial, we wont be able to help you with those, and you will have to reach out to the site/tutorial owners.", "@gunan yes, I was refering to https://www.tensorflow.org/versions/r0.10/get_started/os_setup#run_a_tensorflow_demo_model\r\n\r\nIt would be very helpful if that doc would inform the reader that if he installed tensorflow using anaconda, he needs to download model files from https://github.com/tensorflow/models and add them to the PATH.\r\n\r\n", "The documentation you linked is for 0.10.\r\nIf you install 0.10, the documentation is correct.\r\nBut if you install a later version of TF, you will need to refer to the documentation for that version.\r\n\r\nDoes that answer your concern?\r\n\r\n", "Closing due to inactivity.\r\nPlease reopen if you have more questions."]}, {"number": 8777, "title": "scope issue of '_linear' method in 'tensorflow.python.ops.rnn_cell_impl' module", "body": "\r\nIn 'core_rnn_cell_impl' module _linear method, the arg 'scope' is not used in the method, easily causing variable conflicts. eg:\r\nwith tf.variable_scope(scope):\r\n        zw = core_rnn_cell_impl._linear(self.hyper_output, self.hyper_embedding_size, False, scope=scope + \"z\")        \r\n        alpha = core_rnn_cell_impl._linear(zw, dimensions, False, scope=scope + \"alpha\")\r\nzw, alpha(forgive me for the indent in format) although defined in their specific scope, they will collide each other for using the same variable name.\r\n\r\nI look up the 'core_rnn_cell_impl' code and I find the 'scope' arg has not been truly used.\r\nThis is the 1.0.0 version:\r\nscope = vs.get_variable_scope()\r\nwith vs.variable_scope(scope) as outer_scope:\r\n\r\nThis is the 0.12.0 version:\r\nwith vs.variable_scope(scope or \"Linear\"):\r\n\r\nPlease check if it's truly the problem, thanks a lot !", "comments": ["@ebrevdo, could you comment please.", "yes... it's an internal method and isn't meant for external use.  i think\nwe got rid of the scope= argument in master branch.\n\nOn Tue, Mar 28, 2017 at 4:28 PM, Andrew Selle <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo>, could you comment please.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8777#issuecomment-289935941>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8eMrP1HXhMmCx_MMlCnohxGnhaHks5rqZeWgaJpZM4Mrtu_>\n> .\n>\n", "Thanks"]}, {"number": 8776, "title": "tf.case giving unexpected result in TF 1.0.1", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI posted this SO question on 2017-03-10 which was never answered: http://stackoverflow.com/questions/42728235/tensorflow-why-is-tf-case-giving-me-the-wrong-result\r\n\r\n### Environment info\r\nOperating System: `Linux 312e492cd9df 4.4.0-66-generic #87-Ubuntu SMP Fri Mar 3 15:29:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\nInstalled version of CUDA and cuDNN: none\r\n\r\nInstalled from: I'm running this on official tensorflow-devel Docker image for 1.0.1 (`gcr.io/tensorflow/tensorflow:1.0.1-devel`)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nglobal_step = tf.Variable(0, dtype=tf.int64)\r\ntrain_op = tf.assign(global_step, global_step + 1)\r\n\r\nlearning_rate = tf.Variable(0.1, dtype=tf.float32, name='learning_rate')\r\n\r\n# Update the learning_rate tensor conditionally\r\n# When global_step == 2, update to 0.01\r\n# When global_step == 4, update to 0.001\r\ncase_tensors = [\r\n    (tf.equal(global_step, 2), tf.constant(0.01, dtype=tf.float32)),\r\n    (tf.equal(global_step, 4), tf.constant(0.001, dtype=tf.float32)),\r\n]\r\ncases = [(pred, lambda: fn_tensor) for pred, fn_tensor in case_tensors]\r\nupdate = tf.case(cases, default=lambda: learning_rate)\r\nupdated_learning_rate = tf.assign(learning_rate, update)\r\n\r\nprint tf.__version__\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for _ in xrange(6):\r\n        print sess.run([global_step, case_tensors, learning_rate, update, updated_learning_rate])\r\n        sess.run(train_op)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nNone\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nThe above code prints the following output:\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n1.0.1\r\n[0, [(False, 0.0099999998), (False, 0.001)], 0.1, 0.1, 0.1]\r\n[1, [(False, 0.0099999998), (False, 0.001)], 0.1, 0.1, 0.1]\r\n[2, [(True, 0.0099999998), (False, 0.001)], 0.001, 0.001, 0.001]\r\n[3, [(False, 0.0099999998), (False, 0.001)], 0.001, 0.001, 0.001]\r\n[4, [(False, 0.0099999998), (True, 0.001)], 0.001, 0.001, 0.001]\r\n[5, [(False, 0.0099999998), (False, 0.001)], 0.001, 0.001, 0.001]\r\n```\r\n\r\nI expect that the learning rate should get set to `0.0099999998` when the global step reaches 2.  However, even though the predicate for global_step==2 evaluates to True, the learning rate does not get set to `0.0099999998`, but rather gets set to `0.001` instead.", "comments": ["```\r\ncase_tensors = [\r\n    (tf.equal(global_step, 2), tf.constant(0.01, dtype=tf.float32)),\r\n    (tf.equal(global_step, 4), tf.constant(0.001, dtype=tf.float32)),\r\n]\r\ncases = [(pred, lambda: fn_tensor) for pred, fn_tensor in case_tensors]\r\n```\r\nI believe this is your problem. You are constructing a lambda of already created op handles. You need to give the conditional control over when to actually create the ops... i.e.\r\n```\r\ncases = [\r\n    (tf.equal(global_step, 2), lambda: tf.constant(0.01, dtype=tf.float32)),\r\n    (tf.equal(global_step, 4), labmda: tf.constant(0.001, dtype=tf.float32)),\r\n]\r\n```\r\nYou may also need a control dependency between global_step and your case. Honestly, I'd recommend using tf.where for this type of usage anyways.\r\n", "Great point, thanks. When the lambdas are constructed properly, I get the expected behavior. Please close.", "Incidentally, would you be able to explain why defining the lambda with an already-constructed tensor gives a poor result? Or how I might be able to use tf.where to achieve this result of:\r\n\r\nif global_step == 2, then set learning_rate = 0.01\r\nif global_step == 4, then set learning_rate = 0.001\r\nelse, keep learning_rate the same", "Ah - I figured out the tf.where usage. I suppose you refer to using tf.where to get the index of the condition that is true, and then indexing into an array of desired new values. But would still be curious as to the first question.", "In particular, if I want it such that if no condition is met, then I want to keep learning_rate the same, then this seems difficult with both tf.where and tf.case (the latter because I cannot let a lambda return an already-constructed tensor).\r\n\r\nHappy to post this on SO if that's more appropriate, thanks", "OK, back to the original bug.  This is what weirds me out.  `test(behavior=1)` gives the unexpected behavior, while `test(behavior=2)` gives the expected behavior:\r\n\r\n```\r\ndef test(behavior):\r\n    global_step = tf.Variable(0, dtype=tf.int64)\r\n    train_op = tf.assign(global_step, global_step + 1)\r\n\r\n    learning_rate = tf.Variable(0.1, dtype=tf.float32, name='learning_rate')\r\n\r\n    if behavior == 1:\r\n        case_tensors = [\r\n            (tf.equal(global_step, 2), tf.constant(0.01, dtype=tf.float32)),\r\n            (tf.equal(global_step, 4), tf.constant(0.001, dtype=tf.float32)),\r\n        ]\r\n        cases = [(pred, lambda: fn_tensor) for pred, fn_tensor in case_tensors]\r\n    elif behavior == 2:\r\n        x = tf.constant(0.01, dtype=tf.float32)\r\n        y = tf.constant(0.001, dtype=tf.float32)\r\n        cases = [\r\n            (tf.equal(global_step, 2), lambda: x),\r\n            (tf.equal(global_step, 4), lambda: y),\r\n        ]\r\n    update = tf.case(cases, default=lambda: learning_rate)\r\n    updated_learning_rate = tf.assign(learning_rate, update)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for _ in xrange(6):\r\n            print sess.run([global_step, update, updated_learning_rate])\r\n            sess.run(train_op)\r\n```\r\n\r\nIn particular, the output of `test(1)`:\r\n\r\n```\r\n[0, 0.1, 0.1]\r\n[1, 0.1, 0.1]\r\n[2, 0.001, 0.001]\r\n[3, 0.001, 0.001]\r\n[4, 0.001, 0.001]\r\n[5, 0.001, 0.001]\r\n```\r\n\r\nThe output of `test(2)`:\r\n\r\n```\r\n[0, 0.1, 0.1]\r\n[1, 0.1, 0.1]\r\n[2, 0.0099999998, 0.0099999998]\r\n[3, 0.0099999998, 0.0099999998]\r\n[4, 0.001, 0.001]\r\n[5, 0.001, 0.001]\r\n```", "Another example where `tf.case` gives unexpected behavior, this time looking at the behavior of `exclusive=False`:\r\n\r\nCode:\r\n```python\r\n    orig_label = tf.constant(0.046026)\r\n    label_bounds = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\r\n    preds = [tf.less(orig_label, bound) for bound in label_bounds]\r\n    # Pair each predicate with a function returning the index of the predicate.\r\n    pred_fn_pairs = [(pred, lambda: tf.constant(i))\r\n                     for i, pred in enumerate(preds)]\r\n    # If no predicate evaluates to true, default to returning the index after\r\n    # the index of the last predicate.\r\n    default = lambda: tf.constant(len(pred_fn_pairs))\r\n    case = tf.case(pred_fn_pairs, default=default, exclusive=False)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        print sess.run([preds, case])\r\n    print list(enumerate(label_bounds))\r\n    print len(preds)\r\n```\r\n\r\nOutput:\r\n```\r\n[[False, False, False, False, True, True, True, True], 7]\r\n[(0, 0.01), (1, 0.02), (2, 0.03), (3, 0.04), (4, 0.05), (5, 0.06), (6, 0.07), (7, 0.08)]\r\n8\r\n```\r\n\r\n(I expect that the first line of the output should read `[[False, False, False, False, True, True, True, True], 4]`, based on https://www.tensorflow.org/api_docs/python/tf/case):\r\n\r\n> If exclusive==False, execution stops are the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by default.", "The conditionals need to have access to create the ops themselves because they need to construct them and connect them to control dependencies I believe. Thus, you must let the control structure function have control on when the actual ops are created.", "Thanks @aselle, that makes sense.  Any word on the unexpected behavior when `exclusive=False` in the post above?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/8776#issuecomment-290175897\r\n\r\nI just confirmed that this behavior also occurs in the nightly build.  Will get confirmation from colleague on whether he also thinks the behavior is unexpected @knub ", "I would also expect `case` to evaluate to `4`.", "@yuanbyu, @jwayne's example above seems to be a inconsistency in the documentation and exact behavior with `exclusive=False`.", "@itsmeolivia The bug described in [this comment](https://github.com/tensorflow/tensorflow/issues/8776#issuecomment-290175897), confirmed by @aselle and @knub, still exists in the latest tensorflow version.\r\n\r\nI can work on a patch - can I open a new issue with a cleaner history, referencing this issue, and continue that way?", "yes, go for it.  I initially closed the issue because no one had commented on over 2 months but if you're still interested in finding a solution then have at it.", "This is not a TF issue.\r\n\r\nClosures are defined over names and not over values (https://stackoverflow.com/a/13355291). In this case, all the lambdas ended up with the value the variable had at the end of the loop. A simple solution is to have a lambda generate the lambdas you are trying to iterate over and pass the iterated value as the first and only argument. The closure is now defined over a constant, or the result of the evaluation of the first lambda (which only depends on the argument, which is itself copied).\r\n\r\nSo the code that yields the results you are looking for looks like:\r\n\r\n    import tensorflow as tf\r\n\r\n    orig_label = tf.constant(0.046026)\r\n    label_bounds = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]\r\n    preds = [tf.less(orig_label, bound) for bound in label_bounds]\r\n    # Pair each predicate with a function returning the index of the predicate.\r\n    z = lambda x: lambda: tf.constant(x)\r\n    pred_fn_pairs = [(pred, z(i))\r\n                     for i, pred in enumerate(preds)]\r\n    # If no predicate evaluates to true, default to returning the index after\r\n    # the index of the last predicate.\r\n    default = lambda: tf.constant(len(pred_fn_pairs))\r\n    case = tf.case(pred_fn_pairs, default=default, exclusive=False)\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        print(sess.run([preds, case]))\r\n    print(list(enumerate(label_bounds)))\r\n    print(len(preds))", "@knub and I agree with @kopekC. I incorrectly believed that the issue described was a TF issue. Rather, the problem code was constructed poorly, and this thread should remain closed."]}, {"number": 8775, "title": "Attention Decoder", "body": "Hi,\r\n@ebrevdo Eugene speaks here https://www.youtube.com/watch?v=RIR_-Xlbp7s (29:50) about a class AttentionDecoder and says it's under development (as of Tensorflow Summit date), however I can't find this class anywhere in the master/r1.1/r1.0 tensorflow code. Do you know when it will be added?", "comments": ["We need to update that video somehow.  It turns out it's cleaner to\nimplement attention as an RNNCell wrapper rather than decoder.  This way it\ncan be combined with other decoders easily.  See tensorflow master branch,\ntf.contrib.seq2seq.AttentionWrapper.  It's pretty feature complete, and can\nbe used with other RNNCells, wrappers, and the BasicDecoder.\n\nOn Tue, Mar 28, 2017 at 6:38 AM, mhnatiuk <notifications@github.com> wrote:\n\n> Hi,\n> @ebrevdo <https://github.com/ebrevdo> Eugene speaks here\n> https://www.youtube.com/watch?v=RIR_-Xlbp7s (29:50) about a class\n> AttentionDecoder and says it's under development (as of Tensorflow Summit\n> date), however I can't find this class anywhere in the master/r1.1/r1.0\n> tensorflow code. Do you know when it will be added?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8775>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5clNyjhcV_TYgzOpSy_HbQhRMTeks5rqQ1hgaJpZM4MrogY>\n> .\n>\n", "@ebrevdo Ok I tired with LSTMBasicCell and got \"state has no attribute attention\" exception.. looks like a bug. I will review and report back.\r\nThanks for the talk, it was great to suddenly realize that to scale up training to 8 GPUS I only need to use DeviceWrapper. You saved my whole week dude.\r\n\r\nHowever,\r\nI am trying to use BasicDecoder like you suggested and I get \r\n`InvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [1] [ but saw shape: ] [64]\r\n         [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All/_2029, rnn/Assert/Assert/data_0, rnn/stack/_2031, rnn/Assert/Assert/data_2, rnn/Shape_1/_2033)]]\r\n         [[Node: rnn/while/Identity_12/_2727 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:5\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4293_rnn/while/Identity_12\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:5\"](^_clooprnn/while/multi_rnn_cell/cell_5/lstm_cell/zeros/_208)]]\r\n`\r\nI can't figure it out, sequence_length tensor should be batch-sized and it is and error messege suggests that it should be [1]. This would be a case when a list of Tensors would be used instead of one fat tensor [time, batch_size, 1]\r\n\r\nMy target_vocab_size is 500K\r\nsize of RNN = 64, for testing\r\ndec_inp : <tf.Tensor 'embedded_inputs_1:0' shape=(100, ?, 64) dtype=float32>\r\ndecoder seq len is batch-sized (64) <tf.Tensor 'decoder_seq_len:0' shape=(?,) dtype=int32>\r\n\r\n\r\n```\r\nW_target_emb = tf.Variable(tf.random_uniform([target_vocab_size, size], -1.0, 1.0), name=\"W_target_emb\")\r\n\r\nhalf = tf.constant(0.5)\r\ndec_inp = tf.cast(tf.stack(self.decoder_inputs), tf.float32)\r\ndec_inp = tf.reshape(dec_inp,[encoder_max_size, -1, size], name = \"embedded_inputs\")\r\nif not forward_only:\r\n\t#helper = seq2seq.TrainingHelper(inputs = target_embedded_chars, sequence_length = self.decoder_seq_len, time_major=True)\r\n\thelper = seq2seq.ScheduledEmbeddingTrainingHelper(inputs = dec_inp,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t  sequence_length = self.decoder_seq_len,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t embedding = W_target_emb,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t sampling_probability = half,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t time_major=True)\r\n\t\r\nelse:\r\n\thelper = seq2seq.GreedyEmbeddingHelper(dec_inp, \r\n\t\t\t\t\t\t\t\t\t\t   start_tokens=self.decoder_inputs[0],\r\n\t\t\t\t\t\t\t\t\t\t   end_token=data_utils.EOS_ID)\r\n\t\r\ndecoder_cell = LSTMBlockCell(num_units=size)\r\ndecoder_cell = MultiRNNCell([DeviceWrapper(ResidualWrapper(decoder_cell),device=\"/gpu:%d\" % i) for i in range(8) ])\r\n\r\nmy_decoder = seq2seq.BasicDecoder(\r\n\t\tcell=decoder_cell,\r\n\t\thelper=helper,\r\n\t\tinitial_state=encoder_final_state)\r\n\r\n\r\ndecoder_outputs, decoder_state = seq2seq.dynamic_decode(my_decoder, output_time_major=False, parallel_iterations=32,\r\n\t\t\t   swap_memory = True)\r\n```", "I'm running into that same issue. How did you solve it?\r\n", "I switched back to using TrainingHelper instead of ScheduledEmbeddingHelper, and feeded embedded inputs by looking up embeddings using tf.nn.embedding_lookup. It works, now I run into other problem, the shape of decoder_outputs.rnn_output is (AFAIK) *dynamic* and it's not possible to feed it into tf.contrib.seq2seq.sequence_loss, since it expects TARGETS and LOGITS to have compatible shapes.\r\n@ebrevdo Do you have any solutions for feeding decoder_outputs.rnn_output as logits to sequence_loss?\r\n\r\nBTW Shouldn't \"weights\" argument in sequence to sequence loss become optional as if we don't use padding, we don't need weights == 0 for right-padding in decoding?"]}, {"number": 8774, "title": "can not import tensorflow ", "body": "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: zsx-All-Series\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.57  Mon Oct  3 20:37:01 PDT 2016\r\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.57.0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1065] LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libnvidia-fatbinaryloader.so.375.39: cannot open shared object file: No such file or directory\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/6233\r\nI have a similar problem\r\nThe last time, as if to change a setting, the problem was solved. Six months later, I found that the problem came again.\r\nI found my question from google, carefully read every answer, I still have no way to solve this problem and disappear, is random Mody I used the good, suddenly appeared again\r\n", "You can find where your libcuda.so.1 is located, and add it to your `LD_LIBRARY_PATH` to resolve this problem. You can find where the library is using the \"find\" command in your terminal.\r\n\r\nClosing the issue as it is clearly a user side setup issue, not a bug in TF.\r\nFor problems with your own setup, I recommend using StackOverflow.", "I can find this file, but I did not add it to the path. The way I do this is to copy this file to LD_LIBRARY_PATH, which is a link file. Added a link to this copied file. Still being given. I try your way. May be."]}, {"number": 8773, "title": "Can I use tensorflow on Windows10 with c++ and gpu support", "body": "I wish to use tensorflow on windows10 with c++ and gpu support. Is there any pre build SDK for windows? Or I need to compile from sources?\r\n\r\nwhen I tried to compile on windows with cmake and MSVC, smaple trainer cannot build correctly because of some header files missing, such as \"graph.pb.h\". Is there any solutions?\r\n\r\nENV:\r\nwindows10 vs2015 cmake3.6.2", "comments": ["This information is available in our website.\r\nPlease make sure you search for solutions before filing issues:\r\nhttps://www.tensorflow.org/install/install_windows", "@gunan Following the link you shared it shows only how to install Tensorflow for Python. However, the question of @xiaoguai0992  was about installing Tensorflow on Windows 10 with **C++**, where on the website, nothing was mentionned regarding this issue. \r\nTensorflow is an excellent framework and no body can say the opposit. However, it is mainly focused on python + Linux/Mac. I am a windows user, I use a lot Tensorflow/Keras with python but I'd love to see a demo on how to install Tensorflow with **C++** on **WINDOWS** properly. I know it may not be a priority right now, but a lot of people are learning tensorflow, making their own models, but for deployment, we may want to get the best of it, and c/c++ are better than python (I guess ^^). We may also have a lot of projects using c++ and we want to use tensorflow in these projects (I do).\r\n", "Sorry for the misunderstanding.\r\nThen, our instructions would be here:\r\nhttps://www.tensorflow.org/install/install_c\r\n\r\nI see that these do not mention windows, but I think we also build c libraries for windows.\r\n@asimshankar how different are the instructions for windows?", "We do build C libraries for Windows, but only with CPU support. Building GPU-enabled binaries as part of the release is on my TODO list but I haven't gotten around to it yet.\r\n\r\n- For CPU, you can download the `.dll` from: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.4.0.zip\r\n\r\n- We use bazel to build this, but the environment is a bit complicated. The script we use (which is what is run by the [nightly builds](https://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow-windows/) as well) is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/libtensorflow_cpu.sh)\r\n\r\nSo, that sort of summarizes the state we're in with regards to the C API and Windows (and I believe other language bindings like [those for C#](https://github.com/migueldeicaza/TensorFlowSharp) are using these releases) \r\nI'm happy to provide some pointers to next steps, but we (TensorFlow maintainers) are probably not going to have the bandwidth to do this ourselves in the near future.\r\n\r\n- If you really need the C++ API (as opposed to the smaller C API), then a similar process should work, except that you'll use bazel to build the `//tensorflow:libtensorflow_cc.so` target instead of `//tensorflow:libtensorflow.so` target.\r\n\r\n- For GPU support: The script outlined about should work, as long as the build was configured to use CUDA. But I have not tried that yet.\r\n\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "+1 to getting this to be officially listed in the documentation along with GPUs of course. :)", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I just have a look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_shared_lib.cmake\r\n\r\nthis file seems use a very tricky way to generate dll using cmake.\r\n\r\nIf you directly follow the instruction in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\n\r\nThe tensorflow_BUILD_SHARED_LIB option is default to be off, I am testing on building shared lib with this option on. cmake gui is recommended under windows enviornment to correctly set the dependencies.\r\n\r\nAlso this cmake file can generate TensorflowConfig.cmake after INSTALL build, location will be $INSTALL_DIR/lib/cmake, not sure if everything is alright, but worth a test", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 64 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "What's the status on this? It would be great to have GPU support in the C/C++ API!", "@andreas-eberle I have a C++ API example running on window with GPU [here](https://github.com/jackyko1991/vnet-tensorflow/tree/master/cxx)\r\n\r\nI have tried to issue pull request on tensorflow main repository but they never merge my code", "@jackyko1991: Can you link your PR?", "@andreas-eberle what PR refers to?", "Pull Request. I mean the Pull Request you mentioned that didn't get merged.", "https://github.com/tensorflow/tensorflow/pull/16480\r\n\r\nI have updated the pull request for several times. It is hard for me to catch up with lastest tf release for their slow response...I have stopped to modify the code sinice 1.8.0", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@case540 : If we can get the release process to build the C and JNI libraries for Windows GPU (#16660) then I think we'll be closer to being able to build the C++ libraries the same way.\r\n\r\n@martinwicke @gunan : Would this be something that the build SIG should take on?\r\n(https://www.tensorflow.org/community/lists)\r\n\r\n", "I was under the impression that our Windows GPU release builds were broken (due to some protobuf issue).", "Protobuf issue is fixed. As far as I can remember, we cherrypicked that fix into 1.9. But I am not 100% on that.\r\n1.10 definitely has that fix.", "@asimshankar   Thanks for your explanation, but I still have problems building Tensorflow C++ library for Windows 10, even for CPU. I have replaced tensorflow_jni.so with tensorflow_jni.dylib in libtensorflow_cpu.sh and in BUILD file. I have also replaced libtensorflow.so with libtensorflow.dll and libtensorflow_cc.so with libtensorflow_cc.dll in these two files, as advised in one of the comments. Otherwise, I had error like this\r\n\"/tensorflow/tensorflow/tools/lib_package/BUILD:153:1: no such package '@png_archive//'\" even though I have installed GNUWin32 which should also contain this package.\r\nStill, the problem persists, as now it complains on the absense of package  '@local_config_git//' .  What should I do to fix it?\r\n\r\nAlso, simple including 'tensorflow.dll' from here https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.4.0.zip  doesn't make package tensorflow visible in any way.", "What is the current status of building a tensorflow C (or C++) dll on windows?\r\nFrom what I've been reading here and elsewhere, the CMake build process is deprecated.\r\nThe current Bazel instructions only include C instructions for Mac or Linux, not Windows.\r\nIs there any documentation to use Bazel (or whatever the preferred build process is) for building on Windows?", "@meteorcloudy ", "if possible can you provide the method for library linking and header file locations?", "Working on https://github.com/tensorflow/tensorflow/pull/24963", "Any update on this?  Still wondering if TF Win10 C++ and GPU support will happen.", "So, any updates @gunan because I think, that there is a pretty large audience that wants to use c++ with Win10, including me.", "looks like the referenced PR is merged, and it is definitely in releases.\r\n@meteorcloudy did we have any other issues remaining for this?", "@TsarF Does https://github.com/tensorflow/tensorflow/pull/24963 solve your problem?", "Hi @xiaoguai0992 !\r\nWe are checking to see whether you still need help in this issue . Have you checked this[ thread](https://iq.opengenus.org/build-tensorflow-cpp-library/) on building Tensorflow with C++ using Bazel yet? Thanks!", "@xiaoguai0992 Could you please have a look on the [Build from source ](https://www.tensorflow.org/install/source_windows)and let us know if it helps?Thanks!", "@sushreebarsa do those instructions allow for c++ support? It looks like it may only support building a pip package.", "Thank all of your comments! I have already turned to python. If someday I need to play with C++ and Windows again, I'd like to try your suggestions.", "@xiaoguai0992 Thank you for the update! Could you please move this issue to closed status if it resolved for you?\r\nThanks!", "The original question specifically mentioned c++. @xiaoguai0992 moving to python doesn't really close this issue. ", "there are two legacy repo to use tensorflow with C++\r\n\r\nhttps://github.com/cjweeks/tensorflow-cmake\r\nhttps://github.com/ksachdeva/tensorflow-cc-examples\r\n\r\nSeems they are using bazel build, not sure if they can still be used in 2022 release", "I am not sure if I should close the issue, because it is still unclear that this issue itself is solved properly.", "Can you please check this SO [link](https://stackoverflow.com/questions/33620794/how-to-build-and-use-google-tensorflow-c-api) with the similar issue.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/8773\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/8773\">No</a>\n"]}, {"number": 8772, "title": "Symbol not found: _SSLCreateContext in Python REPL", "body": "I have installed tensorflow 1.0.1 (cpu version) on Mac as following .\r\n \r\n`sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.1-py2-none-any.whl`\r\n\r\nAt the end of installation, I got the following message:\r\n`Successfully installed tensorflow-1.0.1`\r\n\r\nBut unable to import the libraries in python.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\nMac version:\r\n```\r\nSoftware:\r\n    System Software Overview:\r\n      System Version: Mac OS X 10.7.5 (11G63)\r\n      Kernel Version: Darwin 11.4.2\r\n      64-bit Kernel and Extensions: Yes\r\n\r\n```\r\nPython version: \r\n```\r\n$ python\r\nPython 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) \r\n[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> \r\n\r\n```\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n$ ls -l /path/to/cuda/lib/libcud*\r\nls: /path/to/cuda/lib/libcud*: No such file or directory\r\n```\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```\r\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Symbol not found: _SSLCreateContext\r\n  Referenced from: /Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/mobility1/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n", "comments": ["You are using an old version of mac os (lion). We do not support older than El Capitan (10.11).\r\n-A\r\n"]}, {"number": 8771, "title": "Tensorflow 1.0 RNN weights already exists", "body": "i  got a error  \r\n```\r\n`  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 639, in _get_single_variable\r\n    name, \"\".join(traceback.format_list(tb))))\r\nValueError: Variable lstm_def/rnn/basic_lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n```\r\n\r\n```python\r\nthis is my code:\r\n `def lstm(batch):      \r\n    w_in=weights['in']\r\n    b_in=biases['in']\r\n    input=tf.reshape(X,[-1,input_size])  \r\n    input_rnn=tf.matmul(input,w_in)+b_in\r\n    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit]) \r\n    \r\n    #cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)  \r\n    with tf.variable_scope('cell_def'):\r\n    \tcell=tf.contrib.rnn.BasicLSTMCell(rnn_unit)\r\n    \tinit_state=cell.zero_state(batch,dtype=tf.float32)\r\n    with tf.variable_scope('lstm_def'):\r\n    \toutput_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32) \r\n\r\n    #output_rnn,final_states=tf.nn.rnn_cell.LSTMCell(cell, input_rnn,initial_state=init_state, dtype=tf.float32)\r\n    #with tf.variable_scope('lstm'):\r\n    #output_rnn,final_states=tf.contrib.rnn.static_rnn(cell, input_rnn,initial_state=init_state,dtype=tf.float32)\r\n    output=tf.reshape(output_rnn,[-1,rnn_unit]) \r\n    w_out=weights['out']\r\n    b_out=biases['out']\r\n    pred=tf.matmul(output,w_out)+b_out\r\n    return pred,final_states\r\n```", "comments": ["Since this is your custom model,  it's unclear that it is a bug or just a usage error. Could you please ask on stackoverflow to ask for usage advice. Be sure to post the full traceback and your full code (including how lstm() is used). Thanks!", "Thx\uff0ci found the problem ,it is the version of tf.\r\ninstead:cell=tf.contrib.rnn.BasicLSTMCell(rnn_unit)\r\nwith: cell=tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(rnn_unit)\r\n\r\nthen it works"]}, {"number": 8770, "title": "tf.contrib.seq2seq.sequence_loss with tf.nn.sampled_softmax_loss", "body": "There maybe an incompatible matmul when use `tf.contrib.seq2seq.sequence_loss` together with `tf.nn.sampled_softmax_loss`. `sampled_softmax_loss` need a rank 2 tensor as its label, however in `sequence_loss`, the label has been reshape to [-1], which will raise an error:\r\nValueError: Shape must be rank 2 but is rank 1 for 'sequence_loss/sampled_softmax_loss/MatMul_1' (op: 'MatMul') with input shapes: [50], [?,20].\r\n\r\n```\r\n    batch_size = 5\r\n    max_step = 10\r\n    dim = 20\r\n    vocab_size = 100\r\n\r\n    logits = tf.constant(np.random.randn(batch_size, max_step, dim),\r\n                         tf.float32)\r\n    targets = tf.constant(np.random.randint(vocab_size, size=(batch_size, max_step)),\r\n                         tf.int32)\r\n    target_weights = tf.constant(np.ones((batch_size, max_step)), tf.float32)\r\n    proj_w = tf.constant(np.random.randn(vocab_size, dim), tf.float32)\r\n    proj_b = tf.constant(np.zeros(vocab_size), tf.float32)\r\n\r\n    def _sampled_loss(labels, logits):\r\n        labels = tf.cast(labels, tf.int64)\r\n        logits = tf.cast(logits, tf.float32)\r\n        return tf.cast(\r\n                        tf.nn.sampled_softmax_loss(\r\n                            proj_w,\r\n                            proj_b,\r\n                            labels,\r\n                            logits,\r\n                            num_sampled=20,\r\n                            num_classes=vocab_size),\r\n                        tf.float32)\r\n\r\n    softmax_loss_f = _sampled_loss\r\n\r\n    loss = tf.contrib.seq2seq.sequence_loss(\r\n                    logits,\r\n                    targets,\r\n                    target_weights,\r\n                    softmax_loss_function=softmax_loss_f)\r\n\r\n    sess = tf.Session()\r\n    print sess.run(loss)\r\n```\r\nThis error can be fixed if the I change line 81 in contrib/seq2seq/python/ops/loss.py:\r\n`crossent = softmax_loss_function(labels=array_ops.reshape(targets, [-1, 1]), logits=logits_flat)`\r\n\r\ntensorflow version: 1.01", "comments": ["Unfortunately, we cannot support contrib issues since they are by definition experimental. If you fairly confident, please submit a pull-request and we will look at it.", "@NickPoon I think you may invoke the reshape() from within `def _sampled_loss(labels, logits)`?\r\n\r\nSee the code below. I think that will resolve the issue without modifying the existing code, and not causing other incompatibilities with different loss functions.\r\n\r\n```\r\ndef _sampled_loss(labels, logits):\r\n    labels = tf.cast(labels, tf.int64)\r\n    labels = tf.reshape(labels, [-1, 1])\r\n    logits = tf.cast(logits, tf.float32)\r\n    return tf.cast(\r\n                    tf.nn.sampled_softmax_loss(\r\n                        proj_w,\r\n                        proj_b,\r\n                        labels,\r\n                        logits,\r\n                        num_sampled=20,\r\n                        num_classes=vocab_size),\r\n                    tf.float32)\r\n\r\nsoftmax_loss_f = _sampled_loss\r\n\r\n```", "@NickPoon  @yongtang this example is wrong. The `proj_w` matrix is supposed to be the weights matrix used in the seq2aeq model while calculating logits from the outputs of the cells. However, in this example, you have made a new weights matrix. Moreover, the `inputs` expected by sampled-softmax are the inputs before the weight multiplication, not the logits that are made available via the loss function wrapper. This code may not throw any errors and may seem to work, but it is wrong", "Closing as this is resolved"]}, {"number": 8769, "title": "AttributeError: module 'tensorflow' has no attribute 'confusion_matrix'", "body": "I am using GPU version of tensorflow 0.12.1 and python3.5 for building a CNN network. \r\nWhen I am trying to compute confusion matrix it gives me the error. \r\n\r\n`confusion = tf.confusion_matrix(labels = y_, prediction = y, num_classes = model.No_Classes)`\r\n\r\ny_ is the input labels that I am giving to the network and y is the output of my CNN network. model.No_Classes = 4 an integer value.\r\n\r\nis there package missing in this version ?\r\n", "comments": ["I have upgraded to tensorflow 1.0.1 and this works !"]}, {"number": 8768, "title": "Branch 151394592", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 8767, "title": "Branch 151391741", "body": "tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py had some minor conflicts that needed manual fix.", "comments": ["You need to include 151393424 to fix the failure.", "Thanks @gunan. Too bad it didnt make it. Close and push again. "]}, {"number": 8766, "title": "Merge 1.1.0 rc back", "body": "Merging the r1.1 branch back after releasing RC.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 8765, "title": "Update README.md for graph transform", "body": "Do not recommend to use`remove_nodes(op=Identity, op=CheckNumerics)` in the README. It breaks the control flow based operations.", "comments": ["Can one of the admins verify this patch?", "Thanks for sending the PR @raingo. @petewarden mind taking a look? Thanks!", "Thanks for the PR! Can you give some more background on the specific issues you're hitting? Control flow is generally only needed for Variable assignment, which generally doesn't happen during inference, so it would be good to know more about what's going wrong in your case.", "@petewarden For example, for object detection, there is a `tf.image.non_max_suppression` step per each object class. I used `tf.map_fn`to iterate through each class. This iteration is necessary in the deploy time.", "@petewarden any modifications we would need here?", "Sorry for the slow response on this. Can we address this by leaving the code example but adding documentation along the lines of \"this may cause problems if you have a model with control-flow based operations\"? I'd hate people to miss out on the optimizations in other cases.\r\n\r\nI think the real solution would be to warn or otherwise avoid removing ops that are significant for control flows, but I'm not sure how to do that yet.", "ping for @raingo !", "@petewarden PTAL. Just added a note about the control flow ops."]}, {"number": 8764, "title": "configure hangs on download and aborts", "body": "Hi,\r\nI am trying to compile tensorflow from source.\r\nwhen I run ./configure, things start to be downloaded, but generaly, it will hang for ~10s at a large download (>~30MB), and aborts with such error:\r\n`ERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:449:1: no such package '@llvm//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz, https://github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/llvm/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz: Tried to reconnect at offset 31,979,427 but server didn't support it and referenced by '//tensorflow/compiler/xla/service/cpu:elemental_ir_emitter'.\r\nERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:176:1: no such package '@llvm//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz, https://github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/llvm/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz: Tried to reconnect at offset 31,979,427 but server didn't support it and referenced by '//tensorflow/compiler/xla/service/cpu:ir_emitter'.\r\nERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:32:1: no such package '@llvm//': java.io.IOException: Error downloading [http://bazel-mirror.storage.googleapis.com/github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz, https://github.com/llvm-mirror/llvm/archive/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/llvm/94403df1ddb4cf9af6ac7dcbbd629fcc22e19db9.tar.gz: Tried to reconnect at offset 31,979,427 but server didn't support it and referenced by '//tensorflow/compiler/xla/service/cpu:cpu_compiler'.\r\nERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/python/BUILD:2571:1: every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:swig', but this target could not be found because of: no such package '@swig//': Error downloading [http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://pilotfiber.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/swig/swig-3.0.8.tar.gz: Tried to reconnect at offset 7,935,828 but server didn't support it.\r\nERROR: /home/user/f/dependRepos/serving/tensorflow/tensorflow/python/BUILD:2571:1: every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:templates', but this target could not be found because of: no such package '@swig//': Error downloading [http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz, http://pilotfiber.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz] to /home/user/.cache/bazel/_bazel_user/8f93dc993f1ee3fe648b32055340eadb/external/swig/swig-3.0.8.tar.gz: Tried to reconnect at offset 7,935,828 but server didn't support it.\r\nERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\n`\r\n\r\nI once managed to download manually the incriminated files from a browser, and then decompress them manually in the temporary folder where bazel can find it, but it is rather cumbersome to do so.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\nInstalled version of CUDA and cuDNN: cuda8.0.44, cudnn5.1.5 (but this happens even when I ./configure without gpu support)\r\n\r\nThe commit hash (12a98726e769e988f6368a029ec2f5b0ac3ccbd4), but it has been doing this for as long as I have tried to compile from source (v < 0.10).\r\n\r\nbazel version:\r\n`Build label: 0.4.5`\r\n`Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar`\r\n`Build time: Thu Mar 16 12:19:38 2017 (1489666778)`\r\n`Build timestamp: 1489666778`\r\n`Build timestamp as int: 1489666778`\r\n\r\n", "comments": ["I have sent #8761 to remove downloads from `configure` script. However, this means downloads will happen automatically during bazel build. Unfortunately, we have to have the dependencies downloaded to build the code, the PR just changes when they are downloaded.\r\n\r\nLooking at your logs, I see that there is:\r\n`Tried to reconnect at offset 7,935,828 but server didn't support it.`\r\nSo maybe we should remove the sourceforge mirror for swig, and get a more agreeable mirror.\r\n\r\n", "So if I understand correctly, I can ignore this during ./configure and bazel will download the needed packages when building targets...\r\nThx", "You are correct. With bazel 0.4.5, we do not need to run bazel fetch separately anymore.\r\n#8761 is just merged, and it should also help clarify such confusions going forward."]}, {"number": 8763, "title": "Add @org_tensorflow to targets in tensorflow.bzl for submodule usage", "body": "tf_kernel_library and friends do not currently work if called from a\r\nrepo that contains tensorflow as submodule.\r\n\r\nFixes #8439.", "comments": ["This is a version of https://github.com/tensorflow/tensorflow/pull/8606 with `str(Label(` replaced with `@org_tensorflow`.", "@gunan So clearly `@org_tensorflow//tensorflow` is not equivalent to `//tensorflow` if you build tensorflow not as a submodule.  This would seem to imply to me that the `str(Label(` way is better, given that that approach works and this one doesn't.", "They both seem equivalent if tensorflow is used as a submodule.", "I understand.\r\nTo unblock your usecase, let's go with your other PR.\r\nThank you very much for waiting for bazel upgrade and testing this out.\r\n\r\n@damienmg FYI. "]}, {"number": 8762, "title": "`tf.device()` issue", "body": "I posted my issue about In-graph replication for Distributed Tensorflow on StackOverflow: http://stackoverflow.com/questions/43036201/in-graph-replication-for-distributed-tensorflow. I wonder if there is a bug in terms of `tf.device()`, as I expect each process has its own log instead of print out message that belongs to other block of `tf.device()` like my example there. Thanks!", "comments": ["I don't believe this is a bug, but rather a misunderstanding of how distributed TensorFlow works, someone in SO posted links to some docs", "@yaroslavvb Thanks for your response! I read the link there, but it still seems ambiguous about my issue, probably I missed something there. If possible, could you help to take a look? Also, I read distributed tensorflow multiple times, is there any other docs to recommend?", "Bulk of the documentation exists as comments in source code. I've posted some explanations in these SO answers:\r\n\r\nhttp://stackoverflow.com/questions/40819321/understanding-the-conceptual-basics-of-distributed-tensorflow/40873770#40873770\r\n\r\nhttp://stackoverflow.com/questions/41067398/task-assignment-in-tensorflow-distributed-process/41069059#41069059"]}, {"number": 8761, "title": "Do not run bazel fetch during configure anymore.", "body": "Fixes #8713\r\n", "comments": []}, {"number": 8760, "title": "[bug] tf.shape", "body": "Please expose the `optimize` flag of the `array_ops.shape_internal` for `tf.shape`.\r\n\r\nThe current `tf.shape` use `optimize=True` as default, but sometimes we do not want `tf.shape` return the constant. The resulting graph can not be imported by the android runtime.", "comments": ["@petewarden, do you have any insight into this?", "Sorry for the delay on this one. I'm not sure I understand the issue, is the error that the shape op is not included in Android builds by default? If so, this should be fixable by adding it to the list of kernels here in `:android_extended_ops_group2`:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L4222", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 8759, "title": "Simple add tensor example fails", "body": "### Environment info\r\nOperating System: Ubuntu 17.04\r\n\r\nInstalled version of CUDA and cuDNN:\r\n\r\n```bash\r\nstefano@stefano-PC:~$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root root   556000 Jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   415432 Jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 84163560 M\u00e4r 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 84163560 M\u00e4r 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 84163560 M\u00e4r 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 M\u00e4r 23 18:23 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n>>> import tensorflow; print(tensorflow.__version__)\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\n\r\na = tf.placeholder(tf.float32)\r\nb = tf.placeholder(tf.float32)\r\nadder_node = a + b  # + provides a shortcut for tf.add(a, b)\r\n\r\nprint(sess.run(adder_node, {a: 3, b:4.5}))\r\nprint(sess.run(adder_node, {a: [1,3], b: [2, 4]}))\r\n\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nstefano@stefano-PC:~/Dokumente/Programming/Python/TensorflowCoreTutorial$ /home/stefano/tensorflow/bin/python3 /home/stefano/Dokumente/Programming/Python/TensorflowCoreTutorial/src/main.py\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7085\r\npciBusID 0000:22:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 52.19MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:22:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 52.19M (54722560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 46.97M (49250304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n0.0\r\n[ 0.  0.]\r\n```\r\n\r\nExpected result should be\r\n\r\n```\r\n7.5\r\n[ 3.  7.]\r\n```\r\nIt seems, tensorflow reads uninitialized memory, because sometimes it prints some random numbers.", "comments": ["Problem was, that I executed this in Visual Studio Code. I thought this would work, because I referenced the Virtual Env Python Version."]}, {"number": 8758, "title": "Fix to code in Getting Started Guide", "body": "Fix no attribute prediction_key error. This compliments [PR:8735](https://github.com/tensorflow/tensorflow/pull/8735).\r\n\r\n```\r\ntf.contrib.learn.prediction_key.PredictionKey.CLASSES),\r\nAttributeError: module 'tensorflow.contrib.learn' has no attribute 'prediction_key'\r\n```", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I was told the CLA is now in place for our company, KnuEdge. I am a member of tensorflow_developers@knuedge.com which is our authorized contributors group.", "I think we need to verify that on our end, too.\r\n@willnorris, How does the company CLAs work?\r\nWhich email address should the contributor use?", "I could not find a CLA for tensorflow_developers@knuedge.com from the [link](https://github.com/tensorflow/tensorflow/pull/8221#issuecomment-288474498) @willnorris mentioned. @dominic-rossi do you mind double-checking as well? Thanks.\r\n", "I got the following content from the email regarding the KnuEdge CLA:\r\n\r\n> Google CLA Admin voided Google Contributor License Agreement for KnuEdge. Google Contributor License Agreement for KnuEdge has been voided for the following reason:\r\nIt appears that KnuEdge executed this agreement on September 6, 2016.\r\n\r\nFrom this, it is a little difficult to tell what the current state of our company CLA is. Our current assumption is that there is a working CLA in place.", "@willnorris Could you instruct on the next steps?\r\n\r\nWe cannot verify the CLA of the contributors, @dominic-rossi reported the above from their end. What should we do?", "Just to update: we might have to wait until Apr 21.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 8757, "title": "R1.1", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 8756, "title": "Error installing from source", "body": "I'm trying to update tensorflow from the latest source. I'm running Ubuntu 14.04, with cuda 8.0 and cudnn 5.1.5. I was previously running r0.11 with no problem.\r\n\r\nI pulled the new source. After running ./config and enabling cuda, I try to build with bazel:\r\n\r\n> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nand I'm getting the following error:\r\n\r\n> ERROR: /home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n> \tFile \"/home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/BUILD\", line 4\r\n> \t\terror_gpu_disabled()\r\n> \tFile \"/home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n> \t\tfail(\"ERROR: Building with --config=c...\")\r\n> ERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\n> ERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/jason/.cache/bazel/_bazel_jason/ba8a668adda8b53cb2de87e5e6b097b9/external/local_config_cuda/crosstool/BUILD.\r\n\r\nNote where it says \"Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\" There is no such prompt when I run ./configure. I type 'Y' when it prompts if I want to build with cuda support and I give the correct cuda and cudnn versions and paths.", "comments": ["Please will in the full template. There are missing pieces of information, such as your bazel version, that prevents us from being able to help you.", "I updated to the latest version of bazel before trying to update tensorflow", "Our CI is healthy, and we can build OK with the current version of the code:\r\nhttp://ci.tensorflow.org/job/tensorflow-master-linux-gpu/\r\n\r\nSo unless you provide more detailed information to us, we really cant offer much help.", "I gave the error and versions for all relevant packages (that I know of). Again, when I run ./configure, I'm not seeing any prompts to build with GPU support as the error suggests. I'm saying yes to building with cuda.\r\n\r\nI don't know what else is relevant.", "Sorry if I was not clear.\r\nTo make sure we have all we need when debugging user side issues, we tried to refine the issue template.\r\nBy filling that out, you will help us understand your setup, and the problem you are seeing.\r\n\r\nI think you are running into this same problem here:\r\nhttps://github.com/tensorflow/tensorflow/issues/8731\r\n\r\nThere seems to be already a response there which may help get around the issue you are seeing.\r\n", "I had also tried bazel clean. It made no difference.\r\n\r\nI uninstalled and just reinstalled from pip.", "Just note, I had similar problem, by setting following variables during configuration phase:\r\n```\r\nexport CUDNN_INSTALL_PATH=\"/usr/lib64\"\r\n\texport TF_NEED_CUDA=\"1\"\r\n\texport TF_CUDA_VERSION=\"9.0\"\r\n\texport TF_CUDNN_VERSION=\"7\"\r\n```\r\n\r\nAnd all is good."]}, {"number": 8755, "title": "tf.contrib.seq2seq.sequence_loss documentation incorrect", "body": "The function returns a tensor with rank = 0, 1, 2 depending on its arguments. This is inconsistent with its docstring's description of the return:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L58\r\n\r\nI will submit a PR soon to fix the doc.\r\n\r\n\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nirrelevant\r\n\r\n### Environment info\r\nOperating System: \r\n\r\nirrelevant\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Thanks for the help. Please link the PR when you have submitted it.", "This looks fixed in master, and it will roll out over time.  Thank you!  I'm closing this; please reopen if I've missed something."]}]