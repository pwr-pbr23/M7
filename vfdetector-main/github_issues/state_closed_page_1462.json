[{"number": 9090, "title": "Saver/Summary: The process cannot access the file because it is being used by another process.", "body": "Hey guys!\r\n\r\nI have been banging my head for a couple of days with the following:\r\n\r\nI am using a MonitoredTrainingSession with a single local server (but bug is identical even if using Supervisor and/or distributed architecture).\r\n\r\n`#...\r\nsess  = tf.train.MonitoredTrainingSession(...)\\\r\nwhile True:\\\r\n   sess.run(train_op)`\r\n\r\nAssuming sequential runs of the python client without changing a single line of code,  the problem is that I sometimes get the error: \r\n`Tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to rename: .../graph.pbtxt.tmp23e44d8fdce844e6822a56dc886588e6 to: .../graph.pbtxt : The process cannot access the file because it is being used by another process.`\r\n\r\n... and the interesting thing is that sometimes this does not occur and training begins, but it then happens just as the first before_run call of the checkpoint hook tries to save the initial state (as seen in the MonitoredSession source code): \r\n\r\n`Failed to rename: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001.tempstate17529973146728747180 to: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001 : The process cannot access the file because it is being used by another process.`\r\n\r\nSOLVED: don't ever store log data and checkpoints in dropbox (:\r\n", "comments": ["I was stuck from 2 days because of this stupid error before I found this post. Thank you very much for the solution. ", "Thank you!!!!!!!", "What about google drive? Does it work if saving to google drive? ", "I met the same issue.\r\n```\r\nFailedPreconditionError (see above for traceback): Failed to rename: ./logs/checkpoints/model_epoch-1.data-00000-of-00001.tempstate9288023132477748030 to: ./logs/checkpoints/model_epoch-1.data-00000-of-00001 : The process cannot access the file because it is being used by another process.\r\n; Broken pipe\r\n\t [[Node: save/SaveV2 = SaveV2[dtypes= ... ...\r\n```\r\nI did not sync this project with any online drive.\r\n\r\n**I my case, It's `git` reading that file content.** So that file cannot be renamed. I added `logs/` to `.gitignore`, this error was fixed.", "O.M.G. lol. Went to Dropbox Settings, hit Pause Syncing while running and it started working. Thanks @krisjobs ", "I get this error using plain file system so what gives?", "1. Shutdown every backup service, **MEGA**, **DropBox**, **GoogleDrive**. \r\n2. Add to `.gitignore`", "This is apparently a problem with OneDrive as well.", "@grldsndrs And Google Drive :))", "NICE!!!!! It's work", "what a relief!!! had the same problem. selectively not syncing the folder you are saving in works!", "I had the error without saving weights to a cloud service. I use PyCharm what is indexing new files, so maybe the error occured because of that in my case.", "JFC. Thank you for posting this.", "> I was stuck from 2 days because of this stupid error before I found this post. Thank you very much for the solution.\r\n\r\nwhats the solution\r\n", "Wow this saved me so much time. After hours of putting together this model just running into this error knowing it should be working was so frustrating. pausing my dropbox sync did the job. now moving all my docs away from dropbox..."]}, {"number": 9089, "title": "cpp protobuf instructions out-of-date for MacOS", "body": "Instructions to upgrade to cpp protobuf implementation on Mac from https://www.tensorflow.org/install/install_mac#protobuf_pip_package_31 don't work work, makes TF fails with following stacktrace\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"kronecker_benchmark.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 54, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/google/protobuf/descriptor.py\", line 46, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: dlopen(/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-darwin.so, 2): Library not loaded: /usr/local/lib/libprotobuf.10.dylib\r\n  Referenced from: /Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-darwin.so\r\n  Reason: image not found\r\n\r\n```\r\n\r\nWork-around is to use older link:\r\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl\r\n\r\nCheck that it works\r\n`python -c \"from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)\"\r\n`\r\n\r\nMacOS: 10.12.4 (16E195), TensorFlow, latest nightly from today installed as:\r\n`pip install --upgrade https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-py3-none-any.whl\r\n`\r\n", "comments": ["@jhseu : Any suggestions for the appropriate update here?", "Sent a fix for the links in the docs (internally, should be reflected soon).\r\n\r\nAlso filing a request with the protobuf team to upload optimized mac builds to pypi."]}, {"number": 9088, "title": "'tensorboard' command not found (raspberry pi 3)", "body": "Bug in installation (tensorflow python code runs fine)\r\n\r\nhere are the step I used to install TensorFlow :\r\n\r\n```\r\nsudo apt-get install python-pip python-dev\r\nwget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.0.1/tensorflow-1.0.1-cp27-none-linux_armv7l.whl\r\npip install --user tensorflow-1.0.1-cp27-none-linux_armv7l.whl\r\npip uninstall mock\r\npip install --user mock\r\nsudo pip install --user git+https://github.com/tflearn/tflearn.git\r\n```\r\nthe machine is a raspberrry pi v3 (raspbian v 8.0)\r\ntensorflow v 1.0.1\r\n\r\nproblem :\r\n\r\nI try to run tensorboard, I get a command not found\r\n\r\nI tried locate, find, whereis, .... nothing, seems like it did not get installed with tensorflow\r\n\r\nlogs :\r\n\r\ndont know if there are any install logs, if yes let me know where to find them", "comments": ["Those packages (https://github.com/samjabrahams/tensorflow-on-raspberry-pi and https://github.com/tflearn/tflearn.git) are not produced by the maintainers of the TensorFlow repository and are not part of the official release.\r\n\r\nSince this isn't something owned or maintained by the TensorFlow maintainers, I'm going to close this out. I'd recommend building from source (perhaps using the instructions from @samjabrahams )", "ok, thx"]}, {"number": 9087, "title": "ValueError: Restore called with invalid save path", "body": "I had saved the model after each epoch using:\r\n\r\nsavernew = tf.train.Saver(max_to_keep=10000)\r\nsp=\"/ models/epoch\"+str(i+1)+\"/\"\r\nsavernew.save(session, sp,global_step=sv.global_step)\r\n\r\nNow I want to restore the epoch 88:\r\n\r\nnew_saver = tf.train.Saver()\r\nnew_saver.restore(sess, \"/models/epoch88/\")\r\n\r\nI am getting the  error:\r\n\r\nValueError: Restore called with invalid save path: '/ models/epoch88/'. File path is: ' /models/epoch88/'\r\n\r\nMy tensorflow version is 0.11\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  **Make sure you also include the exact command if possible to produce  the output included in your test case.**   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  **Make sure you also include the exact command if possible to produce  the output included in your test case.**   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "OS - Ubuntu 16.04 LTS\r\nArchitechture - x86_64 \r\ncompiled using pip\r\nTF version - 0.11\r\nI had used the above lines in my code which was saved as test.py. So I used python test.py as command.\r\n\r\nIf there is any more info need pls ask, I will provide.\r\n\r\n", "Please provide a full reproducible example. For example, I tried the following on both TensorFlow 0.11.0 and 1.0.1 and was unable to see a problem. It would greatly help if you can provide the failure.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Create a model and save it\r\nx = tf.Variable(1)\r\nwith tf.Session() as sess:\r\n  sess.run(tf.initialize_all_variables()) # tf.global_variables_initializer() in 1.0\r\n  saver = tf.train.Saver()\r\n  saver.save(sess, '/tmp/model')\r\n\r\n# Restore in another session\r\nwith tf.Session() as sess:\r\n  saver = tf.train.Saver()\r\n  saver.restore(sess, '/tmp/model')\r\n```"]}, {"number": 9086, "title": "Correction to word2vec exercise.", "body": "In its existing form, `2 * skip_window` training examples for the skip gram model are incorrect because they draw context words from both the beginning and end of `data`.\r\n\r\nI have an individual CLA with Google already.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9085, "title": "Update README.md", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9084, "title": "delete null and relu to ReLU", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Failure in //bazel_pip/tensorflow/python:layers_normalization_test is unrelated. Merging PR."]}, {"number": 9083, "title": "Feature request: In tensorboard, move \"Image\" and \"Audio\" columns", "body": "A *very* minor feature request: it would make sense to move the Image and Audio columns to the right, and let the first columns be: Scalar, Histogram and Distribution. Those two columns are arguably not as frequently used, and switching between looking at scalars and histograms get tiring sometimes.\r\n\r\n(but if there's plans to allow for visualising different types of variables together, that might be even better)", "comments": ["CC @dandelionmane ", "Happy to take a pull request that does this, it's literally just moving some HTML elements around in tensorboard/components/tf-tensorboard/tf-tensorboard.html. For it to immediate be tested in opensource, there should be a corresponding change to dist/tf-tensorboard.html", "I'm closing this due to inactivity, but please feel free to submit a PR to our new repository at https://github.com/tensorflow/tensorboard/pulls!"]}, {"number": 9082, "title": "OSX compile from sources: \"dyld: Library not loaded: @rpath/libcudart.8.0.dylib\"", "body": "Hi,\r\nI'm trying to complile TF from sources I recieve the following error:\r\n`dyld: Library not loaded: @rpath/libcudart.8.0.dylib`\r\nI know for a fact that cuda is correctly installed as I'm currently using TF on my machine (pip).\r\n\r\nI've been following those steps:\r\n1. clone master\r\n2. select X-code 7.2\r\n3. ./configure, all defaults except:\r\n     - compile options: -march=native -mavx -mavx2 -mfma \r\n     - support cuda [Y]\r\n4. bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nwhat I'm I doing wrong ? \r\n\r\nThanks\r\n\r\n```\r\nCesare:tensorflow-master cesare$ bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWARNING: /Users/cesare/Projects/ml/tensorflow-master/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /Users/cesare/Projects/ml/tensorflow-master/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /Users/cesare/Projects/ml/tensorflow-master/tensorflow/contrib/factorization/BUILD:106:1: Executing genrule //tensorflow/contrib/factorization:gen_factorization_ops_pygenrule failed: bash failed: error executing command \r\n  (cd /private/var/tmp/_bazel_cesare/bcc912c27d26c81cd2b264ad18985527/execroot/tensorflow-master && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PATH=/usr/local/cuda/bin:/opt/local/bin:/opt/local/sbin:/usr/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \\\r\n    TF_CUDA_VERSION='' \\\r\n    TF_CUDNN_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TMPDIR=/var/folders/n5/2dz3mvjj1cs6cn6j93_vj_mc0000gn/T/ \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/contrib/factorization/gen_gen_factorization_ops_py_wrappers_cc 0 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/factorization/python/ops/gen_factorization_ops.py'): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 6.\r\ndyld: Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /private/var/tmp/_bazel_cesare/bcc912c27d26c81cd2b264ad18985527/execroot/tensorflow-master/bazel-out/host/bin/tensorflow/contrib/factorization/gen_gen_factorization_ops_py_wrappers_cc\r\n  Reason: image not found\r\n/bin/bash: line 1: 63383 Abort trap: 6           bazel-out/host/bin/tensorflow/contrib/factorization/gen_gen_factorization_ops_py_wrappers_cc 0 > bazel-out/local_darwin-py3-opt/genfiles/tensorflow/contrib/factorization/python/ops/gen_factorization_ops.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.018s, Critical Path: 0.04s\r\n```\r\n\r\n```\r\nCesare:tensorflow-master cesare$ ls -la /usr/local/cuda/lib/libcudart.8.0.dylib \r\n\r\nlrwxr-xr-x@ 1 cesare  staff  50 Sep 27  2016 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\n```\r\n\r\n```\r\nCesare:tensorflow-master cesare$ echo $DYLD_LIBRARY_PATH \r\n\r\n/usr/local/cuda/extras/CUPTI/lib:/usr/local/cuda/lib:/usr/local/cuda\r\n```", "comments": ["Closing this as a duplicate of #6729 \r\nYou might want to try some approaches recommended there.", "the ticket you posted it about running it, not compiling", "Perhaps I'm misreading, but https://github.com/tensorflow/tensorflow/issues/6729#issue-199435860 and https://github.com/tensorflow/tensorflow/issues/6729#issuecomment-272808870 seem to be about building?", "Nevermind, there is a bit of everything, close it ^_^\n\nOn 11 April 2017 at 03:00, Asim Shankar <notifications@github.com> wrote:\n\n> Perhaps I'm misreading, but #6729 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/6729#issue-199435860>\n> and #6729 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/6729#issuecomment-272808870>\n> seem to be about building?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9082#issuecomment-293119793>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAbVbdUzb6KpY5Bv8lXnl5y-1ENiASGNks5rutDIgaJpZM4M3_Rs>\n> .\n>\n\n\n\n-- \nCesare Montresor\nMob (ES) :: +34 61 22 29 471\nMob (IT) :: +39 346 00 93 133\nLinkedIn :: http://in.linkedin.com/in/cmontresor/\nSkype :: skype://cmontresor\nFB :: https://www.facebook.com/cesare.montresor\n"]}, {"number": 9081, "title": "Add output_shape parameter to transposed2d_convolution layer", "body": "I just added this parameter that was missing in the `conv2d_transpose layer`.\r\nIf `output_shape is None`, then the layer acts as usual returning the smallest shape possible", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@sguada is that something you could take a look at?", "What is the purpose of this extra argument? In theory the output_shape is computed based on the parameters to the layer.", "The main reason is the following: https://github.com/tensorflow/tensorflow/issues/2118#issuecomment-214903814\r\nAlso the next comment, `if stride > 1` the output shape is ambigous.\r\n", "Jenkins, test this please.", "Same coordinator timeout.\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "I will add some test asap.", "@marcociccone any updates?  Thanks!", "one more ping for @marcociccone !", "Sorry for the delay! I'll write the test in the weekend!\r\n\r\nIl giorno 05 mag 2017, alle ore 07:37, Vijay Vasudevan <notifications@github.com<mailto:notifications@github.com>> ha scritto:\r\n\r\n\r\none more ping for @marcociccone<https://github.com/marcociccone> !\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/9081#issuecomment-299382711>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ABECRF2p79BnUOKUfmtRbXJMhg_XO302ks5r2rV4gaJpZM4M3_MR>.\r\n", "Can one of the admins verify this patch?", "@marcociccone any progress?", "I will close this PR for now. Please reopen if you have time to add a test."]}, {"number": 9080, "title": "Support for nvidia-cuda-mps-server ", "body": "I'm experimenting with multiple Tensorflow GPU processes and the NVIDIA Multi-Process Server. \r\nhttps://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf\r\n\r\nI have the following MNIST example as a benchmark (neural.py)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\ndata = input_data.read_data_sets('MNIST_data_%d' % os.getpid(), one_hot=True)\r\n\r\n# construction phase\r\nx = tf.placeholder(tf.float32, shape=[None, 784])\r\ny = tf.placeholder(tf.float32, shape=[None, 10])\r\n\r\nwith tf.name_scope('fc_1'):\r\n  W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\r\n  b1 = tf.Variable(tf.truncated_normal([200], stddev=0.1))\r\n  h = tf.sigmoid(tf.matmul(x, W1) + b1)\r\n\r\nwith tf.name_scope('fc_2'):\r\n  W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\r\n  b2 = tf.Variable(tf.truncated_normal([10], stddev=0.1))\r\n  y_predict = tf.nn.softmax(tf.matmul(h, W2) + b2)\r\n\r\nwith tf.name_scope('eval'):\r\n  with tf.name_scope('loss'):\r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_predict), reduction_indices=[1]))\r\n\r\nlearning_rate = 0.5\r\n\r\nbackprop = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\r\n\r\ncorrect = tf.equal(tf.argmax(y, 1), tf.argmax(y_predict, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\r\n\r\n#execution\r\n\r\nsess = tf.Session()\r\n\r\nsess.run(tf.initialize_all_variables())\r\n\r\ntrain_steps = 2000\r\nbatch_size = 50\r\n\r\nfor i in range(train_steps):\r\n  batch_x, batch_y = data.train.next_batch(batch_size)\r\n  sess.run(backprop, feed_dict={x: batch_x, y: batch_y})\r\n\r\nprint(sess.run(accuracy, feed_dict={x: data.test.images, y: data.test.labels}))\r\n```\r\n\r\nAnd I'm running two processes like this:\r\n`$ time python neural.py &\r\n     time python neural.py\r\n`\r\n\r\nWithout `nvidia-cuda-mps-control` running as a daemon, this is the output:\r\n```\r\n0.9483\r\n0.947\r\n\r\nreal    0m15.602s\r\nuser    0m6.172s\r\nsys     0m5.092s\r\n\r\nreal    0m15.861s\r\nuser    0m6.288s\r\nsys     0m1.964s\r\n```\r\n\r\nWith `nvidia-cuda-mps-control` running as a daemon, I'm getting an internal error:\r\n\r\n```\r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\r\n-bash: line 76: 47018 Aborted                 (core dumped) python neural.py\r\n```\r\n\r\nI can verify from the nvidia-mps logs in /var/log/nvidia-mps that the tensorflow Cuda context successfully started an nvidia-cuda-mps-server and connected to it.\r\n\r\n**/var/log/nvidia-mps/control.log**\r\n> [2017-04-09 10:05:09.539 Control 46322] Start\r\n> [2017-04-09 10:05:21.023 Control 46322] Accepting connection...\r\n> [2017-04-09 10:05:21.024 Control 46322] NEW CLIENT 46325 from user 1000: Server is not ready, push client to pending list\r\n> [2017-04-09 10:05:21.024 Control 46322] Starting new server 46348 for user 1000\r\n\r\nThe MPS server should be compatible with the Cuda API which Tensorflow uses, so I'm uncertain about why I'm getting this error.\r\n\r\n**Tensorflow version: 1.01**\r\n**Ubuntu 16.04**\r\n**Cuda 8.0, CuDNN**\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 8915:00:00.0     Off |                  Off |\r\n| N/A   51C    P8    28W / 149W |     82MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```", "comments": ["I investigated this a bit, it appears calling:\r\n\r\n`tensorflow/core/common_runtime/gpu/gpu_device.cc:    cudaError_t err = cudaStreamAddCallback(*stream_, asyncFree, afData, 0);`\r\n\r\nGives an error 71 \r\n\r\n> cudaErrorNotSupported = 71\r\n> This error indicates the attempted operation is not supported on the current system or device.\r\n> \r\n\r\nRead more at: http://docs.nvidia.com/cuda/cuda-runtime-api/index.html#ixzz4eAO2YpQp \r\n\r\nI think this is probably from MPS since I run the same program on single process Cuda and `cudaStreamAddCallback` passes without errors. \r\n\r\nIt seems that is the only place\r\nthat `cudaStreamAddCallback` is called inside the whole tensorflow project, maybe can we find an MPS friendly implementation for `deallocate`.", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n\r\n(FYI @zheng-xq )", "I'm just beginning to learn about MPS now. One thing I noticed in your above code is that you're running the same job twice concurrently with '&', but not limiting the space that each job can take on the GPU, as referenced here: https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth (maybe try 0.45 for each session if you're running two?). I'll be looking into this sort of use case as I learn more about MPS. :)", "@eth-n hi, I am also struggling with this problem. With `nvidia-cuda-mps-control` running as a daemon, I start a single tensorflow training, it gives me the same error.  T-T\r\nThanks~", "@waynesuzq wow, if it doesn't even work with a single context, it sounds like a problem with MPS like @elefthei said.\r\n\r\nBy the way, I just ran into this GTC presentation with CUDA profiling tools from a few years back (introducing what's new in CUDA 6.0). Maybe some of the described tools could help someone visualize where their conflicts occur? http://www.sie.es/wp-content/uploads/2015/12/cuda-profiling-tools.pdf\r\n\r\nEdit: Found where it is.", "@eth-n Hi, quoting from MPS guide book:\r\n\"Stream callbacks are not supported. Calling any stream callback APIs will return an error.\"\r\nDo you think this is the reason for Tensorflow not working? I tried caffe, it works well. But I just take a guess, coz I didn't really read through the code. Thanks for the reply.", "@waynesuzq is correct, stream callbacks are not supported. I'm working on\nan MPS compatible PR.\nOn Wed, Jul 5, 2017 at 10:40 AM waynesuzq <notifications@github.com> wrote:\n\n> @eth-n <https://github.com/eth-n> Hi, quoting from MPS guide book:\n> \"Stream callbacks are not supported. Calling any stream callback APIs will\n> return an error.\"\n> Do you think this is the reason for Tensorflow not working? I tried caffe,\n> it works well. But I just take a guess, coz I didn't really read through\n> the code. Thanks for the reply.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9080#issuecomment-313026393>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAuzrA_tSfwv32IOwc6bFRPGKwxznThzks5sKz3YgaJpZM4M3_K2>\n> .\n>\n", "Lack of CUDA streams creates some issues, like single stream per TF session\nwith MPS handling most of the optimizations. My best guess is this will be\nfaster for multi-process (i.e. MPI) but slower for single process\ntensorflow use cases.\nOn Wed, Jul 5, 2017 at 10:54 AM Lefteris Ioannidis <rausted@gmail.com>\nwrote:\n\n> @waynesuzq is correct, stream callbacks are not supported. I'm working on\n> an MPS compatible PR.\n> On Wed, Jul 5, 2017 at 10:40 AM waynesuzq <notifications@github.com>\n> wrote:\n>\n>> @eth-n <https://github.com/eth-n> Hi, quoting from MPS guide book:\n>> \"Stream callbacks are not supported. Calling any stream callback APIs\n>> will return an error.\"\n>> Do you think this is the reason for Tensorflow not working? I tried\n>> caffe, it works well. But I just take a guess, coz I didn't really read\n>> through the code. Thanks for the reply.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/9080#issuecomment-313026393>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAuzrA_tSfwv32IOwc6bFRPGKwxznThzks5sKz3YgaJpZM4M3_K2>\n>> .\n>>\n>\n", "Alright, here's my report, and it's not a great one :'(\r\n \r\n**Cuda streams are used everywhere in Tensorflow.** \r\n\r\nFor BLAS (tensorflow/stream_executor/cuda/cuda_blas.cc)\r\nFor DNN (tensorflow/stream_executor/cuda/cuda_dnn.cc)\r\nFor everything else (tensorflow/stream_executor/cuda/cuda_gpu_executor.cc)\r\n\r\nSo yeah, good luck with that. Cuda streams are like operation queues, so they are used throughout as the parallelization semantics for running multiple cuda operations concurrently in Tensorflow. It will require somebody to rewrite _all_ of the Cuda code to be MPS compatible. So yeah, this issue will rewrite a huge rewrite. \r\n\r\nSearching for cudaStream:\r\n\r\n```\r\ntensorflow/contrib/nccl/kernels/nccl_manager.cc:  const cudaStream_t* cu_stream = reinterpret_cast<const cudaStream_t*>(\r\ntensorflow/contrib/nccl/kernels/nccl_manager.cc:      comm_stream->implementation()->CudaStreamMemberHack());\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:class EigenCudaStreamDevice : public ::Eigen::StreamInterface {\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  EigenCudaStreamDevice()\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  ~EigenCudaStreamDevice() override {}\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  void Reinitialize(OpKernelContext* context, const cudaStream_t* cuda_stream,\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  const cudaStream_t& stream() const override { return *stream_; }\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:    cudaError_t err = cudaStreamAddCallback(*stream_, asyncFree, afData, 0);\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  static void CUDART_CB asyncFree(cudaStream_t stream, cudaError_t status,\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  const cudaStream_t* stream_;          // Not owned.\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  TF_DISALLOW_COPY_AND_ASSIGN(EigenCudaStreamDevice);\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  void Reinitialize(OpKernelContext* context, const cudaStream_t* cuda_stream,\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  EigenCudaStreamDevice stream_device_;\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:  const cudaStream_t* cuda_stream = reinterpret_cast<const cudaStream_t*>(\r\ntensorflow/core/common_runtime/gpu/gpu_device.cc:      streams_[stream_id]->compute->implementation()->CudaStreamMemberHack());\r\ntensorflow/core/kernels/cuda_solvers.cc:  explicit CudaSolverHandles(cudaStream_t stream) {\r\ntensorflow/core/kernels/cuda_solvers.cc:    std::unordered_map<cudaStream_t, std::unique_ptr<CudaSolverHandles>>;\r\ntensorflow/core/kernels/cuda_solvers.cc:  const cudaStream_t* cu_stream_ptr = CHECK_NOTNULL(\r\ntensorflow/core/kernels/cuda_solvers.cc:      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\r\ntensorflow/core/kernels/cuda_solvers.cc:                                                ->CudaStreamMemberHack()));\r\ntensorflow/core/kernels/cuda_solvers.h:  cudaStream_t cuda_stream_;\r\ntensorflow/core/kernels/topk_op_gpu.cu.cc:cudaError LaunchTopKKernel(const cudaStream_t& stream, int num_shards,\r\ntensorflow/core/kernels/topk_op_gpu.cu.cc:  const cudaStream_t& cu_stream = GetCudaStream(ctx);\r\ntensorflow/core/kernels/topk_op_gpu.cu.cc:      const cudaStream_t& cu_stream = GetCudaStream(context);\r\ntensorflow/core/kernels/where_op_gpu.cu.cc:    const cudaStream_t& cu_stream = GetCudaStream(ctx);\r\ntensorflow/core/kernels/where_op_gpu.cu.cc:    const cudaStream_t& cu_stream = GetCudaStream(ctx);\r\ntensorflow/core/util/cuda_kernel_helper.h:inline const cudaStream_t& GetCudaStream(OpKernelContext* context) {\r\ntensorflow/core/util/cuda_kernel_helper.h:  const cudaStream_t* ptr = CHECK_NOTNULL(\r\ntensorflow/core/util/cuda_kernel_helper.h:      reinterpret_cast<const cudaStream_t*>(context->op_device_context()\r\ntensorflow/core/util/cuda_kernel_helper.h:                                                ->CudaStreamMemberHack()));\r\ntensorflow/core/util/cuda_kernel_helper_test.cu.cc:  Eigen::CudaStreamDevice stream;\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:  CHECK(AsCUDAStreamValue(stream) != nullptr);\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:      wrap::cublasSetStream(parent_, blas_, AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:    if (!timer->Init() || !timer->Start(AsCUDAStream(stream))) {\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:    if (!timer->Stop(AsCUDAStream(stream))) {\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:    if (!timer->Start(AsCUDAStream(stream))) {\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:    if (!timer->Stop(AsCUDAStream(stream))) {\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:    timer->Start(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:    timer->Stop(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:    timer->Start(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:    timer->Stop(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:                                     AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_event.cc:port::Status CUDAEvent::Record(CUDAStream* stream) {\r\ntensorflow/stream_executor/cuda/cuda_event.h:  port::Status Record(CUDAStream* stream);\r\ntensorflow/stream_executor/cuda/cuda_fft.cc:  auto ret = wrap::cufftSetStream(parent, plan, AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  CUstream custream = AsCUDAStreamValue(stream);\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:      AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:      AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:                                           AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:                                           AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:                                           AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return CUDADriver::AddStreamCallback(context_, AsCUDAStreamValue(stream),\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return AsCUDAEvent(event)->Record(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:                                    AsCUDAStream(stream)->cuda_stream(),\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return AsCUDAStream(stream)->Init();\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  CUDAStream *cuda_stream = AsCUDAStream(stream);\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  CUevent other_completed_event = *AsCUDAStream(other)->completed_event();\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:                                    AsCUDAStreamValue(other))\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return CUDADriver::WaitStreamOnEvent(context_, AsCUDAStreamValue(dependent),\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return AsCUDATimer(timer)->Start(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return AsCUDATimer(timer)->Stop(AsCUDAStream(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return CUDADriver::SynchronizeStream(context_, AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_gpu_executor.cc:  return std::unique_ptr<internal::StreamInterface>(new CUDAStream(this));\r\ntensorflow/stream_executor/cuda/cuda_rng.cc:      wrap::curandSetStream(parent_, rng_, AsCUDAStreamValue(stream));\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:bool CUDAStream::Init() {\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:void CUDAStream::Destroy() {\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:bool CUDAStream::IsIdle() const {\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:CUDAStream *AsCUDAStream(Stream *stream) {\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:  return static_cast<CUDAStream *>(stream->implementation());\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:CUstream AsCUDAStreamValue(Stream *stream) {\r\ntensorflow/stream_executor/cuda/cuda_stream.cc:  return AsCUDAStream(stream)->cuda_stream();\r\ntensorflow/stream_executor/cuda/cuda_stream.h:// Defines the CUDAStream type - the CUDA-specific implementation of the generic\r\ntensorflow/stream_executor/cuda/cuda_stream.h:class CUDAStream : public internal::StreamInterface {\r\ntensorflow/stream_executor/cuda/cuda_stream.h:  explicit CUDAStream(CUDAExecutor *parent)\r\ntensorflow/stream_executor/cuda/cuda_stream.h:  ~CUDAStream() override {}\r\ntensorflow/stream_executor/cuda/cuda_stream.h:  void *CudaStreamHack() override { return cuda_stream_; }\r\ntensorflow/stream_executor/cuda/cuda_stream.h:  void **CudaStreamMemberHack() override {\r\ntensorflow/stream_executor/cuda/cuda_stream.h:  // Precond: this CUDAStream has been allocated (otherwise passing a nullptr\r\ntensorflow/stream_executor/cuda/cuda_stream.h:// Converts a Stream to the underlying CUDAStream implementation.\r\ntensorflow/stream_executor/cuda/cuda_stream.h:CUDAStream *AsCUDAStream(Stream *stream);\r\ntensorflow/stream_executor/cuda/cuda_stream.h:// Extracts a CUstream from a CUDAStream-backed Stream object.\r\ntensorflow/stream_executor/cuda/cuda_stream.h:CUstream AsCUDAStreamValue(Stream *stream);\r\ntensorflow/stream_executor/cuda/cuda_timer.cc:bool CUDATimer::Start(CUDAStream *stream) {\r\ntensorflow/stream_executor/cuda/cuda_timer.cc:bool CUDATimer::Stop(CUDAStream *stream) {\r\ntensorflow/stream_executor/cuda/cuda_timer.h:class CUDAStream;\r\ntensorflow/stream_executor/cuda/cuda_timer.h:  bool Start(CUDAStream *stream);\r\ntensorflow/stream_executor/cuda/cuda_timer.h:  bool Stop(CUDAStream *stream);\r\ntensorflow/stream_executor/host/host_stream.h:  void *CudaStreamHack() override { return nullptr; }\r\ntensorflow/stream_executor/host/host_stream.h:  void **CudaStreamMemberHack() override { return nullptr; }\r\ntensorflow/stream_executor/stream_executor_internal.h:  virtual void *CudaStreamHack() { return nullptr; }\r\ntensorflow/stream_executor/stream_executor_internal.h:  // See the above comment on CudaStreamHack -- this further breaks abstraction\r\ntensorflow/stream_executor/stream_executor_internal.h:  virtual void **CudaStreamMemberHack() { return nullptr; }\r\n```", "@elefthei Thanks man, I have read NVIDIA new GPU Volta white paper, it seems each client of Volta MPS has its own independent address space. \r\nI don't know if this will solve our problem, but worth trying. ", "Just figured out that building tensorflow (1.2.1) **from source** with pretty much all configure options set to default solves this problem. My intuition is that the problem was caused by default allocator in tensorflow from pip package, but when you build it from source, it enables jemalloc by default (I've not reviewed this a lot).\r\n\r\nNow it works flawlessly through NVIDIA MPS, but I couldn't get any performance gain for my inference tasks (mostly RNN-based), only slight speed decrease ~10%.", "That's awesome thanks! Could you try a multiprocess scenario? Something like:\r\n\r\n```\r\nstresstest() {\r\nfor i in {1..100}; do\r\n   python tf_example.py &\r\ndone\r\n}\r\n```\r\nWith MPS enabled\r\n`time stresstest`\r\n\r\nWith MPS disabled\r\n`time stresstest`\r\n\r\nAnd see if MPS is any faster in a multiprocess/Multi-CUDA-context use case?\r\n\r\n\r\n", "@elefthei  I've tested it in multiprocess environment \u2014 3-4 instances of the same process per GPU, each with sufficient amount of GPU memory, all under simultaneous high load (each process handle requests by several GPU inference calls alternating with calculations on CPU), it's actually a LSTM-based dialog model in a nutshell. All the difference was in running it thought MPS or not and I've got ~10% performance decrease on a MPS-enabled configuration.\r\n\r\nBut I didn't benchmark any training tasks (I don't actually think you could benefit from MPS at training time, but who knows) and any other models (like CNN-based, inception v3 i.e). \r\n\r\nSo my advice is to try MPS by your own (building TensorFlow from sources isn't THAT scary as it looks like :) at your own tasks, because I think results may be very different.", "Stream callbacks are not supported on pre-Volta MPS clients. MPS with Tensorflow may need to be tried on volta using cuda9 later. ", "I've tried with the latest TensorFlow 1.6 (require CUDA 9) on 1080ti GPU and it works. ", "Awesome, two people confirmed it worked with Cuda 9 for them on Volta, may as well close.", "Hi, googling ends up on this page, also pointing it should work on new Volta architectures. In my experience it does NOT. My exp setup:\r\n\r\n1.) AWS instance p3.2xlarge with Nvidia V100\r\n2.) Ubuntu 16.04 with CUDA 9.2 and Driver 396.37\r\n3.) nvidia-docker: Docker version 18.09.0, build 4d60db4\r\n4.) start MPS by $ sudo nvidia-cuda-mps-control -d\r\n5.) running nbody example from CUDA works fine with MPS: can be run in parallel if same user\r\n6.) have own Docker image with Tensorflow 1.8.0 (GPU) and CUDA 9.0.176, where Dockerfile starts with \r\nFROM tensorflow/tensorflow:1.8.0-gpu\r\n7.) Trying to run two instances of this image with CNN scripts inside via\r\n$ docker run --ipc=host --runtime=nvidia --rm -ti  dockerimage python ./cnn_tf_script.py\r\n**fails.** (N.B.: one instance works fine)\r\n\r\n- Does it mean that by default Tensorflow (1.8.0) still does not work with MPS and Volta cards? \r\n- And anyway one has to install Tensorflow from sources to make it working with MPS (even when Volta GPU card is used)?\r\n", "@ulto777 did you get any performance gain with mps on 1080ti?"]}, {"number": 9079, "title": "Add output_shape parameter to transposed2d_convolution layer", "body": "I just added this parameter that was missing in the `conv2d_transpose` layer.\r\nIf `None`, then the layer acts as usual returning the smallest shape possible", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 9078, "title": "module 'tensorflow' has no attribute 'summaries'", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["There is no `tensorflow.summaries` module. It is `tensorflow.summary`: https://www.tensorflow.org/api_guides/python/summary"]}, {"number": 9077, "title": "Any idea why \"failed to create cublas handle\"?", "body": "When I run a demo of Faster R-CNN,I meet an error,I have spend a whole week on it,but not work:\r\n\r\nEnv:\r\n- *OS*:Ubuntu16.04\r\n- *TensorFlow version*:1.0.1\r\n- *TensorFlow installed from (source or binary)?*:binary\r\n- *CUDA/cuDNN version*:CUDA8.0 cuDNN5.1\r\n- *GPU Model and Memory*: GTX 950 2G\r\n- *Exact command to reproduce*:python ./tools/demo.py --model model_path\r\n\r\nSee https://github.com/smallcorgi/Faster-RCNN_TF\r\n\r\nThe error as follows:\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 791.02MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 1.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nE tensorflow/stream_executor/cuda/cuda_blas.cc:372] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\nW tensorflow/stream_executor/stream.cc:1390] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"./tools/demo.py\", line 126, in <module>\r\n    _, _= im_detect(sess, net, im)\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/fast_rcnn/test.py\", line 179, in im_detect\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas SGEMM launch failed : m=1369, n=18, k=512\r\n\t [[Node: rpn_cls_score/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rpn_conv/3x3/rpn_conv/3x3, rpn_cls_score/weights/read)]]\r\n\t [[Node: cls_score/cls_score/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_357_cls_score/cls_score\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'rpn_cls_score/Conv2D', defined at:\r\n  File \"./tools/demo.py\", line 114, in <module>\r\n    net = get_network(args.demo_net)\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/networks/factory.py\", line 28, in get_network\r\n    return networks.VGGnet_test()\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/networks/VGGnet_test.py\", line 16, in __init__\r\n    self.setup()\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/networks/VGGnet_test.py\", line 40, in setup\r\n    .conv(1,1,len(anchor_scales)*3*2,1,1,padding='VALID',relu = False,name='rpn_cls_score'))\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 25, in layer_decorated\r\n    layer_output = op(self, layer_input, *args, **kwargs)\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 109, in conv\r\n    conv = convolve(input, kernel)\r\n  File \"/home/tangshouquan/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 100, in <lambda>\r\n    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Blas SGEMM launch failed : m=1369, n=18, k=512\r\n\t [[Node: rpn_cls_score/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](rpn_conv/3x3/rpn_conv/3x3, rpn_cls_score/weights/read)]]\r\n\t [[Node: cls_score/cls_score/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_357_cls_score/cls_score\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  **Make sure you also include the exact command if possible to produce  the output included in your test case**.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@asimshankar Thanks", "@aselle help me\uff0cplease...I changed tensorflow from 1.0.1 to 0.12.1 and reinstall CUDA,but not work.", "That question might be more appropriate for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow).", "Was / where was this solved? Please see my issue: https://github.com/tensorflow/models/issues/2857 "]}, {"number": 9076, "title": "ImportError: No module named 'tensorflow.models'", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: yes\r\n- *TensorFlow installed from (source or binary)?*: binary (anaconda)\r\n- *TensorFlow version*: 1.0.1\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*: 8.0\r\n- *GPU Model and Memory*: GTX 1080 8GB\r\n- *Exact command to reproduce*: import tensorflow.models\r\n\r\n### Describe the problem clearly\r\n$ python\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow.models\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow.models'\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["There is no `tensorflow.models` module.\r\n\r\nPerhaps you're looking for the models github repository? (https://github.com/tensorflow/models)", "from tensorflow.models.rnn.translate import data_utils\r\n\r\nImportError: No module named 'tensorflow.models'", "how solve\uff1f", "same error here as well :(", "You need to build the object_detection api usind setup.py build and setup.py install First\r\n", "write\r\n from tensorflow.python.keras._impl.keras.utils import data_utils\r\n", "> write\r\n> from tensorflow.python.keras._impl.keras.utils import data_utils\r\n\r\nI tried this and received this error: \r\n\"ModuleNotFoundError: No module named 'tensorflow.python.keras._impl'\"", "> > write\r\n> > from tensorflow.python.keras._impl.keras.utils import data_utils\r\n> \r\n> I tried this and received this error:\r\n> \"ModuleNotFoundError: No module named 'tensorflow.python.keras._impl'\"\r\n\r\nwrite\r\nfrom tensorflow.python.keras.utils import data_utils"]}, {"number": 9075, "title": "TensorBoard : Scalars have tags but no content", "body": "can see the graph and Scalars tags\uff0cbut  no Scalar content .\r\n\r\n**Environment info**\r\n\r\nubuntu 15.04\r\nvirtualenv Python 2.7\r\nTensoflow installed from binary pip package,version 1.0.1\r\nChrome  45.0.2454.101 Ubuntu 15.04 (64-bit)\r\n\r\n**About the code**\r\n\r\ncopy and run tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py ,\r\nthen \r\ntensorboard --logdir /tmp/tensorflow/mnist/logs/mnist_with_summaries\r\n\r\n**The following is output by adding \"--debug\":**\r\n\r\n DEBUG:tensorflow:No more events in /tmp/tensorflow/mnist/logs/mnist_with_summaries/test/events.out.tfevents.1491699349.amtb\r\nINFO:tensorflow:No path found after /tmp/tensorflow/mnist/logs/mnist_with_summaries/test/events.out.tfevents.1491699349.amtb\r\nDEBUG:tensorflow:Opening a record reader pointing at /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/events.out.tfevents.1491699347.amtb\r\nDEBUG:tensorflow:No more events in /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/events.out.tfevents.1491699347.amtb\r\nINFO:tensorflow:No path found after /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/events.out.tfevents.1491699347.amtb\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 14.782 secs\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /tmp/tensorflow/mnist/logs/mnist_with_summaries\r\nINFO:tensorflow:Adding events from directory /tmp/tensorflow/mnist/logs/mnist_with_summaries/train\r\nINFO:tensorflow:Adding events from directory /tmp/tensorflow/mnist/logs/mnist_with_summaries/test\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /tmp/tensorflow/mnist/logs/mnist_with_summaries\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:No more events in /tmp/tensorflow/mnist/logs/mnist_with_summaries/test/events.out.tfevents.1491699349.amtb\r\nINFO:tensorflow:No path found after /tmp/tensorflow/mnist/logs/mnist_with_summaries/test/events.out.tfevents.1491699349.amtb\r\nDEBUG:tensorflow:No more events in /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/events.out.tfevents.1491699347.amtb\r\nINFO:tensorflow:No path found after /tmp/tensorflow/mnist/logs/mnist_with_summaries/train/events.out.tfevents.1491699347.amtb\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.003 secs\r\n\r\n**The following is output by adding \"--inspect\":**\r\n\r\n\r\nProcessing event files... (this can take a few minutes)\r\n\r\nFound event files in:\r\n/tmp/tensorflow/mnist/logs/mnist_with_summaries/train\r\n/tmp/tensorflow/mnist/logs/mnist_with_summaries/test\r\n\r\nThese tags are in /tmp/tensorflow/mnist/logs/mnist_with_summaries/train:\r\naudio -\r\nhistograms\r\n   layer1/Wx_plus_b/pre_activations\r\n   layer1/activations\r\n   layer1/biases/summaries/histogram\r\n   layer1/weights/summaries/histogram\r\n   layer2/Wx_plus_b/pre_activations\r\n   layer2/activations\r\n   layer2/biases/summaries/histogram\r\n   layer2/weights/summaries/histogram\r\nimages\r\n   input_reshape/input/image/0\r\n   input_reshape/input/image/1\r\n   input_reshape/input/image/2\r\n   input_reshape/input/image/3\r\n   input_reshape/input/image/4\r\n   input_reshape/input/image/5\r\n   input_reshape/input/image/6\r\n   input_reshape/input/image/7\r\n   input_reshape/input/image/8\r\n   input_reshape/input/image/9\r\nscalars\r\n   accuracy_1\r\n   cross_entropy_1\r\n   dropout/dropout_keep_probability\r\n   layer1/biases/summaries/max\r\n   layer1/biases/summaries/mean\r\n   layer1/biases/summaries/min\r\n   layer1/biases/summaries/stddev_1\r\n   layer1/weights/summaries/max\r\n   layer1/weights/summaries/mean\r\n   layer1/weights/summaries/min\r\n   layer1/weights/summaries/stddev_1\r\n   layer2/biases/summaries/max\r\n   layer2/biases/summaries/mean\r\n   layer2/biases/summaries/min\r\n   layer2/biases/summaries/stddev_1\r\n   layer2/weights/summaries/max\r\n   layer2/weights/summaries/mean\r\n   layer2/weights/summaries/min\r\n   layer2/weights/summaries/stddev_1\r\n\r\n\r\nEvent statistics for /tmp/tensorflow/mnist/logs/mnist_with_summaries/train:\r\naudio -\r\ngraph\r\n   first_step           0\r\n   last_step            0\r\n   max_step             0\r\n   min_step             0\r\n   num_steps            1\r\n   outoforder_steps     []\r\nhistograms\r\n   first_step           1\r\n   last_step            999\r\n   max_step             999\r\n   min_step             1\r\n   num_steps            900\r\n   outoforder_steps     []\r\nimages\r\n   first_step           1\r\n   last_step            999\r\n   max_step             999\r\n   min_step             1\r\n   num_steps            900\r\n   outoforder_steps     []\r\nscalars\r\n   first_step           1\r\n   last_step            999\r\n   max_step             999\r\n   min_step             1\r\n   num_steps            900\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\n\r\n\r\nThese tags are in /tmp/tensorflow/mnist/logs/mnist_with_summaries/test:\r\naudio -\r\nhistograms\r\n   layer1/Wx_plus_b/pre_activations\r\n   layer1/activations\r\n   layer1/biases/summaries/histogram\r\n   layer1/weights/summaries/histogram\r\n   layer2/Wx_plus_b/pre_activations\r\n   layer2/activations\r\n   layer2/biases/summaries/histogram\r\n   layer2/weights/summaries/histogram\r\nimages\r\n   input_reshape/input/image/0\r\n   input_reshape/input/image/1\r\n   input_reshape/input/image/2\r\n   input_reshape/input/image/3\r\n   input_reshape/input/image/4\r\n   input_reshape/input/image/5\r\n   input_reshape/input/image/6\r\n   input_reshape/input/image/7\r\n   input_reshape/input/image/8\r\n   input_reshape/input/image/9\r\nscalars\r\n   accuracy_1\r\n   cross_entropy_1\r\n   dropout/dropout_keep_probability\r\n   layer1/biases/summaries/max\r\n   layer1/biases/summaries/mean\r\n   layer1/biases/summaries/min\r\n   layer1/biases/summaries/stddev_1\r\n   layer1/weights/summaries/max\r\n   layer1/weights/summaries/mean\r\n   layer1/weights/summaries/min\r\n   layer1/weights/summaries/stddev_1\r\n   layer2/biases/summaries/max\r\n   layer2/biases/summaries/mean\r\n   layer2/biases/summaries/min\r\n   layer2/biases/summaries/stddev_1\r\n   layer2/weights/summaries/max\r\n   layer2/weights/summaries/mean\r\n   layer2/weights/summaries/min\r\n   layer2/weights/summaries/stddev_1\r\n\r\n\r\nEvent statistics for /tmp/tensorflow/mnist/logs/mnist_with_summaries/test:\r\naudio -\r\ngraph -\r\nhistograms\r\n   first_step           0\r\n   last_step            990\r\n   max_step             990\r\n   min_step             0\r\n   num_steps            100\r\n   outoforder_steps     []\r\nimages\r\n   first_step           0\r\n   last_step            990\r\n   max_step             990\r\n   min_step             0\r\n   num_steps            100\r\n   outoforder_steps     []\r\nscalars\r\n   first_step           0\r\n   last_step            990\r\n   max_step             990\r\n   min_step             0\r\n   num_steps            100\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\n\r\n\r\n**issue picture**:\r\n![issue](https://cloud.githubusercontent.com/assets/4977856/24833889/94f2d2e2-1d08-11e7-8c11-aa91917549c4.png)", "comments": ["Thanks for reaching out. Any chance you could press Ctrl+Alt+J or Command+Option+J in Chrome to open up the JavaScript log, so you can let us know if there are any errors in there?", "Thank you @jart , the console has  an error :\r\n\r\n Uncaught SyntaxError: Block-scoped declarations (let, const, function, class) not yet supported outside strict mode\r\n/deep/ combinator is deprecated. See https://www.chromestatus.com/features/6750456638341120 for more details.", "Ah, I think we have let the `let` keyword slip into our html files. It's safe when guarded by the TS compiler, but not when it makes it into html/js. Will fix.", "Alternatively, maybe we can just enable strict mode?", "@jart @dandelionmane \r\n   when I update the chrome  to version 57.0.2987.133 ,the issue solved."]}, {"number": 9074, "title": "Documentation: broken links and image in documentation", "body": "The URL:  https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_sum has a broken link labelled \"the section on Segmentation\".  Also, there is a broken image on that page as well.\r\n\r\nSame problems here too (except not all have an image to be broken):\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_prod\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_min\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_max\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_mean\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/unsorted_segment_sum\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/sparse_segment_sum\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/sparse_segment_mean\r\n   https://www.tensorflow.org/versions/master/api_docs/python/tf/sparse_segment_sqrt_n\r\n\r\nThe page I believe they should link back to is here: https://www.tensorflow.org/versions/master/api_guides/python/math_ops#Segmentation\r\n\r\nI came here because I haven't grokked just what segmented operations are yet.  So I would be much obliged if the explanation of segmentation were more thoroughly be explained there, perhaps with a couple of examples.\r\n\r\nI found [a few places] in the code where this could be address.  And I would have attempted a pull request to fix these, but I'm not up on how to generate the documentation to verify that it was fixed.", "comments": ["Thanks for the report, the documentation is indeed broken. Looking into a fix.\r\n\r\nAs a workaround in the mean time, you can see the images at https://www.tensorflow.org/images/SegmentProd.png etc.\r\n", "While you're at it, I noticed that on the unsorted_segment_sum page, it refers to perhaps an older naming scheme:\r\n   \"Unlike **SegmentSum**, segment_ids need not be sorted and need not cover all values in the full range of valid values.\""]}, {"number": 9073, "title": "Errors when building iOS binaries with selective registration", "body": "The header file generated by [print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py) makes use of strcmp, which causes an error when compiling TensorFlow for iOS with selective registration enabled (`-DSELECTIVE_REGISTRATION`)\r\n\r\nApparently this was fixed in [a commit back in October](https://github.com/tensorflow/tensorflow/commit/c37847c1e5aedf5f33151895bdcbf9de89bbd759) but that commit was [reverted the same day](https://github.com/tensorflow/tensorflow/commit/3510b3060abef6b99fe4bc0656d8dc9406d6c46d) for some reason.\r\n\r\nI\u2019ve tried building the tool from the commit of TensorFlow that has the fix, but `./configure` fails for me on that version (missing targets & zlib download). I\u2019ve also tried replacing calls to strcmp with a constexpr variant, but I\u2019m not much help with C++ and couldn\u2019t get anything to work there.\r\n\r\nThis prevents me from compiling a slimmed down TensorFlow binary for iOS.\r\n\r\ncc @petewarden @cwhipkey \r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No, just following the instructions in [print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py) and [selective_registration.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/selective_registration.h)\r\n- *TensorFlow installed from (source or binary)?*: Source\r\n- *TensorFlow version*: master (45115c0a985815feef3a97a13d6b082997b38e5d)\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: N/A\r\n- *GPU Model and Memory*: N/A\r\n- *Exact command to reproduce*:\r\n    ```bash\r\n    $ bazel build tensorflow/python/tools/print_selective_registration_header\r\n    $ bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n      --graphs=graph.pb > tensorflow/core/framework/ops_to_register.h\r\n    $ tensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-Os -DSELECTIVE_REGISTRATION\"\r\n\r\n### Source Code / Logs\r\nHere is the relevant log output on the last command:\r\n\r\n```\r\nIn file included from ./tensorflow/core/framework/selective_registration.h:46:\r\n./tensorflow/core/framework/ops_to_register.h:4:23: error: constexpr function never produces a\r\n      constant expression [-Winvalid-constexpr]\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n                      ^\r\n./tensorflow/core/framework/ops_to_register.h:6:10: note: non-constexpr function 'strcmp' cannot\r\n      be used in a constant expression\r\n     || (strcmp(op, \"Add\") == 0)\r\n         ^\r\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS10.2.sdk/usr/include/string.h:77:6: note:\r\n      declared here\r\nint      strcmp(const char *__s1, const char *__s2);\r\n         ^\r\n```\r\n", "comments": ["It seems you're claiming building TensorFlow for iOS using contrib/makefile hasn't worked since October.\r\n\r\nAssigning to @petewarden who works on mobile support. CC @wrwg wrote the rollback.", "> It seems you're claiming building TensorFlow for iOS using contrib/makefile hasn't worked since October.\r\n\r\nAh sorry, my bad. To be clear, building standard iOS binaries works fine. It\u2019s just trying to build the iOS binaries _with selective registrations_ that seems to be broken. I edited the original issue to be clearer. Thanks @jart!", "it does not work also when compiling for Android with SELECTIVE_REGISTRATION . Same problem.", "A quick workaround: Paste the following functions to the beginning of the ops_to_register.h header:\r\n`\r\n\r\nconstexpr int strcmpc(const char* a, const char* b)\r\n  {\r\n\r\n    return *a == 0 && *b == 0 ? 0 :\r\n      *a == 0 ? -1 :\r\n      *b == 0 ? 1 :\r\n      *a < *b ? -1 :\r\n      *a > *b ? 1 :\r\n      *a == *b ? strcmpc(a+1, b+1) :\r\n      false;\r\n}\r\n\r\ntemplate <unsigned N2>\r\nconstexpr const char * strstrc( const char *str, const char (&substr)[N2])\r\n{\r\nreturn ( 0 == str )? NULL : ( true == strcmpc( str, substr ) )? str : strstrc( str + 1, substr );\r\n}\r\n`\r\n\r\nand replace all the strcmp calls to strcmpc\r\nand the strstr call to strstrc.\r\n\r\nPlease fix this bug in the next version.\r\n\r\nThanks", "Thanks @es007, I got a little further thanks to your comment, but it did not solve the problem.\r\n\r\nFirst of all, your `strstrc` declaration does not seem necessary and creates a compile-time error for my ops_to_register.h. I just removed it, and I got through running `tensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-Os -DSELECTIVE_REGISTRATION\"`. The binary libtensorflow-core.a that comes out is noticeably smaller, 263MB instead of 483.\r\n\r\nI can compile my iOS app with it just fine, but I hit an error whenever I try to build the graph. The error in my logs is:\r\n\r\n```\r\nCould not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](input_1, conv1/kernel)]]\r\n```\r\n\r\nFWIW, the Conv2D OpKernel seems properly listed in [my ops_to_register.h file](https://www.dropbox.com/s/bginmeuipl5i479/ops_to_register.h?dl=0)).\r\n\r\nI thought it could be caused by the `use_cudnn_on_gpu=true` attribute, but the fact is the exact same graph file loads & runs fine if I use a regular TensorFlow iOS library (without selective registration), so it still sounds like an issue related selective registration?", "In some cases, the host compiler used to generate the header file differs\nfrom the compiler used for generating the on-device code - the difference\nbeing in how whitespace pasting works in some parts of macro preprocessing.\n\nThe version of the selective registration python lib at master resolves\nthis by doing comparison ignoring whitespace.\n\nIt also removes use of strstr so that could be useful as well.\n\nThe strcmp issue may still remain, but that could be resolved with strcmp\nimplementation listed in this thread.\n\nOn Apr 9, 2017 11:20 AM, \"Tim Anglade\" <notifications@github.com> wrote:\n\n> Thanks @es007 <https://github.com/es007>, I got a little further thanks\n> to your comment, but it did not solve the problem.\n>\n> First of all, your strstrc declaration does not seem necessary and\n> creates a compile-time error for my ops_to_register.h. I just removed it,\n> and I got through running tensorflow/contrib/makefile/compile_ios_tensorflow.sh\n> \"-Os -DSELECTIVE_REGISTRATION\". The binary libtensorflow-core.a that\n> comes out is noticeably smaller, 263MB instead of 483.\n>\n> I can compile my iOS app with it just fine, I hit an error whenever I try\n> to build the graph. The error in my logs is:\n>\n> Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\n>   <no registered kernels>\n>\n> \t [[Node: conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](input_1, conv1/kernel)]]\n>\n> FWIW, the Conv2D OpKernel seems properly listed in my ops_to_register.h\n> file <https://www.dropbox.com/s/bginmeuipl5i479/ops_to_register.h?dl=0>).\n>\n> I thought it could be caused by the use_cudnn_on_gpu=true attribute, but\n> the fact is the exact same graph file loads & runs fine if I use a regular\n> TensorFlow iOS library (without selective registration), so it still sounds\n> like an issue related selective registration?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9073#issuecomment-292802517>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQw4wbciD70Fch5tBgQ-5gJaJ7pylNmfks5ruSFfgaJpZM4M3315>\n> .\n>\n", "@cwhipkey Thanks. To be clear, I\u2019ve been working from master the past 2 days, and I\u2019m still unable to get selective registration to work as expected when compiling iOS binaries, even when I get past the strcmp issue. Is there something I should be doing to avoid hitting this issue?", "For what it\u2019s worth I also tried to manually fix the whitespace (as suggested in a [GitHub issue](https://github.com/tensorflow/tensorflow/issues/8486#issuecomment-288577876)), but still the same issue remains: libtensorflow-core.a compiles fine, but complains at runtime about no OpKernel being registered for Conv2D.", "@timanglade Can you paste your ops_to_register.h header file here?\r\n\r\nI just saw you posted it, and the format seems different than mine. \r\nYou do need the strstrc function that I posted above, you should replace line 78 in your ops_to_register.h file:\r\n\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nwith:\r\n\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (strstrc(kNecessaryOpKernelClasses, \",\" clz \",\") != nullptr)\r\n\r\nand paste the strstrc function from above to the beginning of the file. And things should work fine for you.\r\n\r\nI succeeded in compiling TF for android with selective registration support with this workaround.\r\n\r\ngood luck", "As for the white spaces, this function is insensitive to them:\r\n`constexpr int strcmpc(const char* a, const char* b)\r\n{\r\n\r\nreturn *a == 0 && *b == 0 ? 0 :\r\n       *a == ' ' ? strcmpc(a+1,b) :\r\n       *b == ' ' ? strcmpc(a,b+1) :\r\n  *a == 0 ? -1 :\r\n  *b == 0 ? 1 :\r\n  *a < *b ? -1 :\r\n  *a > *b ? 1 :\r\n  *a == *b ? strcmpc(a+1, b+1) :\r\n  false;\r\n\r\n}`\r\n\r\nand at least in my ops_to_register.h file I have this macro defined:\r\n`#define SHOULD_REGISTER_OP_KERNEL(clz) (strstr(kNecessaryOpKernelClasses, \",\" clz \",\") != nullptr)`\r\n\r\nI got the talked error:\r\n`non-constexpr function 'strstr' cannot\r\n      be used in a constant expression` too, and this is why I switch strstr with the strstrc implementation above and this solved the error.\r\n\r\nI'm compiling TF (selective registration) for Android  armeabi-v7a.", "I'm compiling for iOS which I think is the difference here. I did find references to a similar issue for Android that was resolved some time ago, but no references to selective registration working (or failing) on iOS.\r\n\r\nAs mentioned above [my ops_to_register.h is available on Dropbox](https://www.dropbox.com/s/bginmeuipl5i479/ops_to_register.h?dl=0) It compiles fine but fails at runtime, with an error staying there is no OpKernel for Conv2D.", "Try this:\r\n\r\nreplace line 78 in your ops_to_register.h file:\r\n\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nwith:\r\n\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (strstrc(kNecessaryOpKernelClasses, \",\" clz \",\") != nullptr)\r\n\r\nand paste the strstrc function from above to the beginning of the file. ", "I think that on ios, the kernel for convolution is the one from core/kernels/conv_ops_using_gemm.cc, which has a different name from what is in your ops_to_register.h file.   One thing to try would be to change the kernel class name in conv_ops_using_gemm.cc to be just Conv2DOp instead of Conv2DUsingGemmOp and see if then the kernel is included.", "@cwhipkey I tried changing all occurrences of \u201cConv2DUsingGemmOp\u201d to \u201cConv2DOp\u201d in core/kernels/conv_ops_using_gemm.cc but the outcome is the same: compiles fine, fails at runtime complaining there is no OpKernel registered for Conv2D.\r\n\r\n@es007 Thanks for suggestion, this fails as well, but at compile time. Complaints about the template keyword and the N2 keyword, which in retrospect seems obvious. Am I missing a part of your implementation in [your comment earlier](https://github.com/tensorflow/tensorflow/issues/9073#issuecomment-292779949)?  I tried alternate implementations of strstrc but couldn\u2019t get anything to work, really.\r\n\r\nThanks both for your help. I\u2019m not very comfortable with C++ so there may be obvious fixes I\u2019m missing\u2026\r\n\r\nI\u2019m hoping someone who has successfully used selective registration on iOS can chime in on what has to be done to get ops_to_register.h to work as expected?", "Sorry, I think my suggestion was bad - because even with that name change,\nthe Conv2DUsingGemmOp won't have the exact same signature as the Conv2DOp.\n\nThe reason I am guessing that is the issue, though, is because:\n1. if all kernels were being omitted, then you would probably see a\ndifferent kind of op being failed to find earlier.\n2. a macro (USE_GEMM_FOR_CONV) is defined to control whether Conv2DOp is\nused or Conv2DUsingGemmOp.  It seems possible that this macro was not\ndefined when building the tool to generate the header, and was defined when\ncompiling for the device.\n\nYou could instead remove definition of that macro from the makefile.  Or\ninstead include that macro when building the tool to generate the header\nfile, so you get a header file that includes Conv2DUsingGemmOp.\n\nI'll be able to try out on my end on Monday (I've been mostly out of the\noffice this week).\n\nOn Wed, Apr 12, 2017 at 1:06 PM, Tim Anglade <notifications@github.com>\nwrote:\n\n> @cwhipkey <https://github.com/cwhipkey> I tried changing all occurrences\n> of \u201cConv2DUsingGemmOp\u201d to \u201cConv2DOp\u201d in core/kernels/conv_ops_using_gemm.cc\n> but the outcome is the same: compiles fine, fails at runtime complaining\n> there is no OpKernel registered for Conv2D.\n>\n> @es007 <https://github.com/es007> Thanks for suggestion, this fails as\n> well, but at compile time. Complaints about the template keyword and the N2\n> keyword, which in retrospect seems obvious. Am I missing a part of your\n> implementation in your comment earlier\n> <https://github.com/tensorflow/tensorflow/issues/9073#issuecomment-292779949>?\n> I tried alternate implementations of strstrc but couldn\u2019t get anything to\n> work, really.\n>\n> Thanks both for your help. I\u2019m not very comfortable with C++ so there may\n> be obvious fixes I\u2019m missing\u2026\n>\n> I\u2019m hoping someone who has successfully used selective registration on iOS\n> can chime in on what has to be done to get ops_to_register.h to work as\n> expected?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9073#issuecomment-293691770>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQw4wZbMOyM-_Jcltoj17P0dcIc-AyCaks5rvS7DgaJpZM4M3315>\n> .\n>\n", "Alright @cwhipkey it worked! The full solution I used is as follows:\r\n\r\n1. Build the print_selective_registration_header with the right macro for iOS\r\n    ```bash\r\n    $ bazel build --copt=\"-DUSE_GEMM_FOR_CONV\" tensorflow/python/tools/print_selective_registration_header\r\n    ```\r\n2. Generate a header in the right path, i.e.\r\n    ```bash\r\n    $ bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n      --graphs=graph.pb > tensorflow/core/framework/ops_to_register.h\r\n    ```\r\n3. Edit **ops_to_register.h**\r\n    * Replace all occurrences of `strcmp` with `strcmpc`\r\n    * Add the following function definition at the top of the file\r\n        ```c++\r\n        constexpr int strcmpc(const char* a, const char* b)\r\n        {\r\n          return *a == 0 && *b == 0 ? 0 :\r\n          *a == ' ' ? strcmpc(a+1,b) :\r\n          *b == ' ' ? strcmpc(a,b+1) :\r\n          *a == 0 ? -1 :\r\n          *b == 0 ? 1 :\r\n          *a < *b ? -1 :\r\n          *a > *b ? 1 :\r\n          *a == *b ? strcmpc(a+1, b+1) :\r\n          false;\r\n        }\r\n        ```\r\n4. Build TensorFlow for iOS with the -DSELECTIVE_REGISTRATION flag:\r\n    ```bash\r\n    $ tensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-Os -DSELECTIVE_REGISTRATION\"\r\n    ```\r\n\r\nIn my case, the resulting libtensorflow-core.a is 45% smaller, the universal .ipa is 6.1MB lighter, and the unzipped payload is 19.5MB lighter. (Of course, your mileage may vary depending on the graph you are using selective registration on.) Thanks everyone!\r\n", "Adding a note here for Windows folks who run into this problem. The symptom in the windows build is:\r\n\r\nerror C2975: 'should_register': invalid template argument for 'tensorflow::register_op::OpDefBuilderWrapper', expected compile-time constant expression\r\n\r\nAdding the strcmpc function and replacing strcmp with strcmpc in ops_to_register.h resolved it.", "Thanks, I've submitted a change that replaces the strcmpc calls with calls to the isequals function that is already emitted in ops_to_register.h.  This should resolve the ios build problem (I tried in a dummy .cc file and it worked).\r\n\r\nThe code should be available on github in the next few days...", "@cwhipkey can you tell me the \"commit id\" and the branch which replaces the strcmp calls with calls to the isequals function. The 1.2 release branch seems to still generate generate strcmpc calls while in master,  isequals calls are generated. ", "https://github.com/tensorflow/tensorflow/commit/f333979e0d8625bca09c96478503c896fda24f25\r\n\r\nYes, it looks like it's not in 1.2 afaict:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/tools/print_selective_registration_header_test.py", "@timanglade @cwhipkey \r\nI tried to use -DUSE_GEMM_FOR_CONV flag for android selective registration as well like this\r\nbazel build --copt=\"-DUSE_GEMM_FOR_CONV\" tensorflow/python/tools/print_selective_registration_header\r\n\r\nBut during runtime it gives error and neural net fails to run. I tested on huawei nexus 6p. Without  --copt=\"-DUSE_GEMM_FOR_CONV\", it runs fine. What does this mean? \r\n\r\nI believe \"DUSE_GEMM_FOR_CONV\" flag transforms convolution operation into matrix multiply to increase throughput(https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/). But since this is failing on android, we are not getting that efficiency on android devices. Is this correct? Can we use gemm for conv on adnroid?", "hello, I am a new user of tensorflow. I wish to integrate graph.pb file into an ios device, and have followed all the compilation instructions carefully, yet when I try to run the camera example app I get the same error as previously mentioned:\r\n```\r\nE \r\n/tensorflow/tensorflow/examples/ios/camera_copy/tensorflow_utils.mm:140] Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node pyramid_regression_0/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](input_2, pyramid_regression_0/kernel/read)]]\r\n2018-08-22 16:54:00.303760: F tensorflow/examples/ios/camera_copy/tensorflowUtil.mm:68] Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node pyramid_regression_0/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](input_2, pyramid_regression_0/kernel/read)]]\r\n```\r\n\r\nI also followed the instructions that @timanglade detailed in the comments above, but there are some differences (probably due to the versions diffs between a year ago and now)\r\n1. `bazel build --copt=\"-DUSE_GEMM_FOR_CONV\" tensorflow/python/tools/print_selective_registration_header` compiled without errors\r\n\r\n2. when I create the `ops_to_register.h` file using the _bazel-bin_ command, it adds a strange print of `tf.estimator package not installed` to the first line. In addition, during the file creation this warning appears multiple times:\r\n` RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88`\r\n\r\n3. when I create the `ops_to_register.h` file using  _python3_ , it creates the file without strange prints\r\n4. in both cases there are no occurrences of `strcmp` or `strcmpc` at all, but instead of `Conv2DUsingGemmOp` got `Conv2DOp`\r\n\r\n5. i tried to use the `ops_to_register.h` file for the `tensorflow/contrib/makefile/compile_ios_tensorflow.sh` compilation, but it fails during the compilation:\r\n```\r\nUndefined symbols for architecture arm64:\r\n  \"nsync::nsync_cv_signal(nsync::nsync_cv_s_*)\", referenced from:\r\n      tensorflow::condition_variable::notify_one() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*)\", referenced from:\r\n      tensorflow::condition_variable::wait_until_system_clock(tensorflow::mutex_lock&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > >) in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_cv_init(nsync::nsync_cv_s_*)\", referenced from:\r\n      tensorflow::condition_variable::condition_variable() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_from_time_point_(std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > >)\", referenced from:\r\n      tensorflow::condition_variable::wait_until_system_clock(tensorflow::mutex_lock&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > >) in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::condition_variable::wait(tensorflow::mutex_lock&) in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_mu_runlock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::unlock_shared() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_mu_rlock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::lock_shared() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::unlock() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_mu_lock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::lock() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_cv_broadcast(nsync::nsync_cv_s_*)\", referenced from:\r\n      tensorflow::condition_variable::notify_all() in libtensorflow-core-arm64.a(mutex.o)\r\n  \"nsync::nsync_mu_init(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::mutex::mutex() in libtensorflow-core-arm64.a(mutex.o)\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [/tensorflow/tensorflow/contrib/makefile/gen/bin/ios_ARM64/benchmark] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'arm64 compilation failed.'\r\narm64 compilation failed.\r\n+ exit 1\r\n```\r\n\r\nenvironment details:\r\n- tensorflow 1.9.0 (there is tensorflow 1.10.0, but is currently not supported for the ios compilations)\r\n- python 3.6.6\r\n- all the compilations are done with **`-a arm64`** only\r\n\r\ndoes anyone have an idea what went wrong here?\r\n\r\n", "@avishayzanbar if you want to use the compile_ios_tensorflow.sh script you can simply use the long command that's in the universal binaries section (under the building by hand section) in the makefile readme (it says you should use that if you encounter issues with undefined nsync symbols)\r\nIf that doesn't work then:\r\nfirst run the compile_nsync.sh script with -t ios -a arm64\r\nthen look where the libnsync.a library is in the downloads/nsync/ subfolders\r\nThen replace the following lines in the build_all_ios.sh script: HOST_NSYNC_LIB=... and TARGET_NSYNC_LIB=... with\r\nHOST_NSYNC_LIB='/absolute/path/to/libnsync.a' \r\nTARGET_NSYNC_LIB='absolute//path/to/libnsync.a' \r\nThen call build_all_ios.sh -a arm64 -g graph.pb \r\nHope that resolves your issues"]}, {"number": 9072, "title": "bezel build problem for OSX: \"No toolchain corresponding to 'local_darwin' found for cpu darwin\" ", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No, this is an installation issue.  \r\n- *TensorFlow installed from (source or binary)?*: Source  \r\n- *TensorFlow version*: 1.0.1  \r\n- *Bazel version (if compiling from source)*: Build label: 0.4.5-homebrew  \r\n- *CUDA/cuDNN version*: 8.0.71  \r\n- *GPU Model and Memory*: NVIDIA GeForce GT 750M, 2048 MB  \r\n- *Exact command to reproduce*: From Installing Tensorflow from sources (https://www.tensorflow.org/install/install_sources#PrepareMac): bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  \r\n\r\n### Describe the problem clearly\r\nReceiving the following error when trying to build from source:  \r\n```\r\n  $ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  \r\n    ERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'.  \r\n    INFO: Elapsed time: 0.101s  \r\n```\r\n\r\nI'm wondering if 'local_darwin' is an incorrect parameter in the TensorFlow bazel build setup files.  If so, they need to be corrected and updated.  \r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n\r\nInput and output pasted in previous questions.  \r\n\r\n", "comments": ["Just to make sure, did you run `./configure` beforehand?", "Im also seeing the same issue. This only seems to happen when trying to build with GPU support, cpu still compiles fine.", "Same here. Here is the ./configure output\r\n```\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: /usr/local/bin/python3\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\r\nGoogle Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] y\r\nClang will be used as CUDA compiler\r\nPlease specify which clang should be used as device and host compiler. [Default is /usr/bin/clang]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n............\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n```", "@udnaan @BKJackson @cody-code-wy If you say no to the `./configure` prompt about using Clang as the CUDA compiler, do things work?", "@jart It goes all the way near to the end and then fails. First complaining about `-zmuldefs`, a flag that is not supported by clang, which is symlinked to gcc on MacOS. If I hand edit the build file and comment it out then it fails at the last step complaining about `-lgomp`. Commenting that out completes the compile, likely without openMP support. \r\n\r\nI'm installing gcc-5.4 now to see whether that improves things.", "@udnaan It complains about things not being supported by Clang when you disable Clang? Copying and pasting the error messages into this issue helps. Especially if the full log output is attached too.", "Here is the diff that seems to work the best so far. You have to have Xcode 8.2 for it to work:\r\n\r\n```\r\n@ BUILD:1984 @ tf_kernel_library(\r\n    # and //third_party/libf2c all contain various parts of BLAS, LAPACK,\r\n    # and f2c helper functions in global namespace. Tell the compiler to\r\n    # allow multiple definitions when linking this.\r\n    linkopts = [\"-Wl,-zmuldefs\"],\r\n    #linkopts = [\"-Wl,-zmuldefs\"],\r\n    visibility = [\"//visibility:private\"],\r\n    deps = [\r\n        \"//tensorflow/core:framework\",\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\nmodified: third_party/gpus/cuda/BUILD.tpl\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n@ BUILD.tpl:99 @ cc_library(\r\n    data = [\"lib/%{cusolver_lib}\"],\r\n    includes = [\"include/\"],\r\n    linkstatic = 1,\r\n    linkopts = [\"-lgomp\"],\r\n    linkopts = [\"-fopenmp\"],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\r\n ~/p/a/tensorflow\r\n ~/p/a/tensorflow \ue0b0 \ue0a0 master * \ue0b0 rlwrap ./configure                                                            Thu Apr 13 23:17:40 2017\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: /usr/local/bin/python3\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\r\nGoogle Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N]\r\nnvcc will be used as CUDA compiler\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n```", "@jart Unfortunately the install from source guide is not clear on what compiler needs to be on the system. I am assuming that either gcc-5.4 or Xcode 8.2 bundled version as this is what CUDA 8.0 supports. So far I've tried gcc-5.4. Here is the error message:\r\n\r\n```\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: /usr/local/bin/python3\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\r\nGoogle Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N]\r\nnvcc will be used as CUDA compiler\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/local/bin/gcc-5\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n ~/p/a/tensorflow \ue0b0 \ue0a0 master \ue0b0 bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /Users/nan/p/ai/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /Users/nan/p/ai/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_nan/b15ef8d15d1d0bee62bab725709ba977/external/protobuf/BUILD:241:1: C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign ... (remaining 31 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc-5: error: unrecognized command line option '-fcolor-diagnostics'\r\ngcc-5: error: unrecognized command line option '-Wthread-safety'\r\ngcc-5: error: unrecognized command line option '-Wself-assign'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /Users/nan/p/ai/tensorflow/tensorflow/core/BUILD:1336:28 C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign ... (remaining 31 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nINFO: Elapsed time: 3.792s, Critical Path: 0.19s\r\n```\r\nNow I'm going back to Xcode 8.2 bundles version and will post the error log once I have it. ", "Error with Xcode 8.2 commandline tools. Similar to what is originally reported by @BKJackson \r\n```\r\n ! \ue0b0 ~/p/a/tensorflow \ue0b0 \ue0a0 master * \ue0b0 rlwrap ./configure                                                20.9s \ue0b3 Thu Apr 13 23:43:28 2017\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: /usr/local/bin/python3\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] y\r\nGoogle Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] y\r\nClang will be used as CUDA compiler\r\nPlease specify which clang should be used as device and host compiler. [Default is /usr/bin/clang]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n ~/p/a/tensorflow \ue0b0 \ue0a0 master * \ue0b0 bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'.\r\nINFO: Elapsed time: 0.502s\r\n```\r\n\r\n@jart I'm going to leave this up to you for now. I have a work around https://github.com/tensorflow/tensorflow/issues/9072#issuecomment-294098995 which should let me continue. ", "There's a lot going on in this thread.\r\n\r\n@rmlarsen It seems `-lgomp` and `-zmuldefs` were introduced four days ago in https://github.com/tensorflow/tensorflow/commit/72c023d3967a3218cd3d830ce6e57f7c4d87a18c. Do we need `select()` expressions to make them cross platform?\r\n\r\n@martinwicke Does \"ERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'.\" mean anything to you?\r\n\r\n@jugglerix Are there opportunities here to improve install_mac.md?", "I can 100% corroborate the results of @BKJackson, @cody-code-wy, and @udnaan. The `ERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'` error only occurs when passing the `--config=cuda` argument to `bazel build` and selecting `clang` as the compiler during the `./configure` script. And, as @udnaan points out earlier in the discussion, on macOS gcc is just a symlink to clang so `-zmuldefs` will cause the linking error in the final steps of the build process.\r\n\r\nReport:\r\n=======================================================================================\r\n### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: `Not at all. I have followed TensorFlow install from source docs` *refs:* [1](https://www.tensorflow.org/install/install_sources#PrepareMac)\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: `macOS 10.11.6 (15G1421)`\r\n- *TensorFlow installed from (source or binary)?*: `Source`\r\n- *TensorFlow version* (use command below): `1.0.1`\r\n- *Bazel version (if compiling from source)*: `0.4.5-homebrew`\r\n- *CUDA/cuDNN version*: `8.0/5`\r\n- *GPU Model and Memory*: `NVIDIA GeForce GT 750M, 2048 MB`\r\n- *Exact command to reproduce*: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` *refs:* [1](https://www.tensorflow.org/install/install_sources#PrepareMac)\r\n\r\n\r\n### Describe the problem clearly\r\nThe problem (@BKJackson references) occurs after I run `./configure` and then `bazel`:\r\n\r\n*First I configure:*\r\n```\r\nPlease specify the location of python. [Default is /usr/local/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\n  /Library/Python/2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] y\r\nClang will be used as CUDA compiler\r\nPlease specify which clang should be used as device and host compiler. [Default is /usr/bin/clang]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n```\r\n\r\n*Then I execute bazel:*\r\n```\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  \r\n  ERROR: Inconsistent crosstool configuration; no toolchain corresponding to  'local_darwin' found for cpu 'darwin'.  \r\n  INFO: Elapsed time: 0.503s\r\n```\r\n\r\nI believe, as @BKJackson stated, this is an issue with the bazel BUILD and/or\r\nCROSSTOOL files (possibly in the third_party/gpus/cuda package?).\r\n\r\n\r\n### Source Code / Logs\r\nSee `Describe the problem clearly` section for output.\r\n", "You get around that first error by saying not to use using clang as CUDA Compiler and instead using nvcc which it will do if you say no to that option", "jart@: I have never seen the error, but it looks like it's a problem with the toolchain definition for clang on mac. I don't think we had clang on mac explicitly, I'm not even sure when the ability to select a compiler was introduced.", "@BKJackson \r\n\u6211\u7684\u662fimac\u548c\u4f60\u4e00\u6837\u7684\u95ee\u9898 \u5728./configure\u65f6\u5019Do you want to use clang as CUDA compiler? [y/N] N\u9009N\uff0c\u4e0d\u7528clang\u7528gcc\u5c31\u884c\u4e86", "thanks to @BKJackson . I followed your suggestion, not choosing clang as the default compiler for cuda, it worked! \r\n\r\nthis is my config log\r\n\r\n```\r\n\u279c  tensorflow-master ./configure                                      \r\nYou have bazel 0.5.1-homebrew installed.\r\nPlease specify the location of python. [Default is /Users/qinyuhang/anaconda/bin/python]: \r\nFound possible Python library paths:\r\n  /Users/qinyuhang/anaconda/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/qinyuhang/anaconda/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /Users/qinyuhang/anaconda/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] n\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] y\r\nHadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] y\r\nVERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] n\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0\r\nDo you wish to build TensorFlow with MPI support? [y/N] n\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n```\r\n\r\n", "As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X.", "GPUs are overrated anyway. ", "I have the same issue. Neither compiler works for me (TF 1.2, CUDA-8.0, cuDNN 6) ", "@ilyaivensky TF 1.2 doesn't have GPU support on Mac. ", "Since there is no supported combination of hardware from Apple and software by NVIDIA (there are currently no NVIDIA drivers that actually work reliably on a MacOS platform), we have stopped supporting native CUDA/GPU support on MacOS. It appears to us that neither old nor new drivers work reliably with new versions of MacOS.\r\n\r\nWe have tried both on old hardware (MacBook Pro with NVIDIA GPUs) and using external GPUs (a bizon box with an external GPU via thunderbolt), and have not been able to get even CUDA to work reliably. Without the ability to test and build on MacOS+GPU, we cannot support it.\r\n\r\nI'm sorry about this situation, if the driver situation improves we'd be happy to add support again. \r\n\r\n@gunan FYI.", "What about Mac Pros with modded GPUs such as [these](http://www.macvidcards.com/store/p97/Nvidia_GTX_1080_Ti_11_GB.html)?", "I think the last hardware these are supported in are Mac Pro 5.1, which are from 2010. We don't have any of those, and they are at this point too old.\r\n\r\nThe problem is not hardware, it's software -- recent versions of MacOS are not compatible with recent versions of NVIDIA drivers.", "@martinwicke I have an older compiled version of tf1.1 with GPU support (cuda 8 + cudnn 5.1) has been working quite reliably so far on a similar setup. (by reliable I mean overnight training). I've been hesitant to upgrade to 1.2 due to lack of GPU support.", "Hi, I have a Macbook Pro 2014 came with NVIDIA GT 750M, and me self compiled a Tensorflow 1.1, It works fine in both macOS 10.12 and macOS 10.13 Developer Preview. The only problem is I have to disable SIP to compile the Tensorflow pip package. \r\nAnd I upload the pip package I compiled in my Github repo:  https://github.com/qinyuhang/tensorflow-mac-build \r\nI think compile 1.2 is worth a try. Could you specify how I can try build 1.2 with GPU by myself? Great Thanks!", "@udnaan We did support it until 1.1, but we haven't been able to make it work reliably with cuda 6 at all, and even in 5.1, I believe there are some test failures (you probably won't notice these during everyday training, I don't think they affect the most common code paths). Of course, the 1.1 binaries are always available for you, it's not like we're deleting the old releases (or the code).\r\n\r\n@qinyuhang You can still try to compile 1.2 with GPU support (simply follow the instructions for an installation from source), but this thread is about the issues you will encounter if you do that.", "@martinwicke Fair enough. Thanks for the explanation. I guess as long as it works on linux + GPU, there is always a possibility to make it work on Mac + GPU with some tinkering.", "As announced in 1.1 we will drop official support for mac-gpu setups.\r\nThe reasons are as @martinwicke described.\r\nHowever, the configuration files and all the necessary code is still in there. There were some test failures the last time we ran the tests, but it should not effect most training tasks.\r\nIf you run into any issues, the contributions are most welcome, and we will consider any PRs fixing mac gpu builds.", "Hey all, \r\nThat's an unfortunate but understandable decision to remove support GPU for macOS in 1.2.\r\n\r\nIf we wanted to try to improve the CUDA driver situation in macOS, who should we go to for support? Is the problem an issue with NVidia's drivers or Apple's OS?\r\n\r\nI presume if there are thousands of tensorflow developers who use macs, all asking either NVidia or Apple to improve the situation, then something might be done about it.", "We believe that this is a driver problem, but the symptoms are not particularly clear cut (look much like random failures) and therefore you may have trouble getting traction getting things fixed.", "I managed to compile and successfully run TensorFlow 1.2 on macOS with GPU support. This thread has been useful in resolving it, thanks guys!\r\n\r\nI wrote a little tutorial on it, as a \"note to self\" for later, but maybe it's also useful to someone else. https://medium.com/@mattias.arro/installing-tensorflow-1-2-from-sources-with-gpu-support-on-macos-4f2c5cab8186", "Hi everyone, just wanted to thank again @mattiasarro for his tuto and give another sign of life for TensorFlow with GPU on mac", "Thanks @mattiasarro! I've updated that work a bit in my own tutorial covering TensorFlow 1.3.0, CUDA 8.0 and cuDNN 6.0 \u2013 hopefully it'll be of use to others as well.\r\n\r\nhttps://metakermit.com/2017/compiling-tensorflow-with-gpu-support-on-a-macbook-pro/\r\n\r\nBTW, here's hoping you reintroduce the tensorflow-gpu package! \ud83d\ude42 "]}, {"number": 9071, "title": "ImportError: libnvidia-fatbinaryloader.so.375.51: cannot open shared object file", "body": "- OS: Ubuntu 17.04\r\n- *TensorFlow installed from: source\r\n- *TensorFlow version*: 1.1.0rc1\r\n- *Bazel version (if compiling from source)*: 0.4.5\r\n- *CUDA/cuDNN version*: 8.0/5.1\r\n- *GPU Model and Memory*: NVIDIA GTX 1060, 6 GB RAM\r\n- *Exact command to reproduce*:\r\n\r\n```python\r\nimport tensorflow as tf\r\n```\r\n\r\n### Describe the problem clearly\r\nI installed Tensorflow from source in order to have all CPU instructions supported.\r\n\r\n```\r\n(tensorflow) stefano@stefano-PC:~$ python3\r\nPython 3.5.3 (default, Jan 19 2017, 14:11:04) \r\n[GCC 6.3.0 20170118] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/stefano/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/stefano/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libnvidia-fatbinaryloader.so.375.51: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/stefano/tensorflow/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/stefano/tensorflow/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libnvidia-fatbinaryloader.so.375.51: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n```\r\n\r\nMy bashrc:\r\n```\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\nexport LIBRARY_PATH=${LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\n```\r\n\r\nProblem is, that Tensorflow tries to load libnvidia-fatbinaryloader.so.375.51, but it exists only libnvidia-fatbinaryloader.so.375.38. I'm puzzled why TF tries to load a newer version, because right now, version 375.38 is the latest driver available!", "comments": ["~~Remark: I don't have this error with TF 1.0.1~~", "Oh, this is not true. It happens in 1.0.1, but TF doesn't abbort.\r\n\r\n(tensorflow) stefano@stefano-PC:~$ python3\r\nPython 3.5.3 (default, Jan 19 2017, 14:11:04) \r\n[GCC 6.3.0 20170118] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: :/usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: stefano-PC\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017\r\nGCC version:  gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.39.0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1065] LD_LIBRARY_PATH: :/usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: **libnvidia-fatbinaryloader.so.375.51: cannot open shared object file: No such file or directory**\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n", "This happens also, when I install TF via external binaries per pip3.", "I have similar issues when installing tensorflow from source.\r\nOS: Ubuntu 14.04.\r\nbazel: 0.4.5\r\nCUDA: 8.0\r\n\r\n`Python 2.7.6 (default, Oct 26 2016, 20:30:19)\r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libnvidia-fatbinaryloader.so.375.51: cannot open shared object file: No such file or directory\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n`", "This happens when I installed tensorflow via pip.\r\n\r\n`Python 2.7.6 (default, Oct 26 2016, 20:30:19)\r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH:\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: dlibgpu\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.26  Thu Dec  8 18:36:43 PST 2016\r\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3)\r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.26.0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1065] LD_LIBRARY_PATH:\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libnvidia-fatbinaryloader.so.375.51: cannot open shared object file: No such file or directory\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n>>>\r\n`\r\n\r\nI have nvidia driver of version: 375.26. However, following facts confused me a lot. \r\n\r\n- `find /usr/ -name 'libcuda.so.1'` it gives:\r\n/usr/lib/i386-linux-gnu/libcuda.so.1\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n\r\n- `ll /usr/lib/x86_64-linux-gnu/libcuda.so*` it gives:\r\nlrwxrwxrwx 1 root root      12  Mar 23 00:54 /usr/lib/x86_64-linux-gnu/libcuda.so -> libcuda.so.1\r\nlrwxrwxrwx 1 root root      17  Apr 10 12:23 /usr/lib/x86_64-linux-gnu/libcuda.so.1 -> libcuda.so.375.51\r\n-rw-r--r-- 1 root root 8296456  Mar 23 00:08 /usr/lib/x86_64-linux-gnu/libcuda.so.375.51\r\n\r\n- `find /usr/ 'libcuda.so.375.26'` gives nothing, means that there is no libcuda.so.375.26 in my system.\r\n\r\n- 'find /usr/ -name '*.375.51'' gives:\r\n/usr/lib/libnvidia-gtk2.so.375.51\r\n/usr/lib/i386-linux-gnu/libcuda.so.375.51\r\n/usr/lib/i386-linux-gnu/libnvidia-opencl.so.375.51\r\n/usr/lib/libnvidia-gtk3.so.375.51\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.375.51\r\n/usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.375.51\r\nSo there is no libnvidia-fatbinaryloader.so.375.51 on my machine. \r\n\r\nI want to know why my nvidia driver version is 375.26, but it is libcuda.so.375.51 in my /usr/lib/x86_64-linux-gnu/, instead of libcuda.so.375.26. How can I fix this? \r\nThanks.", "I tried to find workaround, but this doesn't work:\r\n\r\n```\r\nsudo ln -s /usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.39 /usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.51\r\n```\r\n```\r\nstefano@stefano-PC:~/Dokumente/Programming/Python/TensorflowCoreTutorial$ /home/stefano/tensorflow/bin/python3 /home/stefano/Dokumente/Programming/Python/TensorflowCoreTutorial/src/mnist_beginners.py\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/train-images-idx3-ubyte.gz\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/train-labels-idx1-ubyte.gz\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/t10k-images-idx3-ubyte.gz\r\nExtracting /home/stefano/Dokumente/Programming/Python/MNIST/t10k-labels-idx1-ubyte.gz\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: stefano-PC\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: stefano-PC\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 375.51.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.39  Tue Jan 31 20:47:00 PST 2017\r\nGCC version:  gcc version 4.9.4 (Ubuntu 4.9.4-2ubuntu1)\r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.39.0\r\nE tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version 375.39.0 does not match DSO version 375.51.0 -- cannot find working devices in this configuration\r\n0.9182\r\n```", "I downgraded to Ubuntu 16.04 and it seems to work. But I only tested TF version 1.0.1.\r\n@Songweiping Do you still have the issue?", "I'm glad to hear it's working. Our support matrix for Ubuntu Linux only includes versions 14 LTE to 16 LTE. Thank you for bringing the compatibility issues with 17 to the attention of the community.", "I think, this shouldn't be taken too lightly, because in august, Ubuntu 16.04.3 will get the Mesa stack and the kernel from 17.04.", "I downgraded nvidia driver to 375.26 on my machine and it works for both TF 1.0.1 and TF 1.1.0, the latter was built from source.\r\n @StefanoD @jart I suspect that this issue was caused by upgrading bazel tools. When I updated my bazel, it run into an error:\r\n`E: Failed to fetch http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/./nvidia-375_375.51-0ubuntu1_amd64.deb  Size mismatch`\r\n\r\nThe system can not find libcuda.so.375.26, because there is libcuda.so.375.51 in /usr/lib/x86_64-linux-gnu/ instead. When downgrading libcuda.so.375.51 to libcuda.so.375.26, it works well.\r\n\r\nWish this will help.", "Please note [Nvidia CUDA](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/) only lists support for Ubuntu 14.04 and 16.04.\r\n\r\n@martinwicke @gunan Users are reporting CUDA issues on Ubuntu 17. Our friend @StefanoD believes these issues will trickle down into 16.04 come August when 16.04.3 is released. Should this issue be reopened?", "@jart Actually @Songweiping is using Ubuntu 14.04 as he said in his first post.", "Can we do anything about this? We already specify CUDA library paths, do our heuristics need updating to deal with this?", "I remember that I couldn't update to NVIDIA 375.51, because of some xorg-video-abi incompatibilities. I'm not sure if Ubuntu 16.04.3 will update to the same xorg version as 17.04.", "@meteorcloudy has done work on CUDA heuristics in the past. Perhaps he could take a look into this one?", "I'm having the same issues on Ubuntu 16.04.2LTS on a dell xps15 9560 with gtx 1050 4gb. I have the same paths set up as @StefanoD , nvidia-375.51 installed, CUDA-8.0 installed + CUDANN. \r\n\r\nInstead I'm running into ```ImportError: libnvidia-fatbinaryloader.so.381.09: cannot open shared object file: No such file or directory```. Why does it ask for 381 and not 375?\r\n\r\nLATER EDIT: Symbolic linking fatbinaryloader.so.375.51 to fatbinaryloader.so.381.09 did the trick for me.\r\n", "As a workaround, I would recommend to downgrade to nvidia-375.39. This works for me on Ubuntu 16.04.2.", "Hi @StefanoD ,\r\n\r\nHow do i do that ? Could you please guide me ? \r\nAlso, I compiled tensorflow samples and deviceQuery does not find any cuda-enabled device...\r\n\r\nThanks a bunch, \r\nDaniel.", "If you go to the NVIDIA website, you can download the driver version 375.39\r\n\r\nhttp://www.nvidia.de/Download/index.aspx", "Just because I'm at the same place at the same time as you guys, I'll comment something non-obvious I found. Many hours were sacrificed for this knowledge.\r\n\r\nYou might think cuda can either be installed with a .deb file or a .run file. HOWEVER, if you want to use the slightly older 375.39, you need to use the run file. This annoyed me because I think the rule of thumb is not to mix run files and deb files with drivers.\r\n\r\nThe reason is that the run file will prompt you whether you want to upgrade the driver during installation. Whereas during the procedure: dpkg -i <cuda.deb>&& apt-get update && apt-get install cuda, there is no choice in the matter. Upgrading to 375.51 is forced. You can try to insist not to upgrade the driver with apt-get --nodeps or dpkg --ignore-dep=nvidia-375, but then apt-get will complain of broken dependencies.\r\n\r\nI am currently exploring alternatives. My system is ubuntu16.04, 2*TitanX (pascal).", "Something is really wrong, because on Ubuntu 17.04, I had 375.39 installed and Tensorflow looked for 375.51:\r\n\r\n```\r\nImportError: libnvidia-fatbinaryloader.so.375.51: cannot open shared object file: No such file or directory\r\n```\r\nProblem was, I couldn't update to 375.51, because of dependency errors (see above).\r\nOn Ubuntu 16.04.2 (respective KDE Neon, which uses Ubuntu 16.04) I installed via the deb package and I have 375.39 installed. \u2013 No problems at all!", "I got the same error on Ubuntu 16.04.2LTS with GTX1070 GPU driver 375.51. Googled on to this page and realized that the Original poster StefanoD  has the path  /usr/lib/nvidia-375 in LD_LIBRARY_PATH. \r\nIssue fixed after carefully modifying LD_LIBRARY_PATH.\r\n", "@madhupkumar \r\nI'm looking at my LD_LIBRARY_PATH. and it looks the same as StefanoD. What is your modification of your library path that got you working? \r\n\r\nWorking with a recently updated Driver 375.66\r\n\r\nThu May 11 16:59:37 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 750 Ti  Off  | 0000:04:00.0      On |                  N/A |\r\n| 42%   31C    P8     1W /  52W |    388MiB /  2000MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1299    G   /usr/bin/X                                     150MiB |\r\n|    0      2778    G   /usr/bin/gnome-shell                           116MiB |\r\n|    0      3502    G   ...el-token=1375CC660F0119C776F066266D71C69D   119MiB |\r\n+-----------------------------------------------------------------------------+\r\n", "I added /usr/lib/nvidia-375 to my LD_LIBRARY_PATH and got rid of this import error @danindiana ", "@madhupkumar \r\n\r\n17:27:32 CDT UPDATE: OK. After installing cuDNN and exporting the LD_LIBRARY_PATH I was able to get it working. Thanks for your help. \r\n\r\nOops. OK. It appears I didn't install libcudnn5 so let me see if that works and eliminates the error. Google's Tensorflow install instructions don't mention that unfortunately. \r\n\r\n\r\n\r\nCan you be a little more specific? I seem to have added that to my LD_LIBRARY_PATH as well but am still getting an error. Thanks for the help. \r\n\r\necho $LD_LIBRARY_PATH\r\n/home/speed/torch/install/lib:/home/speed/torch/install/lib:/home/speed/torch/install/lib:/home/speed/torch/install/lib::/usr/lib/nvidia-375\r\n\r\nThis is what my bashrc has in it: \r\n\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\nexport LIBRARY_PATH=${LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\n\r\nPerhaps there is a conflict with minconda3 linking? \r\n\r\n# added by Miniconda3 3.10.1 installer\r\nexport PATH=\"/home/speed/miniconda3/bin:$PATH\"\r\nsource /home/speed/perl5/perlbrew/etc/bashrc\r\nexport PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:   /usr/local/games\"\r\nexport PATH=\"$PATH:$HOME/bin\"\r\n\r\n\r\nexport NVM_DIR=\"/home/speed/.nvm\"\r\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm\r\n\r\n\r\n. /home/speed/torch/install/bin/torch-activate\r\n\r\n\r\n. /home/speed/torch/install/bin/torch-activate\r\n\r\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/lib/nvidia-375\r\nexport LIBRARY_PATH=${LIBRARY_PATH}:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/nvidia-375\r\n", "@StefanoD Looks like this issue was resolved with a workaround?\r\nI will close this issue, please reopen if you think there is more to investigate here.", "What workaround? I don't see one. I had exported all relevant path variables. See my first post.\r\nAnd btw: A workaround means, that users still will encounter this issue and will maybe find by luck this issue.", "I meant you were able to downgrade your driver and get TF working.\r\nI have reopened the issue.\r\nWill try to find some time to look into this.", "Could you share the output of this command, if it is OK?\r\n`ls -l /usr/lib/nvidia-375`", "Hi,\r\nI didn't downgraded the driver, but the OS from 17.04 to 16-04 and then it worked.\r\n@Songweiping used Ubuntu 14.04 LTS and had the same issue.\r\n\r\nIt seems TF can't handle different point releases of the driver.\r\n\r\nI still have the same driver version on 16.04 (which strangely works):\r\n\r\n```\r\nstefano@stefano-linux:~$ ls -l /usr/lib/nvidia-375\r\ninsgesamt 120540\r\n-rw-r--r-- 1 root root        0 M\u00e4r 17 11:30 alt_ld.so.conf\r\ndrwxr-xr-x 2 root root     4096 Apr 13 18:23 bin\r\n-rw-r--r-- 1 root root       42 M\u00e4r 17 11:30 ld.so.conf\r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:31 libEGL_nvidia.so.0 -> libEGL_nvidia.so.375.39\r\n-rw-r--r-- 1 root root  1079216 Feb  1 06:06 libEGL_nvidia.so.375.39\r\nlrwxrwxrwx 1 root root       11 M\u00e4r 17 11:31 libEGL.so -> libEGL.so.1\r\n-rw-r--r-- 1 root root    77264 Feb  1 04:35 libEGL.so.1\r\n-rw-r--r-- 1 root root    20608 Feb  1 04:35 libEGL.so.375.39\r\n-rw-r--r-- 1 root root   822584 Feb  1 04:35 libGLdispatch.so.0\r\nlrwxrwxrwx 1 root root       29 M\u00e4r 17 11:31 libGLESv1_CM_nvidia.so.1 -> libGLESv1_CM_nvidia.so.375.39\r\n-rw-r--r-- 1 root root    51528 Feb  1 06:06 libGLESv1_CM_nvidia.so.375.39\r\nlrwxrwxrwx 1 root root       17 M\u00e4r 17 11:31 libGLESv1_CM.so -> libGLESv1_CM.so.1\r\n-rw-r--r-- 1 root root    39048 Feb  1 04:35 libGLESv1_CM.so.1\r\nlrwxrwxrwx 1 root root       26 M\u00e4r 17 11:31 libGLESv2_nvidia.so.2 -> libGLESv2_nvidia.so.375.39\r\n-rw-r--r-- 1 root root    81744 Feb  1 06:06 libGLESv2_nvidia.so.375.39\r\nlrwxrwxrwx 1 root root       14 M\u00e4r 17 11:31 libGLESv2.so -> libGLESv2.so.2\r\n-rw-r--r-- 1 root root    51848 Feb  1 04:35 libGLESv2.so.2\r\nlrwxrwxrwx 1 root root       10 M\u00e4r 17 11:31 libGL.so -> libGL.so.1\r\nlrwxrwxrwx 1 root root       14 M\u00e4r 17 11:31 libGL.so.1 -> libGL.so.1.0.0\r\n-rw-r--r-- 1 root root   583912 Feb  1 04:35 libGL.so.1.0.0\r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:31 libGLX_indirect.so.0 -> libGLX_nvidia.so.375.39\r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:31 libGLX_nvidia.so.0 -> libGLX_nvidia.so.375.39\r\n-rw-r--r-- 1 root root  1278328 Feb  1 04:36 libGLX_nvidia.so.375.39\r\nlrwxrwxrwx 1 root root       11 M\u00e4r 17 11:31 libGLX.so -> libGLX.so.0\r\n-rw-r--r-- 1 root root    64688 Feb  1 04:35 libGLX.so.0\r\nlrwxrwxrwx 1 root root       15 M\u00e4r 17 11:31 libnvcuvid.so -> libnvcuvid.so.1\r\nlrwxrwxrwx 1 root root       20 M\u00e4r 17 11:31 libnvcuvid.so.1 -> libnvcuvid.so.375.39\r\n-rw-r--r-- 1 root root  2277072 Feb  1 05:50 libnvcuvid.so.375.39\r\nlrwxrwxrwx 1 root root       18 M\u00e4r 17 11:31 libnvidia-cfg.so -> libnvidia-cfg.so.1\r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:31 libnvidia-cfg.so.1 -> libnvidia-cfg.so.375.39\r\n-rw-r--r-- 1 root root   157896 Feb  1 05:47 libnvidia-cfg.so.375.39\r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:31 libnvidia-compiler.so -> libnvidia-compiler.so.1\r\nlrwxrwxrwx 1 root root       28 M\u00e4r 17 11:31 libnvidia-compiler.so.1 -> libnvidia-compiler.so.375.39\r\n-rw-r--r-- 1 root root 48276880 Feb  1 06:08 libnvidia-compiler.so.375.39\r\n-rw-r--r-- 1 root root 27269200 Feb  1 06:06 libnvidia-eglcore.so.375.39\r\n-rw-r--r-- 1 root root    29872 Feb  1 06:06 libnvidia-egl-wayland.so.375.39\r\nlrwxrwxrwx 1 root root       21 M\u00e4r 17 11:30 libnvidia-encode.so -> libnvidia-encode.so.1\r\nlrwxrwxrwx 1 root root       26 M\u00e4r 17 11:30 libnvidia-encode.so.1 -> libnvidia-encode.so.375.39\r\n-rw-r--r-- 1 root root   170192 Feb  1 05:50 libnvidia-encode.so.375.39                                                                                                                                 \r\n-rw-r--r-- 1 root root   308296 Feb  1 04:50 libnvidia-fatbinaryloader.so.375.39                                                                                                                        \r\nlrwxrwxrwx 1 root root       18 M\u00e4r 17 11:30 libnvidia-fbc.so -> libnvidia-fbc.so.1                                                                                                                     \r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:30 libnvidia-fbc.so.1 -> libnvidia-fbc.so.375.39                                                                                                              \r\n-rw-r--r-- 1 root root    94440 Feb  1 04:41 libnvidia-fbc.so.375.39                                                                                                                                    \r\n-rw-r--r-- 1 root root 29077624 Feb  1 04:35 libnvidia-glcore.so.375.39                                                                                                                                 \r\n-rw-r--r-- 1 root root   488800 Feb  1 05:51 libnvidia-glsi.so.375.39                                                                                                                                   \r\nlrwxrwxrwx 1 root root       18 M\u00e4r 17 11:30 libnvidia-ifr.so -> libnvidia-ifr.so.1                                                                                                                     \r\nlrwxrwxrwx 1 root root       23 M\u00e4r 17 11:30 libnvidia-ifr.so.1 -> libnvidia-ifr.so.375.39                                                                                                              \r\n-rw-r--r-- 1 root root   207880 Feb  1 06:06 libnvidia-ifr.so.375.39                                                                                                                                    \r\nlrwxrwxrwx 1 root root       17 M\u00e4r 17 11:31 libnvidia-ml.so -> libnvidia-ml.so.1\r\nlrwxrwxrwx 1 root root       22 M\u00e4r 17 11:31 libnvidia-ml.so.1 -> libnvidia-ml.so.375.39\r\n-rw-r--r-- 1 root root  1197664 Feb  1 05:48 libnvidia-ml.so.375.39\r\n-rw-r--r-- 1 root root  9158056 Feb  1 04:55 libnvidia-ptxjitcompiler.so.375.39\r\n-rw-r--r-- 1 root root    13080 Feb  1 04:40 libnvidia-tls.so.375.39\r\nlrwxrwxrwx 1 root root       23 Apr 13 18:23 libnvidia-wfb.so.1 -> libnvidia-wfb.so.375.39\r\n-rw-r--r-- 1 root root   295416 Dez  7  2012 libnvidia-wfb.so.375.39\r\nlrwxrwxrwx 1 root root       14 M\u00e4r 17 11:31 libOpenGL.so -> libOpenGL.so.0\r\n-rw-r--r-- 1 root root   184264 Feb  1 04:35 libOpenGL.so.0\r\ndrwxr-xr-x 2 root root     4096 Apr 13 18:23 tls\r\ndrwxr-xr-x 2 root root     4096 Apr 13 18:23 vdpau\r\ndrwxr-xr-x 2 root root     4096 Apr 13 18:23 xorg\r\n```\r\n", "Had the same `import` issue. Before you do the `import` I suggest to check if all these apply:\r\n\r\n- [x] OS: `Ubuntu 16.04 LTS`\r\n- [x] Driver: `nvidia-375`, `Secure Boot` should be off.\r\n- [x] cuDNN: `cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0`\r\n- [x] I recommend building TensorFlow from source using bazel and then `pip wheel`.\r\n- [x] I also don't recommend `Anaconda` for TensorFlow.\r\n- [x] `.bashrc` or `zshconfig` : \r\n```\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/lib/nvidia-375:/usr/local/cuda/extras/CUPTI/lib64\"\r\nexport CUDA_HOME=/usr/local/cuda\r\n```\r\n\r\nAll the best. \r\n \r\n", "@sudoankit Environment variables solved the problem for me on ubuntu 16.04\r\n\r\nnvidia-375.66\r\nTF 1.1.0", "I fixed the error by using a symbolic link.\r\n\r\nln -s libnvidia-fatbinaryloader.so.375.66 libnvidia-fatbinaryloader.so.375.51", "hope this help someone pulling out from the mud:\r\n\r\n\t$> ldd ./1_Utilities/deviceQueryDrv/deviceQueryDrv\r\n\r\n\tlinux-vdso.so.1 =>  (0x00007ffdedbe3000)\r\n\r\n\tlibcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007fd94f3d9000)\r\n\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fd94f1d1000)\r\n\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fd94efb3000)\r\n\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fd94edaf000)\r\n\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd94e9e8000)\r\n\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fd94e6dd000)\r\n\r\n\t/lib64/ld-linux-x86-64.so.2 (0x0000563cdb7fb000)\r\n\r\n\tlibnvidia-fatbinaryloader.so.375.66 => not found\r\nas you can see the last line says \"not found\".\r\nBy simply \r\n\r\n\t$> find /usr -name libnvidia-fatbinaryloader.so.375.66\r\n\r\n\t/usr/lib/nvidia-375/libnvidia-fatbinaryloader.so.375.66\r\n\r\n\r\nand\r\n \r\n\tLD_LIBRARY_PATH=/usr/lib/nvidia-375:$LD_LIBRARY_PATH\r\n\r\nI solved the issue.", "Solved a similar issue by installing cuda-toolkit with the local file option for ubuntu 16.04. .deb download/install was not working for me.", "@madhupkumar 's solution worked for me: add /usr/lib/nvidia-375 to my LD_LIBRARY_PATH :\r\n\r\nI added\r\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64:/usr/lib/nvidia-375\r\n\r\nto my ~/.bashrc, then do `source bashrc` if you want to load tensorflow in an ipython sesh in the same terminal", "Best solution is to prepend new paths to old LD_LIBRARY_PATH in ~/.bashrc\nas following:\n\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:/usr/lib/nvidia-375:$LD_LIBRARY_PATH\n", "nvidia drivers that you have installed through cuda-toolkit might be different with the one used in your laptop. you can check this by running 'nvidia-smi' and check whether it is the same with the one shown in \"Additional Drivers\" tab of \"Software & Updates\". This is probably the reason you are getting this error.", "The problem above is not related to \"export\" because he already have .375.38 but TF asked another version. In my case, i have 375.66 but TF asked 375.74. Now, i solved @royshan 's advice. it seems worked.", "@stefanofiorentino for driver version 384, I no longer see this path.. did I not complete installation properly??  I'm almost in a stable setup but CUDA stopped working after a reboot and recompilation of tensorflow cannot find my gpu any longer.. for some reason the pip install was working great with cuda\r\n\r\nOutput when it was working was as follows, pip3 installing in ubuntu 16.04\r\n\r\n![proof_speedup_worked](https://user-images.githubusercontent.com/3691722/30113663-ba0758e0-9315-11e7-8564-659957d3a137.png)\r\n\r\nUpdate : I had removed the modprobe black last given by the 384 installation.. reinstalling with the `--no-opengl-files` flag for the `.run` installer worked like magic.. everything is running again.\r\n\r\nAs noted, 384 does not have this lib folder anymore, so I just have `LD_LIBRARY_PATH=/usr/local/cuda/lib64`\r\n", "Not an expert, but I thought these lines ending with\n\"DMA: 0\"\n\"      0:  Y\"\n\nmeant that it did detect 1 gpu.\n\nI use the top answer to this question\n<https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow>\nwhen\nI'm unsure if gpus are detected.\n\n-Chris\n\n\nOn Wed, Sep 6, 2017 at 8:15 AM, Erik <notifications@github.com> wrote:\n\n> @stefanofiorentino <https://github.com/stefanofiorentino> for driver\n> version 384, I no longer see this path.. did I not complete installation\n> properly?? I'm almost in a stable setup but CUDA stopped working after a\n> reboot and recompilation of tensorflow cannot find my gpu any longer.. for\n> some reason the pip install was working great with cuda\n>\n> [image: proof_speedup_worked]\n> <https://user-images.githubusercontent.com/3691722/30113663-ba0758e0-9315-11e7-8564-659957d3a137.png>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9071#issuecomment-327478549>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKOtSc5jhdQawC56tAbiqbOfZRG2X4ZZks5sfprkgaJpZM4M31fT>\n> .\n>\n", "@EMCP please let me know if you resolve the issue going with\r\n\r\n`find / -iname nvidia-* -type d 2>&1 | grep -v ^find`\r\n\r\nYou should find what is your directory to add to LD_LIBRARY_PATH.", "You might need to add CUDA_HOME as well. My ~/.bashrc is as following:\r\n\r\nexport PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/local/lib\"\r\nexport CUDA_HOME=/usr/local/cuda", "@stefanofiorentino actually I had removed the nvidia provided blacklist file.. which broke it.  As soon as I re installed the driver 384 with the flag `--no-opengl-files` my setup is back to working again, and that path does not exist in this version of the driver at all.. seems they put the lib files elsewhere now\r\n\r\n```\r\n$ find / -name 'libnvidia-*' 2>/dev/null\r\n/usr/local/cuda-8.0/lib64/stubs/libnvidia-ml.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ifr.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-fbc.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ifr.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-encode.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-encode.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-gtk3.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-compiler.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-cfg.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-fbc.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ifr.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-encode.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-gtk2.so.384.69\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ml.so\r\n/usr/lib/x86_64-linux-gnu/libnvidia-fbc.so.1\r\n/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.384.69\r\n/usr/lib/xorg/modules/libnvidia-wfb.so.384.69\r\n/usr/lib/xorg/modules/libnvidia-wfb.so.1\r\n$ \r\n\r\n```\r\nadditionally, all I needed was \r\n```\r\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:\r\n```\r\nonly `.run` files were used for this machine", "@EMCP last thing to check is going with:\r\n\r\n`find / -name 'libnvidia-*' 2>/dev/null | xargs ldd | grep \"not found\"`\r\n\r\nthat lets you know if reinstalling/upgrading the driver version left something broken (decina's 10-tin-underpants principle)", "Adding following line in bashrc worked for me \r\nexport LD_LIBRARY_PATH=$CUDA_HOME/lib64:/usr/lib/nvidia-375:$LD_LIBRARY_PATH\r\nDetails : \r\nubuntu 16.04.3 LTS , GPU : NVIDIA GTX 1050Ti \r\ndownloaded .deb file & followed dpkg method for cuda toolkit 8.0 installation \r\nCuda compilation tools, release 8.0, V8.0.61\r\ntensorflow-gpu 1.3.0 (via pip)\r\n\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "https://stackoverflow.com/questions/42678439/importerror-libnvidia-fatbinaryloader-so-375-39-cannot-open-shared-object-file", "In my new eGPU setup with Ubuntu 17.10 with TF 1.6 (pip) with a GTX1070Ti and nVidia driver 387.34, I ran into the same error just some minutes ago. Even that it was working fine yesterday. I could simply solve it with this command:\r\n\r\n```\r\n$ prime-select nvidia\r\n```\r\nAs far as I know, this is something like manual nVidia Optimus, where I have to select the external GPU. I have to run this command every time before I can use the eGPU. Maybe this is just an eGPU specific solution, but at least it worked in my case and I wanted to share this approach.", "I have nvidia-390 and this work around with LD_LIBRARY_PATH has helped me fix the import error problem, but I still have another issue: When I run `nvidia-smi`, I get `zsh: command not found: nvidia-smi` but I have all the drivers installed and tensorflow running fine. any ideas?", "I had the same error that nvidia had 384.145 version and tensorflow demanded 384.130. i have resolved it by uninstall the tensorflow and then install the tensorflow\r\npip uninstall tensorflow-gpu\r\n**sudo apt-get update\r\nsudo reboot now\r\npip install tensorflow-gpu\r\nsudo apt-get update**\r\n", "@ParmpalGill do you exactly know what each command you issued does? In particular pip and apt-get are not linked in any way.", "see that your GPU is active , this solved my problem. sometimes by default active graphics card will be intel UHD , set it to NVIDIA", "@gabrielziegler3  did you fix your issue? I am having the same problem, despite having everything working recently. ", "```\r\nImportError: libnvidia-fatbinaryloader.so.410.79: cannot open shared object file: No such file or directory\r\n```\r\n\r\nIt was a version mismatch between `nvidia-410-*` and `libcuda1-410` DEB packages.\r\n\r\n```\r\nii  nvidia-410                       410.104-0ubuntu1           amd64        NVIDIA binary driver - version 410.104\r\nii  nvidia-410-dev                   410.79-0ubuntu1            amd64        NVIDIA binary Xorg driver development files\r\nii  libcuda1-410                       410.79-0ubuntu1        amd64                  NVIDIA CUDA runtime library\r\n```\r\nAfter upgrading `libcuda1-410` and others it works.", "@bzamecnik I'm having the exact same error. Could you please elaborate how exactly did you upgrade `libcuda1-410`  and which other packages did you upgrade?\r\nI'm having hard time figuring this out for quite some time.\r\n\r\nThanks!", "Again went into the same issue. @dhruvpatel108:\r\n\r\n```\r\nsudo apt install --reinstall nvidia-418 nvidia-418-dev libcuda1-418\r\n```\r\n\r\nIn my case today:\r\n```\r\n$ dpkg -l '*nvidia*'\r\nii  nvidia-418              418.152.00-0ubun amd64\r\nii  nvidia-418-dev          418.87.01-0ubunt amd64 \r\n$ dpkg -l '*libcuda*'\r\nii  libcuda1-418            418.87.01-0ubunt\r\n```\r\n", "@bzamecnik \r\nVerify ~/.bashrc or ~/.bash_profile (or the system level /etc/profile) has the correct LD_LIBRARY_PATH.\r\n\r\n`export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}:/usr/lib/nvidia-418/\r\n`\r\n\r\nAlso check what cuda versions do you have and match against this [official compatibility table](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver).\r\n\r\n`ls -l /usr/local/ | grep cuda` \r\n\r\nTip: If /usr/local/cuda-10.1 already points to /usr/local/cuda, you should use /usr/local/cuda/lib64..... in the above path. This is to make the LD_LIBRARY_PATH move nicely with future cuda upgrades.\r\n"]}, {"number": 9070, "title": "fix example typo (too many brackets)", "body": "The example was like `json.dumps({{}})`, which fails because `{{}}` is not a valid data structure.\r\n\r\nAlso, just below this, there is `assert config.master == 'host4:2222'`, which I believe would fail since that value comes back as `'grpc://host4:2222'`, but I'm less sure about recommending this change.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9069, "title": "Does Tensorflow support learning of embeddings for output classes in multi-class classification? ", "body": "Hi,\r\n\r\nI came across this research paper released by YouTube, on how they use deep learning neural networks for recommendations. It's located here: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\r\n\r\nIn the paper, the candidate generation neural network model outputs a softmax with 256 dimensions, which acts as an \"output embedding\" of each of the 1M video classes.\r\n\r\nHow is this possible to implement in Tensorflow, for example? Isn't softmax supposed to be only 1-Dimensional. If the model outputs an \"embedding\" like this, as they say it does, how would the training data's labels be formatted as 256-dimensional? In other words, how do they compute the 256-dimensional vector for each of the videos in their training dataset?\r\n\r\nAlso, is it possible to create an output embedding layer when the labels in the training dataset are one-hot encodings of a particular class. In other words, can the output layer automatically learn embeddings for the output class?\r\n\r\nThank you so much for your time and help, guys! I have also asked this question on StackOverflow here: http://stackoverflow.com/questions/43297567/how-to-create-a-multi-dimensional-softmax-output-in-tensorflow\r\n\r\nTensorflow details: Windows, version 1.01, binary (via pip)", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9068, "title": "Building tensorflow for Windows 10", "body": "Hello All\r\nI am trying to build tensorflow for my windows machine. I tried building it from source on ubuntu 16.04, but then that wheel doesn't support on windows. \r\nMy machine is windows 10 \r\npython 3.5.2\r\n\r\nI dont know what the problem is, Can someone help me with this.", "comments": ["You will have to build from source on the Windows machine, however that is not officially supported yet. If you can, I'd suggest using the release binaries instead.\r\n\r\nSee https://www.tensorflow.org/install/ for details.", "I want tenosrflow to compile the SSE and AVX instructions set, I have to remove it in order to finish up a project of mine. Please do not close until knowing everything that is there to be known\r\n.", "I have the same problem too. Have you tried to build TF using Bazel for Windows?", "Any update on this? Google must be building those Windows binaries somehow...", "Updates on this would be awesome! Speed boosts from machine learning specific optimizations would be great! Please share if you have information...", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md", "For anyone interested in x64 CPU only windows build of tensorflow1.4 with AVX2 instructions compatible with python 3.5.3 please visit the following link, details also given on howto compile yourself\r\n[https://github.com/faisalthaheem/tensorflow-windows](https://github.com/faisalthaheem/tensorflow-windows)", "@faisalthaheem thanks a lot ! that was really helpful !", "See my [comment](https://github.com/tensorflow/tensorflow/issues/7258#issuecomment-450802749) which provides a way to build on Windows automatically."]}, {"number": 9067, "title": "Tensorflow 1.1rc/1.01 too many epochs cause error?", "body": "Software:Tensorflow 1.1rc Python3.5.27&Tensorflow 1.0.1 Python3.5.3 (Anaconda3) CUDA8.0+Cudnn5.1\r\nOS:Windows 10 X64 1607\r\nHW:i5 6400 Z170 16G 1070\r\n\r\n### Describe the problem clearly\r\nWhen I train my mlp or lenet5, if I choose a large epoch number, then it will get a bad result?\r\n\r\n### Source Code / Logs\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import  input_data\r\nimport  tensorflow as tf\r\nmnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\r\nsess=tf.InteractiveSession()\r\nin_units=784\r\nh1_units=300\r\nW1=tf.Variable(tf.truncated_normal([in_units,h1_units],stddev=0.1))\r\nb1=tf.Variable(tf.zeros([h1_units]))\r\nW2=tf.Variable(tf.zeros([h1_units,10]))\r\nb2=tf.Variable(tf.zeros([10]))\r\nx=tf.placeholder(tf.float32,[None,in_units])\r\nkeep_prob=tf.placeholder(tf.float32)\r\nhidden1=tf.nn.relu(tf.matmul(x,W1)+b1)\r\nhidden1_drop=tf.nn.dropout(hidden1,keep_prob)\r\ny=tf.nn.softmax(tf.matmul(hidden1_drop,W2)+b2)\r\ny_=tf.placeholder(tf.float32,[None,10])\r\ncross_entropy=tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y),reduction_indices=[1]))\r\ntrain_step=tf.train.AdagradOptimizer(0.03).minimize(cross_entropy)\r\ntf.global_variables_initializer().run()\r\ncorrect_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\nfor i in range(200000):\r\n    batch_xs,batch_ys=mnist.train.next_batch(500)\r\n    train_step.run({x:batch_xs,y_:batch_ys,keep_prob:0.8})\r\n    if i % 500==0:\r\n        print(i,\"--\",accuracy.eval({x:batch_xs,y_:batch_ys, keep_prob: 1.0}))\r\n\r\ncorrect_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\r\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\nprint(accuracy.eval({x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))\r\n```\r\n\r\n\r\n\r\n\r\n> C:\\Anaconda3\\envs\\TensorflowRC\\python.exe D:/DL/3/mlp.py\r\n> Extracting MNIST_data/train-images-idx3-ubyte.gz\r\n> Extracting MNIST_data/train-labels-idx1-ubyte.gz\r\n> Extracting MNIST_data/t10k-images-idx3-ubyte.gz\r\n> Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\n> 2017-04-08 18:07:28.117898: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.118165: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.118511: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.118745: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.120818: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.121070: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.121354: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.121578: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-04-08 18:07:28.511257: I c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:887] Found device 0 with properties: \r\n> name: GeForce GTX 1070\r\n> major: 6 minor: 1 memoryClockRate (GHz) 1.645\r\n> pciBusID 0000:01:00.0\r\n> Total memory: 8.00GiB\r\n> Free memory: 6.65GiB\r\n> 2017-04-08 18:07:28.511561: I c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:908] DMA: 0 \r\n> 2017-04-08 18:07:28.511694: I c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:918] 0:   Y \r\n> 2017-04-08 18:07:28.511840: I c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\n> 0 -- 0.404\r\n> 500 -- 0.914\r\n> 1000 -- 0.932\r\n> 1500 -- 0.95\r\n> 2000 -- 0.966\r\n> 2500 -- 0.944\r\n> 3000 -- 0.952\r\n> 3500 -- 0.966\r\n\r\n> \r\n> 80500 -- 1.0\r\n> 81000 -- 1.0\r\n> 81500 -- 1.0\r\n> 82000 -- 1.0\r\n> 82500 -- 0.094\r\n> 83000 -- 0.112\r\n> 83500 -- 0.106\r\n> 84000 -- 0.094\r\n> 84500 -- 0.118\r\n> 85000 -- 0.096\r\n> 85500 -- 0.088\r\n> 86000 -- 0.114\r\n> 86500 -- 0.114\r\n> 87000 -- 0.092\r\n> 87500 -- 0.114\r\n> 88000 -- 0.1\r\n> 88500 -- 0.104", "comments": ["I change the print step to 1. \r\n\r\n> 65972 -- 1.0\r\n> 65973 -- 1.0\r\n> 65974 -- 1.0\r\n> 65975 -- 1.0\r\n> 65976 -- 1.0\r\n> 65977 -- 0.098\r\n> 65978 -- 0.094\r\n> 65979 -- 0.11\r\n> 65980 -- 0.106\r\n> 65981 -- 0.088\r\n> 65982 -- 0.08\r\n\r\nThe change is suddenly occured.", "First, you should initialize `W2` just like `W1` in order to break symmetry.\r\nSecond, you should compute cross entropy with one of the built-in functions.\r\nIt is because of numerical boo boos that you might experience problems.\r\nYou might wanna try `tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)`\r\nAnd make sure to remove the final `tf.nn.softmax` as this is done internally."]}, {"number": 9066, "title": "tensorflow-gpu rc0 import tensorflow error windows 10 64bit nvidia 1080 TI", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\nNo\r\n- *TensorFlow installed from (source or binary)?*:\r\nbinary\r\n- *TensorFlow version*:\r\npip rc0 or rc1 \r\nnightly\r\n- *Bazel version (if compiling from source)*:\r\nNA\r\n- *CUDA/cuDNN version*:\r\n8.0\r\n- *GPU Model and Memory*:\r\nNvidia 1080 ti\r\n- *Exact command to reproduce*:\r\ninstall tensorflow from nightly or using tensorflow==1.1.0rc0 or 1.1.0rc1 through pip (I'm using conda to create the environment)\r\nstart python\r\nimport tensorflow\r\n\r\n### Describe the problem clearly\r\nI followed the suggestion of installing nightly due to the warnings regarding unknown OP\r\nsee #8500\r\nThe install is successful however the import tensorflow fails with the following log\r\n\r\n### Source Code / Logs\r\n```\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Feb 22 2017, 21:28:42) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\tools\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["Having the same issue, tried pretty much all of the relevant builds on jenkins, and its either the OPS error, or this one.\r\n\r\nIf you just need a version of tensorflow to work with asap, you can get the cpu version running by installing version 1.1.0rc1 and suppressing the level 2 warnings.\r\n\r\n```\r\npip3 uninstall tensorflow-gpu\r\npip3 install --upgrade tensorflow==1.1.0rc1\r\n\r\n```\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n\r\n```\r\n\r\nAnd then install the gpu version when someone replies with a proper fix.\r\n\r\n", "Same thing over here.\r\nSystem specs are identical to @udnaan's except the GPU is a GTX 1080. [This solution](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso) doesn't help either. Any ideas?", "I am facing the same issue with 840M GPU.", "Hi @udnaan, sorry you're facing issues.\r\nCould you please check if your CUDA and cuDNN environment variables are properly set and their DLLs directories are found in your `%PATH%` \r\n\r\n", "Already done.\r\n\r\n```\r\nPATH=C:\\Users\\nan\\Downloads\\ConEmu\\ConEmu\\Scripts;C:\\Users\\nan\\Downloads\\ConEmu;C:\\Users\\nan\\Downloads\\ConEmu\\ConEmu;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\libnvvp;C:\\Python36\\Scripts\\;C:\\Python36\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\ProgramData\\chocolatey\\bin;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\nodejs\\;C:\\Program Files (x86)\\Yarn\\bin;c:\\tools\\Anaconda3\\Scripts;c:\\program files\\Cmake\\bin;C:\\Program Files (x86)\\vim\\vim80;c:\\Program files\\git\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin;c:\\tools\\Anaconda3\\Scripts\r\n```\r\n\r\nThe dll files are inside the `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin` folder\r\n\r\nThe issue seems to be with the following versions: rc0 - onwards and nightlies.\r\nThe 1.0 version works but with the unknown OP warning (the ones that I've  noted in the original report)", "@udnaan After 1.0 there were changes on how TF load CUDA related DLLs, see [this](https://github.com/tensorflow/tensorflow/issues/7705#issuecomment-281448483). \r\n\r\nRegarding OpKernels logs, they can be ignored. Just checking, your cuDNN is 5.1, right? Which nightly build did you install?", "@Carmezim  I've gone though all the nightlies that are successfully built\r\nwith the same error.\r\n\r\nYes, the cudnn version is 5.1\r\n\r\nHowever since you seem confident that it has something to do with dlls,\r\nI'll go though them again and recheck everything.\r\n\r\nOn Apr 9, 2017 16:41, \"Adriano Carmezim\" <notifications@github.com> wrote:\r\n\r\n@udnaan <https://github.com/udnaan> After 1.0 there were changes on how TF\r\nload CUDA related DLLs, see this\r\n<https://github.com/tensorflow/tensorflow/issues/7705#issuecomment-281448483>\r\n.\r\n\r\nRegarding OpKernels logs, they can be ignored. Just checking, your cuDNN is\r\n5.1, right? Which nightly build did you install?\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub\r\n<https://github.com/tensorflow/tensorflow/issues/9066#issuecomment-292821822>,\r\nor mute the thread\r\n<https://github.com/notifications/unsubscribe-auth/AHj4Dtaby8dT7nECJa22AimHimb2dDhQks5ruWyXgaJpZM4M3oIF>\r\n.\r\n", "@udnaan a way for you to check if is not Cuda related is to uninstall TF and install the CPU version and if you still receive the same error is TF or Anaconda wise .", "@Carmezim thanks for the hint. I'll try that as well. Will take a couple of hours since I'm afk atm", "@Carmezim You were right. I was missing the cudnn library all together. Thanks a lot for your help.", "@udnaan you're welcome.", "@LeifRoss @AdityaPrasadMishra @ZacDiggum Did you make progress with the solution?\r\nLet me know if you're still facing issues.\r\n", "HI @mrry, do you think even affecting a very small subset of users is worth a reference on common problems about CUDA related DLLs installation when receiving this error? ", "@Carmezim Indeed, the issue has been resolved, thank you for the assistance!", "@Carmezim That might make sense... it's a little unfortunate that the new way of loading modules doesn't produce an actionable error message (whereas it used to say \"Couldn't open CUDA library cudnn64_5.dll\" and we had a link to [this Stack Overflow answer](http://stackoverflow.com/q/41007279/3574081)). Now the error message for a misconfigured CUDA and for (e.g.) a [missing Visual C++ redistributable](http://stackoverflow.com/q/40898968/3574081) are the same.\r\n\r\nI'll follow up with some people who understand the rationale for how we load modules in the latest version and see if I can get some suggestions.\r\n\r\n(In the meantime, it looks like the link that's printed on error doesn't include a link to the Windows-specific common problems, so I'll fix that first.)", "@mrry now I fully understood what is happening. That error message was useful indeed. \r\n\r\nThe Redistributable is really python.org  dist. specific? Because if it is, do you think for now that's one way to differentiate the solutions? \r\n\r\nAwesome!", "Got it! Previously I had CuDNN 6 installed. After switching to 5.1 everything works fine...", "@ZacDiggum Thank you so much! I have tried all solutions I can find but nothing except yours works! CuDNN 6 does NOT work on my environment!", "@ybsave @ZacDiggum It is defined on installation guide under only cuDNN 5.1 is supported on Windows [**Requirements to run TensorFlow with GPU support**](https://www.tensorflow.org/install/install_windows):\r\n>cuDNN v5.1. For details, see NVIDIA's documentation. Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.\r\n\r\nGlad you sorted it out.", "@Carmezim Thank you for pointing it out. I previously thought that versions no older than the ones on the webpage were OK. Then later I finally noticed that Python 3.6 and cuDNN 6 are both not compatible with Tensorflow GPU windows version. :)", "@ybsave Regarding Python 3.6 on Windows, is a working in progress, if you want you can follow it on #6999", "I've followed all above suggestions but still getting the same error on windows 8.1", "Hi @Kaushal28 could you try running this [script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c) to see what can be happening on your side? "]}, {"number": 9065, "title": "Merge pull request #1 from tensorflow/master", "body": "leatest source", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9064, "title": "Distributed tensorflow", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:Yes\r\n- *TensorFlow installed from (source or binary)?*:binary\r\n- *TensorFlow version*:1.0.0\r\n- *Bazel version (if compiling from source)*:No\r\n- *CUDA/cuDNN version*:No\r\n- *GPU Model and Memory*:No\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\nI am trying to apply distributed tensorflow and want to distribute my task on two pc.\r\npc1 ip: 192.168.43.6->>>>ps\r\npc2 ip:192.168.43.107->>>worker\r\n\r\nCode snippet:\r\n\r\ntf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"192.168.43.107:2223\"\r\n            ],\r\n    \"ps\": [\r\n        \"192.168.43.6:2222\"\r\n        \r\n    ]})\r\n\r\ncluster = tf.train.ClusterSpec({\"local\": [\"192.168.43.6:2222\",\"192.168.43.107:2223\"]})\r\nserver = tf.train.Server(cluster, job_name=\"local\", task_index=0)\r\n\r\n\r\nwith tf.device(\"/job:ps/task:0\"):\r\n    \r\n    weights = {\r\n        \r\n        'wc1': tf.Variable(tf.random_normal([5,5, 5, 1, 32])),\r\n \r\n        'wc2': tf.Variable(tf.random_normal([5,5, 5, 32, 64])),\r\n        \r\n        'wd1': tf.Variable(tf.random_normal([1216, 1024])),\r\n        \r\n        'out': tf.Variable(tf.random_normal([1024,n_classes]))\r\n    }\r\n    \r\n    biases = {\r\n        'bc1': tf.Variable(tf.random_normal([32])),\r\n        'bc2': tf.Variable(tf.random_normal([64])),\r\n        'bd1': tf.Variable(tf.random_normal([1024])),\r\n        'out': tf.Variable(tf.random_normal([n_classes]))\r\n    }\r\n\r\n\r\nwith tf.device(\"/job:worker/task:0\"):\r\n\r\n\r\n    def conv3d(x, W, b, strides=1):\r\n        x = tf.nn.conv3d(x, W, strides=[1, strides, strides,strides, 1], padding='SAME')\r\n        x = tf.nn.bias_add(x, b)\r\n        return tf.nn.relu(x)\r\n\r\n\r\n    def maxpool3d(x, k=3):\r\n        return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\r\n\r\n\r\n    # Create model\r\n    def conv_net(x, weights, biases, dropout):\r\n        x = tf.reshape(x, shape=[1,20,149,239, 1])\r\n    \r\n        conv1 = conv3d(x, weights['wc1'], biases['bc1'])\r\n        print (\"conv1\",conv1)\r\n        conv1 = maxpool3d(conv1, k=3)\r\n        print (\"max1\",conv1)\r\n        conv2 = conv3d(conv1, weights['wc2'], biases['bc2'])\r\n        print (\"conv2\",conv2)\r\n        conv2 = maxpool3d(conv2, k=3)\r\n        print (\"max2\",conv2)\r\n        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\r\n        print (\"fc1\",fc1)\r\n        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\r\n        fc1 = tf.nn.relu(fc1)\r\n        print (\"relu\",fc1)\r\n        fc1 = tf.nn.dropout(fc1, dropout)\r\n    \r\n        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\r\n        print (\"out\",out)\r\n        return out\r\n    \r\n    x = tf.placeholder(tf.float32)\r\n    y = tf.placeholder(tf.float32)\r\n    keep_prob = tf.placeholder(tf.float32) \r\n\r\n    # Construct model\r\n    pred = conv_net(x, weights, biases, keep_prob)\r\n    print (\"pred\",pred)\r\n    cost = tf.reduce_mean(tf.nn.softmax(logits=pred))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n    correct_pred = tf.equal(tf.argmax(pred, -1), tf.argmax(y, -1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n    init = tf.global_variables_initializer()\r\n\r\n    print (\"time1\",time.clock())\r\n    saver = tf.train.Saver()\r\n\r\nwith tf.Session('grpc://192.168.43.107:2222') as sess:\r\n    sess.run(init)\r\n\r\nCommand line on ps machine->> python train3d5.py --job_name=\"ps\" --task_index=0\r\n\r\nCommand line on ps machine->> python train3d5.py --job_name=\"worker\" --task_index=0\r\n\r\nPlease provide a solution how to distribute training in tensorflow.\r\nWe also using hadoop multi-cluster.\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n\r\nError on worker machine:\r\n\r\nCreateSession still working for response from worker: /job:local/replica:0/task:1\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, you might find the following useful:\r\n- [Distributed TensorFlow guide](https://www.tensorflow.org/deploy/distributed)\r\n- [Integrating TensorFlow with your ecosystem talk](https://www.youtube.com/watch?v=yALzr4A2AzY)\r\n- [github.com/tensorflow/ecosystem](https://github.com/tensorflow/ecosystem)\r\n"]}, {"number": 9063, "title": "Ops and kernels for reduce_slice_ops", "body": "This is a PR for https://github.com/tensorflow/tensorflow/issues/7662\r\n\r\nThis PR implements `reduce_slice_sum`, `reduce_slice_prod`, `reduce_slice_max`, `reduce_slice_min`.\r\n\r\nThere are also tests for these new ops.\r\n\r\nNo gradients are implemented now, but I will start working on that after this PR is merged.", "comments": ["Can one of the admins verify this patch?", "This is implementable with current ops by doing a scan followed by a gather.  Is that not sufficient for your purposes?", "I guess @zasdfgbnm has a point on the #7662. @aselle @girving what do you think?", "@drpngx Yep, @zasdfgbnm and @aselle convinced me the op is reasonable, though I switched the reviewer to Andy.", "Gentle ping @aselle ?", "I'm sorry I'll take a thorough look today.", "Before merging this, can we think about if this will ever be useful to extend:\r\n1. to higher order tensors than matrices. \r\n2. to axes that are not 0 and also if there are multiple axes?\r\nIf those things make sense and we can define them well. I'd like to make this function have a signature in the final form of that concept, so that we do not need to change the API to add that functionality.\r\n\r\nIt seems simple that we could define partial products of axis=1 right now where we sum over the columns rather than rows at least. This suggests we should have an axis parameter. ", "@tensorflow-jenkins test this please\r\n", "Response to @aselle \r\n1. Higher order tensor is actually already supported now.\r\n2. This is a great point, I think I will change the code to support this. It won't require too much work to do so.", "When implementing the reduction along any axis, I find it helpful to flat both inner and outer dims in the shape. So I create a new method `flat_inner_outer_dims` for class `Tensor` in a separate PR https://github.com/tensorflow/tensorflow/pull/9452", "Jenkins, test this please.", "@drpngx this hasn't been done (actually still a lot to do). I just merge with master in order to use merged changes I made.", "Before changing this to add support for user specified axis, I would like to add a `GetCuda3DLaunchConfig` to `tensorflow/core/util/cuda_kernel_helper.h`, in a separate PR. The newly added function will be used here.", "Sounds good.", "Can one of the admins verify this patch?", "@zasdfgbnm any update on your progress with GetCuda3DLaunchConfig?", "@rmlarsen There is some part of the existing code that makes me not feeling well.\r\nSee https://github.com/tensorflow/tensorflow/issues/9937\r\nOnce everything is clear, I think it won't take too long to get a `GetCuda3DLaunchConfig`", "@zasdfgbnm OK, thanks for the update! I'll mark this as stalled for now.", "Edit: https://github.com/tensorflow/tensorflow/pull/10068 is merged\r\n\r\n@aselle I think all my changes are done for this PR, please continue the review process. This op now allows users to specify the axis. I also added one test case to test when `axis != 0` and one test case to test when `rank > 2`. All these tests pass on my machine.\r\n\r\nThis PR relies on https://github.com/tensorflow/tensorflow/pull/10032 that has not been merged into master branch yet. I manually merge them into this branch to make this work. So you don't have to worry about changes in `shape_inference.cc` and `cuda_kernel_helper.h` because they are reviewed separately. You may not want to merge this before these two dependencies are merged.\r\n", "Edit: merged in a separate PR\r\n~~By the way, this PR include a fix in `tensor.h` related to const tensors~~", "Just to note, #8954 adds `tf.repeat`, which provides gradients for `partial_sum` (and vice-versa). See discussion at https://github.com/tensorflow/tensorflow/pull/8954#discussion_r118302537.", "@shoyer The name of `partial_sum` has been suggested to change to `reduce_slice_sum`, see https://github.com/tensorflow/tensorflow/pull/9063#discussion_r113259298\r\nThe `tf.repeat` and `tf.reduce_slice_sum` are gradients of each other if we ignore the difference of the API. But the current API of `tf.repeat` and `tf.reduce_slice_sum` makes it a little bit tricky to use directly:\r\n\r\n- In order to use `tf.reduce_slice_sum` as gradient of `tf.repeat`, there must be another op to compute the `indices` that `tf.reduce_slice_sum` takes.\r\n- In order to use `tf.repeat` as gradient of `tf.reduce_slice_sum`, there must be another op to reorder and merge some gradients, and compute the `repeats` argument that `tf.repeat` takes.", "Currently, `indices` can only be a `n*2` matrix where the first column is the starting index and the second column is the ending index. It would be nice to extend the `indices` argument of `tf.reduce_slice_***` here to support a vector input. In the case of vector input, the second column of row `i` will be treated as the first column of row  `i+1`. For example:\r\n```python\r\nindices = [0,5,10,15]\r\n``` \r\n\r\nis equivalent to\r\n\r\n```python\r\nindices = [ [0,5],\r\n            [5,10],\r\n            [10,15]]\r\n```\r\nThis extension would make `tf.reduce_slice_***` more useful. And with this extension, the gradient of `tf.repeat` would become a very simple like\r\n```python\r\ntf.reduce_slice_sum(grad, tf.scan(lambda a, x: a + x, repeats)))\r\n```\r\n\r\n@zycdragonball I think in your pull request, you can submit a `tf.repeat` without implementation of gradient. I will extend my ops as described above and you can wait until my extension is done and this PR being merged. What do you think?", "The vector indices support is done", "@zasdfgbnm I'll leave the gradient of `tf.repeat` as it is now, and change it to `tf.reduce_slice_sum` once it is merged.", "Jenkins, test this please.", "@zasdfgbnm this doesn't seem to build. Could you fix and push again?", "@drpngx It should be fixed now.", "Jenkins, test this please.\n\nOn Jun 27, 2017 4:46 AM, \"Gao, Xiang\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> It should be fixed now.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9063#issuecomment-311333707>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbQImWDEzLsY2HEtYiakCqB70dnOMks5sIOuagaJpZM4M3nG5>\n> .\n>\n", "@aselle WDYT? If OK, then we can trigger an API review, or recommend it go to contrib.", "I'd like to put these into contrib for now. The reason being that we have repeat going into contrib, and this is needed for the gradients of repeat() and repeat() is needed for gradient here. Once they are both in contrib,  the gradients should be added, and once things have stabilized we should do api review and bring them in.", "Jenkins, test this please.", "Not done yet", "done now. should be working\n\n_Sent from my OnePlus ONEPLUS A3000 using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_", "@tensorflow-jenkins test this please", "seems to be unrelated fails?", "@tensorflow-jenkins test this please", "@zasdfgbnm It looks like you need to add your op to the Cmake build. \r\nhttps://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/3768/consoleFull", "@rmlarsen I added the cmake build rules. Hopefully it will work. I'm unable to test it on my machine because I'm running into this issue: https://github.com/tensorflow/tensorflow/issues/12018", "Jenkins, test this please.", "@zasdfgbnm Thanks!", "@zasdfgbnm Thanks a lot for this nice contribution!"]}, {"number": 9062, "title": "Getting more done in GitHub with ZenHub", "body": "Hola! @yucao100 has created a [ZenHub](http://www.zenhub.com) account for the **tensorflow** organization. ZenHub is the only project management tool integrated natively in GitHub \u2013 created specifically for fast-moving, software-driven teams.\n\n----\n\n#### How do I use ZenHub?\n\nTo get set up with ZenHub, all you have to do is **[download the browser extension](https://www.zenhub.com?utm_source=ZHOnboarding)** and log in with your GitHub account. Once you do, you\u2019ll get access to ZenHub\u2019s complete feature-set immediately.\n\n#### What can ZenHub do?\n\nZenHub adds a series of enhancements directly inside the GitHub UI:\n\n- Real-time, customizable task boards for GitHub issues;\n- Multi-Repository burndown charts, estimates, and velocity tracking based on GitHub Milestones;\n- Personal to-do lists and task prioritization;\n- Time-saving shortcuts \u2013 like a quick repo switcher, a \u201cMove issue\u201d button, and much more.\n\n### [Add ZenHub to GitHub](https://www.zenhub.com?utm_source=ZHOnboarding)\n\n_Still curious? See [more ZenHub features](https://www.zenhub.com/features?utm_source=ZHOnboarding) or read [user reviews](https://chrome.google.com/webstore/detail/zenhub-for-github/ogcgkffhplmphkaahpmffcafajaocjbd/reviews). This issue was written by your friendly ZenHub bot, posted by request from @yucao100._\n\n![ZenHub Board](https://cloud.githubusercontent.com/assets/8771909/11153956/233ac4a8-89f1-11e5-94b1-1569d3f38b4d.png)\n", "comments": []}, {"number": 9061, "title": "Tensorflow Training Freezes for sometime, then continue on its own, happen multiple times when training a model", "body": "I recently upgraded to Tensorflow 0.12.1, Python3, Ubuntu 14.04, Cuda 8.0, Cudnn 5.1\r\n\r\nThen I run my old code written for Tensorflow 0.10.1, which is a simple sequence-to-sequence model. And the training keeps freeze in the middle.\r\n\r\nAfter some time the training will continue normally. Sometimes it 1 or 2 minutes, and sometimes it takes about 5 to 6 hours.\r\n\r\nAnyone has opinion on how to trouble-shoot this?", "comments": ["Are you able to try with TensorFlow 1.0? \r\n\r\nTo troubleshoot, it would help greatly if you can provide a small self-contained piece of code that reproduces the problem.", "The code worked no problem with TF 0.10.1 + Cuda 7.5 + Cudnn 4. \r\nI didn't upgrade to TF 1.0 hence cannot try it immediately.\r\n\r\nThe code that occasionally cause the freezing problem is very similar to the TF seq2seq example.\r\nhttps://github.com/TellinaTool/learning_module/blob/master/encoder_decoder/framework.py\r\n\r\nI'm not sure if it's possible to pin-point at which line it freezes.\r\n\r\nThe issue also happens quite non-deterministically. \r\n\r\n", "Unfortunately, that information isn't much to go by - there are many variables at play here - CUDA versions, NVIDIA driver versions, TensorFlow version, any hiccups in the input feeding etc.\r\n\r\nSince there isn't enough information available to help track this down, I'm going to close this issue out. If you are able to get new information (preferably with release 1.0+) that can help zone in on the problem a bit more, please do reopen this issue and/or file a new one with more information.\r\n\r\nThanks!", "I'm having the same problem.  I'm on CPUs so we can rule out CPU/GPU issues.  Also occuring with Keras 2.0.8, TF 1.3, both python 2.7 and 3.5, and also a TF 1.4-dev compiled from sources.  This leads me to believe it's an input feed issue.\r\n\r\nAnother note, the same code and inputs run fine on my Macbook.  I'm getting the problem with a new install on a workstation.   It's a VirtualBox instance running Ubuntu server 16.04.\r\n\r\nMaybe someone could suggest an input feed diagnostic?  "]}]