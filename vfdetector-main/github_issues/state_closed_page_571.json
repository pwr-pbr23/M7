[{"number": 36560, "title": "[ROCm] Fix for a test regression on the ROCm platform - 200207 - 2", "body": "The following commit introduces a test regression on the ROCm platform\r\nhttps://github.com/tensorflow/tensorflow/commit/7a931a2349591f4e2250ac2d3b6c3ca66538b740\r\n\r\nThat commit adds an explicit check for GPU device in the profiler output (if a GPU is present in the list of physical devices).\r\n\r\nSince ROCm platform does not yet support device tracing, this test now fails on the ROCm platform\r\n\r\nThe \"fix\" (until ROCm adds support for device tracing) is to disable that check on the ROCm platform\r\n\r\n\r\n--------------------------\r\n\r\n/cc @whchung @chsigg ", "comments": ["@jaingaurav can you please help merge this change internally. Thank you", "gentle ping", "gentle ping", "@jaingaurav can you please assist this PR to submit internally ?"]}, {"number": 36559, "title": "How to efficiently update a tensor slice?", "body": "Suppose we have `x = K.zeros((4, 6))`, and we wish to add 1 to row 0: `x[0] += 1`. The variable is created via `Layer`'s [`add_weight()`](https://github.com/keras-team/keras/blob/master/keras/engine/base_layer.py#L250) w/ `training=False`, so it isn't updated via backprop. What is the most _speed-efficient_ way to do so? \r\n\r\n<hr>\r\n\r\n**Context**: I'm implementing recurrent batch normalization, with `moving_mean` and `moving_variance` variables distinct for each timestep in an RNN - each thus having a shape of `(units, timesteps)`. The goal is to update one `timesteps` slice per step via `K.moving_average_update()`. One approach is as follows:\r\n\r\n```python\r\nimport tensorflow.keras.backend as K\r\nunits, timesteps = 4, 6\r\nx = K.zeros((units, timesteps), dtype='float32', name='x')\r\n\r\nx_new = x[:units, 0].assign(K.ones((units,), dtype='float32'))  # dummy example\r\nK.set_value(x, K.get_value(x_new))\r\nprint(K.get_value(x))\r\n```\r\n```python\r\n[[1. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0.]\r\n [1. 0. 0. 0. 0. 0.]]\r\n```\r\nLooks good - except, a _new copy_ of `x` was created. In practice, we can have `timesteps > 100` (e.g. 120), so we are creating an array 120x larger than it needs to be, 120 times (1 / step), making it an `O(timesteps**2)` operation - as opposed to usual slicing, `O(timesteps)`.\r\n\r\nIs there anything more efficient? Doesn't have to be `keras`, just at least `tf.keras`-friendly.", "comments": ["**Note**: the Variable is to be updated like BN's [`add_update()`](https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py#L197), as, correct me if I'm wrong, it's more efficient - but it's done _iteratively_, via [`K.rnn`](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L3964)'s repeated calls to LSTM's [`call()`](https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L1970). This complicates an efficient workaround: \"append to list (or fill array) then assign via array at last step\". \r\n\r\n**A workaround**:\r\n\r\n```python\r\ndef call(self, inputs, states, training=None):  # modifying LSTMCell's call()\r\n    # ...\r\n    _, mean, variance = K.normalize_batch_in_training(#...)\r\n    _slice = ([0, self.step], [self.units, 1])\r\n    mmean_slice     = K.variable(K.slice(self.moving_mean, *_slice))\r\n    mvariance_slice = K.variable(K.slice(self.moving_variance, *_slice))\r\n\r\n    self.moving_mean_cache.append(\r\n        K.moving_average_update(mmean_slice, mean, self.momentum))\r\n    self.moving_variance_cache.append(\r\n        K.moving_average_update(mvariance_slice, variance, self.momentum))\r\n\r\n    if self.step == 99:  # assume input_shape = (?, 100, ?)\r\n        self.update_moving_averages(inputs)\r\n    self.add_update(K.update_add(self.step, 1))\r\n\r\ndef update_moving_averages(self, inputs):\r\n    self.add_update([K.concatenate(self.moving_mean_cache, axis=1),\r\n                     K.concatenate(self.moving_variance_cache, axis=1)],\r\n                    inputs)\r\n    # empty cache\r\n    self.moving_mean_cache = []\r\n    self.moving_variance_cache = []\r\n```\r\n\r\nMy concern is the efficiency of `K.variable(K.slice(` - looks like a new Tensor is being added to graph at each iteration, which may interfere with optimization, and can result in a memory leak over many iterations (e.g. 1000 iters, 100 timesteps --> **200,000** Variable tensors added to the graph - they are [not destroyed](https://stackoverflow.com/questions/58888267/what-happens-when-keeping-assigning-a-variable-with-different-constant-tensors-i/58888716#58888716) until the Graph is reset. ... or _are_ they? For one, \"update\" tensors are also regularly created, so _some_ are garbage-collected). \r\n\r\nTo avoid this, we can make `timesteps` separate `K.variable`s in a list, and `self.add_update()` these directly - but I haven't seen anything like this in any of Keras' layers, so maybe there's a reason against it.", "For quick update of a variable, eg adding value to a slide of it, I think you can use variable.assign_add()\r\n\r\neg\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(range(10), shape=(10,), name=\"a\")\r\nprint(a)\r\nfor i in range(10):\r\n  add = [0] * 10\r\n  add[i] = add[i] + 1\r\n  a.assign_add(add)\r\n  print(a)\r\n\r\n### output of the print.\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 4, 5, 5, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 6, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 7, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 8, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9, 9], dtype=int32)>\r\n<tf.Variable 'a:0' shape=(10,) dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)>\r\n```", "@qlzh727 I suppose I could create a \"cache array\" (or list) to this end, and then \"flush cache\" at the end of `K.rnn` to update the moving averages. But the question is, which is more efficient?:\r\n\r\n - **(A)** Create 100 `tf.Variable`'s shaped `(1, 10)`, store in Python list, then update each directly\r\n - **(B)** Create 1 array `tf.Variable` shaped `(100, 10)`, a \"cache\" tensor, iteratively update the cache tensor, then assign to `tf.Variable`\r\n\r\nAs far as I can tell, (A) seems superior; (B) involves more operations overall, and uses a duplicate array for caching. So, (A) should be faster _and_ more memory-efficient. Is this accurate?\r\n\r\nEven in the ideal case of direct slice-updating, (A) doesn't seem far behind; there's just a negligible memory overhead in using many arrays vs. one equivalent.", "Figured out direct slice-update: \r\n\r\n```python\r\nvar_slice = var[4:5]\r\nvar_slice.assign(math_ops.sub(var, const))\r\n```\r\n\r\n`var_slice` appears to nicely reference the original tensor array; for some reason `assign_sub` fails but `assign(sub(` works.", "Hum, what's the error u see from the assign_sub()? I would expect it to work.", "@qlzh727 Github notifications seem broke - didn't see your reply; no Exceptions, but the `tf.Variable` created via `self.add_weight()` would simply not update the original `tf.Variable`. Note that in your example, you aren't slicing `a` and updating the slice, but are assigning some slice value to `a` directly. Also, unsure if it makes a difference, but my ops were in a `while_v2.while_loop()` graph context.", "Oh, sorry to misunderstand your question.\r\n\r\nFirst, the return value from the slide op is a Tensor, which is not the original variable. \r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(range(10), shape=(10,), name=\"a\")\r\nb = a[0:1]\r\nprint(b)\r\n# tf.Tensor([0], shape=(1,), dtype=int32)\r\n```\r\n\r\nWith a tensor, it doesn't have methods like \"assign\" or \"assign_add\" etc. \r\n\r\nI think the most feasible way to create a TensorArray, which contains your slides of mean/variance value, and read/write to it within the call() body for cell. Once the layer goes over all the timesteps, you can do a stack for the TensorArray and assign the value back to the variable itself. You don't have to continuously write to the variable when processing the timesteps, since timestep t shouldn't affect the result in t+1 (if I understand you problem correctly)", "@qlzh727 Thanks for the suggestion; per an overwhelming pile of bugs in 2.1, I've suspended my work on this, but I'll try your idea if it becomes relevant - particularly since my method still involves an update tensor shaped as the full original tensor, which may be poorly-performant.", "I also wanted to know how to efficiently update a tensor slice, but in my case not a model weight (that is, not a `tf.Variable`), but the input of a layer.\r\n\r\nA non-working template of what I am trying to do is:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\n\r\nclass AddToEven(Model):\r\n    def call(self, inputs):\r\n        outputs = inputs\r\n        outputs[:, ::2] += 1\r\n        return outputs\r\n```\r\n\r\nof course when building this (`AddToEven().build(tf.TensorShape([None, None]))`) I get the following error:\r\n\r\n```\r\nTypeError: 'Tensor' object does not support item assignment\r\n```\r\n\r\nI can achieve this simple example via the following:\r\n\r\n```python\r\nclass AddToEvenScatter(Model):\r\n    def call(self, inputs):\r\n        batch_size = tf.shape(inputs)[0]\r\n        n = tf.shape(inputs)[-1]\r\n        update_indices = tf.range(0, n, delta=2)[:, None]\r\n        scatter_nd_perm = [1, 0]\r\n        inputs_reshaped = tf.transpose(inputs, scatter_nd_perm)\r\n        outputs = tf.tensor_scatter_nd_add(\r\n            inputs_reshaped,\r\n            indices=update_indices,\r\n            updates=tf.ones([n//2, 1]),\r\n        )\r\n        outputs = tf.transpose(outputs, scatter_nd_perm)\r\n        return outputs\r\n```\r\n\r\n(you can sanity-check with:\r\n```python\r\nmodel = AddToEvenScatter()\r\nmodel.build(tf.TensorShape([None, None]))\r\nmodel(tf.zeros([1, 10]))\r\n```\r\n)\r\n\r\nBut as you can see it's very complicated to write. And this is only for a static number of updates (here 1) on a 1D (+ batch size) tensor.\r\n\r\nWhat I want to do is a bit more involved and I think writing it with `tensor_scatter_nd_add` is going to be a nightmare.\r\n\r\nI don't know if this is the right place to ask question or if I should open another issue. Please let me know.\r\n\r\n(For anyone curious I am trying to translate [this code](https://github.com/lpj-github-io/MWCNNv2/blob/master/MWCNN_code/model/common.py#L80-L99) in tf).", "@zaccharieramzi New Issue, surely. Can also try [Stack Overflow](https://stackoverflow.com/).", "@OverLordGoldDragon sure, I think I might try SO first. You can see the question [here](https://stackoverflow.com/questions/62092147/how-to-efficiently-assign-to-a-slice-of-a-tensor-in-tensorflow).", "https://stackoverflow.com/questions/67378040/", "Tensorflow requires a function to efficiently modify a tensor subset (row/column/slice). For example, to modify a row/column of a 2D tensor;\r\n\r\n```python\r\n#note if updatedValue isVector, updatedValue should be provided in 2D format\r\ndef modifyTensorRowColumn(a, isRow, index, updatedValue, isVector):\r\n\t\r\n\tif(not isRow):\r\n\t\ta = tf.transpose(a)\r\n\t\tif(isVector):\r\n\t\t\tupdatedValue = tf.transpose(updatedValue)\r\n\t\r\n\tif(index == 0):\r\n\t\tif(isVector):\r\n\t\t\tvalues = [updatedValue, a[index+1:]]\r\n\t\telse:\r\n\t\t\tvalues = [[updatedValue], a[index+1:]]\r\n\telif(index == a.shape[0]-1):\r\n\t\tif(isVector):\r\n\t\t\tvalues = [a[:index], updatedValue]\r\n\t\telse:\r\n\t\t\tvalues = [a[:index], [updatedValue]]\r\n\telse:\r\n\t\tif(isVector):\r\n\t\t\tvalues = [a[:index], updatedValue, a[index+1:]]\r\n\t\telse:\r\n\t\t\tvalues = [a[:index], [updatedValue], a[index+1:]]\r\n\t\t\t\r\n\ta = tf.concat(axis=0, values=values)\r\n\t\t\t\r\n\tif(not isRow):\r\n\t\ta = tf.transpose(a)\r\n\t\t\r\n\treturn a\r\n```"]}, {"number": 36558, "title": "[ROCm] Fix for a test regression on the ROCm platform - 200207 - 1", "body": "The following commit introduces a test regression on the ROCm platform\r\nhttps://github.com/tensorflow/tensorflow/pull/36058/commits/a2aa5e3f045a5916b20a63b58f824ed59710845a\r\n\r\nAfter the commit, the test fails to build with the following error.\r\n\r\n```\r\nERROR: /root/tensorflow/tensorflow/lite/kernels/BUILD:846:1: Couldn't build file tensorflow/lite/kernels/_objs/concatenation_test/concatenation_test.o: C++ compilation of rule '//tensorflow/lite/kernels:concatenation_test' failed (Exit 1)\r\ntensorflow/lite/kernels/concatenation_test.cc:276:26: error: expected ';' at end of member declaration\r\n       std::is_same<Type, int16_t>::value ? TensorType_INT16 : TensorType_INT8;\r\n                          ^\r\ntensorflow/lite/kernels/concatenation_test.cc:276:33: error: expected unqualified-id before '>' token\r\n       std::is_same<Type, int16_t>::value ? TensorType_INT16 : TensorType_INT8;\r\n                                 ^\r\ntensorflow/lite/kernels/concatenation_test.cc:276:20: error: wrong number of template arguments (1, should be 2)\r\n       std::is_same<Type, int16_t>::value ? TensorType_INT16 : TensorType_INT8;\r\n                    ^\r\nIn file included from /usr/include/c++/5/bits/move.h:57:0,\r\n                 from /usr/include/c++/5/bits/stl_pair.h:59,\r\n                 from /usr/include/c++/5/bits/stl_algobase.h:64,\r\n                 from /usr/include/c++/5/memory:62,\r\n                 from external/com_google_googletest/googletest/include/gtest/gtest.h:56,\r\n                 from tensorflow/lite/kernels/concatenation_test.cc:17:\r\n/usr/include/c++/5/type_traits:958:12: note: provided for 'template<class, class> struct std::is_same'\r\n     struct is_same;\r\n            ^\r\n```\r\n\r\nThe fix is to put parens around the RHS expr, to fox what seems to be a parsing error. Don't think this error was ROCm specific.\r\n\r\n\r\n----------\r\n\r\n\r\n/cc @whchung @cheshire @chsigg ", "comments": ["gentle ping", "Sure, seems like a weird compiler bug? Accepting."]}, {"number": 36557, "title": "Dynamically unrolled graphs ~7x slower than statically unrolled graphs within tf.function", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): wheel\r\n- TensorFlow version (use command below): tf-nightly\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.x.x\r\n- GPU model and memory: 4 x titan xp\r\n\r\n**Describe the current behavior**\r\n\r\nI have a snippet of code in my evaluation loop that unrolls a model's predictions. Graph is a dictionary which is updated by both the policy_model and dynamics_model. I call the recurrent loop in this pattern, once outside of the loop to ensure the tensors are set to the correct shape (necessary for tf.range) and then inside the loop to continue the unrolling:\r\n\r\n```\r\n    graph = dynamics_graph_constructor(model_inputs)\r\n    graph = policy_model(graph)\r\n    predicted_value = policy_normalization(graph)\r\n    rollout_value = predicted_value\r\n    graph = dynamics_model(graph)\r\n    graph_updates = get_graph_delta(graph, 0)\r\n    graph = update_graph(graph, model_inputs, graph_updates)\r\n\r\n    for step in tf.range(agent_params[\"search_depth\"]-1):\r\n        graph = policy_model(graph)\r\n        predicted_value = policy_normalization(graph)\r\n        rollout_value += predicted_value\r\n        graph = dynamics_model(graph)\r\n         graph_updates = get_graph_delta(graph, 0)\r\n        graph = update_graph(graph, model_inputs, graph_updatesl)\r\n```\r\n\r\nIf I change tf.range to range, I see an approximately 7x speedup. Is this normal? This seems like a huge performance penalty when using tf.while loops, even when using autograph.\r\n", "comments": ["@mjlbach,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Closing, this was another issue similar to https://github.com/tensorflow/tensorflow/issues/27143, even though I was pretty sure it wasn't... Sometimes a static graph will seem faster when it's actually removing iterations in which the input doesn't change (had a bug in my recurrent code)."]}, {"number": 36556, "title": "TensorFlow should support importing submodules, like a normal Python package", "body": "**System information**\r\n- TensorFlow version (you are using): 2.x\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorFlow 1.x supports importing submodules, e.g., `import tensorflow.io`\r\n\r\nOn TensorFlow 2.x this raises `ModuleNotFoundError: No module named 'tensorflow.io'`. Instead you have to import `tensorflow`, at which point the `io` module is directly available. This is both misleading and highly non-standard for Python.\r\n\r\nIf you not want to support direct importing of submodules, then perhaps they shouldn't be called modules in the online docs?\r\nhttps://www.tensorflow.org/api_docs/python/tf/io\r\n\r\n**Will this change the current api? How?** Yes, but it would be fully backwards compatible.\r\n\r\n**Who will benefit with this feature?** Anyone who expects TensorFlow to work like a normal Python package.", "comments": ["In general, in Python if you can do\r\n\r\n```python\r\nimport foo\r\nfoo.bar.baz.foobar\r\n```\r\n\r\nthat doesn't imply you can also do\r\n\r\n```python\r\nimport foo.bar.baz as m\r\nm.foobar\r\n```\r\n\r\nIt is not just something in Python that TensorFlow breaks, it's just the way things work.\r\n\r\nIn fact, to separate public and private APIs it is actually better to not allow both types of imports.", "It's true that the former behavior does not strictly imply the later, but in practice it essentially always does for modules. If something is called the `foo.bar.baz` module, then it is expected for `import foo.bar.baz` to work. I cannot think of any exceptions to this rule, besides TensorFlow.\r\n\r\nThere are other ways to hide internal implementation details. For example, it is common to use undocumented private modules (name starting with `_`) for implementations, and expose only intentionally public functions/classes in documented APIs.", "I agree with you. There is work on simplifying this structure, see https://github.com/tensorflow/community/pull/182", "Closing this issue since we have other thread capturing this. Thanks!"]}, {"number": 36554, "title": "reduce_euclidean_norm false results", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Google colab\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nreduce_euclidean_norm might return false results?\r\nFor example, the following three snippets are all using reduce_euclidean_norm, but all return wrong results (return 4; should return sqrt(17)).\r\n\r\nx = tf.constant([[1, 2, 3], [1, 1, 1]])\r\ny = tf.math.reduce_euclidean_norm(x)\r\nwith tf.Session() as sess:  print(y.eval()) \r\nreturns 4, should return numerical value of sqrt(17)\r\n\r\nx = tf.constant([[1, 2, 3], [1, 1, 1]])\r\ny = tf.compat.v1.math.reduce_euclidean_norm(x)\r\nwith tf.Session() as sess:  print(y.eval()) \r\nreturns 4, should return numerical value of sqrt(17)\r\n\r\nx = tf.constant([[1, 2, 3], [1, 1, 1]])\r\ny = tf.compat.v2.math.reduce_euclidean_norm(x)\r\nwith tf.Session() as sess:  print(y.eval()) \r\nreturns 4, should return numerical value of sqrt(17)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nfrom tensorflow docs\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/reduce_euclidean_norm\r\nx = tf.constant([[1, 2, 3], [1, 1, 1]])\r\ntf.reduce_euclidean_norm(x)  # sqrt(17)\r\n\r\n**Code to reproduce the issue**\r\nAttached above. \r\n\r\n**Other info / logs**\r\nN/A\r\n\r\nThank you so much!", "comments": ["Update: When the input is in the form\r\n`x = tf.constant([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])`\r\nthe reduce_euclidean_norm returns the correct result.", "Credit: @merterm ", "I have tried on colab with TF version 1.15, Nightly versions and was able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/4dc242a7a9391723df71d17e0bf68aa0/untitled630.ipynb) Thanks!", "Thank you so much!", "@martinmamql All those examples are correctly returning 4 instead of sqrt(17). The reason is `x.dtype is tf.int32` and the doc clearly says `Returns : The reduced tensor, of the same dtype as the input_tensor.`. However, the document has typo (sqrt(17)) which needs to be corrected. I raised a PR to correct it in `master` and will be updated in the docs after merging the PR. Thanks!\r\n\r\nAlternatively, you can add dtype to the end of `x` as follows. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/dd17894e65c1d74841c197750734633a/36615.ipynb).\r\n`x = tf.constant([[1, 2, 3], [1, 1, 1]], dtype = tf.float32)`", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36554\">No</a>\n"]}, {"number": 36553, "title": " output 'tensorflow/core/kernels/_objs/reduction_ops_gpu/tensorflow/core/kernels/reduction_ops_half_mean_sum.cu.pic.o' was not created", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Python version**:3.6.6\r\n- **Bazel version (if compiling from source)**:0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:7.3.0\r\n- **CUDA/cuDNN version**:CUDA10.0  , cuDNN 7\r\n- **GPU model and memory**:GTX1050\r\n- **Exact command to reproduce**:\r\n```\r\n\r\n### Describe the problem \r\nWhile I run the code '''' from https://github.com/VLOGroup/mri-variationalnetwork\r\n\r\nI'm using Ubuntu18.04,\r\nbazel 0.11.1\r\nCuda10.0\r\ncuDNN7.4\r\n\r\nI think some attributes not compile in the file \" paramdefination.py\"\r\n```\r\ndef plt_act_function(x, phi):\r\n    my_dpi = 96.\r\n    fig = matplotlib.figure.Figure(figsize=(350/my_dpi, 250/my_dpi), dpi=my_dpi)\r\n    ax = fig.add_subplot(1, 1, 1)\r\n    images = []\r\n    for s in range(phi.shape[0]):\r\n        images_stage = []\r\n        for i in range(phi.shape[1]):\r\n            ax.clear()\r\n            ax.plot(x, phi[s, i, :])\r\n            if fig.canvas is None:\r\n                FigureCanvasAgg(fig)\r\n            fig.canvas.draw()\r\n            img_data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\r\n            img_data = img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\r\n            images_stage.append(img_data)\r\n        images.append(images_stage)\r\n    return np.asarray(images)\r\n```\r\n\r\nThis is the following issue:\r\n\r\n2020-02-09 14:45:41.165513: W tensorflow/core/framework/op_kernel.cc:1198] Unknown: AttributeError: 'FigureCanvasBase' object has no attribute 'tostring_rgb'\r\nTraceback (most recent call last):\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\nreturn fn(*args)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\nstatus, run_metadata)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in exit\r\nc_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: AttributeError: 'FigureCanvasBase' object has no attribute 'tostring_rgb'\r\n[[Node: activation_plot/PyFunc = PyFunc[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_UINT8], token=\"pyfunc_2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](activation_plot/strided_slice/_371, activation_plot/Reshape_1/_373)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"train_mri_vn.py\", line 315, in\r\ncoord.join(enqueue_threads_data)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 387, in join\r\nsix.reraise(*self._exc_info_to_raise)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/six.py\", line 703, in reraise\r\nraise value\r\nFile \"train_mri_vn.py\", line 295, in\r\noptions=run_options, run_metadata=run_metadata)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\nrun_metadata_ptr)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\nfeed_dict_tensor, options, run_metadata)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\noptions, run_metadata)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: AttributeError: 'FigureCanvasBase' object has no attribute 'tostring_rgb'\r\n[[Node: activation_plot/PyFunc = PyFunc[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_UINT8], token=\"pyfunc_2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](activation_plot/strided_slice/_371, activation_plot/Reshape_1/_373)]]\r\n\r\nCaused by op 'activation_plot/PyFunc', defined at:\r\nFile \"train_mri_vn.py\", line 176, in\r\nvn.paramdefinitions.add_activation_function_params(params, reg_config['activation1'])\r\nFile \"/home/vivi/mri-variationalnetwork-master/vn/paramdefinitions.py\", line 70, in add_activation_function_params\r\nphi_img = tf.py_func(plt_act_function, [x_plt_tf[:, 0], phi_plt], tf.uint8)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 300, in py_func\r\nfunc=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 209, in _internal_py_func\r\ninput=inp, token=token, Tout=Tout, name=name)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 93, in _py_func\r\n\"PyFunc\", input=input, token=token, Tout=Tout, name=name)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\nop_def=op_def)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\nop_def=op_def)\r\nFile \"/home/vivi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in init\r\nself._traceback = self._graph._extract_stack() # pylint: disable=protected-access\r\n\r\nUnknownError (see above for traceback): AttributeError: 'FigureCanvasBase' object has no attribute 'tostring_rgb'\r\n[[Node: activation_plot/PyFunc = PyFunc[Tin=[DT_FLOAT, DT_FLOAT], Tout=[DT_UINT8], token=\"pyfunc_2\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](activation_plot/strided_slice/_371, activation_plot/Reshape_1/_373)]]", "comments": ["@vivien624, Please provide the complete code to replicate the reported issue and also Tensorflow version that you are using. Thanks!", "@vivien624, Any update?", "I think I figured it out. Thank you very much.\n\ngadagashwini <notifications@github.com> \u4e8e2020\u5e742\u670817\u65e5\u5468\u4e00 \u4e0a\u53485:45\u5199\u9053\uff1a\n\n> @vivien624 <https://github.com/vivien624>, Any update?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36553?email_source=notifications&email_token=AOPZSCIBVLUWBKLCLIZQW33RDJTGHA5CNFSM4KRSSXG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEL55WVY#issuecomment-586931031>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOPZSCO2XHSODMO5GE4QCIDRDJTGHANCNFSM4KRSSXGQ>\n> .\n>\n", "@vivien624, Glad that it resolved. ", "@vivien624, I am having the same issue. How did you resolve it? Thanks!", "See this issue: #28796\n<https://github.com/tensorflow/tensorflow/issues/28796>\nThe comment from Tayseer-bme <https://github.com/Tayseer-bme> gives\nsolution:\nhttps://github.com/tensorflow/tensorflow/issues/28796#issuecomment-496881881\nand it was resolved by 3c245f5\n<https://github.com/tensorflow/tensorflow/commit/3c245f52912612ed9d8e20245ddb5de055680969>\n\ncecilr14 <notifications@github.com> \u4e8e2020\u5e744\u67082\u65e5\u5468\u56db \u4e0b\u534812:06\u5199\u9053\uff1a\n\n> @vivien624 <https://github.com/vivien624>, I am having the same issue.\n> How did you resolve it? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36553#issuecomment-607972625>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOPZSCO55XBVU4SEMMNLWCDRKTAXPANCNFSM4KRSSXGQ>\n> .\n>\n", "@vivien624, Thanks! Sadly, the above changes do not work for me. I am using Tensorflow's r1.8 branch which already has the commit 3c245f5 and am still getting the error:\r\n'FigureCanvasBase' object has no attribute 'tostring_rgb\r\nin paramdefinitions.py"]}, {"number": 36552, "title": "TFv1.15.0 run tf_cnn_benchmarks.py bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): From source\r\n- TensorFlow version (use command below): v1.15.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): Bazel 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc-7.4.0\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.4.1.5\r\n- GPU model and memory: Tesla V100, 32510MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n`v1.15.0-0-g590d6eef7e 1.15.0`\r\n\r\n**Describe the current behavior**\r\nWhile running `python3 tf_cnn_benchmarks.py --num_gpus=1 --batch_size=32 --model=resnet50 --variable_update=parameter_server`, output error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_cnn_benchmarks.py\", line 29, in <module>\r\n    import benchmark_cnn\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 51, in <module>\r\n    from models import model_config\r\n  File \"/root/benchmarks/scripts/tf_cnn_benchmarks/models/model_config.py\", line 152, in <module>\r\n    from tensorflow.contrib import slim  # pylint: disable=unused-import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _handle_fromlist\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/__init__.py\", line 54, in <module>\r\n    from tensorflow.contrib import image\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/image/__init__.py\", line 55, in <module>\r\n    from tensorflow.contrib.image.python.ops.dense_image_warp import dense_image_warp\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/image/__init__.py\", line 57, in <module>\r\n    from tensorflow.contrib.image.python.ops.distort_image_ops import adjust_hsv_in_yiq\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/image/python/ops/distort_image_ops.py\", line 29, in <module>\r\n    resource_loader.get_path_to_datafile('_distort_image_ops.so'))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/util/loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/image/python/ops/_distort_image_ops.so: undefined symbol: _ZN4absl5Mutex10ReaderLockEv\r\npython3 tf_cnn_benchmarks.py --num_gpus=1 --batch_size=32 --model=resnet50   3.14s user 2.78s system 291% cpu 2.029 total\r\n```\r\n\r\n**Describe the expected behavior**\r\nSuccessfully run the example code.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nFollowing the command below, `https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks#getting-started`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I added \"--config gdr --config verbs\" while configuring tensorflow for future work. I wonder if it would result in this bug.", "@xcwanAndy, Can you provide the standalone code to replicate the reported issue. Thanks!", "@xcwanAndy, I tried on google colab, its working as expected.\r\nPlease take a look at gist [here](https://colab.research.google.com/gist/gadagashwini/ef1842af2c85528da53b45aa2574081f/untitled382.ipynb). Thanks!", "> @xcwanAndy, Can you provide the standalone code to replicate the reported issue. Thanks!\r\n@gadagashwini Thanks a lot for your reply. As I said, the test code is exactly the official code: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks#getting-started)   under branch \"cnn_tf_v1.15_compatible\".\r\n\r\nI think there should be something wrong when installing tensorflow from source in docker (however, the installation succeeds in the end).\r\n\r\nI use bazel==0.26.1 for installation and have V100 GPU and  RDMA supports in network, \".tf_configure.bazelrc\" is shown below:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.0,7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cudnn/lib64:/usr/local/cuda/lib64:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nccl/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nThanks!", "@xcwanAndy, Is this still an issue?", "@gadagashwini  Yes, I still cannot make it in my docker container.", "@xcwanAndy Can you please try this with Tensorflow 1.14 docker image and let me know if it is working as expected. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36552\">No</a>\n", "@xcwanAndy I have the same issue, how did you solve it finally? You closed it, but you did not specify how it was finally solved. Thanks for your help. \r\n\r\nBTW, I got the exact same error, but running an example from Google Trax, which calls tensorflow. \r\n\r\nTensorFlow installed from (source or binary): From source\r\nTensorFlow version (use command below): v1.15.2\r\nPython version: Python 3.7\r\nBazel version (if compiling from source): Bazel 0.26.1\r\nGCC/Compiler version (if compiling from source): gcc-7.5.0\r\nCUDA/cuDNN version: CUDA 10.0, cuDNN 7.4.1.5\r\nGPU model and memory: RTX2070, 7982MiB\r\n\r\npython -m trax.trainer --config_file=$PWD/trax/configs/mlp_mnist.gin"]}, {"number": 36551, "title": "Merge pull request #1 from tensorflow/master", "body": "Merge", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36551) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 36550, "title": "TF_CONFIG is used even though a ClusterResolver was passed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using a MultiWorkerMirroredStrategy with a cluster_resolver to avoid having to define the (to me obscure) TF_CONFIG env variable. But even though TF_CONFIG is used: https://github.com/tensorflow/tensorflow/blob/925be1049d191f25aa41c26db03205935b2e8582/tensorflow/python/distribute/distribute_coordinator.py#L752\r\n\r\nThis leads to output like:\r\n> WARNING:tensorflow:Skipped evaluation since `eval_fn` is not passed in.\r\n\r\nWhich can only happen when no cluster-spec was found: https://github.com/tensorflow/tensorflow/blob/925be1049d191f25aa41c26db03205935b2e8582/tensorflow/python/distribute/distribute_coordinator.py#L778-L787\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no need to define this env variable and it should not be used but everything should come from the cluster resolver\r\n\r\n**Code to reproduce the issue**\r\n\r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy but with SlurmClusterResolver", "comments": ["This should be [fixed](32aeb9957ede5496aa76a2d9a0b7b202d76a24fc) now. Can you try with the latest nightly release to see if you are able to run without issues?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36550\">No</a>\n", "@anj-s This doesn't look correct. https://github.com/tensorflow/tensorflow/blob/32aeb9957ede5496aa76a2d9a0b7b202d76a24fc/tensorflow/python/distribute/distribute_coordinator.py#L756-L761 shows that the TF_CONFIG variable takes precedence over the cluster_resolver explicitly passed in. This means different parts of the code might use different information depending on whether they use the TF_CONFIG or the cluster_resolver.\r\n\r\nIIRC the strategy (now) always has a cluster resolver and defaults to a resolver using TF_CONFIG. So why not always use the cluster resolver?\r\n\r\nOh and the documentation is incomplete now: https://github.com/tensorflow/tensorflow/blob/cf46f78c72bbcb86aff64d06e6fbbb54f2f31d8a/tensorflow/python/distribute/distribute_coordinator.py#L703-L704", "@Flamefire Are you setting the TF_CONFIG and also passing a cluster resolver argument to the strategy? We generally expect users to either use TF_CONFIG or use a cluster resolver.\r\n\r\nThis codepath is used as an entry point for different use cases which is why we need this. For MultiWorkerMirroredStrategy we can default to the cluster resolver as the single source of truth but that is not the case for all use cases of this function. We will be refactoring this soon so hopefully things will look clearer in the coming months.", "Currently (2.1.0) yes, because only using TF_CONFIG doesn't allow everything the resolver does, e.g. setting num_gpus.\r\n\r\n> For MultiWorkerMirroredStrategy we can default to the cluster resolver as the single source of truth but that is not the case for all use cases of this function.\r\n\r\nWouldn't you agree, that if the strategy passed to this function has a cluster resolver, then it should be used?   \r\nAdditionally: Instead of having multiple sources of truth and multiple parsing of TF_CONFIG why not always get the cluster_resolver from the strategy and default to the TfConfigClusterResolver if none is set? This would make it consistent with MultiWorkerMirroredStrategy and avoids differences in parsing the TF_CONFIG variable.\r\n\r\nBTW: What are the different use cases? What strategy would be valid here that does not have a cluster_resolver but requires setting TF_CONFIG? Wouldn't it make sense to require a strategy with a cluster_resolver?", "FTR: This was only fully fixed in 2.3.0 with https://github.com/tensorflow/tensorflow/commit/32aeb9957ede5496aa76a2d9a0b7b202d76a24fc not in 2.2.0 as indicated here. Hence until 2.3.0 it is required to set TF_CONFIG **in addition** to the cluster_resolver"]}, {"number": 36549, "title": "Corrected docstring for tf.signal.frame", "body": "Corrected docstring for tf.signal.frame.\r\n\r\nIssue opened here: https://github.com/tensorflow/tensorflow/issues/36547", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36549) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36549) for more info**.\n\n<!-- ok -->", "We don't accept PR to branches other than master ,only security fixes are accepted to release branch. Thank you for your contribution.\r\nCC @mihaimaruseac"]}, {"number": 36548, "title": "accuracy loss when running inference on edge TPU with keras neural network", "body": "**System information**\r\n- OS Platform and Distribution : Debian 10.2.0\r\n- TensorFlow installed from : https://www.tensorflow.org/install\r\n- TensorFlow version : 2.0\r\n\r\n\r\nHi, I'm a student at the University of Bologna ( Italy) and I'm using the Google Coral USB accelerator for my thesis. I realized a keras neural network that classifies my data in four classes and the accuracy I get is roughly 97%. I performed a full integer post- training quantization since keras networks don't support quantization-aware training. I followed the guide on TensorFlow site but I had problems running inference on the edge tpu . In particular my network undergoes an accuracy loss when the model is converted to a `tensorflowlite` one ( the accuracy drops to roughly the 25%) . This is due to quantization since the `tensorflowlite` model without quantization that runs on my pc is not affected by the conversion. I tried to scale my input data in a range [0, 255] with `MinMaxScaler` but in this case , even if the accuracy of the `tensorflowlite` quantized model matches the one of the not converted one , the results are not satisfactory since the accuracy of the network itself is low. I wonder if you could help me solve this problem. Perhaps the values on my dataset are too low and the quantization fails to convert `float32` into `uint8` without information loss. Below you'll find my python code.\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom numpy import loadtxt\r\nfrom tensorflow.keras.models import Sequential \r\nfrom tensorflow.keras.layers import Dense\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\r\nfrom sklearn.metrics import confusion_matrix, accuracy_score\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nimport os\r\nimport time\r\nimport tflite_runtime.interpreter as tflite\r\nimport collections\r\nimport operator\r\n\r\n\"\"\"Functions to work with classification models.\"\"\"\r\n\r\n\r\n\r\n\r\nClass = collections.namedtuple('Class', ['id', 'score'])\r\n\r\n\r\n\r\n\r\ndef input_tensor(interpreter):\r\n  \"\"\"Returns input tensor view as numpy array of shape (height, width, 3).\"\"\"\r\n  tensor_index = interpreter.get_input_details()[0]['index']\r\n  return interpreter.tensor(tensor_index)()[0]\r\n\r\ndef output_tensor(interpreter):\r\n  \"\"\"Returns dequantized output tensor.\"\"\"\r\n  output_details = interpreter.get_output_details()[0] \r\n  output_data = np.squeeze(interpreter.tensor(output_details['index'])()) #Remove single-dimensional entries from the shape of an array.\r\n  scale, zero_point = output_details['quantization']\r\n  return scale * (output_data - zero_point)\r\n\r\n\r\ndef set_input(interpreter, data):\r\n  \"\"\"Copies data to input tensor.\"\"\"\r\n  input_tensor(interpreter)[:] = data\r\n  return data   \r\n\r\n\r\ndef get_output(interpreter, top_k=1, score_threshold=0.0):\r\n  \"\"\"Returns no more than top_k classes with score >= score_threshold.\"\"\"\r\n  scores = output_tensor(interpreter)\r\n  classes = [\r\n      Class(i, scores[i])\r\n      for i in np.argpartition(scores, -top_k)[-top_k:]\r\n      if scores[i] >= score_threshold\r\n  ]\r\n  return sorted(classes, key=operator.itemgetter(1), reverse=True)\r\n\r\n#load the dataset\r\n\r\n\r\nModelli_Prova01_Nom01_Acc1L = loadtxt(r'/home/utente/Scrivania/csvtesi/Modelli_Prova01_Nom01_Acc1L.csv',delimiter=',')\r\nModelli_Prova02_Nom01_Acc1L = loadtxt(r'/home/utente/Scrivania/csvtesi/Modelli_Prova02_Nom01_Acc1L.csv',delimiter=',')\r\nModelli_Prova03_Nom01_Acc1L = loadtxt(r'/home/utente/Scrivania/csvtesi/Modelli_Prova03_Nom01_Acc1L.csv',delimiter=',')\r\nModelli_Prova04_Nom01_Acc1L = loadtxt(r'/home/utente/Scrivania/csvtesi/Modelli_Prova04_Nom01_Acc1L.csv',delimiter=',')\r\nModelli_Prova05_Nom01_Acc1L = loadtxt(r'/home/utente/Scrivania/csvtesi/Modelli_Prova05_Nom01_Acc1L.csv',delimiter=',')\r\n\r\ntime_start = time.perf_counter()\r\n\r\n\r\n\r\n#split x and y data (train and test)\r\n\r\nAcc1L01_train,Acc1L01_test = train_test_split(Modelli_Prova01_Nom01_Acc1L ,test_size=0.015,random_state=42)\r\nAcc1L02_train,Acc1L02_test = train_test_split(Modelli_Prova02_Nom01_Acc1L,test_size=0.3,random_state=42)\r\nAcc1L03_train,Acc1L03_test = train_test_split(Modelli_Prova03_Nom01_Acc1L,test_size=0.3,random_state=42)\r\nAcc1L04_train,Acc1L04_test = train_test_split(Modelli_Prova04_Nom01_Acc1L,test_size=0.3,random_state=42)\r\nAcc1L05_train,Acc1L05_test = train_test_split(Modelli_Prova05_Nom01_Acc1L,test_size=0.15,random_state=42)\r\nY1_train= np.zeros([len(Acc1L01_train)+len(Acc1L05_train),1]) \r\nY2_train= np.ones([len(Acc1L02_train),1]) \r\nY3_train= np.ones([len(Acc1L03_train),1]) +1\r\nY4_train= np.ones([len(Acc1L04_train),1]) +2\r\nY1_test= np.zeros([len(Acc1L01_test)+len(Acc1L05_test),1]) \r\nY2_test= np.ones([len(Acc1L02_test),1])  \r\nY3_test= np.ones([len(Acc1L03_test),1]) +1\r\nY4_test= np.ones([len(Acc1L04_test),1]) +2\r\n\r\nxAcc1L_train = np.concatenate((Acc1L01_train,Acc1L05_train,Acc1L02_train,Acc1L03_train,Acc1L04_train),axis=0)\r\nxAcc1L_train=MinMaxScaler([0,255]).fit_transform(xAcc1L_train)\r\n#xAcc1L_train=StandardScaler().fit_transform(xAcc1L_train)\r\n#xAcc1L_train=Normalizer().fit_transform(xAcc1L_train)\r\n#xAcc1L_train=np.transpose(xAcc1L_train)\r\nyAcc1L_train = np.concatenate((Y1_train,Y2_train,Y3_train,Y4_train),axis=0)\r\nxAcc1L_test = np.concatenate((Acc1L01_test,Acc1L05_test,Acc1L02_test,Acc1L03_test,Acc1L04_test),axis=0)\r\nxAcc1L_test=Normalizer().fit_transform(xAcc1L_test)\r\n#xAcc1L_test=MinMaxScaler([0,255]).fit_transform(xAcc1L_test)\r\n#xAcc1L_test=StandardScaler().fit_transform(xAcc1L_test)\r\n#xAcc1L_test=np.transpose(xAcc1L_test)\r\nyAcc1L_test = np.concatenate((Y1_test,Y2_test,Y3_test,Y4_test),axis=0)\r\n#1 hot encode y\r\none_hot_labelsAcc1L =to_categorical(yAcc1L_train, num_classes=4)\r\none_hot_labelsAcc1L_test = to_categorical(yAcc1L_test, num_classes=4)\r\n#fit the model\r\nmodel = Sequential()\r\nmodel.add(Dense(300, activation='relu', input_dim=30))\r\nmodel.add(Dense(4, activation='softmax'))\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\nes1 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\r\nes2 = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=100)\r\n\r\nhistory=model.fit(xAcc1L_train, one_hot_labelsAcc1L,validation_data=(xAcc1L_test,one_hot_labelsAcc1L_test),epochs=500, batch_size=30, verbose=1, callbacks=[es1,es2])\r\n#history=model.fit(tf.cast(xAcc1L_train, tf.float32), one_hot_labelsAcc1L,validation_data=(tf.cast(xAcc1L_test, tf.float32),one_hot_labelsAcc1L_test),epochs=500, batch_size=30, verbose=1, callbacks=[es1,es2])\r\ntime_elapsed = (time.perf_counter() - time_start)\r\nprint (\"%5.1f secs \" % (time_elapsed))\r\n\r\nstart=time.monotonic()\r\n_, accuracy = model.evaluate(xAcc1L_test, one_hot_labelsAcc1L_test, batch_size=30, verbose=1)\r\n#_, accuracy = model.evaluate(tf.cast(xAcc1L_test, tf.float32), one_hot_labelsAcc1L_test, batch_size=30, verbose=1)\r\nprint(accuracy)\r\ninference_time = time.monotonic() - start\r\nprint('%.1fms ' % (inference_time * 1000))\r\n\r\n# summarize history for accuracy\r\nplt.plot(history.history['accuracy'])\r\nplt.plot(history.history['val_accuracy'])\r\nplt.title('model accuracy')\r\nplt.ylabel('accuracy')\r\nplt.xlabel('epoch')\r\nplt.legend(['train', 'test'], loc='upper left')\r\nplt.show()\r\n# summarize history for loss\r\nplt.plot(history.history['loss'])\r\nplt.plot(history.history['val_loss'])\r\nplt.title('model loss')\r\nplt.ylabel('loss')\r\nplt.xlabel('epoch')\r\nplt.legend(['train', 'test'], loc='upper right')\r\nplt.show()  \r\n#predicted labels\r\npredictions = model.predict(xAcc1L_test)\r\ny_pred = (predictions > 0.5)\r\nmatrix = confusion_matrix(one_hot_labelsAcc1L_test.argmax(axis=1), y_pred.argmax(axis=1))\r\nprint('confusion matrix = \\n',matrix)\r\nprint(\"Accuracy:\",accuracy_score(one_hot_labelsAcc1L_test.argmax(axis=1), y_pred.argmax(axis=1)))\r\n\r\n\r\n\r\nmod01=model.save('/home/utente/Scrivania/csvtesi/rete_Nom01.h5')\r\n\r\n#convert the model\r\n\r\n\r\n#representative dataset       \r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (tf.cast(xAcc1L_train, tf.float32))).batch(1)\r\nprint(train_ds)\r\n\r\ndef representative_dataset_gen():\r\n    for input_value in train_ds: \r\n        yield [input_value]\r\n\r\nprint(model.layers[0].input_shape)\r\n\r\n#integer post-training quantization\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('/home/utente/Scrivania/csvtesi/rete_Nom01.h5') #all operations mapped on edge tpu\r\n#converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nprint(converter.representative_dataset)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_quant_model = converter.convert()\r\nopen('/home/utente/Scrivania/csvtesi/rete_Nom01_quant.tflite', \"wb\").write(tflite_quant_model)\r\n\r\n\r\n\r\n#compiler compila il modello quantizzato tflite per edge tpu\r\nos.system(\"edgetpu_compiler \\'/home/utente/Scrivania/csvtesi/rete_Nom01_quant.tflite'\")\r\n\r\n\r\n#interpret the model\r\n\r\ninterpreter = tf.lite.Interpreter('/home/utente/Scrivania/csvtesi/rete_Nom01_quant_edgetpu.tflite',experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\r\n\r\n\r\ninterpreter.allocate_tensors()\r\nidt=print(interpreter.get_input_details())\r\nodt=print(interpreter.get_output_details())\r\n\r\n\r\n\r\nfor j in range(5):\r\n  start = time.monotonic()\r\n  o_test=np.arange(len(xAcc1L_test[:,0]))\r\n  o_test=o_test[:,np.newaxis]\r\n  for i in range (len(xAcc1L_test[:,0])):\r\n    input=set_input(interpreter, xAcc1L_test[i,:])\r\n    #print(\"inference input    %s\" % input)\r\n    interpreter.invoke()\r\n    classes = get_output(interpreter,4)\r\n    output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])#/255 con edgetpu\r\n    #print(\"inference output    %s\" % output)\r\n    #print(\"inference classes      %s\" % classes)\r\n    a=np.array([one_hot_labelsAcc1L_test[i,:].argmax(axis=0)])\r\n    b=np.array(output.argmax(axis=1))\r\n    o_test[i]=b\r\n    #if a==b:\r\n      #print('good classification')\r\n    #else: \r\n      #print('bad classification')\r\n  inference_time = time.monotonic() - start\r\n  print('%.1fms ' % (inference_time * 1000))\r\n  #print(o_test)\r\n  print(\"Accuracy:\",accuracy_score(yAcc1L_test,o_test))\r\n```\r\nMy input train dataset is part of csv files and it's a matrix of dimensions = (1756,30) and my input test one is a matrix of (183,30). This is how the data looks like (first two rows):\r\n```\r\n[[-0.283589    -0.0831421  -0.199936   -0.144523   -0.215593   -0.199029   0.0300179   -0.0299262  -0.0759612  -0.0349733  0.102882    -0.00470235 -0.14267    -0.116636   -0.0842867  -0.124638   -0.107917   -0.0995006  -0.222817   -0.256093   -0.121859   -0.130829   -0.186091   -0.174511   -0.0715493  -0.0595195  -0.054914   -0.0362971  -0.0286576  -0.0409128],\r\n[-0.226151  -0.0386177  -0.16834    -0.0768908  -0.166611   -0.161028   0.0493133   -0.00515959 -0.0362308  -0.00723895 0.105943    -0.010825   -0.142335   -0.10863    -0.0634201  -0.112928   -0.0927994  -0.0556194  -0.180721   -0.218341   -0.0934449  -0.100047   -0.134569   -0.119806   -0.0265749  -0.044841   -0.0538225  -0.017408   -0.00528171 -0.0248457]]\r\n```\r\nP.S. I'm using the TensorFlow 2.0 version. I read that there could be problems with this version during the conversion phase but , as you can see in the code, I used a converter compatible with the previous version ` (converter=tf.compat.v1.lite.TFliteConverter.from_keras_model_file()) `.\r\nI also tried to use the downgraded version of TensorFlow( v 1.15) in my code but it didn't make any difference.\r\n", "comments": ["@veronicatorcolacci \r\nCan you follow the [link](https://www.tensorflow.org/lite/guide/ops_select#converting_the_model) for converting the model to TF Lite model and see if it helps you.Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@veronicatorcolacci \r\n\r\nAny update on this issue please. Thanks!", "Not anymore... You can close it\n\nIl gio 27 feb 2020, 02:02 Alfred Sorten Wolf <notifications@github.com> ha\nscritto:\n\n> It has been 15 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36548?email_source=notifications&email_token=AOPLYHDNSWM5XP4F54P2EHTRE4GKLA5CNFSM4KRRL5K2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENCPUOI#issuecomment-591723065>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOPLYHGVWF7TYXWQCRJHVHDRE4GKLANCNFSM4KRRL5KQ>\n> .\n>\n", "Closing the issue since its resolved. Thanks!"]}, {"number": 36547, "title": "Documentation error for tf.signal.frame ", "body": "## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/cf7fcf164c9846502b21cebb7d3d5ccf6cb626e8/tensorflow/python/ops/signal/shape_ops.py#L55-L199\r\n\r\nDocumentation:\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/frame\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the following example, the count of frames generated with pad_end=False is incorrect\r\n    \r\n    # A batch size 3 tensor of 9152 audio samples.\r\n    audio = tf.random.normal([3, 9152])\r\n\r\n    # Compute overlapping frames of length 512 with a step of 180 (frames overlap\r\n    # by 332 samples). By default, only 50 frames are generated since the last\r\n    # 152 samples do not form a full frame.\r\n    frames = tf.signal.frame(audio, 512, 180)\r\n    frames.shape.assert_is_compatible_with([3, 50, 512])\r\n\r\nIn fact, only 49 frames are generated with pad_true=False (the default).\r\n\r\n### Clear description\r\n\r\nSuppose we are given a tensor x with x.shape[-1] == N, a frame_length == K, and a frame_step == k.\r\n\r\nTo compute tf.signal.frame(x, frame_length, frame_step) (here default axis=-1 and pad_end=False), we could equivalently stack along axis -2 the following list of slices:\r\n\r\n    [x[..., 0:K],\r\n     x[..., k:K+k],\r\n     x[..., 2*k:K+2*k],\r\n     x[..., 3*k:K+3*k],\r\n     ...\r\n     x[..., j*k:K+j*k]]\r\n\r\nwhere j is the maximum integer such that K+j*k <= N.\r\n\r\nWe can compute that j = (N - K) // k, so the number of slices in this list is j+1 = 1 + (N - K) // k. \r\n\r\nIn the example here, N = 9152, K=512, k=180, so:\r\n\r\n    j+1 = 1 + (9152-512) // 180 = 1 + 8640 // 180 = 1+48 = 49\r\n\r\nThus in the example given, the return shape should be [3, 49, 512], not [3, 50, 512]. Attached is a screenshot showing that this is indeed the behavior of tf.signal.frame, so it is an error in the documentation not in the code.\r\n\r\n![tf signal frame](https://user-images.githubusercontent.com/47697814/74045729-497e6480-499b-11ea-9107-12305554ee18.png)\r\n\r\n### Submit a pull request?\r\n\r\nI am planning to submit a pull request.", "comments": ["@jmsmdy \r\nas there is a pr to monitor this issue, can we close this issue.", "Closing this issue since the associated PR has been merged. \r\nIt's effective with [TF 2.3](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/signal/shape_ops.py#L58-L216) documentation.\r\nThanks!\r\n"]}, {"number": 36546, "title": "[MLIR] [XLA] lower xla_lhlo::slice to linalg dialect", "body": "add an implementation of lowering xla_lhlo::slice to linalg dialect.", "comments": ["@pifon2a Could you help to have a look?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36546) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36546) for more info**.\n\n<!-- ok -->", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 36545, "title": "[MLIR] [XLA] lower xla_lhlo::slice to linalg dialect", "body": "\r\nadd an implementation of lowering xla_lhlo::slice to linalg dialect.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36545) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 36544, "title": "tpu closed error ", "body": "when I run tpu on colab, I meet error \r\nEpoch 1/6\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\r\n 1/25 [>.............................] - ETA: 28:10\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-25-7386834902ba> in <module>()\r\n     94 \r\n     95 \r\n---> 96         model.fit(train_dataset,epochs=6)\r\n     97 \r\n     98 \r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: Socket closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1581089996.572389050\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}", "comments": ["some code  here `\r\n \r\n        with strategy.scope():\r\n\r\n            model = create_model_a(bert_trainabel=True)\r\n            BATCH_SIZE = 10\r\n            NUM_EPOCHS = 6\r\n            learning_rate = 2e-5\r\n            decay_steps, warmup_steps = calc_train_steps(\r\n                len(df_train.iloc[train_idx]),\r\n                batch_size=BATCH_SIZE,\r\n                epochs=NUM_EPOCHS,\r\n                )\r\n            adamW_opt = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=learning_rate, min_lr=0,)\r\n            model.compile(loss=loss_func , optimizer=adamW_opt)\r\n\r\n        model_weight = f'{model_path}/bert_fold{fold}.h5'\r\n\r\n        # ieval = SpearmanRhoCallback(training_data=train_dataset, validation_data=valid_dataset,\r\n        #                                         patience=1, model_name=model_weight ) \r\n        \r\n\r\n        model.fit(train_dataset,epochs=6) `", "@bestpredicts \r\n\r\nPlease, let us know which TensorFlow version you are using?. Can you please provide us colab link or full code snippet with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> @bestpredicts\r\n> \r\n> Please, let us know which TensorFlow version you are using?. Can you please provide us colab link or full code snippet with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\ntf2.1  ", "@bestpredicts \r\n\r\nCan you please help us with full code snippet to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "@bestpredicts \r\n\r\nAny update on this issue please. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36544\">No</a>\n"]}, {"number": 36543, "title": "ImportError: cannot import name 'feature_column_v2'", "body": "I found below issue when trying to import tensorflow-hub - have no clue with this issue and appreciate your help.\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-8-9ecaff1b481d> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 import tensorflow_hub as hub\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_sessions\\lib\\site-packages\\tensorflow_hub\\__init__.py in <module>()\r\n     27 # error message is thrown instead of an obscure error of missing\r\n     28 # symbols at executing the imports.\r\n---> 29 from tensorflow_hub.estimator import LatestModuleExporter\r\n     30 from tensorflow_hub.estimator import register_module_for_export\r\n     31 from tensorflow_hub.feature_column import image_embedding_column\r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_sessions\\lib\\site-packages\\tensorflow_hub\\estimator.py in <module>()\r\n     23 from absl import logging\r\n     24 import tensorflow as tf\r\n---> 25 from tensorflow_hub import tf_utils\r\n     26 from tensorflow_hub import tf_v1\r\n     27 \r\n\r\nC:\\Program Files\\Anaconda3\\envs\\tensorflow_sessions\\lib\\site-packages\\tensorflow_hub\\tf_utils.py in <module>()\r\n     31 # depending on TensorFlow internal implementation details.\r\n     32 # pylint: disable=g-direct-tensorflow-import\r\n---> 33 from tensorflow.python.feature_column import feature_column_v2\r\n     34 # pylint: enable=g-direct-tensorflow-import", "comments": ["@venkatamaguluri,\r\nLooks like the issue you are facing is related to TF-Hub. Please raise a new issue for the TF-Hub repo from [this](https://github.com/tensorflow/hub/issues/new) link. Thanks!", "@venkatamaguluri,\r\nAny updates regarding this issue? Thanks!", "I have submitted in HUB. Thanks - Venkata"]}, {"number": 36542, "title": "TF 2.1.0 crashes during shape inference", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): standard docker image with ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\n- Python version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- CUDA/cuDNN version: 10.1.243-1 / 7.6.4.38-1+cuda10.1\r\n- GPU model and memory: titan rtx 24220MiB\r\n\r\n**Describe the current behavior**\r\nUsing rather large complex model with lots of tf.TensorArray operations ends up with the crash. 100% reproducible, but overall codebase (python only) is quite large and input pipeline is tricky.\r\n\r\n**Code to reproduce the issue**\r\nThis is the class I use to operate with tensor arrays, which seems to be a reason for crash.\r\nIt works in evaluation mode though\r\n```\r\nclass RNNBatch:\r\n    def __init__(self, rnn_layer, rnn_batch_size, true_words, true_lengths, training):\r\n        self.rnn_batch_size = rnn_batch_size\r\n        self.training = training\r\n        self.dtype = tf.float32\r\n\r\n        self.rnn_layer = rnn_layer\r\n        max_sequence_len = rnn_layer.max_sequence_len\r\n        dictionary_size = rnn_layer.dictionary_size\r\n\r\n        self.true_words = true_words\r\n        self.true_lengths = true_lengths\r\n\r\n        self.written = 0\r\n        self.rnn_processed_start = 0\r\n\r\n        self.output_written = 0\r\n\r\n    def run(self, arrays):\r\n        selected_features = arrays['selected_features'].concat()\r\n\r\n        batch_size = tf.shape(selected_features)[0]\r\n        states_h = tf.zeros((batch_size, self.rnn_layer.num_rnn_units), dtype=self.dtype)\r\n        states_c = tf.zeros((batch_size, self.rnn_layer.num_rnn_units), dtype=self.dtype)\r\n        states = [states_h, states_c]\r\n\r\n        tw = self.true_words[self.rnn_processed_start : self.rnn_processed_start + self.written, ...]\r\n        tl = self.true_lengths[self.rnn_processed_start : self.rnn_processed_start + self.written, ...]\r\n\r\n        out, out_ar = self.rnn_layer(selected_features, tw, tl, states, self.training)\r\n\r\n        arrays['outputs'] = arrays['outputs'].write(self.output_written, out)\r\n        arrays['outputs_ar'] = arrays['outputs'].write(self.output_written, out_ar)\r\n        self.output_written += 1\r\n\r\n        self.rnn_processed_start += self.written\r\n        self.written = 0\r\n\r\n        return arrays\r\n\r\n    def feed_crop(self, cropped_features, arrays):\r\n        arrays['selected_features'] = arrays['selected_features'].write(self.written, cropped_features)\r\n        self.written += 1\r\n\r\n        if self.written == self.rnn_batch_size:\r\n            arrays = self.run(arrays)\r\n            arrays['selected_features'] = tf.TensorArray(cropped_features.dtype, size=0, element_shape=tf.TensorShape([None] + [cropped_features.shape[1:]]))\r\n\r\n\r\n        return arrays\r\n\r\n    def return_values(self, arrays):\r\n        if self.written != 0:\r\n            arrays = self.run(arrays)\r\n\r\n        return arrays['outputs'].concat(), arrays['outputs_ar'].concat()\r\n\r\n```\r\n\r\nAnd the use case is something like this:\r\n```\r\nimages -> huge tensor of data ->\r\n        rnn_batch = RNNBatch(self.rnn_layer, 64, true_words, true_lengths, training)\r\n        arrays = {\r\n            'selected_features': tf.TensorArray(dtype, size=0, dynamic_size=True, element_shape=tf.TensorShape([None, crop_size, crop_size, features_full.shape[3]])),\r\n            'outputs': tf.TensorArray(dtype, size=0, dynamic_size=True, element_shape=tf.TensorShape([None, self.max_sequence_len, self.dictionary_size])),\r\n            'outputs_ar': tf.TensorArray(dtype, size=0, dynamic_size=True, element_shape=tf.TensorShape([None, self.max_sequence_len, self.dictionary_size])),\r\n        }\r\n\r\n        for idx in tf.range(batch_size):\r\n            for crop_idx in tf.range(num_crops):\r\n                  arrays = rnn_batch.feed_crop(cropped_features, arrays)\r\n```\r\n\r\n**Other info / logs**\r\nFull traceback: [tf.traceback.txt](https://github.com/tensorflow/tensorflow/files/4171246/tf.traceback.txt)\r\nThere is also a core file, gzipped size is about 749M.\r\n\r\nWhat should be the next steps (besides rewriting above class) to debug this problem?\r\n\r\nAttached stack trace which ends up like this:\r\n```\r\nThread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fff647051e0 in tensorflow::(anonymous namespace)::{lambda(tensorflow::shape_inference::InferenceContext*)#14}::_FUN(tensorflow::shape_inference::InferenceContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n(gdb) bt                                             \r\n#0  0x00007fff647051e0 in tensorflow::(anonymous namespace)::{lambda(tensorflow::shape_inference::InferenceContext*)#14}::_FUN(tensorflow::shape_inference::InferenceContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fff5d5a6104 in std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>::_M_invoke(std::_Any_d\r\nata const&, tensorflow::shape_inference::InferenceContext*&&) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff5a018c22 in tensorflow::shape_inference::InferenceContext::Run(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)> const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n#3  0x00007fff64342a14 in tensorflow::ShapeRefiner::RunShapeFn(tensorflow::Node const*, tensorflow::OpRegistrationData const*, tensorflow::ExtendedInferenceContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff643443ee in tensorflow::ShapeRefiner::AddNode(tensorflow::Node const*) () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff5df740f1 in TF_FinishOperation () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fff5d529dd6 in _wrap_TF_FinishOperation () from /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\nUnfortunately there is no debug package I could use to make better trace ", "comments": ["Bug happens during augraph tracing time and is related to the typo in the second string:\r\narrays['outputs'] = arrays['outputs'].write(self.output_written, out)\r\narrays['outputs_ar'] = arrays['outputs'].write(self.output_written, out_ar)", "@bioothod, Can you provide the complete code to replicate the reported issue. Thanks!", "@bioothod, Any update", "This is a bit tricky process to run this large chunk of code, I can put both model and data online, but can you instead release package with debug symbols? It will be order of magnitude faster getting that I have all setup handy, I will also make a core", "Have you considered using [tensorboard debugger](https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/debugger#the-debugger-dashboard) for this case?", "Hmm, how can it help in catching segmentation fault?\r\n\r\nLet me try to export the whole pipeline for this, it should not be hard to reproduce when everything is in place", "I will close it for now, because it looks like this should be fixed here: https://github.com/tensorflow/tensorflow/issues/33862", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36542\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36542\">No</a>\n"]}, {"number": 36541, "title": "Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- TensorFlow installed from (source or binary): Anaconda 4.8.2\r\n- TensorFlow version (use command below): 2.0.0\r\n- TensorrrFlow-probability version: 0.8.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nHi everyone,\r\nthi is my first tensorflow project so I'm not really experienced with this API. At the moment I get the following error message when   using the funtion \r\n\r\ntf.feature_column.categorical_column_with_vocabulary_list(key=key,\r\n                                                                                    vocabulary_list=unique_values)\r\n\r\n**TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.**\r\n\r\nThis error is raised in feature_column_v2.py in the line 1780\r\nif len(set(vocabulary_list)) != len(vocabulary_list):\r\n\r\nThe invalid part here is set(vocabulary_list)\r\n\r\nI checked already if I have compatibility issues with tensorflow-probability version which was recommended in https://github.com/tensorflow/probability/issues/540 and made sure to have both tensorflow and tensorflow-probability up to date. But the error still occures.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe function should create a categorical colun out of a tensor.\r\n\r\n**Code to reproduce the issue**\r\nHere is my code:\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef data_preprocessing():\r\n    print(tf.__version__)\r\n    \r\n    train_file_path = 'Learning Data Input/TrainingData.csv'\r\n       \r\n    # Feature to be predicted\r\n    label_column = 'PARTSET0'  \r\n\r\n    raw_training_data = get_dataset(train_file_path, label_column)\r\n  \r\n    show_batch(raw_training_data)\r\n    \r\n\r\n    numeric_columns, categorical_columns = get_feature_columns(raw_training_data)\r\n    example_batch, labels_batch = next(iter(raw_training_data))\r\n\r\n    preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)\r\n    print(preprocessing_layer(example_batch).numpy()[0])\r\n    return\r\n\r\n\r\ndef get_dataset(file_path, label_column, **kwargs):\r\n    dataset = tf.data.experimental.make_csv_dataset(file_path, label_name=label_column, batch_size=10, field_delim=';',\r\n                                                    **kwargs)\r\n    return dataset\r\n\r\n\r\ndef show_batch(dataset):\r\n    for batch, label in dataset.take(1):\r\n        for key, value in batch.items():\r\n            print(\"{:20s}: {}\".format(key, value.numpy()))\r\n    return\r\n\r\n\r\ndef get_feature_columns(raw_data):\r\n    numeric_columns = []\r\n    categorical_columns = []\r\n\r\n    for batch, label in raw_data.take(1):\r\n        for key, value in batch.items():\r\n            unique_values = []\r\n            for unique_value in value:\r\n                if len(unique_values) == 0:\r\n                    unique_values.append(unique_value)\r\n                elif unique_value not in unique_values:\r\n                    unique_values.append(unique_value)\r\n                else:\r\n                    continue\r\n            if value.dtype == 'int32' or value.dtype == 'int64' or value.dtype == 'float32' or value.dtype == 'float64':\r\n                num_col = tf.feature_column.numeric_column(key=key, shape=len(unique_values))\r\n                numeric_columns.append(tf.feature_column.indicator_column(num_col))\r\n            elif value.dtype == 'string':\r\n                cat_col = tf.feature_column.categorical_column_with_vocabulary_list(key=key,\r\n                                                                                    vocabulary_list=unique_values)\r\n                categorical_columns.append(tf.feature_column.indicator_column(cat_col))\r\n            else:\r\n                print(\"Undefined column category\" + key)\r\n    return numeric_columns, categorical_columns\r\n\r\n**Other info / logs**\r\nAny help is appreciated. thank you!\r\n", "comments": ["Fixed the issue:\r\n\r\nAn eager Tensor was given to the function due to my previous operations. This does not work instead I had to transform this eager Tensor format values into a list. The code for the get_feature_columns() looks now as follows:\r\n\r\ndef get_feature_columns(raw_data):\r\n    numeric_columns = []\r\n    categorical_columns = []\r\n    for batch, label in raw_data.take(1):\r\n        for key, value in batch.items():\r\n            unique_values = []\r\n            for unique_value in value:\r\n                if len(unique_values) == 0:\r\n                    unique_values.append(unique_value)\r\n                elif unique_value not in unique_values:\r\n                    unique_values.append(unique_value)\r\n                else:\r\n                    continue\r\n            if value.dtype == 'int32' or value.dtype == 'int64' or value.dtype == 'float32' or value.dtype == 'float64':\r\n                num_col = tf.feature_column.numeric_column(key=key, shape=len(unique_values))\r\n                numeric_columns.append(tf.feature_column.indicator_column(num_col))\r\n            elif value.dtype == 'string':\r\n                unique_values = list(filter(None, unique_values))\r\n                vocabulary = []\r\n                for unique_value in unique_values:\r\n                    vocabulary.append(format(unique_value)[2:-1])\r\n                cat_col = tf.feature_column.categorical_column_with_vocabulary_list(key=key,\r\n                                                                                    vocabulary_list=vocabulary)\r\n                categorical_columns.append(tf.feature_column.indicator_column(cat_col))\r\n            else:\r\n                print(\"Undefined column category\" + key)\r\n    return numeric_columns, categorical_columns"]}, {"number": 36540, "title": "Can't compute distributed gradient in 2 computers with GradientTape", "body": "- TensorFlow version 2.0.0\r\n\r\nI am training 2 cascading models, model1 and model2, to be used in 2 different computers. Currently I am simulating it in the same computer. If I train the models in the same gradient tape, it trains perfectly, computing the gradients as:\r\n\r\n        import tensorflow as tf\r\n        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            features = model1(x_batch_train)\r\n            logits = model2(features)\r\n            loss_value = loss_fn(y_batch_train, logits)\r\n\r\n        # Computing gradients for model 2\r\n        grads2 = tape.gradient(loss_value, model2.trainable_variables)\r\n        # Computing gradients to pass to model 1\r\n        grads_pass = tape.gradient(loss_value, features)\r\n        # Computing gradients for model 1\r\n        grads1 = tape.gradient(features, model1.trainable_variables, output_gradients=grads_pass)\r\n\r\nHowever, if I want to split the training into 2 different computers, I should have a gradient tape in each one of them, but then the results computed are different!\r\n\r\n        import tensorflow as tf\r\n        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n        with tf.GradientTape() as tape1:\r\n            features = model1(x_batch_train)\r\n        \r\n        with tf.GradientTape(persistent=True) as tape2:\r\n            logits = model2(features)\r\n            loss_value = loss_fn(y_batch_train, logits)\r\n\r\n        # Computing gradients for model 2\r\n        grads2 = tape2.gradient(loss_value, model2.trainable_variables)\r\n        # Computing gradients to pass to model 1\r\n        grads_pass = tape2.gradient(loss_value, features)\r\n        # Computing gradients for model 1\r\n        grads1 = tape1.gradient(features, model1.trainable_variables, output_gradients=grads_pass)\r\n\r\nIt would be very helpful to have a way to be able to compute gradients in different tapes synchronizing them somehow.\r\n\r\nThanks in advance", "comments": ["@jorgeraulgomez \r\n\r\nCan you please provide us colab link or simple stand alone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Yes, sure\r\n\r\nhttps://drive.google.com/file/d/1uCGFklj4EoyrlltgXeOGudMsEGtULbXm/view?usp=sharing\r\n\r\nThank you very much!", "@jorgeraulgomez Please take a look at this [doc](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md) where it explains how to use tensorflow in a distributed mode. Thanks!", "Thank you for your answer. It is a very interesting document.\r\n\r\nHowever, I don't think it solves my problem. I am currently executing it in the same notebook, so I am not distributing it yet. This will be a problem of my future me, and I will address it, but for now I would need to be able to execute it in the same notebook.\r\n\r\nRegards", "@jorgeraulgomez If you are planning to work on the distributed mode then you have to follow the Distributed Tensorflow concept.\r\n\r\nAnyways I am going to close this issue for now and if you encounter any issues in future, create a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36540\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36540\">No</a>\n"]}, {"number": 36538, "title": "Could not download data for \"data = tensorflow_datasets.load('glue/mrpc')\"", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\nThere is nothing about this in stackflow, Thanks!\r\n\r\nSince I execute \"data = tensorflow_datasets.load('glue/mrpc')\" for downloading data. It occurs:\r\n\r\nConnectionError: HTTPSConnectionPool(host='firebasestorage.googleapis.com', port=443): Max retries exceeded with url: /v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fa45008db38>: Failed to establish a new connection: [Errno 101] Network is unreachable',))\r\n\r\nI am from China, so maybe I have a useless network for this.\r\nCould you please tell me how to load this mprc data by offline? I have download them and keep files like \"msr_paraphrase_train.txt\" and I also turned them to tsv like:\r\n\r\nQuality\u00a0#1 ID\u00a0#2 ID\u00a0#1 String\u00a0#2 String | \u00a0\r\n-- | --\r\n1\u00a01355540\u00a01355592\u00a0He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\u00a0\" The foodservice pie business does not fit our long-term growth strategy . | \u00a0\r\n0\u00a02029631\u00a02029565\u00a0Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war .\u00a0His wife said he was \" 100 percent behind George Bush \" and looked forward to using his years of training in the war . | \u00a0\r\n\r\nThank you very much!\r\n\r\n\r\n", "comments": ["@zysNLP, If it is text file, open the text using python as \r\n`file = open('msr_paraphrase_train.txt','r')`.\r\nThanks!", "@gadagashwini Thank you. Do you mean this \"file = open(\"mspr.txt', 'r')\" is equal to \"data = tensorflow_datasets.load('glue/mrpc')\"? So here \"file\" and \"data\" share the same type?\r\n\r\nExcuse me for ask this sample question since I have not execute tensorflow_datasets successfully.", "@zysNLP, TensorFlow Datasets exposes the public research datasets as `tf.data.Datasets` for more read [here](https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html). \r\nfile = open(\"mspr.txt', 'r'), this reads the content of text file later can be used as train and test dataset. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36537, "title": "load_weights problem in Tensorflow.keras.", "body": "Environment\r\nWin10\r\nConda python 3.7\r\ntensorflow version is 2.0.0.\r\n\r\nUsing `ModelCheckpoint` and `save_weights_only=True` to save the weights of best model when training model. Using `model.to_json()` save the structure of model.\r\n\r\nIt's successful to loading model structure from json file, the `model.summary()` is work.\r\nBut the problem is when I use `model.load_weigths(weight_path)`, I get `<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2d871057cc8>\r\n`. And the `summary()` is not work and I cannot use `model.predict` function in this return.\r\n\r\nLoad model structure from json is work. no problem.\r\n![A](https://user-images.githubusercontent.com/42731603/74031411-668f5380-49ec-11ea-98ea-ec173d5b2f6f.PNG)\r\n\r\nLoad weights, this problem happen here.\r\n![B](https://user-images.githubusercontent.com/42731603/74031416-6a22da80-49ec-11ea-9839-da8df2e2b891.PNG)\r\n\r\nMy weight folder, it from checkpoint and save weights only is true.\r\n![C](https://user-images.githubusercontent.com/42731603/74031421-6bec9e00-49ec-11ea-92f2-680263d47f88.PNG)\r\n", "comments": ["@houzeyu2683, Please provide the standalone code to replicate the issue. Thanks!", "> @houzeyu2683, Please provide the standalone code to replicate the issue. Thanks!\r\n\r\nSorry, I make mistake, it work now, I will delete this comment later, thanks.", "Closing as it is resolved. "]}, {"number": 36536, "title": "Failed build on Windows with clang-cl", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 1cd499a7fc0e782cf2a2fe88eb2e64783e631b2b\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): clang-cl\r\n- CUDA/cuDNN version: ---\r\n- GPU model and memory: ---\r\n\r\n**Describe the problem**\r\nconfigure: No XLA / ROCm / CUDA. No \"disable inline\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build -s --config opt --define=no_tensorflow_py_deps=true --compiler=clang-cl //tensorflow:tensorflow\r\n```\r\n\r\nBuild failed on final step: linking tensorflow.dll\r\n\r\n**Any other info / logs**\r\n```\r\nSUBCOMMAND: # //tensorflow:tensorflow.dll [action 'Linking tensorflow/tensorflow.dll', configuration: 9b882a621aaf289d03959a0bb55e368bbb8a258a98684e2f2cbeba1e04f08296]\r\ncd F:/.bazel/tf-llvm/execroot/org_tensorflow\r\n  SET LIB=c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\VC\\Tools\\MSVC\\14.25.28508\\ATLMFC\\lib\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\VC\\Tools\\MSVC\\14.25.28508\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;;c:\\bin\\LLVM\\lib\\clang\\9.0.0\\lib\\windows\r\n    SET PATH=c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\VC\\Tools\\MSVC\\14.25.28508\\bin\\HostX64\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\IDE\\VC\\VCPackages;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\MSBuild\\Current\\bin\\Roslyn;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Team Tools\\Performance Tools\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\IDE\\;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\Tools\\;;C:\\WINDOWS\\system32;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Preview\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/K13/NS/tf-llvm/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=C:/K13/NS/tf-llvm/Lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=F:\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TMP=F:\\Temp\r\n  c:/bin/LLVM/bin/lld-link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll-2.params\r\nINFO: From Linking tensorflow/tensorflow.dll:\r\nlld-link: warning: ignoring unknown argument '-lpthread'\r\nlld-link: warning: ignoring unknown argument '-lm'\r\nlld-link: warning: ignoring unknown argument '-lpthread'\r\nlld-link: warning: ignoring unknown argument '-lm'\r\nERROR: F:/lib/tensorflow-llvm/tensorflow/BUILD:623:1: output 'tensorflow/tensorflow.dll' was not created\r\nERROR: F:/lib/tensorflow-llvm/tensorflow/BUILD:623:1: not all outputs were created or valid\r\nTarget //tensorflow:tensorflow failed to build\r\nINFO: Elapsed time: 9411.542s, Critical Path: 3438.48s\r\nINFO: 1964 processes: 1964 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nBut after repeating same command (with lld-link.exe replaced to LINK.EXE) -- success:\r\n```\r\n(tf-llvm) F:\\.bazel\\tf-llvm\\execroot\\org_tensorflow>LINK.EXE @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll-2.params\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll.if.lib and object bazel-out/x64_windows-opt/bin/tensorflow/tensorflow.dll.if.exp\r\n..... (skip warning) .....\r\n\r\n```", "comments": ["@Korsar13 \r\n\r\nPlease, let us know which version of clang-cl(gcc) you are using? Thanks!", "```\r\nC:\\bin\\LLVM\\bin>clang-cl --version\r\nclang version 9.0.0 (tags/RELEASE_900/final)\r\nTarget: x86_64-pc-windows-msvc\r\nThread model: posix\r\nInstalledDir: C:\\bin\\LLVM\\bin\r\n```", "@Korsar13,\r\nIs this still an issue?\r\n\r\nCould you please follow [this guide](https://www.tensorflow.org/install/source_windows) and let us know if you are facing the same issue with TF v2.4.1 as well? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36536\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36536\">No</a>\n"]}, {"number": 36535, "title": "Tensorflow-gpu running issue: CUDA_UNKNOWN_ERROR", "body": "With:\r\n\r\n* Tensorflow-gpu 2.0.0\r\n* Windows 10 environment\r\n* NVIDIA GTX 1050 gpu\r\n* cuda 10.0 and corresponding cudnn 7.6.5\r\n\r\nI followed the official TF documentation for TF-gpu and I've tried to create and fit a simple CNN model (on a.py file, I've tried with a jupyter but the kernel always dies) but I got the following:\r\n\r\n```\r\n2020-02-06 23:57:14.420911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-02-06 23:57:16.081396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-02-06 23:57:16.861094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\n2020-02-06 23:57:16.861492: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-02-06 23:57:16.862290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2020-02-06 23:58:14.322053: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-02-06 23:58:14.324900: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error\r\n```\r\n", "comments": ["@tawej \r\n\r\nCan you please go through the [issue](https://github.com/tensorflow/tensorflow/issues/19266#issuecomment-399686258) and see if it helps you. If those solutions doesn't work, could you uninstall and reinstall CUDA and cuDNN? Please let us know how it progresses. Also, try to uninstall and reinstall tensorflow-gpu and try following the instructions from [TensorFlow website.](https://www.tensorflow.org/install/source_windows#setup_for_windows)\r\nAlso, see \r\nSee https://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/ . Thanks!", "Hello @ravikyram \r\nThe attached issue is for ubuntu system while mine is windows 10....\r\nThis said I tried to uninstall all and reinstall but it seems to be persisting...", "I've tried to install python 3.6.8 but still having the same error message. Would you recommend working with TF 2.1.0 and cuda 10.1 or not? ", "@tawej Did you install all compatible drivers as mentioned in TF website.\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.0.0 | 3.5-3.7 | MSVC 2017 | Bazel 0.26.1 | 7.4 | 10\r\n\r\nCan you please try some instructions mentioned in this [resolved issue](https://github.com/tensorflow/tensorflow/issues/26227) and [this issue](https://github.com/tensorflow/tensorflow/issues/26182). Also check [this resource](https://github.com/tensorflow/tensorflow/issues/26182#issuecomment-472674508). Windows10 has another issue limiting path length of a folder. Please check [this resource](https://github.com/tensorflow/tensorflow/issues/24705#issuecomment-456498716) for resolving that kind of problems.\r\n\r\nPlease let us know how it progresses. I would also suggest you to go ahead and install TF2.1 which is better than 2.0 interms of performance. Thanks!", "Thanks I'll keep you updated\r\n", "python 3.7.6\r\nTF-gpu 2.1.0\r\ncuda 10.1\r\ncudnn 7.6.5 for cuda 10.1\r\nMSVC 2017\r\nIt's working, at least with a tf basic mnist classification :) ", "@tawej I think it should work with other complex models also. I guess there might have been some issue with compatibility between different dependencies. Thanks!\r\n\r\nI will close this issue as it was resolved. Feel free to reopen if the issue persists again. Thanks!", "> \r\n> \r\n> python 3.7.6\r\n> TF-gpu 2.1.0\r\n> cuda 10.1\r\n> cudnn 7.6.5 for cuda 10.1\r\n> MSVC 2017\r\n> It's working, at least with a tf basic mnist classification :)\r\n\r\nThank you for this. I had the same issue and I reinstalled everything from your list. When I tried again I had the exact same issue again. But once I restarted my laptop it all worked again", "Great ! Adding environment variables requires restart ;) "]}, {"number": 36534, "title": "Tflite micro: Fix #36533 (segfault after failing to allocate tensors in MicroAllocator)", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36534) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36534) for more info**.\n\n<!-- ok -->", "Would be good to add a unit test to prevent regression. but doesn't has to be in this PR.", "@exFalso Can you please address reviewer comments and Ubuntu Sanity errors. Thanks!"]}, {"number": 36533, "title": "Tflite micro: segfault after failing to allocate tensors in MicroAllocator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, NixOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: nope\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): latest master\r\n- Python version: C++\r\n- Bazel version (if compiling from source): nope, Make\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: nope\r\n- GPU model and memory: nope\r\n\r\n**Describe the current behavior**\r\n\r\n`tflite::MicroInterpreter` constructor segfaults\r\n\r\n**Describe the expected behavior**\r\n\r\nit shouldn't segfault\r\n\r\n**Code to reproduce the issue**\r\n\r\ngrrr call MicroInterpreter constructor with a small arena_size\r\n\r\n**Other info / logs**\r\nFix: https://github.com/tensorflow/tensorflow/blob/753ceb91fc8472a5d9f1293ba97814c04c3a4d4c/tensorflow/lite/micro/micro_allocator.cc#L345 this conditional should return an error code instead of continuing. PR coming up", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36533\">No</a>\n"]}, {"number": 36532, "title": "Win10 install: ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Education, Version 1903 (OS Build 18362.592)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip (in a conda environment, conda 4.8.2)\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: none\r\n\r\n**Steps to reproduce:**\r\n\r\nI ran into the issue after installing regular `tensorflow`. Since some reports seem to indicate that cudnn/cuda version incompatibility could be an issue (even though I don't actually have a GPU on this system), I then installed `tensorflow-cpu` (in a fresh environment). The problem persists.\r\n\r\n```\r\nconda create -n tf-cpu-test python=3.7\r\nconda activate tf-cpu-test\r\npip -V\r\npip install tensorflow_cpu\r\npython\r\n```\r\nAlso tried with `pip install tensorflow`, `pip install tensorflow-cpu`\r\nIn Python: \r\n```\r\nimport tensorflow as tf\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Software\\Anaconda37\\envs\\tf-cpu-test\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["@meowcat \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).Please, refer #36167 and see if it helps you . Thanks!", "Thanks! The vc2019 redist was the issue. Problem solved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36532\">No</a>\n"]}, {"number": 36531, "title": "Allow datasets to provide the number of examples they contain", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently there is no good way to get to the number of samples or batches contained by a dataset although the information is usually available.\r\n\r\nWhat you can do: `sum(1 for _ in dataset)` but this might not do what one wants:\r\nWhen the dataset is batched it will return the number of batches including the trailing one. `MultiWorkerMirroredStrategy` can't handle that.\r\n\r\nUsually this information is already available, see e.g. https://github.com/tensorflow/datasets/issues/1403\r\n\r\n**Will this change the current api? How?**\r\n\r\nAdd a member `num_examples` and/or an overload for `__len__`\r\n\r\n**Who will benefit with this feature?**\r\n\r\n- Everyone using `MultiWorkerMirroredStrategy`\r\n- Everyone using `steps_per_epoch`\r\n- TF itself as the number of samples/batches is known before executing the training loop avoid status reports like `10/Unknown`\r\n- This would help to provide correct behavior in https://github.com/tensorflow/tensorflow/commit/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747\r\n\r\n**Any Other info.**\r\n\r\nThere is an experimental op `cardinality` which might be very related. However it often (always?) returns \"Unknown\". Tested with MNIST from TFDS.", "comments": ["For more context, TFDS cannot provides the `tf.data.Dataset` cardinality because it is not supported by `TFRecordDataset` and (maybe) `interleave` op. If there was a way to manually overwrite the cardinality of a `tf.data.Dataset`, we could forward the number of examples to the `tf.data.Dataset`.\r\n\r\nRelated issue: https://github.com/tensorflow/datasets/issues/1456", "Thanks to jsmira, this should be fixed in https://github.com/tensorflow/tensorflow/commit/d25235b1964991dd331de7edef6be4f2c74fd7a4 with `tf.data.experimental.assert_cardinality(123)`\r\n\r\n```\r\nds = tf.data.TFRecordDataset(\"examples.tfrecord\")\r\ntf.data.experimental.cardinality(ds)  # tf.data.experimental.UNKNOWN_CARDINALITY\r\n\r\nds = ds.apply(tf.data.experimental.assert_cardinality(42))\r\ntf.data.experimental.cardinality(ds).numpy()  # 42\r\n```\r\n\r\nI'll update the TFDS side. But this issue can be closed."]}, {"number": 36530, "title": "Distributed training time by MirroredStrategy varies greatly between Keras API and Custom training loop", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\nTF 2.0\r\n- Python version:\r\nPython3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA10\r\n- GPU model and memory:\r\nV100 * 8 \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe training time by MirroredStrategy in Keras API works fine. The training time is almost linear to the number GPU_num. However, the training time in the custom training loop takes longer, and [2 4 8] GPUs takes the same time in training. Please note that both codes are copied from the tf tutorial with a little bit of modification. The result and source codes are as flows.\r\n\r\n\r\n**Describe the expected behavior**\r\nPlease Provide any bug I had in code or tf bug in the case of experimental support in custom training loop.\r\n![image](https://user-images.githubusercontent.com/33815430/74011734-81010700-49c3-11ea-8c2d-0a505903c6c7.png)\r\n \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nCustom API\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\n# Helper libraries\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport tensorflow_datasets as tfds\r\nfrom datasets.readtf_utils.dataset import get_dataset \r\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D\r\n\r\n\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\ndef assemble_model(num_classes, model_name='MobileNetV2'):\r\n    import tensorflow as tf \r\n    base_model = tf.keras.applications.ResNet50(input_shape=(224,224,3),\r\n                                                    weights='imagenet',\r\n                                                    include_top=False)\r\n    model = tf.keras.Sequential([\r\n                                base_model,\r\n                                GlobalAveragePooling2D(),\r\n                                Dense(num_classes, activation='softmax')\r\n                                ])\r\n    model.trainable = True\r\n    return model\r\n\r\n\r\nprint(tf.__version__)\r\n\r\n\r\ngpus = [1,2,4,8]\r\n\r\nfor setting_GPUs_num in gpus:\r\n    devices = [\"/gpu:\"+str(i) for i in range(setting_GPUs_num)]\r\n\r\n    strategy = tf.distribute.MirroredStrategy(devices)\r\n    print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n    BATCH_SIZE_PER_REPLICA = 128\r\n    # GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA\r\n    EPOCHS = 5\r\n\r\n\r\n    ##-----dataset------##\r\n    tfrecords_dir = \"/data121/lijiayuan/test/classify_flowers/datasets\"\r\n    train_ds, classes_num = get_dataset(tfrecords_dir, subset=\"train\", batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n    train_ds = strategy.experimental_distribute_dataset(train_ds)\r\n\r\n\r\n\r\n\r\n \r\n\r\n\r\n    with strategy.scope():\r\n      # Set reduction to `none` so we can do the reduction afterwards and divide by\r\n      # global batch size.\r\n      loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n          reduction=tf.keras.losses.Reduction.NONE)\r\n      # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\r\n      def compute_loss(labels, predictions):\r\n        per_example_loss = loss_object(labels, predictions)\r\n        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n\r\n    with strategy.scope():\r\n\r\n      train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n          name='train_accuracy')\r\n\r\n    # model and optimizer must be created under `strategy.scope`.\r\n    with strategy.scope():\r\n      model = assemble_model(num_classes=classes_num)\r\n      optimizer = tf.keras.optimizers.Adam()\r\n      checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\r\n\r\n    with strategy.scope():\r\n      def train_step(inputs):\r\n        images, labels = inputs\r\n\r\n        with tf.GradientTape() as tape:\r\n          predictions = model(images, training=True)\r\n          loss = compute_loss(labels, predictions)\r\n\r\n        gradients = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n        train_accuracy.update_state(labels, predictions)\r\n        return loss \r\n\r\n\r\n\r\n\r\n    with strategy.scope():\r\n      # `experimental_run_v2` replicates the provided computation and runs it\r\n      # with the distributed input.\r\n      @tf.function\r\n      def distributed_train_step(dataset_inputs):\r\n        per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                          args=(dataset_inputs,))\r\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                               axis=None)\r\n     \r\n     \r\n      times = []\r\n      for epoch in range(EPOCHS):\r\n        # TRAIN LOOP\r\n\r\n        total_loss = 0.0\r\n        num_batches = 0\r\n        epoch_start = time.time()\r\n        for x in train_ds:\r\n          total_loss += distributed_train_step(x)\r\n          num_batches += 1\r\n        train_loss = total_loss / num_batches\r\n        epoch_end = time.time()\r\n        \r\n        if epoch != 0:\r\n          times.append(epoch_end-epoch_start)\r\n        \r\n\r\n\r\n\r\n        template = (\"Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, \"\r\n                    \" Takes: {:.2f}\")\r\n        print (template.format(epoch+1, train_loss,\r\n                               train_accuracy.result()*100, \r\n                               epoch_end-epoch_start))\r\n\r\n        train_accuracy.reset_states()\r\n      print(\"{} GPUs takes average {:.2f} secs\".format(setting_GPUs_num, \r\n                                                        sum(times)/(EPOCHS-1)))\r\n\r\n\r\n```\r\n\r\nKeras API:\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport time\r\nimport datetime\r\n\r\nimport argparse\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport os\r\nimport numpy as np\r\n\r\nfrom functools import partial\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.datasets.fashion_mnist import load_data\r\nimport tensorflow_datasets as tfds\r\nimport read_params\r\nfrom train_config import configure_model, configure_optimizer, configure_lossfunc\r\nfrom datasets.readtf_utils.dataset import get_dataset \r\nfrom datasets.readtf_utils.dataset import _parse_fn\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n\r\nnum_epochs = 5\r\nbatch_size_per_replica = 128\r\nlearning_rate = 0.001\r\nsetting_GPUs_num = 4\r\ndevices = [\"/gpu:\"+str(i) for i in range(setting_GPUs_num)]\r\n\r\nstrategy = tf.distribute.MirroredStrategy(devices)\r\nGPUs_num = strategy.num_replicas_in_sync\r\nprint('Number of devices: %d' % GPUs_num)  # \u8f93\u51fa\u8bbe\u5907\u6570\u91cf\r\n\r\n# batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\r\nbatch_size = batch_size_per_replica\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\n\r\n\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\n    def on_train_begin(self, logs={}):\r\n        self.train_begin = time.time()\r\n        self.times = []\r\n        # print('trian begins at {}'.format(self.train_begin))\r\n        # d\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        self.epoch_begin = time.time()\r\n        # print('epoch: {} begins at {}'.format(epoch, self.epoch_begin))\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        self.epoch_end = time.time()\r\n        self.epoch = epoch\r\n        # print('epoch: {} ends at {}'.format(epoch, self.epoch_end))\r\n        self.times.append(self.epoch_end-self.epoch_begin)\r\n        print(\" epoch: {} takes: {}\".format(epoch, self.epoch_end-self.epoch_begin))\r\n\r\n    def on_train_end(self, epoch, logs=None):\r\n        self.train_end = time.time()\r\n        # print('training takes {} secs/epoch: '.format((self.train_end - self.train_begin)/self.epoch))\r\n        print('training takes average {:.2f} secs/epoch'.format(sum(self.times[1::]) / (self.epoch)))\r\n\r\n\r\n\r\ntfrecords_dir = \"/data121/lijiayuan/test/classify_flowers/datasets/\"\r\ndataset, _ = get_dataset(tfrecords_dir, subset=\"train\", batch_size=batch_size)\r\nend = time.time()\r\n\r\n\r\n\r\nif GPUs_num == 1:\r\n    model = tf.keras.applications.MobileNetV2()\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n    )\r\nelse:\r\n    with strategy.scope():\r\n        model = tf.keras.applications.MobileNetV2()\r\n        model.compile(\r\n            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\r\n            loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n            metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n            # run_eagerly=False\r\n        )\r\n\r\n\r\nstart = time.time()\r\nmodel.fit(dataset, epochs=num_epochs, callbacks=[MyCustomCallback()])\r\n\r\nprint(\"{} GPUs takes {:.2f} secs/epoch = {:.2f} mins/epoch\".format(strategy.num_replicas_in_sync, \r\n                                                                    (end-start)/num_epochs, \r\n                                                                    (end-start)/60/num_epochs))\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi, TF bot. No one helps me ...", "@JiayuanSternLi \r\n\r\nThank you for posting a detailed summarization! To debug performance issues such as the one above, request you to provide us with xprof traces using this [link](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#profiler_service).Thanks!", "@JiayuanSternLi \r\n\r\nAny update on this issue please. Thanks!", "> @JiayuanSternLi\r\n> \r\n> Any update on this issue please. Thanks!\r\n\r\nHey, I have been working on Profiler visualization using tensorboard since last time you suggested. However, kerasAPI works fine with profiler but custom training loop takes long time to excute profiler is even killed. I submit another issue here:\r\nhttps://github.com/tensorflow/tensorboard/issues/3288", "@JiayuanSternLi \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36529, "title": "the tf c version doesn't work when it contains LSTM block, until replacing LSTM with others. It reply that op doesn't register. The tf.contrib module don't work. The tensorflow c version is r1.13. here is the error information: Not found: Op type not registered 'LSTMBlockCell' in binary running on t640_m160p36. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib. accessing(e.g)'tf.contrib.resampler' should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@18970901926, Please fill the issue [Template](https://github.com/tensorflow/tensorflow/issues/new/choose) and try to reduce the issue title length. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]