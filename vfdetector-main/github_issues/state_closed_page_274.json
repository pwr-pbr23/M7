[{"number": 46161, "title": "micro: port op LEAKY_RELU from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator LEAKY_RELU from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro making minimal changes and not including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46161\">No</a>\n"]}, {"number": 46160, "title": "micro: copy op SPACE_TO_DEPTH kernel from lite", "body": "Copy the kernel and test for operator SPACE_TO_DEPTH from\r\ntensorflow/lite/kernels at 49524d6 without making modifications.\r\nAdapt to micro and add to the build later.\r\n\r\nThis PR is part of the work to port operator SPACE_TO_DEPTH\r\nfrom lite to micro, as tracked in issue #45824.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46159, "title": "micro: copy op DEPTH_TO_SPACE kernel from lite", "body": "Copy the kernel and test for operator DEPTH_TO_SPACE from\r\ntensorflow/lite/kernels at 49524d6 without making modifications.\r\nAdapt to micro and add to the build later.\r\n\r\nThis PR is part of the work to port operator DEPTH_TO_SPACE\r\nfrom lite to micro, as tracked in issue #46025.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46158, "title": "Extract reference for op DEPTH_TO_SPACE to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it wtihout including unrelated dependencies via\r\nreference_ops.h.\r\n\r\nThis PR is part of the work to port operator DEPTH_TO_SPACE\r\nfrom lite to micro, as tracked in issue #46025.", "comments": []}, {"number": 46157, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46157) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46156, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46156) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46155, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46155) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46154, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46154) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46153, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46153) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46152, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46152) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46151, "title": "[tf.data] support ~/ alias for home dir in tf.data.Dataset.list_files", "body": "This PR addresses the issue: https://github.com/tensorflow/tensorflow/issues/44264 by:\r\n- Adding support to handle `~/` alias for home dir in file paths while using `tf.data.Dataset.list_files` API.\r\n- Adding a test case to validate the functionality.\r\n", "comments": ["cc: @aaudiber ", "@aaudiber If that's the case, then let me close this PR. Shall I update https://github.com/tensorflow/tensorflow/issues/44264 with the reference to your comment and close it as well?\r\n\r\nOR:\r\n\r\nI can improve the error message which suggests the users to call `os.path.expanduser(\"~\")` when the `file_pattern` contains `~/` in it. What do you suggest?", "@kvignesh1420 Ah sorry, should have replied on #44264 earlier. I prefer not reporting a special error for `~/` to keep the implementation simpler. ", "@aaudiber no problem. Thanks anyways."]}, {"number": 46150, "title": "Rewrite 1D dilated convolution", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/38638. The pattern is obtained from \r\n\r\n```python\r\nlayer = tf.keras.layers.Conv1D(8, 5, 1, dilation_rate=2, padding=\"SAME\", use_bias=False)\r\n\r\n@tf.function(input_signature=(tf.TensorSpec(shape=(1, 128, 3)),))\r\ndef my_func(x):\r\n  return layer(x)\r\n```\r\n\r\nHi @haozha111, can you take a look at this when time allows? Thank you!", "comments": ["@joker-eph Hi Mehdi, could you lgtm this change since the pull request needs additional owner approval to proceed. Thanks!"]}, {"number": 46149, "title": "micro: in CONTRIBUTING.md, fix path to test script", "body": "Fix path to test script in CONTRIBUTING.md.\r\n\r\nFixes #46148.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46148, "title": "micro: in CONTRIBUTING.md, the path a test script is wrong", "body": "@tensorflow/micro\r\n- Tensorflow version (commit SHA if source): 556fa126\r\n \r\nIn micro's CONTRIBUTING.md, the path to a script to run tests prior to submitting a PR uses (what is now, at least) an invalid path.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7958e0cbfa55b779c3682aaaa37f6e7c55f55bc2/tensorflow/lite/micro/CONTRIBUTING.md#L204-L208\r\n\r\nThe fix is trivial and on its way.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46148\">No</a>\n"]}, {"number": 46147, "title": "Failed to build wheel on MacOS Big Sur", "body": "Repaired `build_pip_package.sh` for enable building wheel on MacBook Air with macOS Big Sur.\r\n\r\nSolved issue #45095\r\n\r\n@mihaimaruseac\r\n\r\n", "comments": []}, {"number": 46146, "title": "Model loaded from a SavedModel format in a distribution strategy has weights whose names are not unique", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): colab\r\n- TensorFlow version (use command below): 2.2 and 2.4\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: problem is on CPU and GPU\r\n- GPU model and memory: colab\r\n\r\n**Describe the current behavior**\r\n\r\nI am using a distribution strategy to train my model on several GPUs, and using the `ModelCheckpointCallback` to save my model.\r\nI train my model in 2 steps, in the first I use the SavedModel format and in the second, I want to save my model using the HDF5 format.\r\nHowever, I encounter a problem when loading my saved model in a distribution strategy, where the names of the different weights of my model are stripped down the last part (i.e. for example `'kernel:0'`).\r\nTherefore, my model has several weights with the same name and cannot be saved in the HDF5 format.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like my model to be loaded in the distribution strategy with the proper names.\r\n\r\n**Standalone code to reproduce the issue**\r\nYou can find a colab to illustrate this issue [here](https://colab.research.google.com/drive/1VqhqimaF3zxkj1mBrjugITtRC5XYJrW9?usp=sharing).\r\n\r\n**Other info / logs** \r\nFor reference the typical error that you get is `RuntimeError: Unable to create link (name already exists)`, with the full traceback being:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-10-debcda389c22> in <module>()\r\n      1 # and as a consequence, we cannot save the weights of our model\r\n----> 2 new_model.save_weights('new_model_weights.hdf5')\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in save_weights(self, filepath, overwrite, save_format, options)\r\n   2106     if save_format == 'h5':\r\n   2107       with h5py.File(filepath, 'w') as f:\r\n-> 2108         hdf5_format.save_weights_to_hdf5_group(f, self.layers)\r\n   2109     else:\r\n   2110       if context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in save_weights_to_hdf5_group(f, layers)\r\n    640     save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\r\n    641     for name, val in zip(weight_names, weight_values):\r\n--> 642       param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n    643       if not val.shape:\r\n    644         # scalar\r\n\r\n/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds)\r\n    137             dset = dataset.Dataset(dsid)\r\n    138             if name is not None:\r\n--> 139                 self[name] = dset\r\n    140             return dset\r\n    141 \r\n\r\n/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py in __setitem__(self, name, obj)\r\n    371 \r\n    372             if isinstance(obj, HLObject):\r\n--> 373                 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n    374 \r\n    375             elif isinstance(obj, SoftLink):\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5o.pyx in h5py.h5o.link()\r\n\r\nRuntimeError: Unable to create link (name already exists)\r\n```\r\n\r\nHowever, this issue is different than [this one](https://github.com/tensorflow/tensorflow/issues/27688#issuecomment-595950270) because originally the weights do have different names (and even when saved, the weights have different names, it's just that loading them inside the distribution strategy erases that).\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d0911531f279ff81a99ad3005c71a38a/46146.ipynb). Thanks!", "You need to disable eager execution then it is able to create unique names i was able to run it \r\ntf.compat.v1.disable_eager_execution()", "> Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d0911531f279ff81a99ad3005c71a38a/46146.ipynb). Thanks!\r\n\r\nIn this colab we just need to disable eager execution and it works fine and i was able to run through it.", "@SwathiAunooru good catch!\r\nIt indeed works in eager mode.\r\nHowever the names of the weights are still off. This time they are not identical, but they are still not what they were before, which might prove infefficient for debugging should it happen\r\nI also don't know how easy it is to switch between eager mode and graph mode when loading/saving weights (is it possible?).", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/660acf1315c19d2a99fc2d169f2b46a7/untitled160.ipynb?authuser=1)..Thanks !", "Looks like this is more related to `hdf5` format.  It didn't throw any error when using `saved_model` with `SavedModel` format. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/34e7bee68e2bce77901fbb78d0ed4a48/untitled160.ipynb?authuser=1). Please let me know if I missed anything.\r\n\r\nIs it possible to create it in keras-team/keras repo as Keras code moved to that repo? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46146\">No</a>\n"]}, {"number": 46145, "title": "Pandas pct_change() function results in nan loss when training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: Python 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Running on CPU\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nWhen training a model with data from a pandas DataFrame, the model trains fine unless I use the df.pct_change() pandas function on it. If I do so, the loss is always nan. I specifically made sure to remove any NaN or inf values from the dataset but the problem persists. I don't know if I'm just missing something obvious and the issue is on my end.\r\n\r\nI created a simplified jupyter notebook to demonstrate the issue.\r\n\r\n**Describe the expected behavior**\r\nThe loss should be a real number in the last cell of the notebook (after I remove all NaN and inf values) but it is still nan.\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is the notebook:\r\nhttps://drive.google.com/file/d/1k3dHHtFI4tGTKswF0d0TJdj9TfaqfBnX/view?usp=sharing\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46145\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46145\">No</a>\n", "Figured out the answer on my own. Stupidly, the example code I wrote out above is exactly the issue, haha.\r\n\r\nIndeed, the model will train fine on the actual data, and indeed the model will not train fine on any data that has NaN or inf values in it.\r\n\r\nBut if you try to train on data with NaN or inf values, then the model will never stop having a nan loss, despite if you train it with new, clean data.\r\n\r\nStart training the model only when you know there are no NaN or inf values in your data."]}, {"number": 46144, "title": "TensorFlowLiteSwift build error in the Bazel project", "body": "**System information**\r\n- OS Platform and Distribution: macOS Big Sur 11.1\r\n- TensorFlow version: 2.4.0\r\n- Bazel version: 3.7.2-homebrew\r\n- Compiler version: Apple clang 12.0\r\n- Xcode version: 12.3\r\n\r\n\r\nI want to connect to mediapipe app for ios TensorFlowLiteSwift framework.\r\n\r\nI added a bazel dependency to my project:\r\nBUILD file:\r\n```\r\nswift_library(\r\n    deps = [\r\n      \"@tensorflow_my//tensorflow/lite/swift:TensorFlowLite\",\r\n    ]\r\n)\r\n```\r\n\r\nMy WORKSPACE file (fragment for tensorflow):\r\n```\r\n#Tensorflow repo should always go after the other external dependencies.\r\n# 2020-10-30\r\n_TENSORFLOW_GIT_COMMIT = \"84384703c0d8b502e33ff6fd7eefd219dca5ff8e\"\r\n_TENSORFLOW_SHA256= \"23fb322fc15a20f7a7838d9a31f8b16f60700a494ea654311a0aa8621769df98\"\r\nhttp_archive(\r\n    name = \"org_tensorflow\",\r\n    urls = [\r\n      \"https://github.com/tensorflow/tensorflow/archive/%s.tar.gz\" % _TENSORFLOW_GIT_COMMIT,\r\n    ],\r\n    patches = [\r\n        \"@//third_party:org_tensorflow_compatibility_fixes.diff\",\r\n    ],\r\n    patch_args = [\r\n        \"-p1\",\r\n    ],\r\n    strip_prefix = \"tensorflow-%s\" % _TENSORFLOW_GIT_COMMIT,\r\n    sha256 = _TENSORFLOW_SHA256,\r\n)\r\n\r\nload(\"@org_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\r\ntf_workspace(tf_repo_name = \"org_tensorflow\")\r\n\r\nlocal_repository(\r\n    name = \"tensorflow_my\",\r\n    path = \"third_party/tensorflow\", # < -- cloned repository\r\n)\r\n\r\n\r\nload(\"@tensorflow_my//tensorflow:workspace3.bzl\", \"workspace\")\r\nworkspace()\r\nload(\"@tensorflow_my//tensorflow:workspace2.bzl\", \"workspace\")\r\nworkspace()\r\nload(\"@tensorflow_my//tensorflow:workspace1.bzl\", \"workspace\")\r\nworkspace()\r\nload(\"@tensorflow_my//tensorflow:workspace0.bzl\", \"workspace\")\r\nworkspace()\r\n```\r\n\r\nWhen building a project, it outputs the following:\r\n```\r\nShowing All Messages\r\nQueuing Tulsi build...\r\n<*> Parsing options completed in 0.261 ms\r\nRunning \"/usr/local/bin/bazel build --verbose_failures --bes_outerr_buffer_size=0 --apple_platform_type=ios --cpu=ios_arm64 --watchos_cpus=armv7k --announce_rc '--override_repository=tulsi=/Users/dmitry/Library/Application Support/Tulsi/0.20190814.88/Bazel' --compilation_mode=dbg --define=apple.add_debugger_entitlement=1 --define=apple.propagate_embedded_extra_outputs=1 --define=apple.experimental.tree_artifact_outputs=1 --features=debug_prefix_map_pwd_is_dot --tool_tag=tulsi:bazel_build --build_event_json_file=/Users/dmitry/Documents/mediapipeNew/Mediapipe.xcodeproj/.tulsi/8792_build_events.json --noexperimental_build_event_json_file_path_conversion --aspects @tulsi//:tulsi/tulsi_aspects.bzl%tulsi_outputs_aspect --output_groups=tulsi_outputs,default //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp\", patching output for workspace root at \"/Users/dmitry/Documents/mediapipeNew/mediapipe\" with project path at \"/Users/dmitry/Documents/mediapipeNew\".\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc:\r\n  'build' options: --jobs 128 --define=absl=1 --enable_platform_specific_config --apple_platform_type=macos --apple_generate_dsym\r\nINFO: Found applicable config definition build:macos in file /Users/dmitry/Documents/mediapipeNew/mediapipe/.bazelrc: --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --copt=-w\r\nLoading: \r\nLoading: 0 packages loaded\r\nDEBUG: Rule 'rules_foreign_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = \"3e6b0691fc57db8217d535393dcc2cf7c1d39fc87e9adb6e7d7bab1483915110\"\r\nDEBUG: Repository rules_foreign_cc instantiated at:\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:39:13: in <toplevel>\r\nRepository rule http_archive defined at:\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nAnalyzing: target //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp (0 packages loaded, 0 targets configured)\r\nDEBUG: Rule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = \"fc58ff069f150c81abd10231bb1d9fbff0ba9322e03c9396518db4d054d5f2e6\"\r\nDEBUG: Repository rules_cc instantiated at:\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:33:13: in <toplevel>\r\nRepository rule http_archive defined at:\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /Users/dmitry/Documents/mediapipeNew/mediapipe/WORKSPACE:401:10: in <toplevel>\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/tensorflow_my/tensorflow/workspace0.bzl:65:34: in workspace\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/  /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp (1 packages loaded, 15 targets configured).\r\nINFO: Found 1 target...\r\n\r\n[0 / 7] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\n[25 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/mfcc_mel_filterbank.cc; 1s darwin-sandbox ... (33 actions, 4 running)\r\n[29 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/spectrogram.cc; 2s darwin-sandbox ... (44 actions, 4 running)\r\n[29 / 240] Compiling tensorflow_my/tensorflow/lite/kernels/internal/spectrogram.cc; 3s darwin-sandbox ... (44 actions, 4 running)\r\n[31 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/cpu_backend_gemm_eigen.cc; 4s ... (44 actions, 4 running)\r\n[34 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/experimental/resource/static_hashtable.cc; 6s ... (44 actions, 4 running)\r\n[35 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/minimal_logging_ios.cc; 7s ... (43 actions, 4 running)\r\n[35 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/minimal_logging_ios.cc; 9s ... (43 actions, 4 running)\r\n[38 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/tflite_with_xnnpack_optional.cc; 11s ... (42 actions, 4 running)\r\n[41 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/simple_memory_arena.cc; 13s ... (42 actions, 4 running)\r\n[46 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/deprecated_backends.cc; 15s ... (42 actions, 4 running)\r\n[55 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/core/api/op_resolver.cc; 17s ... (128 actions, 4 running)\r\n[57 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/delegates/utils.cc; 20s ... (128 actions, 4 running)\r\n[63 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/kernels/eigen_support.cc; 24s ... (128 actions, 4 running)\r\n[68 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc; 28s ... (128 actions, 4 running)\r\nINFO: From Linking external/tensorflow_my/tensorflow/lite/kernels/libcpu_backend_gemm.a:\r\nwarning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: archive library: bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/kernels/libcpu_backend_gemm.a the table of contents is empty (no object file members in the library define global symbols)\r\n[94 / 240] [Sched] Compiling tensorflow_my/tensorflow/lite/interpreter.cc; 16s ... (124 actions, 4 running)\r\nERROR: /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/BUILD:22:11: C++ compilation of rule '@tensorflow_my//tensorflow/lite/delegates/xnnpack:xnnpack_delegate' failed (Exit 1): wrapped_clang failed: error executing command \r\n  (cd /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/sandbox/darwin-sandbox/2820/execroot/mediapipe && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=iPhoneOS \\\r\n    APPLE_SDK_VERSION_OVERRIDE=14.3 \\\r\n    PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/libexec:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/local/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    XCODE_VERSION_OVERRIDE=12.3.0.12C33 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g '-std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' -g -iquote external/tensorflow_my -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my -iquote external/flatbuffers -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers -iquote external/eigen_archive -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -iquote external/FP16 -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16 -iquote external/XNNPACK -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK -iquote external/clog -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog -iquote external/pthreadpool -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv -iquote external/psimd -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd -iquote external/cpuinfo -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog/_virtual_includes/clog -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/_virtual_includes/psimd -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/tensorflow_my/tensorflow/lite/schema -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/schema -isystem external/eigen_archive -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -isystem external/FP16/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/include -isystem external/XNNPACK/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/include -isystem external/psimd/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/include -MD -MF bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_SPARSE=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_MEMOPT=1' -DXNN_NO_QS8_OPERATORS -DXNN_NO_QU8_OPERATORS -DXNN_NO_U8_OPERATORS -DXNN_NO_X8_OPERATORS -DXNN_NO_F16_OPERATORS -DXNN_NO_X16_OPERATORS '-frandom-seed=bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/iPhoneOS.platform/Developer/Library/Frameworks '-miphoneos-version-min=10.0' -w '-std=c++14' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -target arm64-apple-ios -c external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc -o bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox wrapped_clang failed: error executing command \r\n  (cd /private/var/tmp/_bazel_dmitry/02a2e3662efe5f9742ce3f7bf49d4039/sandbox/darwin-sandbox/2820/execroot/mediapipe && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=iPhoneOS \\\r\n    APPLE_SDK_VERSION_OVERRIDE=14.3 \\\r\n    PATH=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/libexec:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/usr/local/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/local/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    XCODE_VERSION_OVERRIDE=12.3.0.12C33 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g '-std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' -g -iquote external/tensorflow_my -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my -iquote external/flatbuffers -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers -iquote external/eigen_archive -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -iquote external/FP16 -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16 -iquote external/XNNPACK -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK -iquote external/clog -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog -iquote external/pthreadpool -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv -iquote external/psimd -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd -iquote external/cpuinfo -iquote bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/clog/_virtual_includes/clog -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/_virtual_includes/psimd -Ibazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/tensorflow_my/tensorflow/lite/schema -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/schema -isystem external/eigen_archive -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/eigen_archive -isystem external/FP16/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FP16/include -isystem external/XNNPACK/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/FXdiv/include -isystem external/psimd/include -isystem bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/psimd/include -MD -MF bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.d -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_SPARSE=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_MEMOPT=1' -DXNN_NO_QS8_OPERATORS -DXNN_NO_QU8_OPERATORS -DXNN_NO_U8_OPERATORS -DXNN_NO_X8_OPERATORS -DXNN_NO_F16_OPERATORS -DXNN_NO_X16_OPERATORS '-frandom-seed=bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/iPhoneOS.platform/Developer/Library/Frameworks '-miphoneos-version-min=10.0' -w '-std=c++14' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -target arm64-apple-ios -c external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc -o bazel-out/ios-arm64-min10.0-applebin_ios-ios_arm64-dbg/bin/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/_objs/xnnpack_delegate/xnnpack_delegate.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:277:49: error: use of undeclared identifier 'XNN_FLAG_SPARSE_INFERENCE'\r\n    const uint32_t flags = has_sparse_weights ? XNN_FLAG_SPARSE_INFERENCE : 0;\r\n                                                ^\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1449:33: error: use of undeclared identifier 'xnn_define_depth_to_space'\r\n      const xnn_status status = xnn_define_depth_to_space(\r\n                                ^\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/external/tensorflow_my/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc:1538:11: error: use of undeclared identifier 'xnn_define_elu'; did you mean 'xnn_define_prelu'?\r\n          xnn_define_elu(subgraph, /*alpha=*/1.0f,\r\n          ^~~~~~~~~~~~~~\r\n          xnn_define_prelu\r\n/Users/dmitry/Documents/mediapipeNew/mediapipe/external/XNNPACK/include/xnnpack.h:831:17: note: 'xnn_define_prelu' declared here\r\nenum xnn_status xnn_define_prelu(\r\n                ^\r\n3 errors generated.\r\nAspect @tulsi//:tulsi/tulsi_aspects.bzl%tulsi_outputs_aspect of //mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp up-to-date:\r\n  bazel-bin/mediapipe/examples/ios/handtrackinggpu/HandTrackingGpuApp.tulsiouts\r\nINFO: Elapsed time: 40,984s, Critical Path: 38,06s\r\nINFO: 205 processes: 133 internal, 72 darwin-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n<*> Running Bazel completed in 41186.495 ms\r\n/Users/dmitry/Documents/mediapipeNew/Mediapipe.xcodeproj/.tulsi/Scripts/bazel_build.py:549: error: Bazel build failed with exit code 1. Please check the build log in Report Navigator (\u23189) for more information.\r\n<*> Everything completed in 41246.470 ms\r\n\r\n\r\n```\r\n\r\nXcode highlights the following in red (a fragment from the above log):\r\n```\r\n use of undeclared identifier 'XNN_FLAG_SPARSE_INFERENCE'\r\n      const xnn_status status = xnn_define_depth_to_space(\r\n\r\n use of undeclared identifier 'xnn_define_depth_to_space'\r\n          xnn_define_elu(subgraph, /*alpha=*/1.0f,\r\n          ^~~~~~~~~~~~~~\r\n          xnn_define_prelu\r\n\r\n use of undeclared identifier 'xnn_define_elu'; did you mean 'xnn_define_prelu'?\r\n         enum xnn_status xnn_define_prelu(\r\n                        ^\r\n3 errors generated.\r\n```\r\n\r\n\r\n", "comments": ["@manosh7n Can you provide a link to your repository where this issue can be reproduced?", "I did not make a repository for the project, because I used mediapipe with small changes to the build files. To reproduce the error, you need the following:\r\n\r\n1) Clone mediapipe repository https://github.com/google/mediapipe.git\r\n2) Create an xcode project using tulsi (open mediapipe/MediPipe.tulsiproj)\r\n   To do this, you need install:\r\n   - bazel\r\n   - tulsi\r\n   - python numpy\r\n   - opencv@3\r\n   I used this instruction https://qiita.com/ponte1010/items/ed10cb2b5dcf7579f9bc\r\n\r\n3) Open xcodeproject and build HandTrackerGpuApp (To check that everything works)\r\n\r\n\r\n4) If the project is successfully builded, then you can proceed further. Clone tensorflow repository to third_party folder (/third_party) https://github.com/tensorflow/tensorflow.git\r\n5) In tensorflow folder run ./configure\r\n   install rocm (n)\r\n   install cuda (n)\r\n   fresh clang (n)\r\n   android workspace (n)\r\n   ios workspace (y)\r\n6) In mediapipe WORKSPACE file add \r\n```\r\nlocal_repository(\r\n    name = \"tensorflow_my\",\r\n    path = \"third_party/tensorflow\", # < -- cloned repository\r\n)\r\n\r\n\r\nload(\"@tensorflow_my//tensorflow:workspace3.bzl\", \"workspace\")\r\nworkspace()\r\nload(\"@tensorflow_my//tensorflow:workspace2.bzl\", \"workspace\")\r\nworkspace()\r\nload(\"@tensorflow_my//tensorflow:workspace1.bzl\", \"workspace\")\r\nworkspace()\r\nload(\"@tensorflow_my//tensorflow:workspace0.bzl\", \"workspace\")\r\nworkspace()\r\n\r\nload(\r\n    \"@build_bazel_rules_swift//swift:repositories.bzl\",\r\n    \"swift_rules_dependencies\",\r\n)\r\n\r\nswift_rules_dependencies()\r\n```\r\n7) In mediapipe/examples/ios/handtrackinggpu/BUILD file add\r\n```\r\nload(\"@build_bazel_rules_swift//swift:swift.bzl\",\r\n     \"swift_library\")\r\n\r\n\r\nswift_library(\r\n    deps = [\r\n      \"@tensorflow_my//tensorflow/lite/swift:TensorFlowLite\",\r\n```\r\n8) repeat point 3 ", "The MediaPipe repository seems to be setting `org_tensorflow` external dependency already.\r\nCan't you just use `\"@org_tensorflow//tensorflow/lite/swift:TensorFlowLite\"` without even cloning and adding the TensorFlow project under `third_party`?\r\n\r\nIs there a particular reason why you need a different copy of TensorFlow?", "When using via @org_tensorflow I get the following error:\r\n\r\nShowing All Messages\r\nERROR: /Users/dmitry/Documents/mediapipeNew/mediapipe/mediapipe/examples/ios/handtrackinggpu/BUILD:85:14: no such package '@org_tensorflow//tensorflow/lite/swift': BUILD file not found in directory 'tensorflow/lite/swift' of external repository @org_tensorflow. Add a BUILD file to a directory to mark it as a package. and referenced by '//mediapipe/examples/ios/handtrackinggpu:SwiftBridge'\r\n\r\n", "The TFLite iOS related directories have been migrated from `//tensorflow/lite/experimental/(ios|objc|swift)` to `//tensorflow/lite/(ios|objc|swift)` fairly recently. It looks to me that the version of TensorFlow code pulled in by the MediaPipe example contains the swift directory under `//tensorflow/lite/experimental/swift`.\r\n\r\nCan you try `@org_tensorflow//tensorflow/lite/experimental/swift:TensorFlowLite` instead?", "I get same error\r\n\r\n\r\nShowing All Messages\r\nERROR: /Users/dmitry/Documents/mediapipeNew/mediapipe/mediapipe/examples/ios/handtrackinggpu/BUILD:85:14: no such package '@org_tensorflow//tensorflow/lite/experimental/swift': BUILD file not found in directory 'tensorflow/lite/experimental/swift' of external repository @org_tensorflow. Add a BUILD file to a directory to mark it as a package. and referenced by '//mediapipe/examples/ios/handtrackinggpu:SwiftBridge'\r\n\r\n", "Ah, I see. That's probably because the TensorFlow repository is pulled in but `./configure` script is not properly run.\r\nThis is a bit hacky, but can you try the followings?\r\n\r\n1. Create `third_party/org_tensorflow_ios_build_rename.diff` with the following content.\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/experimental/ios/BUILD.apple b/tensorflow/lite/experimental/ios/BUILD\r\nsimilarity index 100%\r\nrename from tensorflow/lite/experimental/ios/BUILD.apple\r\nrename to tensorflow/lite/experimental/ios/BUILD\r\ndiff --git a/tensorflow/lite/experimental/objc/BUILD.apple b/tensorflow/lite/experimental/objc/BUILD\r\nsimilarity index 100%\r\nrename from tensorflow/lite/experimental/objc/BUILD.apple\r\nrename to tensorflow/lite/experimental/objc/BUILD\r\ndiff --git a/tensorflow/lite/experimental/swift/BUILD.apple b/tensorflow/lite/experimental/swift/BUILD\r\nsimilarity index 100%\r\nrename from tensorflow/lite/experimental/swift/BUILD.apple\r\nrename to tensorflow/lite/experimental/swift/BUILD\r\n```\r\n\r\n2. In the `WORKSPACE` where the `org_tensorflow` dependency is defined, add `@//third_party:org_tensorflow_ios_build_rename.diff` to the `patches` list.\r\n\r\n3. Run `bazel clean` and retry the build.\r\n\r\nIf this still doesn't work for you, it would be very helpful if you could provide a standalone repository (e.g. by creating a fork of MediaPipe repository) where I can take a deeper look.", "Showing All Messages\r\nERROR: /Users/dmitry/Documents/mediapipeNew/mediapipe/mediapipe/examples/ios/handtrackinggpu/BUILD:85:14: in swift_library rule //mediapipe/examples/ios/handtrackinggpu:SwiftBridge: target '@org_tensorflow//tensorflow/lite/experimental/swift:TensorFlowLite' is not visible from target '//mediapipe/examples/ios/handtrackinggpu:SwiftBridge'. Check the visibility declaration of the former target if you think the dependency is legitimate\r\n\r\n\r\nHow can I make it visible? In the local repository, this is not difficult to do, but here I do not know how", "Okay, I think we're pretty close. Try changing the diff file content to \r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/experimental/ios/BUILD.apple b/tensorflow/lite/experimental/ios/BUILD\r\nsimilarity index 100%\r\nrename from tensorflow/lite/experimental/ios/BUILD.apple\r\nrename to tensorflow/lite/experimental/ios/BUILD\r\ndiff --git a/tensorflow/lite/experimental/objc/BUILD.apple b/tensorflow/lite/experimental/objc/BUILD\r\nsimilarity index 100%\r\nrename from tensorflow/lite/experimental/objc/BUILD.apple\r\nrename to tensorflow/lite/experimental/objc/BUILD\r\ndiff --git a/tensorflow/lite/experimental/swift/BUILD.apple b/tensorflow/lite/experimental/swift/BUILD\r\nsimilarity index 98%\r\nrename from tensorflow/lite/experimental/swift/BUILD.apple\r\nrename to tensorflow/lite/experimental/swift/BUILD\r\nindex 28b3c76e52c..d3b3fdd238a 100644\r\n--- a/tensorflow/lite/experimental/swift/BUILD.apple\r\n+++ b/tensorflow/lite/experimental/swift/BUILD\r\n@@ -57,7 +57,7 @@ swift_library(\r\n     }),\r\n     module_name = \"TensorFlowLite\",\r\n     tags = TFL_DEFAULT_TAGS + [\"nobuilder\"],\r\n-    visibility = ios_visibility_whitelist(),\r\n+    visibility = [\"//visibility:public\"],\r\n     deps = [\r\n         \"//tensorflow/lite/experimental/ios:tensorflow_lite_c\",\r\n     ] + select({\r\n```\r\n\r\nand try again. Basically, we're cloning the tensorflow repository at a given commit specified at the `WORKSPACE` file, and then applying a few patch files on top of it.", "Thank you so much for your help, now everything works perfectly!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46144\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46144\">No</a>\n"]}, {"number": 46143, "title": "Tensorflow and Keras installation issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nHere is the version of Tensorflow and keras installed on python 3.7 in win 10\r\n\r\n.C:\\Users\\ZS>pip show tensorflow\r\nName: tensorflow\r\nVersion: 2.4.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\zs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\r\nRequires: termcolor, absl-py, google-pasta, typing-extensions, tensorboard, grpcio, six, wrapt, astunparse, numpy, keras-preprocessing, opt-einsum, gast, flatbuffers, tensorflow-estimator, wheel, protobuf, h5py\r\nRequired-by:\r\n\r\nC:\\Users\\ZS>pip show keras\r\nName: Keras\r\nVersion: 2.4.3\r\nSummary: Deep Learning for humans\r\nHome-page: https://github.com/keras-team/keras\r\nAuthor: Francois Chollet\r\nAuthor-email: francois.chollet@gmail.com\r\nLicense: MIT\r\nLocation: c:\\users\\zs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\r\nRequires: numpy, h5py, scipy, pyyaml\r\nRequired-by:\r\n**But..........................................\r\n******\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ZS\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>  from tensorflow import keras\r\n \r\nSyntaxError: unexpected indent\r\n>>> \r\n\r\n\r\n", "comments": ["@Malikammi,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36138. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46143\">No</a>\n"]}, {"number": 46142, "title": "TensorFlow and TensorFlowLite model produces different results", "body": "\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nSimple test script included\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nGoogle Colaboratory\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device**:\r\nN/A\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\nv2.4.0-0-g582c8d236cb 2.4.0\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\ntest script attached\r\n\r\n### Describe the problem\r\n\r\nI have a TensorFlow model (attached as a zip file) which is converted to TensorFlowLite using the attached script.\r\nBoth models are evaluated using a single test exemplar.\r\nExpected: The two models should produce similar, if not identical, predictions.\r\nActual: The two models produce completely different predictions\r\n\r\n(the tflite model produces the same 'wrong' predictions when running on a mobile device but the \r\ntest can entirely reproduced in Colaboratory using the attached files).\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5765363/model.zip)\r\n\r\n[tflite_model_test.ipynb.txt](https://github.com/tensorflow/tensorflow/files/5765368/tflite_model_test.ipynb.txt)\r\n\r\n### Source code / logs\r\nAttached model.zip and tflite_model_test.ipynb", "comments": ["CCing @terryheo", "I tracked down the cause of the issue to the use of a masking layer in the tensorflow model\r\n\r\n  tf.keras.layers.Masking(mask_value=0., input_shape=(timesteps, features)),  \r\n  \r\n removing this line from the sequential keras model and everything works as expected.\r\n\r\nI didn't find anything saying that masking is not supported in tflite. If it is not supported the converter should display an error rather than creating an incorrect model.\r\n \r\n ", "@pnovapps thanks for the information. We'll check.\r\n\r\n@battery could you check why the converter didn't make any error on the masking layer?", "I guess we need to figure which layer makes a wrong prediction, for example, kernel implementation or converter lowering pattern.\r\n\r\nI will take a look at the attached notebook.", "@pnovapps could you share the minimal reproducible steps for the problem with the sequential keras model? The above notebook does not contain any keras code to build the model. It would be much helpful to debug if you can give us in a form of the notebook and provide the example input and the expected output.", "The previous script used the pre-trained attached model. I've attached a minimal script to train and evaluate the sequential keras model using random data. The model won't do anything useful but is sufficient to illustrate the differences in predictions. The script contains an example input, the script produces the actual and expected outputs.\r\n\r\nOn one run the tensorflow model predictions are\r\n  0.07902689, 0.0826071 , 0.09351914, 0.11532243, 0.13965082, 0.07816847,\r\n  0.07019586, 0.06621424, 0.08247119, 0.06793797, 0.12488594\r\n\r\nand tensorflowlite predictions are\r\n  0.08866501 0.12623808 0.05040342 0.07568947 0.13019517 0.0998802\r\n  0.06874353 0.0756304  0.06352186 0.11585259 0.10518024\r\n\r\nthe differences are more pronounced with live data.\r\n[tflite_model_test2.ipynb.txt](https://github.com/tensorflow/tensorflow/files/5801166/tflite_model_test2.ipynb.txt)\r\n\r\n\r\n", "Thanks a lot! I can reproduce the problem.", "I found the cause of this issue and fixed it at commit 13fa827d04cbe6de0e0f1f1cc190d06ce01aa074", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46142\">No</a>\n"]}, {"number": 46141, "title": "[tf.data service] test zipped datasets with different processing modes", "body": "This PR extends the `tf.data service` tests by testing the functionality on zipped datasets which have been distributed using `parallel_epochs` and `distributed_epoch` processing modes.", "comments": []}, {"number": 46140, "title": "Fix bool Where accumulation bug", "body": "Contains fix for 'CountAccumulator<bool>' function.\r\nCurrent version counted underlying byte values instead of logical state, which could lead to \"Race condition between counting...\".\r\nFor example: if underlying input tensor buffer contained values [0, 1, 2, 1] and was marked as boolean, then 'CountAccumulator<bool>' would return 4 elements (as it counted the third one as two elements, 0+1+2+1=4). Such edge case can be encountered in conjuction with custom ops.\r\n", "comments": ["Hello, should anything else be done? This PR seems to be stuck.", "> Hello, should anything else be done? This PR seems to be stuck.\r\n\r\n@arkjedrz Sorry for the delay. Internal test failures are appearing, we are working on these internally. Thank you!", "The c++ optimizer is reducing your `std::count_if` call down to the same code as `std::accumulate`, so the tests fail when it's compiled with the optimizer enabled.\r\n\r\nThe numpy documentation explicitly states that the byte representing a Numpy bool must be either 0 or 1 (https://numpy.org/doc/stable/reference/c-api/dtype.html).  \r\n\r\nAnd I believe that using a value other than 0 or 1 in a bool might be undefined behavior in c++.  I haven't found a specific c++ standards reference on it, but here's a related stackoverflow post: https://stackoverflow.com/questions/54120862/does-the-c-standard-allow-for-an-uninitialized-bool-to-crash-a-program\r\n\r\nSo I believe that your test and your custom ops have undefined behavior.  You probably need to update your custom op to output valid boolean data (all 0s or 1s), or have it output some other dtype and use an explicit tf.cast to convert it to bool.", "@arkjedrz Can you please check @edloper's comments and keep us posted ? Thanks!", "I checked C99 standard and it states \"When any scalar value is converted to_Bool, the result is 0 if the value compares equal to 0; otherwise, the result is 1.\" (source: http://www.dii.uchile.cl/~daespino/files/Iso_C_1999_definition.pdf page 43), so no undefined behaviour here.\r\n\r\nFor which compiler and compiler option it fails? I can try subtracting 'false' values count from array length, or use explicit for-loop. In both cases I see chance of invalid optimization.", "@edloper  Can you please take a look on above comments from @arkjedrz. Thanks!", "The language you reference from the standard talks about when you \"convert\" to bool.  I.e., this guarantees that `static_cast<bool>(20)` returns `true`.  But I'm not sure what's happening inside numpy when you build a boolean array from bytes.  My guess (without looking at the numpy code) would be that it's using `reinterpret_cast`.  And for *that*, the guarantees are much weaker.  I suspect that this is why the numpy docs explicitly call out that the byte representing a Numpy bool must be either 0 or 1.\r\n\r\nHere's an example on godbolt showing that both `accumulate` and `count_if` get the \"wrong\" result with -O3 in gcc when we build an array of bytes and use `reinterpret_cast` to view it as an array of booleans: https://godbolt.org/z/Whx11E5Eq\r\n\r\n```c++\r\n#include <numeric>\r\n#include <algorithm>\r\n#include <iostream>\r\n\r\n// Type your code here, or load an example.\r\nint main(void) {\r\n    const unsigned char x[3] = {0, 10, 22};\r\n    const bool* begin = reinterpret_cast<const bool*>(x);\r\n    const bool* end = begin + 3;\r\n    auto y = std::accumulate(begin, end, 0LL);\r\n    auto z = std::count_if(begin, end, [](bool value) {return value;} );\r\n    std::cout << y << \" and \" << z;\r\n}\r\n```\r\n```\r\nProgram stdout (with -O3):\r\n32 and 11\r\n```", "@arkjedrz Can you please check @edloper's comments and keep us posted ? Thanks!", "I changed implementation to count 'false' (zeros) and subtract that value from array length.\r\nHope that'll help.", "This still gives the wrong answer for some compilers (e.g., x86-64 gcc trunk -- https://godbolt.org/z/s7a6ehjEx), and even if it didn't, we wouldn't know whether future optimizations added by compilers would make it break.  Also, this will add a performance cost for all other users.  \r\n\r\nSo I still believe that the best option is for you to update your custom op to output valid boolean data (all 0s or 1s), or have it output some other dtype and use an explicit tf.cast to convert it to bool.  Is there a reason why that option won't work?", "Regarding godbolt compiler selection - is it safe to consider \"trunk\" version a viable option? This is in-progress version most susceptible to changes and bugs. Looking through compilers that were actually released I see that current committed version works best (with GCC trunk being the only one with issues). Try this one https://godbolt.org/z/6f4Ee4K4x\r\nRegarding future optimizations: it's impossible to prove that future versions of compilers will retain same behaviour and/or remain bugless. This argument is beyond scope of this commit.\r\n\r\nRegarding performance argument - I checked using quick-bench https://quick-bench.com/q/mwdlUIAjYS4U6IFR6KDfv8Vl-lI\r\nMy implementation is equivalent for clang and marginally faster for GCC.", "@arkjedrz The basic question here is whether what you're doing in the test has undefined behavior.  And from what I can tell, it does.  In particular, [`np.frombuffer`](https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/ctors.c#L3516) just saves the data you give it using a `void*`, which then ends up getting cast back to a `bool*`.  These c-style casts are basically reinterpret_cast, and there's no guarantee from the language that this will do what you want here.\r\n\r\nThe fact that this is undefined behavior is evidenced by the fact that `std::accumulate(begin, end, 0LL)` and `std::count_if(begin, end, [](bool value) {return value;} )` both return the wrong value -- either there's a bug in GCC (which seems less likely), or this is undefined behavior in this context, in which case GCC makes no guarantees about what it will do.\r\n\r\nMaking the code more complicated (to prevent optimizations that GCC is within its rights to perform) doesn't change the fact that this is undefined behavior.\r\n\r\nThe bug here is not in the TensorFlow code, but in the fact that you call `np.frombuffer` with inputs that violate the explicit requirements of that function (namely that [the byte representing a Numpy bool must be either 0 or ](https://numpy.org/doc/stable/reference/c-api/dtype.html)).\r\n\r\nReturning to my previous suggestion: is there a reason you can't update your custom op to output valid boolean data (all 0s or 1s), or have it output some other dtype and use an explicit tf.cast to convert it to bool?\r\n\r\n", "I used np.frombuffer to make smallest possible reproduction. There's no other way to create such unsanitized tensor in Python, at least that I am aware of. Real-life issue was spotted without use of Python or Numpy.\r\nTensorFlow is using Python for op unit tests so there's not much choice. For a hypothetical case of C++ unit tests I'd still generate tensor with {0x00, 0x01, 0x02, 0x01} buffer underneath.\r\nTechnical reasons behind not changing ops will remain undisclosed. It's not a matter of a single op and it's not trivial, that's all I'm willing to say.\r\n\r\nI took a look at C++ standard this time: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4713.pdf\r\nLooking at 7.6.6 \"A prvalue of type bool can be converted to a prvalue of type int, with false becoming zero and true becoming one.\". I checked libstdc++ implementation of std::accumulate with optimizations enabled and it breaks that rule. You can check that here: https://godbolt.org/z/GnMoP1dev\r\nAs You can see - pointer of bool is dereferenced and casted to 'long long' (accumulator type), but its value is not set to 1 as described by standard.\r\nI also checked std::count_if and implementation is very similar, so I believe it may be compiled into same code. Reference:\r\nhttps://gcc.gnu.org/onlinedocs/gcc-10.2.0/libstdc++/api/a00596_source.html\r\nhttps://gcc.gnu.org/onlinedocs/gcc-10.2.0/libstdc++/api/a00635_source.html\r\n\r\nLet's go back to C++ standard. 7.14 states \"A zero value, null pointer value, or null member pointer value is converted to false; any other value is converted to true.\". I find \"false\" to less unambiguous, as '0' will become 'false', and 'false' will become '0' - so it's a safer option.\r\n\r\nI also proved in previous comment that proposed solution is more robust and marginally faster for GCC.", "@arkjedrz The sections of the standard you reference (7.6.6 and 7.14) are both describing standard conversions, which would apply if we used something like `static_cast<bool>(12)` or `bool x = 12`.  Standard conversions do *not* apply with `reinterpret_cast`.  (See the start of section 7 of the standard for a list of contexts where standard conversions occur).  \r\n\r\nI believe that the relevant section of the standard that declares what you're doing as undefined behavior is 8.2.1.11:\r\n\r\n> If a program attempts to access the stored value of an object through a glvalue of other than one of the\r\n> following types the behavior is undefined:\r\n> (11.1) \u2014 the dynamic type of the object,\r\n> (11.2) \u2014 a cv-qualified version of the dynamic type of the object,\r\n> (11.3) \u2014 a type similar (as defined in 7.5) to the dynamic type of the object,\r\n> (11.4) \u2014 a type that is the signed or unsigned type corresponding to the dynamic type of the object,\r\n> (11.5) \u2014 a type that is the signed or unsigned type corresponding to a cv-qualified version of the dynamic type\r\n> of the object,\r\n> (11.6) \u2014 an aggregate or union type that includes one of the aforementioned types among its elements or nonstatic data members (including, recursively, an element or non-static data member of a subaggregate or\r\n> contained union),\r\n> (11.7) \u2014 a type that is a (possibly cv-qualified) base class type of the dynamic type of the object,\r\n> (11.8) \u2014 a char, unsigned char, or std::byte type.\r\n\r\nIn this case, we're accessing a stored value with dynamic type of char via the bool type, which is not covered by any of those cases.\r\n\r\nLooking at the gcc code for std::accumulate, I don't see any bugs.  If the input iterators' value type is `bool`, then `*__first` will return either `true` or `false`, which will be promoted to the accumulator type (`long long`) via the integral promotion rules (N4713 section 7.6).\r\n\r\nYour example https://godbolt.org/z/GnMoP1dev just demonstrates that `reinterpret_cast` results in undefined behavior in this context, not that there's a bug in `std::accumulate`.", "@arkjedrz  Can you please check @edloper's comments and keep us posted ? Thanks!", "I have no more arguments :) I'm still not entirely convinced and couldn't find convincing document, but I can't prove otherwise.\r\nWhat I have proven is that it's slightly faster, non-breaking and helps address my case - even if it's on the edge of undefined behavior.\r\nThanks @edloper for stimulating discussion!", "@edloper  Any update on this PR? Please. Thanks!", "@edloper Any update on this PR? Please. Thanks!", "Closing this PR, because I don't believe there's an actual bug here.  (The behavior that the reporter had was caused by undefined behavior in c++ that resulted from constructing an invalid value, and not by any bug in Tensorflow.)"]}, {"number": 46139, "title": "[tf.data] fix the example in DispatchServer docstring", "body": "this PR addresses the incomplete import of `WorkerConfig` in the [DispatchServer](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/DispatchServer) docs.\r\n\r\ncc: @aaudiber ", "comments": []}, {"number": 46138, "title": "Only use pthread_getname on glibc and FreeBSD", "body": "glibc and FreeBSD are actually the odd ones out and no other platform\r\nsupports pthread_getname or equivelant (from what I can find), so let's check for those instead of checking for platforms that don't support it.\r\n\r\nThis is required to get Tensorflow compiling on Musl libc.\r\n\r\nhttps://linux.die.net/man/3/pthread_getname_np", "comments": []}, {"number": 46137, "title": "MetalDelegate.swift : Unexpected version number in 'available' attribute for non-specific platform '*'", "body": "Warning in line\r\n @available(*, deprecated: 2.4, renamed: \"isPrecisionLossAllowed\")", "comments": ["@KonradMokiejewski \r\n\r\nCan you refer this [source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/swift/Sources/MetalDelegate.swift)  and see if it helps you.\r\n\r\nIf the problem still persists please share more information and exact sequence of commands / steps that you executed before running into the problem.It helps us in localiizng the issue faster.\r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46136, "title": "java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'", "body": "I am going through the following codelab:\r\nhttps://developer.android.com/codelabs/digit-classifier-tflite#5\r\n\r\n## Description of issue:\r\nI get the following error\r\n\r\n  2021-01-04 14:58:02.547 27206-27283/org.tensorflow.lite.codelabs.digitclassifier E/AndroidRuntime: FATAL EXCEPTION: pool-1-thread-1\r\n      Process: org.tensorflow.lite.codelabs.digitclassifier, PID: 27206\r\n      java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n      \r\n      Registration failed.\r\n      \r\n          at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n          at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:70)\r\n          at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:61)\r\n          at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)\r\n          at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.initializeInterpreter(DigitClassifier.kt:64)\r\n          at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.access$initializeInterpreter(DigitClassifier.kt:31)\r\n          at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier$initialize$1.run(DigitClassifier.kt:48)\r\n          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n          at java.lang.Thread.run(Thread.java:919)\r\n\r\n### Clear description\r\n\r\nI have looked this up online. It points to some differences in the TF and TFLite versions. But I followed the steps exactly in the codelab and did not expect something like this.\r\n", "comments": ["@vaibhavgupta2507,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand also the exact sequence of commands / steps that you executed before running into the error. Thanks!", "@amahendrakar \r\nOS: Ubuntu 20.04\r\nMobile Device: Realme 9 Pro\r\nTensorflow: Used the colab notebook\r\nTensorflow version: 2.4.0\r\nPython version: 3.6.9\r\nBazel Version: (`bazel version` command fails, saying that the term bazel is not recognized)\r\nCuda Version: 10.1 (got from the nvidia-smi command)\r\n\r\nSteps:\r\nI simply ran the notebook from (https://developer.android.com/codelabs/digit-classifier-tflite#5)\r\nAfter downloading the model (from the notebook), I placed it in the assets folder of the Android app\r\nBut I get a runtime error with the above logs", "Could you make sure the the android application has the same TensorFlow version with TensorFlow Python version that was used for TFLite model conversion?", "I tried using the same version of Tensorflow for the Android app as suggested\r\n\r\n`implementation 'org.tensorflow:tensorflow-lite:2.4.0'`\r\n\r\nBut now I am getting a new error:-\r\n```\r\n2021-01-05 18:05:16.092 12296-12408/org.tensorflow.lite.codelabs.digitclassifier E/AndroidRuntime: FATAL EXCEPTION: pool-1-thread-1\r\n    Process: org.tensorflow.lite.codelabs.digitclassifier, PID: 12296\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:373)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:85)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:277)\r\n        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.initializeInterpreter(DigitClassifier.kt:64)\r\n        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.access$initializeInterpreter(DigitClassifier.kt:31)\r\n        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier$initialize$1.run(DigitClassifier.kt:48)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n        at java.lang.Thread.run(Thread.java:919)\r\n```\r\n\r\nI looked up this error and found some suggestions to give a specific size input, which is predefined in the model.\r\nBut this has already been done in the above codelab.", "In the colab, it does not fix the batch size of the keras model but it just does preprocessing and postprocessing of the data. You need to set a batch size in the keras model after training or you can set a batch size through TFLite converter V1 API with the input_shapes argument.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46136\">No</a>\n"]}, {"number": 46135, "title": "When activation='tanh' is used, the training gradient no drop", "body": "\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.4.0rc4\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: CUDA11.0/cuDNN8.0.4\r\n- GPU model and memory: NVIDIA RTX3070 8G\r\n- OS version: windows 10\r\n\r\nISSUE:  When activation='tanh' is used, the training gradient no drop. the follow test code from: https://www.tensorflow.org/tutorials/generative/dcgan\r\n\r\n\r\nimport glob\r\nimport imageio\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport PIL\r\nfrom tensorflow.keras import layers,Model\r\nimport time\r\n\r\nfrom IPython import display\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n\r\nclass ResNet(Model):\r\n    def __init__(self):\r\n        super(ResNet, self).__init__()\r\n        self.d1=layers.Dense(7*7*256, use_bias=False, input_shape=(100,))\r\n        self.b1=layers.BatchNormalization()\r\n        self.a1=layers.LeakyReLU()\r\n        self.r1=layers.Reshape((7, 7, 256))\r\n        self.c1=layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)\r\n        self.r2=layers.Reshape((7, 7, 128))\r\n        self.c2=layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)\r\n        self.r3=layers.Reshape((14, 14, 64))\r\n        self.c3=layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\r\n        self.r4=layers.Reshape((28, 28, 1))\r\n\r\n    def call(self, inputs):\r\n        x=self.d1(inputs)\r\n        x=self.b1(x)\r\n        x=self.a1(x)\r\n        x=self.r1(x)\r\n        x=self.c1(x)\r\n        assert Model.output_shape == (None, 7, 7, 128)\r\n        x=self.b1(x)\r\n        x=self.a1(x)\r\n        x=self.c2(x)\r\n        \r\n        x=self.b1(x)\r\n        x=self.a1(x)\r\n        x=self.c3(x)\r\n        x=tf.math.sinh(x)/tf.math.cosh(x)\r\n        \r\n        return x\r\n\r\ndef make_generator_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Reshape((7, 7, 256)))\r\n    assert model.output_shape == (None, 7, 7, 256) # \u6ce8\u610f\uff1abatch size \u6ca1\u6709\u9650\u5236\r\n\r\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\r\n    assert model.output_shape == (None, 7, 7, 128)\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\r\n    assert model.output_shape == (None, 14, 14, 64)\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\r\n    assert model.output_shape == (None, 28, 28, 1)\r\n\r\n    return model\r\n\r\n\r\ndef make_discriminator_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\r\n                                     input_shape=[28, 28, 1]))\r\n    model.add(layers.LeakyReLU())\r\n    model.add(layers.Dropout(0.3))\r\n\r\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\r\n    model.add(layers.LeakyReLU())\r\n    model.add(layers.Dropout(0.3))\r\n\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dense(1))\r\n\r\n    return model\r\n\r\n\r\ndef discriminator_loss(real_output, fake_output):\r\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\r\n    total_loss = real_loss + fake_loss\r\n    return total_loss\r\n\r\ndef generator_loss(fake_output):\r\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\r\n\r\n\r\n@tf.function\r\ndef train_step(images):\r\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\r\n\r\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n      generated_images = generator(noise, training=True)\r\n\r\n      real_output = discriminator(images, training=True)\r\n      fake_output = discriminator(generated_images, training=True)\r\n\r\n      gen_loss = generator_loss(fake_output)\r\n      disc_loss = discriminator_loss(real_output, fake_output)\r\n\r\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n\r\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\r\n\r\n\r\ndef train(dataset, epochs):\r\n  for epoch in range(epochs):\r\n    start = time.time()\r\n\r\n    for image_batch in dataset:\r\n      train_step(image_batch)\r\n\r\n    display.clear_output(wait=True)\r\n    generate_and_save_images(generator,\r\n                             epoch + 1,\r\n                             seed)\r\n\r\n\r\n    if (epoch + 1) % 15 == 0:\r\n      checkpoint.save(file_prefix = checkpoint_prefix)\r\n\r\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\r\n\r\n\r\n  display.clear_output(wait=True)\r\n  generate_and_save_images(generator,\r\n                           epochs,\r\n                           seed)\r\n\r\ndef generate_and_save_images(model, epoch, test_input):\r\n\r\n  predictions = model(test_input, training=False)\r\n\r\n  fig = plt.figure(figsize=(4,4))\r\n\r\n  for i in range(predictions.shape[0]):\r\n      plt.subplot(4, 4, i+1)\r\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\r\n      plt.axis('off')\r\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\r\n # plt.show()\r\n\r\ndef display_image(epoch_no):\r\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\r\nsession = tf.compat.v1.Session(config=config)\r\n\r\n(train_images, train_labels), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\r\ntrain_images = (train_images - 127.5) / 127.5 # \u5c06\u56fe\u7247\u6807\u51c6\u5316\u5230 [-1, 1] \u533a\u95f4\u5185\r\n\r\nBUFFER_SIZE = 60000\r\nBATCH_SIZE = 256\r\n\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n\r\n\r\ngenerator =make_generator_model()\r\n\r\nnoise = tf.random.normal([1, 100])\r\ngenerated_image = generator(noise, training=False)\r\n\r\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')\r\nplt.show()\r\n\r\n\r\ndiscriminator = make_discriminator_model()\r\ndecision = discriminator(generated_image)\r\nprint (decision)\r\n\r\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-2)\r\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-2)\r\n\r\ncheckpoint_dir = './training_checkpoints'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\r\n                                 discriminator_optimizer=discriminator_optimizer,\r\n                                 generator=generator,\r\n                                 discriminator=discriminator)\r\n\r\nEPOCHS = 50\r\nnoise_dim = 100\r\nnum_examples_to_generate = 16\r\n\r\n\r\n\r\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\r\n\r\n\r\ntrain(train_dataset, EPOCHS)\r\n\r\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n\r\nanim_file = 'dcgan.gif'\r\n\r\nwith imageio.get_writer(anim_file, mode='I') as writer:\r\n  filenames = glob.glob('image*.png')\r\n  filenames = sorted(filenames)\r\n  last = -1\r\n  for i,filename in enumerate(filenames):\r\n    frame = 2*(i**0.5)\r\n    if round(frame) > round(last):\r\n      last = frame\r\n    else:\r\n      continue\r\n    image = imageio.imread(filename)\r\n    writer.append_data(image)\r\n  image = imageio.imread(filename)\r\n  writer.append_data(image)\r\n\r\nimport IPython\r\nif IPython.version_info > (6,2,0,''):\r\n  display.Image(filename=anim_file)\r\n\r\ntry:\r\n  from google.colab import files\r\nexcept ImportError:\r\n   pass\r\nelse:\r\n  files.download(anim_file)\r\n", "comments": ["@ymodak \r\nUnable to replicate due to \"runtime disconnected error\"", "Hyperbolic tangent activation functions `(tanh)` often suffer from vanishing gradient problem, You may want to try using `ReLU` activation function in this case.", "This model can't use ReLU in last Conv2DTranspose layer,   this test code from https://www.tensorflow.org/tutorials/generative/dcgan.  Now I use custom tanh function instead of activation='tanh' temporarily solve this problem.", "BTW,  I use pytorch recreate this model code. it's work is normal, so I guess activation='tanh' in tensorflow2.4 have issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46135\">No</a>\n"]}, {"number": 46134, "title": "NotFoundError: No CPU devices are available in this process", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.2\r\n- Python version: 2.7.17\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc 4.8\r\n\r\n\r\nHello!\r\nI have built TF-cpu 1.13.2 from source code and I'm trying to run a benchmark [tensorflow/benchmark](https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.13_compatible/scripts/tf_cnn_benchmarks)\r\nAn error (NotFoundError) occured after I run the following commands:\r\n`python tf_cnn_benchmarks.py --data_format=NHWC --num_gpus=0 --batch_size=8 --model=vgg16 --data_name=imagenet --variable_update=parameter_server --local_parameter_device=cpu --device=cpu > ps.log &`\r\n\r\nHowever tf can detect my CPU device correctly:\r\n```\r\nPython 2.7.17 (default, Sep 30 2020, 13:38:04)\r\n[GCC 7.5.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.python.client import device_lib\r\n>>> device_lib.list_local_devices()\r\n2021-01-04 08:35:20.085746: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 15041826958121108015\r\n]\r\n>>>\r\n```\r\n\r\nAny help would be appreciated. Thanks\uff01\r\n\r\nHere is my log:\r\n\r\n```\r\nroot@ip-172-31-90-169:/home/cluster/benchmarks/scripts/tf_cnn_benchmarks# python tf_cnn_benchmarks.py --data_format=NHWC --num_gpus=0 --batch_size=8 --model=vgg16 --data_name=imagenet --variable_update=parameter_server --local_parameter_device=cpu --device=cpu > ps.log &\r\n[1] 12899\r\nroot@ip-172-31-90-169:/home/cluster/benchmarks/scripts/tf_cnn_benchmarks# Traceback (most recent call last):\r\n  File \"tf_cnn_benchmarks.py\", line 74, in <module>\r\n    app.run(main)  # Raises error on invalid flags, unlike tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"tf_cnn_benchmarks.py\", line 63, in main\r\n    params = benchmark_cnn.setup(params)\r\n  File \"/home/cluster/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 3503, in setup\r\n    with tf.Session(config=create_config_proto(params)) as sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1551, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 676, in __init__\r\n    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No CPU devices are available in this process\r\n```", "comments": ["@ZYCCC927 \r\n\r\nPlease, see similar issue #45156 and see if it helps you.\r\nIf the issue still persists, can you please share more information or provide exact sequence of commands / steps that you executed before running into the problem. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46134\">No</a>\n"]}, {"number": 46133, "title": "Disable Eager Execution for the OPs related to Graphs and Sessions in Tensorflow 2.x", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope\r\n\r\n## Description of issue (what needs changing): \r\nThe command, **`tf.compat.v1.disable_eager_execution()`** should be added in the Code Examples of [the documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) as the `Variable Scope` of `Graphs` is not applicable during `Eager Execution`.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful? : For setting the **`Variable Scope`**.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : NA\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? : Yes\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : Yes", "comments": []}, {"number": 46131, "title": "TensorFlow", "body": "", "comments": ["@hoongywan \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps in localizing the issue faster. Thanks!\r\n", "Closing as spam"]}]