[{"number": 24009, "title": "Keras feature DropConnect", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.1.0 (Keras version: 2.2.2)\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to know if you would accept a pull request in ``DropConnect\u2019\u2019,i.e., https://cs.nyu.edu/~wanli/dropc/, which is a generalization of Hinton\u2019s Dropout. \r\nIn experiments, we may sometimes want to drop edges(weights) instead of neurons, I think it is very convenient to have the general version for regularizing layers. \r\nIf so, I am willing to work on this. I would add a flag to the operator so that users can decide it to run. This changes should not be difficult to make.\r\n\r\n**Will this change the current api? How?**\r\nNo. Probably based on current api. \r\n\r\n**Who will benefit with this feature?**\r\nUsers build their own architectures/algorithms.  \r\n**Any Other info.**\r\n", "comments": ["https://github.com/keras-team/keras/pull/9898#issuecomment-380590161", "/cc @abrad1212 let's see if it will enter from the `tf.keras` backdoor :wink: ", "would use", "@autoih,\r\nSorry for the delayed response. As per this comment, [this Layer](https://github.com/keras-team/keras/pull/9898#issuecomment-380590161) might not be very useful. It has not attracted the attention of the Developers from the Community as well. \r\n\r\nIf you feel this functionality is still relevant, please feel free to raise a PR. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24008, "title": "Create cloudbuild.yaml", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 24007, "title": "Update re2 library to 2018-10-01", "body": "This fix updates re2 library to the latest release of 2018-10-01\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Ping @gunan to take a look."]}, {"number": 24006, "title": "The tensorflow/tensorflow:custom-op is missing from DockerHub", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): docker pull tensorflow/tensorflow:custom-op\r\n- TensorFlow version: n/a\r\n- Python version: n/a\r\n- Installed using virtualenv? pip? conda?: `docker pull`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhile trying to pull `tensorflow/tensorflow:custom-op` to build custom ops, I noticed that `custom-op` tag is missing. (The tag was still there around 11/20/2018)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nubuntu@ubuntu:~/tensorflow_io$ docker pull tensorflow/tensorflow:custom-op\r\nError response from daemon: manifest for tensorflow/tensorflow:custom-op not found\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Sorry, this was my bad. I was doing doing some maintenance and deleted the `custom-op` image by mistake. I've restored the tag now."]}, {"number": 24005, "title": "tf.ConditionalAccumulator does not behave as expected", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): 1.10 cpu\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nResult of the accumulator.take_grad() not predictable if apply more gradients than num_required.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nNUM_REQUIRED = 3\r\n\r\na = np.arange(12).reshape((3, 4)).astype(np.float32)\r\ndataset = tf.data.Dataset.from_tensor_slices((a, a))\r\ndataset = dataset.repeat(1000).batch(1)\r\niterator = dataset.make_one_shot_iterator()\r\nx, y = iterator.get_next()\r\n\r\npredict = tf.layers.dense(x, 4)\r\nloss = tf.nn.l2_loss(predict - y)\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=0.01)\r\ntrainable_vars = tf.trainable_variables()[1]\r\n\r\ngrads_vars = optimizer.compute_gradients(loss, var_list=[trainable_vars])\r\ngrads = [grad_var[0] for grad_var in grads_vars]\r\nvars = [grad_var[1] for grad_var in grads_vars]\r\nshapes = [grad_var[1].get_shape() for grad_var in grads_vars]\r\n\r\nlocal_step = tf.get_variable('accu_local_step', dtype=tf.int32, initializer=0, trainable=False)\r\naccumulators = [tf.ConditionalAccumulator(tf.float32, shape=shapes[i]) for i in range(len(vars))]\r\nwith tf.control_dependencies([tf.assign_add(local_step, 1)]):\r\n    apply_ops = [accumulators[i].apply_grad(grads[i], local_step=local_step) for i in range(len(vars))]\r\n    apply_ops = tf.group(*apply_ops)\r\ngrads_mean = [accumulators[i].take_grad(num_required=NUM_REQUIRED) for i in range(len(vars))]\r\nnum_acc = [accumulators[i].num_accumulated() for i in range(len(vars))]\r\n\r\n# #\r\nsess_local = tf.train.MonitoredSession()\r\nfor i in range(100):\r\n    # print(i)\r\n    if (i + 1) % (NUM_REQUIRED + 1) != 0:\r\n        local_step_np, num_acc_np, grad_tmp, _ = sess_local.run([local_step, num_acc, grads, apply_ops])\r\n        # sess_local.run(reset_op)\r\n        print('local_step_np is: {}, num_ncc_np is: {}'.format(local_step_np, num_acc_np))\r\n        print(grad_tmp)\r\n    else:\r\n        # grad_tmp, _ = sess_local.run([grads, apply_ops])\r\n        # print(grad_tmp)\r\n        grad_tmp, num_acc_np, _, grads_mean_np = sess_local.run([grads, num_acc, apply_ops, grads_mean])\r\n        print('num_ncc_np is: {}'.format(num_acc_np))\r\n        print(grad_tmp)\r\n    # num_acc_np = sess_local.run(num_acc)\r\n        print('grads_mean_np:', grads_mean_np)\r\n```\r\n\r\n**Other info / logs**\r\nRuning the above code gives result:\r\nlocal_step_np is: 1, num_ncc_np is: [0]\r\n[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]\r\nlocal_step_np is: 2, num_ncc_np is: [1]\r\n[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]\r\nlocal_step_np is: 3, num_ncc_np is: [2]\r\n[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],\r\n      dtype=float32)]\r\nnum_ncc_np is: [0]\r\n[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]\r\ngrads_mean_np: [array([  3.1205528,   1.0842255,   3.9395618, -11.6566925], dtype=float32)]\r\nlocal_step_np is: 5, num_ncc_np is: [1]\r\n[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]\r\nlocal_step_np is: 6, num_ncc_np is: [2]\r\n[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],\r\n      dtype=float32)]\r\nlocal_step_np is: 7, num_ncc_np is: [3]\r\n[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]\r\nnum_ncc_np is: [4]\r\n[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]\r\ngrads_mean_np: [array([ 2.8342583,  1.3110911,  3.1894183, -9.993496 ], dtype=float32)]\r\nlocal_step_np is: 9, num_ncc_np is: [1]\r\n[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],\r\n      dtype=float32)]\r\nlocal_step_np is: 10, num_ncc_np is: [2]\r\n[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]\r\nlocal_step_np is: 11, num_ncc_np is: [3]\r\n[array([  3.120553 ,   1.0842252,   3.9395618, -11.656693 ], dtype=float32)]\r\nnum_ncc_np is: [0]\r\n[array([  4.26573   ,   0.17676353,   6.940136  , -18.30948   ],\r\n      dtype=float32)]\r\ngrads_mean_np: [array([  3.1205528,   1.0842254,   3.9395618, -11.656693 ], dtype=float32)]\r\nlocal_step_np is: 13, num_ncc_np is: [1]\r\n[array([ 1.9753755,  1.9916878,  0.9389882, -5.003906 ], dtype=float32)]\r\nlocal_step_np is: 14, num_ncc_np is: [2]\r\n\r\ngrads_mean_np is not the mean of previously applied grad. \r\n\r\nIf we just set run 'apply_ops' 'NUM_REQUIRED' times before run 'grads_mean'(take_grads()), ConditionalAccumulator seems to work properly, but when we run different number of 'apply_ops' before(as in the example, we run 'NUM_REQUIRED' + 1 times), the computed grads_mean(averaged grads) becomes mysterious. \r\n", "comments": ["@tanzhenyu are you familiar with ConditionalAccumulator?", "Hi @ylfzr!\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you checked in[ latest version](https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConditionalAccumulator) yet? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24004, "title": "systemlibs: Unbundle protobuf", "body": "Use system_link_files for protobuf.bzl. The protobuf.bzl file is taken\r\nverbatim from the protobuf repo. This version of protobuf.bzl will only\r\nbe used when opting into using the system version of protobuf and is off\r\nby default.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nI've had this patch in the Gentoo package for TF for the last several versions and not had any issues reported.\r\n@gunan @angersson for review", "comments": ["Cool, it got merged without any problems. Thanks!"]}, {"number": 24002, "title": "Segfault unless .so is compiled in debug mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.19.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWe compile a .so file, loosely based on the docs in https://www.tensorflow.org/xla/tfcompile\r\nWe can then compile a binary using that .so\r\nHowever, at runtime it hits a segfault.\r\n_However_, if we build the .so with `--compilation_mode=dbg` the binary works as expected.\r\n\r\n**Describe the expected behavior**\r\nThe binary that used the .so works without segfault whether or not the .so is compiled in debug mode or not.\r\n\r\n**Code to reproduce the issue**\r\nBUILD looks like this:\r\n```\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_copts\")\r\n\r\n# Use the tf_library macro to compile your graph into executable code.\r\ntf_library(\r\n    name = \"punctuation_decode\",\r\n    cpp_class = \"TensorflowXLA::PunctuationDecode\",\r\n    graph = \"final.pb\",\r\n    config = \"punctuation_decode.config.pbtxt\",\r\n    tfcompile_flags = \"--target_features=+avx\"\r\n)\r\n\r\nnative.cc_binary(\r\n  name = \"libpunctuation.so\",\r\n  srcs = [\"punctuation_decode_tfcompile_function.o\", \"punctuation_decode_tfcompile_metadata.o\", \"punctuation_decode.h\"],\r\n  linkshared=1,\r\n  linkstatic=1,\r\n  linkopts=[\"-fPIC\"],\r\n  deps = [\r\n          # TODO(cwhipkey): only depend on kernel code that the model actually needed.\r\n          # \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\r\n          # \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\r\n          # \"//tensorflow/compiler/aot:runtime\",\r\n          # \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n          \"//tensorflow/compiler/xla:executable_run_options\",\r\n          \"//third_party/eigen3\",\r\n          \"//tensorflow/core:framework_lite\",\r\n          ],\r\n  copts=tf_copts()\r\n)\r\n```\r\npunctuation_decode.config.pbtxt looks like:\r\n```\r\nfeed {\r\n  id { node_name: \"Model/Input/word\" }\r\n  shape {\r\n    dim { size: 1 }\r\n    dim { size: 13 }     # 1 + max_forward + max_backward\r\n  }\r\n}\r\n\r\nfeed {\r\n  id { node_name: \"Model/Input/punctuation\" }\r\n  shape {\r\n    dim { size: 1 }\r\n    dim { size: 1 }\r\n  }\r\n}\r\n\r\nfeed {\r\n  id { node_name: \"Model/Input/capitalisation\" }\r\n  shape {\r\n    dim { size: 1 }\r\n    dim { size: 1 }\r\n  }\r\n}\r\n\r\nfeed {\r\n  id { node_name: \"Model/Transformer/input_cache\" }\r\n  shape {\r\n    dim { size: 9 }     # num_hidden_layers - 1\r\n    dim { size: 1 }     # fixed (batch_size of 1 at test time)\r\n    dim { size: 8 }     # max_backward\r\n    dim { size: 128 }    # layer_size\r\n  }\r\n}\r\n\r\n\r\nfetch {\r\n  id { node_name: \"Model/Transformer/output_cache\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_0/cap/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_0/punc/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_1/cap/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_1/punc/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_2/cap/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_2/punc/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_3/cap/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_3/punc/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_4/cap/predicted\" }\r\n}\r\n\r\nfetch {\r\n  id { node_name: \"Model/Output/Delay_4/punc/predicted\" }\r\n}\r\n```\r\nThen in our binary cpp the code is called like this (simplified, parts removed for clarity):\r\n```\r\nconst int MAX_FORWARD=4;\r\nconst int MAX_BACKWARD=8;\r\nconst int LAYER_SIZE=128;\r\nconst int NUM_HIDDEN_LAYERS_MINUS_ONE=9;\r\n...\r\n    PunctuationDecode punctuator;\r\n    Eigen::ThreadPool tp(1);\r\n    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n    punctuator.set_thread_pool(&device);\r\n\r\n...\r\n    vector<int> word_idxs;\r\n    word_idxs = initialise_word_idxs(eos_idx, max_backward); // (this populates word_idxs with a long vector of ints)\r\n    vector<int> prev_cap;\r\n    prev_cap.push_back(0);\r\n    vector<int> prev_punc;\r\n    prev_punc.push_back(0);\r\n    float cache[NUM_HIDDEN_LAYERS_MINUS_ONE][1][MAX_BACKWARD][LAYER_SIZE] = {0.0};\r\n\r\n    // now iterate through, one buffer to be decoded at a time\r\n   int width=MAX_FORWARD + MAX_BACKWARD + 1;\r\n    for(int i=0;i<num_entries;i++) {\r\n        // get correct part of input feed of word indexes\r\n        vector<int> decode_buffer(word_idxs.begin() + i, word_idxs.begin() + i + width);\r\n\r\n        // feed the correct values to the TF method, run it\r\n        punctuator.set_arg0_data(decode_buffer.data());\r\n        punctuator.set_arg1_data(prev_punc.data());\r\n        punctuator.set_arg2_data(prev_cap.data());\r\n        punctuator.set_arg3_data(cache);\r\n        punctuator.Run();\r\n        // here it falls over if the .so is not compiled in debug mode\r\n    }\r\n```\r\nThe object works if compiled like: `bazel build -c opt //bazel_punctuation:libpunctuation.so --compilation_mode=dbg` but fails if compiled like: `bazel build -c opt //bazel_punctuation:libpunctuation.so`\r\n\r\n**Other info / logs**\r\nFull Valgrind output:\r\n```\r\nvalgrind ./decode /cantab/exp0/inbetweeners/punctuation/20180903iwslt/data/preprocessed/test.nopunc /cantab/exp0/inbetweeners/punctuation/2\r\n0180903iwslt/export/default_build\r\n==9068== Memcheck, a memory error detector\r\n==9068== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\r\n==9068== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info\r\n==9068== Command: ./decode /cantab/exp0/inbetweeners/punctuation/20180903iwslt/data/preprocessed/test.nopunc /cantab/exp0/inbetweeners/punctuation/20180903iwslt/export/default_build\r\n==9068== \r\nbefore main loop\r\nsetting feeds\r\nsetting prev punc feeds\r\nsetting prev cap feeds\r\nsetting cache feeds\r\nrunning punctuator\r\n==9068== Invalid read of size 4\r\n==9068==    at 0x5009807: void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>(float*) const (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x5015C94: Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eige$::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAs$ignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointe$> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generate$/libpunctuation.so)\r\n==9068==    by 0x502F1CA: __xla_cpu_runtime_EigenMatMulF32 (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x4EFCC37: __bazel_punctuation__punctuation_decode (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x413311: tensorflow::XlaCompiledCpuFunction::Run() (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x403548: greedy_decode(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std:$__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x403F18: decode_from_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/$unctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x4040B6: main (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==  Address 0x8 is not stack'd, malloc'd or (recently) free'd\r\n==9068== \r\n==9068== \r\n==9068== Process terminating with default action of signal 11 (SIGSEGV)\r\n==9068==  Access not within mapped region at address 0x8\r\n==9068==    at 0x5009807: void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<E$gen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>(float*) const (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/ge$erated/libpunctuation.so)\r\n==9068==    by 0x5015C94: Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x502F1CA: __xla_cpu_runtime_EigenMatMulF32 (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x4EFCC37: __bazel_punctuation__punctuation_decode (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x413311: tensorflow::XlaCompiledCpuFunction::Run() (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x403548: greedy_decode(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x403F18: decode_from_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x4040B6: main (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==  Address 0x8 is not stack'd, malloc'd or (recently) free'd\r\n==9068== \r\n==9068== \r\n==9068== Process terminating with default action of signal 11 (SIGSEGV)\r\n==9068==  Access not within mapped region at address 0x8\r\n==9068==    at 0x5009807: void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>::evalProduct<true, true, false, 0>(float*) const (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x5015C94: Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 0, long>, 16, Eigen::MakePointer> const> const> const&, Eigen::ThreadPoolDevice const&) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x502F1CA: __xla_cpu_runtime_EigenMatMulF32 (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x4EFCC37: __bazel_punctuation__punctuation_decode (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/generated/libpunctuation.so)\r\n==9068==    by 0x413311: tensorflow::XlaCompiledCpuFunction::Run() (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x403548: greedy_decode(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x403F18: decode_from_file(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==    by 0x4040B6: main (in /home/exp0/exp/inbetweeners/punctuation/20180903iwslt/export/punctuation/decode/decode)\r\n==9068==  If you believe this happened as a result of a stack\r\n==9068==  overflow in your program's main thread (unlikely but\r\n==9068==  possible), you can try to increase the size of the\r\n==9068==  main thread stack using the --main-stacksize= flag.\r\n==9068==  The main thread stack size used in this run was 8388608.\r\n==9068== \r\n==9068== HEAP SUMMARY:\r\n==9068==     in use at exit: 7,997,706 bytes in 96,216 blocks\r\n==9068==   total heap usage: 631,034,209 allocs, 630,937,993 frees, 41,411,688,816 bytes allocated\r\n==9068== \r\n==9068== LEAK SUMMARY:\r\n==9068==    definitely lost: 0 bytes in 0 blocks\r\n==9068==    indirectly lost: 0 bytes in 0 blocks\r\n==9068==      possibly lost: 0 bytes in 0 blocks\r\n==9068==    still reachable: 7,997,706 bytes in 96,216 blocks\r\n==9068==         suppressed: 0 bytes in 0 blocks\r\n==9068== Rerun with --leak-check=full to see details of leaked memory\r\n==9068== \r\n==9068== For counts of detected and suppressed errors, rerun with: -v\r\n==9068== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)\r\nSegmentation fault (core dumped)\r\n```\r\nGDB falls over like this:\r\n```\r\n(gdb) s\r\ntensorflow::XlaCompiledCpuFunction::Run (this=0x7fffffff45f0) at /cantab/exp0/inbetweeners/punctuation/20180903iwslt/export/punctuation/tensorflow/tensorflow/compiler/tf2xla/xla_compiled_cpu_function.cc:52\r\n52        raw_function_(buffer_table_[result_index_], &run_options_, nullptr,\r\n(gdb) s\r\n53                      buffer_table_, profile_counters_);\r\n(gdb) s\r\n52        raw_function_(buffer_table_[result_index_], &run_options_, nullptr,\r\n(gdb) s\r\n53                      buffer_table_, profile_counters_);\r\n(gdb) s\r\n\r\nThread 1 \"decode\" received signal SIGSEGV, Segmentation fault.\r\n__GI___libc_free (mem=0x10011) at malloc.c:2951\r\n2951    malloc.c: No such file or directory.\r\n```\r\n", "comments": ["For the record - the issue is solved with the latest (unreleased) TensorFlow code from master branch, specifically test was run against the following revision\r\n\r\n```\r\ncommit 25337d2065bd3ef79b9018714c0cb5af46ca06dc (HEAD -> master)\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Wed Dec 12 01:04:38 2018 -0800\r\n\r\n    compat: Update forward compatibility horizon to 2018-12-12\r\n    \r\n    PiperOrigin-RevId: 225140840\r\n```\r\n\r\n", "@TommoAsh   -   Can you try with tf nightly and see if this issue persists ?", "@hgadig I work with @TommoAsh and as per my previous comment, it's still an issue in the latest release, but appears solved on master branch already, even if I wasn't able to pin it to any specific Github issue.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 24001, "title": "AttributeError: module 'tensorflow._api.v1.lite' has no attribute 'OpsSet'", "body": "\r\n\r\n### System information\r\n- **OS Platform and Distribution (centos7.3)**:\r\n- **TensorFlow installed from  binary:from binary\r\n- **TensorFlow version **:1.12\r\n- **Python version**:3.6\r\n- **CUDA/cuDNN version**:9.1\r\n- **GPU model and memory**:K80\r\n\r\n\r\n### Describe the problem\r\nwhen i follow the tensorflow lite tutorial. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md\r\nit gets this error.\r\n### Source code / logs\r\nimport tensorflow as tf\r\nimport sys\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('model',input_shapes={\"input\":[1]})\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                         tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nconverter.post_training_quantize = True\r\ntflite_model_quantize = converter.convert()\r\nopen(\"converted_model_quantize_save.tflite\", \"wb\").write(tflite_model_quantize)\r\n\r\n\r\n\r\nINFO:tensorflow:Restoring parameters from model/variables/variables\r\nINFO:tensorflow:The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\r\nINFO:tensorflow:input tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: input\r\nINFO:tensorflow: tensor name: input_6:0, shape: (1), type: DT_FLOAT\r\nINFO:tensorflow:output tensors info: \r\nINFO:tensorflow:Tensor's key in saved_model's tensor_map: out\r\nINFO:tensorflow: tensor name: out_6:0, shape: (1), type: DT_FLOAT\r\nINFO:tensorflow:Restoring parameters from model/variables/variables\r\nINFO:tensorflow:Froze 1 variables.\r\nINFO:tensorflow:Converted 1 variables to const ops.\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-2cf72d9b817e> in <module>()\r\n      7 \r\n      8 converter = tf.lite.TFLiteConverter.from_saved_model('model',input_shapes={\"input\":[1]})\r\n----> 9 converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n     10                          tf.lite.OpsSet.SELECT_TF_OPS]\r\n     11 tflite_model = converter.convert()\r\n\r\nAttributeError: module 'tensorflow._api.v1.lite' has no attribute 'OpsSet'\r\n\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I have the same issue trying to convert model using select ops, as described in:\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md)\r\n\r\nCommand that I tried:\r\n`\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n`\r\n\r\n**OS:** Linux Fedora release 28\r\n**TF version:** 1.13.0-dev20181127 (binary from conda)\r\n\r\n", "I have the same problem (module 'tensorflow.contrib.lite.python.lite' has no attribute 'OpsSet') when trying to convert a frozen graph ,pb file to a TensorFlow Lite FlatBuffer file in Raspberry Pi 3B+ by running:\r\n\r\npython3 /home/pi/tensorflow/tensorflow/contrib/lite/python/tflite_convert.py --output_file=fruit_model.tflite --graph_def_file=frozen_graph.pb --input_arrays=X --output_arrays=softmax.\r\n\r\nI am getting the same error:\r\nAttributeError: module 'tensorflow.contrib.lite.python.lite' has no attribute 'OpsSet'\r\n\r\nTensorFlow version is 1.11.0.\r\n\r\nPlease help.\r\n\r\n\r\n", "Use `tflite_convert` tool instead of doing the conversion through code. That is what worked for me. I used nightly build for this (week old as of writing this post).\r\n\r\nEdit:` --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS `", "@hufangjian : please try the nightly build as bug@ suggested and let us know if it works!", "It does not work for me. I am using TensorFlow version is 1.11.0 on Raspberry Pi.  Here is the command line I am executing:\r\n\r\n./tflite_convert --output_file=/home/pi/sols/demo/src/image_classification/network/fruit_models/fruit_model.tflite --graph_def_file=/home/pi/sols/demo/src/image_classification/network/fruit_models/frozen_graph.pb --input_arrays=X --output_arrays=softmax --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\r\n\r\nHere is the error:\r\nRuntimeError: TOCO failed see console for info.\r\nb'/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module \\'tensorflow.python.framework.fast_tensor_util\\' does not match runtime version 3.5\\n  return f(*args, **kwds)\\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\\n  return f(*args, **kwds)\\nTraceback (most recent call last):\\n  File \"/bin/toco_from_protos\", line 7, in <module>\\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/toco_from_protos.py\", line 22, in <module>\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py\", line 28, in <module>\\n    _tensorflow_wrap_toco = swig_import_helper()\\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/tensorflow_wrap_toco.py\", line 24, in swig_import_helper\\n    _mod = imp.load_module(\\'_tensorflow_wrap_toco\\', fp, pathname, description)\\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\\n    return load_dynamic(name, filename, file)\\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\\n    return _load(spec)\\nImportError: /home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/toco/python/_tensorflow_wrap_toco.so: undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_\\n'\r\n\r\nHow can I install the nightly build in Raspberry Pi?  Doing this:\r\npip3 install --user --upgrade tf-nightly\r\ngives me this message:\r\n  Could not find a version that satisfies the requirement tf-nightly (from versions: )\r\nNo matching distribution found for tf-nightly\r\n\r\nAny other ideas?\r\n\r\n", "For tf.lite.OpsSet, note that it's available in the Python API as of this [commit](https://github.com/tensorflow/tensorflow/commit/d453dfad7c383db0ded338e8de3142a17d55910b#diff-abf1193f0cce5bd043e7e6d2357448a3) from 12/06, so it won't be available except in the tf-nightly build or the upcoming 1.13 release. \r\n\r\nWhat bug@ did should work also in the same (if not earlier) nightly build or upcoming release.\r\n\r\nAs for installing the nightly build on Raspberry Pi, please refer to this [guide](https://www.tensorflow.org/install/pip). The tf-nightly build is on PyPi (https://pypi.org/project/tf-nightly/#files) and if nothing works, I'm not personally too familiar with pip, and I suggest creating a separate issue to get help more quickly (since this is more about installation as opposed to the original issue).\r\n\r\nhufangjian@: I'm closing from the above. If it fails, please reopen and I'll look into it more closely.\r\n\r\n"]}, {"number": 24000, "title": "i changed the \"libtensorflow_demo.so\" file. Even then am facing the same issue", "body": "E/AndroidRuntime: FATAL EXCEPTION: IntentService[Prediction Service]\r\n    Process: com.cognizant.oralclassifier, PID: 25426\r\n    java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.prepareNativeRuntime(TensorFlowInferenceInterface.java:544)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:60)\r\n        at com.cognizant.oralclassifier.analyze.PredictionService.onHandleIntent(PredictionService.java:66)\r\n        at android.app.IntentService$ServiceHandler.handleMessage(IntentService.java:76)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\r\n_Originally posted by @sthuthyraj in https://github.com/tensorflow/tensorflow/issue_comments#issuecomment-442025141_", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23999, "title": "libiomp5 multiple initialization error message when dataset size is large enough", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **OS X El Capitan 10.11.6 (15G22010)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary (anaconda)**\r\noutput from `conda list tensorflow`: \r\n```\r\ntensorflow                1.12.0          mkl_py36h2b2bbaf_0  \r\ntensorflow-base           1.12.0          mkl_py36h70e0e9a_0 \r\n```\r\n- TensorFlow version (use command below): **b'unknown' 1.12.0**\r\n- Python version:\r\n**Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) **\r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n\r\n**Describe the current behavior**\r\nI get the following error message only under **specific circumstances** (see code below):\r\n\r\n> OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.\r\n> OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it candegrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\r\n> Abort trap: 6\r\n\r\n**Describe the expected behavior**\r\nNo error message\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef get_dense(input, widths, activations):\r\n    assert len(widths) == len(activations)\r\n    output = input\r\n    for i, (w, a) in enumerate(zip(widths, activations)):\r\n        output = tf.layers.dense(output, units=w, activation=a)\r\n    return output\r\n\r\nnn = lambda input, depth, width: get_dense(\r\n        input,\r\n        [width      for _ in range(depth - 1)] + [3   ],\r\n        [tf.nn.relu for _ in range(depth - 1)] + [None]\r\n    )\r\n\r\ntf.reset_default_graph()\r\n\r\n# Works fine when N = 8192,\r\n# but breaks with N = 8193:\r\nN = 8193\r\nn = 100\r\n\r\nX = np.random.normal(loc=0.0, scale=1., size=(N, 1)).astype(np.float32)\r\nnx = X.shape[1]\r\nY = X**2\r\n\r\nXY = np.concatenate([X, Y], axis=1)\r\n\r\nds = tf.data.Dataset.from_tensor_slices(XY).batch(n).make_one_shot_iterator().get_next()\r\n\r\npred = nn(ds[:,:nx], 2, 10)\r\nloss = tf.reduce_mean((pred - ds[:,nx:])**2)\r\nop = tf.train.RMSPropOptimizer(0.0005).minimize(loss)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    try:\r\n        for i in range(20):\r\n            print(i)\r\n            _, l1 = sess.run([op, loss])\r\n            print(l1)\r\n    except tf.errors.OutOfRangeError:\r\n        pass\r\n\r\nprint(\"Done\")\r\n```\r\n\r\n**Other info / logs**\r\nI would consider this an anaconda problem if only it happened at all times. However, what I find weird, and why I suspect it might be a tensorflow issue, is that it only occurs when input dataset size is large enough. In this example the error pops up when N is 8193 or higher, no error when N is 8192 or lower. This \"threshold\" value is different in my original project where I first faced the problem - there it happened when dataset array length was 100001 (100k+1) or higher, while working fine with 100000 (100k) long dataset. Apologies if this is irrelevant to tensorflow and indeed an anaconda issue.\r\n\r\nP.S.\r\nAdding KMP_DUPLICATE_LIB_OK=TRUE does silence the error.", "comments": ["By the way, there seems to be no such problem under tensorflow 1.11.0, at least I don't see it after the following downgrade:\r\n\r\n>     mkl:             2019.1-144                --> 2018.0.3-1               \r\n>     tensorboard:     1.12.0-py36hdc36e2c_0     --> 1.11.0-py36hdc36e2c_0    \r\n>     tensorflow:      1.12.0-mkl_py36h2b2bbaf_0 --> 1.11.0-mkl_py36h44b7a51_0\r\n>     tensorflow-base: 1.12.0-mkl_py36h70e0e9a_0 --> 1.11.0-mkl_py36h70e0e9a_0", "I was able to run your code snippet successfully in TF 1.10 and above. It doesn't look like TF related issue.I also increased N to a large number. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@ymodak Thanks a lot for looking into this. Then probably it's a problem of anaconda. So far I'm ok using 1.11. If I need 1.12 I'll first try not using the anaconda build.", "You are welcome. Try running the code as python script in terminal or jupyter notebook to validate if its anaconda issue. I will close this issue since it works fine. Feel free to reopen if you are still running into problems. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "I also have this issue. \r\n\r\nSolution 1, create anaconda envs using Python 3.5\r\nSolution 2, like workaround, Adding KMP_DUPLICATE_LIB_OK=TRUE as Environment Variable ", "I found an alternative solution to this problem [here](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_Known_Issues_Limitations.html), which is to preload the OpenMP runtime using the `LD_PRELOAD` variable:\r\n\r\n```\r\nLD_PRELOAD=<path_to_libiomp5.so> <path_to your_executable>\r\n```\r\n\r\nThis eliminates multiple loadings of libiomp, and makes all the components use this specific version of OpenMP."]}, {"number": 23998, "title": "fix typos", "body": "fix typo\r\n\r\nthat that -> that", "comments": []}, {"number": 23997, "title": "Tensor.eval() hangs with Tensorflow 1.10+", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\nCall to `Tensor.eval()` within a custom gradient function hangs forever (see minimal example below). This seems to happen with all Tensorflow releases >=1.10.0 while it does not happen with <=1.9.0.\r\n\r\n**Describe the expected behavior**\r\nThe tensor evaluates in the session.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nSESSION = None\r\n\r\n@tf.RegisterGradient(\"CustomGrad\")\r\ndef _relu_grad(unused_op, grad):\r\n    print (\"Grad override fun\")\r\n    # ISSUE HERE: The following line never returns in 1.10+\r\n    print (tf.ones((1,1)).eval(session=SESSION))\r\n    return grad\r\n\r\nwith tf.Session() as sess:\r\n    SESSION = sess\r\n    g = tf.get_default_graph()\r\n    with g.gradient_override_map({\"Relu\": \"CustomGrad\"}):\r\n        weights = tf.random_normal((2,2)) \r\n        result = tf.nn.relu(weights)\r\n        grad = tf.gradients(result, weights)[0]\r\n        print (grad.eval())\r\n```\r\nOutput with 1.9.0:\r\n```\r\nGrad override fun\r\n[[1.]]\r\n[[1. 1.]\r\n [1. 1.]]\r\n\r\n```\r\nOutput with 1.10.0:\r\n```\r\nGrad override fun\r\n--> the programs does not terminate and hangs forever here\r\n```", "comments": ["Don't use tf.RegisterGradient, it almost never does what you want it to. Instead use tf.custom_gradient.\r\n\r\nAlso do not use .eval() while building a graph; this is somewhat undefined behavior.\r\n"]}, {"number": 23996, "title": "Fix comparison between signed and unsigned integer expressions in ten\u2026", "body": "\u2026sor_format.h", "comments": ["@mrry  - Can the following kokoro failures be bypassed in order to proceed to get this merged ?", "The clang-format errors need to be fixed first. The Ubuntu and Window GPU failures look like flakes.", "> The clang-format errors need to be fixed first. The Ubuntu and Window GPU failures look like flakes.\r\n\r\n@Kh4L  Could you run clang-format-3.6.0 on this?", "@harshini-gadige done", "> @harshini-gadige done\r\n\r\n@mrry Can you please check now and approve ?"]}, {"number": 23995, "title": "dataset.output_shapes is (TensorShape([]), TensorShape([]))", "body": "\r\nI want to set steps per epoch according to the shape of the dataset, but when I print dataset.output_shapes, I find the result is None, like this:(TensorShape([]), TensorShape([])).", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23994, "title": "[tflite] python script for object detection w/ SSD", "body": "Add a TFLite Python script that uses SSD to detect objets.", "comments": ["@jch1, @achowdhery Hi, Could you PTAL and approve", "Hi @freedomtan, thanks for the PR. We're no longer adding additional examples in lite/examples; most of our examples have moved to the new [TF examples repo](https://github.com/tensorflow/examples/tree/master/lite). We're still trying to determine how we want to present Python-based examples. Once we have that figured out, I'll reopen the PR and we can proceed. Thanks again for your contribution, hopefully we can make this work.", "@jdduke Thanks. As you know, Python examples won't work on Android devices, but they work on many other environments and could helpful when testing if models work well with TFLite. I saw people asking for example scripts for object detection and segmentation several times before.", "Absolutely, Python examples are important and we'd like to expand the set of such examples. Once we have a path forward for how/where we'd like to host them we can get back to you about this particular contribution. Thanks again for your patience."]}, {"number": 23993, "title": "For max_pool_with_argmax op, GPU kernel keeps consistent with the behavior of CPU", "body": "Fix #22025\r\n\r\nRespect with [ tf.nn.max_pool_with_argmax: API](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax) , now GPU implementation returns flattened index `((b * height + y) * width + x) * channels + c`.", "comments": ["@poxvoculi  @rmlarsen Hi, could you take a look or reassign? Thanks.", "I checked those failed checks. Seems unrelated?", "@yifeif Hi, yifei. Could you take a look? Thank you :-)", "Thanks @facaiy!  @rmlarsen would you be the right person to review changes to maxpooling_op?", "Nagging Reviewer @rmlarsen: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied.", "Close it since #26562 is created to fix the issue."]}, {"number": 23992, "title": "Graph Transform Tool fails to optimize away useless Switch node", "body": "**System information**\r\n- TensorFlow version (you are using): v1.12.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe Graph Transform Tool apparently fails to optimize away Switch nodes that have constant pred input, as demonstrated in the test case. The test program shows a GraphDef that includes an unnecessary Switch node that could have been optimized away.\r\n\r\nWhen a Switch node has pred input that is constant, it should be possible to optimize the node away but currently this optimization does not happen. Consequently, when using the Graph Transform Tool to optimize a GraphDef for a network that is built to be used for training, eval and inference, that has a bool placeholder to differentiate the use case, nodes needed only for training remain even when the placeholder node has been converted to a constant False node and 'fold_constants' is specified to the Graph Transform Tool. Together with 'strip_unused_nodes', the desired optimization would result in other nodes also being removed that are used/available only during training.\r\n\r\nThis optimization could be provided as a new transform or as part of 'fold_constants'.\r\n\r\nTest:\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\n\r\nwith tf.Graph().as_default() as g:\r\n    pred = tf.constant(False, dtype=tf.bool, shape=(), name='pred')\r\n\r\n    def one(): return tf.constant(1, dtype=tf.int32, shape=(), name='one')\r\n\r\n    def zero(): return tf.constant(0, dtype=tf.int32, shape=(), name='zero')\r\n\r\n    r = tf.cond(pred, one, zero)\r\n\r\ngd = g.as_graph_def()\r\ntransforms = ['strip_unused_nodes', 'fold_constants', 'strip_unused_nodes']\r\ngd = TransformGraph(gd, [], [r.name], transforms)\r\nprint(gd)\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEspecially users who build one graph for both training and inference and differentiate the use case using a boolean placeholder that controls the behavior of the batch norm or dropout in the graph. This feature would help them get cleaner and smaller graphs for inference.\r\n\r\n**Any Other info.**\r\n", "comments": ["This is similar to #5919 but from one of the comments, I suppose that was for `tensorflow.python.tools.optimize_for_inference_lib`, not the Graph Transform Tool.", "I have the same problem.\r\n\r\nAfter optimized the model for deployment using Graph Transform Tool. Armnn raises error for this optimized model. After checking the network structure, I found all the Switch op are untouched. And the constant value input (which is of type 'DT_BOOL') to all the Switches are causing the trouble.\r\n\r\nAfter that, I tried to add 'op=Switch' to 'remove_nodes()', but the result pb file is exactly the same as before.\r\n\r\nApart from waiting for a new release, is there any workaround for now?", "Hello @jchia , your suggestion for optimization as part of the Graph Transform Tool for Switch nodes is useful.\r\nWe currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. \r\nWe will close this issue now. Thanks!\r\n"]}, {"number": 23991, "title": "Improve SEO score on index page", "body": "I'd like to update the home page on suggestion SEO improvements.  I'm having difficulty finding the index file.  \r\n\r\nThe first issues I'd to correct is related to links not having a descriptive text.  ", "comments": ["Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation."]}, {"number": 23990, "title": "Does GanEstimator support multi GPU ?", "body": "Does GanEstimator support multi gpu?\r\nHow could I using multi GPU by ganEstimator?\r\n\r\nThanks.", "comments": ["I found the only issue was the need to change check_for_unused_update_ops to False in [tensorflow/contrib/gan/python/train.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/train.py)\r\n\r\nThis is because the variable scope is prefixed depending on the tower, and doesn't start with either \"generator\" or \"discriminator\".\r\n\r\nOf course the code to do that, without changing the source, is very long:\r\n\r\n    # Custom estimator classes\r\n    class GanEstimatorTowerOpSafe(tf.contrib.gan.estimator.GANEstimator):\r\n        \"\"\"The only change is the use of new_get_estimator_spec\"\"\"\r\n        def __init__(self,\r\n                     model_dir=None,\r\n                     generator_fn=None,\r\n                     discriminator_fn=None,\r\n                     generator_loss_fn=None,\r\n                     discriminator_loss_fn=None,\r\n                     generator_optimizer=None,\r\n                     discriminator_optimizer=None,\r\n                     get_hooks_fn=None,\r\n                     get_eval_metric_ops_fn=None,\r\n                     add_summaries=None,\r\n                     use_loss_summaries=True,\r\n                     config=None,\r\n                     warm_start_from=None,\r\n                     is_chief=True):\r\n            if not callable(generator_loss_fn):\r\n                raise ValueError('generator_loss_fn must be callable.')\r\n            if not callable(discriminator_loss_fn):\r\n                raise ValueError('discriminator_loss_fn must be callable.')\r\n            if use_loss_summaries not in [True, False, None]:\r\n                raise ValueError('use_loss_summaries must be True, False or None.')\r\n            if get_hooks_fn is not None and not callable(get_hooks_fn):\r\n                raise TypeError('get_hooks_fn must be callable.')\r\n\r\n            from tensorflow.contrib.gan.python.estimator.python.gan_estimator_impl\\\r\n                import _get_gan_model\r\n\r\n            def _model_fn(features, labels, mode):\r\n                \"\"\"GANEstimator model function.\"\"\"\r\n                from tensorflow.python.estimator import model_fn as model_fn_lib\r\n                if mode not in [model_fn_lib.ModeKeys.TRAIN,\r\n                                model_fn_lib.ModeKeys.EVAL,\r\n                                model_fn_lib.ModeKeys.PREDICT]:\r\n                    raise ValueError('Mode not recognized: %s' % mode)\r\n                real_data = labels    # rename inputs for clarity\r\n                generator_inputs = features    # rename inputs for clarity\r\n\r\n                gan_model = _get_gan_model(\r\n                    mode, generator_fn, discriminator_fn, real_data,\r\n                    generator_inputs, add_summaries)\r\n\r\n                return new_get_estimator_spec(\r\n                    mode, gan_model, generator_loss_fn, discriminator_loss_fn,\r\n                    get_eval_metric_ops_fn, generator_optimizer,\r\n                    discriminator_optimizer,\r\n                    get_hooks_fn, use_loss_summaries, is_chief)\r\n\r\n            super(tf.contrib.gan.estimator.GANEstimator, self).__init__(\r\n                model_fn=_model_fn, model_dir=model_dir, config=config,\r\n                warm_start_from=warm_start_from)\r\n\r\n\r\n    def new_get_estimator_spec(\r\n        mode, gan_model, generator_loss_fn, discriminator_loss_fn,\r\n            get_eval_metric_ops_fn, generator_optimizer, discriminator_optimizer,\r\n            get_hooks_fn=None, use_loss_summaries=True, is_chief=True):\r\n        # The only difference and hence fix is check_for_unused_update_ops=False\r\n\r\n        from tensorflow.python.estimator import model_fn as model_fn_lib\r\n        from tensorflow.contrib.gan.python import namedtuples as tfgan_tuples\r\n        from tensorflow.contrib.gan.python import train as tfgan_train\r\n        from tensorflow.contrib.gan.python.estimator.python.gan_estimator_impl\\\r\n            import _get_eval_estimator_spec, _get_train_estimator_spec\r\n\r\n        if mode == model_fn_lib.ModeKeys.PREDICT:\r\n            estimator_spec = model_fn_lib.EstimatorSpec(\r\n                mode=mode, predictions=gan_model.generated_data)\r\n        else:\r\n            gan_loss = tfgan_tuples.GANLoss(\r\n                generator_loss=generator_loss_fn(\r\n                    gan_model, add_summaries=use_loss_summaries),\r\n                discriminator_loss=discriminator_loss_fn(\r\n                    gan_model, add_summaries=use_loss_summaries))\r\n            if mode == model_fn_lib.ModeKeys.EVAL:\r\n                estimator_spec = _get_eval_estimator_spec(\r\n                    gan_model, gan_loss, get_eval_metric_ops_fn)\r\n            else:  # model_fn_lib.ModeKeys.TRAIN:\r\n                gopt = (generator_optimizer() if callable(generator_optimizer) else\r\n                        generator_optimizer)\r\n                dopt = (discriminator_optimizer() if callable(\r\n                    discriminator_optimizer) else discriminator_optimizer)\r\n                get_hooks_fn = (get_hooks_fn or\r\n                                tfgan_train.get_sequential_train_hooks())\r\n\r\n                def new_train_op_fn(model, loss, generator_optimizer,\r\n                                    discriminator_optimizer, is_chief=True):\r\n                    return tfgan_train.gan_train_ops(\r\n                        model, loss, generator_optimizer, discriminator_optimizer,\r\n                        check_for_unused_update_ops=False, is_chief=is_chief)\r\n\r\n                estimator_spec = _get_train_estimator_spec(\r\n                    gan_model, gan_loss, gopt, dopt, get_hooks_fn,\r\n                    train_op_fn=new_train_op_fn, is_chief=is_chief)\r\n\r\n        return estimator_spec", "@nwestlake will it affect the training as other's tower update ops (like batch norm) will not get called? I see explicit filtering of update ops like `tower_1/Generator/...`", "I don't think it is supported. Because for training generator and discriminator at different ratios (as in update generator and discriminator at different rate) they have included the main training steps, i.e. update of discriminator and generator, as training hooks. But in case of estimator with multiGPU training, the trainning hooks from only the first replica (Tower) are run  ( Can be seen in this [line](https://github.com/tensorflow/estimator/blob/r1.13/tensorflow_estimator/python/estimator/estimator.py#L1269)). Hence even if u are using the multiple GPUs the training steps will be taken only once.", "@paigeQ,\r\nSorry for the delayed response. Can you please let us know if this issue is still relevant because there is no documentation corresponding to GAN Estimator in Tensorflow Documentation. Also, please refer the documentation of [Distribution Strategy for Estimator](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator), just in case if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23989, "title": "#23987: LARSOptimizer initializes _learning_rate_tensor", "body": "LARSOptimizer initializes _learning_rate_tensor and _momentum_tensor in _prepare method", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 23988, "title": "#23987: LARSOptimizer initializes _learning_rate_tensor", "body": "LARSOptimizer initializes _learning_rate_tensor and _momentum_tensor in _prepare method", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I used a different email address in the git commits. I have changed it."]}, {"number": 23987, "title": "LARSOptimizer does not initialize _learning_rate_tensor and _momentum_tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: tags/1.12\r\n- Python version: 2.7\r\n- cpu mode\r\n\r\n\r\n**Describe the current behavior**\r\n'LARSOptimizer' object has no attribute '_learning_rate_tensor'\r\n\r\n**Code to reproduce the issue**\r\n\\tensorflow\\tensorflow\\contrib\\opt\\python\\training\\lars_optimizer_test.py\r\n", "comments": ["class LARSOptimizer should implement _prepare() method.", "MR: https://github.com/tensorflow/tensorflow/pull/23989/commits/6d3d0de39f3eaecdbf7fbdb52a7a02b2539efeeb"]}, {"number": 23986, "title": "train.init_from_checkpoint does not support mirrorredStrategy and CollectiveAllReduceStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n   We are changing some code in https://github.com/google-research/bert run_classifier.py to make it can be run on machine with multiple GPUs\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Distributor ID:\tUbuntu\r\n  Description:\tUbuntu 16.04.5 LTS\r\n  Release:\t16.04\r\n  Codename:\txenial\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  NA\r\n\r\n- TensorFlow installed from (source or binary):\r\n  binary\r\n\r\n- TensorFlow version (use command below):\r\n  v1.11.0-0-gc19e29306c. and v1.12.0-0-ga6d8ffae09\r\n- Python version:\r\n  2.7.12\r\n\r\n- Bazel version (if compiling from source):\r\n  NA\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n  NA\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nIn Bert's source code, it will call tf.train.init_from_checkpoint(init_checkpoint, assignment_map) in the model_fn\r\n\r\nwe change some of the Bert's source code to make it can run on mirroredStrategy mode, but we met a failure with the call stack as below:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_classifier_collect.py\", line 859, in <module>\r\n    tf.app.run()\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_classifier_collect.py\", line 815, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 462, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py\", line 279, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 792, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 344, in _run_single_worker\r\n    worker_fn(strategy)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py\", line 246, in _worker_fn\r\n    hooks=hooks)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2409, in train\r\n    rendezvous.raise_errors()\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py\", line 128, in raise_errors\r\n    six.reraise(typ, value, traceback)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2403, in train\r\n    saving_listeners=saving_listeners\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1316, in _train_model_distributed\r\n    self.config)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/distribute.py\", line 721, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 556, in _call_for_each_tower\r\n    return _call_for_each_tower(self, fn, *args, **kwargs)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 183, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 177, in _call_for_each_tower\r\n    **merge_kwargs)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 211, in _init_from_checkpoint\r\n    var = _collect_partitioned_variable(current_var_or_name, store_vars)\r\n  File \"/data/anaconda2/envs/py27tf12/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 365, in _collect_partitioned_variable\r\n    if name + \"/part_0\" in all_vars:\r\nTypeError: unsupported operand type(s) for +: 'PerDevice' and 'str'\r\n\r\n\r\n\r\nwe can see  the code in init_from_checkpoint:\r\n  if distribution_strategy_context.get_cross_tower_context():\r\n    _init_from_checkpoint(None, ckpt_dir_or_file, assignment_map)\r\n  else:\r\n    distribution_strategy_context.get_tower_context().merge_call(\r\n        _init_from_checkpoint, ckpt_dir_or_file, assignment_map)\r\n\r\nseems that above code goes into the merge_call version\r\n\r\nI think init_from_checkpoint is indent to support cross_tower scenario, and seems that above code goes into the merge_call version in BERT's scenario\r\n\r\nbut the function _init_from_checkpoint does not properly handle the parameter passed across tower?\r\n \r\n\r\n**Describe the expected behavior**\r\ninit_from_checkpoint can be called successfully\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n1. Modified BERT's code to make it use MirrorredStrategy\r\n2. Modified BERT's optimization.py to make it can be run in distribution environment\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have the same issue.\r\nraceback (most recent call last):\r\n  File \"run_squad.py\", line 1285, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_squad.py\", line 1212, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1316, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py\", line 721, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 556, in _call_for_each_tower\r\n    return _call_for_each_tower(self, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 183, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 177, in _call_for_each_tower\r\n    **merge_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 213, in _init_from_checkpoint\r\n    var = _collect_partitioned_variable(current_var_or_name, store_vars)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/checkpoint_utils.py\", line 368, in _collect_partitioned_variable\r\n    if name + \"/part_0\" in all_vars:\r\nTypeError: unsupported operand type(s) for +: 'PerDevice' and 'str'\r\n\r\n\r\nJust like libliang said, There are some problems  in checkpoint_util.py\r\nThe assignment_map arg given to init_from_checkpoint are  {str:str} type,  but it is changed to {str:Perdevice} type in _init_from_checkpoint\r\n\r\nHow to handle the restore problem for PerDevice object ?\r\n\r\nPerDevice:{'/replica:0/task:0/device:GPU:0': 'bert/embeddings/LayerNorm/beta', '/replica:0/task:0/device:GPU:1': 'bert/embeddings/LayerNorm/beta', '/replica:0/task:0/device:GPU:2': 'bert/embeddings/LayerNorm/beta'},<class 'tensorflow.contrib.distribute.python.values.PerDevice'>", "Some work around is, you can first create a checkpoint with warm start from BERT using one GPU, say, train 10 steps. Then disable the scaffold(or the warm start init op) and do multiple GPU training. It's ugly, but works.\r\n\r\nFirst run with single GPU:\r\n```python\r\nwarm_start = True\r\n\r\ndef scaffold():\r\n    init_op = tf.train.init_from_checkpoint(\r\n        self.config.init_checkpoint, assignment_map)\r\n    return tf.train.Scaffold(init_op)\r\n\r\nif not warm_start:\r\n    train_scaffold = None\r\nelse:\r\n    train_scaffold = scaffold()\r\n```\r\n\r\nSecond run with MirroredStrategy:\r\n```python\r\nwarm_start = False\r\n\r\ndef scaffold():\r\n    init_op = tf.train.init_from_checkpoint(\r\n        self.config.init_checkpoint, assignment_map)\r\n    return tf.train.Scaffold(init_op)\r\n\r\nif not warm_start:\r\n    train_scaffold = None\r\nelse:\r\n    train_scaffold = scaffold()\r\n```\r\nUpdate:\r\nI have figured out a better solution: use a train hook to initialize from checkpoint.\r\n\r\nYou can refer to this script: \r\nhttps://github.com/JayYip/bert-multiple-gpu/blob/master/src/ckpt_restore_hook.py", "I don't think it's an issue of TensorFlow.\r\nAn alternative solution is modify modeling.py in BERT codes, generating assignment map as string->variable format, instead of string->string.\r\nSpecifically, change line 338 of modeling.py from `assignment_map[name]=name` to `assignment_map[name]=name_to_variable[name]`, and mirroredStrategy works well.", "> I don't think it's an issue of TensorFlow.\r\n> An alternative solution is modify modeling.py in BERT codes, generating assignment map as string->variable format, instead of string->string.\r\n> Specifically, change line 338 of modeling.py from `assignment_map[name]=name` to `assignment_map[name]=name_to_variable[name]`, and mirroredStrategy works well.\r\n\r\nThat's PERFECT \uff01Thank you.", "@sunyerui thanks a lot for your fix! But right after that MirroredStrategy still fails with \r\n\r\n`ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context\"`\r\n\r\n(here's the entire error message if needed:\r\n\r\n`INFO:tensorflow:Error reported to Coordinator: You must specify an aggregation method to update a MirroredVariable in Tower Context.\r\nTraceback (most recent call last):\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2195, in _call_model_fn\r\n    features, labels, mode, config)\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2479, in _model_fn\r\n    features, labels, is_export_mode=is_export_mode)\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1259, in call_without_tpu\r\n    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1533, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"run_pretraining.py\", line 179, in model_fn\r\n    total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n  File \"/home/borislav/bert/optimization.py\", line 77, in create_optimizer\r\n    zip(grads, tvars), global_step=global_step)\r\n  File \"/home/borislav/bert/optimization.py\", line 151, in apply_gradients\r\n    [param.assign(next_param),\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 402, in assign\r\n    return self._assign_func(f=assign_fn, *args, **kwargs)\r\n  File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 379, in _assign_func\r\n    raise ValueError(\"You must specify an aggregation method to update a \"\r\nValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.`", "> @sunyerui thanks a lot for your fix! But right after that MirroredStrategy still fails with\r\n> \r\n> `ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context\"`\r\n> \r\n> (here's the entire error message if needed:\r\n> \r\n> `INFO:tensorflow:Error reported to Coordinator: You must specify an aggregation method to update a MirroredVariable in Tower Context. Traceback (most recent call last): File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception yield File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2195, in _call_model_fn features, labels, mode, config) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2479, in _model_fn features, labels, is_export_mode=is_export_mode) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1259, in call_without_tpu return self._call_model_fn(features, labels, is_export_mode=is_export_mode) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1533, in _call_model_fn estimator_spec = self._model_fn(features=features, **kwargs) File \"run_pretraining.py\", line 179, in model_fn total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu) File \"/home/borislav/bert/optimization.py\", line 77, in create_optimizer zip(grads, tvars), global_step=global_step) File \"/home/borislav/bert/optimization.py\", line 151, in apply_gradients [param.assign(next_param), File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 402, in assign return self._assign_func(f=assign_fn, *args, **kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 379, in _assign_func raise ValueError(\"You must specify an aggregation method to update a \" ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.`\r\n\r\nThis is not related to this issue. You need to re-implement the optimizer.\r\n\r\nYou can take my implementation as a reference:\r\nhttps://github.com/JayYip/bert-multiple-gpu/blob/master/src/optimizer.py", "@JayYip is right, in order to use mirroredStrategy, we need to re-implement the optimizer, because the  current version of optimizer in BERT does not support merge_call, which is required by mirroredStrategy", "Thanks @sunyerui 's solution, which can work fine in the current model_fn code, but we still need to consider one thing is, the logic in model_fn will be executed in every tower, that means the variables will be initialized the as much as the number of gpu, I suspect we still have room to do more improvement to NOT make such redundant initialization", "> Thanks @sunyerui 's solution, which can work fine in the current model_fn code, but we still need to consider one thing is, the logic in model_fn will be executed in every tower, that means the variables will be initialized the as much as the number of gpu, I suspect we still have room to do more improvement to NOT make such redundant initialization\r\n\r\nPlease see my comment above. I think using a session run hook to restore variables will only initialize once. Please correct me if I am wrong.", "Can anyone release multi-gpus pretraining code about bert(tensorflow) using mirroredstragegy", "There is a PR that should fix this issue: https://github.com/tensorflow/tensorflow/pull/24245/files . You can try it out or wait until it is merged.", "Closing this bug since the PR is merged.", "which version of tensorflow has this fix? I m trying 1.12.2 but still facing same error. ", "Hi, some question as above. I'm trying 1.13.1 but still facing the same error.", "Hi, some question as above. I'm trying 1.14.0, but still facing the same error.", "> > @sunyerui thanks a lot for your fix! But right after that MirroredStrategy still fails with\r\n> > `ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context\"`\r\n> > (here's the entire error message if needed:\r\n> > `INFO:tensorflow:Error reported to Coordinator: You must specify an aggregation method to update a MirroredVariable in Tower Context. Traceback (most recent call last): File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception yield File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2195, in _call_model_fn features, labels, mode, config) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2479, in _model_fn features, labels, is_export_mode=is_export_mode) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1259, in call_without_tpu return self._call_model_fn(features, labels, is_export_mode=is_export_mode) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1533, in _call_model_fn estimator_spec = self._model_fn(features=features, **kwargs) File \"run_pretraining.py\", line 179, in model_fn total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu) File \"/home/borislav/bert/optimization.py\", line 77, in create_optimizer zip(grads, tvars), global_step=global_step) File \"/home/borislav/bert/optimization.py\", line 151, in apply_gradients [param.assign(next_param), File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 402, in assign return self._assign_func(f=assign_fn, *args, **kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 379, in _assign_func raise ValueError(\"You must specify an aggregation method to update a \" ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.`\r\n> \r\n> This is not related to this issue. You need to re-implement the optimizer.\r\n> \r\n> You can take my implementation as a reference:\r\n> https://github.com/JayYip/bert-multiple-gpu/blob/master/src/optimizer.py\r\n\r\nhi,could give me your optimizer?? i have same question,thank you ", "> > @sunyerui thanks a lot for your fix! But right after that MirroredStrategy still fails with\r\n> > `ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context\"`\r\n> > (here's the entire error message if needed:\r\n> > `INFO:tensorflow:Error reported to Coordinator: You must specify an aggregation method to update a MirroredVariable in Tower Context. Traceback (most recent call last): File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception yield File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run self.main_result = self.main_fn(*self.main_args, **self.main_kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2195, in _call_model_fn features, labels, mode, config) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn model_fn_results = self._model_fn(features=features, **kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2479, in _model_fn features, labels, is_export_mode=is_export_mode) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1259, in call_without_tpu return self._call_model_fn(features, labels, is_export_mode=is_export_mode) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1533, in _call_model_fn estimator_spec = self._model_fn(features=features, **kwargs) File \"run_pretraining.py\", line 179, in model_fn total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu) File \"/home/borislav/bert/optimization.py\", line 77, in create_optimizer zip(grads, tvars), global_step=global_step) File \"/home/borislav/bert/optimization.py\", line 151, in apply_gradients [param.assign(next_param), File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 402, in assign return self._assign_func(f=assign_fn, *args, **kwargs) File \"/home/borislav/tf/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 379, in _assign_func raise ValueError(\"You must specify an aggregation method to update a \" ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context.`\r\n> \r\n> This is not related to this issue. You need to re-implement the optimizer.\r\n> \r\n> You can take my implementation as a reference:\r\n> https://github.com/JayYip/bert-multiple-gpu/blob/master/src/optimizer.py\r\n\r\nhi,i have same question,could you give me your optimizer.py \uff1f\uff1f Thanks you "]}, {"number": 23985, "title": "What is the encoding of a .tflite file?", "body": "", "comments": ["Any answer to this ?\r\nHow are tflite files encoded ?\r\n"]}, {"number": 23984, "title": "tensorflow  - gpu issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nSystem information\r\n\r\nOS : Windows 10\r\nTensorFlow installed from (source or binary): from pip\r\nTensorFlow version: 1.11.0\r\nPython version: 3.6\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): No\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 10\r\nGPU model and memory: Nvidia 1050Ti\r\n\r\n\r\n**Describe the problem**\r\nRecently I tried updating my tensor flow and later on, my tensorflow-gpu stopped working.\r\nNow I have downgraded to 1.11.0 but then still my tensorflow-gpu is not working.\r\n\r\nnvidia-smi is working fine. \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 416.34       Driver Version: 416.34       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   48C    P8    N/A /  N/A |     78MiB /  4096MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|==================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\ntf.test.is_gpu_available(\r\ncuda_only=False,\r\nmin_cuda_compute_capability=None\r\n)\r\n\r\nI ran this, its coming false\r\n\r\nthen , \r\nI ran this sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nDevice mapping: no known devices.\r\n2018-11-26 17:27:10.277030: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:\r\n\r\nit's coming empty, I read lot of threads usually people had error with cuda, by installing that it worked for lot of people.\r\n\r\nBut then in my case things were working fine earlier, now it got messed up and cuda is installed properly.\r\n\r\nwhy should I do?\r\n\r\n", "comments": ["Duplicate of #23726", "I'm Cuda 9, I have downgraded that and still facing the same issue", "Your nvidia-smi output reflects that you are using CUDA 10. Can you confirm? I would recommend doing a fresh install using cuda 9 if possible.", "Okay, I see that. Now, what should I do, just cuda 9 reinstall will work? since I have already uninstalled cuda 10 long back and tensorflow gpu was working so far in this setup", "Firstly uninstall cuda 10 toolkit and install cuda 9 later. In addition to that you will have to update the environment variables paths as well. Refer this [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup) to know more.", "![image](https://user-images.githubusercontent.com/12907396/49061649-87480080-f1ce-11e8-94ce-26dc2bf8fe1b.png)\r\n\r\nI don't have Cuda 10 in my system, I'm not sure why it is showing up in nvidia-smi", "@gunan Can you please take a look? Thanks!\r\n", "Now i'm getting this error too,\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Tools\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Tools\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Tools\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Tools\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nI checked the other threads regarding this DLL issue with Cuda 10.\r\n\r\nBut then, I don't have Cuda 10 installed in my system", "I fixed the issue, thanks for the help  :)", "Glad it worked for you. Can you please comment below the steps you followed? Thanks!", "I'm still not sure whats the exact issue is, because in Nvidia-smi still shows Cuda 10.\r\n\r\nSteps followed:\r\nPretty much reinstalled everything from scratch :  tensorflow, tensorflow- gpu, cuda ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23983, "title": "Can't import tensorflow", "body": "**System information**\r\n- OS Platform and Distribution: Windows\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version: Cannot check as tensorflow cannot be imported\r\n- Python version: Python 3.5.5 :: Anaconda, Inc.\r\n- Installed using virtualenv: conda\r\n- GCC/Compiler version (if compiling from source): (tdm64-1) 5.1.0\r\n\r\n**Problem**\r\n\r\nI had a working version of tensorflow-gpu. I decided to upgrade it since I had an older version. However, after upgrading it, whenever I try to import it, I now get an error.\r\n\r\nI ran the command:\r\n\r\n```\r\nconda install -c anaconda tensorflow-gpu \r\n```\r\nAfter that it installed and everything went fine. But now, I cannot import tensorflow anymore. The stack trace is shown below.\r\n\r\n**Logs**\r\n>Traceback (most recent call last):\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 243, in load\r\n_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 343, in load\r\n_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\n> During handling of the above exception, another exception occurred:\r\n\r\n> Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\_\r\n_init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\p\r\nython\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 243, in load\r\n_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Shoaib\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 343, in load\r\n_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\n\r\n> Failed to load the native TensorFlow runtime.\r\n\r\n> See https://www.tensorflow.org/install/errors\r\n\r\n> for some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nCould anyone please tell me what could be causing this? Any help would be appreciated.\r\nThank you so much.", "comments": ["@Mohammed-Shoaib  Hi, please refer a similar issue #10033 which helps you to resolve the problem. ", "Hi,\r\n\r\nI went to the issue before as well, I tried the steps again. According to the solution, I added cuda to my path variable.\r\nAccording to @mrry I was also supposed to add cuda\\bin to my path variable. However, cuda\\bin was not present in the cuda directory I had. So I decided to reinstall everything.\r\n\r\nInstalled cuda toolkit 9.0 and with it I installed cuDNN 7.0.5, followed the steps according to the installation guide. Reinstalled tensorflow-gpu, added cuda and cuda\\bin to my path variable. However, I still ended up getting the same error. I am still not sure as to why.\r\n\r\nIn the end, I just went ahead and installed default tensorflow which is for cpu and I did not get any errors after that. Anyways, I appreciate your help and support, so thank you for that.", "> Hi,\r\n> \r\n> I went to the issue before as well, I tried the steps again. According to the solution, I added cuda to my path variable.\r\n> According to @mrry I was also supposed to add cuda\\bin to my path variable. However, cuda\\bin was not present in the cuda directory I had. So I decided to reinstall everything.\r\n> \r\n> Installed cuda toolkit 9.0 and with it I installed cuDNN 7.0.5, followed the steps according to the installation guide. Reinstalled tensorflow-gpu, added cuda and cuda\\bin to my path variable. However, I still ended up getting the same error. I am still not sure as to why.\r\n> \r\n> In the end, I just went ahead and installed default tensorflow which is for cpu and I did not get any errors after that. Anyways, I appreciate your help and support, so thank you for that.\r\n\r\n@mrry Any thoughts please ?", "This is most likely a conflict between the version of CUDA or cuDNN you have installed and the version used by the version of TensorFlow you installed. It should be possible to tell what version of TensorFlow you have installed using `pip show tensorflow-gpu` (or `pip show tensorflow` for the CPU-only version). For example, the current version (1.12) requires cuDNN 7.2 or later, and the other requirements can be found [here](https://www.tensorflow.org/install/gpu#software_requirements).", "Hi,\r\n\r\nI finally got the gpu version of tensorflow working! So, I am still not sure as to what the issue was exactly, but I think as @mrry pointed out, I may have had conflicting versions of CUDA or cuDNN installed. Hence, I decided to start afresh, for the last time.\r\n\r\nI followed a YouTube tutorial this time on installing tensorflow for gpu link to which can be found [here](https://www.youtube.com/watch?v=MnMYCPc82xI).\r\n\r\nI started off by reinstalling [CUDA Toolkit 9.0](https://developer.nvidia.com/cuda-90-download-archive). I also went ahead and ran the patch files present. Next, I downloaded cuDNN 7.0.5 from the [archive](https://developer.nvidia.com/rdp/cudnn-archive). Took the `cudnn64_7.dll` file from `cudnn-9.0-windows7-x64-v7\\cuda\\bin` and pasted it in `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin`.\r\n\r\nWith the NVIDIA dependencies installed, it was time to install the python packages. I started off by creating a new virtual environment, then installed all the dependencies and installed the gpu version.\r\nI used Anaconda and the commands are given below:\r\n```\r\nconda  update conda\r\nconda create \u2013n tensorflow-gpu python=3.6 pip\r\nactivate tensorflow-gpu\r\npip install --ignore-installed --upgrade tensorflow-gpu\r\n```\r\nAnd that's it! After that I had a working version of tensorflow-gpu with no errors.\r\n\r\nThank you so much @harshini-gadige and @mrry for all your help and support! Appreciate it :)", "@Mohammed-Shoaib  Glad to know that your issue is resolved. Thank you !", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23982, "title": "V1.12.0 tf disable mkl testing", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 23981, "title": "Reverting the changes in commit 0c1eb88 which causes Resnet50 accuracy to drop significantly", "body": "Commit 0c1eb88 included some changes that cause the inference accuracy of trained Resnet50 to drop to zero. Reverting these changes fixes the issue.", "comments": ["Nagging Reviewer @drpngx: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 23980, "title": "Classify Java ops in API definitions", "body": "This PR classifies the Java operators so that they do not all end up in the main (core) package, e.g. the `Sqrt` operator is now accessed from `ops.math.sqrt(...)` instead of `ops.sqrt(...)`.\r\n\r\nThe classification is following pretty the one used in Python V2 API. Here's the list of additional packages that are generated using it and a few exceptions to make it work in Java:\r\n```\r\norg.tensorflow.op.audio\r\norg.tensorflow.op.bitwise\r\norg.tensorflow.op.collective\r\norg.tensorflow.op.core\r\norg.tensorflow.op.data\r\norg.tensorflow.op.dtypes\r\norg.tensorflow.op.image\r\norg.tensorflow.op.io\r\norg.tensorflow.op.linalg\r\norg.tensorflow.op.math\r\norg.tensorflow.op.nn\r\norg.tensorflow.op.quantization\r\norg.tensorflow.op.random\r\norg.tensorflow.op.signal\r\norg.tensorflow.op.sparse\r\norg.tensorflow.op.strings\r\norg.tensorflow.op.summary\r\norg.tensorflow.op.train\r\n```\r\nExceptions for Java: \r\n* `Assert` is renamed to `AssertThat` to avoid conflict with `assert` keyword\r\n* `Switch` is renamed to `SwitchCond` to avoid conflict with `switch` keyword\r\n\r\nCC: @asimshankar , @annarev , @martinwicke ", "comments": ["Thanks! \r\n\r\n@annarev : Does it make sense to collapse many of these endpoints into the \"base\" api_def so they are automatically shared across languages, instead of having to duplicate them between Python and Java?", "Interesting, it was definitely compiling locally, I\u2019ll take a look as soon as I can", "@asimshankar : ok found the case, both classes are generated: `Conv3DBackpropFilter` and `Conv3dBackpropFilter` (the former is from `Conv3DBackpropFilter` op and the later is from `Conv3DBackpropFilterV2` op).\r\n\r\nI don't know why this specific test it was failing since in both files the classes was named correctly but I'll rename them to avoid this conflict. I suggest to drop all ops having a V2 version at this point, what do you think?\r\n\r\nAlso, according to [this document](https://google.github.io/styleguide/javaguide.html#s5.3-camel-case), the form with lowercase `3d` should prevail.", "all right, I pushed a new version using lowercase accronyms exclusively and renamed the conflicting op class to `Conv3dBackpropFilterV2`.\r\n\r\n@asimshankar , if you think we should also get rid of any version that has been succeeded, just let me know (e.g. if there is `MyOp`, `MyOpV2` and `MyOpV3` available, we only generate a class for the last one that we rename `MyOp`).", "> all right, I pushed a new version using lowercase accronyms exclusively and renamed the conflicting op class to `Conv3dBackpropFilterV2`.\r\n> \r\n> @asimshankar , if you think we should also get rid of any version that has been succeeded, just let me know (e.g. if there is `MyOp`, `MyOpV2` and `MyOpV3` available, we only generate a class for the last one that we rename `MyOp`).\r\n\r\n@asimshankar   Any update on the approval ? Please keep us posted.", "@karllessard : Yeah, makes sense to only generate for the latest version and drop the older ones. ", "> @karllessard : Yeah, makes sense to only generate for the latest version and drop the older ones.\r\n\r\n@asimshankar I'm having some doubts now and would like to have your advice: if we delete previous versions, how will we handle the next ones? \r\n\r\nFor example, if we are at `MyOpV2` right now, we'll delete `MyOp` and rename endpoint of V2 as `MyOp`... but when V3 appears, who's gonna be `MyOp`? V3? If so, for backward compatibility, I guess we'll have to keep `MyOpV2` but with its original name (forcing the clients to update their code)?\r\n\r\nOr maybe @annarev you have something to share about this?", "> > @karllessard : Yeah, makes sense to only generate for the latest version and drop the older ones.\r\n> \r\n> @asimshankar I'm having some doubts now and would like to have your advice: if we delete previous versions, how will we handle the next ones?\r\n> \r\n> For example, if we are at `MyOpV2` right now, we'll delete `MyOp` and rename endpoint of V2 as `MyOp`... but when V3 appears, who's gonna be `MyOp`? V3? If so, for backward compatibility, I guess we'll have to keep `MyOpV2` but with its original name?\r\n\r\nWe'll do exactly what we do in Python - where we keep the endpoint name the same but change the implementation so that it invokes the new operation. For example, consider this really old change where a `GatherV2` operation was added and we set things up so that the Python `tf.gather` function adds the `GatherV2` operation (see [change to `array_ops.py` in this commit](https://github.com/tensorflow/tensorflow/commit/b1f9e2c89eb007cb4b9483d08dcace1e45e84164#diff-68052049ee5dc7573829a5fdb7a57310R2405))\r\n\r\nFor the example you provided, yes, this amounts to the Java class `org.tensorflow.op.mypkg.MyOp` adding the `MyOpV3` operation to the graph instead of the `MyOpV2` operation. Does that make sense?", "In Java, the endpoints are automatically generated from the signature of their op, so it will work only if V3 has a signature backward compatible with V2. To support other cases, we\u2019ll need to create a wrapper class that will \u201chide\u201d the original ops. That shouldn\u2019t be a problem though and will probably be exceptional.\r\n\r\nOk, I\u2019ll clean up the ops tonight to keep only the last version available, thanks!", "@asimshankar done!", "Note: I experimented a few problems after forcing the version upgrade. \r\n\r\nFor example, `PlaceholderV2` cannot be used with current GraphDef and must be replaced by `Placeholder` (sounds like V2 is older). Also, `AddV2` is not a good candidate since there is no gradient computation attached to it, like `Add` has.\r\n\r\nI guess we'll discover more of those unusual cases and need to adjust as we go.", "@asimshankar is it too late to update this PR once again? I want to make visible those ops that are hidden by default from the `base_api` defs, since there is no wrapper in Java for them (~140 ops, ex: `WriteSummary`).\r\n\r\nOtherwise I'll just create another PR for this, thanks\r\n\r\nNote: In Java, hidden ops can still be constructed and added to a graph using their factory method directly. They are only hidden in the more-convenient `Ops` interface.", "I wouldn't override the hidden ops. Ops are generally marked hidden because they typically require some additional outside-of-graph pre-processing before being useful, i.e., we'd want a hand-crafted Java class instead of having clients use the op directly. Similar to some Python API endpoints.", "Ok, sounds good.", "@harshini-gadige , @asimshankar : is there anything left to be done for this PR to get merged? I thought the review was completed", "> @harshini-gadige , @asimshankar : is there anything left to be done for this PR to get merged? I thought the review was completed\r\n\r\n@karllessard  If  the reviewer approves it, I can proceed further to help this get merged. Before that please make sure you made all changes as suggested by the reviewer(if any). ", "I think I did, @asimshankar?", "@harshini-gadige sorry for bugging you again with this but can we target to merge this PR for 1.13 release? Thanks", "Something got stuck in the tooling. I'm looking into it.", "Really sorry for missing all the questions here earlier!\r\n\r\n@asimshankar Yep, definitely makes sense to collapse into base api_defs if all languages use the same namespaces. Is it true for C++ and Go?\r\n\r\n@karllessard: \"For example, if we are at MyOpV2 right now, we'll delete MyOp and rename endpoint of V2 as MyOp... but when V3 appears, who's gonna be MyOp? V3? If so, for backward compatibility, I guess we'll have to keep MyOpV2 but with its original name (forcing the clients to update their code)?\"\r\n\r\nAre you asking about op name at C++ layer or at Java API layer? At C++ layer, we don't deprecate ops as far as I know, therefore MyOp in V3 would need to be named MyOpV3. At Java layer, we can break backwards compatibility at a major release. That is, when we release TF 3.0, we can name V3 version MyOp. Older versions could either be removed or renamed.\r\n\r\nOn Python side, we provide a `tf.compat.v1` module that keeps V1 names available in V2. For e.g. tf.compat.v1.foo would still point to 1.* version of `foo` even though `tf.foo` points to the 2.* version. May be something like this can be done on Java side too.\r\n\r\nWe also provide a script for Python API that renames endpoints for 2.0 upgrade. For e.g. that script could auto rename 'MyOp' to 'MyOpV2' if 'MyOpV2' is corresponding 2.* name.\r\n\r\n", "> At Java layer, we can break backwards compatibility at a major release.\r\n\r\nOk that make sense, thanks Anna"]}, {"number": 23979, "title": "Save/Load problem with keras.layers.ReLU", "body": "Running this example\r\n```\r\nx = tf.keras.layers.Input([1])\r\ny = tf.keras.layers.ReLU()(x)\r\nmdl = tf.keras.models.Model(x, y)\r\ntf.keras.models.save_model(mdl, \"./keras_model_tmp.hdf5\", overwrite=True)\r\ntf.keras.models.load_model(\"./keras_model_tmp.hdf5\")\r\n```\r\nreturns trackback\r\n```\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/saving.pyc in load_model(filepath, custom_objects, compile)\r\n    228       raise ValueError('No model found in config file.')\r\n    229     model_config = json.loads(model_config.decode('utf-8'))\r\n--> 230     model = model_from_config(model_config, custom_objects=custom_objects)\r\n    231 \r\n    232     # set weights\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/saving.pyc in model_from_config(config, custom_objects)\r\n    308                     '`Sequential.from_config(config)`?')\r\n    309   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n--> 310   return deserialize(config, custom_objects=custom_objects)\r\n    311 \r\n    312 \r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/layers/serialization.pyc in deserialize(config, custom_objects)\r\n     62       module_objects=globs,\r\n     63       custom_objects=custom_objects,\r\n---> 64       printable_module_name='layer')\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/utils/generic_utils.pyc in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    171             custom_objects=dict(\r\n    172                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 173                 list(custom_objects.items())))\r\n    174       with CustomObjectScope(custom_objects):\r\n    175         return cls.from_config(config['config'])\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc in from_config(cls, config, custom_objects)\r\n   1291     for layer_data in config['layers']:\r\n   1292       process_layer(layer_data)\r\n-> 1293     # Then we process nodes in order of layer depth.\r\n   1294     # Nodes that cannot yet be processed (if the inbound node\r\n   1295     # does not yet exist) are re-enqueued, and the process\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc in process_layer(layer_data)\r\n   1276       from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top\r\n   1277 \r\n-> 1278       layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n   1279       created_layers[layer_name] = layer\r\n   1280 \r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/layers/serialization.pyc in deserialize(config, custom_objects)\r\n     62       module_objects=globs,\r\n     63       custom_objects=custom_objects,\r\n---> 64       printable_module_name='layer')\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/utils/generic_utils.pyc in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    173                 list(custom_objects.items())))\r\n    174       with CustomObjectScope(custom_objects):\r\n--> 175         return cls.from_config(config['config'])\r\n    176     else:\r\n    177       # Then `cls` may be a function returning a class.\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc in from_config(cls, config)\r\n   1604         A layer instance.\r\n   1605     \"\"\"\r\n-> 1606     return cls(**config)\r\n   1607 \r\n   1608 \r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/layers/advanced_activations.py in __init__(self, max_value, negative_slope, threshold, **kwargs)\r\n    319     self.max_value = max_value\r\n    320     self.negative_slope = K.cast_to_floatx(negative_slope)\r\n--> 321     self.threshold = K.cast_to_floatx(threshold)\r\n    322 \r\n    323   def call(self, inputs):\r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/tensorflow/python/keras/backend.pyc in cast_to_floatx(x)\r\n    231   ```\r\n    232   \"\"\"\r\n--> 233   return np.asarray(x, dtype=_FLOATX)\r\n    234 \r\n    235 \r\n\r\n/data/virtualenv/venv2/lib/python2.7/site-packages/numpy/core/numeric.pyc in asarray(a, dtype, order)\r\n    499 \r\n    500     \"\"\"\r\n--> 501     return array(a, dtype, copy=False, order=order)\r\n    502 \r\n    503 \r\n\r\nTypeError: float() argument must be a string or a number\r\n```\r\nI temporally fixed it in `tensorflow/python/keras/layers/advanced_activations.py` by changing `get_config`\r\nfrom\r\n```\r\n  def get_config(self):\r\n    config = {\r\n        'max_value': self.max_value,\r\n        'negative_slope': self.negative_slope,\r\n        'threshold': self.threshold\r\n    }\r\n    base_config = super(ReLU, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n```\r\nto \r\n```\r\n  def get_config(self):\r\n    config = {\r\n        'max_value': self.max_value,\r\n        'negative_slope': float(self.negative_slope),\r\n        'threshold': float(self.threshold)\r\n    }\r\n    base_config = super(ReLU, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n```\r\nHowever, the problem might be somewhere in `deserialize_layer`, as for a some reason `negative_slope` is a dictionary, not a `numpy.ndarray` ", "comments": ["@cataclysmus I am able to reproduce with tf 1.12.0, but not with tf-nightly. Can you try with tf-nightly and see if the issue still exist?", "Thanks @yongtang for taking a look. Previously I failed to check with nightly version. However I checked with TF nightly version : '1.13.0-dev20181108' now and the issue seems to be fixed. Will wait for @cataclysmus confirmation.", "@yongtang @ymodak Checked. No bug at the build `1.13.0-dev20181128`", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}]