[{"number": 47711, "title": "std symbols undefined while compiling libtensorflowlite.so for android", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  macOS Big Sur version 11.2.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n  source\r\n- TensorFlow version:\r\n  2.4.0\r\n- Python version:\r\n 3.9\r\n- Installed using virtualenv? pip? conda?:\r\n  trying to build from source\r\n- Bazel version (if compiling from source):\r\n 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n clang-1200.0.32.29\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n  AMD Radeon Pro 5300M 4 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to compile libtensorflowlite.so for android with added \"//tensorflow/lite/delegates/flex:delegate\" dependency (as is stated in the https://www.tensorflow.org/lite/guide/ops_select), compilation would fail due to std::make_unique being undefined.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build \\\r\n-s //tensorflow/lite:libtensorflowlite.so \\\r\n--config=android_arm64 \\\r\n--cxxopt='--std=c++11' \\\r\n-c opt \\\r\n--config=v2 \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSUBCOMMAND: # //tensorflow/core/kernels:portable_tensorflow_kernels [action 'Compiling tensorflow/core/kernels/control_flow_ops.cc', configuration: 17edd87150895742afa8f50ebcaef1b3cd2bf6b4d6974c05ffc14bdd6966f454, execution platform: @local_execution_config_platform//:platform]\r\n(cd /private/var/tmp/_bazel_richardyao/1258fb4559d515393cc4dfc8787a5562/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=30.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=21 \\\r\n    ANDROID_NDK_HOME=/usr/local/Caskroom/android-ndk/21/android-ndk-r21 \\\r\n    ANDROID_SDK_API_LEVEL=30 \\\r\n    ANDROID_SDK_HOME=/Users/richardyao/library/Android/Sdk \\\r\n    PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/opt/python@3.9/bin/python3.9 \\\r\n    PYTHON_LIB_PATH=/usr/local/Cellar/python@3.9/3.9.1_6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=21' -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig '-Werror=return-type' '-Werror=int-to-pointer-cast' '-Werror=pointer-to-int-cast' '-Werror=implicit-function-declaration' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/control_flow_ops.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/control_flow_ops.pic.o' -fPIC '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DSUPPORT_SELECTIVE_REGISTRATION -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/gif -iquote bazel-out/arm64-v8a-opt/bin/external/gif -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/bin/external/nsync -iquote external/libjpeg_turbo -iquote bazel-out/arm64-v8a-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/bin/external/double_conversion -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -iquote external/png -iquote bazel-out/arm64-v8a-opt/bin/external/png -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/bin/external/highwayhash -iquote external/icu -iquote bazel-out/arm64-v8a-opt/bin/external/icu -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/arm64-v8a-opt/bin/external/gemmlowp -isystem external/gif -isystem bazel-out/arm64-v8a-opt/bin/external/gif -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/double_conversion -isystem bazel-out/arm64-v8a-opt/bin/external/double_conversion -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/png -isystem bazel-out/arm64-v8a-opt/bin/external/png -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/icu/icu4c/source/common -isystem bazel-out/arm64-v8a-opt/bin/external/icu/icu4c/source/common -w '-std=c++14' '--std=c++11' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/control_flow_ops.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/control_flow_ops.pic.o)\r\nERROR: /Users/richardyao/Code/tensorflow/tensorflow/core/kernels/BUILD:6467:11: C++ compilation of rule '//tensorflow/core/kernels:portable_tensorflow_kernels' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64 -target ... (remaining 154 argument(s) skipped)\r\ntensorflow/core/kernels/data/dataset_utils.cc:883:17: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?\r\n      counter = std::make_unique<BlockingCounter>(num_batch_elements);\r\n                ^~~~~~~~~~~~~~~~\r\n                absl::make_unique\r\nexternal/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here\r\ntypename memory_internal::MakeUniqueResult<T>::scalar make_unique(\r\n                                                      ^\r\ntensorflow/core/kernels/data/dataset_utils.cc:883:17: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?\r\n      counter = std::make_unique<BlockingCounter>(num_batch_elements);\r\n                ^~~~~~~~~~~~~~~~\r\n                absl::make_unique\r\nexternal/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here\r\ntypename memory_internal::MakeUniqueResult<T>::scalar make_unique(\r\n                                                      ^\r\ntensorflow/core/kernels/data/dataset_utils.cc:884:19: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?\r\n      status_mu = std::make_unique<mutex>();\r\n                  ^~~~~~~~~~~~~~~~\r\n                  absl::make_unique\r\nexternal/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here\r\ntypename memory_internal::MakeUniqueResult<T>::scalar make_unique(\r\n                                                      ^\r\ntensorflow/core/kernels/data/dataset_utils.cc:884:19: error: no template named 'make_unique' in namespace 'std'; did you mean 'absl::make_unique'?\r\n      status_mu = std::make_unique<mutex>();\r\n                  ^~~~~~~~~~~~~~~~\r\n                  absl::make_unique\r\nexternal/com_google_absl/absl/memory/memory.h:168:55: note: 'absl::make_unique' declared here\r\ntypename memory_internal::MakeUniqueResult<T>::scalar make_unique(\r\n                                                      ^\r\n4 errors generated.\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 690.094s, Critical Path: 483.34s\r\nINFO: 757 processes: 13 internal, 744 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@thaink could you take a look at this?", "btw, I also tested on master branch, same thing. Although 2.4.0 seems to have other fails besides the make_unique not being defined.", "std::make_unique is introduced in C++14.\r\nYou should build with --cxxopt='--std=c++14'.\r\nAnd if you add \"//tensorflow/lite/delegates/flex:delegate\" as dependency, --config=monolithic may also be required.", "@thaink could you make sure that the necessary build flags are mentioned in the our guide page?", "By default, we always build with c++14: https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L300\r\nWe don't add it in sample commands in https://www.tensorflow.org/lite/guide/build_android but I think it is good since adding config with default value with have no effect but make the command more complicated.", "IMO, it would be nice to have the sample command at least for  `--config=monolithic`.", "Thank you guys, c++14 flag worked, but it would be very nice to have a more dedicated section and info on how to build the c++ libtensorflowlite with full commands included and for every os it supports. I had to search through issues to get an answer on the proper syntax of building it, and I got that c++11 flag along with it. I know that bazel is great and all, but coming from cmake as industry standard, it's a lot of guess works to get it built. "]}, {"number": 47702, "title": "Model.fit average gradients by batch_size", "body": "So, it seems `Model.fit` is averaging the gradient over the batch size.\r\n\r\nThere is a simple [script](https://github.com/NEGU93/cvnn/blob/master/debug/mwe_testing_learning_algo.py) I did to check that assertion and it was discussed in [stackoverlow](https://stackoverflow.com/questions/66566905/debugging-tensorflow-fit-not-making-sense/)\r\n\r\nIt is highly possible this is by design and not a bug but I wanted to report it just in case it is indeed a bug.", "comments": ["@NEGU93 \r\nPlease share simple stand alone code to replicate the issue reported.", "I already did. Is the link to the script. Is a standalone .py file. The logic of how I test it and why it should work is detailed on the stackoverlow link.", "@NEGU93\r\ni ran the script shared on tf 2.3,2.4 and nightly, please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/4bb39d011175bd41ceccdfac44b93651/untitled564.ipynb) and confirm.", "Yes, this is it. You can see that when I get at least some `True` (Not all of them but maybe due to rounding errors) is by comparing the gradient to:\r\n`(i_w - f_w) * BATCH_SIZE / 0.01`. You can change `BATCH_SIZE` to verify this is not a case for a specific number.", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/58da3b5c5b4c125c1a47da8ecb4aecb8/untitled159.ipynb)..Thanks !"]}, {"number": 47684, "title": "tf.Variable shape is restricted with copy.deepcopy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: ???\r\n- GPU model and memory: GTX 1070\r\n\r\n**Describe the current behavior**\r\n`tf.Variable` shape is restricted when using `copy.deepcopy`\r\n```python\r\n>>> x = tf.Variable([[1.2, 3.4]], shape=[None, 2])\r\n>>> x.shape\r\nTensorShape([None, 2])\r\n>>> import copy\r\n>>> copy.deepcopy(x).shape\r\nTensorShape([5, 2])\r\n```\r\n\r\n**Describe the expected behavior**\r\ncopy has the same shape as original.", "comments": ["@joelberkeley \r\nPlease refer to this [gist](https://colab.research.google.com/drive/1zhKXHpmEdrdWaagNMqTsReKFmdvfwcWH?usp=sharing) and let us know.", "@Saduf2019\r\n\r\n> When setting this argument to tf.TensorShape(None) (representing an unspecified shape), the variable can be assigned with values of different shapes.\r\n\r\nThis implies it's not possible to assign to a `tf.Variable` with partially-defined shape, but it is. However, this doesn't work for a deepcopy, which is the problem I'm experiencing\r\n```python\r\n>>> x = tf.Variable(tf.zeros([5, 2]), shape=[None, 2])\r\n>>> x.assign(tf.zeros([10, 2]))\r\n<tf.Variable 'UnreadVariable' shape=(None, 2) dtype=float32, numpy=\r\narray([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [0., 0.]], dtype=float32)>\r\n>>> y = tf.Variable(tf.zeros([5, 2]), shape=[None, 2])\r\n>>> import copy\r\n>>> copy.deepcopy(y).assign(tf.zeros([10, 2]))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/.../lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 888, in assign\r\n    raise ValueError(\r\nValueError: Cannot assign to variable Variable:0 due to variable shape (5, 2) and value shape (10, 2) are incompatible\r\n```"]}, {"number": 47683, "title": "SentencePieceTokenizer.detokenize fails on TPU with \"No registered 'SentencepieceDetokenizeOp' OpKernel for XLA_TPU_JIT devices\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Collab with TPU\r\n- TensorFlow installed from (source or binary): (pre-installed)\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: Python 3.7.10\r\n\r\n**Describe the current behavior**\r\n\r\nRunning tensorflow_text.SentencepieceTokenizer.detokenize on a TPU gives \"No registered 'SentencepieceDetokenizeOp' OpKernel for XLA_TPU_JIT devices\"\r\n\r\n**Describe the expected behavior**\r\n\r\nRunning tensorflow_text.SentencepieceTokenizer.detokenize on a TPU succeeds, just like on a CPU or GPU device\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n!pip install tensorflow-text\r\n!pip install sentencepiece\r\n\r\n!python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n!python --version\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_text as tf_text\r\nimport sentencepiece\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\n\r\ntokenizer_prefix = \"/tmp/tokenizer\"\r\ncorpus_path = tokenizer_prefix + \".txt\"\r\nmodel_path = tokenizer_prefix + \".model\"\r\n\r\nwith open(corpus_path, \"w\") as f: \r\n  content = f.write(\"a b c d e\")\r\n\r\nsentencepiece.SentencePieceTrainer.Train(input=corpus_path, model_prefix=tokenizer_prefix, vocab_size=9, character_coverage=1.0)\r\n\r\nwith tf.device(\"/TPU:0\"):\r\n  tokenizer = tf_text.SentencepieceTokenizer(model=tf.io.gfile.GFile(model_path, \"rb\").read(), add_eos=True)\r\n  @tf.function\r\n  def tf_detokenize(input):\r\n    return tokenizer.detokenize(input)\r\n  print(tf_detokenize(tf.constant([1, 2, 3, 4, 5], dtype=tf.int32)))\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nInvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_tf_detokenize_280}} = __inference_tf_detokenize_280[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\\202\\001\\000\", executor_type=\"\"](dummy_input, dummy_input).\r\nUncompilable nodes:\r\nSentenceTokenizer/SentenceTokenizer/SentenceTokenizer/SentencepieceDetokenizeOp: unsupported op: No registered 'SentencepieceDetokenizeOp' OpKernel for XLA_TPU_JIT devices compatible with node {{node SentenceTokenizer/SentenceTokenizer/SentenceTokenizer/SentencepieceDetokenizeOp}}\r\n\tStacktrace:\r\n\t\tNode: __inference_tf_detokenize_280, function: \r\n\t\tNode: SentenceTokenizer/SentenceTokenizer/SentenceTokenizer/SentencepieceDetokenizeOp, function: __inference_tf_detokenize_280\r\n\r\nSentenceTokenizer/Reshape: unsupported op: No registered 'Reshape' OpKernel for XLA_TPU_JIT devices compatible with node {{node SentenceTokenizer/Reshape}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, Tshape=DT_INT32\r\n\tStacktrace:\r\n\t\tNode: __inference_tf_detokenize_280, function: \r\n\t\tNode: SentenceTokenizer/Reshape, function: __inference_tf_detokenize_280\r\n\r\nIdentity: unsupported op: No registered 'Identity' OpKernel for XLA_TPU_JIT devices compatible with node {{node Identity}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING\r\n\tStacktrace:\r\n\t\tNode: __inference_tf_detokenize_280, function: \r\n\t\tNode: Identity, function: __inference_tf_detokenize_280\r\n\r\nidentity_RetVal: unsupported op: No registered '_Retval' OpKernel for XLA_TPU_JIT devices compatible with node {{node identity_RetVal}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_STRING, index=0\r\n\tStacktrace:\r\n\t\tNode: __inference_tf_detokenize_280, function: \r\n\t\tNode: identity_RetVal, function: __inference_tf_detokenize_280\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/165f56b8059db42420dca8b11dd3e836/47683-2-3.ipynb), [TF v2.4](https://colab.research.google.com/gist/amahendrakar/914acf4640879a1f7e6a87644a6f14be/47683.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/1ff4b0ff2ff79ef9321be404591290fb/47683-2-5.ipynb). Please check the linked gist for reference. Thanks!", "Was able to replicate the issue in TF  2.6.0-dev20210531 ,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/35532c4057083f2bed50b12cd3296c80/untitled157.ipynb)..Thanks !"]}, {"number": 47663, "title": "Infinite loop in AdjustHue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 / Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source / binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: python 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): clang 12\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nAn infinite loop bug in adjust_hue_op.cc when computing AdjustHue op. It can be triggered easily when argument delta sets as a big floating point value.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1-j_9NiRewoC6DEUk8qS2ceajJ2qbrA7K?usp=sharing\r\n\r\n**Other info / logs** \r\nCode: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/adjust_hue_op.cc#L232-L238\r\n\r\n`h` may be inf or -inf. Infinite loop in separate while loop.\r\n-inf:\r\n```\r\ntensorflow::AdjustHueOp<Eigen::ThreadPoolDevice, float>::DoCompute(tensorflow::OpKernelContext*, tensorflow::AdjustHueOpBase::ComputeOptions const&)::{lambda(long, long)#1}::operator()(long, long) const (this=<optimized out>,\r\n    start_channel=<optimized out>, end_channel=6) at tensorflow/core/kernels/image/adjust_hue_op.cc:232\r\n232              h += delta_h * kChannelRange;\r\n(gdb) p h\r\n$6 = <optimized out>\r\n(gdb) n\r\n233              while (h < 0) {\r\n(gdb) p h\r\n$7 = -inf\r\n(gdb) n\r\n234                h += kChannelRange;\r\n(gdb)\r\n233              while (h < 0) {\r\n(gdb) p h\r\n$8 = -inf\r\n(gdb)\r\n```\r\ninf:\r\n```\r\n(gdb) n\r\n233              while (h < 0) {\r\n(gdb) p h\r\n$5 = inf\r\n(gdb)\r\n```\r\n@mihaimaruseac Can you help to triage this bug to the right team? Thank you!", "comments": ["I ran the code on tf 2.4 gpu and it did not get into an infinite loop, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6d3f4a68a34456b9dc2724fbfd411436/untitled560.ipynb).", "The `DoCompute` logics are different in CPU AdjustHueOp and GPU AdjustHueOp. The problem is in CPU AdjustHueOp DoCompute. Please refer code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/adjust_hue_op.cc#L232-L238", "@jvishnuvardhan \r\nI am able to repliate the issue on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/37d5a9a29edb08b94608e79a765ce7b0/untitled561.ipynb)", "`delta` must be in the range `[-1, 1]`: [tf.image.adjust_hue API doc](https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue)\r\nThe infinite loop is happening because the huge `delta` used in the gist is out of range.\r\nI'll retry adding the input check (https://github.com/tensorflow/tensorflow/commit/ffe683650814604e858f77eff0324160e3fa0e57).", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7ccc4b4e0f31fa6a8f784a09abe39b50/untitled155.ipynb)..Thanks !"]}, {"number": 47661, "title": "eigh fails with 26x26 zero matrix on GPU with eager execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Tesla T4 16GB\r\n\r\n**Describe the current behavior**\r\n`tf.linalg.eigh` fails with `InternalError: tensorflow/core/util/cuda_solvers.cc:648: cuSolverDN call failed with status =7 [Op:SelfAdjointEigV2]` on a 26x26 (and larger) zero matrix when used with both GPU and eager execution. It works with other combinations of eager/graph execution and CPU/GPU.\r\n\r\n**Describe the expected behavior**\r\nEigendecomposition should be successfully calculated.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n# Needs to have GPU\r\nimport tensorflow as tf\r\ntf.linalg.eigh(tf.zeros((26, 26), dtype=tf.float64))\r\n```\r\nAlso see gist: https://colab.research.google.com/drive/1x5sfDIGJdDNOgqpaH7OBrnENfgE14ZQ0?usp=sharing\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e3e9b6bf95af5b88edf0d2266257a0f1/47661.ipynb). Thanks!", "Works fine on TF nightly now.  Can you please cross check?", "@charmasaur,\r\nAs per [sanjoy's comment](https://github.com/tensorflow/tensorflow/issues/47661#issuecomment-844677445), the issue is fixed in the TF Nightly. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/0dd6b435709d08c0f20883e349fdce9e/47661.ipynb) of the working code. Thanks!", "@rmothukuru unfortunately I don't think that test is accurate: notice that `print(device_lib.list_local_devices())` only detects a CPU, which suggests that the op isn't actually running on GPU.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This still seems to be broken.", "Seems to have been fixed upstream: https://docs.nvidia.com/cuda/archive/11.0/cuda-toolkit-release-notes/index.html#cusolver-resolved-issues\r\n\r\nI'm pretty confused by CUDA/cuSOLVER versions though. [NVIDIA says](https://docs.nvidia.com/cuda/archive/11.0/cuda-toolkit-release-notes/index.html#cuda-major-component-versions) that the bug got fixed in CUDA 11.0 / cuSOLVER 10.6.0.245, but TF 2.4 (which is broken) is [meant to](https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0) be using CUDA 11 (and messing around on Colab, if I install TF 2.4 then running `!ls /usr/local/cuda-11.0/lib64/libcusolver*` suggests that `10.6.0.245` is indeed present).\r\n\r\nMaybe an older version is getting statically compiled in?\r\n\r\nIn any case, I guess this is now fixed.\r\n\r\n"]}, {"number": 47659, "title": "tf-model-remediation: Keyword arguments not supported by original model: `['mask']`", "body": "[Reposting ](https://github.com/tensorflow/model-remediation/issues/24) from tensorflow/model-remediation as I was hoping to use MinDiff framework in my work however there hasn't been a response from the team in that repo\r\n\r\n\r\n**System information**\r\nRelevant Versions:\r\nRunning locally for now on Windows 10 Pro Version 10.0.19042 Build 19042\r\npython 3.6.9\r\ntensorflow 2.3.1\r\ntensorflow-model-remediation 0.1.3\r\ntransformers 4.3.2\r\n\r\n\r\nFrom provided: TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\noutput: `v2.3.0-54-gfcc4b966f1 2.3.1`\r\n\r\n**Describe the current behavior**\r\nHi I've been trying to debug this for a few days, I'm using the MinDiff remediation similarly to how it's used in the [tutorial](https://www.tensorflow.org/responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras). Using the debugger I see the 'mask' value is None from the caller. \r\nThe base model I am using to remediate unfairness is `TFBertForSequenceClassification` from HuggingFace's transformers library. It compiles to tf.keras, so from the documentation should be compatible with the MinDiff framework\r\n\r\n**Describe the expected behavior**\r\nI expect the library to be able to train a mindiff framework with the provided code.\r\n\r\n**Standalone code to reproduce the issue**\r\nCode to reproduce (I can't share my data, but I've tried loading the BERT weights from a TF trained Transformers with the same issue. I've also commented out wrapping the TFBertForSequenceClassification in tf.keras model but it does not make a difference to the stacktrace.\r\n```\r\nbert_model = TFBertForSequenceClassification.from_pretrained(path_to_bert, from_pt=True)\r\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n# loss = tf.keras.losses.BinaryCrossentropy()\r\n# bert_model.compile(optimizer=optimizer, loss=loss)\r\n\r\nmin_diff_weight = 1.5 \r\n\r\n# Create the dataset that will be passed to the MinDiffModel during training.\r\ndataset = md.keras.utils.input_utils.pack_min_diff_data(\r\n    train_ds_main, train_ds_unpriv, train_ds_priv)\r\n\r\n# Wrap the original model in a MinDiffModel, passing in one of the MinDiff\r\n# losses and using the set loss_weight.\r\nmin_diff_loss = md.losses.MMDLoss()\r\nmodel = md.keras.MinDiffModel(bert_model,\r\n                              min_diff_loss,\r\n                              min_diff_weight)\r\n\r\n# Compile the model normally after wrapping the original model.  Note that\r\n# this means we use the baseline's model's loss here.\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\nloss = tf.keras.losses.BinaryCrossentropy()\r\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n\r\nmodel.fit(dataset, epochs=epochs)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nStacktrace:\r\n```\r\nEpoch 1/1000\r\nTraceback (most recent call last):\r\n  File \"*/src/benchmarking/run_model.py\", line 450, in <module>\r\n    main()\r\n  File \"*/src/benchmarking/run_model.py\", line 446, in main\r\n    eval_min_diff_bert(**kwargs)\r\n  File \"*/src/benchmarking/run_model.py\", line 211, in eval_min_diff_bert\r\n    model.fit(dataset, epochs=epochs)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 823, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 697, in _initialize\r\n    *args, **kwds))\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3075, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 600, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"*\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 973, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n   *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow_model_remediation\\min_diff\\keras\\models\\min_diff_model.py:473 call  *\r\n        min_diff_loss = self.compute_min_diff_loss(\r\n   *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow_model_remediation\\min_diff\\keras\\models\\min_diff_model.py:371 compute_min_diff_loss  *\r\n        predictions = self._call_original_model(x, training=training, mask=mask)\r\n    *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\tensorflow_model_remediation\\min_diff\\keras\\models\\min_diff_model.py:234 _call_original_model  *\r\n        return self.original_model(inputs, **kwargs)\r\n    *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\transformers-4.2.2-py3.8.egg\\transformers\\models\\bert\\modeling_tf_bert.py:1405 call  *\r\n        inputs = input_processing(\r\n    *\\anaconda3\\envs\\mindiff\\lib\\site-packages\\transformers-4.2.2-py3.8.egg\\transformers\\modeling_tf_utils.py:345 input_processing  *\r\n        raise ValueError(\r\n\r\n    ValueError: The following keyword arguments are not supported by this model: ['mask'].\r\n```\r\n\r\nThank you!", "comments": ["@matanhalevy \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/33d4c0e7e018aab8bfd27a0c6420c8e1/untitled.ipynb).", "Hi @Saduf2019 ,\r\nYou didn't import or install transformers (HuggingFace) in your environment, that is why you are facing that import issue. Please see the relevant versions in my issue to install the relevant packages in your colab system. Additionally the file is needed for the `.from_pretrained(path_to_bertneed...` to be copied in your google drive, here's a sample one: https://drive.google.com/drive/folders/1rHz8BDh6FJv4csLrWUwQUOETUkZ0H75O?usp=sharing", "I also noticed the notebook you provided was relying on my data, that can be substituted: \r\n```\r\ndata_train, data_validate, validate_tfrecord_file, labels_train, labels_validate = min_diff_keras_utils.download_and_process_civil_comments_data()\r\n\r\n# Create masks for the sensitive and nonsensitive groups\r\nminority_mask = data_train.religion.apply(\r\n    lambda x: any(religion in x for religion in ('jewish', 'muslim')))\r\nmajority_mask = data_train.religion.apply(lambda x: x == \"['christian']\")\r\n\r\n# Select nontoxic examples, so MinDiff will be able to reduce sensitive FP rate.\r\ntrue_negative_mask = data_train['toxicity'] == 0\r\n\r\ndata_train_main = copy.copy(data_train)\r\ndata_train_sensitive = data_train[minority_mask & true_negative_mask]\r\ndata_train_nonsensitive = data_train[majority_mask & true_negative_mask]\r\n\r\n# Convert the pandas DataFrames to Datasets.\r\ndataset_train_main = tf.data.Dataset.from_tensor_slices(\r\n    (data_train_main['comment_text'].values, \r\n     data_train_main.pop(LABEL).values.reshape(-1,1) * 1.0)).batch(BATCH_SIZE)\r\ndataset_train_sensitive = tf.data.Dataset.from_tensor_slices(\r\n    (data_train_sensitive['comment_text'].values, \r\n     data_train_sensitive.pop(LABEL).values.reshape(-1,1) * 1.0)).batch(BATCH_SIZE)\r\ndataset_train_nonsensitive = tf.data.Dataset.from_tensor_slices(\r\n    (data_train_nonsensitive['comment_text'].values, \r\n     data_train_nonsensitive.pop(LABEL).values.reshape(-1,1) * 1.0)).batch(BATCH_SIZE)\r\n\r\ndataset = md.keras.utils.input_utils.pack_min_diff_data(\r\n      dataset_train_main, dataset_train_sensitive, dataset_train_nonsensitive)\r\n```\r\nApologies but I am unable to share my data, since the MinDiff model won't even train I don't see an issue with using mock data here. "]}, {"number": 47655, "title": "Implementing a version of the (Sparse) Categorical Cross Entropy Loss that makes use of Integer-Indexed predictions.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 2.\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe Categorical Cross Entropy Loss makes use of both one-hot encoded predictions, and one-hot encoded true labels.\r\nEg, y_true = [ [0,0,1], [1,0,0] ] and y_pred = [ [0,0,1], [0,1,0] ].\r\n\r\nThe Sparse Categorical Cross Entropy Loss makes use of single integer true labels, but still requires an array of probabilities corresponding to each existing class, per label in the predictions.\r\nEg, y_true = [2, 0] and y_pred = [ [0.1, 0.2, 0.7], [0.2, 0.85, 0.05] ].\r\n\r\nI feel it would be really useful to have a loss function which makes use of single integer-indexed categorical predictions as well as single integer-indexed categorical true labels.\r\nEg, y_true = [2, 0] and y_pred = [2, 1].\r\n\r\n**Will this change the current api? How?**\r\nNot really. It could possibly just require the definition of a new loss function.\r\n\r\n**Who will benefit with this feature?**\r\nFor one, it would be possible to use Dense models to process two-dimensional data. This could be done by simply encoding one of the concerned dimensions as a series of integer-indexed categories. You could process text data with a Dense model by simply encoding the characters as integer-indexed categories (in contrast to one-hot encoding) and then work on minimizing the outputs of the proposed loss function.\r\n\r\nEg, \"I am\" would simply be [9,0,1,13] (with ' ' as 0). This is evidently amenable to processing by a Dense model.\r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["@mayowaosibodu,\r\nDo you mean to say that your request is to facilitate effective usage of `Dense Layers` for `Text Data`? If it is so, we have `Recurrent Neural Networks` (`RNNs`, `LSTMs`, `GRUs`, etc..) for the same. You can correct me if I'm missing something.", "@rmothukuru I see what you're saying. There exist models built specifically with sequence data in mind.\r\n\r\nHowever I'm saying there appears to be away to make Dense-layer models capable of processing textual data- specifically functioning as sequence-to-sequence models, with very minimal changes to the current codebase.\r\n\r\nI do not know of academic articles assessing the capabilities of Dense-layer models with regard to S2S on Text Data.\r\nThere do not even seem to be any accessible online, and I understand and somewhat expect that, given that sequence-specific models are now the standard for textual data.\r\n\r\nBut it just seems to me like a promising opportunity to explore. Dense-layer models are definitely free of all of the performance encumbrances that RNNs etc, experience. I've worked with Deep LSTM networks on pretty capable GPU-cloud servers before and I know they are computationally expensive (even more so than ConvNets built to process the same data). And so given how computationally frugal (relatively), Dense-layer models are, I personally think any opportunity to try them out on datasets that normally would be outside their purview is definitely one worth exploring.\r\n\r\nBuilding off of your response, I realize there's definitely a more theoretical AI angle to this, given that it involves a technique that is pretty different from convention, but I feel like with regard to feature implementation, it's a very trivial issue. It's really just one function for the new loss.\r\n\r\n--\r\n\r\nBy the way, there could possibly be other uses for this feature, but working on S2S with Text Data is the most forefront on my mind right now because it was within this context I thought of the idea in the first place. With regard to facilitated programming ease, I don't know if anyone else is complaining that one-hot encoding is bulky, so in that specific respect I'm not sure how much consensus exists there that such a feature is very needed."]}, {"number": 47649, "title": "TF 2.4.1 yields NaNs on Large Models using model.fit and Adam as compared to 2.3.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n\r\n\r\n**Describe the current behavior**\r\nWhenever I train this model on TF 2.4.1, after the second batch, it starts to output NaNs as the loss. However, whenever I run this same exact code on TF 2.3.0, it trains properly. The Adam optimizer has the same hyperparameters in both versions. I'm not sure what would cause the difference. \r\n\r\n**Code to Reproduce NaNs**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n    def __init__(\r\n        self,\r\n        input_dim: int = None,\r\n        output_dim: int = 512,\r\n        context_dim: int = 5,\r\n        stride: int = 1,\r\n        dilation: int = 1,\r\n        kernel_initializer='glorot_uniform'\r\n    ):\r\n        super(MyLayer, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.conv = tf.keras.layers.Conv1D(filters=output_dim,\r\n                                           kernel_size=context_dim,\r\n                                           strides=stride,\r\n                                           dilation_rate=dilation,\r\n                                           kernel_initializer=kernel_initializer)\r\n\r\n    def call(self, x):\r\n        batch_size, num_frames, num_feats = x.shape\r\n        # if self.input_dim:\r\n        #     assert self.input_dim == num_feats\r\n        return self.conv(x)\r\n\r\nclass Reg(tf.keras.layers.Layer):\r\n    def __init__(self,dropout_rate=0.2):\r\n        super(Reg,self).__init__()\r\n        self.bn = tf.keras.layers.BatchNormalization()\r\n        self.do = tf.keras.layers.Dropout(rate=dropout_rate)\r\n    def call(self,x,training=True):\r\n        x = self.bn(x,training=training)\r\n        return self.do(x,training=training)\r\n        \r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, input_dim, output_dim,dropout_rate=0.2,batch_norm=False, return_xvector=False):\r\n        super().__init__()\r\n\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n        self.fc1 = MyLayer(input_dim=self.input_dim,output_dim=512,context_dim=5,dilation=1)\r\n        self.fc2 = MyLayer(input_dim=512  ,output_dim=1536 ,context_dim=3, dilation=2)\r\n        self.fc3 = MyLayer(input_dim=1536 ,output_dim=512  ,context_dim=3, dilation=3)\r\n        self.fc4 = MyLayer(input_dim=512  ,output_dim=512  ,context_dim=1, dilation=1)\r\n        self.fc5 = MyLayer(input_dim=512  ,output_dim=1500 ,context_dim=1, dilation=1)\r\n\r\n        self.fc6 = tf.keras.layers.Dense(512)\r\n        self.fc7 = tf.keras.layers.Dense(512)\r\n        self.output_layer = tf.keras.layers.Dense(self.output_dim)\r\n\r\n        if batch_norm:\r\n            self.reg1 = Reg(dropout_rate)\r\n            self.reg2 = Reg(dropout_rate)\r\n            self.reg3 = Reg(dropout_rate)\r\n            self.reg4 = Reg(dropout_rate)\r\n            self.reg5 = Reg(dropout_rate)\r\n            self.reg6 = Reg(dropout_rate)\r\n        else:\r\n            self.reg1 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg2 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg3 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg4 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg5 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg6 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n\r\n\r\n\r\n    def call(self,x,training=True, return_logits=True):\r\n        with tf.name_scope(\"Extractor\"):\r\n            with tf.name_scope(\"Fc1\"):\r\n                x = tf.nn.relu(self.fc1(x))\r\n                x = self.reg1(x, training=training)\r\n            with tf.name_scope(\"Fc2\"):\r\n                x = tf.nn.relu(self.fc2(x))\r\n                x = self.reg2(x, training=training)\r\n            with tf.name_scope(\"Fc3\"):\r\n                x = tf.nn.relu(self.fc3(x))\r\n                x = self.reg3(x, training=training)\r\n            with tf.name_scope(\"Fc4\"):\r\n                x = tf.nn.relu(self.fc4(x))\r\n                x = self.reg4(x, training=training)\r\n            with tf.name_scope(\"Fc5\"):\r\n                x = tf.nn.relu(self.fc5(x))\r\n                x = self.reg5(x, training=training)\r\n            with tf.name_scope(\"StatsPool\"):\r\n                x = self.statpool(x)\r\n\r\n            with tf.name_scope(\"Segment6\"):\r\n                x = self.fc6(x)\r\n        \r\n        with tf.name_scope(\"Classifier\"):\r\n            x = tf.nn.relu(x)\r\n            x = self.reg6(x)\r\n            x = tf.nn.relu(self.fc7(x))\r\n            x = self.output_layer(x)\r\n        if return_logits:\r\n            return x \r\n        else:\r\n            x = self.softmax(x)\r\n            return x\r\n\r\n    def statpool(self,x):\r\n        mu = tf.math.reduce_mean(x,axis=1)\r\n        sigma = tf.math.reduce_std(x,axis=1)\r\n        return tf.concat([mu,sigma],1)\r\n        \r\nn_feats = 40\r\nmodel = MyModel(n_feats,3)\r\n\r\nloss_fn =  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(loss=loss_fn, optimizer=\"adam\",metrics = [\"accuracy\"])\r\n\r\ndef batch(N=4):\r\n    for i in range(100):\r\n        yield np.random.normal(size=(N,n_feats,200)), np.random.randint(0,3,size=(N))\r\n\r\nhistory = model.fit(x = batch(N=300),\r\n                    validation_data= batch(N=300),\r\n                   )\r\n```", "comments": ["@nicolasshu \r\nThis is models issue, can you please open it in models repo.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "They asked me to keep the issue here\r\n[https://github.com/tensorflow/models/issues/9802#issuecomment-800798183](https://github.com/tensorflow/models/issues/9802#issuecomment-800798183)", "@nicolasshu \r\nI ran the code shared and face a [different error](https://colab.research.google.com/gist/Saduf2019/0d59c4e8588568666eedebe74117a05e/untitled563.ipynb) than reported, please share all dependencies for us to replicate the issue.", "Ah I apologize. The first line was not supposed to be included:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n    def __init__(\r\n        self,\r\n        input_dim: int = None,\r\n        output_dim: int = 512,\r\n        context_dim: int = 5,\r\n        stride: int = 1,\r\n        dilation: int = 1,\r\n        kernel_initializer='glorot_uniform'\r\n    ):\r\n        super(MyLayer, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.conv = tf.keras.layers.Conv1D(filters=output_dim,\r\n                                           kernel_size=context_dim,\r\n                                           strides=stride,\r\n                                           dilation_rate=dilation,\r\n                                           kernel_initializer=kernel_initializer)\r\n\r\n    def call(self, x):\r\n        batch_size, num_frames, num_feats = x.shape\r\n        # if self.input_dim:\r\n        #     assert self.input_dim == num_feats\r\n        return self.conv(x)\r\n\r\nclass Reg(tf.keras.layers.Layer):\r\n    def __init__(self,dropout_rate=0.2):\r\n        super(Reg,self).__init__()\r\n        self.bn = tf.keras.layers.BatchNormalization()\r\n        self.do = tf.keras.layers.Dropout(rate=dropout_rate)\r\n    def call(self,x,training=True):\r\n        x = self.bn(x,training=training)\r\n        return self.do(x,training=training)\r\n        \r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, input_dim, output_dim,dropout_rate=0.2,batch_norm=False, return_xvector=False):\r\n        super().__init__()\r\n\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n        self.fc1 = MyLayer(input_dim=self.input_dim,output_dim=512,context_dim=5,dilation=1)\r\n        self.fc2 = MyLayer(input_dim=512  ,output_dim=1536 ,context_dim=3, dilation=2)\r\n        self.fc3 = MyLayer(input_dim=1536 ,output_dim=512  ,context_dim=3, dilation=3)\r\n        self.fc4 = MyLayer(input_dim=512  ,output_dim=512  ,context_dim=1, dilation=1)\r\n        self.fc5 = MyLayer(input_dim=512  ,output_dim=1500 ,context_dim=1, dilation=1)\r\n\r\n        self.fc6 = tf.keras.layers.Dense(512)\r\n        self.fc7 = tf.keras.layers.Dense(512)\r\n        self.output_layer = tf.keras.layers.Dense(self.output_dim)\r\n\r\n        if batch_norm:\r\n            self.reg1 = Reg(dropout_rate)\r\n            self.reg2 = Reg(dropout_rate)\r\n            self.reg3 = Reg(dropout_rate)\r\n            self.reg4 = Reg(dropout_rate)\r\n            self.reg5 = Reg(dropout_rate)\r\n            self.reg6 = Reg(dropout_rate)\r\n        else:\r\n            self.reg1 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg2 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg3 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg4 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg5 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n            self.reg6 = tf.keras.layers.Dropout(rate=dropout_rate)\r\n\r\n\r\n\r\n    def call(self,x,training=True, return_logits=True):\r\n        with tf.name_scope(\"Extractor\"):\r\n            with tf.name_scope(\"Fc1\"):\r\n                x = tf.nn.relu(self.fc1(x))\r\n                x = self.reg1(x, training=training)\r\n            with tf.name_scope(\"Fc2\"):\r\n                x = tf.nn.relu(self.fc2(x))\r\n                x = self.reg2(x, training=training)\r\n            with tf.name_scope(\"Fc3\"):\r\n                x = tf.nn.relu(self.fc3(x))\r\n                x = self.reg3(x, training=training)\r\n            with tf.name_scope(\"Fc4\"):\r\n                x = tf.nn.relu(self.fc4(x))\r\n                x = self.reg4(x, training=training)\r\n            with tf.name_scope(\"Fc5\"):\r\n                x = tf.nn.relu(self.fc5(x))\r\n                x = self.reg5(x, training=training)\r\n            with tf.name_scope(\"StatsPool\"):\r\n                x = self.statpool(x)\r\n\r\n            with tf.name_scope(\"Segment6\"):\r\n                x = self.fc6(x)\r\n        \r\n        with tf.name_scope(\"Classifier\"):\r\n            x = tf.nn.relu(x)\r\n            x = self.reg6(x)\r\n            x = tf.nn.relu(self.fc7(x))\r\n            x = self.output_layer(x)\r\n        if return_logits:\r\n            return x \r\n        else:\r\n            x = self.softmax(x)\r\n            return x\r\n\r\n    def statpool(self,x):\r\n        mu = tf.math.reduce_mean(x,axis=1)\r\n        sigma = tf.math.reduce_std(x,axis=1)\r\n        return tf.concat([mu,sigma],1)\r\n        \r\nn_feats = 40\r\nmodel = MyModel(n_feats,3)\r\n\r\nloss_fn =  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(loss=loss_fn, optimizer=\"adam\",metrics = [\"accuracy\"])\r\n\r\ndef batch(N=4):\r\n    for i in range(100):\r\n        yield np.random.normal(size=(N,n_feats,200)), np.random.randint(0,3,size=(N))\r\n\r\nhistory = model.fit(x = batch(N=300),\r\n                    validation_data= batch(N=300),\r\n                   )\r\n```", " I am able to replicate the issue for[ tf 2.4 and nightly](https://colab.research.google.com/gist/Saduf2019/ca45262c891488013dda9960db673de3/untitled563.ipynb), it works as expected on tf 2.3, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4d576cdb603e67da3947a6539484b17f/untitled564.ipynb)", "Hi checking in if there is any update or workaround to this issue ? I am having similar issues with Efficient Net models in the new TF2.4. ", "Was able to replicate the issue in TF v2.5,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/2c1b2a7e9055fdb8095e23b2b1bcf0ab/untitled154.ipynb)..Thanks !", "@rajaditya-m Totally not a workaround, but no one ever got back to me, so I just downgraded. "]}, {"number": 47644, "title": "Addition of  kernel variant directory with fast portable kernels", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): n/a\r\n- Tensorflow version (commit SHA if source): \r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\nCurrently tflite(u) only has reference kernel implementations (written for clarity / correctness) and target/platform-specific optimized kernels variants.    It lacks efficient portable kernel implementations needed to achieve more competitive performance on Microcontrollers without dedicated support.   This is especially true of smaller Microcontrollers without DSP/NN ISA extensions (e.g. small RISC-V devices) where good portable C++ implementations can achieve close-to-optimal results.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nn/a", "comments": []}, {"number": 47640, "title": "GPU execution not deallocating memory in iOS swift", "body": "**System information**\r\n- OS Platform - iOS, swift\r\n- Mobile device : iPhone, iPad\r\n- TensorFlow installed from (source or binary): Pod.\r\n- TensorFlow version (use command below): 2.3.0\r\n\r\n**Issue**\r\nInitialised interpreter once with below code\r\nvar delegate = MetalDelegate()\r\nself.interpreter = try Interpreter(modelPath: modelPath, options: nil, delegates: [delegate])\r\nself.interpreter?.allocateTensors()\r\n\r\nThen executed the below code each time on running the inference\r\nvar inputTensor = try interpreter?.copy(input, toInputAt:0)\r\ntry interpreter?.invoke()\r\nvar outputTensor = try interpreter?.output(at: 0)\r\n\r\nOn running the .invoke() in GPU (using MetalDelegate) the memory consumption is increasing and not decreasing. How do i deallocate the unused memory? This is working fine in CPU.", "comments": ["@teijeong, could you help with this as you are more experienced w/ iOS?", "Also cc: @impjdi ", "Hi @jestingeorgejacob1 , can you give more details about your problem? When I tested TFLite runtime the memory usage goes up, but it goes back to normal level once the Interpreter is recycled. You can expect the memory consumption to go up at least by model file size, so if you can provide your model, model size, and amount of memory increased (best to use Instruments) it would be great.\r\n\r\nDoes it increase for every inference, or increase once and stays at the same level?", "Hi @teijeong,  yes, it increases for every inference. For my use case I am initialising the interpreter once and running the inference with same interpreter multiple times say about 200+ times. The memory keeps increasing by 5MB on every inference. I tried with the tflite model in style transfer iOS app by running it in a forloop and it seems to be increasing memory.\r\nIs there a way to release the memory in GPU after an inference is completed? How can i make sure Interpreter is recycled after every inference.?\r\nThanks for the reply", "We're using this on production (ObjC) and memory doesn't seem to increase.  Could it be that there is something leaking in Swift integration?  Unfortunately, I have never used Swift myself, and can't tell.\r\n\r\nAs @teijeong said, providing the model file & the actual code snippet on how you use should help diagnosing the issue.  ", "@impjdi, @teijeong  - Thanks for the reply. \r\nPlease find the attached sample xcode project. This project is downloaded from https://github.com/tensorflow/examples/tree/master/lite/examples/style_transfer/ios. Can you please install the pods and check? I have  made an update in line 226 of ViewController.swift to run the invoke function inside a loop. After running the project, once an image is selected the memory can be seen increasing.\r\n\r\n[ios.zip](https://github.com/tensorflow/tensorflow/files/6154211/ios.zip)\r\n\r\n\r\n"]}, {"number": 47631, "title": "ssd_mobilenet_v2 to 8-bit quantized tflite model conversion error", "body": "Tensorflow version : 2.4.1\r\n\r\nI have been trying to convert tensorflow model 'ssd_mobilenet_v2' to 8 bit quantized tflite model.\r\nI was successful in converting from tf to just tfllite model ,BUT I was unable to convert to 8 bit quantized tflite model as it shows some errors.\r\nMy code is : \r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cv2\r\nimport os\r\n\r\nsaved_model_dir = '/home/ssd_mobilenet_v2_2'\r\n\r\nfolder = '/mscoco_dataset_images'\r\n \r\ndef representative_data_gen() :\r\n i = 1\r\n for filename in os.listdir(folder): \r\n  k = (i/number_of_images)*100\r\n  print(' PROCESSING :  ',k,' %')\r\n  testimage = cv2.imread(os.path.join(folder,filename))\r\n  testimage = np.expand_dims(testimage,0)      \r\n  yield[testimage.astype(np.uint8)]   # doubt,why it is uint8 and not float32\r\n  i = i+1   \r\n\r\nonverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir,signature_keys = [\"serving_default\"]) \r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8 ,tf.lite.OpsSet.SELECT_TF_OPS] # part after coma is important for preventing tf.size() error\r\n\r\nconverter.inference_input_type = tf.uint8\r\n\r\nconverter.inference_output_type = tf.uint8\r\n\r\nconverter.allow_custom_ops = True #is it required?\r\n\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open('batch_q_model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n\r\n**_I'm getting the error as :_**  \r\n**RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor tfl.cast\r\nEmpty min/max for tensor tfl.cast**\r\n\r\nWhy is this error coming?I know it is an issue with related to calibration,i.e,representative dataset,but I've inputted the official mscoco dataset images.\r\nCan someone please tell how to rectify this error \r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@jvishnuvardhan sir I've posted the issue..Please help me with this", "@sandeep22-v \r\nThe code provided has errors can you please share standalone code such that we can replicate the issue reported, or share a colab gist with the error reported.", "> @sandeep22-v\r\n> The code provided has errors can you please share standalone code such that we can replicate the issue reported, or share a colab gist with the error reported.\r\n\r\nSure sir... I think the issue you mentioned was about the represenative_data_gen() function which need external folder to work,.\r\nI've simulated it  in the below code,so this code will run as a standalone one.\r\n\r\nThe python code and the model  ssd_mobilenet_v2 is attached as zip file\r\n[ok.zip](https://github.com/tensorflow/tensorflow/files/6099538/ok.zip)\r\n\r\n\r\n\r\n\r\n", "@ethkim could you triage this issue?", "> @ethkim could you triage this issue?\r\n\r\nThank you sir for the response", "UPDATE : I downgraded tensorflow from 2.4.1 to 2.2.0 and ran the same code..\r\n\r\nGot a new kind of error \r\n**ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[1, None, None, 3]'.**\r\n\r\nCan you please tell me how version change surprisingly changes the type of error \r\n", "### 1. System information\r\n\r\n- OS Platform and Distribution :  Linux Ubuntu 20.04\r\n- TensorFlow installation -pip package \r\n-  TensorFlow version : 2.4.1\r\n\r\nWas trying to convert ssd_mobilenet_v2_2 to tflite version.\r\n\r\n### 2. Code\r\n\r\nThe code was : \r\n\r\nimport tensorflow as tf\r\nsaved_model_directory = '/ssd_mobilenet_v2_2'\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_directory)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n**'tf.Size' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Size {device = \"\"}**\r\n\r\nHow can this error be rectified", "Solved it..", "@sandeep22-v I am getting a similar error. What did you do to fix the issue? Does downgrading tensorflow work?\r\nBrief log of my error:\r\n```\r\nConverterError: <unknown>:0: error: loc(\"strided_slice_10\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"strided_slice_11\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"strided_slice_12\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"strided_slice_13\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.StridedSlice {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}\r\n```", "> @sandeep22-v I am getting a similar error. What did you do to fix the issue? Does downgrading tensorflow work?\r\n> Brief log of my error:\r\n> \r\n> ```\r\n> ConverterError: <unknown>:0: error: loc(\"strided_slice_10\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n> <unknown>:0: error: loc(\"strided_slice_11\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n> <unknown>:0: error: loc(\"strided_slice_12\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n> <unknown>:0: error: loc(\"strided_slice_13\"): 'tf.StridedSlice' op is neither a custom op nor a flex op\r\n> <unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n> \ttf.StridedSlice {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}\r\n> ```\r\n\r\nSir are you trying to convert ssd_mobilenet_v2 to tflite?", "No, I was converting efficientdet model to efficientdet-lite and had trouble with that. But [this](https://github.com/google/automl/issues/966#issuecomment-804352674) worked\r\n\r\nYou may need to set the correct flags while initializing the converter object it seems.", "@sandeep22-v I am facing the same problem. What did you do to fix the issue?"]}, {"number": 47610, "title": "Can't Quantize Inputs As Uint8 In TFLM", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from Source\r\n- Tensorflow version 2.4.1\r\n- Target platform Visual Studio\r\n\r\n**Describe the problem**\r\nI have been troubleshooting problems trying to run inference on a model using Tensorflow Lite for MIcrocontrollers in Visual Studio. I found the issue to be that the lite/micro/kernells/quantize.cc kernel is unable to accept Uint8 inputs, but supports UInt8 outputs. From lite/micro/kernels/quantize.cc in the Prepare() function, UInt8 is not included in the input type check:\r\n\r\n```\r\n  TF_LITE_ENSURE(context, input->type == kTfLiteFloat32 ||\r\n                              input->type == kTfLiteInt16 ||\r\n                              input->type == kTfLiteInt8);\r\n  TF_LITE_ENSURE(context, output->type == kTfLiteUInt8 ||\r\n                              output->type == kTfLiteInt8 ||\r\n                              output->type == kTfLiteInt16 ||\r\n                              output->type == kTfLiteInt32);\r\n```\r\nI originally created my quantized model using the conversion code example provided here, which shows using UInt8 for the input:\r\n\r\nhttps://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization\r\n\r\nShould the TFLM implementation of the quantize kernel support UInt8? Or  perhaps the example should be updated to show using Int8?\r\n\r\n\r\n", "comments": ["CCing @ethkim ", "This issue is still present. I have a lot of trouble using the tflite models provided in tensorflow-hub because of uint8 incompatibility in several operations (softmax-common etc.)."]}, {"number": 47609, "title": "TLFM Problem Compiling micro_allocator.cc In Visual Studio With MSVC", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Windows 10\r\n- TensorFlow installed from source\r\n- Tensorflow version 2.4.1:\r\n- Target platform: Windows Visual Studio 2019 MSVC. Building with C++ Language Standard Default (ISO C++14 Standard) and C Language Standard Default (Legacy MSVC)\r\n\r\n**Describe the problem**\r\nI am working to port TFLM to Visual Studio and almost have it working. I am currently stuck at one issue. In micro_allocator.cc, there is a declaration of a constant TFLiteIntArray:\r\n\r\n//tensorflow/lite/micro/micro_allocator.cc line 63:\r\n`const TfLiteIntArray kZeroLengthIntArray = {};`\r\n\r\nTFLiteIntArray uses a variable length array at the end.\r\n\r\n//tensorflow/lite/c/common.h line 81:\r\n```\r\ntypedef struct TfLiteIntArray {\r\n  int size;\r\n// gcc 6.1+ have a bug where flexible members aren't properly handled\r\n// https://github.com/google/re2/commit/b94b7cd42e9f02673cd748c1ac1d16db4052514c\r\n#if (!defined(__clang__) && defined(__GNUC__) && __GNUC__ == 6 && \\\r\n     __GNUC_MINOR__ >= 1) ||                                      \\\r\n    defined(HEXAGON) || (__clang_major__ == 7 && __clang_minor__ == 1)\r\n  int data[0];\r\n#else\r\n  int data[];\r\n#endif\r\n} TfLiteIntArray;\r\n```\r\n\r\nThe micro_allocator.cc will not compile in MSVC, as it generates an error [C466](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2466?f1url=%3FappId%3DDev16IDEF1%26l%3DEN-US%26k%3Dk(C2466)%26rd%3Dtrue&view=msvc-160), cannot allocate an array of constant size 0:\r\n\r\nThe C466 error can supposedly be disabled with the \"Disable Language Extensions\" compiler setting (/Za). This setting is incompatible with compiler C17, thus I'm using C14. However when I set /Za with C14, the error is still generated for micro_allocator.cc, and creates a lot of problems elsewhere in the project.\r\n\r\nThere are two places in micro_allocator where the kZeroLengthIntArray object is referenced, in both cases the address of kZeroLengthIntArray is being used when a tensor's shape is found to be null:\r\n\r\n```\r\nif (flatbuffer_tensor.shape() == nullptr) {\r\n    // flatbuffer_tensor.shape() can return a nullptr in the case of a scalar\r\n    // tensor.\r\n    result->dims = const_cast<TfLiteIntArray*>(&kZeroLengthIntArray);\r\n  }\r\n```\r\n\r\nI have resolved the compiler issue by creating a pointer to an empty zero length int array object, and a function that initializes the object at runtime and returns a pointer to that object:\r\n\r\n```\r\nTfLiteIntArray* pZeroLengthIntArray = 0;\r\nTfLiteIntArray* GetZeroLengthIntArray()\r\n{\r\n    if (pZeroLengthIntArray == 0)\r\n    {\r\n        pZeroLengthIntArray = (TfLiteIntArray *)malloc(sizeof(TfLiteIntArray));\r\n        pZeroLengthIntArray->size = 0;\r\n    }\r\n    return pZeroLengthIntArray;\r\n}\r\n```\r\n\r\nIf a tensor's shape is found to be null, I set its dims pointer to this dynamically allocated object by calling the function:\r\n\r\n```\r\n  if (flatbuffer_tensor.shape() == nullptr) {\r\n    // flatbuffer_tensor.shape() can return a nullptr in the case of a scalar\r\n    // tensor.\r\n    result->dims = const_cast<TfLiteIntArray*>(GetZeroLengthIntArray());\r\n  }\r\n```\r\n\r\nQuestion:\r\nIs there some other way to get MSVC to compile micro_allocator.cc? If not, should I submit the above work-around as a general pull-request that will be compatible with MVSC as well as gnu/other compilers?\r\n", "comments": ["Update: \r\nI would probably propose the code below as an improvement from the code above, as it avoids a potential memory leak.\r\n\r\n```\r\nunsigned char ZeroLengthIntArrayBuffer[sizeof(TfLiteIntArray)];\r\nTfLiteIntArray* GetZeroLengthIntArray()\r\n{\r\n    TfLiteIntArray* pZeroLengthIntArray = (TfLiteIntArray*)ZeroLengthIntArrayBuffer;\r\n    pZeroLengthIntArray->size = 0;\r\n    return pZeroLengthIntArray;\r\n}\r\n```\r\n\r\n```\r\n if (flatbuffer_tensor.shape() == nullptr) {\r\n    // flatbuffer_tensor.shape() can return a nullptr in the case of a scalar\r\n    // tensor.\r\n    result->dims = const_cast<TfLiteIntArray*>(GetZeroLengthIntArray());\r\n  }\r\n```\r\n", "@embeddetech @advaitjain I have been running into the same C2466 error on micro_allocator.cc Line 63. I was just able to get this to compile on MSVC (Windows 10; VS2019; ISO C++14/C11) by altering line 63 from:\r\n\r\n`const TfLiteIntArray kZeroLengthIntArray = {};\r\n`\r\n\r\nto \r\n\r\n`const TfLiteIntArray kZeroLengthIntArray{0};\r\n`\r\n\r\n@embeddetech I don't know yet if this is a good solution, but there seems to be no harm in explicitly setting `int size = 0` in this way.", "The flexible member in TfLiteIntArray as well as zero length array have been tricky across multiple compiler versions.\r\n\r\nI would recommend the following:\r\n\r\n * (Preferred) add an ifdef for your platform here: https://github.com/tensorflow/tensorflow/blob/425b7e924b180b9979457ab7f36c78f306a5eef8/tensorflow/lite/c/common.h#L81-L93\r\n\r\n * If that does not work, then go with the suggestion from @lindyrock and add an ifdef to micro_allocator.cc https://github.com/tensorflow/tensorflow/blob/425b7e924b180b9979457ab7f36c78f306a5eef8/tensorflow/lite/micro/micro_allocator.cc#L63", "@advaitjain Thank you! Is there any chance an #ifdef _WINDOWS switch could be pulled into the TFLM release so that it is compatible with MSVC out of the box?\r\n\r\n@lindyrock I want to say that doing it your way seemed to cause issues for me, but it may be that I had multiple concurrent issues and was confused about what was causing me problems. I have TFLM up and running in an MSVC C/C++ project. I currently have problems loading models if certain layers are too large, but I'm close. How is your port of TFLM into MSVC going?"]}, {"number": 47607, "title": "Hexagon 685/690 wrong SoC examples for QCS610/QCS410 on TensorFlow Lite Hexagon delegate ", "body": "On this page:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/hexagon_delegate.md\r\n\r\nIt states that QCS610, QCS410 are SoC examples for Hexagon 690. However, according to Qualcomm's product info:\r\nhttps://www.qualcomm.com/products/qcs610\r\n\r\nBoth SoC QCS610 and QCS410 contain Hexagon 685 rather than Hexagon 690.\r\n\r\n", "comments": ["@karimnosseir could you review this suggestion?", "@tomchen1000  Thanks for the feedback. I've reached to QC to confirm and didn't hear back from them yet.\r\n\r\nThanks"]}, {"number": 47579, "title": "Build C API on Windows - test failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.4.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): MSCV 14.16.27023\r\n- CUDA/cuDNN version: 11.0/8.0.5\r\n\r\n**Describe the problem**\r\n\r\nI followed the build instructions on [website](https://www.tensorflow.org/install/source_windows) and lib_package [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md)\r\nBut it failed at the test:\r\n\r\n```\r\nFAILED: //tensorflow/tools/lib_package:libtensorflow_test (Summary)\r\n      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log\r\n      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log\r\n      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log\r\n```\r\nSee the last section for the logs.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Setup configuration\r\n```\r\n> python ./configure.py\r\n\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is D:\\ProgramData\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\ProgramData\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\ProgramData\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\n2. run libtensorflow_test\r\n```\r\n> bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\n\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'test' from d:\\code\\tensorflow_cpp_build\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  Inherited 'build' options: --python_path=D:/ProgramData/Anaconda3/python.exe\r\nINFO: Reading rc options for 'test' from d:\\code\\tensorflow_cpp_build\\.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'test' from d:\\code\\tensorflow_cpp_build\\.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=D:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=D:/ProgramData/Anaconda3/lib/site-packages --python_path=D:/ProgramData/Anaconda3/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Reading rc options for 'test' from d:\\code\\tensorflow_cpp_build\\.bazelrc:\r\n  'test' options: --define open_source_build=true --config=v2\r\nINFO: Reading rc options for 'test' from d:\\code\\tensorflow_cpp_build\\.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium\r\nINFO: Found applicable config definition build:short_logs in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file d:\\code\\tensorflow_cpp_build\\.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\nINFO: Found applicable config definition build:xla in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:v2 in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file d:\\code\\tensorflow_cpp_build\\.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\nINFO: Found applicable config definition build:opt in file d:\\code\\tensorflow_cpp_build\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:windows in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\code\\tensorflow_cpp_build\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow_test (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 test target...\r\nFAIL: //tensorflow/tools/lib_package:libtensorflow_test (see C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log)\r\nFAIL: //tensorflow/tools/lib_package:libtensorflow_test (see C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log)\r\nFAIL: //tensorflow/tools/lib_package:libtensorflow_test (see C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log)\r\n\r\nFAILED: //tensorflow/tools/lib_package:libtensorflow_test (Summary)\r\n      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log\r\n      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log\r\n      C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log\r\nTarget //tensorflow/tools/lib_package:libtensorflow_test up-to-date:\r\n  bazel-bin/tensorflow/tools/lib_package/libtensorflow_test\r\n  bazel-bin/tensorflow/tools/lib_package/libtensorflow_test.exe\r\nINFO: Elapsed time: 38948.796s, Critical Path: 1452.51s\r\nINFO: 7616 processes: 7616 local.\r\nINFO: Build completed, 1 test FAILED, 7612 total actions\r\n//tensorflow/tools/lib_package:libtensorflow_test                        FAILED in 3 out of 3 in 16.0s\r\n  Stats over 3 runs: max = 16.0s, min = 2.6s, avg = 7.1s, dev = 6.3s\r\n  C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test.log\r\n  C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_1.log\r\n  C:/users/kelvin_wu/_bazel_kelvin_wu/xuozzs67/execroot/org_tensorflow/bazel-out/x64_windows-opt/testlogs/tensorflow/tools/lib_package/libtensorflow_test/test_attempts/attempt_2.log\r\n\r\nINFO: Build completed, 1 test FAILED, 7612 total actions\r\n```\r\n\r\n**Any other info / logs**\r\n[test.log](https://github.com/tensorflow/tensorflow/files/6087959/test.log)\r\n[attempt_1.log](https://github.com/tensorflow/tensorflow/files/6087960/attempt_1.log)\r\n[attempt_2.log](https://github.com/tensorflow/tensorflow/files/6087961/attempt_2.log)\r\n", "comments": []}, {"number": 47574, "title": " Undefined symbol micro_test::did_test_fail in tensorflow_lite/tensorflow/lite/micro/kernels/conv_test_common", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source): 2.4.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS\r\n\r\nThe error happens when I try to compile the project in Mbed Studio with tflite micro files generated using make. Steps include:\r\n1. Generate files for Mbed:\r\n```\r\n make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed OPTIMIZED_KERNEL_DIR=cmsis_nn generate_hello_world_mbed_project\r\n```\r\n2. Copy the tensorflow and third part folders from the gen folder into Mbed project folder in a folder named tensorflow_lite  (the two folders are in tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed). the project directory looks something likes this after copying: \r\n![Screenshot from 2021-03-04 13-01-10](https://user-images.githubusercontent.com/20337475/110029831-cfc5c100-7ce9-11eb-8f5e-e2dfd03c62cd.png)\r\nThe project is configured to run on Nucleo-L476RG.\r\n\r\n3. Delete examples folder in tensorflow_lite/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4_default/prj/hello_world/mbed/tensorflow/lite/micro/\r\n\r\n4. Hit 'clean build' button: \r\n![Screenshot from 2021-03-04 13-02-38](https://user-images.githubusercontent.com/20337475/110029932-eec45300-7ce9-11eb-9445-ee450d3ca050.png)\r\n\r\n5. I faced an error with inline TfLiteRegistration Register_FULLY_CONNECTED_INT8()  being redefined. Solution: Comment out the following line in tensorflow_lite/tensorflow/lite/micro/kernels/fully_connected.h:\r\n```\r\ninline TfLiteRegistration Register_FULLY_CONNECTED_INT8() {\r\nreturn Register_FULLY_CONNECTED();\r\n}\r\n```\r\n6. Hit clean build again. The error this time:\r\n```\r\nError: L6218E: Undefined symbol micro_test::did_test_fail (referred from BUILD/NUCLEO_L476RG/ARMC6/tensorflow_lite/tensorflow/lite/micro/kernels/conv_test_common.o).\r\n\r\n```\r\nI checked conv_test_common.cc in tensorflow_lite/tensorflow/lite/micro/kernels/conv_test_common.cc and it doesnt have any micro_test::did_test_fail function, it does reference \"tensorflow/lite/micro/kernels/conv_test.h\", which references \"tensorflow/lite/micro/testing/micro_test.h\", which does seem to have the following namespace:\r\n```\r\n  namespace micro_test {            \\\r\n  int tests_passed;                 \\\r\n  int tests_failed;                 \\\r\n  bool is_test_complete;            \\\r\n  bool did_test_fail;               \\\r\n  } \r\n```\r\nI am not sure why the error is occuring.", "comments": ["Further information, my hunch says the problem is happening here in conv_test_common.cc, as it invokes a function from \"tensorflow/lite/micro/testing/micro_test.h\", where its calling micro_test::did_test_fail boolean variable but not finding it for some reason despite having the variable in the namespace in micro_test.h file.\r\n\r\n![Screenshot from 2021-03-04 14-27-36](https://user-images.githubusercontent.com/20337475/110039253-cf332780-7cf5-11eb-886c-780208f73fa4.png)\r\n\r\nIt seems someone seemed to have solved the problem here, not sure which version of TF and I can't find the commit:\r\nhttps://www.openhub.net/p/tensorflow/commits/1893384553 ", "Update: Solved the issue by changing the following function in micro_test.h:\r\n```\r\n#define TF_LITE_MICRO_EXPECT_NEAR(x, y, epsilon)                              \\\r\n  do {                                                                        \\\r\n    auto vx = (x);                                                            \\\r\n    auto vy = (y);                                                            \\\r\n    auto delta = ((vx) > (vy)) ? ((vx) - (vy)) : ((vy) - (vx));               \\\r\n    if (vx != vy && delta > epsilon) {                                        \\\r\n      MicroPrintf(#x \" (%f) near \" #y \" (%f) failed at %s:%d\",                \\\r\n                  static_cast<double>(vx), static_cast<double>(vy), __FILE__, \\\r\n                  __LINE__);                                                  \\\r\n      micro_test::did_test_fail = true;                                       \\\r\n    }                                                                         \\\r\n  } while (false)\r\n```\r\nto\r\n```\r\n#define TF_LITE_MICRO_EXPECT_NEAR(x, y, epsilon)                              \\\r\n  do {                                                                        \\\r\n    auto vx = (x);                                                            \\\r\n    auto vy = (y);                                                            \\\r\n    auto delta = ((vx) > (vy)) ? ((vx) - (vy)) : ((vy) - (vx));               \\\r\n    if (vx != vy && delta > epsilon) {                                        \\\r\n      MicroPrintf(#x \" (%f) near \" #y \" (%f) failed at %s:%d\",                \\\r\n                  static_cast<double>(vx), static_cast<double>(vy), __FILE__, \\\r\n                  __LINE__);                                                  \\\r\n    }                                                                         \\\r\n  } while (false)\r\n```\r\nI am not sure if this is recommended or it will break something, so waiting for a tensorflower to comment. Also, there's a new warning:\r\n```\r\nObject file kernel_util.o is not unique! It could be made from: tensorflow_lite/tensorflow/lite/micro/kernels/kernel_util.cc tensorflow_lite/tensorflow/lite/kernels/kernel_util.cc\r\n```\r\n"]}, {"number": 47569, "title": "TFLite micro hard fault when using tf.reduce_sum", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- Tensorflow version (commit SHA if source): 2.4.1\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm M4 with Zephyr\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to export my TF-model with one simple custom layer to TFLite micro and get hardware faults when running it.\r\n\r\nI would like to implement a layer that performs this calculation:\r\n\r\n```python\r\ndef call(self, inputs):\r\n    x = (tf.reduce_sum(tf.math.squared_difference(inputs['vector'],inputs['vector_old'])))\r\n    x = tf.reshape(x,(1,1))\r\n    return x\r\n```\r\n`vector` and `vector_old` have both the shape (1,4) and are float tensors and my output should have the shape (1,1)\r\n\r\n**Sequence of commands/steps when I ran into the problem**\r\n\r\nBecause I get a hard fault I tried the following things:\r\n\r\n**A** This works:\r\n\r\n```python\r\ndef call(self, inputs):\r\n    x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)\r\n    x = tf.math.multiply(x,x)\r\n    #x = tf.reduce_sum(x)\r\n    x = x[0][0]\r\n    x = tf.reshape(x,(1,1))\r\n    return x\r\n```\r\n\r\n**B** This doesn't:\r\n\r\n```python\r\ndef call(self, inputs):\r\n    x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)\r\n    x = tf.math.multiply(x,x)\r\n    x = tf.reduce_sum(x)\r\n    #x = x[0][0]\r\n    x = tf.reshape(x,(1,1))\r\n    return x \r\n```\r\n\r\n**C** This doesn't work also:\r\n\r\n```python\r\ndef call(self, inputs):\r\n    x = tf.constant(1, shape=(1,1), dtype='float32')\r\n    #x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)\r\n    #x = tf.math.multiply(x,x)\r\n    #x = tf.reduce_sum(x)\r\n    #x = x[0][0]\r\n    #x = tf.reshape(x,(1,1))\r\n    return x\r\n```\r\n\r\n**D** But this works without a hard fault:\r\n```python\r\ndef call(self, inputs):\r\n    x = tf.math.subtract( inputs['vector'],inputs['vector_old'], name=None)\r\n    x = tf.math.multiply(x,x)\r\n    x1 = tf.math.add(x[0][0], x[0][1], name=None)\r\n    x2 = tf.math.add(x[0][2], x[0][3], name=None)\r\n    x = tf.math.add(x1,x2, name=None)\r\n    x = tf.reshape(x,(1,1))\r\n    return x\r\n```\r\n\r\nThank you for any advice :)\r\n\r\nOliver\r\n", "comments": []}, {"number": 47554, "title": "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.", "body": "`WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\n`\r\n\r\n![image](https://user-images.githubusercontent.com/4510984/109937386-d2d0a980-7d09-11eb-8e3c-b83cb07023f9.png)\r\n\r\n![image](https://user-images.githubusercontent.com/4510984/109937397-d6643080-7d09-11eb-9206-c47efa47bf46.png)\r\n", "comments": ["@GF-Huang \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease refer to similar issues and let us know, [link](https://github.com/tensorflow/tensorflow/issues/44541), #44938 .", "@Saduf2019 \r\n\r\nversion: `tf 2.4.1`\r\n\r\nreproduce:\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow import keras as ks\r\n\r\nmodel = ks.Sequential([\r\n    ks.layers.LSTM(units=1, input_shape=(5, 1))\r\n])\r\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\r\n\r\nmodel.save('models/test')\r\n```\r\n", "@GF-Huang \r\nPlease let us know if the links help.", "Not help.", "@GF-Huang \r\nCan you please try with tf-nightly and let us know if you face the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hello, I have the same issue in tf.2.4.1 si do not close it", "@ClementViricel \r\nCan you please try on nightly and let us know if you still have the issue.", "@Saduf2019 \r\nThis is still happening with tf-nightly and we have a new comment about Keras if we try to use tf.model_save.save  ", "I am able to replicate the issue reported on tf 2.3,tf 2.4 nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b39a0c5f2afbc4e8dcab0e43fc64023d/untitled573.ipynb).", "TensorFlow Version:  2.4.1\r\n![image](https://user-images.githubusercontent.com/43032723/112844192-14faca00-90c1-11eb-8d84-31bb5a0e8e49.png)\r\n![image](https://user-images.githubusercontent.com/43032723/112844417-58553880-90c1-11eb-92c7-ae518ee852a0.png)\r\n\r\nIs this solved ? I have the same issue.", "Still happening with tf2.4.1 \r\n`WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.`", "Facing the same issue with tf 2.4.1\r\n\r\nWARNING:absl:Found untraced functions such as attention_layer_layer_call_fn, attention_layer_layer_call_and_return_conditional_losses, conv_stage1_ecga_layer_call_fn, conv_stage1_ecga_layer_call_and_return_conditional_losses, conv_stage1_ecgb_layer_call_fn while saving (showing 5 of 110). These functions will not be directly callable after loading.", "I'm seeing the same issue in TF 2.5.0 as well as TF nightly.", "> I'm seeing the same issue in TF 2.5.0 as well as TF nightly.\r\n\r\nAccording to this comment [link](https://github.com/tensorflow/tensorflow/issues/47479#issuecomment-815314034), it's just a warning. ", "i'm facing this issue in tf 2.4.1", "Facing the samme issue tf 2.4.1", "im gettin this issue in:\r\n  tf 2.4.1\r\n  tf 2.5.0\r\n tf_nighlty 2.6.0 dev 20210523", "> > I'm seeing the same issue in TF 2.5.0 as well as TF nightly.\r\n> \r\n> According to this comment [link](https://github.com/tensorflow/tensorflow/issues/47479#issuecomment-815314034), it's just a warning.\r\n\r\nIt's the last output message I see before tensorflow segfaults when I try to save my model though so for now that's about all I've been going on. The segfault happens on the following python line https://github.com/tensorflow/tensorflow/blob/30677caa5e53136c74433bee754ae64bddc3bf55/tensorflow/python/framework/ops.py#L3292 according to faulthandler.  I haven't had much luck attaching gdb to it.\r\n", "@jvishnuvardhan wrote in the other issue:\r\n> This is intended behavior and the warning is to encourage users to use model.save instead of tf.saved_model.save when saving keras models.\r\n\r\nHowever, we never used tf.saved_model.save - just the normal model.save - but the warning still occurs. Is this really just a warning that we can ignore? Or: is it safe to update our code to 2.4/2.5 if this warning occurs with our models? I'm a bit careful there... \ud83d\ude05\r\n\r\nAs it occurs not in the described situation and does not give any meaningful hint about what the user has possibly done wrong, this is not optimal imho.\r\n", "Literally every LSTM network is broken for two releases now! How is this not fixed?\r\n\r\nI spent days debugging downstream errors in TFX pipeline as the variables tfrecord gets corrupted causing validation and serving to fail. There can't possibly be a basic failure to save a model in two version I thought?\r\n\r\nIn TFX under Kubeflow this happens totally silently and the corruption is extremely non-obvious and intermittent. \r\n\r\n@ymodak @Saduf2019 \r\n\r\nThis is absolutely critical and affecting production workloads as models are literally being silently corrupted. On top of that the warning appears relatively benign among usual tensorflow warnings. \r\n\r\nPlease escalate this issue immediately, the fact that this is happening in the latest two Tensorflow and TFX release is absolutely crazy.", "And to be clear:\r\n\r\n`lstm_model.save('any_odd_local_or_bucket', save_format='tf')`\r\n\r\nIs broken in both 2.4.1 and 2.5.0, reports note that h5 saving is fine, however this not an option for customers of \"production ready TFX\" as only the saved_model format is supported by the evaluator component.", "To anybody else suffering from segfaults, crashes or silent corruptions there seems to be a solution that was rolled forward, using `save_traces=False`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/52473a84f45ce1bb604e483f13402601eb959d0c#diff-6c0ba1738a1f7e84c6271d096f56ac36a1c34105b3d716bd9f6b3df738092643\r\n\r\n```python\r\nmodle.save('directory', save_traces=False)\r\n```\r\n\r\nI don't quite follow the serialization/tracing logic, and frankly after a week of debugging both my environment and downstream I don't have the willpower to do so.\r\n\r\nLeaving some downstream errors which may help people like me in the future, who try and google them if using TFX but end up dry:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.DataLossError: truncated block read [Op:RestoreV2]\r\n```\r\n```\r\nOP_REQUIRES failed at save_restore_v2_ops.cc:205 : Data loss: truncated block read\r\n```\r\n```\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n```\r\n\r\nWhen running massive pipelines under Cloud/Kubernetes/Beam/TFX/Live Data, the last thing you expect to be the problem is a plain `model.save()`, especially when you downgrade multiple minor/patch version. I'm very upset as I spent a long time analysing artifacts and debugging my own code.", "Update: tried the MultiHeadAttention layer, similar case.\r\n\r\n```\r\nWARNING:absl:Found untraced functions such as multi_head_attention_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, query_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\r\n\r\nWARNING:absl:Found untraced functions such as multi_head_attention_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, query_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\r\n```\r\n\r\n_Even in an official tensorflow tutorial, such warnings are included as the \"expected output\" and left without explanation:_ https://www.tensorflow.org/agents/tutorials/10_checkpointer_policysaver_tutorial\r\n\r\n", "Same Issue, even spikes in loss can be seen, side by side with the warnings.. (Using tf 2.5.0). I think I just got rid of the warning by switching off pruning in hyperparameter optimization, but it might be a coincidence.", "Same with tensorflow-lattice layers:\r\n\r\n`\r\nabsl - WARNING - Found untraced functions such as pwl_calibration_593_layer_call_fn, pwl_calibration_593_layer_call_and_return_conditional_losses, pwl_calibration_590_layer_call_fn, pwl_calibration_590_layer_call_and_return_conditional_losses, pwl_calibration_591_layer_call_fn while saving (showing 5 of 25). These functions will not be directly callable after loading\r\n`\r\n", "Same warning when saving MoVinet models.\r\nMoVinet's implementation by tensorflow 2.5.0 (at tensorflow/models/blob/master/official/vision/beta/projects/movinet/):\r\n\r\nWARNING:absl:Found untraced functions such as stem/stem/conv3d_layer_call_fn, stem/stem/conv3d_layer_call_and_return_conditional_losses, expansion_layer_call_fn, expansion_layer_call_and_return_conditional_losses, feature_layer_call_fn while saving (**showing 5 of 2375**). These functions will not be directly callable after loading.", "Adding reset_after=false to the LSTM layer works to me", "I am getting the same error when saving a model with BERT Keras layer with TensorFlow 2.5.0 (Google Colab) . Unfortunately, the saved model can't be loaded later.\r\n\r\n```\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 915). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: bert_epoch5_adamw/assets\r\nINFO:tensorflow:Assets written to: bert_epoch5_adamw/assets\r\n```", "I was having this same issue saving my lstm model. I fixed it by adding the save_format parameter as \"h5\":\r\n\r\n`model.save(model_name, save_format=\"h5\")`\r\n", "> I was having this same issue saving my lstm model. I fixed it by adding the save_format parameter as \"h5\":\r\n> \r\n> `model.save(model_name, save_format=\"h5\")`\r\n\r\nThis is not a fix, you're just using the old h5 format. Many users write their deployment around the tf format as you do not have to redefine custom stuff when loading from it. Falling back to h5 would make some things unneccessarily messy&complicated.", "I have successfully suppressed the warnings by changing the log-level of `absl.logging`:\r\n\r\n`import absl.logging`\r\n`absl.logging.set_verbosity(absl.logging.ERROR)`\r\n\r\nEdit 30/08/2021 10:00 a.m. C.E.T.: I use Tensorflow 2.5.", "> I have successfully suppressed the warnings by changing the log-level of `absl.logging`:\r\n> \r\n> `import absl.logging`\r\n> `absl.logging.set_verbosity(absl.logging.ERROR)`\r\n\r\nDoes this mean that the problem is solved? I saw the warning in tensorflow==2.6.0 as well", "I consider it bodged, not fixed.", "> > I have successfully suppressed the warnings by changing the log-level of `absl.logging`:\r\n> > `import absl.logging`\r\n> > `absl.logging.set_verbosity(absl.logging.ERROR)`\r\n> \r\n> Does this mean that the problem is solved? I saw the warning in tensorflow==2.6.0 as well\r\n\r\nNo. You're just not showing the warnings.", "For me, the problem was gone when I commented this line in my source code:\r\n\r\n`tf.config.run_functions_eagerly(True) `\r\n\r\nThis function will make all invocations of tf.function run eagerly instead of running as a traced graph function...\r\n\r\nI use `eager run` because I need to covert a TF tensor to a numpy array. Without using eagar call, however, the tensor.numpy() will always report an error. This seems a tricky problem.", "> For me, the problem was gone when I commented this line in my source code:\r\n> \r\n> `tf.config.run_functions_eagerly(True) `\r\n> \r\n> This function will make all invocations of tf.function run eagerly instead of running as a traced graph function...\r\n> \r\n> I use `eager run` because I need to covert a TF tensor to a numpy array. Without using eagar call, however, the tensor.numpy() will always report an error. This seems a tricky problem.\r\n\r\nIs this a fix for the problem? I am using tensorflow 2.6 currently.\r\n\r\nI have a model script completely written with tensorflow (keras not used). All the functions are written by myself so errors are possible. It is a simple model and I am trying to save it as a pb file so that I can run inference of this model in a edge device. The model works fine in eager mode.\r\n\r\nI am creating an object of tf.Module and passing it to tf.saved_model.save(), during when I face this error.\r\nplease do suggest if there is any other methods to save model from tf script.\r\n\r\nJust for clarification, I am not using any tf.variable in the function rather only tensors and TensorArray. Does tracing require a tf.variable to be present in the Function to generate graphs?\r\n\r\nIf I ignore the warnings, I get this error when I load it back, `ValueError: Found zero restored functions for caller function.`\r\n\r\nAlso the example code in the TF documents [saving custom model](https://www.tensorflow.org/guide/saved_model#saving_a_custom_model) also throws me same error.\r\n\r\nPlease feel free to suggest anything that you think might help me. Thank you.", "This is also NOT a fix as this should not rely on whether eager execution is enabled or not.", "> Update: tried the MultiHeadAttention layer, similar case.\r\n> \r\n> ```\r\n> WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, query_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\r\n> \r\n> WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, query_layer_call_and_return_conditional_losses while saving (showing 5 of 40). These functions will not be directly callable after loading.\r\n> ```\r\n> \r\n> _Even in an official tensorflow tutorial, such warnings are included as the \"expected output\" and left without explanation:_ https://www.tensorflow.org/agents/tutorials/10_checkpointer_policysaver_tutorial\r\n\r\nCan confirm that I got the same warning when using MultiHead attention.\r\n\r\n`\r\nWARNING:absl:Found untraced functions such as dropout_6_layer_call_and_return_conditional_losses, dropout_6_layer_call_fn, point_wise_feed_forward_layer_3_layer_call_and_return_conditional_losses, point_wise_feed_forward_layer_3_layer_call_fn, dropout_16_layer_call_and_return_conditional_losses while saving (showing 5 of 535). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: mod/assets\r\nINFO:tensorflow:Assets written to: mod/assets\r\n`\r\n\r\nGot the same error in both the following cases:\r\n`\r\nmodel.save('model_name', save_format='tf')\r\ntf.saved_model.save(model, 'model_name')\r\n`\r\n\r\nAlso got NotImplementedError when trying to save in model.h5, saying subclassed models cannot be saved that way. ", "Having the same issue for the first time, but apparently, this is also the first time RAM-memory doesn't grow up while training! \r\n```\r\nWARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\r\n```\r\nHowever, the model is not saved due to another issue: \r\n```\r\nINFO:tensorflow:Assets written to: <path-to-results>/models/assets\r\n```\r\nI haven't defined the `assets` folder, though.\r\n\r\nUsing:\r\n```\r\ntensorflow                2.4.1         \r\ntensorflow-base           2.4.1          \r\ntensorflow-estimator      2.5.0               \r\ntensorflow-gpu            2.4.1              \r\nkeras                     2.3.1  \r\n\r\n```\r\nI disabled the GPU, though with the command:\r\n```\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n```", "I'm facing the same issue in GRU , here is the model:\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nfrom tensorflow.keras import Input, Model\r\n\r\n\r\ninput1 = tf.keras.layers.Input(shape=(512,2),name=\"input_1\")\r\n#input2 = tf.keras.layers.Input(shape=(None,512),name=\"input_2\")\r\n#inputs= tf.keras.layers.Concatenate(axis=0)([input1, input2])\r\n#inputs = tf.reshape(input1,[-1,512,1])\r\nx = tf.keras.layers.Conv1D(filters=16, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\",input_shape=(512,2))(input1)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, activation=\"tanh\", return_sequences=True))(x)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, activation=\"tanh\", return_sequences=True))(x)\r\nx = tf.keras.layers.Dense(128, activation=\"tanh\")(x)\r\no1 = tf.keras.layers.Dense(1, activation=\"linear\",name=\"ed\")(x)\r\no2 = tf.keras.layers.Dense(1, activation=\"sigmoid\",name=\"sd\")(x)\r\n\r\nmodel = Model(inputs=[input1], outputs=[o1, o2])\r\n\r\nmodel.compile(loss={'ed': 'mean_squared_error', \r\n                    'sd': 'binary_crossentropy'},\r\n              loss_weights={'ed':0.4,\r\n                            'sd':0.6},\r\n              optimizer='adam',\r\n              metrics={'ed': tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None),\r\n                       'sd': tfa.metrics.F1Score(name=\"f1_score\",num_classes=2, threshold=0.5, average = \"micro\")})\r\n```\r\nThe callback along with model.fit():\r\n```Python\r\nimport os\r\ncheckpoint = '/gdrive/MyDrive/washing.ckpt'\r\ncdir = os.path.dirname(checkpoint)\r\n\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint, \r\n    verbose=1, \r\n    monitor='loss',\r\n    save_weights_only=False,\r\n    save_best_only=True)\r\n\r\n\r\n\r\nhistory = model.fit(train_dataset,epochs=3,callbacks=[cp_callback],verbose=1,steps_per_epoch= 78)\r\n```\r\nThe problem persists even with eager implementation. Moreover, when I'm loading the model , the loaded model seems to be corrupted as the loss is coming out to be infinity.\r\n\r\nThis is how I'm loading the model:\r\n```Python\r\nimport os\r\ncheckpoint = '/gdrive/MyDrive/washing.ckpt'\r\ncdir = os.path.dirname(checkpoint)\r\nlatest = tf.train.latest_checkpoint(cdir)\r\nfrom tensorflow import keras\r\nmodel = keras.models.load_model('/gdrive/MyDrive/washing.ckpt')\r\n```", "For any future readers.. \r\n\r\nfollowing this [answer](https://stackoverflow.com/a/66779515/7317733) has solved the issue for me.\r\n", "This is a severe bug, not a warning! The performance of the trained model has dropped after load the model. It should be corrected ASAP! (TF 2.6, CUDA 11.2)", "I have the same losses while inference. I haven't observed this Performance drain that you mention. ", "> I have the same losses while inference. I haven't observed this Performance drain that you mention.\r\n\r\nOK. The performance - I meant that the inference process is extremely slow after the loading model. I have a very big dataset ~ 7 million examples. The training process lasted 1.5 days (5 epochs using Early Stopping). The test applies 5-fold splitting. The inference process was stopped after 12 hours. \r\n\r\nBut I have not tried your solution yet. I simply deleted every saving and loading process and use my model only for experimenting.  Clearly it cannot be used for production code. ", "Well, I'm afraid I cannot comment anything more on the \"extremely slow\" inference after you've loaded your saved model, because I am unaware of the architecture and your tasks itself. \nMany other factors come to mind that might be at play. But I havent faced such problems using the loaded model, all the variables seem to be loaded successfully in my case, and the inference times are quite comparable too.. \n\nMaybe this needs more attention by the devs themselves.", "**UPDATE**\r\nI raised an [issue](https://github.com/keras-team/keras/issues/15538#issuecomment-949467892) on a similar topic.\r\nIt turns out that the model does _remember_ and _improve_ the previous loss , but it's displaying `inf` for some unknown reason.", "@akshit0201 yes, it is known where tensorflow doesn't serialize the loss functions with `model.save()`.. in the `h5` format of keras. You can read more about the limitation [here](https://www.tensorflow.org/guide/keras/save_and_serialize#limitations).\r\nHowever you can compile the model after it is loaded with a loss function like mentioned in this [stackoverflow-answer](https://stackoverflow.com/a/48373959/7317733). \r\n\r\nBut as far as my experimentation goes, the inference is performed just as efficiently and accurately. \r\n\r\nP.S. If you want to load a saved-model only for inference, for example in production environments, you can pass a flag to `load_model()` like so\r\n\r\n```\r\nmodel = load_model(model_path, compile=False)\r\n```\r\nAlso i am assuming the same behaviour for any other formats other than `tf`, too. ", "Hi @GF-Huang , I was able to resolve this by adding  .h5 extension here in this snippet `model.save('models/test')` in latest stable version TF 2.6, Attaching [Gist ](https://colab.sandbox.google.com/gist/mohantym/6e22817510aaa4c5ad9d88f080a11e76/github_47554.ipynb#scrollTo=GSzWcBpLzWiW)for reference . Thank you!\r\n", "> Hi @GF-Huang , I was able to resolve this by adding .h5 extension here in this snippet `model.save('models/test')` in latest stable version TF 2.6, Attaching [Gist ](https://colab.sandbox.google.com/gist/mohantym/6e22817510aaa4c5ad9d88f080a11e76/github_47554.ipynb#scrollTo=GSzWcBpLzWiW)for reference . Thank you!\r\n\r\nFalling back to the legacy h5 format is not a fix as it brings major limitations.", "Someone asked what the limitations of the H5 format are. Please consult [the Tensorflow guide](https://www.tensorflow.org/guide/keras/save_and_serialize#keras_h5_format) on saving and loading Keras models. I reproduce the relevant comments here:\r\n\r\n> Compared to the SavedModel format, there are two things that don't get included in the H5 file:\r\n> \r\n> * **External losses & metrics** added via model.add_loss() & model.add_metric() are not saved (unlike SavedModel). If you have such losses & metrics on your model and you want to resume training, you need to add these losses back yourself after loading the model. Note that this does not apply to losses/metrics created inside layers via self.add_loss() & self.add_metric(). As long as the layer gets loaded, these losses & metrics are kept, since they are part of the call method of the layer.\r\n> * The **computation graph of custom objects** such as custom layers is not included in the saved file. At loading time, Keras will need access to the Python classes/functions of these objects in order to reconstruct the model. See Custom objects.\r\n\r\nAnd to reiterate: **H5 is not a solution.** It is another beast entirely. There's congestion on one road and using H5 taking another uncongested but rough road. It is not the solution to the problem. **And neither is eager execution.**", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@google-ml-butler @GF-Huang @mohantym @ymodak @Saduf2019 This issue should not be stalled... It's still a major broken functionality and people still struggle because of this.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I got this model:\r\n\r\n```\r\ndescription=\"created by layer 'tf_distil_bert_model'\")\r\nModel: \"BERT\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_ids (InputLayer)          [(None, 120)]        0                                            \r\n__________________________________________________________________________________________________\r\nattention_mask (InputLayer)     [(None, 120)]        0                                            \r\n__________________________________________________________________________________________________\r\ntf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \r\n                                                                 attention_mask[0][0]             \r\n__________________________________________________________________________________________________\r\ntf.__operators__.getitem (Slici (None, 768)          0           tf_distil_bert_model[0][0]       \r\n__________________________________________________________________________________________________\r\nhidden (Dense)                  (None, 512)          393728      tf.__operators__.getitem[0][0]   \r\n__________________________________________________________________________________________________\r\noutput (Dense)                  (None, 125)          64125       hidden[0][0]                     \r\n__________________________________________________________________________________________________\r\nY1 (Dense)                      (None, 4)            504         output[0][0]                     \r\n__________________________________________________________________________________________________\r\nY2 (Dense)                      (None, 16)           2016        output[0][0]                     \r\n__________________________________________________________________________________________________\r\nY3 (Dense)                      (None, 1)            126         output[0][0]                     \r\n==================================================================================================\r\nTotal params: 66,823,379\r\nTrainable params: 66,823,379\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n\r\n```\r\n\r\nmodel.save output:\r\n\r\n```\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x000002220F89A400>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x00000222844BC6A0>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x0000022207D09A30>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x0000022208E15DC0>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x0000022208E2A190>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.Dropout object at 0x0000022208E35520>, because it is not built.\r\nWARNING:absl:Found untraced functions such as embeddings_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, transformer_layer_call_and_return_conditional_losses, transformer_layer_call_fn, add_layer_call_and_return_conditional_losses while saving (showing 5 of 415). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: saved_model/model\\assets\r\nINFO:tensorflow:Assets written to: saved_model/model\\assets\r\nC:\\Users\\username\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  warnings.warn('Custom mask layers require a config and must override '\r\n```", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Bad robot!", "Oh come on, really? <_<\r\n\r\n@mohantym This is still an issue and should *not* be closed. ", "Ok! @Luux! Re-opening this issue as requested . Thanks!", "@Saduf2019  Hi, are you the owner of the issue?\r\n\r\nCan we fix it?", "> Can we fix it?\r\n\r\nI guess we desperately need Bob the Builder for this one. (Really sorry for OT, but I couldn't resist here :'D)\r\n", "I have this model:\r\n```\r\nModel: \"bert_token_classifier\"\r\n__________________________________________________________________________________________________\r\n Layer (type)                   Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\n input_word_ids (InputLayer)    [(None, None)]       0           []                               \r\n                                                                                                  \r\n input_mask (InputLayer)        [(None, None)]       0           []                               \r\n                                                                                                  \r\n input_type_ids (InputLayer)    [(None, None)]       0           []                               \r\n                                                                                                  \r\n bert_encoder (BertEncoder)     {'sequence_output':  109482240   ['input_word_ids[0][0]',         \r\n                                 (None, None, 768),               'input_mask[0][0]',             \r\n                                 'pooled_output': (               'input_type_ids[0][0]']         \r\n                                None, 768),                                                       \r\n                                 'encoder_outputs':                                               \r\n                                 [(None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                , (None, None, 768)                                               \r\n                                ]}                                                                \r\n                                                                                                  \r\n dropout_1 (Dropout)            (None, None, 768)    0           ['bert_encoder[0][13]']          \r\n                                                                                                  \r\n predictions/transform/logits (  (None, None, 7)     5383        ['dropout_1[0][0]']              \r\n Dense)                                                                                           \r\n                                                                                                  \r\n==================================================================================================\r\nTotal params: 109,487,623\r\nTrainable params: 109,487,623\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nWhen I save I get this output:\r\n\r\n```\r\n2021-11-23 12:25:46.353223: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:absl:Found untraced functions such as self_attention_layer_call_fn, self_attention_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, self_attention_layer_norm_layer_call_fn while saving (showing 5 of 900). These functions will not be directly callable after loading.\r\n/home/antpc/Documents/tf2.6/lib/python3.8/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  return generic_utils.serialize_keras_object(obj)\r\n```\r\n\r\nNote: It says showing 5 of 900, does it mean none of these functions get saved?\r\nIn addition, I also get this `CustomMaskWarning`\r\n\r\nThe model still gets saved but on loading it back throws another error as follow:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 17, in <module>\r\n    models = model.load()\r\n  File \"/home/antpc/DS/bert/bert_ner_2/model.py\", line 360, in load\r\n    return tf.keras.models.load_model(self._params.model_export_path)\r\n  File \"/home/antpc/Documents/tf2.6/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/home/antpc/Documents/tf2.6/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 530, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\nTypeError: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'\r\n```\r\n\r\nBut I am not certain if this error is related to the issue mentioned during saving", "Just found assignee is @ymodak ,  do you have any update?\r\n\r\nThanks!", "I get a similar warning with this Model:\r\n\r\n`model = tf.keras.Sequential([\r\n    tf.keras.layers.LSTM(88, return_sequences=True),\r\n    tf.keras.layers.Dense(88, activation='elu'),\r\n  ])`\r\n\r\n`WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\nWARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7ff41a768be0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.`", "35 participants affected by this problem.\r\n\r\nIs there any way to escalate this issue?", "even  i  am  getting  this  warning  on tensorflow 2.7.0  ,\r\n![image](https://user-images.githubusercontent.com/7962189/143269509-9a098e69-0c48-444d-b281-e20a4c918d5f.png)\r\n", "I met the same issue and still can't fix it", "same issue guys using LSTM and GRU", "Same here! ", "yep same here still facing this issue", "Well guess what. I got it too but it doesn't seem to affect my code...", "I am writing my masters thesis and have built and saved around 20 models.. each containing close to 150,000 trainable params. These models contain multiple LSTM and CNN layers and all of those complain about this same warning. \r\nI am yet to observe a performance drop or de-serialization problem while loading the models.\r\n\r\nSome people complained about the compile warning when loading the model and speculated this might be because of the issue in this thread, for that particular case, even the official TensorFlow-TensorRT video showing the performance boost over inferring with TensorFlow-TensorRT released on 2nd Dec 2021, has the same warning that the metrics haven't been built, but seems to infer close to 4000 images per second.. \r\nhttps://www.youtube.com/watch?v=O-_K42EAlP0&ab_channel=NVIDIADeveloper\r\n\r\nSo i say the issue is ignored on purpose since it doesn't have any major limitations. Also i don't recall anyone  mentioning any serious performance bottlenecks.. \r\n\r\nso for future readers: Dont worry if you have this warning (unless you don't know if you are doing something wrong), it is quite common and the devs know about it...\r\n\r\n \r\nP.S. I use the older .h5 format and not the newer tf savefile format, however i found those perform kinda the same(I know there are some limitations to using the older .h5 format, please see my posts above)..\r\n  ", "@KirannBhavaraju Some people complained about issues after loading such models again. It might indeed be the case that these issues are unrelated to this problem and this is merely a coincidence. However, it would be nice to get some statement from the devs which kind of implications this warning has...", "I am having this issue as well with tensorflow version 2.7.0. I had issues saving and reloading models, and lost a few hours worth of training because of it. It also makes the terminal output untidy looking, but that wouldn't be so much of a issue if the models would reliably save/load.\r\n\r\nI did notice that it only failed with the h5 file format (I had to have it in the h5 format), and so if I saved the model with the .tf extension and then reloaded it, and then saved it back into the .h5 extension, it worked. (Just in case anyone else had the issue...)", "> I am having this issue as well with tensorflow version 2.7.0. I had issues saving and reloading models, and lost a few hours worth of training because of it. It also makes the terminal output untidy looking, but that wouldn't be so much of a issue if the models would reliably save/load.\r\n> \r\n> I did notice that it only failed with the h5 file format (I had to have it in the h5 format), and so if I saved the model with the .tf extension and then reloaded it, and then saved it back into the .h5 extension, it worked. (Just in case anyone else had the issue...)\r\n\r\nIf you don't have to use h5 file format, just use tf file format instead.", "same issue with tf.keras.models.save_model if I call _set_inputs to specify signatures", "Similar warning message and errors using **onmt-main --model_type NMTMediumV1**.\r\n```\r\n2022-01-16 17:49:04.622069: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_HALF } } inputs { dtype: DT_HALF shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3080\" frequency: 1710 num_cores: 68 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8201\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5242880 shared_memory_size_per_multiprocessor: 102400 memory_size: 8484487168 bandwidth: 760080000 } outputs { dtype: DT_HALF shape { unknown_rank: true } }\r\n2022-01-16 17:49:04.623323: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_HALF } } inputs { dtype: DT_HALF shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3080\" frequency: 1710 num_cores: 68 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8201\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5242880 shared_memory_size_per_multiprocessor: 102400 memory_size: 8484487168 bandwidth: 760080000 } outputs { dtype: DT_HALF shape { unknown_rank: true } }\r\n2022-01-16 17:49:08.676000: I training.py:192] Evaluation predictions saved to run/eval/predictions.txt.5000\r\n2022-01-16 17:49:08.748000: I training.py:192] Evaluation result for step 5000: loss = 1.632232 ; perplexity = 5.115281 ; bleu = 30.196384\r\n2022-01-16 17:49:08.771000: I training.py:192] Exporting model to run/export/5000 (best bleu so far: 30.196384)\r\n2022-01-16 17:49:11.031344: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:absl:Found untraced functions such as nmt_medium_v1_1_layer_call_fn, nmt_medium_v1_1_layer_call_and_return_conditional_losses, sequence_to_sequence_inputter_1_layer_call_fn, sequence_to_sequence_inputter_1_layer_call_and_return_conditional_losses, rnn_encoder_1_layer_call_fn while saving (showing 5 of 200). These functions will not be directly callable after loading.\r\n```", "I get the same warnings when trying to convert a model to tflite, which results in a KeyError (referring the incriminating functions).", "Hi, guys. has your problem been resolved?\r\nI have the same problem. I just upgraded Tensorflow from 2.6.2 to 2.7,  the Loss of my LSTM model does not decrease and the same warning message occurred when model.save() called.\r\nhttps://github.com/tensorflow/tensorflow/issues/54090", "Hi @miccio-dk, I also got the same issue when converting the model to tflite when using the export_tflite_graph_tf2.py script, but I didn't get any KeyError.", "Just started having this issue with a new model i'm developing\r\npython3.10.2 and tf 2.9.0.dev20220202", "I am also experiencing the same kind of issues. I am currently using tf 2.7.0", "# tensorflow 2.8.0 still have this issue.\r\n\r\nHere some code to reproduce:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python import keras as K\r\n\r\nprint(tf.version.VERSION, tf.version.COMPILER_VERSION, tf.version.GIT_VERSION)\r\nprint(tf.keras.__version__, K.__version__)\r\ninputs = np.random.random ((4,5,6))\r\ninputs = tf.keras.layers.Input(shape=inputs.shape[1:])\r\nx = tf.keras.layers.LSTM(8, return_sequences=True)(inputs)\r\nx = tf.keras.layers.Dropout(0.01)(x)\r\noutputs =tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\r\nmodel.summary()\r\nmodel.save('bug.tf')\r\n```\r\n\r\nAnd the output:\r\n```\r\n2.8.0 MSVC 192729117 v2.8.0-rc1-32-g3f878cff5b6\r\n2.8.0 2.6.0\r\nModel: \"model_7\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n input_3 (InputLayer)        [(None, 5, 6)]            0         \r\n                                                                 \r\n lstm_2 (LSTM)               (None, 5, 8)              480       \r\n                                                                 \r\n dropout_2 (Dropout)         (None, 5, 8)              0         \r\n                                                                 \r\n time_distributed_7 (TimeDis  (None, 5, 1)             9         \r\n tributed)                                                       \r\n                                                                 \r\n=================================================================\r\nTotal params: 489\r\nTrainable params: 489\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nWARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: bug.tf\\assets\r\nINFO:tensorflow:Assets written to: bug.tf\\assets\r\nWARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001827197E9A0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\r\n```\r\n\r\n", "I am also experiencing the same kind of issues. tf2.4.4", "Using `tf.keras.layers.LSTM(units = chin, activation = 'tanh', return_sequences = True)` \r\ngot this warning.\r\n> WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\r\nWARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fb22c11c160> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\r\n\r\nNo error shown when using `tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units = chin, activation = 'tanh'), return_sequences = True)` this.", "This was also affecting me but I just noticed that if I save the model as `hdf5` I don't get the error :man_shrugging: ", "Same issue with tf 2.7 and LSTM:\r\n\r\n```\r\nWARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\r\nWARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f99b8647310> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\r\n```", "I had this issue today as well and when I saved the model using `lstmModel.save(\"trainedLSTM.h5\")` it did not give me this error and I am able to load the model in again for predictions", "tf 2.8.0\r\nWARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\r\nWARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000218C8D0F130> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\r\n###\r\nthis will also cause model.fit() to slow to a crawl if tf.keras.models.load_model() is used with the above samed model, major performance issue\r\n", "(By the way: This is now the 2nd most commented open tf issue :'D)", "Same results here\r\n\r\nVersions tested: \r\n- tensorflow==2.8.0\r\n- tf-nightly==2.10.0-dev20220416\r\n\r\nMethods tested:\r\n- model.save\r\n- tf.keras.models.save_model\r\n\r\nFormats tested:\r\n- 'tf'\r\n- 'hdf5'.\r\n", "i am having the same issue"]}, {"number": 47548, "title": "Different behavior between eager execution and tf.function", "body": "Hello Tensorflow community\r\n\r\nI have a loss function with a few logarithms in it:\r\n\r\n```\r\ndef NegLogProb(a, b, targ):\r\n    return -tf.reduce_mean(tf.reduce_sum(tf.math.log(a*b) + (a-1)*tf.math.log(targ+1e-8) + (b-1)*tf.math.log((1-targ**a)+1e-8), axis=-1))\r\n```\r\n\r\nThis is a form of the beta distribution. My network outputs parameters _a_ and _b_: the range for both outputs is >=(1+1e-8). _targ_ is a spectrum whose values are normalized from 0-1. There are three terms in this equation. Where necessary you can see that I added 1e-8 to avoid log(0). I believe I have put in all the safeguards to prevent the logarithm from failing.\r\n\r\nThis equation works when running in eager execution, but if I create a graph out of it, the equation will always output inf followed by NaN. Can anyone account for the fact that the behavior changes when creating a graph?\r\n\r\nI am using Tensorflow version 2.1.0\r\n\r\nMany thanks in advance.", "comments": ["@jlapin1 \r\nThe code shared above is not sufficient to reproduce the issue reported, please share code such that we can reproduce your issue or is possible share a colab gist with the error reported.", "Hopefully this is enough code to help you reproduce it. If it helps, targ, a, b all have dimensions batch_size, 9100\r\n\r\n```\r\ndef NegLogProb(a, b, targ):\r\n    return -tf.reduce_mean(tf.reduce_sum(tf.math.log(a*b) + (a-1)*tf.math.log(targ+1e-8) + (b-1)*tf.math.log((1-targ**a)+1e-8), axis=-1))\r\n    \r\nLossFunc = NegLogProb\r\ndef train_step(samples, targ):\r\n    with tf.GradientTape() as tape:\r\n        a,b = model(samples, training=True)\r\n        loss = LossFunc(a, b, targ)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    opt.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss, (a,b)\r\n\r\nsimintime = []\r\ntestintime = []\r\ndef train(epochs, batch_size=100, svwts=False):\r\n    graph = train_step#tf.function(train_step)\r\n    \r\n    tot = intf.shape[0]\r\n    steps = (tot//batch_size) if tot%batch_size==0 else (tot//batch_size)+1\r\n    \r\n    for m in range(epochs):\r\n        tots = 0\r\n        start_epoch = time()\r\n        for n in range(steps):\r\n            start_step = time()\r\n            begin = n*batch_size\r\n            end = (n+1)*(batch_size)\r\n            \r\n            targ = create_target(Pos[begin:end])\r\n            sim, var = graph(intf[begin:end], targ)\r\n            \r\n            tots+=sim.numpy()*(end-begin)\r\n            sys.stdout.write(\"\\rStep %d/%d; -log_prob: %8.1f; Time: %.1f s\"%(n+1, steps, sim, time()-start_step))\r\n        final_sim = tots/tot\r\n        simintime.append(final_sim)\r\n```", "I ran the above code on tf 2.3, tf 2.4 and nightly and do not see nay output or error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/55a64fddbfd6c68aa5fb85e36a720a48/untitled558.ipynb), can you please share a colab gist with the error reported.", "My apologies Saduf. The code as I sent it to you is running with eager execution, since the tf.function is commented out (right below the beginning of def train()).\r\n\r\nCan you run it again but replace the train subroutine with this code? Notice that the variable \"graph\" is now an actual tensorflow graph of the function \"train_step\"\r\n\r\n```\r\ndef train(epochs, batch_size=100, svwts=False):\r\n    graph = tf.function(train_step)\r\n    \r\n    tot = intf.shape[0]\r\n    steps = (tot//batch_size) if tot%batch_size==0 else (tot//batch_size)+1\r\n    \r\n    for m in range(epochs):\r\n        tots = 0\r\n        start_epoch = time()\r\n        for n in range(steps):\r\n            start_step = time()\r\n            begin = n*batch_size\r\n            end = (n+1)*(batch_size)\r\n            \r\n            targ = create_target(Pos[begin:end])\r\n            sim, var = graph(intf[begin:end], targ)\r\n            \r\n            tots+=sim.numpy()*(end-begin)\r\n            sys.stdout.write(\"\\rStep %d/%d; -log_prob: %8.1f; Time: %.1f s\"%(n+1, steps, sim, time()-start_step))\r\n        final_sim = tots/tot\r\n        simintime.append(final_sim)\r\n```", "@jlapin1 \r\nPlease share a colab gist with the error reported for us to analyse.", "I'm not 100% sure how to share the colab gist, but I just ran this in colab and this is what I got. No error statement but clearly there is different behavior between eager execution and the tf graph\r\n\r\n```\r\nimport numpy as np\r\nimport os\r\n# import re\r\nimport sys\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as L\r\nimport tensorflow.keras.activations as A\r\nfrom time import time\r\nimport matplotlib.pyplot as plt\r\nplt.close('all')\r\nfrom matplotlib.ticker import MultipleLocator\r\n\r\ntarg = tf.constant(\r\n    [[0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.20933868,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.2082187 , 0.        ,\r\n        0.18404533, 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.37072948, 0.        ,\r\n        0.        , 0.        , 0.        , 0.31289002, 0.        ,\r\n        0.        , 0.        , 0.915976  , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.23793682, 0.        , 0.        ,\r\n        0.        , 0.4674082 , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.18847302,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.2036586 , 0.17317393, 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.29346868,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.40215385, 0.        , 0.16911463, 0.        ,\r\n        0.        , 0.        , 0.24255644, 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.24563654, 0.        , 0.        , 0.        , 0.33256173,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.30373514, 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.2429015 , 0.15512827, 0.        , 0.        ,\r\n        0.        , 0.41071343, 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.29341364, 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.21696085, 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.1478434 , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.35621345, 0.        , 0.        , 0.        ,\r\n        0.        , 0.15217698, 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.38775736, 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.19092496,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.18351643, 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.5620028 ,\r\n        0.3221243 , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.18488021, 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.14928055, 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.28359178,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 1.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.17243865, 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.4688842 , 0.30614266, 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.18065585, 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.1995775 , 0.        , 0.        , 0.        , 0.        ,\r\n        0.59800917, 0.        , 0.        , 0.        , 0.        ,\r\n        0.2842905 , 0.        , 0.        , 0.98819506, 0.20961763,\r\n        0.        , 0.        , 0.        , 0.        , 0.30928153,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ,\r\n        0.        , 0.        , 0.        , 0.        , 0.        ]], dtype=tf.float32\r\n)\r\n\r\nsample = tf.constant([[19,  4, 15, 15,  4, 16, 16,  6,  8,  8, 20, 21, 21, 21, 21, 21,\r\n        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\r\n        21, 21, 21, 21, 21, 21, 21, 21, 21]], dtype=tf.int32)\r\n\r\nclass ResBlock2(L.Layer):\r\n    def __init__(self, filters, KS=3, conv_type=L.Conv1D, act=A.relu):\r\n        super(ResBlock2, self).__init__()\r\n        self.filters = filters\r\n        self.conv_type = conv_type\r\n        self.norm1 = L.BatchNormalization()\r\n        self.norm2 = L.BatchNormalization()\r\n        self.conv1 = conv_type(filters, KS, 1, padding='SAME', use_bias=False) # bias removed after running\r\n        self.conv2 = conv_type(filters, KS, 1, padding='SAME', use_bias=False)\r\n        self.act = act\r\n    def build(self, x):\r\n        self.shortcut = self.conv_type(self.filters, 1, 1) if self.filters!= x[-1] else A.linear\r\n    def call(self, x):\r\n        out = self.act(self.norm1(self.conv1(x)))\r\n        out = self.act(L.Add()([self.shortcut(x), self.norm2(self.conv2(out))]))\r\n        return out\r\n\r\ndef CustomBlock(feat_map, rang, feat_out=128, residual=True, pad='SAME', alpha=0.0):\r\n    rang = tf.range(*rang)\r\n    out = tf.concat([L.Conv1D(feat_out, int(m), 1, padding=pad)(feat_map) for m in rang], axis=-1)\r\n    out = L.BatchNormalization()(out)\r\n    if residual:\r\n        shortcut = L.Conv1D(out.shape[-1], 1, 1) if feat_map.shape[-1]!=out.shape[-1] else A.linear \r\n        out = L.Add()([shortcut(feat_map), out])\r\n    return A.relu(out, alpha)\r\n\r\nupsample = lambda x: tf.reshape(tf.tile(tf.expand_dims(x, axis=2), [1,1,2,1]), (-1, 2*x.shape[1], x.shape[-1]))\r\n\r\ndef Model_beta(seq_len, \r\n               AA_types, \r\n               out_dim,\r\n               floors=(1,1),\r\n               filtfirst=64,\r\n               rang=(2,10,1),\r\n               outin=(3,5), \r\n               filtmid=(150, 200), \r\n               filtlast=200):\r\n    inp = L.Input((seq_len,), dtype=tf.int32)\r\n    \r\n    out = tf.one_hot(inp, AA_types) # Original\r\n    out = CustomBlock(out, (3,4,1), feat_out=64, residual=False, alpha=0.0)\r\n    \r\n    outer,inner = outin\r\n    filts = np.linspace(filtmid[0], filtmid[1], outer, dtype='int')\r\n    for m in range(outer):\r\n        out = upsample(out) if m>0 else out\r\n        for n in range(inner):\r\n            filters = filts[m]\r\n            ks = 3\r\n            out = ResBlock2(filters, ks)(out)\r\n    \r\n    out = CustomBlock(out, (1,2,1), filtlast, residual=False)\r\n    \r\n    a = L.Conv1D(out_dim, 1, 1, activation='relu')(out)\r\n    a = L.GlobalAveragePooling1D()(a)\r\n    a = tf.squeeze(a)+floors[0]\r\n    b = L.Conv1D(out_dim, 1, 1, activation='linear')(out)\r\n    b = L.GlobalAveragePooling1D()(b)\r\n    b = tf.squeeze(a)+floors[1]\r\n    \r\n    return tf.keras.Model(inputs=inp, outputs=[a,b])\r\n\r\nmodel = Model_beta(41, 22, 910, 2*(1+1e-6,))\r\nopt = tf.keras.optimizers.Adam(3e-4)\r\n\r\ndef NegLogProb(a, b, targ):\r\n    return -tf.reduce_mean(tf.reduce_sum(tf.math.log(a*b) + (a-1)*tf.math.log(targ+1e-8) + (b-1)*tf.math.log((1-targ**a)+1e-8), axis=-1))\r\n\r\ndef train_step(samples, targ):\r\n    with tf.GradientTape() as tape:\r\n        a,b = model(samples, training=True)\r\n        loss = NegLogProb(a, b, targ)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    opt.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss, (a,b)\r\n\r\nouteager,_ = train_step(sample, targ)\r\n\r\nprint(outeager)\r\n\r\ngraph = tf.function(train_step)\r\n\r\noutgraph,_ = graph(sample, targ)\r\n\r\nprint(outgraph)\r\n```\r\n\r\nAnd the output is\r\n\r\ntf.Tensor(1576.0958, shape=(), dtype=float32)\r\ntf.Tensor(inf, shape=(), dtype=float32)", "I am able to replicate the issue reported on tf 2.3, 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d5864d05ecb3babc5e023af943305869/untitled562.ipynb).", "This is a bug in the constant folding logic (which is not applied in eager execution). See example below.\r\n\r\nIn a very basic test, increasing the epsilon from 1e-8 to 1e-7 was enough to avoid the inconsistency, not sure if that's acceptable for your model.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.config.optimizer.set_experimental_options(\r\n    {'constant_folding': True})  # Returns correct results with False\r\n\r\n@tf.function\r\ndef NegLogProb(a, b, targ):\r\n    return tf.add((1 - tf.pow(targ, a)), 1e-7)\r\n\r\nNegLogProb(tf.constant([1.0]), tf.constant([1.0]), tf.constant(1.0))\r\n```\r\n\r\nAlternatively, you may be able to replace `x + eps` with `tf.where(x > 0.0, x, 1e-8)`, which is more accurate for very small values as well.", "Thank you mdanatg. That should fix my issue because 1e-8 was an arbitrary choice, 1e-7 should be sufficiently small as well.", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/e2dde5dd833e3ceb05925229e9cb2cb0/untitled149.ipynb)..Thanks !"]}, {"number": 47547, "title": "TRT\uff1aconverted tensorRT model pb too large", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu-18.04.x86_64\r\n- TensorFlow installed from (source or binary): build from source \r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: cuda=10.1, cuDnn=7.6.4\r\n- GPU model and memory:\r\n- TensorRT version: 6.0.1\r\n\r\n**Describe the current behavior**\r\nthe converted tensorRT model (pb file) is too large (**9.5M --> 1.1G**, see below)\r\n\r\n**Describe the expected behavior**\r\nI guess the converted model could be larger than the original (9.5M), but should not be **too large**\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\nconversion_params = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP32)\r\n\r\ninput_saved_model_dir = 'bert_finetune_20210303'\r\noutput_saved_model_dir = 'bert_finetune_20210303_fp32'\r\n\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir, conversion_params=conversion_params)\r\nconverter.convert()\r\nconverter.save(output_saved_model_dir)\r\n```\r\n\r\nthe size bewteen converted and the original:\r\n\r\n```sh\r\n# the original\r\n4.0K\t./bert_finetune_20210303/assets\r\n9.5M\t./bert_finetune_20210303/saved_model.pb\r\n387M\t./bert_finetune_20210303/variables\r\n397M\t./bert_finetune_20210303\r\n\r\n# precision_mode=trt.TrtPrecisionMode.FP16\r\n4.0K\t./bert_finetune_20210303_fp16/assets\r\n1.1G\t./bert_finetune_20210303/saved_model.pb\r\n387M\t./bert_finetune_20210303_fp16/variables\r\n1.5G   \t./bert_finetune_20210303_fp16\r\n\r\n# precision_mode=trt.TrtPrecisionMode.FP32\r\n4.0K\t./bert_finetune_20210303_fp32/assets\r\n1.1G\t./bert_finetune_20210303_fp32/saved_model.pb\r\n387M\t./bert_finetune_20210303_fp32/variables\r\n1.5G \t./bert_finetune_20210303_fp32\r\n```\r\n\r\nanyone could help me, thanks a lot.\r\n\r\n**NOTE**: the model used here: https://drive.google.com/file/d/1sSu08U4-c-iopi5SGLfPUWP68MTpC5wg/view?usp=sharing", "comments": ["Colab session crashes on running the code with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4ca6a70325eac68fe87033d8dd8702a6/47547.ipynb). Thanks!", "> Colab session crashes on running the code with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4ca6a70325eac68fe87033d8dd8702a6/47547.ipynb). Thanks!\r\n\r\nsee the log \r\n```\r\nINFO:tensorflow:Linked TensorRT version: (0, 0, 0)\r\nINFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\r\n```\r\n\r\nmaybe you are using a non-gpu TF or did not compile with TensorRT", "@bixia1 available for checking this? Thanks a lot", "i also face this problem, anyone can solve it?"]}, {"number": 47546, "title": "About reshape op on TF Lite nnapi delegate", "body": "Android NN API only supported tensor rank: up to 4. So maybe need to add rank restriction on TF Lite nnapi delegate. If not, some models which have reshape op(rank > 4) will be failed on Android platform.\r\n\r\nFor example:\r\ntensorflow/lite/delegates/nnapi/nnapi_delegate.cc:\r\n\r\n```c++\r\nbool NNAPIDelegateKernel::Validate(\r\n    const TfLiteContext* context, int builtin_code, int version,\r\n    int android_sdk_version, const TfLiteNode* node,\r\n    bool is_accelerator_specified,\r\n    std::vector<NNAPIValidationFailure>* map_failures) {\r\n...\r\ncase kTfLiteBuiltinReshape: {\r\n...\r\n      // add these lines for rank restriction\r\n      const auto& input = context->tensors[node->inputs->data[0]];\r\n      Expect(input.dims->size <= 4,\r\n             NNAPIValidationFailureType::kUnsupportedOperandRank,\r\n             \"Input rank should be <= 4\", &val_ctx);\r\n...\r\n}\r\n...\r\n}\r\n```\r\n", "comments": ["@miaowang14 could you review this suggestion regarding shape op validation in NNAPI?", "@antkillerfarm good idea, would you like to create a PR implementing this? I am happy to review and approve it."]}, {"number": 47535, "title": "Error grabbing variables after a forward pass using autograph, dropout and GRUCell", "body": "**System information**\r\n- TensorFlow version: 2.4.0\r\n\r\n**Describe the current behavior**\r\n```\r\nTypeError: '<' not supported between instances of 'FuncGraph' and '_WeakReferencableClass'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nValueError: Error processing property '_dropout_mask_cache' of <ContextValueCache at 0x7fa31138ab10>\r\n```\r\n\r\n**Describe the expected behavior**\r\nthe variable method can safely be called after performing a forward pass with autograph and dropout.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow\r\n\r\nclass GRU(tf.Module):\r\n  def __init__(self):\r\n    super(GRU, self).__init__()\r\n    self.cell = tf.keras.layers.GRUCell(units=2, dropout=0.1, recurrent_dropout=0.1)\r\n\r\n  @tf.function\r\n  def bad_infer(self, inputs, states):\r\n    o, h = self.cell(inputs=inputs, states=states, training=True)\r\n    return o, h\r\n\r\n  def good_infer(self, inputs, states):\r\n    o, h = self.cell(inputs=inputs, states=states, training=True)\r\n    return o, h\r\n\r\ngru = GRU()\r\ninputs = tf.ones((1, 2))\r\nstates = gru.cell.get_initial_state(inputs)\r\ntargets = tf.ones(1, 2)\r\n\r\ngru.good_infer(inputs=inputs, states=states)\r\ngru.variables\r\n\r\ngru.bad_infer(inputs=inputs, states=states)\r\ngru.variables\r\n```\r\n\r\n[colab notebook](https://colab.research.google.com/drive/1kEjy5l8r5GbAKCPFtgFdHDLmVYKob_YY?usp=sharing)\r\n\r\nguessing @qlzh727 may know the issue\r\n", "comments": ["Adding Kathy since _dropout_mask_cache is more for savedmodel.", "Was able to reproduce the issue with TF v2.3, TF v2.4, TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2f0c09a1c4472c437bfc15cad432c256/47535.ipynb). Thanks!", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7600cd3c4f1b30a08b3f318c9643ae3f/untitled146.ipynb)..Thanks !"]}, {"number": 47533, "title": "tf.device breaks when using tf.data.Dataset.from_generator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 / 8.0.4\r\n- GPU model and memory: GeForce RTX 3090 24GB RAM\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.device` inside a generator which is used for dataset creation (using `tf.data.Dataset.from_generator`), the device passed to `tf.device` is not guaranteed to be used.\r\n\r\n**Describe the expected behavior**\r\nTensors created from operations within the scope of `tf.device` should be placed on the given device, unless the operations don't support that device. Anyhow, tensor placement should be deterministic.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n    with tf.device(\"CPU\"):\r\n        i = 0\r\n        for i in tf.range(5000):\r\n            i = tf.add(i, 1)\r\n            print(i.device)\r\n            yield i\r\n\r\nd = tf.data.Dataset.from_generator(gen, output_types=(tf.int32))\r\n\r\nfor i in d:\r\n    print(i)\r\n```\r\nSample output of the above code:\r\n```\r\n...\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\ntf.Tensor(4993, shape=(), dtype=int32)\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\ntf.Tensor(4994, shape=(), dtype=int32)\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\ntf.Tensor(4995, shape=(), dtype=int32)\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\ntf.Tensor(4996, shape=(), dtype=int32)\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\ntf.Tensor(4997, shape=(), dtype=int32\r\n...\r\n```\r\nIn some cases, the above code will also throw a RuntimeError: \"Exiting device scope without proper scope nesting\". I'll discuss why this happens later.\r\n\r\nThe following sample code **does not** reproduce the bug (which is expected, as discussed below):\r\n(I moved the `tf.device` call inside the loop).\r\n```\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n    i = 0\r\n    for i in tf.range(5000):\r\n        with tf.device(\"CPU\"):\r\n            i = tf.add(i, 1)\r\n            print(i.device)\r\n            yield i\r\n\r\nd = tf.data.Dataset.from_generator(gen, output_types=(tf.int32))\r\n\r\nfor i in d:\r\n    print(i)\r\n```\r\n\r\n\r\n**Detailed explanation of cause**\r\nThe issue is caused by the interaction of the implementations of `tf.device` and `tf.data`.\r\n`tf.device` is a context manager. On entrance, it sets the `device_name` to the given device, and on exit - it restores it to its previous value. However, the `device_name` is a thread-local variable, stored in the context's thread_local_data. You can see the implementation [here](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/eager/context.py#L1749).\r\nThe problem is that, due to the multi-threaded nature of `tf.data`, the python generator used by our dataset might run from different threads. I think it is only guaranteed to run on the same thread until the next `yield` statement (I didn't make sure, but it seems like it).\r\nThus, if we call `tf.device`, then call `yield`, and then resume execution on a different thread - our context's thread local data would contain a different `device_name`.\r\n\r\nIn the second code snippet I provided the issue does not reproduce, because each time we return from a `yield` statement, we set the device scope again.\r\n\r\nThe RuntimeError I mentioned above, stems from [here](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/eager/context.py#L1797), and it happens when the context manager exits with a different thread, rather than the one it entered with.", "comments": ["@ymodak \r\nI am able to replicate this issue on tf 2.3, tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/dcad70fe2cccd8fc8c4aa82f2ed6f09a/untitled557.ipynb).", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7034fbb6b3210c44a2a8bbc8738473dd/untitled143.ipynb)..Thanks !\r\n", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5ac1e412b0cf81a29b59a3e62423d618/untitled169.ipynb?authuser=1) ..Thanks!"]}, {"number": 47504, "title": "LSTMs with XNNPACK: ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.4.1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\nThe conversion is actually correct, the problem occurs when trying to use the model with a delegate that requires static-sized tensors. I'm currently testing with XNNPACK Delegate on the tflite benchmark whose output is:\r\n```\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [encoder.tflite]\r\nEnable op profiling: [1]\r\nUse xnnpack: [1]\r\nLoaded model encoder.tflite\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\nFailed to apply XNNPACK delegate.\r\nBenchmarking failed.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nThe problem can be reproduced by using the profiler as following with the model attached:\r\n```\r\n./linux_x86-64_benchmark_model --graph=encoder.tflite --enable_op_profiling=true --use_xnnpack=true\r\n```\r\n\r\nhttps://drive.google.com/file/d/1xZOAS67A0szLJEqdPTC8vtISZXvKcbO6/view?usp=sharing\r\n\r\n**Any other info / logs**\r\n\r\n\r\nI converted an encoder-decoder model for machine translation using two separate tflite models respectively for the encoder and the decoder.\r\n\r\nThe encoder uses BiLSTM and generates the error in the title. The same error doesn't occur if I use unroll=True, but this is unfeasible for me as I need input sequences of length 100. In this case, I don't get the error but then the usage of RAM explodes.\r\n\r\nThe decoder, on the other hand, uses input of length 1 and LSTMCell instead of LSTM, and it doesn't produce the error.\r\n\r\nIs there a way to make the operation static-length without using unroll?\r\nI have already tried to set the full shape of the input and output shapes of every layer.\r\nMoreover, it is a keras model but I wrap it into a tf.function for which I set the full input shape in the signature, and then I do the conversion from the concrete function but also this doesn't help to get a static size tensor. Using from_saved_model or from_keras_model doesn't help either.", "comments": ["@multiverse-tf could you take a look at this?", "CCing @renjie-liu as well for LSTM."]}, {"number": 47474, "title": "\"import tensorflow\" configures Python logging if tensorboard is not installed", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7, Python 3.8 from MacPorts, Python venv\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary wheel `tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl` from PyPI (using `python -m pip install tensorflow`) \r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version:  N/A\r\n- GPU model and memory: Radeon Pro 560X 4 GB, Intel UHD Graphics 630 1536 MB\r\n\r\n**Describe the current behavior**\r\n\r\nAfter uninstalling `tensorboard` from the Python virtual environment with `pip uninstall tensorboard`, executing `import tensorflow` at the prompt causes logging to be configured: in particular, a `StreamHandler` handler is added to the root logger.\r\n\r\nAs a result, in a large Python GUI (PyQt5) application, an `import tensorflow` at application startup time resulted in many log messages unrelated to tensorflow being printed to the console; without importing tensorflow, those log messages weren't visible. (Note that the application had set the level of the root logger to `logging.DEBUG`, and added its own root-level loggers, under the assumption that it would be the only part of the Python environment working with the configuration for the root-level logger.)\r\n\r\n**Describe the expected behavior**\r\n\r\nFollowing usual logging best practices (libraries should only emit log messages but not configure logging; log configuration should be left to the application using the library), `import tensorflow` does not affect the root logger.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere's an interpreter session demonstrating the issue\r\n\r\n```python\r\nPython 3.8.8 (default, Feb 22 2021, 09:21:28) \r\n[Clang 12.0.0 (clang-1200.0.32.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import logging\r\n>>> root_logger = logging.root\r\n>>> root_logger.handlers\r\n[]\r\n>>> import tensorflow\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.summary API due to missing TensorBoard installation.\r\n>>> root_logger.handlers  # expected an empty list\r\n[<StreamHandler <stderr> (NOTSET)>]\r\n```\r\n\r\n**Additional information**\r\n\r\nThis looks like a shallow bug and an easy fix. Here's the troublesome code: https://github.com/tensorflow/tensorflow/blob/c023a829167aa4a233a36479cd338ef9784b6990/tensorflow/compat_template.__init__.py#L43\r\n\r\nDiagnosis: because the Python logging system hadn't already been configured at the point of the `import tensorflow` execution, and because `tensorboard` wasn't available,`_logging.warning` was then called as above: that then implicitly configured logging by calling `logging.basicConfig`.\r\n\r\nA simple solution would be to set up a logger in the normal way (`logger = logging.getLogger(__name__)`) and use `logger.warning` instead of `logging.warning`; that way, implicit configuration of the logging machinery is avoided.\r\n\r\nWe were able to work around this problem in our application by making sure that we configured logging (adding a null handler to the root logger) _before_ importing `tensorflow`, so that `logging.basicConfig` was no longer invoked.\r\n\r\nAnother workaround was to ensure that `tensorboard` was installed, but since we don't generally need the functionality provided by `tensorboard`, our packaging system leaves `tensorboard` out as a explicit dependency of `tensorflow`. If the recommendation is that `tensorboard` should be treated as a required dependency of `tensorflow`, that would be good to know.", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/fb1db566517e29553062515d8262aeff/47474.ipynb). Thanks!", "@mdickinson I cannot reproduce the issue with `TF2.4.1` and `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/6dd00b069523a678946a89468eb93049/47474.ipynb) is a gist with `tf-nightly` and [here](https://colab.research.google.com/gist/jvishnuvardhan/3d2fbe4df01dea9c1e36ed8cf55406a6/47474.ipynb) is a gist with `TF2.4.1`. Thanks!\r\n\r\nCan you please verify my gist and let me know why I am not seeing that warning? Thanks!", "Can you send a PR please?", "@mihaimaruseac: I can definitely create a PR with just the fix. Would this need a regression test, too?\r\n\r\n@jvishnuvardhan: I'm not really sure how I could go about finding out why you're not seeing that warning: I can't reproduce your non-reproduction here, and I don't know anything about your system or Python environment. One suggestion would be to try this at a plain old Python prompt: it looks as though you're using IPython, and that brings in a whole lot of extra variables. (E.g., I don't know exactly what log configuration IPython does, under what circumstances.)\r\n\r\n", "@jvishnuvardhan: Ah, looking harder at your gist, you've already imported `tensorflow` to get the version, right at the top. That means that your later import will be using the existing cached import. You need to import tensorflow for the first time _after_ uninstalling tensorboard.", "Would be good to also get a regression test.", "Sorry, I'm not going to be able to provide a PR with test any time soon; I've already used up most of the time I had available trying to get bazel installed on my machine (and still haven't succeeded).", "Was able to reproduce the issue in TF v2.5 and didn't face any error,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7fcb43019480cc32719b0fefd449869b/untitled140.ipynb)..Thanks ! ", "@sushreebarsa The issue is still present with Tensorflow 2.5. Below is the output from an example shell session demonstrating the issue. Note that (a) this is not using IPython, and (b) Python caches imports, so it matters that `tensorboard` is uninstalled *before* starting the Python process and importing `tensorflow`.\r\n\r\n```\r\nlovelace:~ mdickinson$ python3.8 --version\r\nPython 3.8.9\r\nlovelace:~ mdickinson$ python3.8 -m venv --clear ~/.venvs/tensorflow47474/\r\nlovelace:~ mdickinson$ source ~/.venvs/tensorflow47474/bin/activate\r\n(tensorflow47474) lovelace:~ mdickinson$ pip install -q tensorflow\r\nWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\r\nYou should consider upgrading via the '/Users/mdickinson/.venvs/tensorflow47474/bin/python3.8 -m pip install --upgrade pip' command.\r\n(tensorflow47474) lovelace:~ mdickinson$ pip uninstall -q tensorboard\r\nProceed (y/n)? y\r\n(tensorflow47474) lovelace:~ mdickinson$ pip list\r\nPackage                 Version\r\n----------------------- -------------------\r\nabsl-py                 0.12.0\r\nastunparse              1.6.3\r\ncachetools              4.2.2\r\ncertifi                 2021.5.30\r\nchardet                 4.0.0\r\nflatbuffers             1.12\r\ngast                    0.4.0\r\ngoogle-auth             1.30.1\r\ngoogle-auth-oauthlib    0.4.4\r\ngoogle-pasta            0.2.0\r\ngrpcio                  1.34.1\r\nh5py                    3.1.0\r\nidna                    2.10\r\nkeras-nightly           2.5.0.dev2021032900\r\nKeras-Preprocessing     1.1.2\r\nMarkdown                3.3.4\r\nnumpy                   1.19.5\r\noauthlib                3.1.0\r\nopt-einsum              3.3.0\r\npip                     20.2.3\r\nprotobuf                3.17.1\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\nrequests                2.25.1\r\nrequests-oauthlib       1.3.0\r\nrsa                     4.7.2\r\nsetuptools              49.2.1\r\nsix                     1.15.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.0\r\ntensorflow              2.5.0\r\ntensorflow-estimator    2.5.0\r\ntermcolor               1.1.0\r\ntyping-extensions       3.7.4.3\r\nurllib3                 1.26.5\r\nWerkzeug                2.0.1\r\nwheel                   0.36.2\r\nwrapt                   1.12.1\r\nWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\r\nYou should consider upgrading via the '/Users/mdickinson/.venvs/tensorflow47474/bin/python3.8 -m pip install --upgrade pip' command.\r\n(tensorflow47474) lovelace:~ mdickinson$ python\r\nPython 3.8.9 (default, Apr  3 2021, 05:57:58) \r\n[Clang 10.0.1 (clang-1001.0.46.4)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import logging\r\n>>> root_logger = logging.root\r\n>>> root_logger.handlers\r\n[]\r\n>>> import tensorflow\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.summary API due to missing TensorBoard installation.\r\n>>> root_logger.handlers\r\n[<StreamHandler <stderr> (NOTSET)>]\r\n```", "Note though that `tensorboard` is listed as a dependency in TF's `setup.py`. Whenever you `pip install tensorflow` you should also get TB."]}, {"number": 47457, "title": "TF-TRT: batch dimension is failing in FasterRCNN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Nvidia docker image  `nvcr.io/nvidia/tensorflow:20.11-tf1-py3`\r\n- TensorFlow version (use command below): 1.15.4\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  10.2 / 7.6.5 \r\n- GPU model and memory: T4\r\n\r\n\r\n**Describe the current behavior**\r\nI am using the script `https://github.com/tensorflow/tensorrt/tree/r1.14%2B/tftrt/examples/object_detection` to optimize the model faster_rcnn_inception_v2, although the test was completed successfully I got the below warning about` incomplete shapes`: \r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\r\n**1752 ops no flops stats due to incomplete shapes.\r\nParsing Inputs...\r\nIncomplete shape.**\r\n.\r\n.\r\n.\r\norm/default/dso_loader.cc:49] Successfully opened dynamic library libnvinfer.so.7\r\nWARNING:tensorflow:INT8 precision mode with calibration is supported with dynamic TRT ops only. Disregarding is_dynamic_op parameter.\r\n.\r\n.\r\n.\r\nDONE (t=1.91s).\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.390\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.265\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.262\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.223\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.300\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.314\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\r\n{\r\n    \"avg_latency_ms\": 246.9332789430524,\r\n    \"avg_throughput_fps\": 32.397415343295854,\r\n    \"map\": 0.24546921626010776\r\n}\r\nASSERTION PASSED: statistics['map'] > (0.243 - 0.01)\r\n```\r\n\r\n\r\nThe test was completed successfully; however, when I tried to deploy the model with Nvidia DeepStream I got the below input shape errors, even though the engine was created with max batch size 8, it seems the object_detection.py script didn't assign the parameter input_shape for some operations\r\n```\r\n  (0) Invalid argument: Input shape axis 0 must equal 8, got shape [5,600,1024,3]\r\n         [[{{node Preprocessor/unstack}}]]\r\n         [[SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression_4/unstack/_2859]]\r\n  (1) I**nvalid argument: Input shape axis 0 must equal 8, got shape [5,600,1024,3]**\r\n         [[{{node Preprocessor/unstack}}]]\r\n```\r\n\r\nHi @samikama / @trevor-m , could you please suggest how to solve this input shapes issue?, also it seems to be related to `NMS` custom ops . A similar case was reported [here](https://github.com/tensorflow/tensorflow/issues/21081)\r\n", "comments": ["@vilmara,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "Hi @amahendrakar, thanks for your prompt reply. The reason I was using TF 1.x is that TF-TRT object detection integration with TF 2.0 doesn't include force_nms_cpu optimization technique as it was included in the branch r1.14. I have reported the issue [here ](https://github.com/tensorflow/tensorrt/issues/235) but didn't get any feedback. On the other hand, I optimized the FasterRCNN model with TF-TRT, TF 2.x (which doesn't include nms optimization feature) but didn't get a performance improvement.\r\n\r\nI need to accelerate an object detection model with int8 that works well with DS-Triton, some recommendations?"]}, {"number": 47448, "title": "custom op on gpu/dsp", "body": "The official website just tell me how to custom ops on CPU, i wonder if there\u2018s something could teach me how to do this on GPU/DSP.\r\nAnd we need  **_\u201clibhexagon_nn_skel.*.so\"_** to use DSP. If i custom ops on hexagon delegate, do i have to compile new  \u201d.so\u201c through hexagon SDK ,or just build AAR and use the old .so files?\r\nThanks a lot!!!\ud83d\ude2d\ud83d\ude2d\ud83d\ude2d", "comments": ["@karimnosseir could you take a look at this?", "Hi @1173710201 \r\nThere is a way to support user defined operation on hexagon DSP, but it is experimental and not supported by Qualcomm, so we didn't expose it.\r\n\r\nIf you can share details on your usage, we can look if we can plan adding this.\r\n\r\nThanks", "@karimnosseir  Thanks for your help! Actually this is my graduation project, so there's no specific usage now. My teacher asked me to figure out whether the tflite supported custom ops on GPU/DSP.", "@karimnosseir  Thanks for your help! Actually this is my graduation project, so there's no specific usage now. My teacher asked me to figure out whether the tflite supported custom ops on GPU/DSP.", "Hi @xmy19029 \r\nFor DSP no supported API for doing this as mentioned.\r\n\r\nFor GPU, added @impjdi to help answer this.\r\n\r\nThanks", "For GPU, it's possible to use custom ops, but you need a lot of plumbing.  There is no clean API that lets you do that currently.\r\n\r\n1. You first need to create a model that has the custom op.  I unfortunately don't know how to do that.  https://www.tensorflow.org/lite/guide/ops_custom is probably the best place for entry and if you have questions,  @karimnosseir  is probably at a better position to answer that.\r\n2. Once you have a model with custom ops, you need to update the op resolver to recognize the op.  See MutableOpResolver.\r\n3. Then, you need to update GPU's model_builder.cc to recognize your new custom op.\r\n4. Finally, you have to write GPU shader code (depending on whether you use OpenGL/OpenCL/Vulkan) that implements the kernel.\r\n\r\nThese are going to be quite a lot of work, if you're new to the codebase or GPU programming.", "Have you @xmy19029 tried to add custom op for GPU?\r\n\r\n@impjdi I have a followup question. I am running a model on Android with GPU delegate. The `tf.gather` op in my model is not supported. If I add custom op for it in Android GPU delegate, should I follow the same steps as you suggested above?"]}, {"number": 47425, "title": "Disable unroll batch matmul pass", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): nightly\r\n\r\n### 2. Code\r\n\r\nhttps://colab.research.google.com/drive/1VxiMyDs5B_Oxcg29EmPSu9qgvPejCWqx?usp=sharing\r\n\r\n### 3. Failure after conversion\r\n\r\nIt's all successful. But with unrolling (large batch dim), the model will be significant larger, and probably decrease the performance. I suppose it's just a workaround when tflite does not have batch matmul kernel, but we have it now.", "comments": ["Thanks @WindQAQ  for filing a feature request.", "@talumbau could you take a look at this?", "Many delegates don't support the BatchMatMul op, so we default to unrolling. We are working on a more user friendly approach here, but for now, you could manually set the flag here for your individual case:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/common/tfl_pass_config.h#L36", "Got it! Thanks for the response \ud83d\ude04 "]}, {"number": 47424, "title": "SELU activation in TFlite micro", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): **2.5**\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.** Currently no good alternative exists for deployment if micro. Batch Norm for micros seem to have som issues in certain Cortex M4 platforms.\r\n\r\n**Will this change the current api? How?** It will add SELU activation to the supported ops in TF Lite Micro\r\n\r\n**Who will benefit with this feature?** Embedded developers, interested in building NN's with multple layers.\r\n\r\n**Any Other info.**\r\nSELU activation as per [Klaumbauer et.al](https://arxiv.org/abs/1706.02515) provides self normalising, similar to Batch Normalisation, but requiring less compute, as it is an activation function. \r\nAt the time of writing we haven't found alternatives, to running self normalising on microprocessor implementations. \r\n\r\nThe application on wich the operator is desired is a predicitive maintainence system we are developing.\r\nOur model is trained on field data captured from a large number of rotating machines, where the data mean and variance differs a lot, from from machine to machine, caused by mechanical tolerances and different mechanical transfer functions. Thus a degree of normalisation through the network is required, if we are to use identical models for detecting emerging defect in rotating components. \r\nWe are using a convolutional model, fed with raw data, to be able to capture transient events, as opposed to traditional FFT based feature extraction, based on [this proposal](https://www.researchgate.net/publication/314247372_A_New_Deep_Learning_Model_for_Fault_Diagnosis_with_Good_Anti-Noise_and_Domain_Adaptation_Ability_on_Raw_Vibration_Signals) \r\n\r\nFrom what i can see the SELU could be implemented similarily to the ELU `/lite/kernels/elu.cc` that uses a lookuptable when performing inference, and the selu itself, works as sketched below  :\r\n\r\n\r\n```\r\n#define ALPHA   1.6733f\r\n#define LAMBDA  1.0507f\r\n\r\ninline void Selu(const RuntimeShape& input_shape, const float* input_data,\r\n                const RuntimeShape& output_shape, float* output_data) {\r\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\r\n  for (int i = 0; i < flat_size; ++i) {\r\n    const float val = input_data[i];\r\n    output_data[i] = val < 0.0f ? LAMBDA * (ALPHA * std::expm1(val) - ALPHA) : LAMBDA * val;\r\n  }\r\n}\r\n```\r\n", "comments": ["Hi, are there any updates on this? I have a DNN with SeLu activation for the hidden layers. I am trying to deploy the DNN on an ARM Cortex M4 MCU. I still can't see SeLu in the allowed Op list. Can using SELECT_TF_OPS solve this in any way? "]}]