[{"number": 39398, "title": "TensorFlowOpLayer messes up the TensorBoard graphs / grouping with TensorBoard", "body": "I am not sure if this is a bug, feature request or something I do wrong. I apologize in advance. I do believe this is a non-intended behaviour, or at the very least, a missing documentation. \r\n\r\nAs you can see in https://stackoverflow.com/questions/61594352/tensorflowoplayer-messes-up-the-tensorboard-graphs, I have asked this question and get no response. \r\n\r\nI copy the question from Stackoverflow to here:\r\n\r\nThis question is about TensorFlow (and TensorBoard) version 2.2 (running in tensorflow/tensorflow:2.2.0-gpu-jupyter on Ubuntu 18.04.3 LTS), but I have experienced the same issue with 2.2rc3 and 2.1. It is a continuation of the question '[Messed up TensorBoard graphs due to Python operations](https://stackoverflow.com/questions/61581114/messed-up-tensorboard-graphs-due-to-python-operations)'.\r\n\r\nConsider the following code:\r\n\r\n```python3\r\nfrom datetime import datetime\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninputs = keras.layers.Input(shape=(784, ))    \r\n\r\noutputs = tf.zeros([32, 10], tf.float32)\r\n\r\nfor i in range(0, 3):\r\n    x = keras.layers.Dense(32, activation='relu', name='Model/Block' + str(i) + '/relu')(inputs) \r\n    x = keras.layers.Dropout(0.2, name='Model/Block' + str(i) + '/dropout')(x)\r\n    x = keras.layers.Dense(10, activation='softmax', name='Model/Block' + str(i) + '/softmax')(x)\r\n    outputs = keras.layers.Lambda(lambda x: x[0] + x[1], name='Model/add/add' + str(i))([outputs, x])\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.summary(line_length=100, positions=[.45, .58, .67, 1.])\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              optimizer=keras.optimizers.RMSprop(),\r\n              metrics=['accuracy'])\r\n\r\nlogdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\r\nmodel.fit(x_train, y_train,\r\n          batch_size=32,\r\n          epochs=5,\r\n          validation_split=0.2,\r\n          callbacks=[tensorboard_callback])\r\n```\r\n\r\nWhen run, it prints the following summary:\r\n``` \r\n____________________________________________________________________________________\r\nLayer (type)                          Output Shap Param  Connected to               \r\n====================================================================================\r\ninput_1 (InputLayer)                  [(None, 784 0                                 \r\n____________________________________________________________________________________\r\nModel/Block0/relu (Dense)             (None, 32)  25120  input_1[0][0]              \r\n____________________________________________________________________________________\r\nModel/Block0/dropout (Dropout)        (None, 32)  0      Model/Block0/relu[0][0]    \r\n____________________________________________________________________________________\r\nModel/Block1/relu (Dense)             (None, 32)  25120  input_1[0][0]              \r\n____________________________________________________________________________________\r\nModel/Block0/softmax (Dense)          (None, 10)  330    Model/Block0/dropout[0][0] \r\n____________________________________________________________________________________\r\nModel/Block1/dropout (Dropout)        (None, 32)  0      Model/Block1/relu[0][0]    \r\n____________________________________________________________________________________\r\nModel/Block2/relu (Dense)             (None, 32)  25120  input_1[0][0]              \r\n____________________________________________________________________________________\r\ntf_op_layer_AddV2 (TensorFlowOpLayer) [(32, 10)]  0      Model/Block0/softmax[0][0] \r\n____________________________________________________________________________________\r\nModel/Block1/softmax (Dense)          (None, 10)  330    Model/Block1/dropout[0][0] \r\n____________________________________________________________________________________\r\nModel/Block2/dropout (Dropout)        (None, 32)  0      Model/Block2/relu[0][0]    \r\n____________________________________________________________________________________\r\nModel/add/add1 (Lambda)               (32, 10)    0      tf_op_layer_AddV2[0][0]    \r\n                                                         Model/Block1/softmax[0][0] \r\n____________________________________________________________________________________\r\nModel/Block2/softmax (Dense)          (None, 10)  330    Model/Block2/dropout[0][0] \r\n____________________________________________________________________________________\r\nModel/add/add2 (Lambda)               (32, 10)    0      Model/add/add1[0][0]       \r\n                                                         Model/Block2/softmax[0][0] \r\n====================================================================================\r\n```\r\n\r\nNote the strange entry `tf_op_layer_AddV2 (TensorFlowOpLayer)`. This kind of entry makes the TensorBoard graphs very messy. It turns out that when avoiding the use of the `tf.zeros()`, this strange `tf_op_layer_AddV2` element is not added. So the following code will not generate any `tf_op_layer` element: \r\n\r\n```python3\r\nfrom datetime import datetime\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninputs = keras.layers.Input(shape=(784, ))\r\n\r\nx = keras.layers.Dense(32, activation='relu', name='Model/Block0/relu')(inputs) \r\nx = keras.layers.Dropout(0.2, name='Model/Block0/dropout')(x)\r\noutputs = keras.layers.Dense(10, activation='softmax', name='Model/Block0/softmax')(x)\r\n\r\nfor i in range(1, 3):\r\n    x = keras.layers.Dense(32, activation='relu', name='Model/Block' + str(i) + '/relu')(inputs) \r\n    x = keras.layers.Dropout(0.2, name='Model/Block' + str(i) + '/dropout')(x)\r\n    x = keras.layers.Dense(10, activation='softmax', name='Model/Block' + str(i) + '/softmax')(x)\r\n    outputs = keras.layers.Lambda(lambda x: x[0] + x[1], name='Model/add/add' + str(i))([outputs, x])\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nmodel.summary(line_length=84, positions=[.46, .60, .69, 1.])\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              optimizer=keras.optimizers.RMSprop(),\r\n              metrics=['accuracy'])\r\n\r\nlogdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\r\nmodel.fit(x_train, y_train,\r\n          batch_size=32,\r\n          epochs=5,\r\n          validation_split=0.2,\r\n          callbacks=[tensorboard_callback])\r\n\r\n```\r\nThere are many more complex examples where the `tf_op_layer_` elements are created. It will be appreciated if it is exaplained why they are created and how to avoid them.", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/b2104ca83ce67e0789c4fa912a2bc624/39398.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/f8d53f50bc7dfceae9e31d93f88204e3/39398-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@rani-pinchuk Just want to know you use-case with more details. Why do we have two things (`outputs` layer and `outputs` tensor) in the model? What is your intentions in defining two with the same name? I think that resulted the complicated model. Can you change `outputs` tensor to some other name and define you model. Thanks!", "My use case is to get a useful Tensorboard graph for a large model (which I am afraid I cannot share). In that model, I use different operations like '+' or t[begin:end] and these operations mess up the model due to the prefixes created by TensorFlowOpLayer. \r\n\r\nThe example above is just an example to reproduce this behaviour. I have more examples where this is happening, for example with the t[begin:end] operation. Also shape operations create this problem. \r\n\r\nIn the low level TensorFlow 1.0 it was possible to group operations and therefore generate very nice graphs. It seems that in TensorFlow 2.0 with Keras, the way to do that is by using name hierarchy (e.g. 'Model/add/add') (I could not find any documentation about how to group operations in TF2 and found about the name hierarchy trick from examples online, so I am not sure that I don't mis anything here). But the TensorFlowOpLayer messes this up. \r\n\r\nRegarding the `outputs` Tensor in the example - there is one `outputs` identifier. There is no `outputs` layer. The `outputs` identifier is first used to hold a Tensor of zeros. In the loop, the `outputs` identifier gets the sum of the last Tensor referred by that identifier and the result from the Dense and Dropout layers. It could be written more clearly like this:\r\n```python\r\noutputs = tf.zeros([32, 10], tf.float32)\r\n\r\nfor i in range(0, 3):\r\n    x = keras.layers.Dense(32, activation='relu', name='Model/Block' + str(i) + '/relu')(inputs) \r\n    x = keras.layers.Dropout(0.2, name='Model/Block' + str(i) + '/dropout')(x)\r\n    x = keras.layers.Dense(10, activation='softmax', name='Model/Block' + str(i) + '/softmax')(x)\r\n    outputs = outputs + x\r\n```\r\nHowever, in the above (which is the origin of whole this problem) the `+` operation is unnamed, and therefore will mess the TensorBoard graph even more (see https://stackoverflow.com/questions/61581114/messed-up-tensorboard-graphs-due-to-python-operations/61594411). \r\n\r\nBTW, I have tried to change the `outputs` Tensor name (see https://stackoverflow.com/questions/61594352/tensorflowoplayer-messes-up-the-tensorboard-graphs). This had no effect. ", "This looks more like an issue with the naming of the TensorFlowOpLayer as generated within Keras than an issue with TensorBoard per se; if the layer name discards the hierarchy information we can't really recreate it.\r\n\r\nAssigning to @omalleyt12 to assess since it looks like you wrote TensorFlowOpLayer?\r\n\r\ncc @davidsoergel as current TensorBoard onduty", "@rani-pinchuk Seems the issue is fixed in latest TF version(2.5 and Nightly) and `tf_op_layer_AddV2 (TensorFlowOpLayer)` is not getting added in the model summary. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/23cfb01b7ad8a9f7d0aa05d35ce43b86/untitled79.ipynb).\r\n\r\nClosing this issue as of now. Please feel free to re-open the issue if you any concern.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39398\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39398\">No</a>\n"]}, {"number": 39397, "title": "Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] ", "body": "Training was going fine till 47K steps and then got Resource exhausted error. \r\n```\r\nFile \"/home/saini/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n\u00a0 \u00a0 raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n\u00a0 (0) Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node gradients/AddN_162-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Loss/Cast_232/_16919]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\u00a0 (1) Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node gradients/AddN_162-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\nOS: Ubuntu 18.04\r\nTensorflow: 1.14.0 GPU\r\nCUDA: 10.0\r\nCUDNN:: 7.6\r\nBatch Size: 32\r\n\r\nFollow changes i have made in default TF object detection API: \r\n\r\n```\r\nmodel_lib.py\r\ntf.estimator.EvalSpec(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=eval_spec_name,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_fn=eval_input_fn,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 steps=None,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 throttle_secs = 172800,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exporters=exporter))\r\neval.proto\r\noptional uint32 eval_interval_secs = 3 [default = 172800]; # default = 600\r\nmodel_main.py\r\nconfig = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps=5000)\r\n```\r\n\r\n", "comments": ["@sainisanjay  \r\nIn order to expedite the trouble-shooting process, please provide a simple stand alone code to reproduce the issue reported here. Thanks!\r\n ", "Hi @Saduf2019 \r\nError got during object detection training with ssd_mobilenet_v2_quantized_300x300_coco model. \r\nI am running below command to start the training:\r\n`python ../../models/research/object_detection/model_main.py --pipeline_config_path=./ssd_mobilenet_v2_quantized_300x300_coco.config --model_dir=./training/ --num_train_steps=2000000 --sample_1_of_n_eval_examples=1 --alsologtostderr `\r\nTraining was going fine till 47900 steps, after that i got error:\r\n\r\n```\r\n2020-05-11 09:53:31.475093: W tensorflow/core/common_runtime/bfc_allocator.cc:319] ****************************************************************************************************\r\n2020-05-11 09:53:31.475131: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at transpose_op.cc:199 : Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"/home/adas/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/home/adas/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/adas/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node gradients/AddN_162-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Loss/Cast_232/_16919]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node gradients/AddN_162-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n", "**Note :** The error occurred after 47900 steps. My question is why after 47900 steps there is an error. Why not at the initial steps?", "@sainisanjay Can you please try `TF1.15.2` as there were lots of performance improvements after `TF1.14`. Please let us know whether you still face same issue. Did you monitor memory? Can you also fill the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) with details of your platform. Thanks!", "Hi @jvishnuvardhan, I have tried with `TF1.15.0 GPU` and found same issue. And surprisingly OOM error got exactly same 47900 steps.\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Using Default TF Object Detection API to train SSD Model (`model_main.py`)`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 18.04`\r\n- TensorFlow installed from (source or binary): `TF installed using pip3 install tensorflow-gpu==1.15.0`\r\n- TensorFlow version (use command below): `1.15.0`\r\n- Python version: `Python3`\r\n- CUDA/cuDNN version: `10.0/7.6`\r\n- GPU model and memory: `Quadro RTX 5000 GPU (16GB)`\r\n", "Hi @sainisanjay,\r\n\r\nCan you try using getting a [count of all live Python](https://stackoverflow.com/questions/26889459/get-count-of-reachable-live-objects) objects and check that it isn't increasing on every step?  If it is increasing then you probably have a memory leak in your program (you can also try getting the types of the live objects to see e.g. if the number of live  `Tensor`s is increasing to narrow this down further).  If the number of live objects stays the same but you still OOM after a specific number of iterations then it could be a problem with TensorFlow.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 39396, "title": "TFlite model performance issues on Snapdragon 855+ - using benchmark_model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n   \r\n   using benchmark_model built from source (tf 2.2.0) using bazel. have a custom network model\r\n   involving tf.matmul  (variable inputs) as target network graph for evaluation.\r\n \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\n  Ubuntu 18.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\n  OnePlus 7t (Snapdragon 855+)\r\n  Android 10\r\n\r\n- TensorFlow installed from (source or binary):\r\n  source 2.2.0\r\n\r\n- TensorFlow version (use command below):\r\n  2.2.0\r\n\r\n- Python version:\r\n\r\n  3.5+\r\n\r\n- Bazel version (if compiling from source):\r\n\r\n  3.1.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n clang via android ndk toolchain\r\n\r\n- CUDA/cuDNN version:\r\n\r\n N/A, GPU does not support the model and some ops\r\n\r\n- CPU model and memory:\r\n\r\n  Custom model involving series of Convolutions, BN, Matmuls.\r\n  Model Size ~23 MB\r\n  Max Memory usage ~65 MB\r\n\r\n**Describe the current behavior**\r\n\r\nModel has to be run on CPU since tf.matmul (variable inputs) operation is not supported on both GPU and Hexagon (or even via SNPE).\r\n\r\nAverage inference delay on cold start (of the phone) is low with low standard deviation (e.g., with 1000 runs). As the number of runs/iterations are increased (e.g., to 15000) for the benchmark_model script, the inference delay starts increasing and the phone gets hotter.\r\n\r\n(pls note that higher delay for warm-up runs are ignored)\r\n\r\n**Describe the expected behavior**\r\n\r\n The inference delay is expected to be fairly constant for continuous operation. The degradation in performance with time is not expected and perhaps related to thermal issues.\r\n\r\nAny suggestions or ideas to support custom operation via GPU or Hexagon delegate would be useful.  \r\n\r\nRecommendations of a chipset/platform with good thermal performance for sustained operations would also be helpful.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nAny tflite model taking e.g. average inference execution time of 50-100ms on Snapdragon 855+ would do.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["CPU throttling is a system feature so user application usually an app can't control it.\r\nSince running ML model with CPU is highly CPU intensive task, it's difficult to avoid it. Maybe you need to reduce the number of computation.\r\n\r\nUsing GPU or NNAPI helps since it reduces CPU burden. As you said, some ops are not supported with these delegates, you might need to refactor your model to leverage it.\r\nBTW, did you try master branch?\r\nThere is a new feature that you can run a partial graph on GPU more efficiently.", "Thanks Terry for your response. I have tried NNAPI and CPU/GPU combined operation but the inference delays are higher than CPU-only operation for *my* network. I suspect the intermediate layer data  exchange between CPU and GPU may be causing it (large buffers at partial graph outputs).  I am surprised that MLPerf or AI-benchmark have not included the throttling and its effect on inference delays in the published studies/reports.  It would be great if you can suggest the best Edge-AI platform from a inference delay stability perspective esp. for sustained inferencing (several hours at stretch).\r\n\r\nCan you pls provide a pointer to the partial graph and corresponding checkin on master branch, i have compiled master branch about a month ago.", "There are several changes merged for partitioning support. I think https://github.com/tensorflow/tensorflow/commit/1912ef16d67af82aff8a18a44cd555a919145046 is the last.\r\n\r\nThis tool is useful to see what causes the delay during graph execution. (including partial delegation)\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#profiling-model-operators\r\n\r\nAre you building an Edge device? If it is, I think Coral EdgeTPU is the best solution. https://coral.ai/products/accelerator/", "Terry, thanks for the pointers on partitioning and profiling- will check.\r\n\r\nCan i use the Coral Edge TPU on a custom imx8 based board by Toradex, assuming i can build yocto linux (with tensorflow and Edge TPU driver support) for this platform ? \r\n ", "As far as I know it uses libusb based userlevel driver so it doesn't require any kernel patch.\r\nhttps://github.com/google-coral/edgetpu#linux", "Thanks Terry, I am looking at the Coral Edge TPU specs - https://static6.arrow.com/aropdfconversion/bfaa407713d5f91af2008bfe17f2d2cb97118f7d/usb_accelerator.pdf\r\n\r\nIt says - \r\n\r\n\"Caution: If you enable the maximum operating frequency, the USB Accelerator becomes very hot to\r\nthe touch during operation and might cause burn injuries. Either keep the device out of reach when\r\nusing with the maximum frequency or instead use the default clock frequency to avoid injury. \"\r\n\r\nIf we clock at half the maximum speed, i am assuming the inferencing delays will be doubled. Is there some sort of thermal testing report for the Edge TPU, that i can refer to ? I am really looking to see whether Edge TPU is built for continuous operation (days at stretch) and at what frequency is this sustainable?\r\n\r\n", "@amitmate Does your Fully Connected / Matmul same shape but dynamic data, or dynamic shape ?\r\nNote that, we support now FullyConnected with dynamic weights (with constant shape) in Hexagon Delegate.\r\nIt might be good to share more details about the unsupported ops that needs to run on CPU.\r\n", "thanks Karim - it is same shape and dynamic data. i had tried the hexagon delegate earlier, but it supported only certain models (it failed for my model that contained matmul with both inputs variables). Can you provide some pointers or samples to use the hexagon delegate for a custom model?", "The support was added recently, so try again. Nothing different just follow the guide and use the nightly\r\nhttps://www.tensorflow.org/lite/performance/hexagon_delegate\r\n\r\nIf you are having issue, let us know what is the issue.\r\n\r\nThanks", "Thanks, we will try it out and let you know.", "@karimnosseir , thanks , am able to delegate nodes to the hexagon processor now . still need to do more tests. will post after we finish more detailed tests on this.", "@karimnosseir  I get the following error for one of the tf operation, when i am quantizing our model using the hexagon delegate conversion instructions above. \r\n\r\nRuntimeError: Quantization not yet supported for op: : REDUCE_MAX\r\n\r\nIf i bypass this quantization, the delegate does not run any nodes on the hexagon. REDUCE_MAX operator seems to be induced by the quantization process, i don't see that operator in the fp version of the model.\r\n\r\nHow do we handle this? ", "Hi @amitmate \r\nThese are 2 separate issues\r\n1) Why Reduce Max is not quantized ?, can you share more details about the Op (params, type and quantization type you passed). I will check it\r\n\r\n2) Why no nodes are delegated, it really depends on the resulting model, if your model has float ops we can't execute float on delegate, if you share the model or more details about the ops in the model, i can tell you why ?\r\n\r\n", "@karimnosseir thanks for looking into this.\r\n\r\nFor issue 1, have attached the model summary , hope that is sufficient, let me know if anything else is needed. The MatrixMul custom layer has tf.matmul.\r\n\r\n[GMAC_net.txt](https://github.com/tensorflow/tensorflow/files/4916381/GMAC_net.txt)\r\n\r\nFor issue 2, since REDUCE_MAX was not getting quantized (as INT8), i had to remove the instruction to force supported ops to be int8 i.e., i removed the following line from quantization code , but input/output types were still uint8, and so the model stayed float as you have guessed.\r\n\r\n##converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\nI have tried analyzing the tf operations using netron , but no insights. Please provide some pointers on how to analyze these issues as well. If the solution or its diagnostics involve making an open source contribution, we would be happy to help.\r\n", "@amitmate I sent a fix for the Reduce max quantization. Can you retry from head of master branch, or wait for nightly and retry.", "@karimnosseir , we get the following error now with nightly (tf 2.4.0-dev20200716)\r\n\r\nRuntimeError: Quantization not yet supported for op: 'REDUCE_PROD'.\r\n\r\nThanks for looking into this.\r\n", "@amitmate Currently we don't support quantized reduce_prod, can you convert your model partially quantized - not all ops are quantized - instead.\r\nYou can do this by using tf.lite.OpsSet.TFLITE_BUILTINS instead of tf.lite.OpsSet.TFLITE_BUILTINS_INT8\r\n\r\nTry this and let me know \r\n\r\nThanks", "Tried this before, when we set to  tf.lite.OpsSet.TFLITE_BUILTINS , none of the nodes are delegated to Hexagon. (netron shows most operations are floating point). Can try again with the latest to see any difference", "@amitmate Can you please paste your exact options used ?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39395, "title": "add support of reduce max to tensorflow lite micro", "body": "@tensorflow/micro\r\n\r\ntf lite micro 2.2\r\n\r\nissue: Didn't find op for builtin opcode 'REDUCE_MAX' version '1'\r\n\r\nsuggestion: add support of reduce max to tensorflow lite micro", "comments": ["Hi @wenhui-prudencemed ! I am working on an implementation for int8 support in reduce_max for TFL micro. I expect to deliver within a few weeks", "I will also be adding float32 support in the same PR.", "Hi @wenhui-prudencemed ! I think tf.reduce_max is supported now for lite models. Attaching relevant[ thread ](https://www.tensorflow.org/lite/guide/ops_compatibility)for reference.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39394, "title": "Is the TFLite-Micro Library Thread-safe?", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- Tensorflow version (commit SHA if source): 2.1.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): N/A\r\n\r\n**Describe the problem**\r\nMay I know whether the TF-Lite Micro library is thread-safe or not?\r\n(It is not documented in the official documents nor in the source code)\r\n\r\nMeaning if I create multiple instances of resolver/interpreter/working buffer, I can run multiple models at the same time?\r\n(I am a little worried about the ::tflite::GetModel() function is NOT thread-safe)\r\n\r\nThanks for your reply!\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Hi @williepan1121 ,\r\nI had the same question and didn't find it in the TensorFlow 2.2 documentation. So I decided to write a test script to async request an API and test if the outcome is equal!\r\nMy code is available in this repo:  https://github.com/DanielOverdevest/testStabilityPredictionsAPI ", "- tflite::GetModel is basically memory mapping the data from bytes array to structured data. This is readonly, so thread safe.\r\n\r\n- We're pretty cautious about the use of global variables etc. (if kernel uses global variable, then there is a bug that we need to fix.) The resolver and error reporter is readonly once initialized. The interpreter can only handle one model inference at a time. The MicroAllocator can be used for multiple models to share the same arena, but not concurrently.\r\n\r\nSo I would say if you create a new Interpreter (and a new allocator if needed) for each model you want to run in parallel, it should be threadsafe. You can share Resolver and Error Reporter as you want.\r\n\r\n- TFLM was designed for tiny chips and micro controllers, which typically doesn't have multiple cores dedicated for AI that can benefit from multi-threading. Do you mind share more information on your usecase as why multi-threading is needed?\r\n\r\nThanks,\r\nTiezhen", "@williepan1121, Did you refer information mentioned [above](https://github.com/tensorflow/tensorflow/issues/39394#issuecomment-686323375) by @wangtz. As requested please share more information on your usecase as why multi-threading is needed?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39394\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39394\">No</a>\n"]}, {"number": 39393, "title": "Is the TFLite-Micro Library Thread-safe?", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": []}, {"number": 39392, "title": "[RNN]Failed to do full integer quantization and got error: Failed to parse the model: pybind11::init(): factory function returned nullptr.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.15.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.LSTM(256, input_shape=(60, 388), activation='tanh',\r\n                               return_sequences=True))\r\nmodel.add(tf.keras.layers.Dropout(0.3))\r\nmodel.add(tf.keras.layers.Dense(388, activation='softmax'))\r\n\r\ndef representative_data_gen():\r\n  for input_value in test_ds.take(100):\r\n    yield [input_value]\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model_quant = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 51, in __init__\r\n    _calibration_wrapper.CalibrationWrapper(model_content))\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/lisichao/PycharmProjects/KerasSavedModel/mnistExample.py\", line 155, in <module>\r\n    tflite_model_quant = converter.convert()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 611, in convert\r\n    constants.FLOAT, True)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 316, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 53, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n```\r\n**Failure details**\r\nWhen I convert the TensorFlow Model without optimization, it works.\r\nWhen I do post-training integer quantization without a data representative, it works.\r\nWhen I provide a data representative, it fails.\r\nThe error can be reproduced from the following file.\r\n[ReproduceTheError.zip](https://github.com/tensorflow/tensorflow/files/4607083/ReproduceTheError.zip)\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@LesterRe,\r\nOn running the given code snippet with TF v2.2, I am facing an error stating `AttributeError: 'str' object has no attribute 'call'`. \r\n\r\nWhereas, running the `mnistExample.py` file I am facing an error stating `IndexError: index 302 is out of bounds for axis 2 with size 60`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/58b42b158d9ce310a66fd5133194ac32/39392.ipynb).\r\n\r\nCould you please provide the complete code and the supporting files you are using, in order to reproduce the issue reported here. Thanks!\r\n", "> @LesterRe,\r\n> On running the given code snippet with TF v2.2, I am facing an error stating `AttributeError: 'str' object has no attribute 'call'`.\r\n> \r\n> Whereas, running the `mnistExample.py` file I am facing an error stating `IndexError: index 302 is out of bounds for axis 2 with size 60`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/58b42b158d9ce310a66fd5133194ac32/39392.ipynb).\r\n> \r\n> Could you please provide the complete code and the supporting files you are using, in order to reproduce the issue reported here. Thanks!\r\n\r\nHi amahendrakar,\r\nSorry for the previous zip file. I did make a mistake and I corrected it. You can find it below.\r\nLooking forward to your reply.\r\n[producingError.zip](https://github.com/tensorflow/tensorflow/files/4612919/producingError.zip)\r\n", "Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/cd8abc23078bf4cdbff771bab387770e/39392.ipynb#scrollTo=vG36_oc_ItLL) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/bfbe8488883f9f0e7f7f7b34fda11154/39392-tf-nightly.ipynb). Please find the attached gist. Thanks!", "I am facing the same issue. Any solution for that?", "Also having the same issues. Would love to hear if there was a workaround/update for this.", "I am also having this issue, any updates on this so far? Thanks!", "Same error here, any idea? A workaround would be definitely enough for the time being, but its a bit urgent.", "I have come across the issue as well, using both an LSTM and GRU keras layer. Is this due to  lack of support for full integer quantization for RNN? ", "Yeah it seems the Calllibrator class doesn't support RNN yet, but I don't know how to fix that. Would love to though, if I could get some guidance.", "I'm also getting this error with an [object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection) model. Since it is a community-driven project, I don't know whether this is a TF bug or something in the model that TF doesn't support yet.", "I'm also having this issue on MobileNet v2 + SSD with tf 2.3.1", "I'm getting  this issue now also with tf-nightly (2.4.0.dev20201002).\r\nCould you advise or update?", "Same error with 2.4.0-dev20201021 on CenterNet + MobileNet v2", "It looks like indexing /slicing are not supported. when I remove these ops, it works.", "@shlomi-amitai can you kindly expantiate on how to remove indexing/slicing\r\n", "It's very model dependent.. I had operations like tensor2 = tensor1[:,:,:-1,:]. removing these solved the problem. it probably means that these operations are yet not implemented for tflite quantization.", "I am getting the same issue with a model that contains both RNN and extensive slicing. No way I can remove that from the model. Is there an estimate on when these operations will be implemented for tflite quantization?", "I have the same issue.\ud83d\ude2d", "I have the same issue, tested under TF2.4 and nightly (2.5.0-dev20201216), with model contains LSTM, but no explicit slicing.  I wonder if this is due to the lack of support of RNN.. Is this documented anywhere?", "I have the same issue, with TF2.4.1 and , with model contains LSTM, but no explicit slicing. \r\n\r\nhttps://www.tensorflow.org/lite/convert/rnn specifies : The feature is part of TensorFlow 2.3 release. is there an undocumented limitation there ?\r\n\r\n\r\n", "Hi,\r\n\r\nSame issue here with TF2.4.1.\r\n\r\nConversion without optimization works, but when I provide a representative dataset it fails with:\r\n`ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.`\r\n\r\nWhich is the planned timeline for this fix? Is there another TF version that can workaround the issue?", "I had the same error. I was able to get around it by setting `unroll=True` for the LSTM.\r\n\r\nIt is strange that, according to [this talk](https://youtu.be/2tnWHCxP8Bk?t=261), RNN/LSTM is natively supported in TF2.x, without any change to the model. ", "Same error here with this model:\r\n```python\r\n    input = Input(shape=(time_steps, data_dim))\r\n    lstm_units = 256\r\n    lstm_out = LSTM(lstm_units, return_sequences=False)(input)\r\n    output = Dense(4, activation=\"softmax\")(lstm_out)\r\n    model = Model(inputs=[input], outputs=output)\r\n    model.summary()\r\n```", "> I had the same error. I was able to get around it by setting `unroll=True` for the LSTM.\r\n> \r\n> It is strange that, according to [this talk](https://youtu.be/2tnWHCxP8Bk?t=261), RNN/LSTM is natively supported in TF2.x, without any change to the model.\r\n\r\nSetting \"unroll=True\" for the LSTM allows for INT Quantization, but has a significantly negative impact on the accuracy and inference time (+50%) of the model, so it is unfortunately not a viable work-around.", "I tried to run your code & didn't faced any error reported here.I faced a different error .Please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/5ac844ec60236bca29acb7e91bcecaf1/untitled319.ipynb)..and let us know if this issue has been resolved ?Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39392\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39392\">No</a>\n"]}, {"number": 39391, "title": "Calling model.load_weights() on model built with Sequential API throws ValueError error - TensorFlow Save and loading APIs Guide", "body": "**System information**\r\n- Have I written custom code: No. Following a TF Guide and adding a line of code mentioned in its comments - no custom code/implemention.\r\n- OS Platform and Distribution: Code is running on Google Colab from Google Chrome (Version 81.0.4044.129 (Official Build) (64-bit))\r\n- TensorFlow installed from (source or binary): Binary - TF is running in Google Colab\r\n- TensorFlow version: 2.2.0-dev20200508 (tf.version.GIT_VERSION = v1.12.1-31489-g6047d50555)\r\n- Python version: 3.6.9\r\n- Bazel version: NA\r\n- GCC/Compiler version: NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nI am running the [Save and loading APIs Guide](https://www.tensorflow.org/guide/keras/save_and_serialize) by TensorFlow and came across an error that should not be raised as per the documentation provided in the same Notebook.\r\n\r\nI was exploring the behavior of model.load_weights('pretrained_ckpt') [in this part of the guide](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/save_and_serialize.ipynb#scrollTo=Zdnpw_6PEsAN). The code cell contains the following:\r\n\r\n```\r\n# Example 2: Sequential model\r\n# Recreate the pretrained model, and load the saved weights.\r\ninputs = keras.Input(shape=(784,), name='digits')\r\nx = keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\r\nx = keras.layers.Dense(64, activation='relu', name='dense_2')(x)\r\npretrained_model = keras.Model(inputs=inputs, outputs=x, name='pretrained')\r\n\r\n# Sequential example:\r\nmodel = keras.Sequential(\r\n    [pretrained_model, keras.layers.Dense(5, name='predictions')])\r\nmodel.summary()\r\n\r\npretrained_model.load_weights('pretrained_ckpt')\r\n\r\n# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error,\r\n# but will *not* work as expected. If you inspect the weights, you'll see that\r\n# none of the weights will have loaded. `pretrained_model.load_weights()` is the\r\n# correct method to call.\r\n```\r\n\r\nThe comment at the end states that,\r\n\r\n> ```# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error```\r\n\r\nHowever, if I add a new code cell right after the mentioned cell with the following line, it throws an error.\r\n\r\n`model.load_weights('pretrained_ckpt')\r\n`\r\n\r\nThe error is:\r\n\r\n```\r\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\r\n\r\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.engine.training.Model object at 0x7f4335f43ac8> and <tensorflow.python.keras.layers.core.Dense object at 0x7f4334612748>).\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-22-d50d92a829e2> in <module>()\r\n----> 1 model.load_weights('pretrained_ckpt')\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)\r\n   1126     \"\"\"\r\n   1127     if not self.is_compatible_with(other):\r\n-> 1128       raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n   1129 \r\n   1130   def most_specific_compatible_shape(self, other):\r\n\r\nValueError: Shapes (5,) and (64,) are incompatible\r\n```\r\n\r\nWhy is this happening? Has something changed recently causing this to break? \r\n\r\n**Describe the expected behavior**\r\n\r\nAs the comment states, the function call must not throw an error and return a function to be used.\r\n\r\n> ```# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error```\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease see the sample code provided in the notebook linked below to reproduce the issue: \r\n[Google Colab Notebook](https://colab.research.google.com/drive/1-ilxHnd2gdqG8NvrJ2dvj0hO_d4_gV7u?usp=sharing)\r\n\r\n**Other info / logs**\r\nNA\r\n", "comments": ["@abdulsamadzahir\r\nI am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/f56a3468e8d049b18f058683845e03a0/untitled178.ipynb). \r\nCan you please refer to these links and  let us know if it helps.\r\n[link](https://stackoverflow.com/questions/49703387/tensorflow-valueerror-shapes-1-and-are-incompatible/50180397)\r\n[link1](https://stackoverflow.com/questions/47482082/tensorflow-error-raise-valueerrorshapes-s-and-s-are-not-compatible-self)\r\n[link2](https://stackoverflow.com/questions/54871922/value-error-shapes-are-incompatible-tensorflow-tfrecord)\r\n[link3](https://forum.opennmt.net/t/valueerror-tensor-s-shape-is-not-compatible-with-supplied-shape/3297)", "@Saduf2019 \r\nThanks for the links. I went through them and I'm afraid I could not use them. However, scrutinizing the lines, I can see why calling the `load_weights()` method would fail. Below is a brief explanation.\r\n\r\n`model` is built using Sequential API and is based on `pretrained_model` followed by Dense(5) layer.\r\n\r\n`pretrained_model`: Input((784,)) -> Dense(64) -> Dense(64)\r\n\r\n`model`: [`pretrained_model`, Dense(5)] = Input((784,)) -> Dense(64) -> Dense(64) -> Dense(5)\r\n\r\nHowever, the `save_weights()` method is called on `pretrained` model which has the following architecture:\r\n\r\n`pretrained`: Input((784,)) -> Dense(64) -> Dense(64)\r\n\r\nSince `model` and `pretrained` have different architectures (and the extra layer in `model` does have weights), calling `load_weights()` method on model will/should pretty much throw an error. \r\n\r\nSo, basically, changing this line\r\n\r\n`model = keras.Sequential([pretrained_model, keras.layers.Dense(5, name='predictions')])`\r\n\r\nto this\r\n\r\n`model = keras.Sequential([pretrained_model])`\r\n\r\nallows the code to run without an error ([Google Colab Notebook](https://colab.research.google.com/drive/1-ilxHnd2gdqG8NvrJ2dvj0hO_d4_gV7u?usp=sharing&authuser=2#scrollTo=99j72ke4EP8t)). \r\n\r\nAccording to the remaining part of the comment, calling `model.load_weights('pretrained_ckpt')` should not have left the weights unchanged. But they *do* change. \r\n\r\n> \\# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error,\r\n> \\# but will *not* work as expected. **If you inspect the weights, you'll see that**\r\n> \\# **none of the weights will have loaded**. `pretrained_model.load_weights()` is the\r\n> \\# correct method to call.\r\n\r\nThe following assertion test did not throw an error which indicated the weights in `model` were actually updated.\r\n\r\n```\r\nfor a, b in zip(pretrained.weights, model.weights):\r\n  np.testing.assert_allclose(a.numpy(), b.numpy())\r\n```\r\n\r\n(Why are we comparing `model.weights` against `pretrained.weights`? Because the weights were saved from `pretrained` model by calling `pretrained.save_weights('pretrained_ckpt')`)\r\n\r\nAfter a bit of digging, I realized that `model` is inheriting its weights from `pretrained_model` when building the model calling `Sequential([pretrained_model])`. Since `pretrained_model` weights were pre-loaded a few lines earlier with a call to `pretrained_model.load_weights('pretrained_ckpt')`, `model` ends up having the same weights as `pretrained` model and the assertion above will not throw any errors.\r\n\r\nTo actually verify that the `model.load_weights('pretrained_ckpt')` fails to change the weights in `model`, simply comment out `pretrained_model.load_weights('pretrained_ckpt')` line and watch everything fall apart.\r\n\r\nI guess it was a case of unclear documentation where the writer had a different idea in mind and the writing did not convey that.\r\n\r\nNote to any future readers:\r\nThe variable names in this example are *very* confusing. We have pretrained, pretrained_model, and model variables which are variables of type Model which means we have pretrained model, pretrained_model model and, of course, model model. This makes the last paragraphs a tad confusing, but I've done my best to ensure the variable names are correct.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39391\">No</a>\n"]}, {"number": 39390, "title": "Mississippi Coronavirus Historic Data", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:https://covidtracking.com/data/state/mississippi#historical\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## The last four days of data does not match what is posted on the Mississippi State Health Departments website of Coronavirus statistics on number of tests conducted.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@rickbarb \r\n\r\nThis is not a Tensorflow Documentation issue. Request you to close this issue if your question is not regarding to Tensorflow. Thanks!", "spam issue"]}, {"number": 39389, "title": "<removed>", "body": "import tensorflow\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nprint(tensorflow.__version__)\r\n\r\nthats ALL my code\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Anon/PycharmProjects/Imgrecn1/img.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Anon\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Anon\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\Anon\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\", line 77, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"C:\\Users\\Anon\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 42, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Anon\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 50, in <module>\r\n    DEVICE_PLACEMENT_EXPLICIT = pywrap_tensorflow.TFE_DEVICE_PLACEMENT_EXPLICIT\r\nAttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT'\r\n\r\n these are all the errors. i honestly despise who ever made tensorflow\r\n", "comments": ["thanks to anyone that can help", "how about you try\r\n\r\n````python\r\nimport tensorflow\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nprint(tensorflow.__version__)\r\n````\r\n\r\nhere is a snippet\r\n\r\n````python\r\n>>> import tensorflow\r\n>>> print(tensorflow.__version__)\r\n2.2.0\r\n````", "@Ihatetensorflow,\r\nTensorFlow is supported only on 64-bit systems. Could you please check if you have 64-bit Python installed on your computer.\r\n\r\nWith 64-bit Python installed, run the below commands to install the latest version of TensorFlow \r\n```\r\npip install --upgrade pip\r\npip install tensorflow==2.2.0\r\n```\r\n\r\nFor more information, please check this official [installation guide](https://www.tensorflow.org/install#download-a-package) from TensorFlow. Thanks!", "This is likely #32315. We don't support 32 bits Python builds.\r\n\r\nFinally, please follow guidance listed in [code of conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39389\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39389\">No</a>\n", "This is the most useless and undocumented open source package of all time!"]}, {"number": 39388, "title": "transfer learning for speech recognition ( short word vocabulary)", "body": "Dear Authors, \r\n\r\nI am trying to apply transfer learning for my dataset looking into the blog posted: \r\nhttps://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md\r\nIs there a way to download the pertained checkpoints and load it for further training? \r\n\r\nThank you so much team TF", "comments": ["no", "@krishnabairavi95 \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 39387, "title": "tf.keras.Model does not support `*` in call method signature.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- TensorFlow version (use command below): latest\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nA tf.keras.Model will fail if the call method signature includes the `*` named argument only identifier from Python 3.7. (and if python 3.8 is supported by TF then it should also support the `/` identifier)\r\n\r\nhttps://docs.python.org/3.7/reference/compound_stmts.html#function-definitions\r\n\r\n**Describe the expected behavior**\r\nShould be robust to the use of these indentifiers\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1-5-BygHPbpuBlOXNme2PEKsFqyfG6jvl?usp=sharing\r\n\r\n**Other info / logs** \r\nN/A", "comments": ["I have tried in colab with TF version 2.2 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c44c3ac0461260927d892fbd5f7974e8/untitled875.ipynb).Thanks!", "Huh. Well, I've learned something today. We have not yet considered adding support for these new symbols. If you are interested in doing so, please consider putting together an [RFC](https://www.tensorflow.org/community/contribute/rfc_process), as I'm sure there is a wider community discussion to be had here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39387\">No</a>\n"]}, {"number": 39386, "title": "TF 2.1.0 doesn't recognize GPU, PyTorch does", "body": "**System information**\r\n- Windows 10 home\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.7\r\n- Installed using conda\r\n- CUDA/cuDNN version: CUDA is 10.1, cuDNN is 7.6.5\r\n- GPU model and memory: GeForce GTX 960\r\n\r\n**Describe the problem**\r\nTF 2.1.0 doesn't recognize my GPU.\r\nI've double and triple checked the versions of CUDA, cuDNN and everything is matching.  \r\nI have set the environment variables as required in the end of TF installation, and checked that they are exposed in my code.  \r\nIn addition, **PyTorch do recognize and able to use my GPU**.  \r\nI also tried installing `tf-nightly`, without help.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe following prints 0:\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\n", "comments": ["@CodeJjang Can you please check whether [this](https://github.com/tensorflow/tensorflow/issues/38007#issuecomment-618210369) can help you. Are you running any other code on GPUs while you executed `list_physical_devices`? Thanks!", "@jvishnuvardhan This IOMMU looks more related to multi GPU thing and not my case.  \r\nIn my case, PyTorch identifies it fine, and TF doesn't.\r\nAnd yes, some other code is running on the GPU, mainly code related to display.", "@CodeJjang Can you please check with `TF2.2` and let us know whether it detects or not. Thanks!", "@CodeJjang \r\nCould you please verify on latest stable tf version if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39386\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39386\">No</a>\n"]}, {"number": 39385, "title": "Very slow compared to my PC", "body": "\r\n![Screenshot (3)](https://user-images.githubusercontent.com/39881819/81509318-5490ba00-9327-11ea-990f-d35c11b67c3d.png)\r\n\r\nGoogle Colaboratory notebook training speed(On GPU) is very slow compare to my PC.\r\nSpecs:- Windows 10\r\n            GPU - Nvidia Geforce GTX 950M\r\n            CUDA - v10.1\r\n            CUDNN - v7.6\r\nPer Epoch Training time on my PC(GPU) - 5-6 minutes\r\nPer Epoch Training time on Google Colaboratory(GPU)- More than 3 hr\r\nI am using mounted drive for providing the dataset.\r\n\r\nplease provide me a solution so i can train my model faster.", "comments": ["@rajkr476  \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n ", "![Screenshot (4)](https://user-images.githubusercontent.com/39881819/81647316-733ba180-944a-11ea-8c93-3eecd18fcf6a.png)\r\nOS - Windows 10\r\nAnaconda Navigator\r\nTensorflow Version - 2.1.0", "![Screenshot (5)](https://user-images.githubusercontent.com/39881819/81706173-7c019700-948d-11ea-8767-9078915e0711.png)\r\n![Screenshot (6)](https://user-images.githubusercontent.com/39881819/81706204-80c64b00-948d-11ea-8250-c41d40d9f390.png)\r\n\r\nI have also getting these UserWarning.", "Please provide a github gist/use proper markdown formatting. Screenshot attachments paint the picture but does very little to repro the issue at our end. Thank you.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39384, "title": "Update protobuf-java to 3.9.2", "body": "This PR updates protobuf-java to 3.9.2, to match C++ version\r\nin tensorflow/workspace.bzl (3.9.2), and to fix the issue raised\r\nin #39381 about Java9+ specific warning messages (related https://github.com/protocolbuffers/protobuf/issues/3781)\r\n\r\nThis PR fixes #39381.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Looks like a problem with the build system, not the PR. Is there a way to retry the build?", "The build system is hosed momentarily with a fixed ETA for the end of the week.", "Build system should be fixed now, retriggering jobs.", "Thank you @yongtang and @mihaimaruseac for the quick turnaround."]}, {"number": 39383, "title": "Manual cherrypick of #38292", "body": "", "comments": []}, {"number": 39382, "title": "Tflite Heaxgon delegate - incompatible library", "body": "I am going through the example [here](https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_c_api) and having trouble building the binary to due to a link error:\r\n\"skipping incompatible C:/REPOSITORIES/tflite/v2.2.0/arm64-v8a/libhexagon_nn_skel.so when searching for -lhexagon_nn_skel\". The error is repeated for libhexagon_nn_skel_v65 and libhexagon_nn_skel_v66 as well.\r\n\r\nThe device architecture is arm64-v8a.\r\n\r\nI'm building the example as a part of a larger project which already uses TFlite on GPU successfully. I'm building using ndk-build, so the build.grade file is irrelevant. What should I replace it with? What else I might be doing wrong? \r\n", "comments": ["just noticed that the .so libraries provided are 32 bit. Do you have a 64 bit version?", "to clarify, my intention is to use this as part of native c++ code. NDK version is 19, device is Snapdragon 855", "@yakovdan What is the problem you're facing.\r\nYou just need to package the provided libraries with your app", "My build was configured for the target arm64-v8a instead of armeabi-v7a which caused the previous issue. Is there a version of those libraries for arm64-v8a? \r\nIn the mean time, I switched to armeabi-v7a and created a simple native activity project in android studio. Then I followed the provided example precisely and got a link error:\r\n\r\nerror: undefined reference to 'TfLiteHexagonDelegateDelete'\r\n\r\nTfLiteHexagonDelegateDelete is declared in hexagon/hexagon_delegate.h, and defined in hexagon/hexagon_delegate.cc. Am I supposed to build hexagon_delegate.so using bazel and package it with my app? I've already added libtensorflowlite.so to the makefile. \r\n\r\nSee attached my CMakeLists.txt\r\n[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/4645427/CMakeLists.txt)\r\n\r\n\r\n ", "For you convenience,  the entire project is attached\r\n[HelloJNI3.zip](https://github.com/tensorflow/tensorflow/files/4645510/HelloJNI3.zip)\r\n", "The easiest might be to fetch the nightly build see [hexagon_delegate guide](https://www.tensorflow.org/lite/performance/hexagon_delegate)\r\n\r\nAnother way, you can build the libraries yourself using bazel\r\n* hexagon_delegate static library\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/BUILD#L70\r\n* libtensorflowlite_hexagon_jni.so (shared library)\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/java/BUILD#L13\r\n* libhexagon_interface.so:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/hexagon_nn/BUILD#L34\r\n\r\n\r\nFor your issue with TfLiteHexagonDelegateDelete, looks like you are missing link to the hexagon_delegate library.", "I did as you suggested. Now I have a bunch of error of the following form:\r\ntensorflow/lite/experimental/delegates/hexagon/hexagon_delegate.cc:231: error: undefined reference to 'tflite::HexagonDelegateKernel::InitState()'\r\n\r\nI went ahead and manually built all the dependent libs and added them statically. This works. Is there a better way? ", "Also, I need to switch the target from armeabi-v7a to arm64-v8a. What should I do except recompile the libs for arm64?", "Using shared library is easier. You will need to load the .so file and call the needed functions.", "Ok, this works. \r\nWhat about compiling for arm64-v8a? just recompile the shared library for arm64-v8a and that's it?", "If you're using the prebuilt we already provide arm 64 and 32. If you're building them just pass --config=android_arm64 ", "I verified that this works, for arm64 too. Can you explain how this works, since libhexagon_nn_skel* are 32bit libraries?", "libhexagon_nn_skel* are compiled for hexagon hardware and not Arm. You don't need to worry about them. As long as they are in correct place the delegate will load them.\r\n\r\nThanks"]}, {"number": 39381, "title": "Upgrade protobuf-java to resolve Java9+ specific warning messages", "body": "Please upgrade the com.google.protobuf:protobuf-java dependency to the latest version to resolve https://github.com/protocolbuffers/protobuf/issues/3781\r\n\r\nI forced an upgrade on the application-side and it seemed to work cleanly.", "comments": ["Added a PR #39384 for to update protobuf-java to 3.9.2 (match C++ version in tensorflow).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39381\">No</a>\n"]}, {"number": 39380, "title": "Can't install Tensorflow v1.15 with pip", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.15\r\n- Python version: 3.8.1\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm unable to install v1.15 (or any version older than 2.2.0) using pip\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nRun command: `pip install tensorflow==1.15`\r\n\r\nResults in the error:\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)\r\nERROR: No matching distribution found for tensorflow==1.15\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[Verbose log](https://github.com/tensorflow/tensorflow/files/4605851/pip-log.txt)\r\n", "comments": ["@tobico \r\nIs there any particular reason for installing an older version of tensorflow when there are later versions available.\r\nFor the issue reported,could you please refer to the [comments here](https://github.com/tensorflow/tensorflow/issues/34302#issuecomment-571183939) and let us know if it helps.\r\n\r\n- With Python 3.8 you cannot install TF-1.15,  please downgrade python and try (python3.8  is not supported in TF before 2.2)", "User was trying to use python 3.8, which was not officially supported when tensorflow was at version 1.15. After installation of python 3.7, problem was resolved.\r\n\r\nPlease check on pypi, there are no files available for cp38 for 1.15, Only the versions listed by command have (i.e 2.2.0rc1, 2.2.0rc2) a cp38 whl file available [here](https://pypi.org/project/tensorflow/2.2.0rc1/#files).", "Icould neither install Tensorflow 2 using python 38{\r\n\r\nHere is the problem stack\r\n\r\nPS C:\\Users\\mtalo> python\r\nPython 3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\",\r\n line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_inte\r\nrnal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_inte\r\nrnal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <mo\r\ndule>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50,\r\n in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\",\r\n line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\",\r\n line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_inte\r\nrnal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_inte\r\nrnal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\mtalo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "@tobico: you need to use Python3.7, (3.6, 3.5 also work) for that, not Python3.8, as @jyrj mentioned in the first comment\r\n\r\n@egmtalo solution is in https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156 and many duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204. No need to downgrade as @jyrj mentions in the last comment", "Closing since this is not a coding issue but a tool usage one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39380\">No</a>\n", "Thanks Mihai,\n\nWill replace python 3.8 with 3.7 version\n\nMtalo EG\n\n<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>\nVirus-free.\nwww.avg.com\n<http://www.avg.com/email-signature?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>\n<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>\n\nOn Fri, May 15, 2020 at 3:27 AM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> @tobico <https://github.com/tobico>: you need to use Python3.7, (3.6, 3.5\n> also work) for that, not Python3.8, as @jyrj <https://github.com/jyrj>\n> mentioned in the first comment\n>\n> @egmtalo <https://github.com/egmtalo> solution is in #36167 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156>\n> and many duplicate issues: #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> #36151\n> <https://github.com/tensorflow/tensorflow/issues/36151> #36138\n> <https://github.com/tensorflow/tensorflow/issues/36138> #36054\n> <https://github.com/tensorflow/tensorflow/issues/36054> #36045\n> <https://github.com/tensorflow/tensorflow/issues/36045> #36020\n> <https://github.com/tensorflow/tensorflow/issues/36020> #36003\n> <https://github.com/tensorflow/tensorflow/issues/36003> #35988\n> <https://github.com/tensorflow/tensorflow/issues/35988> #35903\n> <https://github.com/tensorflow/tensorflow/issues/35903> #35880\n> <https://github.com/tensorflow/tensorflow/issues/35880> #35865\n> <https://github.com/tensorflow/tensorflow/issues/35865> #35805\n> <https://github.com/tensorflow/tensorflow/issues/35805> #35789\n> <https://github.com/tensorflow/tensorflow/issues/35789> #35773\n> <https://github.com/tensorflow/tensorflow/issues/35773> #35772\n> <https://github.com/tensorflow/tensorflow/issues/35772> #35767\n> <https://github.com/tensorflow/tensorflow/issues/35767> #35766\n> <https://github.com/tensorflow/tensorflow/issues/35766> #35749\n> <https://github.com/tensorflow/tensorflow/issues/35749> #35721\n> <https://github.com/tensorflow/tensorflow/issues/35721> #35618\n> <https://github.com/tensorflow/tensorflow/issues/35618> #35204\n> <https://github.com/tensorflow/tensorflow/issues/35204>. No need to\n> downgrade as @jyrj <https://github.com/jyrj> mentions in the last comment\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39380#issuecomment-628955571>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APQ5SC5DTQOT3PS4MFUMAJTRRSD7XANCNFSM4M5I6DAA>\n> .\n>\n", "make sure you are using python 3.7 version. You can also refer to [here](https://stackoverflow.com/questions/52584907/how-to-downgrade-python-from-3-7-to-3-6)\r\n "]}, {"number": 39379, "title": "test", "body": "Signed-off-by: Lalatendu Mohanty <lmohanty@redhat.com>", "comments": []}, {"number": 39377, "title": "data_element may be type tuple, instead of tf.Tensor", "body": "https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/base_preprocessing_layer.py#L171\r\n\r\nIs it expected that a `tf.data` Dataset should only contain features, without labels?\r\n\r\nConcretely, `next_data()` would return a tuple if the Dataset contains separate features and labels (common when `model.fit(dataset, ...)` ). Due to the subsequent try-except, this will cause `shape` to be set to `None` (i.e. tuple has no attribute shape) and the preprocessing normalization layer will TypeError since `len(None)` etc.\r\n\r\nApologies if I've misunderstood.", "comments": ["It's expecting a dataset with a single input, it can be either feature or label", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39377\">No</a>\n"]}, {"number": 39376, "title": "Does tensorflow 2.X provide functions similar to \"tf.contrib.lookup.index_table_from_tensor\" in TensorFlow 1.X?", "body": "In TensorFlow 1.X, \"tf.contrib.lookup.index_table_from_tensor\" is a very useful function. But I haven't found a similar function yet in TensorFlow 2X. So does tensorflow 2.X provide functions similar to \"tf.contrib.lookup.index_table_from_tensor\" in TensorFlow 1.X?\r\nThank you very much.", "comments": ["@mrchor \r\nCould you please refer to this link and let us know if it helps:\r\n[link](https://stackoverflow.com/questions/53383032/option-for-tf-contrib-lookup-index-table-from-tensor)\r\n[link2](https://www.programcreek.com/python/example/90379/tensorflow.tables_initializer)", "Yes, these link is useful to me.\r\nBut I found that tensorflow's \"index_table\" does not appear to be able to train in parallel in GPU.\r\nThere will be some trouble\uff1a\r\n![image](https://user-images.githubusercontent.com/10007145/81764598-814df880-9504-11ea-9843-3fb1093d49b6.png)\r\nAnd I tested that it is normal to run without \"index_table\" in parallel in GPU.", "@mrchor \r\nAs the issue reported in is addressed, could you please open a new issue for the one reported above.", "@Saduf2019 OK\uff0cthanks ~"]}, {"number": 39374, "title": "I have encounter with the following issue;", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@spj05251,\r\nIn order to expedite the trouble-shooting process, could you please fill in the issue template and provide the complete error log of the issue you are facing. Thanks!", "Empty issue. Closing as spam", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39374\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39374\">No</a>\n"]}, {"number": 39373, "title": "Problematic image loading with `image_dataset_from_directory`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0-dev20200508\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen using `image_dataset_from_directory` to load images it drops the color of the images significantly as can be seen below:\r\n\r\n![image](https://user-images.githubusercontent.com/22957388/81499518-d4982f00-92e9-11ea-9f0c-7a188bdf1fe1.png)\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe color drop should not be there.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab Gist: https://colab.research.google.com/gist/sayakpaul/f4c686a65ec88fa9bb0e62f4bc32004d/scratchpad.ipynb. \r\n\r\n", "comments": ["@sayakpaul \r\ni am able to replicate this issue, please find the [gist here for nightly](https://colab.sandbox.google.com/gist/Saduf2019/cda059b126a6a616e0d38b06ffc767a0/untitled172.ipynb)\r\ni ran the code on tf 2.1 and the results are better, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/bb44ff0c0a8f26cefb8208e30552f04b/untitled172.ipynb) and let us know if that helps.", "The results are not better I think as the image quality should not be degraded to this level. ", "@fchollet have you experienced anything like this? ", "@sayakpaul The warning clearly points to the root-cause of the issue.\r\n\r\n```\r\nshow_batch(image_batch.numpy(), label_batch.numpy(), image_data_gen=False) \r\n\r\n# the above line throws a warning as shown below\r\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers\r\n```\r\nAs the `image_batch` is of `float32`, we need to pass values between 0 and 1. However, the values are between 0 and 255. As mentioned in the warning, it clips values between 0 and 1 and pass the data, which is why the images are not good.\r\n\r\nSo, if you divide the `image_batch` array by 255. then the images are good. Please take a look at the gist [here](https://colab.research.google.com/gist/jvishnuvardhan/f7281d40804697ef0aa56b0e31798a26/scratchpad.ipynb) for your reference.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "Fair enough. Thank you :)\r\n\r\n@jvishnuvardhan, I was wondering if it is possible to do something like the following?\r\n\r\n```python\r\ndef scale(image, label):\r\n    return tf.image.convert_image_dtype(image, tf.float32), label\r\n\r\nbatch = tf.keras.preprocessing.image_dataset_from_directory(flowers)\r\nbatch = batch.map(scale)\r\n```\r\n\r\nWhen I run `print(tf.reduce_max(image_batch[1,:]))` I get `tf.Tensor(**255.0**, shape=(), dtype=float32)`. My understanding is `batch.map(scale)` should have taken care of the pixel scaling steps as reflected [in the documentation](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype) also. Here's the [Colab Gist](https://colab.research.google.com/gist/sayakpaul/831f2a584f92cdcc97f81bf0eb344ad1/scratchpad.ipynb) that would reproduce this. \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39373\">No</a>\n", "@sayakpaul Can you please close this issue as the original issue was resolved and open a new issue so that it will be easy for the community to follow. Your question might help others in the community. Please ping me in that issue. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39373\">No</a>\n"]}, {"number": 39372, "title": "Failed to compile TF 1.15.2 from source code", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): want install tensorflow from source\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): bazel 0.26.1\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: GPU: - Memory: 4GB\r\n\r\n**Describe the problem**\r\n\r\nHello, I wanted to build TensorFlow 1.15.2 from source to works with my old non AVX supported CPU( Intel Pentium(R) Dual-Core E5300 2.60GHz). I looked [TensorFlow community wheels](https://github.com/yaroslavvb/tensorflow-community-wheels/issues) but not found any TF 1.15.2 with Python 3.8.2 and  without AVX support.\r\n\r\nI just downloaded [TF 1.15.2 source code from github release](https://github.com/tensorflow/tensorflow/archive/v1.15.2.tar.gz), Extract it, According to [here](https://www.tensorflow.org/install/source) I installed dependencies and ...\r\n\r\n```\r\n$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.8/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\nan strange error throws durring compile: /usr/bin/python not found :|\r\nyou can see in configure file I selected to use /usr/bin/python3 file so I don't know why it want that file.\r\n`ln -s /usr/bin/python3 /usr/bin/python` silents the error but after 30 min another error occurred. I saved the details of the second error in [here](https://drive.google.com/open?id=12PtN4_7W6I1x6lS1ypT48ujmuXIecNQ8).\r\n\r\n\r\n \r\n\r\n\r\n", "comments": ["@amir28 \r\n\r\nWith Python 3.8 you cannot install TF-1.15, please downgrade python to 3.7 and it should work fine.\r\n(python3.8 is not supported in TF before 2.2).\r\n\r\nPlease check on pypi, there are no files available for cp38 for 1.15, Only the versions listed by command have (i.e 2.2.0rc1 to rc4) a cp38 whl file available [here.](https://pypi.org/project/tensorflow/2.2.0rc4/#files).Thanks!", "Thanks to reply\r\nI another question maybe a little not related to this issue, If I compile master branch using `bazel build --config=opt --config=v1` it will compile lastest version of TF 1.x?\r\nI'll downgrade to put 3.7 but It could be great if it works with py 3.8+. Correctly I can't use TF 2.x.\r\nThanks a lot.", "@amir28 \r\n\r\nCan you please raise a new issue by filling the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) and close this issue here.Thanks!", "Yes, of course.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39372\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39372\">No</a>\n", "@amir28 yes, passing `--config=v1` will always build the V1 API and behavior. But note that we no longer test and support those from master.", "Thanks, but between download and compile lastest release (2.x) with `--config=v1` and download and compile lastest 1.x release which one is suggested (which is better)?"]}, {"number": 39370, "title": "fit function doesn't throw any error or warning if validation_data is a list instead of tuple but shows unexpected validation logs", "body": "<em>From [this StackOverflow question](https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0/61707324#61707324), the `fit` function with validation_data passed as a list continuously shows zero accuracy and loss without any warning or error. Here is a simple model trained with validation_data that shows such behavior [colab notebook](https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing)\r\n</em>\r\n\r\n**System information**\r\nRan on Google Colab\r\n\r\n**Describe the current behavior**\r\nIf the `validation_data` parameter is passed as a list `[X, y]`, the model shows +0.000 for all the loss and metrics. Even though in the documentation it's mentioned the validation_data should be a tuple [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), the function accepts a list too but in the `unpack_x_y_sample_weight` function the data is sent in a wrong format if it's a list without throwing any error or warning. The log finally shows +0.0000 for all the validation results. \r\n\r\n**N.B: If `keras` is used instead of `tensorflow.keras` the validation logs show the desired output.**\r\n\r\n**Describe the expected behavior**\r\nValidation metrics shouldn't show +0.0000, either the list should be cast to a tuple or a warning/error message should be invoked.\r\n\r\n**Standalone code to reproduce the issue**\r\n[colab notebook](https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing)\r\n\r\nHere is a minimal example to reproduce the behavior:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import *\r\nimport numpy as np\r\n\r\nx = np.random.randn(1,19,)\r\ny = np.ones((1,1))\r\n\r\ndef make_model():\r\n    input_vec = tf.keras.layers.Input((19,))\r\n    final = tf.keras.layers.Dense(12, activation='relu')(input_vec)\r\n    final = tf.keras.layers.Dense(1, activation='sigmoid')(final)\r\n\r\n    model = tf.keras.models.Model(inputs=input_vec, outputs=final)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n    return model\r\n\r\nmodel = make_model()\r\n\r\nmodel.fit(x, y, batch_size=1, epochs = 10, validation_data=[x,y])\r\n```\r\nLog:\r\n```\r\nEpoch 1/10\r\n1/1 [==============================] - 0s 61ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/10\r\n1/1 [==============================] - 0s 36ms/step - loss: 0.8296 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 3/10\r\n1/1 [==============================] - 0s 32ms/step - loss: 0.8106 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 4/10\r\n1/1 [==============================] - 0s 30ms/step - loss: 0.7918 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 5/10\r\n1/1 [==============================] - 0s 31ms/step - loss: 0.7734 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 6/10\r\n1/1 [==============================] - 0s 30ms/step - loss: 0.7554 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 7/10\r\n1/1 [==============================] - 0s 31ms/step - loss: 0.7377 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 8/10\r\n1/1 [==============================] - 0s 30ms/step - loss: 0.7203 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 9/10\r\n1/1 [==============================] - 0s 34ms/step - loss: 0.7033 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 10/10\r\n1/1 [==============================] - 0s 30ms/step - loss: 0.6866 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\n\r\n<tensorflow.python.keras.callbacks.History at 0x7f71a41a2710>\r\n```\r\n\r\n**Other info / logs** \r\n[stackoverflow question](https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0/61707324#61707324)\r\n\r\n", "comments": ["It should probably be added that, as you point out in your Stack Overflow answer, if one uses stand-alone Keras instead of `tf.keras`, everything works out OK without any issue.", "Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/34a31272c7b16f6eece0d03d4b793474/39370.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/573c68e65ac61b7f32423df6b2fdd38b/39370-tf-nightly.ipynb). Please find the attached gist. Thanks!", "change \"validation_data=[X_val, y_val]\" to \"validation_data=(X_val, y_val)\", then you are all good", "@zabir-nabil \r\nI ran your code with above modifications (as per the stackoverflow shared by you), can you please verify and confirm it solves the issue,please find the [gist here.](https://colab.research.google.com/gist/Saduf2019/f8e44e68c322100950884043ed386a43/untitled280.ipynb)", "@Saduf2019 not sure I understand your rationale (or @cqlc94's above, for that matter); the fact that it does work with `(x, y)` instead of `[x, y]` was already pointed out in the linked SO answer. The issue here is not *how I can make it work?* (this is already answered), it is *it **should** also work with* `[x, y]`*(as it does in stand-alone Keras), but it does not*, i.e. it is a **bug**.\r\n", "@jvishnuvardhan not sure why the `stat:awaiting tensorflower` label was removed. Should it be?", "Looking at the documentation, the fact that lists are not supported is partially disclosed (https://github.com/keras-team/keras/blob/master/keras/engine/training.py):  \r\n```\r\nNote that `validation_data` does not support all the data types that are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\r\n```\r\n\r\nIMO, the problem comes with the fact that if you pass something different than a tuple, keras replace it with `None` __without any warning__ (https://github.com/keras-team/keras/blob/6a46d5259d079a58a9d32ad31a9e9da9c0ea563f/keras/engine/data_adapter.py#L1381):\r\n```python\r\nif not isinstance(data, tuple):\r\n    return (data, None, None)\r\n```\r\nThis returns the unpacked tuple, with `None`s for `y` and `sample_weight` __AS__ they are not provided (but they are provided in __data[1]__ for lists exactly as for tuples). Hence, users get __0s validation errors__ and this is disorienting. \r\n\r\nI would add at least a warning for the user to be aware of what's happening, i.e. \r\n ```python\r\nif not isinstance(data, tuple):\r\n    logging.warning(\"Validation data should be a tuple. We assume None for y and sample_weight. Change the type of validation_data to tuple to avoid this behaviour\".)\r\n    return (data, None, None)\r\n```\r\n\r\nAn extreme alternative, could be raising an error (but it requires to update the documentation as well), i.e.  \r\n```python\r\nif not isinstance(data, tuple):\r\n    raise ValueError(\r\n            \"Only tuple is supported as validation_data type.\")\r\n```", "@zabir-nabil \r\nCould you confirm if the issue still persist.Thanks", "@zabir-nabil Looks like this was resolved. I checked your code with `tf-nightly` and it throws an error when validation data is a list but works as expected when `valiadtion_data` is a tuple. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/774d24540ab2dbe07d7c47a9fc990795/stack-overflow-keras-zero-loss-issue.ipynb)\r\n\r\nwhen validation_data is a list, it throws \r\n\r\n`    ValueError: Layer model_2 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(1, 19) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(1, 1) dtype=float32>]`\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39370\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39370\">No</a>\n", "> change \"validation_data=[X_val, y_val]\" to \"validation_data=(X_val, y_val)\", then you are all good\r\n\r\nIt worked. Thanks"]}, {"number": 39369, "title": "Unnecessary tracing in tf.functions loaded from a saved model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.7.0-x86_64-i386-64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: Python 3.6.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nA custom keras model is saved using tf.saved_model.save and provided with a signature with None in the first dimension to allow for variable batch size. When the model is loaded back and used, it does tracing for each different batch size.\r\n\r\n**Describe the expected behavior**\r\n\r\nSince the model is saved with a signature, the expectation is that it should not perform tracing for different batch sizes.\r\n\r\nThe issue can be avoided by specifying the signature in the tf function instead of specifying it while saving the model, as shown in the linked gist.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe issue and the workaround can be seen in -\r\n\r\nhttps://gist.github.com/saswatac/40aea67d71bbac021f7280e3a0cca419\r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@saswatac \r\nI ran the code shared by you and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/dbe66f9401e01f4109fb6e9c50841bb7/untitled172.ipynb), if possible please share a colab gist for us to analyse the error", "@Saduf2019 I was able to reproduce the my issue in the gist you shared. The issue you faced is probably due to installing tf 2.1.0 , i think you need to restart the kernel after installing 2.1.0 version. Alternatively you can also reproduce the issue with version 2.2.0-rc4, the default installed in colab.", "@saswatac \r\nI ran the shared code again and do not face any error, it replicated the issue reported by you, please confirm the same as per the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/dfa6d64d7848cf8edc904a72639dd344/untitled176.ipynb)", "@Saduf2019 yes looks good.", "Thanks for reporting.  After looking into this, I don't believe there's a bug, though I do believe we could improve the documentation around signatures.  I have a PR out for clarifying this.\r\n\r\nThe function returned from `get_concrete_function` that you're passing to the `signatures` argument of `save` isn't doing anything special to the function itself; it's simply exposing that concrete function in the signatures dictionary at load time.\r\n\r\nFor example, after loading your model, since you didn't pick a specific name for the signature, it could be called via:\r\n\r\n```\r\nloaded.signatures['serving_default'](inputs=tf.ones((2, 2)))\r\n```\r\n\r\nThis isn't all that useful in TensorFlow 2 in Python, but it does allow you to access these functions in the graph-based v1 and C++.  There isn't any additional magic that modifies your original function when you set things up this way.  From the point of view of the original function, you've simply created another concrete function and can do whatever you wish with it.\r\n\r\nTo further illustrate the problem / interaction, consider this: when defining a function, we support a single optional signature.  Attempts to request a function that don't fit this signature will throw an error. Example:\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[None, 2], dtype=tf.float32])\r\ndef foo(arg1):\r\n  return tf.multiply(0.5, arg1)\r\n\r\nfoo.get_concrete_function(tf.ones((3, 3, 3)))\r\n```\r\n\r\nThis will throw an exception, since the concrete function you're requesting isn't valid with the input signature you provided.  If we were to attempt to retroactively enforce a signature at export time, we may find concrete functions whose inputs are incompatible with the signature.  Trying to work around this would probably only make an already-confusing interface even more so.\r\n\r\nMore broadly, it might be possible to add support for multiple input signatures, but this is not something we have on the roadmap for supporting at this time, and regardless, the behavior of automatically adding additional ones when a concrete function is requested would seemingly violate the purpose of creating a specific input signature in the first place.\r\n\r\nHope this makes sense.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39369\">No</a>\n"]}, {"number": 39368, "title": "Remove Python2 badge about raspberryPi in README.md", "body": "After TF 2.2.0, Python 2 is no longer supported.\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.2.0\r\n\r\nThose badges confuse to reader about broken build in PaspberryPi.", "comments": ["What happend into CI ?  \ud83e\udd14 "]}, {"number": 39367, "title": "ImportError: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model'", "body": "I tried reinstalling Tensorflow as pip install tensorflow and Keras too by pip install keras\r\nBut still i get the same ImportError\r\n\r\nPlease help me out, Thank you.", "comments": ["@sohamsahare123 \r\n\r\nCan you please provide the code before running into the problem.Also, include your operating system and Tensorflow version.Please, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)Thanks!", "i have installed  packages using \"conda install tensorflow\" but in spyder when i am trying to  imports that getting the import error \"export_saved_model\"", "please help me out!!! ASAP\r\n", "The same issue here! I get error both on Windows and on Ubuntu 20.04 . ", "I have the same issue in colab also", "guys, i have the same problem but finally, it got solved by removing and installing tensorflow and  \r\nthen restarting the kernel...\r\n!pip uninstall -y tensorflow\r\n!pip install tensorflow", "After reinstalling 4-5 time and restarting it worked.\r\nThank you all."]}, {"number": 39366, "title": "How to build TensorFlow Lite and use in Qt project ?", "body": "System information\r\n- Android 5.1:\r\n- Qt Creator 4.11.2:\r\n- Android NDK 20.1.5948944:\r\n- Android SDK 26.1.1:\r\n- Compiler Clang Qt 5.13.2 for Android ARMv7:\r\n- JVM java-8-oracle:\r\n\r\nI am trying to start project:\r\nhttps://mechatronicsblog.com/tensorflow-lite-integration-with-qt-and-v-play-for-multi-platform-machine-learning-apps-on-ios-and-android/\r\n\r\nBut there is an issue - Tensorflow submodule is incorrect.\r\n\r\nI downloaded Tensorflow library from main repo by the following command:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n```\r\n\r\nThen I updated dependencies:\r\n```\r\ntensorflow/lite/tools/make/download_dependencies.sh\r\n```\r\nThen I built the library by the following command:\r\n```\r\nbazel build --cxxopt='-D_GLIBCXX_USE_CXX11_ABI=0' -c opt --config=android_arm tensorflow/lite/java:libtensorflowlite_jni\r\n```\r\nHere's what WORKSPACE looks like:\r\n```\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 21,\r\n    build_tools_version = \"26.0.2\",\r\n    path = \"/home/user/Android/SDK\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name = \"androidndk\",\r\n    api_level = 20,\r\n    path = \"/home/user/android-ndk-r20b\",\r\n)\r\n```\r\nThe result was successful.\r\n\r\nThen I copied the library folder and pasted it in the root of the project folder. And edited .pro file to add libraries:\r\n```\r\n# TensorFlow Lite - Global\r\nTENSORFLOW_PATH = $$PWD/tensorflow/\r\nTFLITE_MAKE_PATH = $$TENSORFLOW_PATH/tensorflow/lite/tools/make\r\nINCLUDEPATH +=  $$TENSORFLOW_PATH \\\r\n                $$TFLITE_MAKE_PATH/downloads/ \\\r\n                $$TFLITE_MAKE_PATH/downloads/eigen \\\r\n                $$TFLITE_MAKE_PATH/downloads/gemmlowp \\\r\n                $$TFLITE_MAKE_PATH/downloads/neon_2_sse \\\r\n                $$TFLITE_MAKE_PATH/downloads/farmhash/src \\\r\n                $$TFLITE_MAKE_PATH/downloads/flatbuffers/include\r\n\r\n# TensorFlow Lite - Android - armv7a\r\nandroid {\r\n    QT += androidextras\r\n\r\n    LIBS += -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/c \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/core/api \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/kernels \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/kernels/internal \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/nnapi \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/androidndk \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/farmhash_archive \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/fft2d \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/flatbuffers \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/flatbuffers/src \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/ruy/ruy \\\r\n            -L$$TENSORFLOW_PATH/bazel-bin/external/ruy/ruy/profiler \\\r\n            -lallocation.pic -larena_planner.pic -larena_planner.pic -lminimal_logging.pic \\\r\n            -lsimple_memory_arena.pic -lstring_util.pic -lutil.pic \\\r\n            -lapi.pic -lbuiltin_op_kernels.pic -lbuiltin_ops.pic -lcpu_backend_context.pic -lcpu_backend_gemm.pic -leigen_support.pic \\\r\n            -lkernel_util.pic -llstm_eval.pic -laudio_utils.pic -lkernel_utils.pic -lneon_tensor_utils.pic \\\r\n            -lportable_tensor_utils.pic -ltensor_utils.pic -lquantization_util.pic -ltranspose_utils.pic \\\r\n            -lfarmhash.pic -lfft2d.pic -lflatbuffers.pic \\\r\n            -lallocator.pic -lapply_multiplier.pic -lblocking_counter.pic -lblock_map.pic -lcontext.pic -lcontext_get_ctx.pic \\\r\n            -lctx.pic -ldetect_arm.pic -ldetect_x86.pic -lhave_built_path_for_avx2.pic -lhave_built_path_for_avx512.pic \\\r\n            -lhave_built_path_for_avxvnni.pic -lhave_built_path_for_sse42.pic -lkernel_arm.pic -lkernel_avx2.pic \\\r\n            -lkernel_avx512.pic -lkernel_avxvnni.pic -lkernel_sse42.pic -lpack_arm.pic -lpack_avx2.pic -lpack_avx512.pic \\\r\n            -lpack_avxvnni.pic -lpack_sse42.pic -lprepacked_cache.pic -lthread_pool.pic -ltrace.pic -ltrmul.pic \\\r\n            -ltune.pic -lwait.pic -linstrumentation.pic -lnnapi_implementation.pic -lnnapi_util.pic\r\n}\r\n```\r\nThen I built it up.\r\nErrors:\r\n```\r\ntensorflow/lite/util.cc:47: error: undefined reference to 'TfLiteIntArrayCreate'\r\ntensorflow/lite/util.cc:47: error: undefined reference to 'TfLiteIntArrayCreate'\r\ntensorflow/lite/kernels/activations.cc:265: error: undefined reference to 'TfLiteIntArrayCopy'\r\ntensorflow/lite/kernels/activations.cc:291: error: undefined reference to 'TfLiteIntArrayCopy'\r\ntensorflow/lite/kernels/activations.cc:380: error: undefined reference to 'TfLiteIntArrayCopy'\r\ntensorflow/lite/kernels/activations.cc:612: error: undefined reference to 'TfLiteIntArrayCopy'\r\ntensorflow/lite/kernels/activations.cc:729: error: undefined reference to 'TfLiteTypeGetName'\r\ntensorflow/lite/kernels/activations.cc:757: error: undefined reference to 'TfLiteTypeGetName'\r\ntensorflow/lite/kernels/activations.cc:840: error: undefined reference to 'TfLiteTypeGetName'\r\ntensorflow/lite/kernels/activations.cc:1084: error: undefined reference to 'TfLiteTypeGetName'\r\ntensorflow/lite/kernels/arg_min_max.cc:40: error: undefined reference to 'TfLiteIntArrayCreate'\r\ntensorflow/lite/kernels/arg_min_max.cc:40: error: undefined reference to 'TfLiteIntArrayCreate'\r\ntensorflow/lite/kernels/basic_rnn.cc:104: error: undefined reference to 'TfLiteIntArrayFree'\r\ntensorflow/lite/kernels/basic_rnn.cc:110: error: undefined reference to 'TfLiteIntArrayEqual'\r\ntensorflow/lite/kernels/basic_rnn.cc:120: error: undefined reference to 'TfLiteIntArrayEqual'\r\ntensorflow/lite/kernels/basic_rnn.cc:133: error: undefined reference to 'TfLiteIntArrayEqualsArray'\r\ntensorflow/lite/kernels/basic_rnn.cc:144: error: undefined reference to 'TfLiteIntArrayEqualsArray'\r\ntensorflow/lite/kernels/basic_rnn.cc:157: error: undefined reference to 'TfLiteIntArrayEqualsArray'\r\ntensorflow/lite/kernels/basic_rnn.cc:168: error: undefined reference to 'TfLiteIntArrayEqualsArray'\r\ntensorflow/lite/kernels/batch_matmul.cc:108: error: undefined reference to 'TfLiteIntArrayFree'\r\ntensorflow/lite/kernels/bidirectional_sequence_lstm.cc:527: error: undefined reference to 'TfLiteIntArrayFree'\r\ntensorflow/lite/kernels/bidirectional_sequence_lstm.cc:527: error: undefined reference to 'TfLiteIntArrayFree'\r\ntensorflow/lite/kernels/bidirectional_sequence_lstm.cc:630: error: undefined reference to 'TfLiteIntArrayEqual'\r\ntensorflow/lite/kernels/bidirectional_sequence_lstm.cc:642: error: undefined reference to 'TfLiteIntArrayEqual'\r\n./tensorflow/lite/kernels/internal/reference/densify.h:36: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, TfLiteSparsity const&)'\r\n./tensorflow/lite/kernels/internal/reference/densify.h:38: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'\r\n./tensorflow/lite/kernels/internal/reference/densify.h:36: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<signed char>::FormatConverter(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, TfLiteSparsity const&)'\r\n./tensorflow/lite/kernels/internal/reference/densify.h:38: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<signed char>::SparseToDense(signed char const*)'\r\ntensorflow/lite/kernels/embedding_lookup_sparse.cc:178: error: undefined reference to 'TfLiteTensorRealloc'\r\ntensorflow/lite/kernels/expand_dims.cc:105: error: undefined reference to 'TfLiteTensorRealloc'\r\n./tensorflow/lite/kernels/internal/reference/sparse_ops/fully_connected.h:35: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, TfLiteSparsity const&)'\r\n./tensorflow/lite/kernels/internal/reference/sparse_ops/fully_connected.h:37: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'\r\ntensorflow/lite/kernels/if.cc:85: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'\r\ntensorflow/lite/kernels/if.cc:92: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'\r\ntensorflow/lite/kernels/if.cc:85: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'\r\ntensorflow/lite/kernels/if.cc:92: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'\r\ntensorflow/lite/kernels/if.cc:155: error: undefined reference to 'tflite::impl::Subgraph::Invoke()'\r\ntensorflow/lite/kernels/reshape.cc:156: error: undefined reference to 'TfLiteTensorRealloc'\r\ntensorflow/lite/kernels/while.cc:152: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'\r\ntensorflow/lite/kernels/while.cc:170: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'\r\ntensorflow/lite/kernels/while.cc:57: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'\r\ntensorflow/lite/kernels/while.cc:267: error: undefined reference to 'tflite::impl::Subgraph::Invoke()'\r\ntensorflow/lite/kernels/while.cc:292: error: undefined reference to 'tflite::impl::Subgraph::Invoke()'\r\ntensorflow/lite/kernels/while.cc:57: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'\r\ntensorflow/lite/kernels/register.cc:34: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:35: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:36: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'\r\ntensorflow/lite/kernels/register.cc:38: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:39: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'\r\ntensorflow/lite/kernels/register.cc:41: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'\r\ntensorflow/lite/kernels/register.cc:43: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'\r\ntensorflow/lite/kernels/register.cc:52: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:291: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:294: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:295: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'\r\ntensorflow/lite/kernels/register.cc:297: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/unordered_map:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n./tensorflow/lite/kernels/register.h:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n/home/sergey/FelgoProjects/TensorFlowLiteQtVPlay/tensorflow//bazel-bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):register.cc:vtable for tflite::ops::builtin::BuiltinOpResolver: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'\r\n/home/sergey/FelgoProjects/TensorFlowLiteQtVPlay/tensorflow//bazel-bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):register.cc:vtable for tflite::ops::builtin::BuiltinOpResolver: error: undefined reference to 'tflite::MutableOpResolver::FindOp(char const*, int) const'\r\n/home/sergey/FelgoProjects/TensorFlowLiteQtVPlay/tensorflow//bazel-bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):register.cc:typeinfo for tflite::ops::builtin::BuiltinOpResolver: error: undefined reference to 'typeinfo for tflite::MutableOpResolver'\r\n../../Felgo/Felgo/android_armv7/include/QtQml/qqml.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'\r\n../../Felgo/Felgo/android_armv7/include/QtQml/qqml.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'\r\n../TensorFlowLiteQtVPlay/auxutils.h:0: error: undefined reference to 'vtable for AuxUtils'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../../Felgo/Felgo/android_armv7/include/QtCore/qmetatype.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'\r\n../../Felgo/Felgo/android_armv7/include/QtCore/qmetatype.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.h:0: error: undefined reference to 'vtable for ObjectsRecogFilter'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowthread.h:0: error: undefined reference to 'vtable for TensorflowThread'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowthread.h:0: error: undefined reference to 'vtable for WorkerTF'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflow/tensorflow/lite/mutable_op_resolver.h:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n/home/sergey/android-ndk-r20b/sources/cxx-stl/llvm-libc++/include/memory:2339: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'\r\n/home/sergey/android-ndk-r20b/sources/cxx-stl/llvm-libc++/include/memory:2339: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'\r\nmain.o:main.cpp:vtable for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'ObjectsRecogFilter::metaObject() const'\r\nmain.o:main.cpp:vtable for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'ObjectsRecogFilter::qt_metacast(char const*)'\r\nmain.o:main.cpp:vtable for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'ObjectsRecogFilter::qt_metacall(QMetaObject::Call, int, void**)'\r\nmain.o:main.cpp:typeinfo for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'typeinfo for ObjectsRecogFilter'\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:16: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:0: error: undefined reference to 'vtable for ObjectsRecogFilter'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:161: error: undefined reference to 'ObjectsRecogFilter::runTensorFlow(QImage)'\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:300: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:248: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'\r\n../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:276: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'\r\n../TensorFlowLiteQtVPlay/tensorflowthread.cpp:16: error: undefined reference to 'WorkerTF::results(int, QStringList, QList<double>, QList<QRectF>, double)'\r\n../TensorFlowLiteQtVPlay/tensorflowthread.cpp:17: error: undefined reference to 'WorkerTF::finished()'\r\n../TensorFlowLiteQtVPlay/tensorflowthread.cpp:0: error: undefined reference to 'vtable for TensorflowThread'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowthread.cpp:47: error: undefined reference to 'TensorflowThread::results(int, QStringList, QList<double>, QList<QRectF>, double)'\r\n../TensorFlowLiteQtVPlay/tensorflowthread.h:0: error: undefined reference to 'vtable for WorkerTF'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:109: error: undefined reference to 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:118: error: undefined reference to 'tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:121: error: undefined reference to 'tflite::impl::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::impl::Interpreter, std::__ndk1::default_delete<tflite::impl::Interpreter> >*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:128: error: undefined reference to 'tflite::impl::Interpreter::UseNNAPI(bool)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:131: error: undefined reference to 'tflite::impl::Interpreter::SetNumThreads(int)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:136: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:166: error: undefined reference to 'tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:166: error: undefined reference to 'tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:272: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'\r\n../TensorFlowLiteQtVPlay/tensorflow/tensorflow/lite/stderr_reporter.h:0: error: undefined reference to 'vtable for tflite::StderrReporter'\r\n/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::DefaultErrorReporter()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::impl::Interpreter::Interpreter(tflite::ErrorReporter*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:321: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:324: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:327: error: undefined reference to 'tflite::impl::Interpreter::SetInputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:328: error: undefined reference to 'tflite::impl::Interpreter::SetOutputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:337: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:340: error: undefined reference to 'tflite::impl::Interpreter::AddNodeWithParameters(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, char const*, unsigned int, void*, TfLiteRegistration const*, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:341: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:354: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::DefaultErrorReporter()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::impl::Interpreter::Interpreter(tflite::ErrorReporter*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:321: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:324: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:327: error: undefined reference to 'tflite::impl::Interpreter::SetInputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:328: error: undefined reference to 'tflite::impl::Interpreter::SetOutputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:337: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:340: error: undefined reference to 'tflite::impl::Interpreter::AddNodeWithParameters(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, char const*, unsigned int, void*, TfLiteRegistration const*, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:341: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:354: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'\r\n../TensorFlowLiteQtVPlay/tensorflow/tensorflow/lite/interpreter.h:178: error: undefined reference to 'tflite::impl::Interpreter::SetTensorParametersReadWrite(int, TfLiteType, char const*, unsigned int, int const*, TfLiteQuantizationParams, bool, unsigned int, int const*)'\r\ntensorflow/lite/string_util.cc:108: error: undefined reference to 'TfLiteTensorReset'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [libTensorFlowLiteQtVPlay.so] Error 1\r\n```\r\n", "comments": ["After you building tensorflow/lite/java:libtensorflowlite_jni, you'll get bazel-bin/tensorflow/lite/java/libtensorflowlite_jni.so share library.\r\nThat's the only library you need to link.\r\nYou don't need to link following libraries separately.\r\n\r\n```\r\n-lallocation.pic -larena_planner.pic -larena_planner.pic -lminimal_logging.pic \\\r\n            -lsimple_memory_arena.pic -lstring_util.pic -lutil.pic \\\r\n            -lapi.pic -lbuiltin_op_kernels.pic -lbuiltin_ops.pic -lcpu_backend_context.pic -lcpu_backend_gemm.pic -leigen_support.pic \\\r\n            -lkernel_util.pic -llstm_eval.pic -laudio_utils.pic -lkernel_utils.pic -lneon_tensor_utils.pic \\\r\n            -lportable_tensor_utils.pic -ltensor_utils.pic -lquantization_util.pic -ltranspose_utils.pic \\\r\n            -lfarmhash.pic -lfft2d.pic -lflatbuffers.pic \\\r\n            -lallocator.pic -lapply_multiplier.pic -lblocking_counter.pic -lblock_map.pic -lcontext.pic -lcontext_get_ctx.pic \\\r\n            -lctx.pic -ldetect_arm.pic -ldetect_x86.pic -lhave_built_path_for_avx2.pic -lhave_built_path_for_avx512.pic \\\r\n            -lhave_built_path_for_avxvnni.pic -lhave_built_path_for_sse42.pic -lkernel_arm.pic -lkernel_avx2.pic \\\r\n            -lkernel_avx512.pic -lkernel_avxvnni.pic -lkernel_sse42.pic -lpack_arm.pic -lpack_avx2.pic -lpack_avx512.pic \\\r\n            -lpack_avxvnni.pic -lpack_sse42.pic -lprepacked_cache.pic -lthread_pool.pic -ltrace.pic -ltrmul.pic \\\r\n            -ltune.pic -lwait.pic -linstrumentation.pic -lnnapi_implementation.pic -lnnapi_util.pic\r\n```", "@terryheo, Hi,\r\nIt didn\u2019t work\r\n\r\n```\r\n# TensorFlow Lite - Global\r\nTENSORFLOW_PATH = $$PWD/tensorflow/\r\nINCLUDEPATH += $$TENSORFLOW_PATH/lite\r\n\r\n# TensorFlow Lite - Android - armv7a\r\nandroid {\r\n    QT += androidextras\r\n    LIBS += -L$$TENSORFLOW_PATH/lib \\\r\n                   -L${ANDROID_NDK_ROOT}/sources/android/cpufeatures/obj/local/$$ANDROID_TARGET_ARCH \\\r\n                      -ltensorflowlite_jni -lcpufeatures\r\n}\r\n```\r\n\r\nErrors:\r\n```\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'\r\n../TensorFlowLiteQtVPlay/tensorflow/lite/mutable_op_resolver.h:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'\r\n/home/sergey/android-ndk-r19c/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:14: error: undefined reference to 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:109: error: undefined reference to 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:118: error: undefined reference to 'tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:121: error: undefined reference to 'tflite::impl::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::impl::Interpreter, std::__ndk1::default_delete<tflite::impl::Interpreter> >*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:128: error: undefined reference to 'tflite::impl::Interpreter::UseNNAPI(bool)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:131: error: undefined reference to 'tflite::impl::Interpreter::SetNumThreads(int)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:136: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:166: error: undefined reference to 'tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:166: error: undefined reference to 'tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:272: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'\r\n../TensorFlowLiteQtVPlay/tensorflow/lite/stderr_reporter.h:0: error: undefined reference to 'vtable for tflite::StderrReporter'\r\n/home/sergey/android-ndk-r19c/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::DefaultErrorReporter()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::impl::Interpreter::Interpreter(tflite::ErrorReporter*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:321: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:324: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:327: error: undefined reference to 'tflite::impl::Interpreter::SetInputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:328: error: undefined reference to 'tflite::impl::Interpreter::SetOutputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:336: error: undefined reference to 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:337: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:340: error: undefined reference to 'tflite::impl::Interpreter::AddNodeWithParameters(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, char const*, unsigned int, void*, TfLiteRegistration const*, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:341: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:354: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'\r\n/home/sergey/android-ndk-r19c/sources/cxx-stl/llvm-libc++/include/memory:2325: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::DefaultErrorReporter()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::impl::Interpreter::Interpreter(tflite::ErrorReporter*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:321: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:324: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:327: error: undefined reference to 'tflite::impl::Interpreter::SetInputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:328: error: undefined reference to 'tflite::impl::Interpreter::SetOutputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:336: error: undefined reference to 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:337: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:340: error: undefined reference to 'tflite::impl::Interpreter::AddNodeWithParameters(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, char const*, unsigned int, void*, TfLiteRegistration const*, int*)'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:341: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'\r\n../TensorFlowLiteQtVPlay/tensorflowlite.cpp:354: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'\r\n../TensorFlowLiteQtVPlay/tensorflow/lite/interpreter.h:178: error: undefined reference to 'tflite::impl::Interpreter::SetTensorParametersReadWrite(int, TfLiteType, char const*, unsigned int, int const*, TfLiteQuantizationParams, bool, unsigned int, int const*)'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [libTensorFlowLiteQtVPlay.so] Error 1\r\n```", "If you're using C++ API, could you try use tensorflow/lite:tensorflowlite instead of tensorflow/lite/java:libtensorflowlite_jni ?\r\nAlso \"-lcpufeatures\" isn't necessary.\r\n\r\nFYI, this is simple e2e flow to use tensorflowlite with minimal.cc\r\n```\r\n$ bazel build tensorflow/lite:tensorflowlite\r\n$ g++ tensorflow/lite/examples/minimal/minimal.cc -I . -I bazel-tensorflow/external/com_google_absl/ -I bazel-tensorflow/external/flatbuffers/include/ -Lbazel-bin/tensorflow/lite/ -ltensorflowlite \r\n$ LD_LIBRARY_PATH=$LD_LIBRARY_PATH:bazel-bin/tensorflow/lite/ ./a.out\r\n```", "@terryheo Hi,\r\nI built up the project. The ```.pro``` file is bellow:\r\n```\r\n# TensorFlow Lite - Global\r\nTENSORFLOW_PATH = $$PWD/tensorflow/\r\nTFLITE_MAKE_PATH = $$TENSORFLOW_PATH/tensorflow/lite/tools/make\r\nINCLUDEPATH += $$TENSORFLOW_PATH \\\r\n$$TENSORFLOW_PATH/lite \\\r\n$$TENSORFLOW_PATH/external \\\r\n$$TFLITE_MAKE_PATH/downloads \\\r\n$$TFLITE_MAKE_PATH/downloads/eigen \\\r\n$$TFLITE_MAKE_PATH/downloads/gemmlowp \\\r\n$$TFLITE_MAKE_PATH/downloads/neon_2_sse \\\r\n$$TFLITE_MAKE_PATH/downloads/farmhash/src \\\r\n$$TFLITE_MAKE_PATH/downloads/farmhash/src \\\r\n$$TFLITE_MAKE_PATH/downloads/flatbuffers/include \\\r\n$$TENSORFLOW_PATH/bazel-tensorflow/external \\\r\n$$TENSORFLOW_PATH/bazel-tensorflow/external/eigen_archive \\\r\n$$TENSORFLOW_PATH/bazel-tensorflow/external/protobuf_archive/src\r\n\r\n# TensorFlow Lite - Android - armv7a\r\nandroid {\r\nQT += androidextras\r\nLIBS += -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/c \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/core/api \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/kernels \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/kernels/internal \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/nnapi \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/delegates/nnapi \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/experimental/ruy \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/experimental/resource_variable \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/external/farmhash_archive \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/external/fft2d \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/external/flatbuffers \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/external/flatbuffers/src \\\r\n-L$$TENSORFLOW_PATH/bazel-bin/external/androidndk/ndk/sources/android/cpufeatures/obj/local/armeabi-v7a \\\r\n-lframework.pic -larena_planner.pic -lallocation.pic -lexternal_cpu_backend_context.pic -lminimal_logging.pic \\\r\n-lsimple_memory_arena.pic -lstring_util.pic -lutil.pic -lc_api_internal.pic -lapi.pic -lnnapi_implementation.pic \\\r\n-lbuiltin_op_kernels.pic -lbuiltin_ops.pic -lcpu_backend_context.pic -lcpu_backend_gemm.pic \\\r\n-leigen_support.pic -lkernel_util.pic -llstm_eval.pic -laudio_utils.pic -lkernel_utils.pic -lneon_tensor_utils.pic \\\r\n-lportable_tensor_utils.pic -lquantization_util.pic -ltensor_utils.pic \\\r\n-lfarmhash.pic -lfft2d.pic -lflatbuffers.pic -lflatbuffers.pic -lcpufeatures -lnnapi_delegate.pic \\\r\n-lallocator.pic -lcontext.pic -lresource_variable.pic -lthread_pool.pic -lblocking_counter.pic -lwait.pic\r\n}\r\n```\r\nAlso I added flags:\r\n```\r\nandroid {\r\nANDROID_PACKAGE_SOURCE_DIR = $$PWD/android\r\nOTHER_FILES += android/AndroidManifest.xml \\\r\nandroid/build.gradle\r\nQMAKE_LFLAGS += -Wl,--allow-multiple-definition -Wl,--whole-archive, -nostdlib++\r\n}\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39366\">No</a>\n"]}]