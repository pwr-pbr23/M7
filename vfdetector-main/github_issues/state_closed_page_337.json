[{"number": 44033, "title": "triggered tf.function retracing warning", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **python3 pip**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **python 3.6**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWARNING:tensorflow: triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, \r\n(2) passing tensors with different shapes, \r\n(3) passing Python objects instead of tensors. \r\nFor (1), please define your @tf.function outside of the loop. \r\nFor (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. \r\nFor (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n\r\nI add experimental_relax_shapes=True option in my code but not work.\r\n\r\n**Describe the expected behavior**\r\nI would like not to have retracing if there is no need.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n#### example code\r\n```\r\nimport tensorflow as tf\r\nfrom random import randint\r\nfrom tensorflow.keras.layers import Conv1D\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv1D(8, 3))\r\nmodel.build([None, 12, 1])\r\n\r\npredict_tensors = [\r\n    tf.random.normal([randint(1, 8), randint(4, 40), 1])\r\n    for _ in range(10)\r\n]\r\nfor t in predict_tensors:\r\n    print(model.predict(t))\r\n```", "comments": ["@LzyRapx,\r\nOn running the code with TF v2.3, I did not face any warnings. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2cee6a35a6845ab9170d9b7c2dce5975/44033.ipynb).\r\n\r\nCould you please check if you are facing the same issue in a virtual environment? \r\n\r\nAlternatively, you can also try suppressing the warnings by running the below code before importing TensorFlow.  \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44031, "title": "[INTEL_MKL] Fix TF_DISABLE_MKL not work in remapper.cc", "body": "This PR fix bug that `TF_DISABLE_MKL=1` not work in remapper.cc. \r\n\r\nUsers can use such env variable to disable MKL-implement op, even when they are using tensorflow with MKL support.\r\n\r\nThis PR only affects tensorflow built with --config=mkl.", "comments": ["@ezhulenev I saw there is one test failed. feedback/copybara Google internal checks FAILED. Is it related to this PR? Thanks!", "Penporn took care of errors, looks like everything is fine on our side and this PR sticked.", "@ezhulenev \r\nThat's good. Thanks for your reply."]}, {"number": 44030, "title": "Multi-gpus call failed by  tf.distribute.MirroredStrategy()", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):pip install tf-nightly-gpu\r\n- TensorFlow version (use command below):2.4.0-dev20201014\r\n- Python version:3.7.8\r\n- CUDA/cuDNN version:CUDA11.1+CUDNN8.0.4.30\r\n- Multi GPUs: RTX3080*2\r\n\r\n**Describe the current behavior**\r\nCode\r\n```sh\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nnum_epochs = 5\r\nbatch_size_per_replica = 64\r\nlearning_rate = 0.001\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint('Number of devices: %d' % strategy.num_replicas_in_sync)  \r\nbatch_size = batch_size_per_replica * strategy.num_replicas_in_sync\r\n\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\ndataset = tfds.load(\"cats_vs_dogs\", split=tfds.Split.TRAIN, as_supervised=True)\r\ndataset = dataset.map(resize).shuffle(1024).batch(batch_size)\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.applications.MobileNetV2(weights=None, classes=2)\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n    )\r\n\r\nmodel.fit(dataset, epochs=num_epochs)\r\n```\r\n<details>\r\n<summary>The error log is as follows</summary>\r\n<pre><code>\r\nEpoch 1/5\r\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 158 all-reduces with algorithm = nccl, num_packs = 1\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-5-ca3f8dc7254f> in <module>\r\n----> 1 model.fit(dataset, epochs=num_epochs)\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1083                 _r=1):\r\n   1084               callbacks.on_train_batch_begin(step)\r\n-> 1085               tmp_logs = self.train_function(iterator)\r\n   1086               if data_handler.should_sync:\r\n   1087                 context.async_wait()\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    886         # Lifting succeeded, so variables are initialized and we can run the\r\n    887         # stateless function.\r\n--> 888         return self._stateless_fn(*args, **kwds)\r\n    889     else:\r\n    890       _, _, _, filtered_flat_args = \\\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2948        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n   2949     return graph_function._call_flat(\r\n-> 2950         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2951 \r\n   2952   @property\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1926       # No tape is watching; skip to running the function.\r\n   1927       return self._build_call_outputs(self._inference_function.call(\r\n-> 1928           ctx, args, cancellation_manager=cancellation_manager))\r\n   1929     forward_backward = self._select_forward_and_backward_functions(\r\n   1930         args,\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    559               inputs=args,\r\n    560               attrs=attrs,\r\n--> 561               ctx=ctx)\r\n    562         else:\r\n    563           outputs = execute.execute_with_cancellation(\r\n\r\n~/anaconda3/envs/sshtf24/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nNotFoundError: 3 root error(s) found.\r\n  (0) Not found:  No algorithm worked!\r\n\t [[node mobilenetv2_1.00_224/Conv1/Conv2D (defined at /home/librazxc/anaconda3/envs/sshtf24/lib/python3.7/threading.py:926) ]]\r\n\t [[div_no_nan_1/ReadVariableOp_1/_264]]\r\n  (1) Not found:  No algorithm worked!\r\n\t [[node mobilenetv2_1.00_224/Conv1/Conv2D (defined at /home/librazxc/anaconda3/envs/sshtf24/lib/python3.7/threading.py:926) ]]\r\n  (2) Not found:  No algorithm worked!\r\n\t [[node mobilenetv2_1.00_224/Conv1/Conv2D (defined at /home/librazxc/anaconda3/envs/sshtf24/lib/python3.7/threading.py:926) ]]\r\n\t [[Adam/Adam/group_deps/NoOp/_307]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_44264]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node mobilenetv2_1.00_224/Conv1/Conv2D:\r\n cond_2/Identity (defined at <ipython-input-5-ca3f8dc7254f>:1)\r\n\r\nInput Source operations connected to node mobilenetv2_1.00_224/Conv1/Conv2D:\r\n cond_2/Identity (defined at <ipython-input-5-ca3f8dc7254f>:1)\r\n\r\nInput Source operations connected to node mobilenetv2_1.00_224/Conv1/Conv2D:\r\n cond_2/Identity (defined at <ipython-input-5-ca3f8dc7254f>:1)\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n</code></pre>\r\n</details>\r\n\r\nCan you give me some suggestions on how to use multiple gpus in tf2.4?", "comments": ["Hi @PureHing, I am unable to reproduce this error in Colab. Please [see the gist here](https://colab.research.google.com/gist/nikitamaia/3e602bdc300f4526e83ea03f0ac8c26a/untitled9.ipynb).\r\nThe Colab GPU runtime only has a single GPU, but I was also able to run this code without error with two P100s on GCP.\r\n\r\nI wonder if this is an issue with your GPUs. I did a search for this error message and it seems it might have to do with incompatible versions of CUDA/cuDNN/TF. Please check and see if you are using a tested [build configuration.](https://www.tensorflow.org/install/source#linux)", "@nikitamaia Hi,Because I am using the RTX30 series of graphics cards, I cannot call gpu after installing the existing release version of tensorflow ([my issue](https://github.com/tensorflow/tensorflow/issues/43701#issuecomment-706089283)). Therefore, I installed the tf-nightly-gpu version, the above problem occurred when calling multiple gpus. I also tried to use it by building tensorflow 2.4.0 from source code, the same error occurs, and there is no problem in using a single gpu.\r\n\r\nYou are using tf2.3 in the [colab](https://colab.research.google.com/gist/nikitamaia/3e602bdc300f4526e83ea03f0ac8c26a/untitled9.ipynb#scrollTo=EdfLrmkgKVCc), tf2.3 does not support cuda11.1 which is necessary for rtx3080,Can you test in the tf-nightly-gpu version?", "Need to add the following code:\r\n```\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor physical_device in physical_devices:\r\n    tf.config.experimental.set_memory_growth(physical_device, True)\r\n```\r\n\r\n@nikitamaia why you needn't do this with two P100s on GCP\uff1f", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44030\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44030\">No</a>\n"]}, {"number": 44029, "title": "[Intel MKL] Disable  _MklFusedBatchNorm op when input is 5D tensor", "body": "This PR disables the use of _MklFusedBatchNorm op when input is 5D tensor as it is currently not supported.", "comments": []}, {"number": 44028, "title": "Unable to build TensorFlow targeting Raspberry Pi", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: v2.3.0\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am following the instructions on https://www.tensorflow.org/install/source_rpi to build a `.whl` package that I can install on a Raspberry Pi 3B, but the build process is failing. I can clone TensorFlow, check out a revision, and begin the build in the Docker container without issue, but the build fails with the following output at the end:\r\n\r\n```\r\nRemoving intermediate container 8f1ac04cb8a3\r\n ---> 9c723ba9afae\r\nStep 14/14 : COPY install/.bazelrc /etc/bazel.bazelrc\r\n ---> 8dba22ce896f\r\nSuccessfully built 8dba22ce896f\r\nSuccessfully tagged tf_ci.pi-python37:latest\r\nRunning 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nsudo is already the newest version (1.8.16-0ubuntu1.9).\r\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\r\naddgroup: Please enter a username matching the regular expression configured\r\nvia the NAME_REGEX[_SYSTEM] configuration variable.  Use the `--force-badname'\r\noption to relax this check or reconfigure NAME_REGEX.\r\n```\r\n\r\nIf I instead try to build `v2.3.1`, I get the following output:\r\n```\r\nStep 15/15 : ENV TF_ENABLE_XLA=0\r\n ---> Using cache\r\n ---> e7647e308801\r\nSuccessfully built e7647e308801\r\nSuccessfully tagged tf_ci.pi-python37:latest\r\nRunning 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh' inside tf_ci.pi-python37...\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nsudo is already the newest version (1.8.16-0ubuntu1.9).\r\n0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\r\naddgroup: To avoid problems, the username should consist only of\r\nletters, digits, underscores, periods, at signs and dashes, and not start with\r\na dash (as defined by IEEE Std 1003.1-2001). For compatibility with Samba\r\nmachine accounts $ is also supported at the end of the username\r\n```\r\n\r\n\r\nThis happens well before the 30 minutes the documentation suggests the process should take, and there is no `.whl` in the `tensorflow/output-artifacts` directory.\r\n\r\nI am experiencing this issue when trying to compile v2.3.0, but I have encountered when trying to build v1.15.0 as well. I would expect the use of containers and Bazel to make the build reproducible, but it would seem that something has changed that broke the build process. Please let me know if there are any additional logs I could provide to help diagnose this.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n> Note: you must have docker installed for the following steps\r\n\r\n```shell\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ cd tensorflow\r\n$ git checkout v2.3.0\r\n$ tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```\r\n\r\nAt this point the build will fail, with no output `.whl` generated.\r\n", "comments": ["This seems to be related to the [`with_the_same_user` script](https://github.com/tensorflow/tensorflow/blob/9d558ddba177859ebd7e4018a405f68dcc5e1836/tensorflow/tools/ci_build/builds/with_the_same_user) and the environment variable [`CI_BUILD_USER_FORCE_BADNAME`](https://github.com/tensorflow/tensorflow/blob/054738d5340879f638c0261fe998d864d0e20a48/tensorflow/tools/ci_build/ci_build.sh#L155), but these don't appear to be well documented, and it's not clear to an end user that a workaround might exist. \r\n\r\n", "@TylerADavis, Could you please let us know if this issue still persists? If not, does [this](https://www.tensorflow.org/lite/guide/build_arm), [this](https://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_raspberry_pi_zero_armv6) helps you? Thanks!", "I tried following my previous steps on the 2.6 release, and got a new error message. I'll try the other two approaches you recommended next.\r\n\r\n\r\nError message after using build script to spin up container and compile:\r\n```\r\nERROR: An error occurred during the fetch of repository 'llvm-project':\r\n   Traceback (most recent call last):\r\n\tFile \"/workspace/third_party/repo.bzl\", line 53, column 33, in _tf_http_archive_impl\r\n\t\tctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error extracting /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-project/temp704287839711893859/f7b1fa6f5ebec5780e626aa48d582f2519a01632.tar.gz to /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-project/temp704287839711893859: /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-project/clang/test/CodeGenObjC/arc-captured-block-var-inlined-layout.m (Input/output error)\r\nERROR: /workspace/tensorflow/compiler/mlir/BUILD:132:11: //tensorflow/compiler/mlir:mlir_graph_optimization_pass depends on @llvm-project//llvm:Support in repository @llvm-project which failed to fetch. no such package '@llvm-project//llvm': java.io.IOException: Error extracting /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-project/temp704287839711893859/f7b1fa6f5ebec5780e626aa48d582f2519a01632.tar.gz to /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-project/temp704287839711893859: /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-project/clang/test/CodeGenObjC/arc-captured-block-var-inlined-layout.m (Input/output error)\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 435.160s\r\n```", "I tried using `build.sh` when on master, and it failed there a well with the same error message. It might just be that `build.sh` is pulling artifacts that no longer exist for some reason.\r\n\r\nAs for the other two links you provided, those describe building TensorFlow lite, while I had been interested in building full (non-lite) TensorFlow for aarch64. Based on the updated information in the [building for raspberry pi documentation](https://github.com/tensorflow/build/tree/master/raspberry_pi_builds), perhaps this issue should be moved under `Tensorflow/build`.\r\n\r\n```\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://github.com/llvm/llvm-project/archive/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz failed: class javax.net.ssl.SSLException Read timed out\r\nERROR: An error occurred during the fetch of repository 'llvm-raw':\r\n   Traceback (most recent call last):\r\n\tFile \"/workspace/third_party/repo.bzl\", line 53, column 33, in _tf_http_archive_impl\r\n\t\tctx.download_and_extract(\r\nError in download_and_extract: java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz, https://github.com/llvm/llvm-project/archive/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz] to /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-raw/temp8753830120791134403/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz: Read timed out\r\nERROR: no such package '@llvm-raw//utils/bazel': java.io.IOException: Error downloading [https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz, https://github.com/llvm/llvm-project/archive/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz] to /Users/tylerdavis/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_tylerdavis/eab0d61a99b6696edb3d2aff87b585e8/external/llvm-raw/temp8753830120791134403/57e00810edd7c4359e3de9242bcbd8874d58dc64.tar.gz: Read timed out\r\nINFO: Elapsed time: 83.000s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\n", "Hi @TylerADavis ! Could you try with instructions in this [repo](https://github.com/PINTO0309/Tensorflow-bin)? Thanks!", "I don't have an rpi to test on at the moment, but it does look like that repo would allow for one to get a prebuilt wheel of full (non-lite) tensorflow for an aarch64 linux system.\r\n\r\nhttps://github.com/PINTO0309/Tensorflow-bin#usage\r\n", "Ok @TylerADavis ! Is this issue good to close then?", "Sounds good to me. Thank you for referring me to that repo @mohantym !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44028\">No</a>\n"]}, {"number": 44027, "title": "Change description of drop_remainder argument of Dataset method window", "body": "In general, several windows are dropped when drop_remainder is set to True.", "comments": []}, {"number": 44026, "title": "Everything is as expected when batch size <= 64 but having nan loss after few epochs if batch size > 64", "body": "<em>When I was training my image dataset with InceptionResNetV2 everything works fine if the batch size is less than or equal to 64, however, if I set the batch size larger than 64 (For example, 128), after a few epochs the loss becomes nan</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux-4.15.0-112-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- TensorFlow installed from (source or binary): NGC Container\r\n- TensorFlow version (use command below): 2.2.0+nv\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 11.0  \r\n- GPU model and memory: Tesla V100 32510MiB\r\n\r\n**Code**\r\n##\r\n## Check multiple GPUs and model selection\r\n##\r\n    my_strategy = tf.distribute.MirroredStrategy()\r\n    with my_strategy.scope():\r\n        if args.model == 'usDL_InceptionResNetV2':\r\n            print(\"train usDL_InceptionResNetV2...!\")\r\n            logging.info(\"train usDL_InceptionResNetV2 with {} GPUs...\".format(args.gpus))\r\n            model = models.usDL_InceptionResNetV2(args)\r\n\r\n        elif args.model == 'usDL_InceptionV3':\r\n            print(\"train usDL_InceptionV3...!\")\r\n            logging.info(\"train usDL_InceptionV3 with {} GPUs...\".format(args.gpus))\r\n            model = models.models.usDL_InceptionV3(args)\r\n\r\n        else:\r\n            print(\"No model is available!\")\r\n            sys.exit()\r\n\r\n            # Replicates the model on multiple GPUs.\r\n            #model = multi_gpu_model(model, gpus=args.gpus)\r\n\r\n        ##\r\n        ## compile the model\r\n        ##\r\n\r\n        if (args.model == \"usDL_InceptionResNetV2\"):\r\n\r\n            # 2020-05-01\r\n            logging.info(\"loss: categorical_crossentropy, optimizer: Adam()\")\r\n            model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n\r\n            # model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['acc'])\r\n\r\n        else:\r\n            logging.info(\"optimizer: Adam, loss: ssim_loss\")\r\n            model.compile(optimizer=Adam(), loss=ssim_loss, metrics=[ssim_loss, 'accuracy'])\r\n\r\n        ##\r\n        ## use call back functions\r\n        ##\r\n\r\n        ModelCheckpointFileName = os.path.join(model_save_dir, 'model_{epoch:02d}.h5')\r\n        ckpt = ModelCheckpoint(ModelCheckpointFileName, monitor='val_loss', verbose=0, period=args.save_every)\r\n        csvlogFileName = os.path.join(model_save_dir, 'log.csv')\r\n        csv_logger = CSVLogger(csvlogFileName, append=True, separator=',')\r\n        lr_sched = step_decay_schedule(initial_lr=args.lr)\r\n\r\n    ##\r\n    ## train\r\n    ##\r\n    ## The \"steps_per_epoch\" is not provoded here since the \"__len__()\" function\r\n    ## has been provoded in the \"My_Custom_Generator\"\r\n    ##\r\n\r\n    print(\"train: start...\")\r\n\r\n    if (args.model == \"usDL_InceptionResNetV2\"):\r\n        history = model.fit_generator(generator=train_generator,\r\n                                      # steps_per_epoch=len(data)//args.batch_size,\r\n                                      epochs=args.epoch,\r\n                                      verbose=1,\r\n                                      callbacks=[ckpt, csv_logger, lr_sched])", "comments": ["@Tengger Please give us the entire code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44025, "title": "Change description of drop_remainder argument of Dataset method window", "body": "In general, several windows are dropped when drop_remainder is set to True.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44025) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 44024, "title": "Validation accuracy stuck at 0.6 in my CNN model", "body": "I've been creating a CNN to map facial emotions to certain emojis, but after certain epochs I cant seem to get the validation accuracy past the 0.61 mark, while the training accuracy keeps increasing smoothly as required. I can recognize this as an overfitting problem since the validation loss also dips and then keeps increasing, but I cant seem to find methods like regularization or dropouts showing any change in the validation accuracy. So I'm wondering if something is wrong with my model itself, like it being having too many parameters or such? Here is the model:\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(Conv2D(32, kernel_size = (3,3), input_shape = (48,48,1)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(64, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(128, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(128, kernel_size=(3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1024))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(7))\r\nmodel.add(Activation('softmax'))\r\n\r\nData consists of 48x48 pixel images which has 28701 training instances and 7178 validation instances ", "comments": ["Can you please provide a google colab gist along with the dataset link?", "@requiem-of-souls \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using?\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram I'm using TF 2.3.0 in Colab. here is the gist of the model: \r\n\r\nhttps://gist.github.com/requiem-of-souls/ecdccf36c8cace79bc83e32fd3dbddd8#file-trainer-ipynb \r\n\r\n@PrattJena the dataset is the FER2013 dataset from Kaggle:\r\nhttps://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data", "@requiem-of-souls \r\n\r\nI am not able to see see the gist from the link you have shared. Please, help me with the reproducible code. Thanks!", "@ravikyram \r\n\r\nsorry for that, here's the colab file itself https://colab.research.google.com/drive/1dcTbpQZSzl8e9VGAsMnSZYpAo6wqnkMQ?usp=sharing\r\n", "@requiem-of-souls \r\n\r\nI tried in colab and i am facing the error (`Found 0 images belonging to 0 classes.`). Can you please provide the data under train and test folder.Please, find the [gist](https://colab.research.google.com/gist/ravikyram/c07a09789197150e65e708627996222b/untitled456.ipynb) for your reference.Thanks!\r\n", "@ravikyram try importing this dataset, this includes the actual images rather than csv files:\r\n\r\nhttps://www.kaggle.com/msambare/fer2013\r\n", "I havent tried running the code fully yet but my views after seeing your code are you are applying too much of dropout. Everytime you apply dropout it loses us some data. Sure it makes training faster amd prevents overfitting. But you have applied dropouts in 4 places which i think is resulting in underfitting. One usually applies dropout only at the very end of 0.5. And if you actually go to count them at the end you are effectively only training on 6000 images which is way less than even the test data. So i guess the reason for low accuracy is  lot of dropout. I will test my theory and get back to you. \n"]}, {"number": 44023, "title": "In TFnighty TFLite  Interpreter missing setUseNNAPI \"wrapper\" method  to call Interpreter.Options.setUseNNAPI", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android emulator\r\n- TensorFlow installed from (source or binary):  org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n- TensorFlow version (use command below): nightly\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nMaking call to Interpreter's setUseNNAPI method which no longer exists in the tfnightly version as of Oct. 12.   The method it should call Interpreter.Options.setUseNNAPI still exists but, not in the Interperter itself.\r\n\r\n**Describe the expected behavior**\r\nEither change the documentation to reflect this method does not exist or fix this.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\ntfLite = new Interpreter(loadModelFile(assetManager, modelFilename));\r\n\r\nif (tfLite != null) tfLite.setUseNNAPI(isChecked);\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["`Interpreter.Options` is a class inside `Interpreter`. It was implemented back in 2018. To use it, you have to create an instance of `Interpreter.Options`. That is, for your code, it should be something like:\r\n```\r\nInterpreter.Options tfliteOptions = new Interpreter.Options();\r\ntfliteOptions.setUseNNAPI(isChecked);\r\ntfLite = new Interpreter(loadModelFile(assetManager, modelFilename), tfliteOptions);\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L82-L183", "I already know that Intrepreter.Options has this method and that is a work around to set the options in the constructer.  My issue report was that prior Oct. 12 (or there abouts) like the setNumThreads it was implemented in BOTH the Interpreter class and the Interpreter.Options.  This allowed the modification of the Interpreter object after construction.  Also, note: this is performed by the sample in the new TF2 Object Detection API mobile app as they have a GUI element that can toggle ON/OFF the use of the NNAPI for acceleration and I believe others will complain.    ", "I understand now from the documentation you linked to it is removing of a deprecated method however, given this I suggest that the TF2 Object Detection sample provided be reworked and the option removed or altered to recreate with GUI event each time the Interpreter.   Otherwise, that team will get reports on broken sample code.", "@grewe Got your point. I don't work for Google, just happened to use `Interpreter.Options` before. Tag Jared @jdduke who removed  `Interpreter.setUseNNAPI` method in https://github.com/tensorflow/tensorflow/commit/89c2f7f062ee125bd3783035b1be07b5c0f9ea73 according to git log.", "The object detection sample has already been updated to use the `Interpreter.Options` variant. It may not be as convenient, but since 2019, using `Interpreter.setUseNNAPI` had a few nasty corner cases (e.g., you couldn't disable it once it has been enabled and used, and the first inference result is almost always incorrect).\r\n\r\nWe'll also likely be removing the `Interpreter.setNumThreads()` setter as well. It's note quite as flawed, but it does have some unfortunate corner cases as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44023\">No</a>\n", "If we don't use Interpreter.setNumThreads() as deprecated already then Interpreter.Options automatically collects data ? or new alternative method arrived."]}, {"number": 44022, "title": "[MLIR] Fix verifier for TF::Conv2DOp and TF::Conv3DOp.", "body": "This commit addresses a bug where in tf.conv2D and tf.conv3D, incorrect sizes of output shape was not verified. Added changes to verify the output shape and emit error for incorrect output shapes.\r\n\r\nSigned-off-by: Prateek Gupta <prateek@polymagelabs.com>", "comments": ["@smit-hinsu for visibility - as the author of the surrounding code.", "Thanks for the improvements!\r\n\r\nCould you make use the new inferReturnTypes trait for shape inference?\r\n\r\nSee this commit for an example. https://github.com/tensorflow/tensorflow/commit/edeb46930e7e6cef0154ef7fa8014bb42d4052c2", "> Thanks for the improvements!\r\n> \r\n> Could you make use the new inferReturnTypes trait for shape inference?\r\n> \r\n> See this commit for an example. [edeb469](https://github.com/tensorflow/tensorflow/commit/edeb46930e7e6cef0154ef7fa8014bb42d4052c2)\r\n\r\nIt isn't immediately clear from the other revision as to how inferReturnTypes should be used/incorporated. Could you please expand a bit more on what changes/additions should be made here? Thanks", "Once we declare InferTypeOpInterface interface and implement inferReturnTypes methods for these ops, we don't need to explicitly call shape verification. You should see that shapes are being verified automatically without explicit call to inferReturnTypes. The benefit of this generic approach is that the interface can be shared between buillder, verifier and allows other uses as all the ops follow the same mechanism.\r\n\r\nCheckout interfaces documentation at https://mlir.llvm.org/docs/Interfaces/.\r\n\r\nLet me know if you need further explanation or specific details.", "> Once we declare InferTypeOpInterface interface and implement inferReturnTypes methods for these ops, we don't need to explicitly call shape verification. You should see that shapes are being verified automatically without explicit call to inferReturnTypes. The benefit of this generic approach is that the interface can be shared between buillder, verifier and allows other uses as all the ops follow the same mechanism.\r\n> \r\n> Checkout interfaces documentation at https://mlir.llvm.org/docs/Interfaces/.\r\n> \r\n> Let me know if you need further explanation or specific details.\r\n\r\n@smit-hinsu , there is an issue in using the ``InferTypeOpInterface``. Previously,  when I was using the verifier to check the output type all other parameters like non-negative strides, a correct number of input dimensions/strides etc were checked before only(as I kept my check at the end of the verifier).\r\nBut now if I use the ``inferReturnTypes`` method,  the output type is inferred before doing other sanity checks resulting in a segmentation fault. The output type in the case of ``tf.conv2D`` and ``tf.conv3D`` can only be inferred if the attributes are correct.\r\nWhen I run my code using ``inferReturnTypes`` with correct attributes, it runs fine. But if there are incorrect attributes they have to be checked first\r\nIf I emit errors in those instances, then there will be no use of a verifier. So could you suggest something here?", "We need to emit error in that case and fail the type inference. See the following op inference in HLO dialect for an example, \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/hlo_ops.cc#L1687", "@pr4tgpt  Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "> @pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!\r\n\r\nYes I am working on smit's suggestion. Thanks.", "> We need to emit error in that case and fail the type inference. See the following op inference in HLO dialect for an example,\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/hlo_ops.cc#L1687\r\n\r\nHi @smit-hinsu I have made changes as requested by you. Please review them. Thanks.", "Thank you @smit-hinsu for suggestions. Will update the code with relevant changes.", "@pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "> @pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!\r\n\r\nI have checked and resolved the comments. Thanks.", "Hi @smit-hinsu , I have made changes as requested by you. Please review the changes and feel free to post suggestions/improvements. Thanks!", "@pr4tgpt  Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "> @pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!\r\n\r\nI have checked and resolved the comments. Thanks", "Hi @smit-hinsu , I have made changes as requested by you. Please review the changes and feel free to post suggestions/improvements. Thanks!", "> While updating the PR, consider adding new commits instead of amending it so that it is easier to see the changes.\r\n\r\nHi @smit-hinsu I have made changes as suggested by you in a new commit. Please review it. Thanks!", "Hi, @smit-hinsu Some of the newer test cases were using wrong output sizes for ``tf.conv2d``. I have corrected them. Please review that. Thanks!", "Hi, @smit-hinsu some more failing test cases corrected. Please review them also. Thanks!", "Hi, the code has been approved. Please merge this after running checks thanks.", "Hi @smit-hinsu, Can you look into why the internal build is failing here?  I cannot access the build report. Thanks!", "Hi @smit-hinsu, can you look into what error has occurred? Thanks!", "Could you run all the tests in these directories and fix the failing ones?\r\n\r\ntensorflow/compiler/mlir\r\ntensorflow/compiler/tests\r\ntensorflow/lite/python\r\ntensorflow/lite/testing/model_coverage\r\n", "@pr4tgpt Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "> Could you run all the tests in these directories and fix the failing ones?\r\n> \r\n> tensorflow/compiler/mlir\r\n> tensorflow/compiler/tests\r\n> tensorflow/lite/python\r\n> tensorflow/lite/testing/model_coverage\r\n\r\nYes I am looking into this. Thanks!", "> Could you run all the tests in these directories and fix the failing ones?\r\n> \r\n> tensorflow/compiler/mlir\r\n> tensorflow/compiler/tests\r\n> tensorflow/lite/python\r\n> tensorflow/lite/testing/model_coverage\r\n\r\nHi @smit-hinsu I checked the tests you asked. \r\n\r\nThe tests in ``tensorflow/compiler/mlir`` specific to ``tf_to_gpu`` and ``tf_to_kernel`` pass fail. These tests have no dependency to my patch.\r\nThe ``python`` tests in the ``tensorflow/compiler/tests`` are failing due to reasons I can't determine(mostly ``ModuleNameError``). I find no dependency to my patch here also.\r\nThe ``python`` tests in the ``tensorflow/lite/python`` are failing due to reasons I can't determine(mostly ``ImportError``). I find no dependency to my patch here also.\r\nThe tests in ``tensorflow/lite/testing/model_coverage`` pass successfully.\r\n\r\nCould you see the failing tests once and tell me what could be the possible reason for them to fail? Thanks!", "> > Could you run all the tests in these directories and fix the failing ones?\r\n> > tensorflow/compiler/mlir\r\n> > tensorflow/compiler/tests\r\n> > tensorflow/lite/python\r\n> > tensorflow/lite/testing/model_coverage\r\n> \r\n> Hi @smit-hinsu I checked the tests you asked.\r\n> \r\n> The tests in `tensorflow/compiler/mlir` specific to `tf_to_gpu` and `tf_to_kernel` pass fail. These tests have no dependency to my patch.\r\n> The `python` tests in the `tensorflow/compiler/tests` are failing due to reasons I can't determine(mostly `ModuleNameError`). I find no dependency to my patch here also.\r\n> The `python` tests in the `tensorflow/lite/python` are failing due to reasons I can't determine(mostly `ImportError`). I find no dependency to my patch here also.\r\n> The tests in `tensorflow/lite/testing/model_coverage` pass successfully.\r\n> \r\n> Could you see the failing tests once and tell me what could be the possible reason for them to fail? Thanks!\r\n\r\nHi @smit-hinsu could you help with this? Thanks!", "All these tests depend on your change. tensorflow/compiler/mlir/tensorflow/tests uses tf-opt tool which links everything andd tensorflow/lite/python links the TF dialect so there are dependencies.\r\n \r\nThere are many tests failing. Following are some of these:\r\n\r\ntensorflow/compiler/mlir/tensorflow/tests:tf-ops.mlir.test\r\ntensorflow/compiler/mlir/lite/tests:prepare-tf.mlir.test\r\ntensorflow/compiler/tests:conv3d_mlir_bridge_test_cpu\r\ntensorflow/lite/python:lite_test\r\ntensorflow/lite/testing/model_coverage:model_coverage_test_user_vgg_tflite_builtins_select_tf_ops\r\n\r\nPlease take a look. You can use 'tensorflow/compiler/mlir/tensorflow/tests:all' to test all tests in the directory.", "These failures are all clearly related to this PR. An example from `tensorflow/compiler/mlir/tensorflow/tests/mlir/tensorflow/tests/tf_optimize.mlir.test`:\r\n\r\n```\r\n2020-12-10 21:49:06.560035: F ./tensorflow/core/util/tensor_format.h:266] Check failed: format == FORMAT_OIHW_VECT_I (0 vs. 3)\r\nTensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\r\nStack dump:\r\n0.      Program arguments: /home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir -tf-optimize \r\n1.      Program arguments: /home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt /home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir -tf-optimize \r\n #0 0x00000000122c096f llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0x122c096f)\r\n #1 0x00000000122be5ed llvm::sys::RunSignalHandlers() (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0x122be5ed)\r\n #2 0x00000000122bee6d SignalHandler(int) (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0x122bee6d)\r\n #3 0x00007fe1f5214dd0 __restore_rt (/lib64/libpthread.so.0+0x12dd0)\r\n #4 0x00007fe1f4c5f70f raise (/lib64/libc.so.6+0x3770f)\r\n #5 0x00007fe1f4c49b25 abort (/lib64/libc.so.6+0x21b25)\r\n #6 0x00000000111d2b10 virtual thunk to tensorflow::internal::LogMessageFatal::~LogMessageFatal() (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-o\r\n #7 0x000000000fe767ec mlir::LogicalResult mlir::TF::inferConvReturnTypes<mlir::TF::Conv2DOpAdaptor, (void*)0>(mlir::TF::Conv2DOpAdaptor, llvm::SmallVectorImpl<mlir::Type>&, llvm::Optional<mlir::Location>, llvm::ArrayRef<mlir::Attribute>) (.cold.3446) (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0xfe767ec)\r\n #8 0x000000000fe77224 mlir::TF::Conv2DOp::inferReturnTypes(mlir::MLIRContext*, llvm::Optional<mlir::Location>, mlir::ValueRange, mlir::DictionaryAttr, mlir::RegionRange, llvm::SmallVectorImpl<mlir::Type>&) (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0xfe77224)\r\n #9 0x0000000011eeb238 mlir::detail::verifyInferredResultTypes(mlir::Operation*) (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0x11eeb238)\r\n#10 0x000000000fc884d9 mlir::Op<mlir::TF::Conv2DOp, mlir::OpTrait::ZeroRegion, mlir::OpTrait::OneResult, mlir::OpTrait::ZeroSuccessor, mlir::OpTrait::NOperands<2u>::Impl, mlir::InferTypeOpInterface::Trait, mlir::MemoryEffectOpInterface::Trait, mlir::TF::LayoutSensitiveInterface::Trait, mlir::DerivedAttributeOpInterface::Trait>::verifyInvariants(mlir::Operation*) (/home/uday/.cache/bazel/_bazel_uday/97da8b8d2f64c0a54677e92074ce1d42/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_optimize.mlir.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tf-opt+0xfc884d9)\r\n\r\n\r\n```", "> All these tests depend on your change. tensorflow/compiler/mlir/tensorflow/tests uses tf-opt tool which links everything andd tensorflow/lite/python links the TF dialect so there are dependencies.\r\n> \r\n> There are many tests failing. Following are some of these:\r\n> \r\n> tensorflow/compiler/mlir/tensorflow/tests:tf-ops.mlir.test\r\n> tensorflow/compiler/mlir/lite/tests:prepare-tf.mlir.test\r\n> tensorflow/compiler/tests:conv3d_mlir_bridge_test_cpu\r\n> tensorflow/lite/python:lite_test\r\n> tensorflow/lite/testing/model_coverage:model_coverage_test_user_vgg_tflite_builtins_select_tf_ops\r\n> \r\n> Please take a look. You can use 'tensorflow/compiler/mlir/tensorflow/tests:all' to test all tests in the directory.\r\n\r\nHi @smit-hinsu, @bondhugula  I have fixed the code. The filter dimensions were fetched incorrectly. The ``tensorflow/compiler/mlir/tensorflow/tests`` are now passing. \r\nLooking at other failing tests also. Thanks! Please review.", "> > All these tests depend on your change. tensorflow/compiler/mlir/tensorflow/tests uses tf-opt tool which links everything andd tensorflow/lite/python links the TF dialect so there are dependencies.\r\n> > There are many tests failing. Following are some of these:\r\n> > tensorflow/compiler/mlir/tensorflow/tests:tf-ops.mlir.test\r\n> > tensorflow/compiler/mlir/lite/tests:prepare-tf.mlir.test\r\n> > tensorflow/compiler/tests:conv3d_mlir_bridge_test_cpu\r\n> > tensorflow/lite/python:lite_test\r\n> > tensorflow/lite/testing/model_coverage:model_coverage_test_user_vgg_tflite_builtins_select_tf_ops\r\n> > Please take a look. You can use 'tensorflow/compiler/mlir/tensorflow/tests:all' to test all tests in the directory.\r\n> \r\n> Hi @smit-hinsu, @bondhugula I have fixed the code. The filter dimensions were fetched incorrectly. The `tensorflow/compiler/mlir/tensorflow/tests` are now passing.\r\n> Looking at other failing tests also. Thanks! Please review.\r\n\r\nTests in ``mlir/lite/tests`` are also passing.\r\n", "It finally got submitted! Thanks Prateek for the contribution and working through all the issues.", "> It finally got submitted! Thanks Prateek for the contribution and working through all the issues.\r\n\r\nThank you Smit for your valuable help and suggestions. "]}, {"number": 44021, "title": "Different output for custom tf.keras.Model within tf.function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Colab Pre-Installed\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: \r\n- GPU model and memory: \r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to obtain reproducible results between graph and eager execution. While doing that I observed that calling my tf.keras.Model within a tf.function wouldn't exactly give me the same results as in eager execution. I was wondering if that's expected ? If yes, what's the reason for it ?\r\n\r\n**Describe the expected behavior**\r\nI would expect to get exactly the same value.\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is a Colab : https://colab.research.google.com/drive/1n4p-Iq_g7dT-Vv5cb-s6nyeqjuYjyyGL?usp=sharing\r\n\r\n**Other info / logs** \r\n", "comments": ["I ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/77a8239694426f5c721cf17da53f4727/untitled435.ipynb).", "Hey,\r\nDo you have any update on the matter @ymodak ? \r\n\r\nThanks in advance!", "When I set `training=True` the issue goes away. When `training=False`about 90% of the values are the same between graph mode and eager mode.\r\n```\r\nnp.isclose(encoder(inputs, False).numpy(), graph_call(inputs, False).numpy()).astype(np.int).mean()\r\n=> 0.903798828125\r\n```\r\nI wouldn't be surprised if this is the expected behavior.", "Hi there,\r\n\r\nThe output in eager and graph appears to be the same -- the issue is with the code you use to check the difference. Run the following to demonstrate that the outputs are identical:\r\n\r\n```\r\na = encoder(inputs, False)\r\nb = graph_call(inputs, False)\r\nprint(a)\r\nprint(b)\r\nprint(tf.reduce_max(a - b))  # prints 0\r\n```\r\n\r\n> When I set training=True the issue goes away\r\n\r\nYou are using `SyncBatchNormalization` in your model, which would introduce potential differences in training mode (hence triggering your check).\r\n", "> Hi there,\r\n> \r\n> The output in eager and graph appears to be the same -- the issue is with the code you use to check the difference. Run the following to demonstrate that the outputs are identical:\r\n> \r\n> ```\r\n> a = encoder(inputs, False)\r\n> b = graph_call(inputs, False)\r\n> print(a)\r\n> print(b)\r\n> print(tf.reduce_max(a - b))  # prints 0\r\n> ```\r\n> \r\n> > When I set training=True the issue goes away\r\n> \r\n> You are using `SyncBatchNormalization` in your model, which would introduce potential differences in training mode (hence triggering your check).\r\n\r\nHi,\r\nThanks for the answer! I'm sorry I had changed the colab without noticing while doing more tests. My issue was when `training=True`. \r\nYou are right, this difference only occurs when using `tf.keras.layers.experimental.SyncBatchNormalization`. \r\n\r\nDo you have a idea why it is the case ? Especially here, we aren't using any distribution strategy so the layer should behave as a `tf.keras.layers.BatchNormalization` no ? The tensorflow documentation says \"Without tf.distribute strategy, this layer behaves as a regular tf.keras.layers.BatchNormalization layer.\"\r\n\r\nI modified the colab to highlight what I'm stating here.", "@hugoych Difference in the results from eager and graph mode is very very low. I tried with the following tolerance levels and are close upto 6th decimal. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/64f870eb338876c070b621fcaed8c978/issue_tf_function.ipynb). \r\n\r\n```\r\ntf.experimental.numpy.allclose(\r\n    eager_true_s, graph_true_s, rtol=1e-06, atol=1e-06)\r\n\r\n# ndarray<<tf.Tensor: shape=(), dtype=bool, numpy=True>>\r\n```\r\n\r\nPlease close the issue if this was resolved for you. thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44021\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44021\">No</a>\n"]}, {"number": 44020, "title": "Problem to import keras from tensorflow 2.3.1", "body": "I succesfully upgraded my python files to tensorflow 2. with tf_upgrade_2.0\r\n**System information**\r\n- I have written custom code \r\n- OS Platform : windows 10\r\n- TensorFlow : 2.3.1\r\n- TensorFlow-gpu 2.3.1\r\n- Keras version : 2.3.1\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0.130 / 7.6.5\r\n- GPU model and memory: NVIDIA 1060 6GB\r\n\r\nWhen I start importing various libriaries, the python file bugs rapidly. I used to run the following imports :\r\n```\r\nimport os\r\nimport warnings\r\nimport numpy as np\r\nimport numpy_financial as npf\r\nimport pandas as pd\r\nimport sqlite3 as sq\r\nimport time\r\nfrom itertools import chain\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import Dropout, Activation, Dense, LSTM, Flatten\r\nfrom tensorflow.keras.layers import Concatenate\r\nfrom tensorflow.keras.layers import CuDNNLSTM, CuDNNGRU\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom sklearn.metrics import confusion_matrix\r\n\r\nfrom util_data import *\r\nfrom util_prepa import *\r\nfrom util_model import *\r\nfrom util_invest import *\r\nfrom util_RESNET import *\r\n```\r\n\r\nI automaticaly get an error message  : \r\n`ModuleNotFoundError: No module named 'tensorflow.python.keras.activations'`\r\npreceeded with \r\n```\r\nFile \"D:\\GD Navagne\\AI\\tf2\\ResNet1D3D_list.py\", line 27, in <module>\r\n    from tensorflow import keras\r\n\r\n  File \"D:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\keras\\__init__.py\", line 14, in <module>\r\n    from . import activations\r\n\r\n  File \"D:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\keras\\activations\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.keras.activations import deserialize\r\n```\r\nI don't know what to do... Thanks in advance for the help.", "comments": ["Please check to see if u have latest tensorflow version running by writing` tf.__version__`. Also there was a small error in code and I corrected it and ran the following code and found no errors. You can find the google colab gist [here](https://colab.research.google.com/drive/1g4cDJzvdHFbQGOF07NpfuDW0h8jO3qd0?usp=sharing). If you are still facing problems please provide a screenshot of the error along with the full code. A stackoverflow question to your query can also be found [here](https://stackoverflow.com/questions/41415629/importerror-no-module-named-tensorflow-python). See if doing the following helps", "I have tried in colab with TF version 2.3 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/18da0d26cc065f374bf073c6470c2dfc/untitled466.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44020\">No</a>\n"]}, {"number": 44019, "title": "Update README.md", "body": "Add practical tutorial on Tensorflow 2.2 on GCP by Google Codelabs", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44019) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 44018, "title": "I've created a DCGAN model and when i run that in tensorflow ver 1.x and 2.x, it gives different result.What is the the reason for this?Thanks", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@khoanguyenvietmanh,\r\nIn order to expedite the trouble-shooting process, could you please fill in the issue template and also provide the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44017, "title": "[PluggableDevice] Kernel C API enhancement for retrieving attributes", "body": "Adding more attribution C API for pluggable device kernels implementation.", "comments": ["@annarev @penpornk @yisitu  We have added some kernel C API for retrieving more op attributions. Can you help to review it? thanks. ", "Looks good to me. I added API review label so that API owners can take a look as well", "@jzhoulon  Can you please resolve conflicts? Thanks!\r\n", "> @jzhoulon Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned I have resolved conflicts."]}, {"number": 44015, "title": "Add correct scatter file values for STM32F4 target", "body": "The emulated target platform has more SRAM and flash than what is currently stated in the scatter file.\r\n\r\nFixes #43743 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain this should make it possible to run:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=stm32f4 TAGS=cmsis-nn person_detection_benchmark`"]}, {"number": 44014, "title": "Update download_and_extract script for faster CMSIS patching", "body": "This is a short term solution to make the CMSIS patching execute faster. Instead of doing a 'find' for each header file, one find is executed per folder DSP/ and NN/ and sed is expanded to replace multiple include paths on those two find's. \r\n\r\nPartly fixes #44013", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@freddan80 your changes work for me under MSYS64/Win10. It looks like the cmsis_compiler.h is missing in the list of files. Please add\r\n`  corefiles=\"cmsis_compiler.h\"`\r\n`  find tensorflow/lite/micro/tools/make/downloads/cmsis \\`\r\n`    \\( -name *.c -or -name *.h -or -name *.cpp \\) -exec \\`\r\n`    sed -i \"s@#include \\\"\\($corefiles\\)\\\"@#include \\\"cmsis/CMSIS/Core/Include/\\1\\\"@g\" {} \\;`", "@ml-0 thanks for the heads up! I will add that. What make command reproduces this issue? I know these files are added as a consequence of #43726. But I can add them (cmsis_compiler.h and arm_helium_utils.h) in this PR as well and we avoid a merge conflict.\r\n", "I added the CMSIS include paths to tensorflow/lite/micro/tools/make/ext_libs/cmsis.inc\r\nin the context of #41860, assuming they are necessary. Later, I realized that I was using an incompletely patched CMSIS due to #44001. So, after your changes to download_and_extract.sh, the CMSIS include path is not necessary in cmsis.inc (and this is what I understood from @advaitjain and @mansnils). \r\n\r\nIf you comment out lines 122...129 in cmsis.inc and then issue the command\r\n`make -j -f tensorflow/lite/micro/tools/make/Makefile TAGS=\"cmsis-nn\" TARGET=cortex_m_gcc_generic CORTEX_M_CORE=M4F microlite`\r\nyou will get\r\n`tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/arm_math_types.h:76:10: fatal error: cmsis_compiler.h: No such file or directory`\r\nbecause\r\n`#include \"cmsis_compiler.h\"`\r\nis not patched.\r\n", "Got it, me and @mansnils synced up. I'll upload a new patch soon.", "@ml-0 let me know that if works for you as well", "Tried again: works perfectly on MSYS64/Win10. Thx.", "This PR is failing on the Arduino builds with the following error:\r\n```\r\nIn file included from /root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/arm_math.h:199:0,\r\n                 from /root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:33,\r\n                 from /root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:164,\r\n                 from /root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/micro/kernels/cmsis-nn/add.cpp:18:\r\n/root/Arduino/libraries/tensorflow_lite/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/arm_math_types.h:76:10: fatal error: cmsis/CMSIS/Core/Include/cmsis_compiler.h: No such file or directory\r\n #include \"cmsis/CMSIS/Core/Include/cmsis_compiler.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\nIt is reproducible locally with (note that this will affect your local Arduino install):\r\n```\r\ntensorflow/lite/micro/tools/ci_build/test_arduino.sh\r\n```\r\n\r\nSInce the Arduino project generation is hairy, I have manually pushed a fix with https://github.com/tensorflow/tensorflow/pull/44014/commits/e38f03bd810d501d56aac69b9027222051834392"]}, {"number": 44013, "title": "Patching CMSIS source file takes a lot of time", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 261bc3aba4e5c1611a417cf9d916c916996afad2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Any \r\n\r\n**Describe the problem**\r\nThe download and extract script patches CMSIS source files to use qualified paths in order to be compatible with Arduino IDE build system. Currently it takes a significant amount of time, since there are many include files that need to be patched.\r\n\r\nIdeally, there would be no need for patching CMSIS at all, but that would require to change the entire CMSIS repo to using qualified paths. Another solution could be to do the patching only when generating Arduino projects. However, that would most likely require us to patch the TFLu optimized op code (kernels/cmsis-nn/*) instead. \r\n\r\nA short term solution is to optimize the patching algorithm.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nAny command running using TAGS=\"cmsis-nn\". For example:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=sparkfun_edge person_detection_int8_bin`\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44013\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44013\">No</a>\n"]}, {"number": 44012, "title": "CUDA issues with 'tf-2.4.0-dev20201012'", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-2.3.0` and then `pip install --quiet --upgrade tf-nightly` in\r\na `virtualenv`\r\n\r\n- TensorFlow version: `2.4.0-dev20201012`\r\n- Python version: `3.7.5`\r\n- Installed using virtualenv? pip? conda?: `virtualenv` and `pip`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: `Cuda compilation tools, release 10.1, V10.1.243` and \r\n```\r\n$apt list --installed | grep -i 'libcudnn'\r\nlibcudnn7/unknown,now 7.6.5.32-1+cuda10.1 amd64 [installed,upgradable to: 7.6.5.32-1+cuda10.2]\r\nlibcudnn7-dev/unknown,now 7.6.5.32-1+cuda10.1 amd64 [installed,upgradable to: 7.6.5.32-1+cuda10.2]\r\nlibcudnn7-doc/now 7.6.5.32-1+cuda10.1 amd64 [installed,local]\r\n```\r\n- GPU model and memory: RTX 2080 Ti (12GB) and RTX 2080 SUPER (8 GB)\r\n\r\n\r\n\r\n\r\n**I am trying to test the new tensorflow NumPy API so I installed `tf-2.3.0` in a `virtualenv` and then pip upgraded it to nightly. Within `tf-2.3.0` I didn't have any issues with **\r\n```\r\nassert tf.test.is_gpu_available()\r\nassert tf.test.is_built_with_cuda()\r\ntf.config.list_physical_devices('GPU')\r\n```\r\n\r\n** When I upgraded the tensorflow to nightly I get the following errors **\r\n```\r\n(tf2.3_env) ruthvik@ruthvik:~$ pythonPython 3.7.5 (default, Nov \u00a07 2019, 10:50:52) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-10-13 06:51:35.053420: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:51:35.053436: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nINFO:tensorflow:Using local port 19355\r\nINFO:tensorflow:Using local port 18622\r\nINFO:tensorflow:Using local port 24757\r\nINFO:tensorflow:Using local port 15110\r\nINFO:tensorflow:Using local port 23483\r\nINFO:tensorflow:Using local port 24017\r\nINFO:tensorflow:Using local port 15691\r\nINFO:tensorflow:Using local port 19679\r\nINFO:tensorflow:Using local port 16397\r\nINFO:tensorflow:Using local port 15952\r\n>>> tf.__version__'2.4.0-dev20201012'\r\n>>> assert tf.test.is_gpu_available()\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-10-13 06:52:14.678269: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: \u00a0AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-13 06:52:14.680128: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-13 06:52:14.683285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-10-13 06:52:14.995295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-13 06:52:14.995671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-10-13 06:52:14.995713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-13 06:52:14.996014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 462.00GiB/s\r\n2020-10-13 06:52:14.996088: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:14.996156: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:14.997073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-13 06:52:14.997223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-13 06:52:14.998150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-13 06:52:14.998217: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:14.998270: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:14.998278: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-13 06:52:14.998291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-13 06:52:14.998296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] \u00a0 \u00a0 \u00a00 1 \r\n2020-10-13 06:52:14.998301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: \u00a0 N N \r\n2020-10-13 06:52:14.998304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1: \u00a0 N N \r\nTraceback (most recent call last):\r\n\u00a0 File \"<stdin>\", line 1, in <module>\r\nAssertionError\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-10-13 06:52:57.975881: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-13 06:52:57.976201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-13 06:52:57.978121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-10-13 06:52:57.978335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-13 06:52:57.979850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 462.00GiB/s\r\n2020-10-13 06:52:57.980178: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:57.980437: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:57.980506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-13 06:52:57.980552: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-13 06:52:57.980591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-13 06:52:57.980798: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:57.981034: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/magma/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/opt/ros/melodic/lib\r\n2020-10-13 06:52:57.981075: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\nFrom what it looks like it's looking for `*.so*` files that are both `*.so.10*` and `*.so.11*` and the cuDNN file `libcudnn.so.8` isn't that not supposed to happen if say I have just CUDA-10.1? Does this mean that I have to have multiple CUDA versions in my `$PATH` ? Why is it looking for multiple versions of libraries? In `tf-2.3.0` I noticed that it looks for all files that are only `*.so.10*`  and `libcudnn.so.7`", "comments": ["@ruthvik92,\r\nEvery TensorFlow release is compatible with certain CUDA/cuDNN version. \r\n\r\nTensorFlow v2.3 requires CUDA 10.1 and cuDNN 7.6. Whereas, for TensorFlow 2.4 you'll need CUDA 11 with cuDNN 8.\r\n\r\nFor more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu). Thanks!", "Yes, I am aware of that. However, requirements for tensorflow-2.4.0 are not mentioned in the suggested link. I will appreciate it if you can update the page.   ", "@ruthvik92 The build/install documentation for TF 2.4 will be posted on the website as soon as TF 2.4 stable version is launched officially.\r\nWe generally do not provide build installation docs for TF nightly versions since its unstable.\r\nThank you."]}, {"number": 44011, "title": "ops defined inside tf.while_loop's cond/body or tf.cond's true_fn/false_fn functions ignore their enclosed tf.device if the tf.while_loop/tf.cond itself is inside a tf.device", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: Python 3.7.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.print` doesn't print on specified device (using `tf.device`) if it is inside a for loop in graph mode.\r\n\r\nInitially I though it's an autograph issue. But if you trace the graph and inspect it in tensorboard (color by device), the graph does have that PrintV2 placed where it's supposed to.\r\n\r\nNow, I'm not sure why it's happening.\r\nI'm yet to verify if this happens across all the ops, or just the `tf.print`.\r\n\r\nWaiting for the team to let me know what to check next.\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.print` should print on specified device (using `tf.device`). ~Everything works fine if I'm using tf.while_loop instead.\r\nCan be seen from sample code.~\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**THIS SNIPPET IS NOT CORRECT ANYMORE, PLEASE READ A COUPLE OF COMMENTS BELOW.**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# assume there's a tf.distribute.Server running on rpi.local:2222\r\ntf.config.experimental_connect_to_cluster(\r\n    tf.train.ClusterSpec({'worker': ['dhruvins-macbook-air.local:2222', 'rpi.local:2222']}),\r\n    job_name='worker',\r\n    task_index=0\r\n)\r\n\r\nlaptop = '/job:worker/task:0'\r\nrpi = '/job:worker/task:1'\r\n\r\n\r\n@tf.function\r\ndef test_for():\r\n    with tf.device(rpi):\r\n        for i in tf.range(3):\r\n            tf.print('rpi', 'for', i)\r\n            with tf.device(laptop):\r\n                tf.print('laptop', 'for', i)\r\n\r\n\r\n@tf.function\r\ndef test_while():\r\n    def cond(i):\r\n        return i < 3\r\n\r\n    def body(i):\r\n        with tf.device(rpi):\r\n            tf.print('rpi', 'while', i)\r\n            with tf.device(laptop):\r\n                tf.print('laptop', 'while', i)\r\n        return [i + 1]\r\n\r\n    tf.while_loop(cond, body, [0], parallel_iterations=1)\r\n\r\n\r\ntest_for()\r\ntest_while()\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jvishnuvardhan \r\nI ran this code on nightly, the code keep running and does not produce any output, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/cdad7be80aca26f332c5e366cfbced34/untitled445.ipynb).", "@Saduf2019 One will need to modify their colab code a bit to point to existing (running) `tf.distribute.Server`(s).\r\nThe reason why it's stuck is because it's unable to connect to rpi.local:2222 while running inside colab's container.\r\nThe sample code I provided requires at least two tensorflow servers. On same machine, or on different ones (i.e. my case).\r\n~I'll soon share a link to a colab notebook reproducing this issue.~ Here's the [reproducible version](https://colab.research.google.com/gist/dhruvin2910/d8b0765a28b8182303105b29dd57262c/untitled445.ipynb). Let me know if this suffices.", "@dhruvin2910 there is a notable difference between the two snippets of code: In the autograph case, the `tf.device` is places outside the loop. In the tf.while_loop case, the `tf.device` is placed inside the loop. So to make them equivalent, you'd need to either write:\r\n\r\n```\r\nfor i in tf.range(3):\r\n    with tf.device(rpi):\r\n```\r\n\r\nor \r\n\r\n```\r\nwith tf.device(rpi):\r\n    def cond(i):\r\n        ...\r\n    def body(i):\r\n        ...\r\n\r\n    tf.while_loop(...)\r\n```\r\n\r\nAnd I believe that highlights the bug: if we place a `tf.device` outside a `tf.while_loop`, its body does not respect it. Things work as expected when the `tf.device` is placed inside the loop body. Could you verify that in your setup?\r\n\r\nAt any rate, this looks like a placement bug in TensorFlow.", "@mdanatg You sure this is intended for me? Did you mean to tag dhruvin2910 and accidentally selected me in dropdown somehow? Because although I have contributed to tensorflow in past, I don't think I have touched this part of the code.\r\n\r\n*Just realized I am logged into this from my work account (JayNakrani) and my personal account (dhananjay92) was tagged*", "@JayNakrani sorry about that. I meant to tag @dhruvin2910. Not sure how the autocomplete filled your tag.", "> there is a notable difference between the two snippets of code: In the autograph case, the tf.device is places outside the loop. In the tf.while_loop case, the tf.device is placed inside the loop. So to make them equivalent, you'd need to either write: ...\r\n\r\n@mdanatg Yes, I intended to provide what you just suggested. The problem originated from a bit involving code. Somehow, I missed wrapping the `while_loop` with `tf.device` instead, while I was simplifying it to reproduce this issue.\r\n\r\n> And I believe that highlights the bug: if we place a tf.device outside a tf.while_loop, its body does not respect it. Things work as expected when the tf.device is placed inside the loop body. Could you verify that in your setup?\r\n\r\nYes. [Updated Colab](https://colab.research.google.com/gist/dhruvin2910/d8b0765a28b8182303105b29dd57262c/untitled445.ipynb#scrollTo=vFcc6P_S_ebz)", "As I mentioned earlier, the graph seems to have `PrintV2` placed on expected device.\r\n\r\nI'm unable to upload the logs properly to tensorboard.dev, so here's a screenshot (blue ones are on rpi and green ones are on my macbook):\r\n\r\n![The Graph](https://user-images.githubusercontent.com/9679326/96669104-27d2db00-137a-11eb-8dc9-63eeaae26a48.png)\r\n\r\n> At any rate, this looks like a placement bug in TensorFlow.\r\n\r\nYes, I think the problem lies in the execution of the graph.", "I tested if all ops are misplaced or not with this code.\r\n\r\nI don't really know a proper way to find it out. But here's an experiment (and assumptions) to test it:\r\n\r\n1. A very long while loop may be substituted for a long running task (with parallel iterations set to 1)\r\n2.  1. With proper `control_dependencies` set, one can measure computation time of a set of ops.\r\n     2. If two devices have different compute power, one can guess the device placement by the amount of time taken, faster task would mean it was placed on the device with better compute.\r\n3. 1. Since both long running tasks are assigned to different devices, they should run concurrently.\r\n    2. One can guess the device placement is wrong if they are run in sequence. Can be guessed from total time, or from print statements.\r\n\r\n```python\r\n\r\ndef task(n):\r\n    with tf.name_scope('task'):\r\n        return tf.while_loop(lambda i: i < n, lambda i: i + 1, [0], parallel_iterations=1)[0]\r\n\r\n\r\ndef timed(f, *args):\r\n    with tf.name_scope('timed'):\r\n        start = tf.timestamp(name='start')\r\n        with tf.control_dependencies([start]):\r\n            r = f(*args)\r\n        with tf.control_dependencies([r]):\r\n            end = tf.timestamp(name='end')\r\n        return tf.subtract(end, start, name='time'), r\r\n\r\n\r\n@tf.function\r\ndef test_while():\r\n    with tf.device(rpi):\r\n        def cond(i):\r\n            return i < 1\r\n\r\n        def body(i):\r\n            t, n = timed(task, 100_000)\r\n            tf.print('rpi', 'while', t, n)\r\n            with tf.device(laptop):\r\n                t, n = timed(task, 100_000)\r\n                tf.print('laptop', 'while', t, n)\r\n            return [i + 1]\r\n\r\n        tf.while_loop(cond, body, [0], parallel_iterations=1)\r\n```\r\n\r\nObservations:\r\n1. The tasks are running in sequence, so there's something more to this story.\r\n2. I have a rasppberry pi running at 700MHz and a MacBook Air running at 1.8GHz. The tasks take roughly the same time (6s), and the total time was around 12s. Again there's something missing.\r\n```\r\nrpi while 6.2778100967407227 100000\r\nlaptop while 6.16434907913208 100000\r\n```\r\n\r\nNote:\r\nSince I may have made mistakes in my assumptions or the code, I don't think the experiment is conclusive.\r\nI think it may still help further investigation.", "Just discovered `tf.debugging.set_log_device_placement(True)`, and I can confirm that the device placement is indeed incorrect.\r\n\r\n```\r\n...\r\nwhile/body/_1/while/PrintV2: (PrintV2): /job:worker/replica:0/task:1/device:CPU:0\r\n...\r\nwhile/body/_1/while/PrintV2_1: (PrintV2): /job:worker/replica:0/task:1/device:CPU:0\r\n...\r\n```\r\n\r\nBoth of these are placed on `task:1` (aka `rpi`).\r\nAlso, not just the `tf.print`, but all the ops in the loop (edit: both cond and body) are are placed incorrectly. Tested with `tf.random.uniform`.", "`tf.cond` and `if ...: else: ...` (with autograph) both have same problem.\r\n\r\n```python\r\n\r\n@tf.function\r\ndef test_cond():\r\n    with tf.device(rpi):\r\n        def true_fn():\r\n            tf.print('rpi')\r\n            with tf.device(laptop):\r\n                tf.print('laptop')\r\n\r\n        def false_fn():\r\n            tf.print('rpi')\r\n            with tf.device(laptop):\r\n                tf.print('laptop')\r\n\r\n        tf.cond(tf.greater_equal(tf.random.uniform(()), 0.5), true_fn, false_fn)\r\n\r\n\r\n@tf.function\r\ndef test_if():\r\n    with tf.device(rpi):\r\n        if tf.greater_equal(tf.random.uniform(()), 0.5):\r\n            tf.print('rpi')\r\n            with tf.device(laptop):\r\n                tf.print('laptop')\r\n        else:\r\n            tf.print('rpi')\r\n            with tf.device(laptop):\r\n                tf.print('laptop')\r\n```\r\n\r\nNo `tf.print` (or any other op placed via `tf.device`) will be placed on laptop", "After a lot of experiments, here's a concise example of what is working and what is not.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# assume there's a tf.distribute.Server running on rpi.local:2222\r\ntf.config.experimental_connect_to_cluster(\r\n    tf.train.ClusterSpec({'worker': ['dhruvins-macbook-air.local:2222', 'rpi.local:2222']}),\r\n    job_name='worker',\r\n    task_index=0\r\n)\r\n\r\nlaptop = '/job:worker/task:0'\r\nrpi = '/job:worker/task:1'\r\n\r\n\r\n@tf.function\r\ndef works():\r\n    def true_fn():\r\n        # this tf.device will be respected\r\n        with tf.device(laptop):\r\n            tf.print('expected on', 'laptop')\r\n\r\n    def false_fn():\r\n        # this tf.device will be respected\r\n        with tf.device(rpi):\r\n            tf.print('expected on', 'rpi')\r\n\r\n    tf.cond(tf.constant(True), true_fn, false_fn)\r\n\r\n\r\n@tf.function\r\ndef does_not_work():\r\n    # we enclose everything inside a tf.device\r\n    with tf.device(rpi):\r\n        def true_fn():\r\n            # this tf.device will be ignored\r\n            with tf.device(laptop):\r\n                tf.print('expected on', 'laptop')\r\n\r\n        def false_fn():\r\n            # this tf.device will be ignored\r\n            with tf.device(rpi):\r\n                tf.print('expected on', 'rpi')\r\n\r\n        # tf.cond is in tf.device now\r\n        tf.cond(tf.constant(True), true_fn, false_fn)\r\n\r\n\r\nworks()\r\ndoes_not_work()\r\n```\r\n\r\nBoth calls must print 'expected on laptop' on laptop.\r\nBut the second call (aka `does_not_work`) prints on rpi instead.\r\n\r\n@mdanatg can you verify this?", "Thanks for the detailed investigation. We're having a closer look at the cause.\r\n\r\nJust to double check, in the last snippet you meant `tf.device(laptop)`, right (since the outer device is already `rpi`)?\r\n", "> Just to double check, in the last snippet you meant tf.device(laptop), right (since the outer device is already rpi)?\r\n\r\n`works` used to flip a coin and printed on a device at random, but to make the code deterministic, I set the condition as `True`. So in the example `false_fn` is never evaluated and any of `rpi` or `laptop` would work.\r\n\r\nI tried to demonstrate that if you take a correctly behaving `tf.cond` code (`works` in our example), and wrap it with a `tf.device`, the ops inside `true_fn` and `false_fn` don't get distributed the way they were originally.\r\n\r\nAll the ops are placed where `tf.cond` is placed.", "https://github.com/tensorflow/tensorflow/commit/d3698cdfd94c858092ebbb9af7ab0815e9bc78c1 should fix the `tf.device` scope inheritance issue. It also has a test that ops don't always follow cond placement (even if there's a device scope around the cond).\r\n\r\nPlease give it a try and re-open if something's still broken.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011\">No</a>\n", "Thank you so much @allenlavoie. I'll see if the issue is fixed in the nightly (when the commit gets in nightly) and let you know.\r\n\r\nFor my future reference, [colab](https://colab.research.google.com/gist/dhruvin2910/d8b0765a28b8182303105b29dd57262c/untitled445.ipynb#scrollTo=vFcc6P_S_ebz)", "@allenlavoie I think the commit you added must have arrived in nightly.\r\nI have been trying to see if the issue is still there or not for 3 days, and the problem persists.\r\nSee the colab above.\r\n\r\nPS: I'm unable to reopen this issue. @mdanatg can you still reproduce the issue?", "@dhruvin2910 I'm not sure if the issue can be accurately reproduced in the colab setup. IIUC, the colab doesn't see the output from any of the two cluster workers. At the same time, I suspect w1_proc will capture outputs from both workers.\r\n\r\nLooking at the op placement: `print(test_while.get_concrete_function().graph.as_graph_def())`, if we search for `op: \"PrintV2\"`, I can see that the two Print ops are placed one on `worker/task:0` and `worker/task:1`, as expected.\r\n\r\nI don't know if it's possible to configure a cluster that includes the current colab process. @guptapriya ?", "> I'm not sure if the issue can be accurately reproduced in the colab setup. IIUC, the colab doesn't see the output from any of the two cluster workers. At the same time, I suspect w1_proc will capture outputs from both workers.\r\n\r\n@mdanatg I tried a simple `tf.function` code (example below) and the workers (in the same colab) had correct outputs.\r\n\r\n```python\r\n@tf.function\r\ndef test():\r\n    with tf.device(w0):\r\n      tf.print(\"This is a test print. Should be done on w0\")\r\n    with tf.device(w1):\r\n      tf.print(\"This is a test print. Should be done on w1\")\r\n```\r\n\r\n> Looking at the op placement: print(test_while.get_concrete_function().graph.as_graph_def()), if we search for op: \"PrintV2\", I can see that the two Print ops are placed one on worker/task:0 and worker/task:1, as expected.\r\n\r\nI mentioned exactly this in my previous https://github.com/tensorflow/tensorflow/issues/44011#issuecomment-713271763. Ops in the graph have proper device assigned, but the ops are placed incorrectly at runtime. See this https://github.com/tensorflow/tensorflow/issues/44011#issuecomment-713360321.", "Thanks, and sorry I missed those bits. I concur, there is a bug still present in the placement logic. I suggest updating the test colab to make that clearer:\r\n\r\n```\r\n@tf.function\r\ndef test_while():\r\n    with tf.device(w0):\r\n      tf.print('w0', 'control test')\r\n    with tf.device(w1):\r\n      ...\r\n```", "The second issue looks like it was related to function inlining (when inlining, the function's placement partially overrode the body's placement; conds were inlined as two functions). I have a change that should go through soon for that.\r\n\r\nThank you for the report.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011\">No</a>\n", "The issue seems to be fixed in 2.4 \ud83c\udf89. Thanks @mdanatg and @allenlavoie."]}, {"number": 44010, "title": "ImportError: cannot import name 'MomentumParameters'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Its on COLAB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI find your beautiful code and run the very second line of code and was thrilled to get the error and wasted my one full day to figure out the issue\r\n\r\nLine of Code - [from tflite_model_maker import configs]\r\nError  - ImportError: cannot import name 'MomentumParameters'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI just ran your code from this - https://www.tensorflow.org/lite/tutorials/model_maker_text_classification, and it fails at second line of code\r\n\r\n**Any other info / logs**\r\nFollowing imports fail as well\r\nfrom tflite_model_maker import ExportFormat\r\nfrom tflite_model_maker import model_spec\r\nfrom tflite_model_maker import text_classifier\r\nfrom tflite_model_maker import TextClassifierDataLoader\r\n\r\nThanks So much\r\nHappy Coding\r\n\r\n", "comments": ["Same with: https://www.tensorflow.org/lite/tutorials/model_maker_image_classification\r\nOpened in Colab but fails:\r\n\r\n`\r\n!pip install tflite-model-maker\r\n[...]\r\n`\r\n\r\n```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nassert tf.__version__.startswith('2')\r\n\r\nfrom tflite_model_maker import configs\r\nfrom tflite_model_maker import ExportFormat\r\nfrom tflite_model_maker import image_classifier\r\nfrom tflite_model_maker import ImageClassifierDataLoader\r\nfrom tflite_model_maker import model_spec\r\n\r\nimport matplotlib.pyplot as plt\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-09786bbd7d5a> in <module>()\r\n      4 assert tf.__version__.startswith('2')\r\n      5 \r\n----> 6 from tflite_model_maker import configs\r\n      7 from tflite_model_maker import ExportFormat\r\n      8 from tflite_model_maker import image_classifier\r\n\r\n15 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/_tpu_estimator_embedding.py in <module>()\r\n     33 from tensorflow.python.tpu.tpu_embedding import AdamParameters\r\n     34 from tensorflow.python.tpu.tpu_embedding import FtrlParameters\r\n---> 35 from tensorflow.python.tpu.tpu_embedding import MomentumParameters\r\n     36 from tensorflow.python.tpu.tpu_embedding import RMSPropParameters\r\n     37 from tensorflow.python.tpu.tpu_embedding import StochasticGradientDescentParameters\r\n\r\nImportError: cannot import name 'MomentumParameters'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\r\n```", "I am from VOLVO, and I have a live project where I need a tflite model for Android device, and I am stuck with this issue, I really repent on this on why I chose TF.", "also seeing this bug, even tried tflite-model-maker-nightly to no avail\r\nHope someone can chime in with a workaround/fix", "This seems to be connected to https://github.com/tensorflow/estimator/commit/1d460d1009dd2de36803f887a21384fe0d54d7c6\r\nAs a workaround, try using an estimator nightly version from before the commit instead:\r\n\r\n```\r\npip uninstall tf-estimator-nightly tensorflow-estimator && pip install tf-estimator-nightly==2.4.0.dev2020101001\r\n```", "We have fixed this.  Could you please try it again?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44010\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44010\">No</a>\n", "**I am running a text sentiment classifier** \r\n`logdir = os.path.join(\"/tmp/logs\",datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)\r\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/sentiment_analysis.hdf5',verbose=1,save_best_only=True)\r\nearlystopping = tf.keras.callbacks.EarlyStopping('val_loss',patience=2)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_data , epochs=5 , validation_data=test_data,callbacks=[tensorboard_callback,checkpointer,earlystopping])\r\n%reload_ext tensorboard\r\n%tensorboard --logdir /tmp/logs\r\n\r\n\r\n**I got error after this** \r\n\r\nERROR: Failed to launch TensorBoard (exited with 1).\r\nContents of stderr:\r\n2020-10-24 04:53:36.358387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tensorboard\", line 5, in <module>\r\n    from tensorboard.main import run_main\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 43, in <module>\r\n    from tensorboard import default\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/default.py\", line 40, in <module>\r\n    from tensorboard.plugins.beholder import beholder_plugin_loader\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/beholder/__init__.py\", line 22, in <module>\r\n    from tensorboard.plugins.beholder.beholder import Beholder\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/plugins/beholder/beholder.py\", line 225, in <module>\r\n    class BeholderHook(tf.estimator.SessionRunHook):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/__init__.py\", line 13, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import tpu\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/tpu/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1.estimator.tpu import experimental\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/_api/v1/estimator/tpu/experimental/__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator.python.estimator.tpu._tpu_estimator_embedding import EmbeddingConfigSpec\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/_tpu_estimator_embedding.py\", line 35, in <module>\r\n    from tensorflow.python.tpu.tpu_embedding import MomentumParameters\r\nImportError: cannot import name 'MomentumParameters'`"]}, {"number": 44009, "title": "ValueError: Unable to create group (Name already exists)", "body": "```\r\ndef create_group(self, name, track_order=None):\r\n        #Create and return a new subgroup.\r\n\r\n        #Name may be absolute or relative.  Fails if the target name already\r\n        #exists.\r\n\r\n        #track_order\r\n          #  Track dataset/group/attribute creation order under this group\r\n          #  if True. If None use global default h5.get_config().track_order.\r\n        \r\n        if track_order is None:\r\n            track_order = h5.get_config().track_order\r\n\r\n        with phil:\r\n            name, lcpl = self._e(name, lcpl=True)\r\n            gcpl = Group._gcpl_crt_order if track_order else None\r\n            gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)\r\n            return Group(gid)\r\n\r\n```\r\n\r\nI am training a ssd-mobilenet model, after the end of 1st epoch, I am facing the issue. \r\nAny ideas here ? \r\n\r\npython2.7 ", "comments": ["```\r\n File \"training/train_mobilenet_ssd.py\", line 275, in <module>\r\n    train(args)\r\n  File \"training/train_mobilenet_ssd.py\", line 252, in train\r\n    callbacks=callbacks\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/engine/training.py\", line 2262, in fit_generator\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/callbacks.py\", line 77, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/callbacks.py\", line 458, in on_epoch_end\r\n    self.model.save(filepath, overwrite=True)\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 2580, in save\r\n    save_model(self, filepath, overwrite, include_optimizer)\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/models.py\", line 119, in save_model\r\n    topology.save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n  File \"/home/h338979/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 2949, in save_weights_to_hdf5_group\r\n    g = f.create_group(layer.name)\r\n  File \"/cm/shared/apps/ml-pythondeps-py27-cuda10.1-gcc/lib64/python2.7/site-packages/h5py/_hl/group.py\", line 68, in create_group\r\n    gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5g.pyx\", line 161, in h5py.h5g.create\r\nValueError: Unable to create group (name already exists)\r\n\r\n```", "@ninenerd,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAnd also, the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44008, "title": "Using skip() of tf.data.Dataset is slow for big datasets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using _skip()_ to skip most of a big _tf.data.Dataset_ (e.g. to create a validation set from a training split by just taking the last 10% of the training data), iteration over the dataset takes very long to begin. In the standalone code below skipping samples takes about 1.5 minutes to return the first sample. Another test with imagenet took ~25 minutes before returning the first sample.\r\n\r\n**Describe the expected behavior**\r\n\r\n1. I would expect the first sample from a dataset to be available in a shorter time (seconds not minutes, <10 seconds). \r\n2. I would also expect the time to get the first sample when skipping to depend on the size of the shards not on the size of the dataset.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nBelow is a small script using COCO from tfds to demonstrate the problem.\r\n\r\nFor me the output is something like this:\r\n> First sample for complete set: 0.49 sec\r\n> First sample for skipped set: 89.66 sec\r\n> \r\n\r\nStandalone code:\r\n```python\r\nimport tensorflow_datasets as tfds\r\nimport time\r\n\r\ndef test_tfds(dataset_name='coco'):\r\n    print('### Test tfds ###')\r\n\r\n    # Download dataset\r\n    print(\"Downloading dataset '{}'...\".format(dataset_name))\r\n    builder = tfds.builder(dataset_name)\r\n    download_dir = dataset_name\r\n    builder.download_and_prepare(download_dir=download_dir)\r\n    print('...dataset downloaded:\\n{}'.format(builder.info.splits))\r\n\r\n    # Get train split \r\n    split = 'train'\r\n    dataset = tfds.load(dataset_name, split=split)\r\n    \r\n    test(dataset, builder.info.splits[split].num_examples)\r\n\r\ndef test(dataset, num_samples):\r\n    # Get first sample of complete dataset: this should be ok\r\n    t = time.time()\r\n    _ = next(iter(dataset))\r\n    print('First sample for complete set: {:.2f} sec'.format(time.time() - t))\r\n\r\n    # Now skip 90% of the train split\r\n    num_skip_samples = int(0.9 * num_samples)\r\n    skipped_dataset = dataset.skip(num_skip_samples)\r\n\r\n    # Get first sample of skipped dataset: this takes extremely long\r\n    t = time.time()\r\n    _ = next(iter(skipped_dataset))\r\n    print('First sample for skipped set: {:.2f} sec'.format(time.time() - t))\r\n\r\nif __name__ == '__main__':\r\n    test_tfds('coco')\r\n```", "comments": ["@benkli01 \r\nI ran the code shared , please find the [gist here](https://colab.research.google.com/gist/Saduf2019/80c2269b05f0024295dcf2154977dd14/untitled439.ipynb),and let us know it confirms the issue reported.", "Thank you @Saduf2019 . The final output with the timing information seems to be missing. Did the process finish?", "@benkli01 \r\nPlease share \"/Python/Datasets/v1.0/train/\" this file for us to replicate.", "@Saduf2019 I'm afraid I don't understand what file you mean? Could you elaborate please? There is no such a file (\"/Python/Datasets/v1.0/train/\") on my machine. The example should be self-contained and download everything it needs.", "The tf.data `skip(n)` transformation does not avoid the computation needed to process the `n` elements, it just discards them. This is because in general, tf.data sources and transformations do not provide an efficient skip implementation (e.g. a compressed file without an index needs to be fully decompressed and iterated to determine how many elements it contains or the `filter` transformation needs to evaluate all inputs in order to determine which of them to keep).\r\n\r\nIf you would like to efficiently divide your dataset into multiple \"shards\", you should do so at the file-level.", "Sorry, didn't mean to close the issue. Feel free to close it, if my answer provides satisfactory explanation for why `skip` works the way it does.", "Hi @jsimsa , \r\nthank you for your explanation. I was suspecting something like this. \r\nSo the general process for skipping is 1) consuming/loading samples and 2) discarding them immediately?\r\n\r\nCould this be implemented more efficiently for specific types of datasets to speed things up? E.g. the COCO dataset, as used in my example script, is just saved as a bunch of jpg files on the disk (with one file equal to one sample) that could be easily skipped without loading the files. ", "If you were to use tf.data directly (as opposed to through TFDS), you could use the `skip` transformation to filter filenames to read from, which will provide you with an efficient implementation.\r\n\r\nSince you are using TFDS, you cannot easily do that because TFDS abstracts away the details it uses for loading the dataset. Having said that, I believe that you should be able to use the [Split](https://github.com/tensorflow/datasets/blob/master/docs/splits.md) API to inform TFDS what subset of elements would you like to extract and TFDS should be able to translate this to an efficient implementation (i.e. skipping at the level of files as opposed to loading everything and dropping a subset).\r\n\r\n", "Thanks @jsimsa . Using the split API when loading the dataset speeds things up significantly compared to skipping samples afterwards. Unfortunately from what I've gathered this doesn't really work for TFRecords though as they do not encode the number of samples per file, so that the iteration over all samples is unavoidable. Please correct me, if I'm wrong.\r\nI'm closing this ticket now. Thanks again for the answers!", "To enable efficient skipping of data for TFRecords (or any other format that does not support efficient indexing), you can be provide coarse-grained indexing functionality yourself by sharding your data across multiple files (and either using a fix number of elements per file or storing the number of files in filename)."]}, {"number": 44007, "title": "Output of Regression Quantized Aware Training Model is always restricted between 0 and 6", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\nI have successfully QAT a regression model for company purposes. However, when I run it on edge_tpu model, the output inference is limited between 0 and 6. This is ridiculous when my output should be in a bigger range: From the label data, the output is from (0-95) and (0-2pi).\r\n\r\n![min-max limit between 0-6](https://user-images.githubusercontent.com/27914179/95973523-bd3c0f80-0e4e-11eb-9fc7-a8e2a2b0a761.png)\r\n\r\nI have also tried normalize the output to prevent the outcome exceed 0-255 for unit8 model. However, it still be the same range (0-6) no matter what\r\n\r\nI have also tried with a different structure but always get the same result. Because this is private code so I cant share out, but I attached some frozen file for people viewing.\r\n\r\nAny recommendation? \r\n\r\nIn addition, this problem does not happen if I post quantize my model with a representative dataset.\r\n\r\n```\r\nhttps://drive.google.com/drive/folders/14L4Yrsk_9rB6UOgJn8La7n1epanAFI0B?usp=sharing\r\n```\r\n", "comments": ["@dtlam26 \r\n\r\nCan you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "> @dtlam26\r\n> \r\n> Can you please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\nI have uploaded the train code and export frozen graph to the attached drive above", "I have figured out the problem. It is the problem when that part of the model is not really trained QAT. This happens for the output node that somehow forgets to QAT when training. To overcome the problem, we should provide some op to trigger the QAT for the output nodes. In my regression case, I add a dummy op: tf.maximum(output,0) in the model to make the node QAT. If your output is strictly between 0-1, applying \"sigmoid\" activation at output instead of relu can also solve the problems.\r\n\r\n![image](https://user-images.githubusercontent.com/27914179/96995343-03892100-1569-11eb-9180-679aabfcbd60.png)\r\n"]}, {"number": 44006, "title": "tf.keras.utils.Generator doesn't get called after first batch.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution : Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GTX 1060 (6gb)\r\n\r\n**Describe the current behavior**\r\nI've created a generator class which inherits from `tf.keras.utils.Sequence` that reads from directory and returns a tuple of `X, y`. I use it with a custom model and pass it to the fit method. After using logging to debug, I found out that it returns the very first batch, passes it to the model and trains, but after that it doesn't get called again, I have a logging statement in its `__getitem__` method which should get called each batch but only gets called in the first one, and so the model then gets an array of shape `(None, None, None)`.\r\n\r\n**Describe the expected behavior**\r\nThe `__getitem__` should get called after each epoch.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab: https://colab.research.google.com/drive/14Pz5z65Cb5V8J_IsOJQ1Kk7SCU-ubwfy#scrollTo=iZm4LeXuUfja\r\n\r\n**Other info / logs**\r\nThis is the logfile I used\r\n```\r\n2020-10-14 10:42:01,329:train.DataGenerator:Preparing batch #0\r\n2020-10-14 10:42:04,687:train.CustomLSTM:Epoch #1\r\n2020-10-14 10:42:04,688:train.CustomLSTM:Xe shape: (32, 1, 160, 40)\r\n2020-10-14 10:42:05,851:train.CustomLSTM:Epoch #2\r\n2020-10-14 10:42:06,333:train.CustomLSTM:Xe shape: (None, None, None, None)\r\n```\r\nAs you can see the generator the first batch before logging the first epoch and passes it to the model, and then doesn't return the second batch and the model doesn't find anything to train on.\r\n", "comments": ["Please unlock the access to your colab.", "@bhack I did so.", "I think they you can minimize the code sample and use dummy data inputs if It Is not totally mandatory to use files to reproduce your issue.\nAs we are trying to cover as many ISSUES as possible here please help us minimizing the code that reproduce your issue with the very minimal code lines set that are required.\n", "@bhack Okay I just did.", "Ok but now It fails with a different error", "Is the error  `TypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, None, None). Consider casting elements to a supported type.`? That's the error that I get yes, because it doesn't get the second batch from the generator it recieves `Xe` and `Xs` as all `None`.", "@bhack notice there's no `Preparing batch #1` before the `Epoch #2` log.\r\n2020-10-14 14:36:00,730:train.DataGenerator:Preparing batch #0\r\n2020-10-14 14:36:05,436:train.CustomLSTM:Epoch #1\r\n2020-10-14 14:36:05,440:train.CustomLSTM:Xe shape: (32, 1, 160, 40)\r\n2020-10-14 14:36:05,442:train.CustomLSTM:Xs shape: (32, 3, 160, 40)\r\n2020-10-14 14:36:06,700:train.CustomLSTM:Epoch #2\r\n2020-10-14 14:36:07,193:train.CustomLSTM:Xe shape: (None, None, None, None)\r\n2020-10-14 14:36:07,197:train.CustomLSTM:Xs shape: (None, None, None, None)", "I think that if you want to use with that modality you could try with \r\n`model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"],run_eagerly=True)`\r\n", "@bhack I tried it and it worked, thank you. I have two questions though:\r\n1. What \"modality\" are you referring to? What could I have done differently that I wouldn't need that flag? Isn't eager execution on by defaulty in TF2+?\r\n2. In my logs, the `index` (of the batch) that gets passed to the `__getitem__` method is an arbitrary number i.e: 0, 31, 32, 51, etc. Isn't supposed to be normally incremented i.e: 0, 1, 2, 3, etc?", "@kareemamrr,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!"]}, {"number": 44004, "title": "Cannot import Tensorflow due to DLL import error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 OS Build: 19041.508\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Installed from pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Intel(R) HD Graphics Full Display Device 2160 MB, VRAM: 112 MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nNot able to import Tensorflow due to some DLL\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Prerak\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["@prerakl123 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, refer duplicate issue #43130\r\nThanks!", "@ravikyram I have Intel i3 whereas AVX instructions are in all the Qs. Also I have the latest 2019 version of C++ redistributable Do you think I am missing anything here? (See, the fact is that I am very very new so much so that I started with tensorflow yesterday only)", "OK I understood the problem - that my GPU does not meet the tensorflow requirements and AVX instructions also not available in CPU (maybe).\r\nThanks for helping me out.\r\n", "@prerakl123 \r\n\r\nPlease, close this thread since the query is been answered.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44004\">No</a>\n"]}, {"number": 44003, "title": "segmentation fault using detection postprocess for ssd_mobilenet_v3     ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source):  tf 1.15\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS\r\n\r\n**Describe the problem**\r\nI have to run the object detection model on Mbed OS. Currently while testing directly on Linux, we ran into a custom op unsupported issue. following is the error:  \r\n\r\n     ` Failed to get registration from op code CUSTOM`\r\n\r\nSo, we tried to use [detection_postprocess](https://github.com/mansnils/tensorflow/tree/detection_postprocess). we ran into a segmentation fault.\r\n\r\n`Error Stack :: SIGSEGV,\r\n#0  0x000000000807a31c in tflite::ops::micro::custom::detection_postprocess::ComputeIntersectionOverUnion(float const*, int, int) ()\r\n#1  0x000000000807a6a6 in tflite::ops::micro::custom::detection_postprocess::NonMaxSuppressionSingleClassHelper(TfLiteContext*, TfLiteNode*, tflite::ops::micro::custom::detection_postprocess::OpData*, float const*, int*, int*, int) ()\r\n#2  0x000000000807b062 in tflite::ops::micro::custom::detection_postprocess::NonMaxSuppressionMultiClassFastHelper(TfLiteContext*, TfLiteNode*, tflite::ops::micro::custom::detection_postprocess::OpData*, float const*) ()\r\n#3  0x000000000800f4e6 in tflite::MicroInterpreter::Invoke() ()\r\n#4  0x0000000008007da1 in main ()`\r\n\r\n\r\nWhen can we expect  [detection_postprocess](https://github.com/mansnils/tensorflow/tree/detection_postprocess) to be merged?.\r\n \r\n\r\n**Steps to reproduce the issue**\r\n\r\n\r\n1.Transfer Learning \r\nTrained the model with a custom dataset using the following weights-\r\n[ssd_mobilenet_v3](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz)\r\n\r\n\r\n2.Freeze graph::\r\n\r\npython /content/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path 'exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/output_inference_graph_v1.pb/pipeline.config' \\\r\n--trained_checkpoint_prefix 'exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/output_inference_graph_v1.pb/model.ckpt'  \\\r\n--output_directory 'exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/tflite' \r\n\r\n3.Conversion to tflite ::\r\n\r\n```\r\ninput_arrays = [\"normalized_input_image_tensor\"]\r\noutput_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\ninput_shapes = {'normalized_input_image_tensor': [1, 320, 320, 3]}\r\n\r\nimage_list = glob.glob('/content/drive/My Drive/Colab Notebooks/colddrink/tf1/ssd_mobilenet_v3/images/train/*.bmp')\r\ndef representative_dataset_gen():\r\n    for i in image_list:\r\n        img = cv2.imread(i)\r\n        img = img.astype(np.float32)\r\n        img = cv2.resize( img,(320,320), interpolation = cv2.INTER_NEAREST)\r\n        img = img\r\n        img = img.reshape(1,320,320,3)\r\n        # Get sample input data as a numpy array in a method of your choosing.\r\n        yield [img]\r\n\r\n\r\ngraph_def_file = PATH_TO_CKPT\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"exported-model/ssd_mobilenet_v3_small_coco_2020_01_14/tflite/ssd_mobilenet_v3_small_int8.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n4. Converting the model and image to cc and header files\r\n\r\n```\r\nxxd -i ssd.tflite object_detection.cc\r\nxxd -s 54 -i img.bmp > img.h\r\n```\r\n\r\n5.  Test\r\n[image_recognition_test.txt](https://github.com/tensorflow/tensorflow/files/5376363/image_recognition_test.txt)\r\n[original](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/image_recognition_experimental/image_recognition_test.cc)\r\n\r\n\r\ncommand\r\n`make -f tensorflow/lite/micro/tools/make/Makefile image_recognition_test`", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44003\">No</a>\n"]}, {"number": 44002, "title": "How to serve a model containing ```tensorflow.keras.layers.experimental.preprocessing``` layers ?", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n[https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nHow to serve the model in that document by tf_serving ? Is there any example ?  I have read the document [https://www.tensorflow.org/guide/keras/preprocessing_layers#benefits_of_doing_preprocessing_inside_the_model_at_inference_time](https://www.tensorflow.org/guide/keras/preprocessing_layers#benefits_of_doing_preprocessing_inside_the_model_at_inference_time) , but it doesn't give an example , either .\r\n", "comments": ["Check https://github.com/tensorflow/tensorflow/issues/31055", "> Check #31055\r\n\r\n@bhack  Hi , I have read it , but I have no idea about how to solve my problem bacause there are many input layers in my model but the #31055 only has one input layer .", "@DachuanZhao \r\nRefer to [these docs](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding) and [this link](https://keras.io/guides/transfer_learning/) let us know.", "I have solved it and I will give an example here :\r\n```Python\r\nx_input_list_dict = {\"Age\":[1,2,3]}\r\nchannel = grpc.insecure_channel(\"url\")\r\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\r\n\r\ngrpc_request = predict_pb2.PredictRequest()\r\ngrpc_request.model_spec.name = \"model_name\"\r\ngrpc_request.model_spec.signature_name = \"\"\r\n\r\nfor key,v_list in x_input_list_dict.items():\r\n    grpc_request.inputs[key].CopyFrom(\r\n        tf.make_tensor_proto(\r\n            tf.convert_to_tensor([[v] for v in v_list]),\r\n        )\r\n    )\r\n\r\n# Call the TFServing Predict API\r\npredict_response = stub.Predict(grpc_request)\r\n\r\nprobability_list = predict_response.outputs['dense_14'].float_val[:]\r\n\r\n```"]}, {"number": 44001, "title": "fixing problem with patching CMSIS lib when installing under msys64.", "body": "When patching CMSIS under Win10/msys64 the command\r\n\r\n`find tensorflow/lite/micro/tools/make/downloads/cmsis -iname '*.*'`\r\n\r\nmatches on the directory name '.settings' and sends it to the 'sed' command which then results in an error\r\n\r\n`sed: couldn't edit tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/DSP_Lib_TestSuite/DspLibTest_FVP_A5/.settings: not a regular file`\r\n\r\nThis terminates patching and leaves a partially patched CMSIS package behind. To improve this, 'find' should pass only files that match certain pattern to 'sed'.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Thanks for the contribution @ml-0 ! It seems we've been working on a similar issue or at least the same part of the code. I wrote an issue #44013 that relates to CMSIS patching taking time. I also pushed a PR #44014 to speed it up.\r\n\r\nIt would be interesting to see if it works well on a Windows machine as well (it should I guess... ). ", "Glad that @freddan80 got to this before me! I'm guessing https://github.com/tensorflow/tensorflow/pull/44014 will address the issue.\r\n\r\nIf there is any change on top of https://github.com/tensorflow/tensorflow/pull/44014 that is needed for windows then let us know.", "I tried what @freddan80 proposes in #44014. Basically, it works for me but I left a comment in #44014. Thx.", "OK, closing this pull request."]}]