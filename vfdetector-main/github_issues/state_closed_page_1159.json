[{"number": 18435, "title": "1.8.0-rc0 cherry-pick request: Fix Windows GPU TensorFlow Bazel builds.", "body": "The configure.py script will error out on Windows GPU builds due\r\nto NCCL attempted to be configured (and is currently Linux only).\r\n", "comments": []}, {"number": 18434, "title": "Replace tuple<int,int,int> for version info with a class in DnnSupportr::GetVersion()", "body": "tuple<int,int,int> is falling into too perfect forwarding issue with gcc-6. This PR replaces tuple with a class until a more permanent solution can be found.", "comments": ["+cc @reedwm ", "LGTM", "@reedwm It looks like your MacOS builders are having some problems, perhaps a disk space issue. Do we still wait for it to be fixed or is this good to be merged?", "I'm not sure what to do in this situation. I'll try running the tests again and if that fails I'll ask someone else what to do.", "@reedwm it looks like macos issue was a transient problem. Is this good to go?"]}, {"number": 18433, "title": "[tftrt update]", "body": "  Added support for TRT plugin during conversion\r\n  - converter & shape inference are now aware of plugin factory.\r\n  - PluginTensorRT\r\n    - wrapper for nvinfer1::IPlugin\r\n    - base class for custom plugins\r\n    - provides serialization/deserialization of plugin op type & input tensor dimensions\r\n  - PluginFactoryTensorRT\r\n    - wrapper for nvinfer1::PluginFactory\r\n    - singleton with plugin registration, allow user to register plugin op type (string), plugin construction function & plugin deserialization function.\r\n    - owns constructed & deserialized plugins.\r\n  - Unit tests for plugin registration\r\n\r\n  * compatible with TRT 3.0.4 plugin API.\r\n  * future plugin API changes willl be updated.", "comments": ["@aaroey ", "@jjsjann123 Please also update the description of this PR to include detailed changes. Thanks!", "- Updated the description.\r\n- Added unit test.\r\nLet me know if we want more details & examples here @aaroey .", "I didn't enable the custom_plugin_examples as default.\r\nWill do that for following PRs when I wrapped TensorRT official plugins that comes with TRT package (nvplugins)", "Hi @jjsjann123,\r\n\r\nThere seems to be conflicts, would you help to resolve them? After that I'll import this PR to test.\r\nThanks. :)", "Resolved conflicts and comments.\r\nFeel free to ping me if there's issue with internal build. :)", "Seems like the CI build is complaining about SWIG? Seems to have issue with importing tensorrt and my SWIG interface calling tensorflow::tensorrt::RegisterIncOpPlugin(). Build on local machine is working.\r\nAny hint on how to fix this?\r\nPing @aaroey in case you missed the thread. \ud83d\ude04 \r\n", "Hi @jjsjann123,\r\n\r\nSorry for the delay, I was distracted by some internal trt build problems and is still working on it. Is this PR urgent? If not, may I take a look at it later this week?\r\n\r\nThanks.", "Thanks for getting back.\r\nThis PR is not urgent. Feel free to prioritize the build problems and get back to this at your convenience.", "Yay~~~\r\n@aaroey Thanks a lot for handling internal build errors and the format issue \ud83d\udcaf ", "@jjsjann123 my pleasure. :)"]}, {"number": 18432, "title": "1.8.0-rc0 cherry-pick request: Fix windows build", "body": "Currently, Windows build is failing with errors such as:\r\ntensorflow/c/c_api_experimental.cc(1741): error C2026: string too big, trailing characters truncated", "comments": []}, {"number": 18431, "title": "Relaxing float comparison and removing unneeded include", "body": "Fixing two build failures:\r\n1. contrib/layers/python/layers/rev_block_lib_test.py failure in testForwardBackward:\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nMismatched value: a is different from b.\r\n(mismatch 1.5625%)\r\n x: array([[0.386929, 0.360026, 0.97688 , 0.086516],\r\n       [0.424401, 0.507681, 0.344677, 0.482503],\r\n       [0.260149, 0.651255, 0.529058, 0.537508],...\r\n y: array([[0.386929, 0.360026, 0.976879, 0.086516],\r\n       [0.424401, 0.507682, 0.344677, 0.482503],\r\n       [0.26015 , 0.651255, 0.529058, 0.537508],...\r\n\r\n2. Removing unneeded include.", "comments": []}, {"number": 18430, "title": "Docs: Clarify using_tpu.md", "body": "", "comments": ["Error caught during translation and I've confirmed with the XLA team. b/77901366"]}, {"number": 18429, "title": "Get error when compiling with gcc 6.4.0: error: mismatched argument pack lengths while expanding...", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Testing buster/sid\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1cd76c209ce6f74298843568a7fc397c2e6f958f\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0.5\r\n- **GPU model and memory**: Titan V\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nbazel build  --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nWhen compiling TensorFlow, I get the compilation error shown at the bottom of this post. This is the same error as #16246, but I am opening a new issue since the issue has been fixed, then broken again by 1cd76c209ce6f74298843568a7fc397c2e6f958f. The commit before 1cd76c209ce6f74298843568a7fc397c2e6f958f, e7ea87f97e03360719d132a71acc1eb2f93c249f, compiles fine.\r\n\r\nAccording to [this link](https://devtalk.nvidia.com/default/topic/1028112/cuda-setup-and-installation/nvcc-bug-related-to-gcc-6-lt-tuple-gt-header-/), CUDA 9 does not support gcc 6.4.0, so it may not be possible to fix this. If so, this issue should be closed. But gcc 6.4.0 is currently the earliest version of gcc in Debian Testing (at least on our internal workstations), so I cannot compile TensorFlow otherwise.\r\n\r\n/CC @gunan @jlebar\r\n### Source code / logs\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/dynamic_stitch_op_gpu.cu.cc:\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::tuple<int, int, int>&}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<int, int, int>&}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:662:419:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<int, int, int>&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:626:248:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding 'std::is_constructible<_Elements, _UElements&&>'\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:626:362:   required by substitution of 'template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<int, int, int>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), int, int, int>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<int, int, int>}; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]':\r\n/usr/include/c++/6/tuple:686:422:   required by substitution of 'template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {int, int, int}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), int, int, int>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), int, int, int>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]'\r\n./tensorflow/stream_executor/dnn.h:891:91:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for 'template<class _From, class _To> struct std::is_convertible'\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<int, int, int>&&; bool <anonymous> = true; _Elements = {int, int, int}]' not a return-statement\r\n     }\r\n ^\r\n```\r\n", "comments": ["> But gcc 6.4.0 is currently the earliest version of gcc in Debian Testing (at least on our internal workstations), so I cannot compile TensorFlow otherwise.\r\n\r\nIf you're blocked, you may want to try using the version of clang that TF offers to build with.  @ilya-biryukov can help if you run into problems.\r\n\r\nAnyway, this is a dupe of #18402.", "@jlebar I'm having the same problem with Ubuntu 18.04, which also has gcc 6.4.0, aside from gcc-7 which is also unsupported? You mentioned that TensorFlow can be built with clang? Which version of clang needs to be installed?", "Have you sync'ed to HEAD?  This particular issue has been fixed -- unless it's happening again.  :)\r\n\r\nWRT clang, when you configure, it should give you the option of downloading a clang binary; I'd try that.", "Ah thanks! I specifically checked out 1.8rc1 before, but it works with master.\r\n\r\nWith some tweaks to the build script (related to CUDA library paths) I managed to compile TF against the CUDA 9.1 package that comes with Ubuntu 18.04 LTS.", "Glad you got it working.  :)\r\n\r\n> With some tweaks to the build script (related to CUDA library paths)\r\n\r\nIf you're willing to send a PR, other people trying to build on 18.04 LTS would probably appreciate it!", "@pwuertz I'd also be interested in the fixes you had to do, I'm also having problems building TF on ubuntu 18.04...\r\n\r\n@jlebar I tried the clang option, but when I start the build I get:\r\n```\r\nIllegal ambiguous match on configurable attribute \"deps\" in //tensorflow/tools/pip_package:included_headers_gather:\r\n@local_config_cuda//cuda:using_nvcc\r\n@local_config_cuda//cuda:using_clang\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\n```\r\nSeems like there's a bug in the configure script... (I'm using the latest master code version)", "Ok so here is the basic idea of what I did. I was planning on adding the required bits to build TF on Ubuntu to the configure scripts but didn't have the time yet.. so here is my crude setup:\r\n\r\nBasically, the current TF build system only supports the CUDA development kit file/folder layout as given by the NVIDIA installer (e.g. the lib folder must be in `lib64/` relative to a CUDA base dir). The new Ubuntu packages (`nvidia-cuda-toolkit` and friends) however follow the distribution standard of splitting everything up into include, lib, bin. So I created a fake CUDA base dir with symlinks to all required dev files:\r\n```bash\r\n$ ls -l /usr/local/cuda-symlink/\r\ntotal 4\r\nlrwxrwxrwx 1 root root    8 Apr 24 11:33 bin -> /usr/bin\r\nlrwxrwxrwx 1 root root   12 Apr 24 11:24 include -> /usr/include\r\nlrwxrwxrwx 1 root root   25 Apr 24 11:21 lib64 -> /usr/lib/x86_64-linux-gnu\r\ndrwxr-xr-x 2 root root 4096 Apr 24 11:37 nvvm\r\n```\r\n\r\n```bash\r\n$ ls -l /usr/local/cuda-symlink/nvvm\r\ntotal 0\r\nlrwxrwxrwx 1 root root 38 Apr 24 11:37 libdevice -> /usr/lib/nvidia-cuda-toolkit/libdevice\r\n```\r\nSo now the folder `/usr/local/cuda-symlink` (almost) looks like a normal CUDA base directory to TF's configure, with the exception of the CUPTI header files. Luckily, TF already has a mechanism for providing alternative relative locations for those, so I just added the one for my approach:\r\n```diff\r\ndiff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl\r\nindex ede7e31897..14c7a363b9 100644\r\n--- a/third_party/gpus/cuda_configure.bzl\r\n+++ b/third_party/gpus/cuda_configure.bzl\r\n@@ -57,6 +57,7 @@ CUDA_LIB_PATHS = [\r\n # On most systems, the cupti library is not installed in the same directory as\r\n # the other CUDA libraries but rather in a special extras/CUPTI directory.\r\n CUPTI_HEADER_PATHS = [\r\n+  \"include/\",\r\n   \"extras/CUPTI/include/\",\r\n   \"include/cuda/CUPTI/\",\r\n ]\r\n```\r\n\r\nI guess for proper Ubuntu support you would rather add the path-alternatives feature that exists for CUPTI_HEADER_PATHS to other places in the build system too and enable building TF with `/usr` as CUDA base directory.", "@ioeric @ilya-biryukov see clang configure-time errors in https://github.com/tensorflow/tensorflow/issues/18429#issuecomment-387026279"]}, {"number": 18428, "title": "TensorFlow Hub models crashing on TF Lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Nvidia GTX 1050Ti 4GB\r\n- **Exact command to reproduce**: Running TF Lite label_image example with a retrained TF Hub model\r\n\r\n### Describe the problem\r\n[Bug] I used TensorFlow image model retraining scripts to train a custom model using TensorFlow Hub feature extractors (MobileNetV1 and MobileNetV2). When I try to run the retrained TensorFlow Hub image models in Tensorflow Lite, I get errors about dimension mismatches and allocations (see error below). I have successfully run retrained MobileNetV1 classifiers (that were not downloaded from TF Hub) using TensorFlow Lite without issue, so I am assuming that the problem stems from an issue with TF Lite/TOCO and TF Hub models.\r\n\r\nI am running TF Lite 1.6 right now because the build process for 1.7 is broken (lite/kernels/internal/spectogram.cc depends on the FFT2D library which is built with the full TF Makefile/dependency download script but not in the TF Lite Makefile/dependency download script).\r\n\r\n### Source code / logs\r\nThe error output:\r\n```\r\ntensorflow/contrib/lite/kernels/sub.cc:48 NumDimensions(input1) != NumDimensions(input2) (4 != 0)\r\nFATAL: Failed to allocate tensors!\r\n```\r\n\r\nThe verbose graph definition:\r\n```\r\nINFO: Resolved reporter\r\nINFO: Tensors size: 95\r\nINFO: Nodes size: 33\r\nINFO: Inputs: 1\r\nINFO: Input(0) name: Placeholder\r\nINFO: 0: Placeholder, 602112, 1, 0, 0\r\nINFO: 1: final_result, 20, 1, 0, 0\r\nINFO: 2: final_retrain_ops/Wx_plus_b/MatMul_bias, 20, 1, 0, 0\r\nINFO: 3: final_retrain_ops/Wx_plus_b/add, 20, 1, 0, 0\r\nINFO: 4: final_retrain_ops/weights/final_weights/transpose, 20480, 1, 0, 0\r\nINFO: 5: module/MobilenetV1/Conv2d_0/weights, 3456, 1, 0, 0\r\n......... (standard MobileNetV1 definition, I can post the full definition if it would help)\r\nINFO: 86: module_apply_default/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_bias, 2048, 1, 0, 0\r\nINFO: 87: module_apply_default/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6, 401408, 1, 0, 0\r\nINFO: 88: module_apply_default/hub_input/Mul, 602112, 1, 0, 0\r\nINFO: 89: module_apply_default/hub_input/Mul/y, 4, 1, 0, 0\r\nINFO: 90: module_apply_default/hub_input/Sub, 602112, 1, 0, 0\r\nINFO: 91: module_apply_default/hub_input/Sub/y, 4, 1, 0, 0\r\nINFO: 92: module_apply_default/hub_output/feature_vector/SpatialSqueeze, 4096, 1, 0, 0\r\n```", "comments": ["It looks like you are subtracting a scalar. Scalars are currently not supported. You can try using a 1d vector. Do you have a small reproducible test case that you can provide so I can help you further?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18427, "title": "Failed to load the native TensorFlow runtime on Win 10 64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I'm just importing tensorflow\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (x64) Home\r\n- **TensorFlow installed from**: N/A\r\n- **TensorFlow version (use command below)**: r1.7\r\n- **Python version**: 3.6.4\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN v7.0.5\r\n- **GPU model and memory**: NVidia Gforce GTX 660ti\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n- **Bazel version**: N/A\r\n\r\nI'm trying to install TensorFlow as described [here ](https://www.tensorflow.org/install/install_windows)on my Windows 10 (x64) machine.\r\n\r\nFirst I installed CUDA. My GPU (GTX 660ti, Compute Capability 3.0) is supported. Versions: Toolkit 9.0 (accidently installed 9.1 aswell beforehand), cuDNN v7.0.5 for CUDA 9.0\r\n\r\nThen I installed Tensorflow using native pip3. Python-Version: 3.6.4\r\n\r\nWhen I'm just importing the tensorflow package I'm getting the following error:\r\n\r\n\r\n```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (dll) initialization routine failed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n      File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Max\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed: A dynamic link library (dll) initialization routine failed\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n\r\n```\r\n\r\nI have the following System environment variables set:\r\n```\r\nCUDA_PATH - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\r\nCUDA_PATH_V9_0 - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\r\nCUDA_PATH_V9_1 - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\r\nPath - C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\libnvvp;\r\n\r\n```\r\nI have Microsoft Visual C++ 2015 Redistributable (x64) - 14.0.24215 installed. MSVCP140.dll is in my System32 folder.\r\n\r\nWhat I also tried:\r\n\r\n- Using Python version 3.6.2 (What i currently have)\r\n- Installing via anaconda\r\n- Using pip instead of pip3\r\n- Upgrading pip3, pip\r\n- Reinstalling (python, pip, cuda toolkit, tensorflow)\r\n- Using tensorflow version from uci repository", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nBazel version", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Is this still a problem?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 18426, "title": "Branch 192461382", "body": "Manually merged following files:\r\n\r\n\ttensorflow/compiler/xla/tests/slice_test.cc\r\n\ttensorflow/java/src/gen/cc/source_writer.h\r\n\ttensorflow/python/keras/_impl/keras/integration_test.py\r\n", "comments": ["All tests passed. Waiting for owner approval."]}, {"number": 18425, "title": "Non chief worker is throwing exception while saving the model", "body": "System information\r\n------------------\r\n\r\n - Have I written custom code - Yes\r\n - OS Platform CentOS - 7.2.1511\r\n - TensorFlow installed from - Binary\r\n - TensorFlow version - 1.3.0\r\n - Python version - 2.7\r\n - Bazel version - N/A\r\n - CUDA/cuDNN version - N/A\r\n - GPU model and memory - N/A\r\n\r\nI am having three nodes Tensorflow cluster with one parameter server, two workers, first one is chief worker.\r\nAll three servers are running on the single machine but on different ports.\r\n\r\nSource Code\r\n-----------\r\nSharing my code snippet below,\r\n\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path,\r\n                                     summary_op = my_summary_op, init_fn = restore_fn)\r\n     with sv.prepare_or_wait_for_session(server.target) as sess:\r\n                for step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\r\n                    #Log info at each epoch:\r\n                    if step % num_batches_per_epoch == 0:\r\n                        logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\r\n                        learning_rate_value, accuracy_value = sess.run([lr, accuracy])\r\n                        logging.info('Current Learning Rate: %s', learning_rate_value)\r\n                        logging.info('Current Streaming Training Accuracy: %s', accuracy_value)\r\n    \t\t\t\t\t\r\n                    #Log the summaries every 10 step.\r\n                    if step % FLAGS.steps_update_frequency == 0 and step != 0:\r\n                        loss, gs = train_step(sess, train_op, sv.global_step)\r\n                        summaries = sess.run(my_summary_op)\r\n                        \r\n                        sv.summary_computed(sess, summaries)\r\n                        print \"**** SAVE THE MODEL ****\"\r\n                        sv.saver.save(sess,sv.save_path,global_step=sv.global_step)\r\n    \r\n                    else:\r\n                        loss, _ = train_step(sess, train_op, sv.global_step)\r\n    \r\n                #We log the final training loss and accuracy\r\n                logging.info('Final Loss: %s', loss)\r\n                logging.info('Final Training Accuracy: %s', sess.run(accuracy))\r\n    \r\n                #Save the model on completing the training\r\n                logging.info('Finished training! Saving model to disk now.')\r\n                sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\r\n\r\n\r\nDescribe the problem\r\n--------------------\r\n\r\nParameter server and workers have been started fine and training is running on the both the workers.\r\nPlz note that I am using tensorflow Supervisor API to run my training.\r\n\r\nProblem starts at non-chief worker when I try to save summaries and checkpoint after certain number of training steps.\r\nI am using the single script for running parameter server and workers.\r\nI don't understand why is chief worker not reporting any issue while saving summaries and checkpoint and only non-chief worker is complaining about it.\r\nAm I doing anything wrong in my code ?\r\n\r\nAs per my understanding, every worker will run training on it's own data and store the checkpoints periodically.\r\nHowever, I have read somewhere that it's the responsibility of only chief worker to save summaries and checkpoints.\r\nIs that true and that's the reason saving model is failing for me at non-worker node?\r\n\r\nIf yes, then my question is what is the use of other worker here if it doesn't store the model, don't we loose pottentially a better model at other non-chief worker ?\r\n\r\nError Logs\r\n----------\r\nPosting error logs received at non-chief worker while saving summaries.Similarly some other exception was thrown while saving the model.\r\n\r\n    Traceback (most recent call last):\r\n      File \"./DTF_train_image_classifier.py\", line 469, in <module>\r\n        tf.app.run()\r\n      File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n      File \"./DTF_train_image_classifier.py\", line 433, in main\r\n        sv.summary_computed(sess, summaries)\r\n      File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 852, in summary_computed\r\n        raise RuntimeError(\"Writing a summary requires a summary writer.\")\r\n    RuntimeError: Writing a summary requires a summary writer.\r\n\r\nPlz note that error is thrown at below two lines,\r\n\r\n    sv.summary_computed(sess, summaries)\r\n    sv.saver.save(sess,sv.save_path,global_step=sv.global_step)", "comments": ["By default when using a `tf.train.Supervisor`, summaries are only written by the chief worker. To write summaries from a non-chief worker, you must  pass a `tf.summary.FileWriter` object when creating the supervisor: `sv = tf.train.Supervisor(..., summary_writer=tf.summary.FileWriter(path))`.", "@mrry - Thanks for your reply.\r\n\r\nI had passed tf.summary.FileWriter object at non-chief worker when creating the supervisor the way you told.\r\nBut still error is the same while saving summaries.\r\nMoreover, If I comment below line,\r\n[sv.summary_computed(sess, summaries)](url)\r\nThen non-chief worker fails while saving the checkpoint at below line,\r\n[sv.saver.save(sess,sv.save_path,global_step=sv.global_step)](url)\r\n\r\nI have checked the implementation of tf.train.Supervisor. SummaryWriter is set only if it is the chief worker.If it's not a chief worker, then it doesn't set SummaryWriter  even if you explicitly pass it at the time of creation of Supervisor object.\r\n\r\nPlease let me know if you differ from my opinion.\r\n\r\n\r\n\r\n", "Can you share the error message when you tried to save the checkpoint?\r\n\r\n> I have checked the implementation of tf.train.Supervisor. SummaryWriter is set only if it is the chief worker.If it's not a chief worker, then it doesn't set SummaryWriter even if you explicitly pass it at the time of creation of Supervisor object.\r\n\r\nYou're correct. Since `tf.train.Supervisor` is deprecated, it's unlikely that this will be fixed.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could someone plz answer the above question?", "@deepak-2017 Could you please provide the information @mrry requested earlier?", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 33 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18424, "title": "sneaky re-release of TF 1.7.0?", "body": "Someone informed us that the checksum of the source tarball from https://github.com/tensorflow/tensorflow/archive/v1.7.0.tar.gz changed according to what we had in place (was `2df5e308357356c9ffb7e9a7816dfa9947bd8928f9d406cf7e851b82853931ea`, now is `c676a96fc8700722816b2b98c85578b2f99fac7a7b2484c9c7f0641484f8d50d`).\r\n\r\nAfter some investigation, it seems there was indeed a \"re-release\" without changing the version number, see https://github.com/easybuilders/easybuild-easyconfigs/issues/6146 for more details.\r\n\r\nCan someone confirm this, and share some more information on why this was done?\r\nThis sort of practice should really be avoided at all costs...\r\n\r\nI guess @annarev could probably shed some light on this, since the most recent `1.7.0` tag done by @annarev.", "comments": ["This is covered in [section I of @boegel's talk](https://www.youtube.com/watch?v=NSemlYagjIU#t=4m10s) \ud83d\ude04 ", "We re-created the release because we had a last-minute cherrypick (after the release page in Github was created but before we built our pip packages). So, we included the cherrypick in our pip packages and updated release page for consistency.\r\n\r\nSorry for the trouble, I will make sure to avoid it in the future.\r\n", "When you do this, it makes it very hard on packagers to keep up with your releases and to guarantee users that they're getting a validated version of Tensorflow.  The update process sounds like it's not transactional.\r\n\r\nWhat should really happen in situations like this is that the cherry pick should go out as a point release.  That way packagers have a way to differentiate the versions and checksums.", "Thanks for confirming my suspicion @annarev. Do you have any idea how much time passed between the initial 1.7.0 release on GitHub and the current one?\r\n\r\n+1 on @tgamblin's remarks. I know it can be very tempting to just quickly include another minor change and replacing the tag (probably assuming that nobody will notice), but it's really bad practice, especially for a popular project like TensorFlow (where you can be damned sure *somebody* will suffer from doing this).\r\nNote that PyPI hard prevents you from doing this, if you would have pushed any `pip` packages there for the initial release, you would have been forced to do another point release (and rightfully so).", "Did the tag v1.7.0 change too ?", "I don't remember exactly how much time passed in between, but I think it was ~1-2 days.\r\nThat's right, v1.7.0 tag was re-created.\r\n\r\nThe points you bring up make sense. Sorry for the trouble!", "This was a mistake and we are taking steps to prevent it in future. No tags should ever change (and definitely no releases).\r\n\r\nI'm sorry about the additional work (and confusion) this has caused."]}, {"number": 18423, "title": "Fix print function with tf_logging.info to keep consistence", "body": "This PR is to fix print function with tf_logging.info to keep consistence.\r\n\r\nWithin the single file, some places uses print() while some places use tf_logging.info.", "comments": ["Mind addressing the lint errors? (Line too long)", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18422, "title": "Update docs of reduce_max/reduce_min for real numeric type", "body": "Both reduce_max and reduce_min only work for real numeric types as complex numbers do not apply. This fix update the docs with `numeric type` -> `real numeric type`.\r\n\r\nNote that the current kernel registration in `reduction_ops_max.cc` and `reduction_ops_min.cc`\r\nuse `TF_CALL_REAL_NUMBER_TYPES` so no change needed. The op registraton for Max and Min inside math_ops.cc should be `.Attr(\"T: realnumbertype\")` instead of `numbertype`. However, such a change will break API compatibility so leave it alone.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18421, "title": "Cannot apply convolution twice", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: b'v1.3.0-rc1-3011-gd86448938' 1.3.0\r\n\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\ninp1 = tf.ones((1, 2, 2, 5))\r\ninp2 = tf.zeros((1, 2, 2, 5))\r\n\r\nconv_layer = tf.layers.Conv2D(\r\n    filters=2,\r\n    kernel_size=2,\r\n    padding='SAME',\r\n    kernel_initializer=tf.glorot_uniform_initializer(),\r\n    name=\"vconv\")\r\ndense_layer = tf.layers.Dense(units=1, use_bias=False, activation=None)\r\n\r\ntdout1 = dense_layer.apply(inp1)\r\ntdout2 = dense_layer.apply(inp2)\r\nsess.run(tf.global_variables_initializer())\r\n# This works:\r\nprint(sess.run((tdout1, tdout2)))\r\n\r\ntcout1 = conv_layer.apply(inp1)\r\ntcout2 = conv_layer.apply(inp2)\r\nsess.run(tf.global_variables_initializer())\r\n# This does not work:\r\nprint(sess.run((tcout1, tcout2)))\r\n# ValueError: cannot add op with name vconv/convolution as that name is already used\r\n```\r\nThrows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"div/tf_2conv.py\", line 16, in <module>\r\n    tcout2 = conv_layer.apply(inp2)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 659, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 563, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\", line 171, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 835, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 499, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 187, in __call__\r\n    name=self.name)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 631, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2941, in create_op\r\n    self._add_op(ret)\r\n  File \"/home/torstein/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2595, in _add_op\r\n    \"is already used\" % op.name)\r\nValueError: cannot add op with name vconv/convolution as that name is already used\r\n```\r\n", "comments": ["Looking at the version string (\"v1.3.0-rc1-3011-gd86448938\") it seems you compiled a non-release version of the code.\r\n\r\nThis problem shouldn't exist in release branches for version 1.4 onwards.\r\nSo if you could build from a release branch (or just use release binaries), that should do the trick.\r\n\r\nClosing this out since I don't believe the problem exists in release branches. Please feel free to reopen if I'm mistaken. Thanks."]}, {"number": 18420, "title": "Imporve shape function of RandomUniformInt", "body": "The input of `minval` and `maxval` of `RandomUniformInt` should be scalar though it is not checked in the shape function. This fix improves the shape function with the rank check, and adds test case for it.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18419, "title": "Add deprecated_args decoration to expand_dims", "body": "This fix adds deprecated_args decoration to expand_dims as `dims` has been deprecated and in favor of `axis`.\r\nThis fix also enhances deprecated args with deprecation.deprecated_argument_lookup\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18418, "title": "Add string_strip to remove leading and trailing whitespaces", "body": "This fix tries to address the issue raised in #18384 to add an op tf.string_strip so that the leading and trailing whitespaces could be removed.\r\n\r\nThis fix fixes #18384.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Good for API change."]}, {"number": 18417, "title": "freeze_graph", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: None\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  macOS High Sierra 10.13.3\r\n- **TensorFlow installed from (source or binary)**:  binary, Installing with Virtualenv and pip install \r\n- **TensorFlow version (use command below)**:1.7.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:   freeze_graph \\\r\n--input_graph=/Users/cc/TF/MNIST/mnist_convnet_model/graph.pbtxt \\\r\n--input_checkpoint=/Users/cc/TF/MNIST/mnist_convnet_model/model.ckpt-31817 \\\r\n--output_graph=/tmp/frozen_graph_mnist.pb \\\r\n--output_node_names='save/restore_all' \\\r\n--input_binary=false\r\n\r\n\r\n### Describe the problem\r\n\r\nI'm trying to freeze my mnist graph which name is graph.pbtxt with these command:\r\n\r\n```\r\n      freeze_graph \\\r\n--input_graph=/Users/cc/TF/MNIST/mnist_convnet_model/graph.pbtxt \\\r\n--input_checkpoint=/Users/cc/TF/MNIST/mnist_convnet_model/model.ckpt-31817 \\\r\n--output_graph=/tmp/frozen_graph_mnist.pb \\\r\n--output_node_names='save/restore_all' \\\r\n--input_binary=false\r\n```\r\nbut the following error always acours:\r\n\r\n```\r\n    Instructions for updating:\r\nUse the retry module or similar alternatives.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/freeze_graph\", line 11, in <module>\r\n    sys.exit(main())\r\nTypeError: main() takes exactly 1 argument (0 given)\r\n\r\n```\r\n\r\n\r\n### Source code / logs\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/freeze_graph\", line 11, in <module>\r\n    sys.exit(main())\r\nTypeError: main() takes exactly 1 argument (0 given)\r\n\r\n\r\n", "comments": ["I solved this by use \"python -m tensorflow.python.tools.freeze_graph\" instead of \"freeze_graph\" . But I'm still confused about this. Why does the command does not work on my computer?  Thanks a lot", "Nagging Assignee @tatatodd: It has been 164 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There could be an issue of the wrong version. I am glad it worked out for you. \r\n Thanks!\r\n"]}, {"number": 18416, "title": "Fix typo", "body": "", "comments": []}, {"number": 18415, "title": "A bug related to conv2d_transpose and tf.cond", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: GTX 980M / 4G\r\n- **Exact command to reproduce**: python the_following_code.py\r\n\r\n### Describe the problem\r\nI am trying to implement a model named progressive GAN. I met some very weird problems. My problems could be reproduced by the following code. I believe it is a bug of tensorflow because removing any trivial tf.cond or removing conv2d_transpose will make the code work.\r\n\r\n\r\n```\r\n  File \"/home/yfeng23/test/tf/cond_test1.py\", line 38, in <module>\r\n    net = upscale2d(net_out[0])                                                   # net.shape = [16, 16, 16, 512]\r\n  File \"/home/yfeng23/test/tf/cond_test1.py\", line 12, in upscale2d\r\n\r\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 2097152 values, but the requested shape has 524288\r\n\t [[Node: Upscale2D_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Upscale2D_1/Tile, Upscale2D_1/Reshape/shape)]]\r\n```\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\n\r\ndef upscale2d(x, factor=2):\r\n  \"\"\"increase the resolution\"\"\"\r\n  with tf.variable_scope('Upscale2D'):\r\n    channels = x.shape[-1]\r\n    s = tf.shape(x)\r\n    x = tf.expand_dims(tf.expand_dims(x, axis=3), axis=2)\r\n    x = tf.tile(x, [1, 1, factor, 1, factor, 1])\r\n    x = tf.reshape(x, [s[0], s[1] * factor, s[2] * factor, channels])\r\n    return x\r\n\r\n\r\ndef to_rgb(x, lod, num_outputs):\r\n  \"\"\"generate image output\"\"\"\r\n  with tf.variable_scope('ToRGB_lod%d' % lod):\r\n    return slim.conv2d(x, num_outputs, 1, activation_fn=None)\r\n\r\n\r\nbatch_size = 16\r\nnum_outputs = 3\r\nnoise = tf.random_normal([batch_size, 128])\r\nwith slim.arg_scope([slim.conv2d, slim.conv2d_transpose],\r\n                    activation_fn=tf.nn.leaky_relu):\r\n  net = tf.expand_dims(tf.expand_dims(noise, 1), 1)\r\n  net = slim.conv2d_transpose(net, 512, kernel_size=4, padding='VALID')         # net.shape = [16, 4, 4, 512]\r\n  net0 = net\r\n  net_out = (net, tf.zeros([batch_size, 2, 2, 3]))                              # ([16, 4, 4, 512], [16, 2, 2, 3])\r\n\r\n  out = to_rgb(net_out[0], 1, num_outputs)                                      # out.shape = [16, 4, 4, 3]\r\n  net = upscale2d(net_out[0])                                                   # net.shape = [16, 8, 8, 512]\r\n  net_out = tf.cond(tf.less(0, 1), lambda: (net, out), lambda: net_out)         # ([16, 8, 8, 512], [16, 4, 4, 3])\r\n\r\n  with tf.control_dependencies([tf.assert_equal(tf.shape(net_out[0])[1], 8)]):\r\n    out = to_rgb(net_out[0], 2, num_outputs)                                    # out.shape = [16, 8, 8, 3]\r\n  net = upscale2d(net_out[0])                                                   # net.shape = [16, 16, 16, 512]\r\n  net = slim.conv2d(net, 512, 3, scope='conv1')\r\n  #net_out = tf.cond(tf.less(3, 4), lambda: net_out, lambda: (net, out))\r\n  net_out = tf.cond(tf.less(3, 1), lambda: (net, out), lambda: net_out)         # ([16, 8, 8, 512], [16, 4, 4, 3])\r\n\r\n  out = to_rgb(net_out[0], 3, num_outputs)                                      # out.shape = [16, 8, 8, 3]\r\n  up_out = upscale2d(net_out[1])                                                # out_up.shape = [16, 8, 8, 3]\r\n  net = tf.cond(tf.equal(0.0, 0.0), lambda: out, lambda: up_out + out)\r\n\r\nloss = tf.reduce_mean(net)\r\ngrad0 = tf.gradients(loss, net0)[0]\r\ngrad1 = tf.gradients(loss, noise)[0]\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  print(sess.run(net).shape)\r\n  print(sess.run(grad0).shape)\r\n  print(sess.run(grad1).shape)\r\n```\r\n", "comments": ["If I add a tab in line 38, which makes the code look like\r\n```\r\n  with tf.control_dependencies([tf.assert_equal(tf.shape(net_out[0])[1], 8)]):\r\n    out = to_rgb(net_out[0], 2, num_outputs)                                    # out.shape = [16, 8, 8, 3]\r\n    net = upscale2d(net_out[0])                                                 # net.shape = [16, 16, 16, 512]\r\n  net = slim.conv2d(net, 512, 3, scope='conv1')\r\n```\r\n\r\nThe error would become \r\n```\r\nassertion failed: [] [Condition x == y did not hold element-wise:] [x (strided_slice:0) = ] [4] [y (assert_equal/y:0) = ] [8]\r\n\t [[Node: assert_equal/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](assert_equal/Equal/_9, assert_equal/Assert/Assert/data_0, assert_equal/Assert/Assert/data_1, assert_equal/Assert/Assert/data_2, strided_slice/_11, assert_equal/Assert/Assert/data_4, assert_equal/y/_13)]]\r\n```\r\n\r\n\r\nIf I comment line 41 and uncomment line 40, the code would work.\r\n```\r\n  net_out = tf.cond(tf.less(3, 4), lambda: net_out, lambda: (net, out))\r\n  #net_out = tf.cond(tf.less(3, 1), lambda: (net, out), lambda: net_out)         # ([16, 8, 8, 512], [16, 4, 4, 3])\r\n```", "Without changing any code in the original post, I randomly get either the original error you had, or the error you get if you add a tab in line 38. I mostly get the second error, but occasionally get the first randomly, without changing any code.\r\n\r\nIn any case, neither error should be occur. @skye can you take a look", "tf.contrib has been deprecated and moved to tf.addons , please try with latest versions , if the issue still persists , please open a new issue using template. closing this issue for now."]}, {"number": 18414, "title": "Updating highwayhash library to fix kernel_tests:lookup_ops_test, lookup:lookup_ops_test and string_to_hash_bucket_op_test tests on ppc", "body": "All three of these tests were failing on ppc64le ,because the build was using a version of the highwayhash library which did not properly support the power architecture. Updating the library to include commit e2098b (google/highwayhash@e2098bc) seems to fix the issue.\r\n\r\nThanks !", "comments": ["This change breaks the Windows bazel build due to the include. Mind taking a look?", "As per log , Windows bazel build is failing with below error -\r\n```\r\nexternal/highwayhash\\highwayhash/endianess.h(56): fatal error C1083: Cannot open include file: 'sys/param.h': No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 92.322s, Critical Path: 3.10s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nLooks like https://github.com/google/highwayhash/commit/fd3d9af80465e4383162e4a7c5e2f406e82dd968 fixed the endianess.h build on Windows.\r\nWill update the highwayhash library to include commit https://github.com/google/highwayhash/commit/fd3d9af80465e4383162e4a7c5e2f406e82dd968 , hope that will help.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jhseu Can you please merge this."]}, {"number": 18413, "title": "Make op unique name generation case insensitive", "body": "Unique name generation for operations depends on checking a dict for names currently in use. This commit makes it so that the names stored in this dict are always lowercase so that we can check if a name already exists regardless of the capitalization.\r\n\r\nThis helps in filesystems where file paths are case insensitive and tensor dumps (like with tfdbg) try to follow directory structures that correspond to the tensor names. If two tensors have names with the same spelling, but different capitalizations, then this can lead to unintended side effects/errors on these case-insensitive file systems.\r\n\r\nThese are likely just edge cases, but one such issue where this can be a problem is shown in #16261. Here, because both tf.multiply and the '\\*' overloaded operator were used, this caused name collisions for some intermediate tensors where you would have  tensor names like \"gradients/mul_grad/tuple/control_dependency_1\" and \"gradients/Mul_grad/Mul\" (since tf.multiply uses name=\"Mul\" while using the '\\*' operator uses name=\"mul\"). So when these tensors are dumped, only one \"mul_grad\" or \"Mul_grad\" directory can be created on specific file systems (like with Mac and Windows).\r\n\r\nFixes #16261", "comments": ["This seems reasonable, but adding @asimshankar to review since this changes the names of ops that have this collision.", "@skye might have a more informed opinion, reassigning :)", "Some tests failing due to name changes. Will investigate.", "Nagging Assignee @jhseu: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@skye can you give a formal approval so we can merge this PR?", "I think its a bad idea.\r\nOld checkpoints will not load properly, this has the potential to break a bunch of saved models on all file systems, just to make file-systems with non case sensitive naming be more friendly with a particular use case. "]}, {"number": 18412, "title": "Reorder section `Using SavedModel with Estimators`", "body": "Outputs should be specified before performing an export.", "comments": []}, {"number": 18411, "title": "[INTEL MKL] AVX512 Fix for Eigen Vectorized sqrt/rsqrt Function", "body": "This PR has two modifications:\r\n1. Downloads latest Eigen library that has AVX512 fix for vectorized sqrt/rqrt functions.\r\n\r\n2. Added a new unit-test python script [tensorflow/python/kernel_tests/sgd_optimizers_test.py] that tests several SGD optimizers calling sqrt/rsqrt function. The test fails with older Eigen library [https://bitbucket.org/eigen/eigen/get/6913f0cf7d06.tar.gz], but passes with new downloads.\r\n\r\n3. In order to check, use the following commands\r\n`bazel test --config=mkl --copt=\"-mfma\" --copt=\"-march=native\" -c opt //tensorflow/python/kernel_tests:sgd_optimizers_test\r\n`\r\n`bazel test --copt=\"-mfma\" --copt=\"-march=native\" -c opt //tensorflow/python/kernel_tests:sgd_optimizers_test`\r\n", "comments": ["@benoitsteiner Could you please check this PR?", "@jhseu I see there is one failure: keras integration test. Will look into this.", "@jhseu Could you please suggest me how should I address pylint errors. ", "Can you run pylint locally with this pylintrc file?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/pylintrc", "@jhseu Have corrected style errors.", "@benoitsteiner @jhseu Could you please review the code. Looks like all tests passes now.", "@benoitsteiner @jhseu Any update on this PR?", "@benoitsteiner @jhseu Any update on review?\r\n", "@jhseu Any progress on this PR?", "@benoitsteiner Mind taking another look?", "@benoitsteiner Any update on this PR.", "@jhseu @benoitsteiner Any update on this PR?", "Nagging Reviewer @benoitsteiner, @rmlarsen: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied.", "@jhseu @benoitsteiner Should I close this PR? I think the issue has been fixed by downloading the correct version of Eigen. However, do you think the test is need to be included?", "I think we can close it for now. Up to @rmlarsen if we should still add the test."]}, {"number": 18410, "title": "Add scanr support for tensorflow", "body": "This fix tries to address the issue raised in #17387 where unlike `tf.foldl` vs `tf.foldr`, there is no `scanr` support which is the opposite of `tf.scan` (`scanl`).\r\n\r\nThis fix adds the `scanr` support and add additional test cases.\r\n\r\nThis fix fixes #17387.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["(Good for API addition)", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks all for the review. The PR has been updated. Please take a look.", "So the high-level question is still there: isn't this nest.flatten, tf.reverse, tf.scan, tf.reverse, nest.pack_as?\r\nIf so, it's fine to have the interface but is there a downside to wrap the code so that we don't have duplicate implementations to maintain?\r\n", "(From API review):\r\n\r\nApologies, but it seems that there was a similar change sent out internally that reduces code duplication by adding a `reverse` argument to `tf.scan`. On reflection, that seems like a better approach since otherwise we have to duplicate all the arguments and behavior between `tf.scan` and `tf.scanr`.\r\n\r\nSo, we'd recommend that we close this PR in favor of a change that adds a `reverse` argument to `tf.scan` (which will be submitted shortly).\r\n\r\n@yongtang : Sincerely sorry for the delay. Thanks for your understanding.", "@asimshankar Understand, thanks all for the help during the process!"]}, {"number": 18409, "title": "Add negative axis support for tf.manip.roll", "body": "The `tf.manip.roll` is supposed to be compatible with `numpy.roll`. `numpy.roll` support negative axis which is not available in `tf.manip.roll`. This fix adds negative axis support for `tf.manip.roll`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["thanks!"]}, {"number": 18408, "title": "Unable to write an event file for TF-TRT model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: 1080 Ti 8GB\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nwith tf.Session() as sess:\r\n    model_filename ='test_TFTRT.pb'\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        g_in = tf.import_graph_def(graph_def)\r\nLOGDIR=\"/check_graph\"\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\n```\r\n### Describe the problem\r\nI was trying to write an event file for a TF-TRT model so I can visualize it after the graph was optimized by TensorRT. When running the snippet above, I got error message:\r\n\r\n### Source code / logs\r\ng_in = tf.import_graph_def(graph_def)\r\n  File \"/home/user/.virtualenvs/BuildTF_1.7/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/.virtualenvs/BuildTF_1.7/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 570, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named TRTEngineOp in defined operations.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation. Thanks!\r\n", "@angersson Thanks for the reply! This seems to be a common problem for ops in tf.contrib. I was wondering if this could be solved by registering the TRTEngineOp (generated by TensorRT) to the Op dict and if someone can fix that for future release. Just want to make sure that people who work on TF-TRT are aware of this issue.  \r\n\r\nProblem solved after build 1.7.0 from source. Was on release candidate 1.", "I also met the problem,could you please tell me how did you solve it?\r\nMy tensorflow was build 1.7.0 from source", "Just add this to your python script:\r\n\r\n```\r\nimport tensorflow.contrib.tensorrt\r\n```"]}, {"number": 18407, "title": "Updating the sed command for docker parameterized build.", "body": "", "comments": []}, {"number": 18406, "title": "Improving S3 documentation.", "body": "Added a copy-pastable guide on the variables, and also provided usable examples that give immediate feedback.", "comments": []}]