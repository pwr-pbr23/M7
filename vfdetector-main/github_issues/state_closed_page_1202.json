[{"number": 17118, "title": "Add missing `override'", "body": "This fixes a warning produced by clang:\r\n<pre>\r\n./tensorflow/contrib/tensor_forest/kernels/v4/grow_stats.h:470:8: warning: 'InitLeafClassStats' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]\r\n  void InitLeafClassStats(int best_split_index, LeafStat* left_stats,\r\n       ^\r\n./tensorflow/contrib/tensor_forest/kernels/v4/grow_stats.h:190:16: note: overridden virtual function is here\r\n  virtual void InitLeafClassStats(int best_split_index, LeafStat* left_stats,\r\n               ^\r\n</pre>", "comments": []}, {"number": 17117, "title": "Added custom loss logging frequency to estimator train", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/17115\r\n\r\nAdded an optional parameter to log losses to tf.estimator", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "If we take this change, we should take it for train_and_evaluate, not necessarily contrib.learn.", "RunConfig has log_step_count_steps already. For consistency, probably we should define it there and let Estimator respect this configuration. wdyt?", "Oh that's great - I would definitely support having that config parameter getting passed to the appropriate functions. Alternatively, we can add a new param to RunConfig called something like \"loss_logging_steps\"\r\n", "I figured it'd be simpler to just make a new PR with that change - here it is: https://github.com/tensorflow/tensorflow/pull/17157. Feel free to close this one if you prefer that. I can switch it to have a new param from RunConfig if you prefer that - for now it just uses `log_step_count_steps`", "based on new PR, closing this one."]}, {"number": 17116, "title": "Apparent thread-safety issue in tensorflow/core/common_runtime/executor.cc", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.6.0-rc1-277-g993006fa76', '1.6.0-rc1')\r\n- **Python version**: Python 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.10.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: 4.2.1\r\n- **CUDA/cuDNN version**: 9.1 / 7.0.5\r\n- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory (CUDA compute capability 3.0)\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nClang warns about a thread-safety issue in `tensorflow/core/common_runtime/executor.cc` at line 2338 which warning appears to be valid.\r\n\r\nHere is the code around that line:\r\n\r\n```c++\r\n  if (parent_frame != nullptr) {\r\n    mutex_lock paranet_frame_lock(parent_frame->mu);\r\n    // Propagate all the dead exits to the parent frame.\r\n    for (const Node* node : frame->dead_exits) {                         // Line 2338\r\n      auto parent_iter_state = parent_frame->GetIteration(parent_iter);\r\n```\r\n\r\nA lock is held on `parent_frame->mu`, but not `frame->mu`.\r\n\r\nIf there is no thread safety issue, I think that a comment should be added to explain why, as it's not clear.\r\n\r\n### Source code / logs\r\n<pre>\r\ntensorflow/core/common_runtime/executor.cc:2338:27: warning: reading variable 'dead_exits' requires holding mutex 'frame->mu' [-Wthread-safety-precise]\r\n    for (const Node* node : frame->dead_exits) {\r\n                          ^\r\ntensorflow/core/common_runtime/executor.cc:2338:27: note: found near match 'parent_frame->mu'\r\ntensorflow/core/common_runtime/executor.cc:2338:27: warning: reading variable 'dead_exits' requires holding mutex 'frame->mu' [-Wthread-safety-precise]\r\n    for (const Node* node : frame->dead_exits) {\r\n                          ^\r\ntensorflow/core/common_runtime/executor.cc:2338:27: note: found near match 'parent_frame->mu'\r\n</pre>", "comments": ["@skye : Mind taking a look? (Since you might have been looking into `FrameState` lately?)", "@mrry do you know if this is intentional? I'll take a look when I get the chance if not.", "It looks like that code is only called here on L2027:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a4f1478134cdbf73f0ad7eda3e73407135a54566/tensorflow/core/common_runtime/executor.cc#L2024-L2027\r\n\r\nI suspect that `is_frame_done` implies that there is only one thread operating on `frame`, but it doesn't look like any annotations or lock acquisitions protect that invariant.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think that this was fixed by 3d9c27742693f9859e2fb75de57fe108520de712"]}, {"number": 17115, "title": "Feature Request: Let Estimator take custom every_n_iter for LoggingTensorHook", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Amazon Deep Learning AMI\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9\r\n- **GPU model and memory**: NVIDIA K80\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\nCurrently, `tf.estimator.Estimator` functions automatically log every 100 steps, from this line: https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/estimator/estimator.py#L760\r\n\r\nCan there be an optional argument (something like train_loss_logging_frequency=100) passed in instead? PR here: https://github.com/tensorflow/tensorflow/pull/17117, https://github.com/tensorflow/tensorflow/pull/17157\r\n\r\n", "comments": ["@ispirmustafa how about adding an optional every_n_iter argument in config?\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The PR for this has been code reviewed and approved, but not yet merged. "]}, {"number": 17114, "title": "Added a way to accept list as input to get_variable_value in Estimat\u2026", "body": "Hi there,\r\n\r\nThe function get_variable_value in the class Estimator is suppose to accept a list of strings. However, it was left as a todo item. I am submitting a implementation to take this one off the list .\r\n\r\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi there, I am very sorry to be pushy.  when you asked to make _check_string_or_not local, do you mean local to the function to or class?", "I was thinking local to the function. It's not super-important, I just felt it was so specific, and reducing its scope would allow you to be much more specific in the error it produces.", "Hi Martin, \r\n\r\nI made it local to the function. Have you seen the new error message?\r\n\r\nThanks for your response", "I'll start the tests so you can see what's wrong.", " Hi Martin,\r\n\r\nWould you rerun the tests again, please?\r\n", "The tests ran since you last change, the error reported is\r\n\r\n```\r\n  File \"/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/estimator/estimator_test.runfiles/org_tensorflow/tensorflow/python/estimator/estimator.py\", line 271, in <dictcomp>\r\n    for v in name}\r\nNameError: name 'traning' is not defined\r\n```", "I do apologize for the type. I just fixed it", "(From API review): The new contract of the function is a bit cumbersome - where the return type depends on the argument.\r\n\r\nWe'd prefer to not add this feature and instead remove the TODO :)\r\n\r\n", "True, but there is a use case for searching multiple variables. Do you want me to add get_variables_value that accepts a list and returns a dict?", "(From API-review)\r\n\r\nIf we add this function, users could do:\r\n\r\n```python\r\nvariables = estimator. get_variable_value(list_of_vars)\r\n```\r\n\r\nWithout this function, users would have to do:\r\n\r\n```python\r\nvariables = {n: estimator.get_variable_value(n) for n in list_of_vars}\r\n```\r\n\r\nIs that right?\r\nIf so, we'd prefer to not add an API function for that.\r\n\r\nHowever, note that the current implementation will open (and close) the checkpoint file once per variable. If instead we try to be more efficient by opening the checkpoint file once and reading all the variables, then a new API would be reasonable since it would be more performant than what can be achieved with a simple composition of the existing API.", "True, I could use `training.list_variables` instead, but we still haven't solved the original problem where the response changes depending on the input type.", "To clarify Asim's message: Let's do the following:\r\n\r\n- leave `get_variable_value` as is\r\n- add a new method `get_variable_values` which accepts a list and returns a dict\r\n- in this new function, make a `CheckpointReader` (using `load_checkpoint`) and then loop over the variable names to be loaded and load them using `get_tensor`. \r\n\r\nThis is much more efficient, since you only read the checkpoint once. The current implementation in this PR loads the checkpoint again for each variable to be loaded.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17113, "title": "Implement the bilinear initializer op", "body": "Bilinear interpolation is commonly used to initialize upsampling layers like `conv2d_transpose`. This PR implements a CPU op to initialize the filters.\r\n\r\nSimilar feature request: https://github.com/tensorflow/tensorflow/issues/5790\r\n", "comments": ["Fine for API review.", "Hi @protoget , thanks for offering to review this issue! Please feel free to let me know of any suggestions.", "Whats ETA on this? We really need this feature.", "@srikalyan The implementation is ready and tested.\r\n@martinwicke @protoget Would you mind taking a look at this PR? It's a quite simple one. Most of the code are just boilerplate templates.", "Could you add documentation that explains what exactly this does? I can tell from the code, but it's not clear from the docs alone.\r\n\r\nAlso, the quadruple loop appears to be inefficient. Since you only assign a value for `k==l`, three loops should suffice. For bonus points, can you express this loop in eigen operations? That will give it both GPU support and significant optimizations. \r\n\r\nI am wondering whether a C++ op is even necessary for this. Can you build this initializer from existing op kernels (e.g. ResizeBilinear for appropriate inputs?). That would be easier to maintain.", "@martinwicke Thanks for the comments.\r\n\r\nI added more docs/comments on the code to make it easier to understand. \r\n\r\nI fixed the quadruple loop into a triple loop. Thank you for the suggestion!\r\nUsing eigen will make it more concise and gain the benefit of GPU support. But it will also make it harder to understand and maintain. As an initializer, it only runs once at the beginning. Thus the performance impact is relatively limited.\r\n\r\nIt's not very straightforward to implement it using ResizeBilinear, because ResizeBilinear directly performs the upsampling on the data while BilinearInitializer generates a kernel with which the convolution conducts the upsampling.", "@martinwicke Do you have some time to take a look at this PR?\r\nI've heard from several people who wanted to use this feature in their fully convolutional networks (FCN).\r\nThanks!", "Can someone take a look at this PR? It has been waiting for a while. Thanks!", "Can one of the admins verify this patch?", "@bowang Could you please resolve the conflicts? Thanks!", "I am happy to fix the conflicts if any maintainer agrees to review this PR... otherwise it's just work in vain.", "I'll take a look", "@bowang Could you please check reviewer comments and keep us posted. Thanks!", "@bowang Could you please check reviewer comments and resolve conflicts?. Thanks!", "I said I'll take a look but it's going to be wasted time to look at it before conflicts are fixed.", "Actually, contrib is removed so closing this PR. Please reopen against TF Addons."]}, {"number": 17112, "title": "Fix a bug in tf.multiply documentation", "body": "In this commit, a bug that caused \"NOTE: `tf.multiplytiply` supports broadcasting.\" to appear on tf.multiply documentation page was fixed.\r\nSee https://www.tensorflow.org/api_docs/python/tf/multiply", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I'm not really sure about the second commit Mark has made. If you take a look at the following file, you'll see the word `Mul` not `Multiply`.\r\nhttps://github.com/tensorflow/tensorflow/blob/4e69e02241067129379f73dd4fefe57f0a12bdc9/tensorflow/core/api_def/base_api/api_def_Mul.pbtxt", "> You'll see the word Mul not Multiply.\r\n\r\nYes. But at the point when this replacement is run, it's already been switched to `Multiply`, or there wouldn't be the trailing \"tiply\". Also, look at the [c api doc for the op](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/multiply)\r\n\r\nsame for [`Sub`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_Sub.pbtxt), and it's [generated docs](https://www.tensorflow.org/api_docs/python/tf/subtract), the only reason it didn't fail the same way is that it had the back-quotes in the replace string.\r\n\r\nSo I think this is correct.", "Looks like you have some lint problems.", "Re @ssd352 please fix the lint problem then we can merge.", "I am sorry but I don't know what the lint problem is. I just changed a few characters, that's all.", "It turns out the problem was \"line too long\" on both. \r\nThe reports are buried deep in the test report interface.\r\n\r\nFixed, as suggested by `g4 fix`", "Nagging Assignee @protoget: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like an equivalent fix was merged. Thanks for your pull request!"]}, {"number": 17111, "title": "Add cast functions for complex64 and complex128", "body": "Maybe the plan is to eventually remove the `to_float`, `to_int32`, etc. functions and promote calling `tf.cast` directly, but if not then it would be nice for API symmetry to have the corresponding cast functions for the complex dtypes as well.", "comments": ["Fine for API review, please run the API golden file updates (you'll see instructions with the failing tests. You'll need to run them in Py2 on Linux or Mac."]}, {"number": 17110, "title": "Add NumPy style warning when casting complex to float", "body": "This is a real numeric gotcha, IMO.\r\n\r\nI just used `tf.losses.mean_squared_error` with complex tensors and was pleasantly surprised when everything ran smoothly. However, it turns out that the function calls `tf.to_float` on its input tensors, which will just remove the imaginary part and cast the real part to float. I should preferably have been warned about this (like in NumPy) instead of reading through the implementation.\r\n\r\nAlternatively (and perhaps even better), `tf.losses` should sum up the loss for both the real and imaginary components but that seems like a bigger change. ", "comments": ["Perhaps it would be better to move this warning into tf.cast instead so that `tf.cast(complex_tensor, tf.float32)` will also have a warning and not only `tf.to_float`.", "@caisq Can you update your review - it looks like your comments have been addressed.", "@carlthome can you fix the merge conflicts?", "@caisq fixed merge conflict.\r\n\r\nDo you think it would be better to move the warning into `tf.cast` instead of `tf.to_float` so that `tf.cast(complex_tensor, tf.float32)` will also have a warning?\r\n", "@carlthome that sounds good to me. please do the move.", "@caisq moved!", "There are test failures after the move, e.g.,\r\nhttps://source.cloud.google.com/results/invocations/94c909ec-7af9-45ec-920e-d61d6fd7c838/targets/%2F%2Ftensorflow%2Fpython:adagrad_da_test/log", "Sorry, forgot that `tf.cast(\"hello\", tf.int32)` etc. is legal syntax at the moment (!). Moved the warning to after `convert_to_tensor` to let that work.", "Looks like there are still build issues.\r\n```\r\n  File \"/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/svd_op_test.runfiles/org_tensorflow/tensorflow/python/ops/math_ops.py\", line 778, in cast\r\n    if x.dtype.is_complex and dtype.is_floating:\r\nAttributeError: type object 'numpy.complex64' has no attribute 'is_floating'\r\n```", "@gunan green now? \r\n\r\n(by the way, can I run the CI test suite locally somehow?)", "Retriggering tests.\r\nTo run tests locally, you can follow the install from sources instructions to set up TF dependencies, and then run `bazel test //tensorflow/python:math_ops_test`"]}, {"number": 17109, "title": "Add NumPy style warning when casting complex to float", "body": "https://github.com/tensorflow/tensorflow/pull/17110", "comments": ["Reopening with master as base. ", "https://github.com/tensorflow/tensorflow/pull/17110"]}, {"number": 17108, "title": "tensorflow build failure on windows", "body": "```\r\n:: Windows Server 2012R2 build of Tensorflow\r\n\r\n:: INSTALL \r\n:: - MSVC Community 2015 Update 3\r\n:: - C/C++   19.00.24215.1 for x64\r\n:: - ANACONDA        4.4.4 (Python 3.5.5)\r\n:: - CMake          3.10.2\r\n:: - SWIG           3.0.12\r\n:: - GIT            2.15.1.windows.2\r\n:: - NVIDIA CUDA       8.0 \r\n:: - NVIDIA CUDNN      6.0\r\n::\r\n:: No BAZEL\r\n:: TensorFlow version (latest from GIT repository)\r\n```\r\n\r\n```\r\n       \"G:\\test\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj\" (default target) (102) ->\r\n       (CustomBuild target) -> \r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(685): error : function call must have a constant value in a constant expression [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(718): error : function call must have a constant value in a constant expression [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(716): error : class template \"tensorflow::functor::BatchNarrowMatrixTransposeDispatcher\" has already been defined [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(248): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(250): error : the size of an array must be greater than zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(343): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(247): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n         G:/test/tensorflow/tensorflow/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc(297): error : division by zero [G:\\ap\\p\\tensorflow\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_gpu_kernels.vcxproj]\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Please close, windows support of Tensorflow GPU build appears to have stopped at 1.3.0."]}, {"number": 17107, "title": "cmake test fail on Windows", "body": "```\r\n:: - MSVC Community 2015 Update 3\r\n:: - ANACONDA        4.4.4 (Python 3.5.5)\r\n:: - CMake          3.10.2\r\n:: - SWIG           3.0.12\r\n:: - GIT            2.15.1.windows.2\r\n:: - NVIDIA CUDA       8.1 \r\n:: - NVIDIA CUDNN     6.0\r\n```\r\n\r\nCommand\r\ncmake .. ^\r\n-A x64 ^\r\n-DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=%SWIGEXE% ^\r\n-DPYTHON_EXECUTABLE=%PYEXE% ^\r\n-DPYTHON_LIBRARIES=%PYLIB% ^\r\n-DPYTHON_INCLUDE_DIR=%PYINC% ^\r\n-DNUMPY_INCLUDE_DIR=%NPYINC% ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=%CUDNNH% ^\r\n-Dtensorflow_BUILD_PYTHON_TESTS=OFF ^\r\n-Dtensorflow_BUILD_CC_TESTS=OFF ^\r\n-Dtensorflow_TF_NIGHTLY=OFF ^\r\n-Dtensorflow_CUDA_VERSION=8.0\r\n\r\nThis code block fails an causes a CMAKE error\r\n\"Selected compiler (or version) is not supported for CUDA\"\r\n\r\n```\r\n  # Test compatibility of compiler on CUDA\r\n  try_compile(CUDA_TEST_COMPILE_C\r\n     ${CMAKE_CURRENT_BINARY_DIR}/tests/cuda\r\n    ${CMAKE_CURRENT_SOURCE_DIR}/tests/cuda/compatibility_test.c\r\n    CMAKE_FLAGS -DINCLUDE_DIRECTORIES=${CUDA_INCLUDE_DIRS})\r\n  try_compile(CUDA_TEST_COMPILE_CXX\r\n    ${CMAKE_CURRENT_BINARY_DIR}/tests/cuda\r\n    ${CMAKE_CURRENT_SOURCE_DIR}/tests/cuda/compatibility_test.cc\r\n    CMAKE_FLAGS -DINCLUDE_DIRECTORIES=${CUDA_INCLUDE_DIRS})\r\n  if(NOT (CUDA_TEST_COMPILE_C AND CUDA_TEST_COMPILE_CXX))\r\n    message(FATAL_ERROR \"Selected compiler (or version) is not supported for CUDA\")\r\n  endif()\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "You can close as research has led me to agree with others that building Tensorflow on windows with GPU support is a no longer a viable option. (After TF v1.3)."]}, {"number": 17106, "title": "Bug: tf.dynamic_partition appears to produce bad outputs on GPU (0s appended) (TF 1.5/1.6)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16\r\n- **TensorFlow installed from (source or binary)**: Pip installed 1.5 and 1.6rc0\r\n- **TensorFlow version (use command below)**: 1.5 and 1.6rc0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9\r\n- **GPU model and memory**: nvidia k80\r\n- **Exact command to reproduce**: \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nx  = tf.constant(np.random.randn(3072))\r\ninds = [0]*189 + [1]*184 + [2]*184 + [3]*191 + [4]*192 + [5]*195 + [6]*195 + [7]*195 + [8]*188 + [9]*195 + [10]*188 + [11]*202 + [12]*194 + [13]*194 + [14]*194 + [15]*192\r\nassert(len(inds) == 3072)\r\npartitioned = tf.dynamic_partition(x, inds, 16)\r\nsess = tf.InteractiveSession()\r\nres = sess.run(partitioned)\r\nprint(res[-1].shape) # This should be (192,) but is (198,)\r\n```\r\n\r\n### Describe the problem\r\n\r\nThere appears to be a bug with tf.dynamic_partition. When I run the above commands, I see that there are some extra 0s appended to the final partitioned value, and its shape is incorrect - I'm not sure why. \r\n\r\nI checked that this problem does not appear to occur with Tensorflow 1.4, but does occur with Tensorflow 1.5/1.6rc0 on their GPU versions. It does NOT appear to have any issue in the CPU version of Tensorflow 1.5/1.6rc0. I was able to reproduce the error on two separate computers with GPUs.\r\n\r\nThis _might_ be related to https://github.com/tensorflow/tensorflow/issues/16872.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Thanks for the report.\r\nThe GPU kernel for dynamic partition was introduced in TensorFlow 1.5 via #13905 - seems like there is a bug. @codrut3 @ekelsen - mind taking a look?\r\n\r\n(This problem isn't visible in TensorFlow 1.4 since it didn't have a GPU kernel for the DynamicPartition op and thus that would always execute on CPU. The CPU kernel is seemingly fine)", "I've also run into a problem with dynamic partition for TensorFlow 1.5. I'll try to illustrate it as best as I can in code below.\r\n\r\n```\r\npadding_mask = tf.where(tf.not_equal(Zs, 0))\r\ndxyzs = tf.expand_dims(xyzs, axis=2) - tf.expand_dims(xyzs, axis=1)\r\ndxyzs = tf.gather_nd(dxyzs, padding_mask)\r\ndist_tensor = tf.norm(dxyzs, axis=-1)\r\ngauss = tf_gauss(dist_tensor, gauss_params)\r\nharmonics = tf_spherical_harmonics(dxyzs, dist_tensor, l_max)\r\nchannel_scatter = tf.gather(tf.equal(tf.expand_dims(Zs, axis=-1), elements), padding_mask[:,0])\r\nchannel_scatter = tf.where(channel_scatter, tf.ones_like(channel_scatter)), tf.zeros_like(channel_scatter)))\r\nchannel_gauss = tf.expand_dims(gauss, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\r\nchannel_harmonics = tf.expand_dims(harmonics, axis=-2) * tf.expand_dims(channel_scatter, axis=-1)\r\nembeds = tf.reshape(tf.einsum('ijkg,ijkl->ikgl', channel_gauss, channel_harmonics), [tf.shape(padding_mask)[0], -1])\r\npartition_idx = tf.cast(tf.where(tf.equal(tf.expand_dims(tf.gather_nd(Zs, padding_mask), axis=-1), tf.expand_dims(elements, axis=0)))[:,1], tf.int32)\r\nembeds = tf.dynamic_partition(embeds, partition_idx, num_elements)\r\nmol_idx = tf.dynamic_partition(padding_mask, partition_idx, num_elements)\r\n```\r\n\r\nSo dxyzs is gathered according to padding_mask on line 3 which gives them the same first dimension sizes. The rest of the code here preserves that same first dimension up through the einsum on line 11. Line 12 defines my partitioning indices for dynamic partition which is then used for both lines 12 and 13. So embeds and mol_idx should each be a list of tensors where every tensor in embeds has the same size first dimension as every corresponding tensor in mol_idx. \r\n\r\nThe issue is that the last tensor in embeds and mol_idx do not always (but sometimes do) have the same first dimension. This never occurs for me with any of the other tensors in each list. I haven't figured out whether or not the one with the wrong dimension size is from embeds or mol_idx (or both) for sure since these sizes will vary based on the batch, but I very strongly suspect it is from mol_idx based on typical sizes when the error does not occur. One possible cause may be the difference in dtypes. embeds contains tensors of either float32 or float64, and mol_idx contains tensors of int32. I haven't checked whether this occurs if mol_idx is int64 yet.\r\n\r\nThe same problem does not happen in Tensorflow <=1.4, and putting the dynamic partition ops on the cpu manually does fix this issue for me. Hope this helps.\r\n", "Hi @ankitvgupta @jeherr,\r\n\r\nThank you for reporting the bug(s)! I will look into it, but I am a bit busy right now and it might take me a couple days to figure out what is happening.", "This is due to a bug in CUB that is fixed by updating to the latest version.  This will happen soon and be shortly after.", "A fix to update CUB has been submitted internally and should sync to github shortly.\r\nThe 1.6 final release will contain this updated CUB version I believe, which will fix the problem.", "@asimshankar Thanks so much! This is great news. Any estimate on when the final 1.6 release will be?", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since this was fixed by 1.6.0's final release thanks to the upgrade to the CUB library version.\r\n\r\nPlease feel free t reopen if I'm mistaken.\r\nThanks"]}, {"number": 17105, "title": "Typ it", "body": "\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 17104, "title": "Feature Request: Adamax optimizer", "body": "It will be really nice to have a adamax implementation along with the regular adam optimization algorithm. Adamax superior than adam in certain cases, generally in models with word embedding.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hello As this is just a feature request i don't think those fields are necessary. ", "Sounds reasonable, though I don't think anyone on the TensorFlow team will be actively pursuing this right now, but contributions are welcome.", "Hey, \r\n\r\nI would like to work on this implementation", "@kdhingra307 Hi, do you have start writing code? I'm watching the issue since it's opened, and a prototype is nearly complete (CPU test passed):  https://github.com/facaiy/tensorflow/tree/ENH/adamax_optimizer \r\nIf you agree, I'd like to open a PR for future discussion. Thank you.", "@facaiy Have you implemented sparse gradient update?", "@facaiy please go on, I will look into another issue\r\n\r\nthough can u help me with entry point", "@dchatterjee172 Not yet. I'll finish it in the weekend.", "Thanks for your contribution. Sparse update is required in this case\nbecause adamax is mainly used with word embeddings. Can you post a comment\nhere after implementing sparse update?\n\nOn 02-Mar-2018 17:09, \"Yan Facai (\u989c\u53d1\u624d)\" <notifications@github.com> wrote:\n\n> @dchatterjee172 <https://github.com/dchatterjee172> Not yet. I'll finish\n> it in the weekend.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17104#issuecomment-369898522>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJ_0Nn4LcRx1bGOfj5v-Z1WPzo9OqnD8ks5taS9WgaJpZM4SJl_k>\n> .\n>\n", "@facaiy \r\nCan you explain this operation?\r\nhttps://github.com/facaiy/tensorflow/blob/006871750bb76dda4dfa65e0c707ca12e759d6f8/tensorflow/contrib/opt/python/training/adamax.py#L124\r\nlr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\r\nIs this required in adamax?", "Hi, I added a very rudimentary implementation of AdaMaxOptimizer in PR  #17395. It is pretty rough. Please take a look and comment.\r\n\r\n@dchatterjee172 Thanks for feedback. Many codes are copied from AdmaOptimizer, and I forget to delete the line. I have added the corresponding tests, and welcome to review. "]}, {"number": 17103, "title": "Tensorflow 1.5 issue on ipython notebook", "body": "I recently upgrade tensorflow using the command \" py -3.6 -m pip install --upgrade tensorflow \" \r\n\r\nand after that why i run python on command prompt windows 10 tensorflow works like charm\r\nbut when i open ipython notebook for python3 and try to import tensorflow i get this error\r\n\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     39 # pylint: disable=undefined-variable\r\n     40 del python\r\n---> 41 del core\r\n     42 # pylint: enable=undefined-variable\r\n\r\nNameError: name 'core' is not defined", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17102, "title": "tf.data.Dataset API how to batch un-same size images", "body": "\r\n### System information\r\n\r\ntensorflow1.5.0\r\n\r\n### Describe the problem\r\n\r\n`tf.data.Dataset` API how to batch un-same size images?\r\n\r\n### Source code / logs\r\n\r\nI don't want pad or crop to resize original image, how to batch all images using dataset api. if I call it directly:\r\n\r\n```\r\ntrain_data = tf.data.Dataset.from_tensor_slices((tf.constant(all_images), tf.constant(all_labels)))\r\n    train_data = train_data.map(input_map_fn)\r\n    train_data = train_data.batch(tf.contrib.data.batch_and_drop_remainder(2))\r\n```\r\nerror says:\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot batch tensors with different shapes in component 0. First element had shape [3543,3543,3] and element 1 had shape [1024,768,3].\r\n```\r\nSeems due the un same size of images.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17101, "title": "ImportError: Could not find 'cudart64_90.dll'.", "body": "Installed tensorflow 1.5.0 on windows 10 education (version 1709) using \"C:\\> pip3 install --upgrade tensorflow-gpu\"\r\n\r\nInstalled CUDA 9.0 from https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exenetwork\r\n\r\nInstalled cuDNN 7.0.5 for CUDA 9.0 from https://developer.nvidia.com/rdp/cudnn-download\r\n\r\nI have python 3.6.3 and nvidia gtx 1060 6GB\r\n\r\nExact command to reproduce: python neural.py\r\n\r\nBazel version: N/A\r\n\r\n### Describe the problem\r\nTried creating a simple network but running into the error described in title. I checked the directory where the CUDA_PATH refers to and the file is there.\r\n\r\n### Source code / logs\r\n    from keras.models import Sequential\r\n    from keras.layers import Dense\r\n    import pandas as pd\r\n    \r\n    def load_mnist(path):\r\n    \ttrain = pd.read_csv(path + 'train.csv')\r\n    \ty = train.ix[:,0]\r\n    \ttrain = train.drop('label',1)\r\n    \ttest = pd.read_csv(path + 'test.csv')\r\n    \treturn [train, y, test]\r\n    \r\n    [train, y, test] = load_mnist('data/')\r\n    model = Sequential()\r\n    model.add(Dense(12, input_dim=8, activation='relu'))\r\n    model.add(Dense(8, activation='relu'))\r\n    model.add(Dense(1, activation='sigmoid'))\r\n    model.compile(loss='binary_crossentropy', optimizer='adam')\r\n    \r\n    model.fit(train, y, epochs=150, batch_size=10)\r\n    \r\n    predictions = model.predict(test)\r\n\r\n![importerror](https://user-images.githubusercontent.com/25662411/36347024-c66f8fea-144c-11e8-905e-b519759c8def.PNG)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler updated the post, the code fragment is the content of neural.py", "Can you please try to install tensorflow gpu nightly build?", "I have the same problem, I used CUDA Toolkit 9.1, I am not sure if this is the problem, going to try 9.0", "Figured it out, had forgotten to set the cuDNN path variable.", "what cuDNN path variable ?\r\n", "same message for me Could not find 'cudart64_90.dll\r\n\r\nmy config \r\nwin 10\r\nCUDA 9.0\r\ncuDNN 7.0.5 \r\npython 3.6.4 and nvidia gtx 1060 6GB\r\n\r\nmy error with : import tensorflow\r\n![error](https://user-images.githubusercontent.com/1012515/37651861-da3f6338-2c39-11e8-9ef9-63148e28e0be.PNG)\r\n\r\nmy /bin\r\n![bin](https://user-images.githubusercontent.com/1012515/37651860-da24499a-2c39-11e8-89ea-961d7402c8ec.PNG)\r\n\r\nmy /include\r\n![include](https://user-images.githubusercontent.com/1012515/37651862-da5afa4e-2c39-11e8-80fa-09c85afc8057.PNG)\r\n\r\nmy /lib/x64\r\n![lib-64](https://user-images.githubusercontent.com/1012515/37651864-da74110a-2c39-11e8-83b4-7154453f269a.PNG)\r\n\r\nmy environment variables\r\n![path](https://user-images.githubusercontent.com/1012515/37651967-35cd6e84-2c3a-11e8-93a1-5677526a470c.PNG)\r\n", "I'm having the same issue with CUDA 9.0, CUDNN 7.1.1, Python 3.6.4 (AMD-64) and tensorflow-gpu. I also tried it with tf-nightly-gpu. \r\nThe cudart64_90.dll is in the bin folder but the import fails\r\nError\r\n![image](https://user-images.githubusercontent.com/32106185/37685138-a39be4d8-2c92-11e8-844a-9742da1d9759.png)\r\n\r\n\r\nMy Vars\r\n![image](https://user-images.githubusercontent.com/32106185/37685169-c0b2a020-2c92-11e8-8302-be6c21ebf3e5.png)\r\n\r\n\r\n", "I had same error. The \"bin\" directory contains \"cudart64_91.dll\" instead of \"cudart64_90.dll\". \r\nI found \"build_info.py\" file contains DLL file name to use import tensorflow on python. \r\nAnd I fixed DLL file name in \"build_info.py\". \r\nAfter that it cause a punch of error that I can not try to handle.", "I am having similar issues.\r\n\r\nVersions:\r\n```\r\n$ pip --version\r\npip 9.0.3 from [...]local\\programs\\python\\python35\\lib\\site-packages (python 3.5)\r\n```\r\n* `CUDA 9.1` \r\n* `tensorflow-gpu (1.7.0)`\r\n\r\nContent of folders:\r\n* `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\cudnn\\bin`\r\n    * `cudnn64_7.dll`\r\n* `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\bin`\r\n    * `cudart64_91.dll`\r\n    * and a bunch other dll\r\n\r\nError:\r\n```\r\nImportError: Could not find 'cudart64_90.dll'. \r\nTensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. \r\nDownload and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n```\r\n\r\nAs a test, I copied the `94_91` dll and renamed it to match what `tensorflow` is looking for but I get that:\r\n```\r\n[...]\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\nI tried with `tf-nightly-gpu (1.8.0.dev20180329)` but get the same error about `cudart64_90.dll`\r\n\r\nAny idea what I should do ? \r\n", "@leobenkel Use CUDA 9.0, found out that the hard way, but now it works.", "@21011996 , how do I get CUDA 9.0 ? I tried and I could not find it anywhere on the nvidia website. \r\n\r\nUpdate:\r\nfound it here: https://developer.nvidia.com/cuda-90-download-archive", "@leobenkel here you go https://developer.nvidia.com/cuda-90-download-archive", "@21011996 , is there any trick to uninstall cuda 9.1 and reinstall cuda 9.0 ? I went in the uninstaller control pannel window on windows 10 to uninstall all cuda 9.1 components. but now the cuda 9.0 installation keep failing even after reboot. ", "I have the same problem.. Is there any solution now?", "I suggest Google :) Installed 9.1 by mistake, but successfully installed\n9.0 after, GL m8\n\nOn Mon, 2 Apr 2018, 22:36 Leo, <notifications@github.com> wrote:\n\n> @21011996 <https://github.com/21011996> , is there any trick to uninstall\n> cuda 9.1 and reinstall cuda 9.0 ? I went in the uninstaller control pannel\n> window on windows 10 to uninstall all cuda 9.1 components. but now the cuda\n> 9.0 installation keep failing even after reboot.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17101#issuecomment-378020114>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG2_3xQROC7ZpIIXJF5I5K6vnlqDOZFCks5tkn2egaJpZM4SJhYy>\n> .\n>\n", "@kevindjf66 , to pass the installation, I disabled the visual studio support. I dont know what downstream error that will create but it succeed so far. ", "This looks like it has been solved, so I'm closing the issue to keep the tracker focused. Thanks!", "Switching to CUDA 9.0 and CUDNN7.1 worked for me", "I had same error but I switched to CUDA 9.0 and cudnn7.1\r\nIt's works now.\r\nMaybe you can change your environment path like this:\r\n![2018-05-02_211719](https://user-images.githubusercontent.com/18047583/39525330-3c3ea8ec-4e4e-11e8-8593-e52734b80acd.png)\r\n", "Same here. Uninstalling 9.1 and installing 9.0 solved it.", "same issue and solved by using CUDA 9.0 as described above", "Also keep in mind to download the base package of CUDA 9.0. Other packages (Patch1, patch2) didn't seem to include cudart64_90.dll.", "installing tf-nightly (pip install tf-nightly) helped", "I'm seeing this error as well, unfortunately. \r\n\r\n```\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n```\r\n\r\nI'm using `tensorflow-gpu-1.8.0`, but I've tried `tf-nightly-gpu` as well with no luck. I have CUDA v9.0, cuDNN 7.1, and the DLL is definitely where it's supposed to be: `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin`\r\n\r\nMy PATH is set correctly, as well (I think!)\r\n\r\n![image](https://user-images.githubusercontent.com/2983780/40287407-fe8a4fe8-5c61-11e8-8725-57b8e4f12a94.png)\r\n\r\nBeen at this for a while and not sure what else to try :(\r\n\r\nUPDATE:\r\n\r\nI solved this by removing the %CUDA_HOME% variables from the paths and simply listing `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin`, etc. instead.\r\n\r\nI also had to install the latest versions of `scipy` and `scikit-learn` to remove errors that I got after I made the path change.\r\n\r\nHope this helps someone! :)", "Is there a simple solution.  I've tried everything to no avail.", "Just download the base package, check if cudart64_90.dll is in the directory (should be in CUDA/v9.0/bin), once you see it there, ensure the directory is added to your PATH. Done. That's about as simple as it gets.", "Well, I can now instantiate a TensorFlow!\r\n\r\nI get an error that says my cpu supports instructions that the TF was not compiled to use : AVX2.\r\n\r\nI\u2019ll ignore like the internet says.\r\n\r\nThanks for your help.\r\n\r\nLuke Hamm\r\n806-535-4741\r\n\r\nOn May 24, 2018, at 12:13 PM, Armaan Puri <notifications@github.com<mailto:notifications@github.com>> wrote:\r\n\r\n\r\nJust download the base package, check if cudart64_90.dll is in the directory (should be in CUDA/v9.0/bin), once you see it there, ensure the directory is added to your PATH. Done. That's about as simple as it gets.\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/17101#issuecomment-391790328>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AlraFRaru0yR9Ivdd93mw4fQEpYFeQhYks5t1upEgaJpZM4SJhYy>.\r\n", "@lh47383 \r\nhttps://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u", "well still doesn't work with CUDA 9.0 and cuDNN 7, I have this file 100% (cudart64_90.dll) and all needed environment variables (paths) \r\n\r\ndetails: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/issues/57", "@Ap1075 well it's still not that simple https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/issues/57", "Hi,\r\nI am having the same problem when trying to run tensorflow-gpu that returns\r\nImportError: Could not find 'cudart64_90.dll'\r\n\r\nI have installed CUDA 9.0 onto my computer.\r\nAlso, copied cuDNN files into my path /NVIDIA GPU Computing Toolkit/CUDA/v9.0/\r\n\r\nIt still isn't working.\r\n\r\nAny idea how this can be fixed?\r\n\r\nThanks \r\n", "There is a \r\n`cudart64_92.dll` file in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\bin\r\nBut when trying to load TensorFlow the error is that TF is trying to make use of \r\n'cudart64_90.dll'\r\nTried renaming the file but then I get other errors \ud83d\ude2d ", "@robinekenheim  create separate issue or just install CUDA v9.0, not v9.2 (simple as that...)\r\n\r\nthis issue is about CUDA v9.0 and cudart64_90.dll", "if you'll create the new issue, please give us a number, I have same issue, tensorflow doesn't works with cuda 9.2:\r\n\r\n> ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL\r\n\r\nI'm have searching for this dll and found only cudart64_92.dll", "i installed cuda 9.2 but tensorflow is finding cudart64_92.dll ,\r\nshould i reinstall as cuda 9.0 ?  are there any answers ?\r\n", "Yes, you shoud, it helps.", "PLEASE DO NOT INSTALL CUDA 9.2 FOR TENSORFLOW USAGE. ONLY 9.0 WORKS", "### **_try it on cmd:_**\r\n`set PATH=%PATH%;C:\\tools\\cuda\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\n`\r\n", "I was going to comment \"why can't they clearly specify which CUDA version is supported in the installation docs\", but then I re-checked the installation docs just to be sure and this is what I found:\r\n![install tensorflow on windows tensorflow](https://user-images.githubusercontent.com/29166153/43369202-63f41704-9387-11e8-81f1-cf133babb048.png)\r\n\r\nLesson learned: **Read the docs carefully if you don't want to waste your valuable time.**\r\n", "I also had this problem with __cuda 9.0__ installed and the __cudart64_90.dll__ file exactly in path. \r\n\r\nAfter I had a look at the PATH in my terminal, I realized that it's because I'm using conda virtual environment and I just add PATH in system environment variable setting after installing cuda and cudnn, but **the path in virtual environment is not updated.** I solved this by deactivate-activate the environment and restart python.\r\n\r\nFor those who have similar issues in conda virtual environment, you may try [deactivate and activate the virtual environment](https://conda.io/docs/user-guide/tasks/manage-environments.html) and restart python.", "@UmeanNever was spot on. If you are using a conda environment a simple restart may be all you need to do.", "restart CMD after PATH configuration", "Found a solution, in my case it was related to Windows 10 and Pycharm.\r\nSomehow Pycharm doesn't have access to Program Files in Windows 10 if you don't run it as admin and this is default location of CUDA folder and all dlls .\r\nI closed all my projects and opened Pycharm as admin and suddenly everything works.\r\n\r\nAlso I edited two lines of build_info.py of tensorflow module:\r\n\r\ncudart_dll_name = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v9.0\\\\bin\\\\cudart64_90.dll'\r\ncudnn_dll_name = 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v9.0\\\\bin\\\\cudnn64_7.dll'\r\n\r\nHope it helps!\r\n", "Don't forget to restart cmd/terminal after applying changes in path !!", "Always check the cuda version, in this case you have to install cuda version 9.0 this will create the cudart64_90.dll file in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin folder. \r\n\r\nThis will surely work.\r\n", "> \u627e\u5230\u4e86\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u6211\u800c\u8a00\uff0c\u5b83\u4e0eWindows 10\u548cPycharm\u6709\u5173\u3002\r\n> \u4e0d\u77e5\u4f55\u6545\uff0c\u5982\u679c\u4e0d\u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884c\uff0cPycharm\u65e0\u6cd5\u8bbf\u95eeWindows 10\u4e2d\u7684Program Files\uff0c\u8fd9\u662fCUDA\u6587\u4ef6\u5939\u548c\u6240\u6709dll\u7684\u9ed8\u8ba4\u4f4d\u7f6e\u3002\r\n> \u6211\u5173\u95ed\u4e86\u6240\u6709\u9879\u76ee\u5e76\u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u6253\u5f00\u4e86Pycharm\uff0c\u7a81\u7136\u4e00\u5207\u6b63\u5e38\u3002\r\n> \r\n> \u6211\u8fd8\u7f16\u8f91\u4e86\u4e24\u884ctensorflow\u6a21\u5757\u7684build_info.py\uff1a\r\n> \r\n> cudart_dll_name ='C\uff1a\\ Program Files \\ NVIDIA GPU Computing Toolkit \\ CUDA \\ v9.0 \\ bin \\ \r\n> cudart64_90.dll'cudnn_dll_name ='C\uff1a\\ Program Files \\ NVIDIA GPU Computing Toolkit \\ CUDA \\ v9.0 \\ bin \\ cudnn64_7\u3002 DLL\u201d\r\n> \r\n> \u5e0c\u671b\u80fd\u5e2e\u52a9\u5230\u4f60\uff01\r\n\r\nThank you. This is very useful.", "I had a similar issue except mine was:\r\nImportError: Could not find 'cudart64_100.dll'.\r\n\r\nMy initial tensorflow installation was tensorflow-gpu 2.0 beta which requires CUDA 10.0.\r\n\r\nWhat I did:\r\n- uninstall tensorflow via 'pip uninstall tensorflow-gpu==2.0.0-beta1'\r\n- uninstall CUDA 9.0\r\n- install CUDA 10.0\r\n- 'pip install tensorflow-gpu' (This will install tensorflow-gpu 1.14)\r\n- download the latest cuDNN from\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\n- copy out cudnn64_7.dll, cudnn.h, cudnn.lib to the relevant folders\r\nfurther instructions: https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html\r\n- set the environment variable lastly to:\r\nVariable Name: CUDA_PATH \r\nVariable Value: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\n\r\nI am able to get tensorflow running again through these steps.", "> \r\n> \r\n> PLEASE DO NOT INSTALL CUDA 9.2 FOR TENSORFLOW USAGE. ONLY 9.0 WORKS\r\n\r\nWhat about CUDA 10/10.1/10.1 Update 1/10.1 Update 2 ?\r\nFor CUDA 10.1, it now says cudart64_100.dll that is missing.", "> PLEASE DO NOT INSTALL CUDA 9.2 FOR TENSORFLOW USAGE. ONLY 9.0 WORKS\r\n> \r\n> What about CUDA 10/10.1/10.1 Update 1/10.1 Update 2 ?\r\n> For CUDA 10.1, it now says cudart64_100.dll that is missing.\r\n\r\nI tried with Cuda 10.1, it looks for cudart64_100.dll, even if I changed the entry in build_info. So I downgrade to Cuda 10.0 and it worked. Actually from the other way round cuda 10.0 is not detecting visual studio", "Having a catch 22 situation here, I have CUDA 10.1 which means I have cudart64_101.dll not 100.dll. It's looking for 100. I cannot downgrade to CUDA 10.0 because I'll have to just wipe out my whole computer to redo the whole NVidia driver situation. NVidia is not friendly towards simply uninstalling drivers in Windows 10. The install will definitely fail as it was doing all weekend till I finally gave up and did a full format reload of Windows 10 which is a pain.  I have tensorflow 1.13.1 installed because 1.14 gets errors when trying to use it with imageai. So that's out and besides, I tested it, 1.14.0 still wants the older version of CUDA, 10.0", "So naturally I ended up having to blow away my whole Windows 10 installation and start with a clean install, just to go back a version to 10.0. Unfortunately 10.0 doesn't support Visual Studio 2019 like 10.1 does. So go figure. This is an impossible situation and I might as well give up on Machine Learning altogether.", "GETTING ERROR..\r\nImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 \r\n![ERROR](https://user-images.githubusercontent.com/6049201/64925507-1df1b680-d80f-11e9-89c0-053a8f3abd04.JPG)\r\nfrom this URL: https://developer.nvidia.com/cuda-90-download-archive\r\n\r\n\r\n", "I don't wanna spoil you guys but, recently I've installed Pop OS and the team already compiled all the necessary needs into single cmd line. Either make it dual-boot or, different disk. Both are good.\r\nOnce in Linux, open terminal and run, \r\n\r\n> sudo apt install tensorflow-cuda-latest\r\n\r\nwhich is a package of CUDA Toolkit, cuDNN, and Tensorflow itself. CUDA, TF and cuDNN version is determined by the current stable version, and your GPU version. Still using it up until now.", "> So naturally I ended up having to blow away my whole Windows 10 installation and start with a clean install, just to go back a version to 10.0. Unfortunately 10.0 doesn't support Visual Studio 2019 like 10.1 does. So go figure. This is an impossible situation and I might as well give up on Machine Learning altogether.\r\n\r\nHi, fortunately 0NE-LINE of code solves this issue. Refer [this article](https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc)", "fortunately 0NE-LINE of code solves this issue. Refer [this article](https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc).", "> fortunately 0NE-LINE of code solves this issue. Refer [this article](https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc).\r\n\r\nthis works thank you! ", "just rename the file cudart64_XXX.dll to cudart_100.dll.\r\nThis is because command is still searching for version 10.0 but u have installed 10.1 or something else.", "im having the same problem \"Could not find 'cudart64_90.dll'\"\r\nmy card is GTX 1050Ti 4GB\r\nCan Anyone Help please", "Fix for the issue!!!\r\nhttps://stackoverflow.com/a/59703666/9203336", "In my case,the problem solved by deleting tf-nightly and reinstalling tensorflow", "https://www.youtube.com/watch?v=O8yye2AHCOk"]}, {"number": 17100, "title": "Minor improvements to `estimator.predict()` docs", "body": "Just some grammatical and syntactical improvements.", "comments": []}, {"number": 17099, "title": "Add ability to use default values via environment variables", "body": "Extend get_from_env_or_user_or_default() to look for an environment\r\nvariable named 'USE_DEFAULT_' + var_name, which if set to 1, causes the\r\ndefault value to be used and not prompt the user for input.\r\n\r\nSimilar to how setting USE_DEFAULT_PYTHON_LIB_PATH to 1 causes the\r\ndefault Python path to be used, this allows the default values for other\r\nconfig vars to be used (e.g. CUDA_TOOLKIT_PATH, CUDNN_INSTALL_PATH,\r\nGCC_HOST_COMPILER_PATH, and CC_OPT_FLAGS).", "comments": ["I am not sure I see the benefit of this change.\r\nWhy cant we just override the environment variables when running configure, like follows:\r\n```\r\nCC_OPT_FLAGS=\"\" CUDA_TOOLKIT_PATH=\"\" ./configure\r\n```", "Right now, setting the environment variable to an empty string still causes the configure script to prompt for user input.\r\n\r\nget_from_env_or_user_or_default() could be alternatively changed as follows:\r\n * If the environment variable is set, then don't prompt for user input.\r\n * Also, if the environment variable is set to \"\", then assume the user wants to use the default value.\r\n\r\nThis would accomplish basically the same thing of allowing default values for configure variables to be used through setting environment variables, and thus to allow a small shell script to be written which configures and builds TensorFlow without requiring user input. However, I personally think that `USE_DEFAULT_*=1` communicates the intention of using a default value a bit better than `*=\"\"`. Moreover, in the case of CC_OPT_FLAGS, what if someone actually wants to use no extra compiler flags in the opt build configuration. They might use something like `CC_OPT_FLAGS=\" \"`, but this is one character away from `CC_OPT_FLAGS=\"\"`. It's not clear unless one is familiar with the implementation of the configure script that this would mean something quite different from `CC_OPT_FLAGS=\" \"`.", "How about:\r\n```\r\nyes \"\" | ./configure\r\n```\r\nAfter setting the non-default environment variables?", "The problem with this is that if a new configuration option is added, then I won't know about it because the default will automatically be selected.", "In the availability of defaults case, you will be broken until you set the variable. This is usually undesirable for upstream projects, but in your case you specifically are looking for being broken when there is a change in configure?\r\n\r\nNevertheless, I still do not see a benefit that outweighs the added complexity in configuration script.\r\nAlso, our future plans involve deprecating configure script, and routing everything through `--config` flags in bazel. So I am inclined to reject this PR.", "> In the availability of defaults case, you will be broken until you set the variable. This is usually undesirable for upstream projects, but in your case you specifically are looking for being broken when there is a change in configure?\r\n\r\nYes.  This is not for an upstream project, but for my personal use in building TensorFlow at HEAD.  Whenever I pull from TensorFlow's repository, I need to reconfigure and rebuild.  The small shell script that I have written helps me with that so I don't make a configuration mistake.  If there is a new configuration option added, I would like to know about it.\r\n\r\n> Nevertheless, I still do not see a benefit that outweighs the added complexity in configuration script. Also, our future plans involve deprecating configure script, and routing everything through `--config` flags in bazel. So I am inclined to reject this PR.\r\n\r\nWhat is the issue number for this?  I would like to subscribe to that ticket.", "There is an internal bug, but there is no external issue we have for deprecating configure script.\r\n@case540  how far are you into the work for removing configuration script?", "configure.py is still used for some things that dont have good replacements within Bazel. Working on it, but I would expect configure.py to still exist for several months.\r\n\r\nNow that configure.py is a python script, one thing I was thinking we could do short-term is just have python arguments to set these options instead of environment variables. That way its clear how your project is being configured by the single-line configure.py invocation. Also, could add proper --help option.\r\n\r\nIm pretty against support further ENV variables for the configure script."]}, {"number": 17098, "title": "Add missing `override'", "body": "This fixes a warning produced by clang:\r\n<pre>\r\n./tensorflow/core/common_runtime/gpu/gpu_device.h:70:10: warning: 'FillContextMap' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]\r\n  Status FillContextMap(const Graph* graph,\r\n         ^\r\n./tensorflow/core/common_runtime/device.h:124:18: note: overridden virtual function is here\r\n  virtual Status FillContextMap(const Graph* graph,\r\n</pre>", "comments": []}, {"number": 17096, "title": "Updating layers.md (fixing typo)", "body": "Fixing typo. Changing 'ans calling' to 'and calling'", "comments": []}, {"number": 17095, "title": "tensorflow_self_check.py needs updates for CUD 9.0 to look for cudnn64_7.dll", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 7\r\n- **TensorFlow installed from (source or binary)**: pip.exe install --upgrade tensorflow-gpu\r\n- **TensorFlow version (use command below)**:  b'unknown' '1.5.0'\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVIDIA GeForce GTX 660\r\n- **Exact command to reproduce**:tensorflow_self_check.py\r\n\r\n### Describe the problem\r\nThe current tensorflow_self_check.py script needs updates for CUDA 9.0 for use with tensorflow 1.5.0. \r\n\r\nI followed the script error messages and suggestions and removed CUDA 9.0 and installed the older CUDA 8.0 and the requisite cudnn64_6.dll. The script told me that I had all the required .dll, but tensorflow was still not working and to go file a bug report.\r\n\r\nWhen I manually did \"import tensorflow\" and followed the detailed error messages, I realized that tensorflow 1.5.0 really does support CUDA 9.0 and really does want cudnn64_7.dll, see below.\r\n\r\nImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn\r\n\r\nWith CUD 9.0 and cudnn64_7.dll installed, tensorflow 1.5.0 is working, GPU is enabled. The tensorflow_self_check.py needs an update to look for cudnn64_7.dll. Maybe also get rid of code that looks for older dll versions?\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["Did you refer to https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c#file-tensorflow_self_check-py-L2\r\nMaybe post an issue there?\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Comment from mrry on Mar 5:  Just a note to anybody who is still using this script: since TF 1.4, the checks have been integrated into TensorFlow when you import tensorflow, and this script is no longer maintained. In particular, TF 1.5 and 1.6 require a newer version of CUDA (9.0) and cuDNN (7.0), and you may need to upgrade your CUDA/cuDNN installation to get these.\r\n\r\nSo that clarifies the script is now longer supported. \r\n\r\nIt appears that the references to this script have been removed from the install pages, so there are no loose ends.\r\n\r\nClosing bug report\r\n"]}, {"number": 17094, "title": "make the TfLiteCameraDemo.apk built with bazel work again", "body": "Current bazel build rule needs \"labels.txt\". Build with current head, you get \"Uninitialized Classifier or invalid context\" as shown in the figure below. ![failed](https://user-images.githubusercontent.com/3395998/36347403-69fb75cc-1491-11e8-81df-822c27e162f9.png)\r\n", "comments": ["it's fixed in 4b7511f4, which is merged. So I'll close this.\r\n"]}, {"number": 17093, "title": "enqueue inside while loop does not work as expected", "body": "VERSION 1.5.0-rc0\r\nGIT_VERSION v1.3.0-rc1-7323-g8d5741f\r\nCompiled from source\r\n\r\n```python\r\nwith tf.device('cpu:0'):\r\n    x = tf.contrib.framework.local_variable(1024)\r\n    q = tf.FIFOQueue(-1, tf.int32, shapes=[[]])\r\n\r\n    def cond(_):\r\n        remaining = tf.assign_sub(x, 1, use_locking=True)\r\n        with tf.control_dependencies([q.enqueue(remaining)]):\r\n            return remaining > 0\r\n\r\n    def body(_):\r\n        return _\r\n\r\n    op = tf.while_loop(cond, body, [tf.constant([])], parallel_iterations=1, back_prop=False).op\r\n\r\nx.initializer.run()\r\nop.run()\r\nq.dequeue_many(q.size()).eval()\r\n# array([0, 0, 0, ..., 0, 0, 0], dtype=int32)\r\n# expected: array([1023, 1022, 1021, ..., 2, 1, 0], dtype=int32)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Yes\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.5.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: 8.0/6.0\r\nGPU model and memory: 1080Ti\r\nExact command to reproduce: N/A", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@bignamehyp I think this is a bug.", "@gaohuazuo Sorry for spamming this closed issue. Did you find a solution? I am facing similar issues using queue enqueue within a while loop."]}, {"number": 17092, "title": "[BUG] GPU memory is not freed before execution of following operation + report_tensor_allocations_upon_oom is wrong", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: both\r\n- **TensorFlow version (use command below)**: 1.4 and 1.6\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: ?\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9 / 6\r\n- **GPU model and memory**: Tesla P100-PCIE-16GB\r\n- **Exact command to reproduce**:\r\n\r\nThe following code defines an operation that performs two big multiplications and a sum reduction on the GPU:\r\n```\r\ndef op(alpha, Xder, Xdertest, i):\r\n      cols = size #tf.shape(X)[0]\r\n      first = tf.matmul(Xder, alpha, transpose_a=True, name=\"first_{}\".format(i))            # cols x num_des x 1  \r\n      with tf.control_dependencies([first]):\r\n            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\r\n            third = tf.matmul(xdt, first, name=\"third_{}\".format(i))                         # cols x num_dim x 1\r\n            total = tf.reduce_sum(third, name=\"total_{}\".format(i))                          # single number\r\n      return total \r\n```\r\nThis will run fine if performed once, but if you repeat it with:\r\n```\r\nsingleExecution = op(alpha, Xder, Xdertest[0,:,:], 0)\r\nwith tf.control_dependencies([singleExecution]):\r\n      secodExecution = op(alpha, Xder, Xdertest[1,:,:], 1)\r\n      doubleExecution = singleExecution + secodExecution # this should only add two doubles!\r\n```\r\n it will produce an OOM-Execption. The expected behavior would be the calculation of the first result, clearing the GPU of all used memory and then calculating the second result.\r\n\r\nComplete Code to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\nimport numpy as np\r\nimport argparse\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('-size', type=int, default=100000)\r\nparser.add_argument('-displayPlacement',action='store_true', default=False)\r\nargs = parser.parse_args()\r\n\r\nrows = 2\r\nsize = args.size\r\nnum_des = 190\r\nnum_dim = 60\r\n\r\ndef op(alpha, Xder, Xdertest, i):\r\n      cols = size #tf.shape(X)[0]\r\n      first = tf.matmul(Xder, alpha, transpose_a=True, name=\"first_{}\".format(i))            # cols x num_des x 1  \r\n      with tf.control_dependencies([first]):\r\n            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\r\n            third = tf.matmul(xdt, first, name=\"third_{}\".format(i))                         # cols x num_dim x 1\r\n            total = tf.reduce_sum(third, name=\"total_{}\".format(i))                          # single number\r\n      return total  \r\n\r\n\r\nXder = tf.placeholder(tf.float64, [None, num_dim, num_des], name=\"XDerivation\")\r\nXdertest = tf.placeholder(tf.float64, [None, num_dim, num_des], name=\"XDerivation2\")\r\nalpha = tf.placeholder(tf.float64, [None, num_dim, 1], name=\"alpha\")\r\n\r\n\r\nsingleExecution = op(alpha, Xder, Xdertest[0,:,:], 0)\r\nwith tf.control_dependencies([singleExecution]):\r\n      secodExecution = op(alpha, Xder, Xdertest[1,:,:], 1)\r\n      doubleExecution = singleExecution + secodExecution\r\n\r\nfdict = {\r\n      alpha: np.random.rand(size, num_dim, 1),\r\n      Xder: np.random.rand(size, num_dim, num_des),\r\n      Xdertest: np.random.rand(rows, num_dim, num_des),\r\n}\r\nprint(\"Memory of input:\")\r\nprint(\"alpha: {:.2f} MB\".format(size*num_dim * 8. / 1024**2))\r\nprint(\"Xder: {:.2f} MB\".format(size*num_dim*num_des * 8. / 1024**2))\r\nprint(\"Xdertest: {:.2f} MB\".format(rows*num_dim*num_dim * 8. / 1024**2))\r\n\r\nprint(\"first operation should need Xder + alpha + result: {:.2f} MB\".format(( size*num_dim*num_des + size*num_dim  + size*num_des )* 8. / 1024**2))\r\nprint(\"result of first operation alone needs: {:.2f} MB\".format(( size*num_des )* 8. / 1024**2))\r\nprint(\"xdt operation should need : {:.2f} MB\".format(( size*num_dim*num_des )* 8. / 1024**2))\r\nprint(\"third operation should need xdt + first + result: {:.2f} MB\".format((size*num_dim + size*num_des + size*num_dim*num_des )* 8. / 1024**2))\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = False\r\nconfig.log_device_placement = args.displayPlacement\r\nsess = tf.Session(config=config) \r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE, report_tensor_allocations_upon_oom = True)\r\nrun_metadata = tf.RunMetadata()\r\nprint(sess.run(singleExecution, feed_dict=fdict, options=run_options, run_metadata=run_metadata))\r\nprint(\"singleExecution finished!\")\r\n\r\nfetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\nchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\r\nwith open('timeline_single.json', 'w') as f:\r\n      f.write(chrome_trace)\r\n\r\nrun_metadata = tf.RunMetadata()\r\nprint(sess.run(doubleExecution, feed_dict=fdict, options=run_options, run_metadata=run_metadata))\r\nprint(\"doubleExecution finished!\")\r\nfetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\nchrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)\r\nwith open('timeline_double.json', 'w') as f:\r\n      f.write(chrome_trace)\r\n```\r\nYou may have to change the size of the tensor to trigger the OOM if you use a different GPU. \r\nThe test-Code will also produce timelines, one for the single execution and a second for the second execution (only if you choose a size small enough - for example 70000 on the P100)\r\n\r\nThe print created by the report_tensor_allocations_upon_oom is not helpful, because it indicates a nearly complete free GPU. Total Log of the execution:\r\n\r\n```\r\n $$  CUDA_VISIBLE_DEVICES=1 python gpuMem.py \r\nMemory of input:\r\nalpha: 45.78 MB\r\nXder: 8697.51 MB\r\nXdertest: 0.05 MB\r\nfirst operation should need Xder + alpha + result: 8888.24 MB\r\nresult of first operation alone needs: 144.96 MB\r\nxdt operation should need : 8697.51 MB\r\nthird operation should need xdt + first + result: 8888.24 MB\r\n2018-02-17 14:02:30.712016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:0b:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\r\n2018-02-17 14:02:30.712072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-02-17 14:02:31.024207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15128 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:0b:00.0, compute capability: 6.0)\r\n2018-02-17 14:02:36.312709: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally\r\n8563879389.460578\r\nsingleExecution finished!\r\n2018-02-17 14:02:53.755140: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.49GiB.  Current allocation summary follows.\r\n2018-02-17 14:02:53.755215: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755233: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755256: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2018-02-17 14:02:53.755271: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755285: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755358: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755375: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755390: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755427: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 1, Chunks in use: 1. 178.2KiB allocated for chunks. 178.2KiB in use in bin. 178.1KiB client-requested in use in bin.\r\n2018-02-17 14:02:53.755445: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755459: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755487: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755502: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755532: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 1, Chunks in use: 1. 45.78MiB allocated for chunks. 45.78MiB in use in bin. 45.78MiB client-requested in use in bin.\r\n2018-02-17 14:02:53.755570: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-02-17 14:02:53.755589: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 1, Chunks in use: 1. 144.96MiB allocated for chunks. 144.96MiB in use in bin. 144.96MiB client-requested in use in bin.\r\n2018-02-17 14:02:53.755606: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 2, Chunks in use: 1. 14.59GiB allocated for chunks. 8.49GiB in use in bin. 8.49GiB client-requested in use in bin.\r\n2018-02-17 14:02:53.755622: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 8.49GiB was 256.00MiB, Chunk State: \r\n2018-02-17 14:02:53.755645: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 6.09GiB | Requested Size: 781.2KiB | in_use: 0, prev:   Size: 144.96MiB | Requested Size: 144.96MiB | in_use: 1\r\n2018-02-17 14:02:53.755662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10216400000 of size 1280\r\n2018-02-17 14:02:53.755676: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10216400500 of size 9120000000\r\n2018-02-17 14:02:53.755688: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10435d82d00 of size 48000000\r\n2018-02-17 14:02:53.755699: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10438b49900 of size 182528\r\n2018-02-17 14:02:53.755711: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x10438b76200 of size 152000000\r\n2018-02-17 14:02:53.755723: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x10441c6b800 of size 6543709184\r\n2018-02-17 14:02:53.755731: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \r\n2018-02-17 14:02:53.755746: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2KiB\r\n2018-02-17 14:02:53.755759: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 182528 totalling 178.2KiB\r\n2018-02-17 14:02:53.755772: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 48000000 totalling 45.78MiB\r\n2018-02-17 14:02:53.755786: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 152000000 totalling 144.96MiB\r\n2018-02-17 14:02:53.755799: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 9120000000 totalling 8.49GiB\r\n2018-02-17 14:02:53.755812: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 8.68GiB\r\n2018-02-17 14:02:53.755827: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \r\nLimit:                 15863893197\r\nInUse:                  9320183808\r\nMaxInUse:               9322583808\r\nNumAllocs:                      22\r\nMaxAllocSize:           9120000000\r\n\r\n2018-02-17 14:02:53.755846: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ***********************************************************_________________________________________\r\n2018-02-17 14:02:53.755879: W tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at tile_ops.cc:123 : Resource exhausted: OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[Node: xdt_0 = Tile[T=DT_DOUBLE, Tmultiples=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ExpandDims, xdt_0/multiples)]]\r\n\r\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\r\n  144.96MiB from first_0\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"gpuMem.py\", line 65, in <module>\r\n    print(sess.run(doubleExecution, feed_dict=fdict, options=run_options, run_metadata=run_metadata))\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[Node: xdt_0 = Tile[T=DT_DOUBLE, Tmultiples=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ExpandDims, xdt_0/multiples)]]\r\n\r\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\r\n  144.96MiB from first_0\r\n\r\n\r\nCaused by op 'xdt_0', defined at:\r\n  File \"gpuMem.py\", line 30, in <module>\r\n    singleExecution = op(alpha, Xder, Xdertest[0,:,:], 0)\r\n  File \"gpuMem.py\", line 19, in op\r\n    xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5587, in tile\r\n    \"Tile\", input=input, multiples=multiples, name=name)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ghiero/anaconda3/envs/tftest/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100000,60,190] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[Node: xdt_0 = Tile[T=DT_DOUBLE, Tmultiples=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ExpandDims, xdt_0/multiples)]]\r\n\r\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\r\n  144.96MiB from first_0\r\n \r\n```\r\n\r\n\r\n", "comments": ["I have some problems to interpret the timelines (an in depth tutorial would be awsome! see #17076 and #1824 -> @prb12 ):\r\n\r\nA problem could be:\r\nfirst_1 is allocated at the same time as total_0. (in timeline_double)\r\nBut its only 145 MB so it shouldn't really matter. \r\n\r\nAnd the execution of third_0  at device:GPU:0/stream:all is at the same time as first_1 of  job:localhost/replica:0/task:0/device:GPU:0. \r\nNot sure what  device:GPU:0/stream:all  and job:localhost/replica:0/task:0/device:GPU:0 are, seems to be both on the GPU?\r\n\r\n![timeline](https://user-images.githubusercontent.com/1831252/36341421-4ebef0de-13ee-11e8-8166-477b6326b4e8.png)\r\n\r\n\r\n[timeline.zip](https://github.com/tensorflow/tensorflow/files/1733678/timeline.zip)\r\n", "Somehow related to the bug: Is there any operation to copy the memory back to cpu and free the gpu?", "As suggested by @yaroslavvb I used his mem_util script to further inspect the memory usage:\r\nThis is the output for -size 80000 (no OOM crash):\r\n```\r\nsingleExecution finished!\r\nPrinting timeline for /gpu:0\r\n-------------------------------------------\r\ntime[ms] |  total[MB] |   diff[MB] | kernel\r\n       0 |       0.00 |       0.00 | _SOURCE\r\n    1188 |       0.00 |       0.00 | strided_slice/stack\r\n    1204 |       0.00 |       0.00 | strided_slice/stack_1\r\n    1216 |       0.00 |       0.00 | strided_slice/stack_2\r\n    2233 |       0.00 |       0.00 | strided_slice\r\n  903921 |     115.97 |     115.97 | first_0\r\n 1098084 |     116.58 |       0.61 | first_0\r\n 1098092 |     117.19 |       0.61 | first_0\r\n 1098093 |     117.80 |       0.61 | first_0\r\n 1098340 |     117.19 |      -0.61 | first_0\r\n 1098895 |     116.58 |      -0.61 | first_0\r\n 1098898 |     115.97 |      -0.61 | first_0\r\n 1098949 |     115.97 |       0.00 | ExpandDims/dim\r\n 1098962 |     115.97 |       0.00 | xdt_0/multiples\r\n 1098967 |     115.97 |       0.00 | Const\r\n 1098972 |     115.97 |       0.00 | ExpandDims\r\n 1099000 |    7073.97 |    6958.01 | xdt_0\r\n 1099067 |    7110.77 |      36.80 | third_0\r\n 1102177 |    7111.38 |       0.61 | third_0\r\n 1102179 |    7111.99 |       0.61 | third_0\r\n 1102180 |    7112.60 |       0.61 | third_0\r\n 1144064 |    7111.99 |      -0.61 | third_0\r\n 1144065 |    7111.38 |      -0.61 | third_0\r\n 1144066 |    7110.77 |      -0.61 | third_0\r\n 1144072 |    6994.80 |    -115.97 | first_0\r\n 1144072 |      36.80 |   -6958.01 | xdt_0\r\n 1144098 |      36.80 |       0.00 | total_0\r\n 1144139 |      36.82 |       0.02 | total_0\r\n 1144171 |      36.80 |      -0.02 | total_0\r\n 1144177 |       0.00 |     -36.80 | third_0\r\n 1168509 |       0.00 |      -0.00 | total_0\r\nPeaks: \r\n7458102528\r\ndoubleExecution finished!\r\nPrinting timeline for /gpu:0\r\n-------------------------------------------\r\ntime[ms] |  total[MB] |   diff[MB] | kernel\r\n       0 |       0.00 |       0.00 | _SOURCE\r\n      81 |       0.00 |       0.00 | strided_slice/stack\r\n     124 |       0.00 |       0.00 | strided_slice/stack_1\r\n     139 |       0.00 |       0.00 | strided_slice/stack_2\r\n   10894 |       0.00 |       0.00 | strided_slice\r\n 1313920 |     115.97 |     115.97 | first_0\r\n 1320691 |     116.58 |       0.61 | first_0\r\n 1320695 |     117.19 |       0.61 | first_0\r\n 1320696 |     117.80 |       0.61 | first_0\r\n 1320969 |     117.19 |      -0.61 | first_0\r\n 1321476 |     116.58 |      -0.61 | first_0\r\n 1321479 |     115.97 |      -0.61 | first_0\r\n 1321510 |     115.97 |       0.00 | ExpandDims/dim\r\n 1321517 |     115.97 |       0.00 | xdt_0/multiples\r\n 1321521 |     115.97 |       0.00 | Const\r\n 1321525 |     115.97 |       0.00 | ExpandDims\r\n 1321537 |    7073.97 |    6958.01 | xdt_0\r\n 1321587 |    7110.60 |      36.62 | third_0\r\n 1325130 |    7111.21 |       0.61 | third_0\r\n 1325132 |    7111.82 |       0.61 | third_0\r\n 1325133 |    7112.43 |       0.61 | third_0\r\n 1366693 |    7111.82 |      -0.61 | third_0\r\n 1366695 |    7111.21 |      -0.61 | third_0\r\n 1366695 |    7110.60 |      -0.61 | third_0\r\n 1366702 |    6994.63 |    -115.97 | first_0\r\n 1366702 |      36.62 |   -6958.01 | xdt_0\r\n 1366717 |      36.62 |       0.00 | total_0\r\n 1366740 |      36.64 |       0.02 | total_0\r\n 1366812 |      36.62 |      -0.02 | total_0\r\n 1366819 |       0.00 |     -36.62 | third_0\r\n 1366822 |       0.00 |       0.00 | strided_slice_1/stack\r\n 1366828 |       0.00 |       0.00 | strided_slice_1/stack_1\r\n 1366832 |       0.00 |       0.00 | strided_slice_1/stack_2\r\n 1366838 |     115.97 |     115.97 | first_1\r\n 1369862 |     116.58 |       0.61 | first_1\r\n 1369863 |     117.19 |       0.61 | first_1\r\n 1369864 |     117.80 |       0.61 | first_1\r\n 1391237 |     117.19 |      -0.61 | first_1\r\n 1391238 |     116.58 |      -0.61 | first_1\r\n 1391239 |     115.97 |      -0.61 | first_1\r\n 1391255 |     115.97 |       0.00 | strided_slice_1\r\n 1391273 |     115.97 |       0.00 | ExpandDims_1/dim\r\n 1391278 |     115.97 |       0.00 | xdt_1/multiples\r\n 1391281 |     115.97 |       0.00 | Const_1\r\n 1391285 |     115.97 |       0.00 | ExpandDims_1\r\n 1391293 |    7073.97 |    6958.01 | xdt_1\r\n 1391321 |    7110.77 |      36.80 | third_1\r\n 1394347 |    7111.38 |       0.61 | third_1\r\n 1394348 |    7111.99 |       0.61 | third_1\r\n 1394349 |    7112.60 |       0.61 | third_1\r\n 1436988 |    7111.99 |      -0.61 | third_1\r\n 1436989 |    7111.38 |      -0.61 | third_1\r\n 1436989 |    7110.77 |      -0.61 | third_1\r\n 1436996 |    6994.80 |    -115.97 | first_1\r\n 1436996 |      36.80 |   -6958.01 | xdt_1\r\n 1437008 |      36.80 |       0.00 | total_1\r\n 1437017 |      36.82 |       0.02 | total_1\r\n 1437045 |      36.80 |      -0.02 | total_1\r\n 1437050 |       0.00 |     -36.80 | third_1\r\n 1437053 |       0.00 |       0.00 | add\r\n 1437117 |       0.00 |      -0.00 | total_1\r\n 1461492 |       0.00 |      -0.00 | total_0\r\n```\r\nA few things to note:\r\nthe order of the operations is as expected and looks good\r\nmemory consumption of first is not expected, no memory for the input tensor Xder is allocated (or reported)\r\nbeside first, memory consumption looks mostly as expected\r\n\r\nLog for the single run before the OOM-crash (with -size 90000 or more):\r\n\r\n```\r\nsingleExecution finished!\r\nPrinting timeline for /gpu:0\r\n-------------------------------------------\r\ntime[ms] |  total[MB] |   diff[MB] | kernel\r\n       0 |       0.00 |       0.00 | _SOURCE\r\n    1568 |       0.00 |       0.00 | strided_slice/stack\r\n    1584 |       0.00 |       0.00 | strided_slice/stack_1\r\n    1622 |       0.00 |       0.00 | strided_slice/stack_2\r\n  978725 |       0.00 |       0.00 | strided_slice\r\n  978781 |     130.46 |     130.46 | first_0\r\n 1228768 |     131.15 |       0.69 | first_0\r\n 1228776 |     131.84 |       0.69 | first_0\r\n 1228778 |     132.52 |       0.69 | first_0\r\n 1229123 |     131.84 |      -0.69 | first_0\r\n 1229758 |     131.15 |      -0.69 | first_0\r\n 1229760 |     130.46 |      -0.69 | first_0\r\n 1229818 |     130.46 |       0.00 | ExpandDims/dim\r\n 1229833 |     130.46 |       0.00 | xdt_0/multiples\r\n 1229839 |     130.46 |       0.00 | Const\r\n 1229845 |     130.46 |       0.00 | ExpandDims\r\n 1229876 |    7999.42 |    7868.96 | xdt_0\r\n 1229955 |    8040.62 |      41.20 | third_0\r\n 1235602 |    8041.31 |       0.69 | third_0\r\n 1235605 |    8041.99 |       0.69 | third_0\r\n 1235606 |    8042.68 |       0.69 | third_0\r\n 1280551 |    8041.99 |      -0.69 | third_0\r\n 1280553 |    8041.31 |      -0.69 | third_0\r\n 1280553 |    8040.62 |      -0.69 | third_0\r\n 1280560 |     171.66 |   -7868.96 | xdt_0\r\n 1280561 |      41.20 |    -130.46 | first_0\r\n 1280586 |      41.20 |       0.00 | total_0\r\n 1280617 |      41.22 |       0.02 | total_0\r\n 1280652 |      41.20 |      -0.02 | total_0\r\n 1280660 |       0.00 |     -41.20 | third_0\r\n 1308070 |       0.00 |      -0.00 | total_0\r\nPeak: \r\n8433360384\r\n```\r\nI see no reason why the following execution should fail? Somebody any idea?\r\n\r\n\r\n", "I extended the mem_util script to display the timeline name of state_stats and the operations of the partitions_graph. As far as I understand is the partitions_graph the optimized operations graph and should include all executed (?). Unfortunately it does not contain any timestamps or memory information.\r\nHere is the output for the not OOM-execution with -size 70000:\r\n\r\n```\r\ndoubleExecution finished!\r\nPrinting timeline for /gpu:0\r\n-------------------------------------------------------\r\ntime[ms] |  total[MB] |   diff[MB] |          kernel | timeline_label\r\n       0 |       0.00 |       0.00 | _SOURCE         | _SOURCE = NoOp()\r\n      82 |       0.00 |       0.00 | strided_slice/stack | strided_slice/stack = Const()\r\n     101 |       0.00 |       0.00 | strided_slice/stack_1 | strided_slice/stack_1 = Const()\r\n     110 |       0.00 |       0.00 | strided_slice/stack_2 | strided_slice/stack_2 = Const()\r\n 1061612 |     101.47 |     101.47 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1061643 |     101.47 |       0.00 | strided_slice   | strided_slice = StridedSlice(_arg_XDerivation2_0_0/_13, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)\r\n 1072556 |     102.01 |       0.53 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1072560 |     102.54 |       0.53 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1072561 |     103.07 |       0.53 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1072887 |     102.54 |      -0.53 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1073494 |     102.01 |      -0.53 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1073495 |     101.47 |      -0.53 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1073522 |     101.47 |       0.00 | ExpandDims/dim  | ExpandDims/dim = Const(^first_0)\r\n 1073530 |     101.47 |       0.00 | xdt_0/multiples | xdt_0/multiples = Const(^first_0)\r\n 1073533 |     101.47 |       0.00 | Const           | Const = Const(^first_0)\r\n 1073536 |     101.47 |       0.00 | ExpandDims      | ExpandDims = ExpandDims(strided_slice, ExpandDims/dim)\r\n 1073548 |    6189.73 |    6088.26 | xdt_0           | [GPU_0_bfc 6088.3MB 6088.3MB] xdt_0 = Tile(ExpandDims, xdt_0/multiples)\r\n 1073600 |    6221.77 |      32.04 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1077343 |    6222.31 |       0.53 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1077346 |    6222.84 |       0.53 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1077347 |    6223.37 |       0.53 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1112893 |    6222.84 |      -0.53 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1112895 |    6222.31 |      -0.53 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1112895 |    6221.77 |      -0.53 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1112903 |     133.51 |   -6088.26 | xdt_0           | [GPU_0_bfc 6088.3MB 6088.3MB] xdt_0 = Tile(ExpandDims, xdt_0/multiples)\r\n 1112904 |      32.04 |    -101.47 | first_0         | [GPU_0_bfc 103.1MB 103.1MB] first_0 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11)\r\n 1112920 |      32.04 |       0.00 | total_0         | total_0 = Sum(third_0, Const)\r\n 1112940 |      32.07 |       0.02 | total_0         | total_0 = Sum(third_0, Const)\r\n 1112985 |      32.04 |      -0.02 | total_0         | total_0 = Sum(third_0, Const)\r\n 1112992 |       0.00 |     -32.04 | third_0         | [GPU_0_bfc 33.6MB 33.6MB] third_0 = BatchMatMul(xdt_0, first_0)\r\n 1112996 |       0.00 |       0.00 | strided_slice_1/stack | strided_slice_1/stack = Const(^total_0)\r\n 1113001 |       0.00 |       0.00 | strided_slice_1/stack_1 | strided_slice_1/stack_1 = Const(^total_0)\r\n 1113004 |       0.00 |       0.00 | strided_slice_1/stack_2 | strided_slice_1/stack_2 = Const(^total_0)\r\n 1113011 |     101.47 |     101.47 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1116695 |     102.01 |       0.53 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1116698 |     102.54 |       0.53 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1116698 |     103.07 |       0.53 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1134374 |     102.54 |      -0.53 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1134375 |     102.01 |      -0.53 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1134376 |     101.47 |      -0.53 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1134395 |     101.47 |       0.00 | strided_slice_1 | strided_slice_1 = StridedSlice(_arg_XDerivation2_0_0/_13, strided_slice_1/stack, strided_slice_1/stack_1, strided_slice_1/stack_2)\r\n 1134407 |     101.47 |       0.00 | ExpandDims_1/dim | ExpandDims_1/dim = Const(^first_1)\r\n 1134411 |     101.47 |       0.00 | xdt_1/multiples | xdt_1/multiples = Const(^first_1)\r\n 1134414 |     101.47 |       0.00 | Const_1         | Const_1 = Const(^first_1)\r\n 1134416 |     101.47 |       0.00 | ExpandDims_1    | ExpandDims_1 = ExpandDims(strided_slice_1, ExpandDims_1/dim)\r\n 1134425 |    6189.73 |    6088.26 | xdt_1           | [GPU_0_bfc 6088.3MB 6088.3MB] xdt_1 = Tile(ExpandDims_1, xdt_1/multiples)\r\n 1134458 |    6221.77 |      32.04 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1138132 |    6222.31 |       0.53 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1138134 |    6222.84 |       0.53 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1138135 |    6223.37 |       0.53 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1174411 |    6222.84 |      -0.53 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1174412 |    6222.31 |      -0.53 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1174413 |    6221.77 |      -0.53 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1174422 |    6120.30 |    -101.47 | first_1         | [GPU_0_bfc 103.1MB 103.1MB] first_1 = BatchMatMul(_arg_XDerivation_0_1/_9, _arg_alpha_0_2/_11, ^total_0)\r\n 1174422 |      32.04 |   -6088.26 | xdt_1           | [GPU_0_bfc 6088.3MB 6088.3MB] xdt_1 = Tile(ExpandDims_1, xdt_1/multiples)\r\n 1174436 |      32.04 |       0.00 | total_1         | total_1 = Sum(third_1, Const_1)\r\n 1174447 |      32.07 |       0.02 | total_1         | total_1 = Sum(third_1, Const_1)\r\n 1174479 |      32.04 |      -0.02 | total_1         | total_1 = Sum(third_1, Const_1)\r\n 1174485 |       0.00 |     -32.04 | third_1         | [GPU_0_bfc 33.6MB 33.6MB] third_1 = BatchMatMul(xdt_1, first_1)\r\n 1174488 |       0.00 |       0.00 | add             | add = Add(total_0, total_1)\r\n 1174536 |       0.00 |      -0.00 | total_1         | total_1 = Sum(third_1, Const_1)\r\n 1195860 |       0.00 |      -0.00 | total_0         | total_0 = Sum(third_0, Const)\r\nPeak: \r\n6525680640\r\npartition_graphs:\r\nop: _arg_XDerivation2_0_0/_13      name: _Recv                on /gpu:0 input: [] \r\nop: strided_slice/stack            name: Const                on /gpu:0 input: [] \r\nop: strided_slice/stack_1          name: Const                on /gpu:0 input: [] \r\nop: strided_slice/stack_2          name: Const                on /gpu:0 input: [] \r\nop: strided_slice                  name: StridedSlice         on /gpu:0 input: ['_arg_XDerivation2_0_0/_13', 'strided_slice/stack', 'strided_slice/stack_1', 'strided_slice/stack_2'] \r\nop: _arg_alpha_0_2/_11             name: _Recv                on /gpu:0 input: [] \r\nop: _arg_XDerivation_0_1/_9        name: _Recv                on /gpu:0 input: [] \r\nop: first_0                        name: BatchMatMul          on /gpu:0 input: ['_arg_XDerivation_0_1/_9', '_arg_alpha_0_2/_11'] \r\nop: ExpandDims/dim                 name: Const                on /gpu:0 input: ['^first_0'] \r\nop: ExpandDims                     name: ExpandDims           on /gpu:0 input: ['strided_slice', 'ExpandDims/dim'] \r\nop: xdt_0/multiples                name: Const                on /gpu:0 input: ['^first_0'] \r\nop: xdt_0                          name: Tile                 on /gpu:0 input: ['ExpandDims', 'xdt_0/multiples'] \r\nop: third_0                        name: BatchMatMul          on /gpu:0 input: ['xdt_0', 'first_0'] \r\nop: Const                          name: Const                on /gpu:0 input: ['^first_0'] \r\nop: total_0                        name: Sum                  on /gpu:0 input: ['third_0', 'Const'] \r\nop: strided_slice_1/stack          name: Const                on /gpu:0 input: ['^total_0'] \r\nop: strided_slice_1/stack_1        name: Const                on /gpu:0 input: ['^total_0'] \r\nop: strided_slice_1/stack_2        name: Const                on /gpu:0 input: ['^total_0'] \r\nop: strided_slice_1                name: StridedSlice         on /gpu:0 input: ['_arg_XDerivation2_0_0/_13', 'strided_slice_1/stack', 'strided_slice_1/stack_1', 'strided_slice_1/stack_2'] \r\nop: first_1                        name: BatchMatMul          on /gpu:0 input: ['_arg_XDerivation_0_1/_9', '_arg_alpha_0_2/_11', '^total_0'] \r\nop: ExpandDims_1/dim               name: Const                on /gpu:0 input: ['^first_1'] \r\nop: ExpandDims_1                   name: ExpandDims           on /gpu:0 input: ['strided_slice_1', 'ExpandDims_1/dim'] \r\nop: xdt_1/multiples                name: Const                on /gpu:0 input: ['^first_1'] \r\nop: xdt_1                          name: Tile                 on /gpu:0 input: ['ExpandDims_1', 'xdt_1/multiples'] \r\nop: third_1                        name: BatchMatMul          on /gpu:0 input: ['xdt_1', 'first_1'] \r\nop: Const_1                        name: Const                on /gpu:0 input: ['^first_1'] \r\nop: total_1                        name: Sum                  on /gpu:0 input: ['third_1', 'Const_1'] \r\nop: add                            name: Add                  on /gpu:0 input: ['total_0', 'total_1'] \r\nop: add/_14                        name: _Send                on /gpu:0 input: ['add'] \r\n```\r\nI dont really get new information from it. Optimization seems not to change much in this case.\r\nThe partitions graphs includes the Placeholder input, so thats good, but not sure how memory is allocated for it?\r\nThe only strange thing is\r\n`op: first_1                        name: BatchMatMul          on /gpu:0 input: ['_arg_XDerivation_0_1/_9', '_arg_alpha_0_2/_11', '^total_0']`  \r\nwhy has the second \"first\"-op total_0 as an input? But I dont have any clue what the ^ represents :/", "One explanation would be, if the placeholder data for XDerivation is copied to the GPU and in the case of single execution cleared after the execution of op \"first\".\r\nIn the case of double execution the data is not cleared and allocation for the tile-operation fails.\r\n=> Is there a way to force tf to clear the placeholder data after the first operation?\r\nwhy contains the log no information about allocation for the placeholder?\r\n\r\n", "Changing the op to \r\n```\r\ndef op(alpha, Xder, Xdertest, i):\r\n      with tf.device('/cpu:0'):\r\n            Xder = tf.identity(Xder, name='copyOfXder_{}'.format(i))\r\n      cols = size #tf.shape(X)[0]\r\n      first = tf.matmul(Xder, alpha, transpose_a=True, name=\"first_{}\".format(i))            # cols x num_des x 1  \r\n      with tf.control_dependencies([first]):\r\n            xdt = tf.tile(tf.expand_dims(Xdertest,0), [cols,1,1], name=\"xdt_{}\".format(i))   # cols x num_dim x num_des\r\n            third = tf.matmul(xdt, first, name=\"third_{}\".format(i))                         # cols x num_dim x 1\r\n            total = tf.reduce_sum(third, name=\"total_{}\".format(i))                          # single number\r\n      return total \r\n```\r\nwill result in the expected behavior: xder is copied for every repetition. This allows the execution up to -size 16k as it should.\r\n\r\nAfter talking with myself for two days now, my problem is mostly solved. But if some of the mods reviews this, I think the issue shouldn't be closed. First the following should be changed:\r\n* report_tensor_allocations_upon_oom should be fixed that it includes memory of placeholders\r\n* a tool like mem_util should be included in tf-core - could be mixed with timeline?\r\n\r\nI am not sure how stable my solution is. Could be that the tf.identity-op is removed by the optimizer in some cases. => A \"clear memory on GPU\" op could be useful\r\n@yaroslavvb Thanks again for you help. I found your work about memory reduction: https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9\r\nBut I am not sure how you implement the poor memory strategy. Is there a way to tell tf to forget the result of an op, even though he uses it later on again? Or did you rewrite the operations graph so that it does not include references to the first operations?  \r\n", "Some ideas for predictable memory usage:\r\n1. `export TF_CUDNN_USE_AUTOTUNE=0` to remove performance tuning that takes (unpredictable) amounts of extra memory. \r\n2. Remove other optimizations\r\n```\r\n  optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\r\ngraph_options=tf.GraphOptions(optimizer_options=optimizer_options))\r\n  config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\r\n  config.graph_options.place_pruned_graph = True\r\n  sess= tf.Session(config=config)\r\n```\r\n\r\n3. TensorFlow will forget outputs of an op as soon as all of its consumers have executed. So you have to connect some of the original consumers of an op to the copy of an op (using graph editor), this will cause outputs to be forgotten/then recomputed\r\n4. About placeholder....this op is somewhat special, I'm guessing the memory gets allocated by runtime in a different code path from other ops and may not get captured by `mem_util`. However, I recall seeing memory allocations from it in the `LOG_MEMORY` messages", "Thanks for your reply.\r\nKnowing about the optimization is probably useful, even though I would like to find a solution with optimization turned on. But I should probably check how much effect the optimization has.\r\n\r\nPlaceholder produce some entries in the state_stats protobuf, but as far as I can tell only for the CPU.\r\nThe gpu-stream creates a lot of mesages instead, but they do not include a free message and do not explicitly denote the placeholder.\r\nAdding allocation messages for the target of the streams might be a solution for this issue, or at least a starting point.\r\nThe LOG_MEMORY messages are nice, but I don't think its a good solution to parse them to get basic information about allocation and frees. \r\nAnd report_tensor_allocations_upon_oom is a flag that should definitely work without the LOG.\r\n\r\nI had a short look in the code for this but the tensorflow codebase is quite huge. A lot of the code for ops seems to be scattered in different files. Is there any tutorial or overview for the internal structure of the tf code? \r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 72 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 88 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17091, "title": "Build Android on Windows failed", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: N/A\r\n- **OS Platform**: Windows10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: 6.3\r\n- **CUDA/cuDNN version**: CUDA v9.1\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI saw your instruction on [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android](url) , but I want to replace the model in the demo to my retrained model. Is there anyone know how to do that? If my description is not clear enough, please tell me. Thanks in advance.\r\n\r\n### Source code / logs\r\nError:Execution failed for task ':buildNativeMake'.\r\n> A problem occurred starting process 'command 'tensorflow/contrib/makefile/build_all_android.sh''", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution", "@tensorflowbutler Thank you for your reply. I updated the fields. I saw other issues about building TensorFlow Android on Windows, but it seems nobody solved it, so I am wondering if anybody finds a new solution to this problem.", "@zeshaoaaa We currently don't support building the TensorFlow library on Windows. \r\n\r\nHowever, we do provide a precompiled AAR download that should work in your case if you set the build type in build.gradel to `none`. This will remove the need to build native .so libraries locally.\r\n\r\n"]}, {"number": 17090, "title": "XLA CHECK-fails when using tf.random_normal op", "body": "this issue is about XLA core dump similar to #12683\r\n\r\n### code\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nD = 2\r\nA = tf.random_normal(shape=[D, D, 2], dtype=tf.float32,name=\"A\")\r\nB = tf.random_normal(shape=[D, D, 2], dtype=tf.float32, name=\"B\")\r\nif len(sys.argv)%2:\r\n    fun = tf.random_normal\r\nelse:\r\n    fun = tf.ones\r\nE = fun(shape=[D], dtype=tf.float32, name=\"EBA\")\r\nH = tf.reshape(tf.constant([[0.25,0,0,0],[0,-0.25,0.5,0],[0,0.5,-0.25,0],[0,0,0,0.25]],\r\n                           dtype=tf.float32),[2,2,2,2],name=\"Hamiltonian\")\r\nEA = tf.multiply(A,tf.reshape(E,[D,1,1]))\r\nAB = tf.tensordot(EA,B,[[1],[0]],name=\"AB\")\r\nS, U, V = tf.svd(tf.reshape(AB,[2*D,2*D]))\r\nUU = tf.transpose(tf.multiply(tf.reshape(U[:,:D],[D,2,D]),tf.reshape(E,[D,1,1])),[0,2,1],name=\"nA\")\r\ndata = UU / tf.reduce_max(UU)\r\nconfig = tf.ConfigProto()\r\nif len(sys.argv)>2:\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run(data))\r\n```\r\n\r\n### output\r\n```\r\n \u2718 hzhangxyz@zhanghao \ue0b0 ~/Documents/test \ue0b0 LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py\r\n2018-02-17 17:13:19.904413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-17 17:13:19.904879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2018-02-17 17:13:19.904896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0\r\n2018-02-17 17:13:20.109302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-02-17 17:13:20.109333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-02-17 17:13:20.109342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-02-17 17:13:20.109495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\n2018-02-17 17:13:20.292794: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55eaa51abab0\r\n[[[-0.3788082  -0.63152224]\r\n  [ 0.69478613 -0.15805985]]\r\n\r\n [[-0.12801631 -1.0104226 ]\r\n  [ 1.         -0.25424612]]]\r\n hzhangxyz@zhanghao \ue0b0 ~/Documents/test \ue0b0 LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py -\r\n2018-02-17 17:13:26.235761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-17 17:13:26.236224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2018-02-17 17:13:26.236250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0\r\n2018-02-17 17:13:26.438962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-02-17 17:13:26.439018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-02-17 17:13:26.439028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-02-17 17:13:26.439205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\n2018-02-17 17:13:26.629210: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x564cf97d4cf0\r\n[[[-0.95954746 -0.42915264]\r\n  [-0.46691963 -0.69855815]]\r\n\r\n [[-0.7818373   0.17119625]\r\n  [ 1.          0.19870493]]]\r\n hzhangxyz@zhanghao \ue0b0 ~/Documents/test \ue0b0 LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py - -\r\n2018-02-17 17:13:30.150087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-17 17:13:30.150570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2018-02-17 17:13:30.150595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0\r\n2018-02-17 17:13:30.354220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-02-17 17:13:30.354264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-02-17 17:13:30.354271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-02-17 17:13:30.354448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\n2018-02-17 17:13:30.413176: I tensorflow/compiler/xla/service/service.cc:158] XLA service 0x7f05400021d0 executing computations on platform CUDA. Devices:\r\n2018-02-17 17:13:30.413215: I tensorflow/compiler/xla/service/service.cc:166]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0\r\n2018-02-17 17:13:30.555655: W tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:351] *** WARNING *** You are using ptxas 9.0.176, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n2018-02-17 17:13:30.945780: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55a05a6aa980\r\n2018-02-17 17:13:31.100741: F tensorflow/compiler/xla/util.cc:187] Check failed: p1.size() == p2.size() (3 vs. 0)\r\n[1]    17796 abort (core dumped)  LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py - -\r\n \u2718 hzhangxyz@zhanghao \ue0b0 ~/Documents/test \ue0b0 LD_LIBRARY_PATH=/opt/cuda-9.0/lib64 python2 main.py - - -\r\n2018-02-17 17:13:36.566031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-02-17 17:13:36.566533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1331] Found device 0 with properties:\r\nname: GeForce GTX 950M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.91GiB\r\n2018-02-17 17:13:36.566560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1410] Adding visible gpu devices: 0\r\n2018-02-17 17:13:36.780285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-02-17 17:13:36.780327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-02-17 17:13:36.780334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-02-17 17:13:36.780575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3654 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:0a:00.0, compute capability: 5.0)\r\n2018-02-17 17:13:36.838113: I tensorflow/compiler/xla/service/service.cc:158] XLA service 0x7f4490001fd0 executing computations on platform CUDA. Devices:\r\n2018-02-17 17:13:36.838151: I tensorflow/compiler/xla/service/service.cc:166]   StreamExecutor device (0): GeForce GTX 950M, Compute Capability 5.0\r\n2018-02-17 17:13:36.856927: W tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:351] *** WARNING *** You are using ptxas 9.0.176, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n2018-02-17 17:13:37.011879: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x557af19c7660\r\n[[[-4.4751650e-01 -2.8206566e-01]\r\n  [ 8.2662034e-01 -5.5483520e-01]]\r\n\r\n [[ 1.0000000e+00 -8.6649950e-04]\r\n  [ 2.1385331e-01  4.9290603e-01]]]\r\n```\r\n\r\n# system info\r\n\r\n### output of tf_env_collect.sh\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux zhanghao 4.15.3-2-ARCH #1 SMP PREEMPT Thu Feb 15 00:13:49 UTC 2018 x86_64 GNU/Linux\r\nLSB_VERSION=1.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 7.3.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux zhanghao 4.15.3-2-ARCH #1 SMP PREEMPT Thu Feb 15 00:13:49 UTC 2018 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ntensorflow (1.5.0)\r\ntensorflow-tensorboard (1.5.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0-rc0\r\ntf.GIT_VERSION = v1.6.0-rc1-167-g56422034fe\r\ntf.COMPILER_VERSION = v1.6.0-rc1-167-g56422034fe\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /opt/cuda-9.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSat Feb 17 18:25:00 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.25                 Driver Version: 390.25                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 950M    Off  | 00000000:0A:00.0 Off |                  N/A |\r\n| N/A   50C    P0    N/A /  N/A |      0MiB /  4046MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\ntf is compiled from source (56422034fe)\r\nwith cuda, with xla, with opt, with mkl, without anything else\r\n\r\n### OS is archlinux: \r\n```\r\nLinux zhanghao 4.15.3-2-ARCH #1 SMP PREEMPT Thu Feb 15 00:13:49 UTC 2018 x86_64 GNU/Linux\r\n```\r\n\r\n### python\r\n```\r\nPython 2.7.14 (default, Jan  5 2018, 10:41:29)\r\n[GCC 7.2.1 20171224] on linux2\r\n```\r\n\r\n### bazel\r\n```\r\n..............................................................\r\nBuild label: 0.10.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 18 09:24:24 +50064 (1517714702664)\r\nBuild timestamp: 1517714702664\r\nBuild timestamp as int: 1517714702664\r\n```\r\n\r\n### GPU\r\ncuda 9.0, cudnn 7.0.5\r\nGPU: GeForce GTX 950M\r\n\r\n### Describe the problem\r\ncore dump when use xla with gpu\r\n\r\n@jlebar @bixia1 ", "comments": ["BTW, when I test tensorflow 1.5, the situation is the same to #12683, (it is installed from pacman, I don't know which git version it is)\r\n\r\nand for (56422034fe039d7e8a57392957ed0c8b4b4efe56), the core dump can reproduced when chaning `ones` to `random_normal`", "This isn't related to your bug, but note the warning\r\n\r\n> *** WARNING *** You are using ptxas 9.0.176, which is in range [9.0.0, 9.0.276) + [9.1.0, 9.1.121). These versions are known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nThis is sadly not a joke.", "FYI @bixia1 is actively working on this, should have a fix soon.  It is at once simple and unfortunately complicated.  :)"]}, {"number": 17089, "title": " How to output * LIB static library", "body": "I want to develop on Windows with VS 2017", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "By \"develop\", do you mean develop on the TensorFlow codebase itself, or use TensorFlow as a library for your own project?\r\n\r\nIf the former, then at this time, the only instructions for building TensorFlow from source on Windows are at https://www.tensorflow.org/install/install_sources. We do not have instructions for Visual Studio 2017 and do not have the bandwidth to support that. Instead, we look to the community for that support.\r\n"]}, {"number": 17088, "title": "Can't stop TF from printing probabilities ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using the tutorial code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**:  3.6.4\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: python cnn_mnist.py\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI can't stop TF from printing the \"INFO:tensorflow:probabilities = []\" part, where in the angle brackets is a list that spans for several hundred lines. Trying to suppress the verbosity either does nothing or causes lines like \"INFO:tensorflow:loss = 2.314889, step = 2\" to stop appearing as well. \r\n\r\n### Source code / logs\r\n\r\nI used the source code from this page: https://www.tensorflow.org/tutorials/layers\r\n", "comments": ["How about using tf.logging.set_verbosity(tf.logging.ERROR)? "]}]