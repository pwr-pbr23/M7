[{"number": 20324, "title": "1.9-cherry-pick-request: Doc updates for get_started page and nav", "body": "This is a docs only change.\r\n\r\n* Fix section links on get_started landing page 388a267\r\n* Explicit TOC names for long notebook headlines 9f4fbdb\r\n", "comments": []}, {"number": 20323, "title": "Fix XLA header issue for pip build.", "body": "", "comments": ["This and the eigen roll are the last two things we need for rc2"]}, {"number": 20322, "title": "Fix leftnav for get_started", "body": "", "comments": []}, {"number": 20321, "title": "Branch 202187999", "body": "", "comments": ["Doing another push since I want to cherry-pick the StatusOr change to 1.9. Thanks!"]}, {"number": 20320, "title": "_dataset_ops.so not found", "body": "Hi, \r\n\r\nTrying to run this code https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a\r\n\r\nHowever, I keep running into an error:\r\n\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-8-c79e12159a4e> in <module>()\r\n----> 1 g = generator(noise, keep_prob, is_training)\r\n      2 print(g)\r\n      3 d_real = discriminator(X_in)\r\n      4 d_fake = discriminator(g, reuse=True)\r\n      5 \r\n\r\n<ipython-input-7-61000d187a2b> in generator(z, keep_prob, is_training)\r\n     42         x = tf.layers.dense(x, units=d1 * d1 * d2, activation=activation)\r\n     43         x = tf.layers.dropout(x, keep_prob)\r\n---> 44         x = tf.contrib.layers.batch_norm(x, is_training=is_training, decay=momentum)\r\n     45 \r\n     46         x = tf.reshape(x, shape=[-1, d1, d1, d2])\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py in __getattr__(self, item)\r\n     51 \r\n     52   def __getattr__(self, item):\r\n---> 53     module = self._load()\r\n     54     return getattr(module, item)\r\n     55 \r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py in _load(self)\r\n     40   def _load(self):\r\n     41     # Import the target module and insert it into the parent's namespace\r\n---> 42     module = importlib.import_module(self.__name__)\r\n     43     self._parent_module_globals[self._local_name] = module\r\n     44 \r\n\r\nD:\\anaconda\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    124                 break\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n    128 \r\n\r\nD:\\anaconda\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nD:\\anaconda\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nD:\\anaconda\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nD:\\anaconda\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nD:\\anaconda\\lib\\importlib\\_bootstrap_external.py in exec_module(self, module)\r\n\r\nD:\\anaconda\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\__init__.py in <module>()\r\n     32 from tensorflow.contrib import crf\r\n     33 from tensorflow.contrib import cudnn_rnn\r\n---> 34 from tensorflow.contrib import data\r\n     35 from tensorflow.contrib import deprecated\r\n     36 from tensorflow.contrib import distribute\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\data\\__init__.py in <module>()\r\n     65 from tensorflow.contrib.data.python.ops.counter import Counter\r\n     66 from tensorflow.contrib.data.python.ops.enumerate_ops import enumerate_dataset\r\n---> 67 from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n     68 from tensorflow.contrib.data.python.ops.get_single_element import get_single_element\r\n     69 from tensorflow.contrib.data.python.ops.grouping import bucket_by_sequence_length\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\error_ops.py in <module>()\r\n     18 from __future__ import print_function\r\n     19 \r\n---> 20 from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n     21 from tensorflow.contrib.data.python.ops import gen_dataset_ops\r\n     22 from tensorflow.python.data.ops import dataset_ops\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\contrib_op_loader.py in <module>()\r\n     22 \r\n     23 _dataset_ops = loader.load_op_library(\r\n---> 24     resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py in load_op_library(path)\r\n     54       return None\r\n     55   path = resource_loader.get_path_to_datafile(path)\r\n---> 56   ret = load_library.load_op_library(path)\r\n     57   assert ret, 'Could not load %s' % path\r\n     58   return ret\r\n\r\nD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py in load_op_library(library_filename)\r\n     54     RuntimeError: when unable to load the library or get the python wrappers.\r\n     55   \"\"\"\r\n---> 56   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n     57 \r\n     58   op_list_str = py_tf.TF_GetOpList(lib_handle)\r\n\r\nNotFoundError: D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\..\\..\\_dataset_ops.so not found\r\n\r\nI'm running Python 3.6 and have same problem on Windows 10 and Mac Osx. Tensorflow is latest available through Anaconda. Cuda 9.0. Tensorflow 1.8.0\r\n\r\nThis error also affects my efforts to look at MNIST tutorials for commands such as:\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code? Yes - https://towardsdatascience.com/implementing-a-generative-adversarial-network-gan-dcgan-to-draw-human-faces-8291616904a\r\nOS Platform and Distribution - Mac OSX and Windows 10\r\nTensorFlow installed from - Anaconda\r\nTensorFlow version - 1.8.0\r\nBazel version - N/A\r\nCUDA/cuDNN version - 9.0 (or NA for Mac OSX)\r\nGPU model and memory - NVIDIA GeForce GTX 650 Ti Boost - 2 GB video. 16 GB RAM (Mac OSX no GPU, 16GB RAM)\r\nExact command to reproduce", "+1 I get the same error when trying to load tensorboard. \r\n\r\nOS Platform and Distribution - Windows 10\r\nTensorFlow installed from - pip\r\nTensorFlow version - 1.8.0\r\nBazel version - N/A\r\nCUDA/cuDNN version - 9.0 \r\nGPU model and memory -NVIDIA Quadro P2000, 5GB\r\nExact command to reproduce:\r\n\r\ncallbacks = list()\r\ncallbacks.append(TensorBoard(log_dir=save_path_logs, write_graph=True, write_images=False))\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2018.1.4\\helpers\\pydev\\pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2018.1.4\\helpers\\pydev\\pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2018.1.4\\helpers\\pydev\\pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2018.1.4\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Users/PHWE/Documents/gits/ia_fracture_detection/detect_fracs/run_training.py\", line 89, in <module>\r\n    callbacks=get_callbacks(config, n_patches=len(map_train)),\r\n  File \"C:\\Users\\PHWE\\Documents\\gits\\ia_fracture_detection\\detect_fracs\\callbacks.py\", line 72, in get_callbacks\r\n    callbacks.append(TensorBoard(log_dir=save_path_logs, write_graph=True, write_images=False))\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\keras\\callbacks.py\", line 709, in __init__\r\n    from tensorflow.contrib.tensorboard.plugins import projector\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 34, in <module>\r\n    from tensorflow.contrib import data\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\contrib\\data\\__init__.py\", line 67, in <module>\r\n    from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\error_ops.py\", line 20, in <module>\r\n    from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\contrib_op_loader.py\", line 24, in <module>\r\n    resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py\", line 56, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: C:\\Users\\PHWE\\AppData\\Local\\Continuum\\miniconda3\\envs\\ia_fracture_detection\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\..\\..\\_dataset_ops.so not found", "This solved it for me: \r\n\r\nhttps://stackoverflow.com/questions/50609845/tensorflow-1-8-0-with-python-3-6-4-in-anaconda-show-error-dataset-ops-so-not-fo/50942347\r\n\r\n\"follow this path and locate _dataset_ops.so\r\n\r\nD:\\programfiles\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\contrib\\data\r\n\r\nthen move _dataset_ops.so file out of that folder to another location.\"", "@gunan, it seems contrib ops are not loading in  Windows. Could you PTAL?", "@guschmue @mrry looks like contrib op loading is broken?\r\nSoon this should not be a problem as we will switch to using bazel to build our windows binaries.\r\nBut if we have a quick idea about what is wrong, we may be able to triage this.", "@gunan The file `_dataset_ops.so` should not be generated in the Windows/CMake build: those kernels are statically linked into the main Python extension DLL. The [StackOverflow answer](https://stackoverflow.com/questions/50609845/tensorflow-1-8-0-with-python-3-6-4-in-anaconda-show-error-dataset-ops-so-not-fo/50942347) suggests that the file *is* being generated, and moving it away should fix things.\r\n\r\nPerhaps one of the PIP packages is broken (e.g. for TF 1.8, since that seems to be a common factor)?", "@case540 @av8ramit \r\nIs it possible that one of our recent changes are causing windows pip packages to include .so files?", "follow the path and locate _dataset_ops.so\r\n\r\n\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\contrib\\data\r\n\r\nthen move _dataset_ops.so file out of that folder to another location.\r\n\r\nI got the same issue and doing this solved the issue.", "@case540 @av8ramit ping!", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue now that we moved to using bazel for windows?", "I'm not sure with regards to the .so files. I haven't made any modifications.", "ClosingAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", ">           follow the path and locate _dataset_ops.so\r\n> \\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\contrib\\data\r\n> then move _dataset_ops.so file out of that folder to another location.\r\n> I got the same issue and doing this solved the issue.\r\n\r\nTks!!!! Work fine!", "> follow the path and locate _dataset_ops.so\r\n> \r\n> \\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\contrib\\data\r\n> \r\n> then move _dataset_ops.so file out of that folder to another location.\r\n> \r\n> I got the same issue and doing this solved the issue.\r\n\r\nThx!! it works for me.. but is there any one knows what caused this error?!\r\ni got no this error just 3 hours ago. I have no idea about it.", "I am getting peculiar issue while running detector_utils.py. while hand_label_map.pbtxt is very much there in the specified folder, i am still getting following error - please help me how to resolve:\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-20-a486b1bd49eb> in <module>\r\n----> 1 label_map = hand_label_map_util.load_labelmap(PATH_TO_LABELS)\r\n\r\nC:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\utils\\hand_label_map_util.py in load_labelmap(path)\r\n    115     \"\"\"\r\n    116     with tf.gfile.GFile(path, 'r') as fid:\r\n--> 117         label_map_string = fid.read()\r\n    118         label_map = string_int_label_map_pb2.StringIntLabelMap()\r\n    119         try:\r\n\r\nC:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in read(self, n)\r\n    123       string if in string (regular) mode.\r\n    124     \"\"\"\r\n--> 125     self._preread_check()\r\n    126     with errors.raise_exception_on_not_ok_status() as status:\r\n    127       if n == -1:\r\n\r\nC:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py in _preread_check(self)\r\n     83       with errors.raise_exception_on_not_ok_status() as status:\r\n     84         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\r\n---> 85             compat.as_bytes(self.__name), 1024 * 512, status)\r\n     86 \r\n     87   def _prewrite_check(self):\r\n\r\nC:\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    517             None, None,\r\n    518             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 519             c_api.TF_GetCode(self.status.status))\r\n    520     # Delete the underlying status object from memory otherwise it stays alive\r\n    521     # as there is a reference to status from this from the traceback due to\r\n\r\nNotFoundError: NewRandomAccessFile failed to Create/Open: C:/Anaconda/envs/tf_gpu/Lib/site-packages/tensorflow/models/research/object_detection/utils/hand_detection_inference_graph/hand_label_map.pbtxt : The system cannot find the file specified.\r\n; No such file or directory", "> This solved it for me:\r\n> \r\n> https://stackoverflow.com/questions/50609845/tensorflow-1-8-0-with-python-3-6-4-in-anaconda-show-error-dataset-ops-so-not-fo/50942347\r\n> \r\n> \"follow this path and locate _dataset_ops.so\r\n> \r\n> D:\\programfiles\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\contrib\\data\r\n> \r\n> then move _dataset_ops.so file out of that folder to another location.\"\r\n\r\nI am having the same error using TensorFlow on win10 and installed from Anaconda with tensorflow version of 1.10. I tried to follow this method but there is no \"_dataset_ops.so\" file in that directory. There are only three folders of \"\\_\\_pychache\\_\\_\", \"kernels\", \"python\" and a file \"\\_\\_init\\_\\_.py\". Did I miss anything?"]}, {"number": 20319, "title": "DOC: add missing parenthesis in tf.keras docs", "body": "", "comments": ["THanks!"]}, {"number": 20318, "title": "Use tf_optimizer.OptimizeGraph to implement create_inference_graph", "body": "This PR also fixed two bugs:\r\n\r\n1. in `GetDeviceAndAllocator()` it didn't set `cuda_device_id` when device is found in cluster;\r\n2. in the same method if cluster is not available but device name is set in engine, it'll use the tf gpu id from the device name, but the tf_gpu_id->cuda_gpu_id mapping could be unset, in which case it'll failed. E.g. #20780\r\n\r\nI've run all available tests and verified that the generated graphdefs are the same as before.", "comments": ["Hi @samikama, I added the c++ interface, ptal. Thanks.", "+@pooyadavoodi as reviewer.", "Thanks for the review!"]}, {"number": 20317, "title": "Remove empty section links on get_started page", "body": "", "comments": []}, {"number": 20316, "title": "Branch 202152026", "body": "", "comments": ["Was a simple conflict in RELEASE.md fyi", "Retriggering window Bazel build. I keep on seeing this presubmit fail in a lot of places.", "Ok, looks like a new tf_lite test requires h5py python module."]}, {"number": 20315, "title": "R1.8", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20314, "title": "Fix source URLs", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Thanks for the fix @kaasasolut. Could you also sign the CLA?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 46 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 20313, "title": "Can't build tf-lite benchmark_model", "body": "\r\n\r\nHi all. I am trying to run tflite models on my desktop and for that trying to build benchmark models as given in the benchmark tools github page https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark . However when I run the command\r\n\r\n`bazel build -c opt tensorflow/contrib/lite/tools/benchmark:benchmark_model`\r\n\r\nPlease help me with this :) Thanks in advance.\r\nI encounter the following error -\r\nSystem information\r\n\r\n    OS Platform and Distribution (Linux Ubuntu):\r\n    TensorFlow installed from (source): Yes\r\n    **TensorFlow version **: 1.9.0rc1\r\n    Python version: 2.7\r\n    Bazel version (if compiling from source):\r\n    GCC/Compiler version (if compiling from source):\r\n    CUDA/cuDNN version: No CUDA\r\n    GPU model and memory:\r\n    Exact command to reproduce:\r\n\r\nDescribe the problem\r\n\r\nThe command I ran\r\n\r\nbazel build -c opt tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n\r\n**Source code / logs**\r\n\r\n**tensorflow/contrib/lite/profiling/profile_summarizer.cc: In function 'tflite::profiling::{anonymous}::OperatorDetails tflite::profiling::{anonymous}::GetOperatorDetails(const tflite::Interpreter&, int)':\r\ntensorflow/contrib/lite/profiling/profile_summarizer.cc:86:27: error: 'string' was not declared in this scope\r\ndetails.name += \":\" + string(profiling_string);\r\n^~~~~~\r\ntensorflow/contrib/lite/profiling/profile_summarizer.cc:86:27: note: suggested alternatives:\r\nIn file included from /usr/include/c++/7/iosfwd:39:0,\r\nfrom /usr/include/c++/7/memory:72,\r\nfrom ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\nfrom ./tensorflow/contrib/lite/allocation.h:25,\r\nfrom ./tensorflow/contrib/lite/interpreter.h:24,\r\nfrom ./tensorflow/contrib/lite/profiling/profile_summarizer.h:21,\r\nfrom tensorflow/contrib/lite/profiling/profile_summarizer.cc:16:\r\n/usr/include/c++/7/bits/stringfwd.h:74:33: note: 'std::__cxx11::string'\r\ntypedef basic_string string;\r\n^~~~~~\r\n/usr/include/c++/7/bits/stringfwd.h:74:33: note: 'std::__cxx11::string'\r\nTarget //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2.069s, Critical Path: 1.77s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully**\r\n", "comments": ["I'm having the same issues", "I created a patch to fix this and sent a PR https://github.com/tensorflow/tensorflow/pull/20330", "@freedomtan @achowdhery any ETA as to when it will be fixed and updated finally?\r\n", "@abhi-rf  no idea. You can apply the patch yourself.", "This should be std::string, our internal build aliases string -> std::string and so we didn't catch it internally. I am going to send a patch.", "PR #20330 looks good to me, I am going to approve it. Thanks @freedomtan  for the quick patch.", "PR #20330 is merged.", "Hi guys. I seem to be hitting a new issue now.\r\n\r\nC++ compilation of rule '//tensorflow/contrib/lite/kernels:eigen_support' failed (Exit 1)\r\ntensorflow/contrib/lite/kernels/eigen_support.cc:32:1: error: static assertion failed: kDefaultArenaAlignment doesn't comply with Eigen alignment requirement.\r\n static_assert(\r\n ^~~~~~~~~~~~~\r\nTarget //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n\r\nI am getting this in a few builds such as the lite/toco also. Kindly help.", "@abhi-rf it seems you had bad luck. You must be somewhere between ff4945f86e04d403cdf46c19392b2041bc75c2ad and 37cf4e0783ad06e6cfc94c98a77a48734190ed48. The problematic code introduced in ff4945f was reverted already."]}, {"number": 20312, "title": "Fix for RPi OpenBLAS build problem", "body": "Main branch version of https://github.com/tensorflow/tensorflow/pull/20310", "comments": ["Jenkins, test this please."]}, {"number": 20311, "title": "Could not find 'nvcuda.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\\Windows\\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "ImportError: could not find nvcuda.dll"]}, {"number": 20310, "title": " r1.9-rc2 cherry-pick request: Fix for RPi OpenBLAS compile issues, by pinning to known good version", "body": "OpenBLAS broke the Raspberry Pi build, and I wasn't pinning to a particular version before this, so it broke even the 'frozen' 1.9 branch.", "comments": ["Merged into 1.9. Ty for the fix!"]}, {"number": 20309, "title": "Very high discrepancy in memory usage and computation time in convolution operation between tf versions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\n    I attach a small code example that should explain the issue,  but in principle it is nothing but multiple 1x1 convolutions with a conv1D layer.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\n    VERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\n    VERSION_ID=\"16.04\"\r\n    VERSION_CODENAME=xenial\r\n\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\n    pip install tensorflow-gpu==1.3.0\r\n    and\r\n    pip install tensorflow-gpu==1.8.0\r\n\r\n- **TensorFlow version (use command below)**:\r\nThe following tf versions were used to recreate the issue\r\n    \r\n    tf.GIT_VERSION = v1.8.0-0-g93bc2e2072 \r\n    tf.VERSION = 1.8.0\r\n    and\r\n    tf.GIT_VERSION = b'unknown' \r\n    tf.VERSION = 1.3.0\r\n\r\n- **Python version**:\r\nPython 3.5.5 :: Anaconda, Inc.\r\n- **CUDA/cuDNN version**:\r\n    For tf 1.3.0:\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2016 NVIDIA Corporation\r\n    Built on Tue_Jan_10_13:22:03_CST_2017\r\n    Cuda compilation tools, release 8.0, V8.0.61\r\n    \r\n    For tf 1.8.0\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2017 NVIDIA Corporation\r\n    Built on Fri_Sep__1_21:08:03_CDT_2017\r\n    Cuda compilation tools, release 9.0, V9.0.176\r\n\r\n- **GPU model and memory**:\r\n    GeForce GTX 1080 Ti\r\n    total memory shown as 10.91GiB\r\n- **Exact command to reproduce**:\r\n    python 'script shown below'\r\n\r\n### Describe the problem\r\nWhen trying to update from tensorflow 1.3.0 to 1.8.0, we noticed that memory consumption and computation time increased significantly for our networks (both >10%). \r\nWe tried to find out what was causing this issue and realized that there is a large discrepancy for memory and computation time in our 1D convolutional layers, both increasing roughly by a factor of two when going from tf 1.3 to tf 1.8. \r\nSomething similar happens in the tf 1.3 version when changing the data format from NWC to NCW, but tf 1.8 is slow regardless of the data format.\r\nI attach a small code snippet, which creates a network of 30 1x1 1D convolutinal layers with some arbitrary parameters for batch size, width, and number of filters. \r\nThe network output is evaluated 10,000 times and the time spent for this is written to stdout, for different data formats and two different libraries for the 1D convolution (nn and layers). \r\nThe following output is generated for the different tf versions. \r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 5.840178s\r\ntf version 1.3.0 \t order NWC \t library nn\t time 5.809642s\r\ntf version 1.3.0 \t order NCW \t library layers\t time 10.344764s\r\ntf version 1.3.0 \t order NCW \t library nn\t time 9.630965s\r\n\r\ntf version 1.8.0 \t order NWC \t library layers\t time 13.982041s\r\ntf version 1.8.0 \t order NWC \t library nn\t time 13.616668s\r\ntf version 1.8.0 \t order NCW \t library layers\t time 11.336001s\r\ntf version 1.8.0 \t order NCW \t library nn\t time 10.919789s\r\n\r\n~~~~\r\n\r\nTracking memory consumption for this network with nvidia-smi showed that tf 1.8 allocates around 467MiB for all different configurations, whereas tf 1.3 allocates 223MiB for NWC data format and up to 449MiB for the NCW format.\r\n\r\nMoreover, we compared the outputs of the two different data formats. They appear to be the same.\r\n\r\n### Source code / logs\r\n~~~~\r\n#############################################\r\n## Created by Moritz Boehle moritz@audatic.ai\r\n#############################################\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\n# Exclude tf logs in output for better overview\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\n# Some arbitrary parameters for the network\r\nnum_filters = 16\r\nnum_layers = 30\r\nwidth = 100\r\nbatch_size = 20\r\nsteps = 10000\r\n\r\n# Allow GPU growth\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\n# Below, we will compare performance of the two different data formats.\r\nformats = [\"NWC\", \"NCW\"]\r\n\r\n# Tested with 1.3.0 and 1.8.0\r\ntf_v = tf.__version__\r\n\r\n# Different libs for performing 1D convolution.\r\nlibs = [\"layers\", \"nn\"]\r\n\r\n# Options for the two different data formats.\r\nncw_opts = {\"shape\": [batch_size, num_filters, width],\r\n            \"data_format_layers\": \"channels_first\",\r\n            \"data_format_nn\": \"NCW\" if tf_v == \"1.8.0\" else \"NCHW\"}\r\nnwc_opts = {\"shape\": [batch_size, width, num_filters],\r\n            \"data_format_layers\": \"channels_last\",\r\n            \"data_format_nn\": \"NWC\" if tf_v == \"1.8.0\" else \"NHWC\"}\r\ndata_format_opts = {\"NCW\": ncw_opts, \"NWC\": nwc_opts}\r\n\r\nfor data_format in formats:\r\n    opts = data_format_opts[data_format]\r\n    for lib in libs:\r\n        with tf.Session(config=config) as sess:\r\n            with tf.device(\"/gpu:0\"):\r\n                shape = opts[\"shape\"]\r\n                format_str = opts[\"data_format_\"+lib]\r\n\r\n                # Create arbitrary input.\r\n                layer = tf.constant(np.random.random(shape), dtype=tf.float32)\r\n\r\n                # Create num_layers of 1D convolutions.\r\n                for _ in range(num_layers):\r\n                    if lib == \"layers\":\r\n                        layer = tf.layers.conv1d(layer, filters=num_filters,\r\n                                                 kernel_size=[1], strides=[1],\r\n                                                 data_format=format_str)\r\n                    elif lib == \"nn\":\r\n                        filters = tf.Variable(tf.random_normal(\r\n                            [1, num_filters, num_filters]))\r\n                        bias = tf.Variable(tf.random_normal(shape))\r\n                        layer = tf.nn.conv1d(layer, filters, 1, \"VALID\",\r\n                                             data_format=format_str)\r\n                        layer += bias\r\n                sess.run(tf.global_variables_initializer())\r\n\r\n                # Measure time to run 'steps' steps.\r\n                start = time.time()\r\n                for i in range(steps):\r\n                    sess.run(layer)\r\n                total = time.time() - start\r\n                print(\"tf version {tf} \\t order {order} \\t library {lib}\"\r\n                      \"\\t time {total:f}s\".format(tf=tf_v, total=total,\r\n                                                  order=data_format, lib=lib))\r\n\r\n\r\n~~~~\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Thanks for the very detailed report @moboehle . Simply running your program on various versions (and looking at the timings, I didn't focus on memory yet), it seems this happened between versions 1.6 and 1.7:\r\n\r\n```\r\ntf version 1.3.0         order NWC       library layers  time 8.185683s\r\ntf version 1.3.0         order NWC       library nn      time 7.606029s\r\ntf version 1.3.0         order NCW       library layers  time 15.392093s\r\ntf version 1.3.0         order NCW       library nn      time 13.677395s\r\n\r\ntf version 1.4.0         order NWC       library layers  time 8.245580s\r\ntf version 1.4.0         order NWC       library nn      time 7.654847s\r\ntf version 1.4.0         order NCW       library layers  time 15.371269s\r\ntf version 1.4.0         order NCW       library nn      time 14.107784s\r\n\r\ntf version 1.5.0         order NWC       library layers  time 8.871874s\r\ntf version 1.5.0         order NWC       library nn      time 8.447964s\r\ntf version 1.5.0         order NCW       library layers  time 16.345905s\r\ntf version 1.5.0         order NCW       library nn      time 14.314268s\r\n\r\ntf version 1.6.0         order NWC       library layers  time 8.817178s\r\ntf version 1.6.0         order NWC       library nn      time 7.883244s\r\ntf version 1.6.0         order NCW       library layers  time 16.071247s\r\ntf version 1.6.0         order NCW       library nn      time 14.459396s\r\n\r\ntf version 1.7.0         order NWC       library layers  time 20.263543s\r\ntf version 1.7.0         order NWC       library nn      time 19.896915s\r\ntf version 1.7.0         order NCW       library layers  time 15.326541s\r\ntf version 1.7.0         order NCW       library nn      time 16.310148s\r\n\r\ntf version 1.8.0         order NWC       library layers  time 20.150042s\r\ntf version 1.8.0         order NWC       library nn      time 20.640730s\r\ntf version 1.8.0         order NCW       library layers  time 16.102693s\r\ntf version 1.8.0         order NCW       library nn      time 15.144002s\r\n```\r\n(I was using the release docker images).\r\n\r\nI believe one of the changes between 1.6 and 1.7 was that the layout optimizer was turned on by default\r\n(i.e., it had to be [turned on explicitly in 1.6](https://github.com/tensorflow/tensorflow/blob/080d59b76ca27b184f0fce605db7f5339ea5a8cf/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L100) and starting [1.7 had to be turned explicitly off](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L117)). Since `tf.nn.conv1d` is actually implemented as [a wrapper over the `conv2d` operation](https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/nn_ops.py#L2458), this \"optimization\" kicks in but is probably not actually helpful :)\r\n\r\n@rmlarsen @benoitsteiner @zhangyaobit - Please do take a look. Perhaps `LayoutOptimizer` needs to  be made aware of 1D convolutions and handle them differently?\r\n\r\n@moboehle - In the mean time, you could work around this problem by explicitly turning off the layout optimizer using something like:\r\n\r\n```python\r\nfrom google.protobuf import text_format\r\nconfig = from google.protobuf import text_format\r\nconfig = text_format.Parse(\"\"\"\r\ngraph_options {\r\n  rewrite_options {\r\n    layout_optimizer: OFF\r\n  }\r\n}\r\n\"\"\", tf.ConfigProto())\r\n```\r\n\r\nIf I do that, I see the following numbers in 1.7 and 1.8:\r\n```\r\ntf version 1.7.0         order NWC       library layers  time 8.370198s\r\ntf version 1.7.0         order NWC       library nn      time 7.873047s                      \r\ntf version 1.7.0         order NCW       library layers  time 15.633171s\r\ntf version 1.7.0         order NCW       library nn      time 13.691546s \r\n\r\ntf version 1.8.0         order NWC       library layers  time 9.071059s\r\ntf version 1.8.0         order NWC       library nn      time 8.561103s\r\ntf version 1.8.0         order NCW       library layers  time 16.833199s\r\ntf version 1.8.0         order NCW       library nn      time 14.826180s\r\n```\r\n\r\nHopefully this workaround tides you over for the short term and we'll look into an appropriate fix for how the layout optimizer handles 1D convolutions. I suspect the memory increase is also explained by the added transposition operation required for the layout change.\r\n\r\nThanks!", "Thanks @asimshankar for your quick response. I tried your fix and unfortunately, it seems to have messed up my current installation. I now get the following error\r\n~~~~\r\n2018-06-27 10:56:57.354254: E tensorflow/stream_executor/cuda/cuda_dnn.cc:393] possibly insufficient driver version: 384.130.0\r\n2018-06-27 10:56:57.354261: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-06-27 10:56:57.354266: F tensorflow/core/kernels/conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n~~~~\r\nThe only thing that happened since my post yesterday was including the lines of code you provided. Removing the lines or resetting the layout optimizer does not fix this error, CUDA seems to be messed up somewhat now.\r\nDo you have an idea what could be causing this?\r\n\r\nFurthermore, if I am not mistaken, this problem is not only related to the 1D convolution, the same computation time and memory increase occur in 2D convolutions, too. However, I cannot properly test this right now, since my installation is now faulty, but I remember seeing the same issue yesterday with 2D convolutions.\r\n\r\nLet me know if you need any extra information, thank you for your help!", "Thank you very much, my CuDNN must have crashed for an unrelated reason, I reinstalled it now and your fix works as promised :) \r\nThe memory consumption issue goes away, too.\r\nAnd contrary to what I said before, this only affects conv1D!\r\n\r\nBest regards and thank you", "Sorry for going back and forth, but my previous test of the conv2D was insufficiently thorough as it seems. For conv2D with a large height, the layout optimizer indeed boosts performance, as seen in the following output \r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 18.740896s\r\ntf version 1.5.0 \t order NWC \t library layers\t time 17.115397s\r\ntf version 1.8.0 \t order NWC \t library layers\t time 17.450639s\r\n~~~~\r\nHere, I only tested the data format NWC for an input of height 20 for the different tensorflow versions, and I explicitly turned on the optimizer also for tf 1.5.\r\n\r\nThe issue with the optimizer arises, once the height becomes too small, for example with height of 2 I get the following results:\r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 4.977753s\r\ntf version 1.5.0 \t order NWC \t library layers\t time 9.492385s\r\ntf version 1.8.0 \t order NWC \t library layers\t time 9.792400s\r\n~~~~\r\nwhich is what I remembered seeing when first testing this.\r\n\r\nMoreover, while for large heights the computation time is indeed decreased, it should be noted that the memory consumption suffers heavily from this optimization, which could be more costly than computation time for some users. E.g. for an input with width and height of 200, memory consumption almost triples(!) from 479MiB to 1359MiB in order to achieve a time speed-up of about 18% in the following output\r\n~~~~\r\ntf version 1.3.0 \t order NWC \t library layers\t time 31.275929s\r\ntf version 1.5.0 \t order NWC \t library layers\t time 25.768378s\r\ntf version 1.8.0 \t order NWC \t library layers\t time 26.140614s\r\n~~~~\r\n", "Thanks for the analysis @moboehle. \r\n\r\nI am under the impression that disabling the optimizer in certain cases is a sufficient workaround for now. Do let me know if that is incorrect.\r\n\r\nAnd we'll look into making the optimizer more robust.\r\n\r\nSound fair?\r\nOnce again, thanks for the report and analysis, it is very helpful.\r\n\r\n", "Yes, thanks again for the workaround! Just wanted to share my findings to help you fix the issue.\r\nCheers ", "Thanks @moboehle for the detailed report and @asimshankar helping on debugging the issue.\r\n\r\nWith 1x1 filter in NWC format, the underlying Conv implementation is GEMM-based, which could be faster than the non-GEMM-based Conv implementation in NCW format, as you have already observed. NCW is in general faster than NWC, but 1x1 filter is an exception (which we also observed previously).\r\n\r\nWe plan to add auto-tuning support in future for layout optimizer, which will address this issue (@zheng-xq can provide more details); currently it would use NCW layout for the whole model in a blanket manner. This is less ideal but ok for most models because (1) there are typically fewer conv layers with 1x1 filter than the conv layers with non-1x1 filters (2) the conv layers with 1x1 filter are also much cheaper computationally. But I think your model might be an exception, which extensively uses 1x1 filters.\r\n\r\nBefore the auto-tuning support, you have a few options to work around this: (1) turn off layout optimizer, and manually specify the data format for each conv, this will achieve the optimal result; it is just that it takes manual effort to implement, (2) either turn on or off layout optimizer considering the overall model-level speed and memory metrics; this produces less than optimal results, but at least give you some control.", "Nagging Assignee @zhangyaobit: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@moboehle Auto-tuning support has been added for layout optimizer, could you try the latest TensorFlow version and post any errors here.", "Closing, feel free to reopen if problem persists", "Hi @wt-huang, can you link to the commit that added auto-tuning? According to my experiments, manually disabling the layout optimizer is still required in TensorFlow 1.12 to get good performance in the scenario described in this issue."]}, {"number": 20308, "title": "Results are different on MacOS for fixed data", "body": "Hello.\r\n\r\nWe've encountered strange behaviour of TF (tested on versions 1.7.0 and 1.8.0) on MacOS (tested on 10.11.6) with Python 2.7 (tested on 2.7.10). While running this code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Model(object):\r\n    def __init__(self):\r\n        one_hot = tf.constant([[1, 0, 0, 1], [0, 1, 1, 2], [1, 1, 1, 3]])\r\n        logits = tf.constant([[0.3, 0.2, 0], [0.1, 0.0004, 0], [0, 0.02, 0.2]])\r\n\r\n        self.one_hot = one_hot\r\n        self.logits = logits\r\n        self.rec_at_k = self._recall_at_k()\r\n        self.prec_at_k = self._precision_at_k()\r\n\r\n        config = tf.ConfigProto()\r\n        config.gpu_options.allow_growth = True\r\n        self.sess = tf.Session(config=config)\r\n        self.sess.run(tf.global_variables_initializer())\r\n        self.sess.run(tf.local_variables_initializer())\r\n\r\n\r\n    def _recall_at_k(self, top=3):\r\n        recalls = []\r\n        for k in range(1,top+1):\r\n            recalls.append(tf.metrics.recall_at_k(tf.cast(self.one_hot, tf.int64), tf.cast(self.logits, tf.float32), k=k))\r\n        return recalls\r\n\r\n    def _precision_at_k(self, top=3):\r\n        precs = []\r\n        for k in range(1,top+1):\r\n            precs.append(tf.metrics.recall_at_k(tf.cast(self.one_hot, tf.int64), self.logits, k=k))\r\n        return precs\r\n\r\n    def fit(self):\r\n        recalls, precs = self.sess.run([self.rec_at_k, self.prec_at_k])\r\n        for rec,prec in zip(recalls,precs):\r\n            print rec[0],rec, prec\r\n        l,o = self.sess.run([self.logits, self.one_hot])\r\n        print l\r\n        print o, o[:,:-1]\r\n\r\n\r\nmodel = Model()\r\nmodel.fit()\r\n```\r\n\r\n\r\nwe receive results which are different from run to run, like:\r\n\r\n\r\n```\r\n0.2857142857142857 ('0.2857142857142857', '0.2857142857142857') ('1.0', '0.2857142857142857')\r\n0.7142857142857143 ('0.7142857142857143', '0.7142857142857143') ('1.0', '0.7142857142857143')\r\n0.0 ('0.0', '0.8571428571428571') ('1.0', '0.8571428571428571')\r\n[[0.3    0.2    0.    ]\r\n [0.1    0.0004 0.    ]\r\n [0.     0.02   0.2   ]]\r\n[[1 0 0 1]\r\n [0 1 1 2]\r\n [1 1 1 3]] [[1 0 0]\r\n [0 1 1]\r\n [1 1 1]]\r\n```\r\n\r\n\r\nand\r\n\r\n\r\n```\r\n0.4 ('0.4', '0.2857142857142857') ('1.0', '0.2857142857142857')\r\n1.0 ('1.0', '0.7142857142857143') ('0.7142857142857143', '0.7142857142857143')\r\n0.0 ('0.0', '0.8571428571428571') ('1.0', '0.8571428571428571')\r\n[[0.3    0.2    0.    ]\r\n [0.1    0.0004 0.    ]\r\n [0.     0.02   0.2   ]]\r\n[[1 0 0 1]\r\n [0 1 1 2]\r\n [1 1 1 3]] [[1 0 0]\r\n [0 1 1]\r\n [1 1 1]]\r\n```\r\n\r\n\r\nOn linux (Ubuntu 16.04 4.13.0-45-generic) and Python (2.7.14) results are stable.", "comments": ["additionally tested on MacOS 10.13.3 with Python 2.7.14 Anaconda custom (64-bit)\r\n\r\nand linux:\r\nUbuntu linux 4.4.0-64-generic with Python 2.7.12", "This reminds me of #15933 (https://github.com/tensorflow/tensorflow/issues/15933). Any chance you're hitting the same thing, or a relative of it? I think the primary recommendation from that bug was to try at least 10.12.6 (which your latest comment suggests you've done). ", "It seems to be kinda related, as far as I understand that issue, the problem is that two versions of tensorflow outputs different matrices in same conditions. I haven't tried to draw the output, but I think they are pretty different in a visual way too. \r\nIn my case the output is changing from run to run, like some information is random and/or stored between the runs.\r\n\r\nI've written above that I've tried different recent version of MacOS, Tensorflow and Python as well. \r\n\r\n", "@madrugado within one version of TF, do you get self-consistent results? \r\nAnd, do either versions' results match Linux? Or do they both differ from Linux?", "Yes, I get self consistent results, I've tried to re-run the script multiple times on different macOS, with restart of mac.\r\n\r\nThe results on linux are also matched (and they are as expected in the first place).", "@quaeler @Gemesys @lewisl can we interest you in this bug? ", "(sorry - i've not enough free time these days.)", "@rmlarsen is this something you've got an opinion about? Seems like it might be about deterministic numerics. ", "Hi @madrugado \r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20307, "title": "update pin of bazel-toolchain repo", "body": "bazel-toolchain repo updated to the latest release according to:\r\nhttps://releases.bazel.build/bazel-toolchains.html", "comments": ["@nlopezgi could you help review this PR as well?", "@yifeif : ping this PR because we found something strange. \r\n\r\nThis PR is merged, however the bazel-toolchains' pin in file has NOT updated in the master branch (and we didn't find that this commit got reverted):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L696\r\n\r\nCould you help look into this as well?\r\n", "OK now I know why: this PR got (accidentally I think) reverted in the following commit: [\r\nhttps://github.com/tensorflow/tensorflow/commit/bfcfad55b7b3fa4a1093fa748d4241f9457b2a84#diff-455a4c7f8e22d7c514e8c2caa27506c5\r\n\r\nI will send another PR to update the bazel-toolchain's pin for the 0.15.0 config release"]}, {"number": 20306, "title": "How to install 0.9 or 1.0.0 in 2018?", "body": "i have posted this issue on stackoverflow as well, i have searched all over internet but my issue is still unresolved. I want to use tensorflow 0.9 or 1.0.0 or earlier versions on windows 10. a project needs older version of tensorflow to run. can somebody please guide me? and sorry if this question does not follow guidelines, i just need to install tensorflow earlier version.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. It sounds like you have already cross-posted there, so I will close this issue in favor of that question.", "yes i have posted this on stackoverflow, but i haven't found a solution yet. i just need a solution that's all"]}, {"number": 20305, "title": "Compiling TF from source on Debian9 does not work as documented - partial fix", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nDebian 9, with nvidia-driver 390, and nvidia-cuda-toolkit 9.1.85 from the repository \"stretch-backports\"\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\ntrying to install from source - but this fails (see description below) \r\n\r\n- **TensorFlow version (use command below)**:\r\n1.8\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n6.3\r\n- **CUDA/cuDNN version**:\r\n9.1/7.1\r\n- **GPU model and memory**:\r\nGTX1080Ti\r\n- **Exact command to reproduce**:\r\n```\r\n\r\nTF_NEED_CUDA=1 \\\r\nGCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\nCUDA_TOOLKIT_PATH=/usr \\\r\nTF_CUDA_VERSION=9.1 \\\r\nTF_CUDNN_VERSION=7.1 \\\r\nCUDNN_INSTALL_PATH=/mnt/nfs/clustersw/shared/cuda/cudnn-9.1-linux-x64-v7.1/ \\\r\nTF_CUDA_COMPUTE_CAPABILITIES=5.2,6.1,6.2 \\\r\n./configure \r\n\r\nbazel build \r\nbazel build //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n### Describe the problem\r\nI was trying to install TF 1.8 on a  Debian 9 system with python/3.5.2, Cuda 9.1, cudnn 7.1. Because all attempts to used some some precomiled TF wheels failed, I'm trying to install TF from source. Cuda, and libcupti are installed from stretch-backports repository \r\n    apt-get install -t stretch-backports nvidia-cuda-toolkit libcupti-dev nvidia-driver\r\n\r\nIn order to compile TF from source, the following changes to the build system where necessary.\r\n\r\n```\r\ndiff --git a/configure.py b/configure.py\r\nindex ad585fa52e..af5ead70da 100644\r\n--- a/configure.py\r\n+++ b/configure.py\r\n@@ -33,12 +33,12 @@ except ImportError:\r\n   from distutils.spawn import find_executable as which\r\n # pylint: enable=g-import-not-at-top\r\n \r\n-_DEFAULT_CUDA_VERSION = '9.0'\r\n-_DEFAULT_CUDNN_VERSION = '7'\r\n+_DEFAULT_CUDA_VERSION = '9.1'\r\n+_DEFAULT_CUDNN_VERSION = '7.1'\r\n _DEFAULT_NCCL_VERSION = '1.3'\r\n-_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.5,5.2'\r\n-_DEFAULT_CUDA_PATH = '/usr/local/cuda'\r\n-_DEFAULT_CUDA_PATH_LINUX = '/opt/cuda'\r\n+_DEFAULT_CUDA_COMPUTE_CAPABILITIES = '5.2,6.1,6.2'\r\n+_DEFAULT_CUDA_PATH = '/usr'\r\n+_DEFAULT_CUDA_PATH_LINUX = '/usr'\r\n _DEFAULT_CUDA_PATH_WIN = ('C:/Program Files/NVIDIA GPU Computing '\r\n                           'Toolkit/CUDA/v%s' % _DEFAULT_CUDA_VERSION)\r\n _DEFAULT_TENSORRT_PATH_LINUX = '/usr/lib/%s-linux-gnu' % platform.machine()\r\n@@ -839,7 +839,7 @@ def set_tf_cuda_version(environ_cp):\r\n     if is_windows():\r\n       cuda_rt_lib_path = 'lib/x64/cudart.lib'\r\n     elif is_linux():\r\n-      cuda_rt_lib_path = 'lib64/libcudart.so.%s' % tf_cuda_version\r\n+      cuda_rt_lib_path = 'lib/x86_64-linux-gnu/libcudart.so.%s' % tf_cuda_version\r\n     elif is_macos():\r\n       cuda_rt_lib_path = 'lib/libcudart.%s.dylib' % tf_cuda_version\r\n \r\ndiff --git a/third_party/gpus/cuda_configure.bzl b/third_party/gpus/cuda_configure.bzl\r\nindex c90c66912d..f55d0a455e 100644\r\n--- a/third_party/gpus/cuda_configure.bzl\r\n+++ b/third_party/gpus/cuda_configure.bzl\r\n@@ -59,6 +59,7 @@ CUDA_LIB_PATHS = [\r\n CUPTI_HEADER_PATHS = [\r\n   \"extras/CUPTI/include/\",\r\n   \"include/cuda/CUPTI/\",\r\n+  \"include/\"\r\n ]\r\n \r\n # Lookup paths for the cupti library, relative to the\r\n@@ -67,7 +68,7 @@ CUPTI_HEADER_PATHS = [\r\n # the other CUDA libraries but rather in a special extras/CUPTI directory.\r\n CUPTI_LIB_PATHS = [\r\n   \"extras/CUPTI/lib64/\",\r\n-  \"lib/x86_64-linux-gnu\",\r\n+  \"lib/x86_64-linux-gnu/\",\r\n   \"lib64/\",\r\n   \"extras/CUPTI/libx64/\",\r\n   \"extras/CUPTI/lib/\",\r\n@@ -94,6 +95,7 @@ CUDNN_INCLUDE_PATHS = [\r\n NVVM_LIBDEVICE_PATHS = [\r\n   \"nvvm/libdevice/\",\r\n   \"share/cuda/\",\r\n+  \"lib/nvidia-cuda-toolkit/libdevice/\"\r\n ]\r\n \r\n load(\"//third_party/clang_toolchain:download_clang.bzl\", \"download_clang\")`\r\n```\r\n\r\n\r\nWith these changes, \"bazel build\" was sucessful. However, the command \r\n   bazel build //tensorflow/tools/pip_package:build_pip_package\r\nwould still fail with \r\n\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package \r\nWARNING: Output base '/nfs/scistore12/jonasgrp/schloegl/.cache/bazel/_bazel_schloegl/8678486dd934320ead8a15f39d2403d9' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nStarting local Bazel server and connecting to it...\r\n..........................\r\nERROR: /nfs/scistore_grp/schloegl/.cache/bazel/_bazel_schloegl/8678486dd934320ead8a15f39d2403d9/external/local_config_cuda/cuda/BUILD:48526:11: in cmd attribute of genrule rule @local_config_cuda//cuda:cuda-include: '$SocketInputStream' syntax is not supported; use '$(SocketInputStream)' instead for \"Make\" variables, or escape the '$' as '$$' if you intended this for the shell\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis of target '@local_config_cuda//cuda:cuda-include' failed; build aborted\r\nINFO: Elapsed time: 7.423s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (186 packages loaded)\r\n`\r\n\r\n", "comments": ["tested compiling TF1.8 also with \r\n    bazel 0.15.0 / cuda 9.1 / cudnn 7.1\r\nas well as with \r\n    bazel 0.15.0 / cuda 8.0 / cudnn 6.0 \r\nand it fails with similar error messages. \r\n\r\n", "Hi @schloegl ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Have you checked in [latest version](https://www.tensorflow.org/install/source) yet ? Thanks!", "Recent versions of tensorflow work fine with Debian 10 and 11, and I do not have any more need for Tensorflow on Debian9. So this issue can be closed. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20305\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20305\">No</a>\n", "Yes"]}, {"number": 20304, "title": "d", "body": "#20303 close #20303 ", "comments": []}, {"number": 20303, "title": "#24", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 20302, "title": "Error while importing tensorflow in ubuntu", "body": "Hi all,\r\nI have installed tensorflow=1.5.0 in ubuntu 14.04. while importing tensorflow I got the below error. Please let me know how to handle this issue.\r\n\r\n**>>> import tensorflow as tf**\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 22,                          in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im                         port\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", l                         ine 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/graph_p                         b2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_do                         t_framework_dot_node__def__pb2\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/node_de                         f_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_                         dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/attr_va                         lue_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_                         framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/tensor_                         pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_                         core_dot_framework_dot_resource__handle__pb2\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/core/framework/resourc                         e_handle_pb2.py\", line 22, in <module>\r\n    serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\nte                         nsorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x                         12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\                         thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x1                         8org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorf                         low/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\r\n**TypeError: __init__() got an unexpected keyword argument 'serialized_options'**\r\n", "comments": ["first,you should in python environment,then import tensorflow", "Yes I was in python environment", "https://stackoverflow.com/questions/33622842/error-in-python-after-import-tensorflow-typeerror-init-got-an-unexpect/33638454    this website may help you", "I followed the same procedure but I couldn't solve it.\r\npip3 uninstall protobuf\r\npip3 uninstall tensorflow==1.5.0\r\npip3 install tensorflow==1.5.0 (It installed protobuf-3.6.0 tensorflow-1.5.0)\r\n\r\n", "This is due to latest protobuf having a backwards incompatible change.\r\nYou can try installing an older version of protobuf with this command for tensorflow 1.5\r\n```\r\npip install protobuf==3.4.0\r\n```", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing because this is due to protobuf version incompatibility.\r\nIf you are using tensorflow 1.8 or older, please use protobuf version 3.4 or older.\r\nIf you are using TF 1.9 or newer, please use the latest version of protobuf.", "uninstalling Protobuf 3.6 and pip installing Protobuf 3.4.0 as per the above instructions but importing tensorflow still yields same error message about **TypeError: init() got an unexpected keyword argument 'serialized_option**\r\n\r\nInstailling Protobuf==3.4   gives this error:\r\n\r\ntensorflow 1.11.0 has requirement protobuf>=3.6.0, but you'll have protobuf 3.4.0 which is incompatible.\r\n\r\nSo using the most up-to-date version of Tensorflow requires protobuf>-3.6.0 but using that version yields a serialized_option error.  What to do?", "@cpoptic I think the suggestion was to use the latest version of protobuf (3.6.0+) for TF v1.11 \r\n\r\nYou probably need to try uninstall protobuf for every user account (root and non-root) to avoid your user setting picking a system installed protobuf. \r\n\r\nThen you do the \"pip install protobuf\" again after making sure there are no protobuf system-wide. "]}, {"number": 20301, "title": "InternalError: Dst tensor is not initialized.", "body": "`2018-06-26 09:22:30.314633: W tensorflow/core/common_runtime/bfc_allocator.cc:277] **************xx***x******************************************xx********xx***********_____**********\r\nTraceback (most recent call last):\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n         [[Node: dynamic_seq2seq/decoder/embedding_lookup/_61 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_50_dynamic_seq2seq/decoder/embedding_lookup\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 21, in <module>\r\n    tf.app.run(main=nmt.main, argv=[os.getcwd() + '\\nmt\\nmt\\nmt.py'] + unparsed)\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/nmt.py\", line 529, in main\r\n    run_main(FLAGS, default_hparams, train_fn, inference_fn)\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/nmt.py\", line 522, in run_main\r\n    train_fn(hparams, target_session=target_session)\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py\", line 251, in train\r\n    sample_tgt_data)\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py\", line 125, in run_full_eval\r\n    eval_model, eval_sess, model_dir, hparams, summary_writer)\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py\", line 56, in run_internal_eval\r\n    summary_writer, \"dev\")\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/train.py\", line 405, in _internal_eval\r\n    ppl = model_helper.compute_perplexity(model, sess, label)\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/model_helper.py\", line 414, in compute_perplexity\r\n    loss, predict_count, batch_size = model.eval(sess)\r\n  File \"/datadrived/jai/nmt-chatbot-v1/nmt/nmt/model.py\", line 221, in eval\r\n    self.batch_size])\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/bitwise/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n         [[Node: dynamic_seq2seq/decoder/embedding_lookup/_61 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_50_dynamic_seq2seq/decoder/embedding_lookup\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n`\r\n\r\nI get the above error when i try to run the train.py using the gpu server", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code - yes\r\nOS Platform and Distribution - Ubuntu 16.04\r\nTensorFlow installed from - official tensorflow site\r\nTensorFlow version - tensorflow-gpu 1.4\r\nBazel version - N/A\r\nCUDA/cuDNN version - 8.0/6.0\r\nGPU model and memory - NVIDIA Tesla K80 GPU on azure GPU server NC6 instance\r\nExact command to reproduce ", "Nagging Assignee @bignamehyp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20300, "title": "W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. I am trying to run the Speech_Commands example\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nThe training of 18000 steps is over successfully and I have got both the confusion matrix and the accuracy. I am trying to freeze the model by running freeze.py. I am getting errors. \r\n\r\n### Source code / logs\r\nThe command I typed is as follows: as given in the tutorial.\r\n`C:\\Program Files\\Python35\\Lib\\site-packages\\tensorflow>python \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\freeze.py\" \\`\r\n\r\n\r\nThe output I got is as follows:\r\n\r\n```\r\n2018-06-26 12:55:44.437915: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-26 12:55:45.011031: W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at save_restore_tensor.cc:170 : Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\models.py\", line 123, in load_variables_from_checkpoint\r\n    saver.restore(sess, start_checkpoint)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1802, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"C:\\Program Files\\python35\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\models.py\", line 122, in load_variables_from_checkpoint\r\n    saver = tf.train.Saver(tf.global_variables())\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1384, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 835, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 472, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 886, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1546, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```\r\nI found a similar issue here [#15426](https://github.com/tensorflow/tensorflow/issues/15426)\r\n\r\nDoesn't mention how they solved the problem though. I have commented on that as well. Still no response. \r\n\r\nSo I am posting a new thread here. ", "comments": ["There's a bad link in #15426 that in turn connects to #7547 (https://github.com/tensorflow/tensorflow/issues/7547). To summarize that link, I think you want to modify your \"model.load\" to try either a local relative path (./filename) or a fully qualified path. Can you please try that? ", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20299, "title": "Layer norm for cudnnlstm?", "body": "I am training a very large and deep lstm-RNN. Using cudnnlstm can save memories and speed up the training process. However, I need some extra functions such as layer norm, adding attention wrapper, etc.\r\nIs there a way to achieve this? Should I rewrite the c++ code and recompile? If so, which file of the source code contains the cudnnlstm implementation?\r\n\r\nPS: In the documentation, there is only one bit mentioning topics related to the layer norm of cudnn. It is in the performance guides. It only tells developers there is no layer norm for cudnnlstm. However, layer norm is very important for a decent lstm network. I would like to know the method to adding the layer norm algorithm similar to the LayerNormBasicLstm class. I explored a bit via trying to scoop out the params (weights and biases) of cudnn. It seems the shape of those two are not specific in the process of building the graph.\r\n\r\n**Have I written custom code**\r\nNot yet\r\n**OS Platform and Distribution**\r\nUbuntu 16.04\r\n**TensorFlow installed from**\r\npip \r\n**TensorFlow version**\r\n1.8.0\r\n**Bazel version**\r\nN/A\r\n**CUDA/cuDNN version**\r\nCuda 8.0  \r\n**GPU model and memory**\r\nNvidia GTX 1080\r\n**Exact command to reproduce**\r\nN/A\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I think I just need a hint that if implementing the layer norm is possible and some simple guidelines regarding how to do this. If simply I can register a new op written in C++ and implement a new version of cudnn wrapper mimicking the cudnn_rnn_ops.cc functions. As I know, layer norm is just a moving average of weights and biases. If that part can be buried into C++ code, it should be okay. Or there can be a solution to embed the algo into python code. \r\n\r\nI think this would be a great contribution to tensorflow community as cudnnlstm is very memory and computation saving. Adding layer norm and some other functions might be useful to make it perform better.\r\n\r\n@michaelisard ", "Still no response for this topic?", "I'm very sorry for the slow response. @zheng-xq is probably better placed to comment on whether this can be done in Python or would need a new custom op.", "Any update on this issue? @zheng-xq @michaelisard ", "Nagging Assignee @protoget: It has been 134 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Implementing layernorm in TF cudnn kernel (not cuda kernel) probably will not offer much performance benefits. \r\n\r\nTF cudnn rnn classes are complex wrappers around nvdia cudnn library, which at present doesn't offer layer norm. It's a good feature request to ask for nvidia.\r\n"]}, {"number": 20298, "title": "Memory leak when run multi-times in C++", "body": "I build tensorflow dll in Win10 and call it in my project. I get correct results, but when running serveral times it will leak CPU memory, is there any resource I do not release ? Here is some C++ code\r\n```\r\nfor (int i = 0; i < 1000; i++)\r\n{\r\n  NewSession(SessionOptions(), &_session);\r\n  ReadBinaryProto(Env::Default(), model_path, &_graphdef);\r\n  _session->Create(_graphdef);\r\n  _session->Run(inputs, _output_tensor_names, {}, &outputs);\r\n  _session->Close();\r\n}\r\n```\r\nif I change to the following code, there is no CPU memery leak, but I hope to release GPU memory when not using GPU\r\n```\r\nNewSession(SessionOptions(), &_session);\r\nReadBinaryProto(Env::Default(), model_path, &_graphdef);\r\n_session->Create(_graphdef);\r\nfor (int i = 0; i < 1000; i++)\r\n{\r\n  _session->Run(inputs, _output_tensor_names, {}, &outputs);\r\n  _session->Close();\r\n}\r\n```\r\nmake code simple , only newsession and close multi-times, it will leak CPU memory\r\n```\r\nfor (int i = 0; i < 1000; i++)\r\n{\r\n  NewSession(SessionOptions(), &_session);\r\n  _session->Close();\r\n}\r\n```\r\n", "comments": []}, {"number": 20297, "title": "Why does tf.placeholder() change the random state?", "body": "![image](https://user-images.githubusercontent.com/22118890/41892870-4e437440-794c-11e8-9267-31f600c34181.png)\r\n", "comments": ["You need to quite the current ipython console, and reopen a new one.", " see #14675: Small change to graph changes initial values of variables ", "@facaiy Thx!"]}, {"number": 20296, "title": "fix tests in lite/kernels/internal", "body": "make\r\n```\r\nbazel test --config opt //tensorflow/contrib/lite/kernels/internal:all\r\n```\r\nwork. So that\r\n```\r\nbazel test --config opt //tensorflow/contrib/lite/kernels/...\r\n```\r\nworks.\r\n\r\n\r\nThis is a sequel to https://github.com/tensorflow/tensorflow/pull/17700", "comments": ["Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20295, "title": "Add #define guards for get_xxx_tensorrt_version() to fix an internal ci build error.", "body": "I verified the fix by running:\r\n```\r\nbazel build --verbose_failures --jobs=32 -c opt --copt=-mavx --output_filter=DONT_MATCH_ANYTHING tensorflow/contrib/tensorrt:init_py\r\n```\r\non a local repository where trt is not enabled.", "comments": ["Thank you @aaroey for fixing this compile failure running the unit test, I was just about to open an issue for it, been struggling since Friday to determine the commit that caused the issue.\r\n\r\nThis is a good reference for how to fix future issues like this. "]}]