[{"number": 8724, "title": "Error occurred in Tensorflow version 1.0.1 date: 26th March 2017.", "body": "I just reinstalled Tensorflow on my laptop this morning and got this problem. \r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'what the hell?'\r\n\r\n![tensorflow](https://cloud.githubusercontent.com/assets/10723417/24329241/5ffd36fe-123b-11e7-8298-c957a13b3141.PNG)\r\n\r\n", "comments": ["Duplicate #7859", "This bug is fixed in the 1.1 release candidate. Please upgrade to that version or a nightly build (or else you can simply ignore the error messages, as they are harmless).", "@mrry can I take the opportunity to ask you a question if you don't mind? I don't want to open an issue just for it.\r\n\r\nI am having no luck with bazel (#8482) then I decided to build with CMake but the COMPILER_OPT_ARCH_NATIVE_SUPPORTED test fails even with the instructions available (gcc march=native shows them enabled, mavx, mfma, msse etc).\r\nWould you have any tip on this?\r\nThanks!", "I think this test fails because MSVC doesn't support the exact `-march=native` option that we use for this in GCC. I suspect there are equivalent options for MSVC to enable these instructions (perhaps @guschmue knows them off-hand), and the most likely workaround would be to modify CMakeLists.txt to use those options explicitly.", "@mrry got it. Awesome, thank you! :)", "@mrry I successfully built replacing `-march=native` for `/arch:[AVX2|FMA|SSE4.2|FPMATH|MMX]` specifically.\r\n```\r\n-- Building for: Visual Studio 14 2015\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Success\r\n```\r\nI don't know the MSVC `-march=native` equivalent then as you pointed I added each option explicitly like above. \r\n \r\nShould this be a side note on CMake docs?\r\n\r\nThank you again!\r\n\r\nReference: https://msdn.microsoft.com/en-us/library/jj620901.aspx\r\n\r\n", "@mrry did version 1.1 release yet?  \r\n@Carmezim I have two laptops, one laptop works perfectly and no strange message(I installed it about 3 months ago) and the other which I just installed Tensorflow in it yesterday and these messages show up. both have the same version.", "@mrry. @Carmezim I updated to 1.1.0rc and the problem still not resolved. Here is screen capture of my current version\r\n![capture 3](https://cloud.githubusercontent.com/assets/10723417/24345010/1028df92-1309-11e7-86e4-9a9624f6a470.JPG)\r\n", "I think its hard to get rid of the SSE/AVX message: if ci builds with, say avx2, the builds will not work on boxes that don't support avx2. If ci builds don't enable the highest instruction set, somebody will see the error messages. No easy way out other than building from source.\r\nShould be the same on linux (just tried and see the same messages).\r\nFor some reason 1.1.0rc0 is on pypi but it does not seem to be the default download so you need to force it with:\r\npip install --upgrade tensorflow-gpu==1.1.0rc0\r\n", "Oh, I am so sorry, I totally misread that. \r\nYou can set TensorFlow environment variable `TF_CPP_MIN_LOG_LEVEL` to 2 as follows:\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\n```\r\nIt will filter out WARNING logs.", "@guschmue would you have any insight on using `/arch` with MSVC? It seems MSVC only accepts `AVX2` option and won't build with any other instruction, even gcc showing they are available. I think I might have set something wrong when I built above because now I cannot build it anymore setting the same way.", "@guschmue thank you anyway, I did figure out the way to upgrade to 1.1.0rc0. but it doesn't help much with those Warning. I decided to wait. \r\n@Carmezim thank you for your support ", "I think /arch:avx and /arch:avx2 work, the later I used for builds. But /arch:avx2 compiles broke for the master some time ago ... can try to find time fix this (can't be much since it used to work).", "@guschmue Nice. AVX2 is at least configuring, didn't try to compile yet though as were trying to figure it out how to use the others. \r\nSo SSE and others really don't work with MSVC? Is there any workaround to compile using them as well, SSE, FMA etc with CMake? ", "https://msdn.microsoft.com/en-us/library/7t5yh4fd.aspx says SSE and SSE2 should work too but have not tried those.\r\n", "@guschmue Well noted. I personally was trying for x64 arch but is good to know. Do you know if is possible to select GCC as the compiler by CMake and build with MSYS2 for instance with `-march=native` enabled or is it too much of a hack? I know other compilers like Intel's allow SSE4.2 for instance but MSVC is a bit limited. Thanks a lot!", "not sure if you'd get gpu work with gcc on windows.", "oh yeah, but for CPU would work something like Cygwin to build right?", "Hi @Carmezim , I am facing the same the [issue] I think.(https://github.com/tensorflow/tensorflow/issues/24076) I have tried replacing `-march=native` for `/arch:[AVX2|FMA|SSE4.2|FPMATH|MMX]` in the CMakeLists.txt file but still getting `Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed`. What do you suggest me to do? Thanks."]}, {"number": 8723, "title": "Disturbing Grammatical error...", "body": "Grammatical error with this sentence...\r\n\"In this lab, we will be using transfer learning, which means we are starting with a model that has been already trained on another problem\"\r\nCorrection: \"has already been trained on another problem\"\r\n\r\n\r\n", "comments": ["Would you like to send a pull request to fix the problem?\r\nI have no idea which tutorial you are referring to, and all these should be checked into our repository.", "The tutorial is at https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0 -- although I don't believe this is a grammatical error.", "I agree with @Timeroot that to me both forms look correct. And it not being on the repo may make it more difficult to fix it. So I do not know if it is worth the effort to modify.\r\nRedirecting to @wolffg , but due to the current load, the most likely outcome for this issue is we will close it.", "Actually, I am pretty sure this is correct grammar. already is indeed an adverb so you can use it to modify the verb in a simple sentence.\r\n```\r\nI already ate.\r\n```\r\nBut in the present or past perfect it is commonly used next to the main verb instead of hte hlper verb had.\r\n```\r\nThe plane had already landed.\r\n```\r\n\r\n"]}, {"number": 8722, "title": "Added support for case callables to return namedtuples", "body": "This commit adds support for` namedtuple` to be used with `tf.case`. \r\n\r\n`tf.cond` already supports `namedtuple`:\r\n\r\n```\r\nnt = collections.namedtuple('Point', ['x', 'y'])\r\nf1 = lambda: nt(tf.Variable(1), tf.Variable(2))\r\nf2 = lambda: nt(tf.Variable(3), tf.Variable(4))\r\nc  = tf.cond(tf.less(1,0), f1, f2)\r\nprint c\r\n\r\nOutput\r\n[<tf.Tensor 'cond/Merge:0' shape=() dtype=int32>, <tf.Tensor 'cond/Merge_1:0' shape=() dtype=int32>]\r\n```\r\n\r\nHowever, if \r\n```\r\nc =  tf.case([(tf.less(1,0), f1)], default=f2)\r\nprint c\r\n\r\nOutput\r\nTypeError: __new__() takes exactly 3 arguments (2 given)\r\n```\r\n\r\nAfter this commit, the output for above is\r\n`[<tf.Tensor 'case/If_1/Merge:0' shape=() dtype=int32>, <tf.Tensor 'case/If_1/Merge_1:0' shape=() dtype=int32>]\r\n`\r\n\r\nUsing `namedtuple` with `tf.case` should have the same effect as using a regular tuple.\r\n\r\n```\r\nnt = collections.namedtuple('Point', ['x', 'y'])\r\n\r\nf1 = lambda: nt(tf.Variable(1), tf.Variable(2))\r\nf2 = lambda: nt(tf.Variable(3), tf.Variable(4))\r\n\r\nf3 = lambda: (tf.Variable(1), tf.Variable(2))\r\nf4 = lambda: (tf.Variable(3), tf.Variable(4))\r\n\r\nc1  = tf.case([(tf.less(1,0), f1)], default=f2)\r\nc2  = tf.case([(tf.less(1,0), f3)], default=f4)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print \"With namedtuple: \",\r\n    v = sess.run(c1)\r\n    print v\r\n\r\n    print \"With tuple: \",\r\n    v2 = sess.run(c2)\r\n    print v2\r\n\r\nOutput\r\nWith namedtuple:  [3, 4]\r\nWith tuple:  [3, 4]\r\n```\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@ebrevdo could you take a look at this? Thanks!", "Someone is working on this internally, with a slightly more general solution.", "Closing as per Eugene said. Thanks @palimarrao for sending the PR. "]}, {"number": 8721, "title": "Issue while executing https://github.com/suriyadeepan/easy_seq2seq", "body": "I tried running execute.py from https://github.com/suriyadeepan/easy_seq2seq however, I'm facing trouble with this.\r\n\r\nNot sure if I'm doing something wrong, but a suggestion or a solution would be great help.\r\n\r\nThis is the dump from my Windows CMD prompt:\r\n\r\n`C:\\Users\\myPC\\Downloads\\easy_seq2seq-master\\easy_seq2seq-master>python execu\r\nte.py`\r\n\r\n`Traceback (most recent call last):`\r\n\r\n`  File \"execute.py\", line 31, in <module>`\r\n`    import seq2seq_model`\r\n`  File \"C:\\Users\\myPC\\Downloads\\easy_seq2seq-master\\easy_seq2seq-master\\seq2\r\nseq_model.py\", line 28, in <module>`\r\n`    from tensorflow.models.rnn.translate import data_utils`\r\n`ImportError: No module named 'tensorflow.models'`\r\n", "comments": ["This belongs on StackOverflow, not as a github issue. tensorflow.models is a separate repository from this one. The translate directory can be found [here](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate). ", "The code referenced is not a TF owned code.\r\nIt simply seems to be out of date.\r\nPlease contact the owner of the code, or reach out to StackOverflow as @mckinziebrandon suggested.\r\nClosing this issue."]}, {"number": 8720, "title": "split not working correctly", "body": "```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel_input = tf.placeholder(tf.float32, shape=(1, 10))\r\nmodel_split = tf.split(model_input, [4], axis=1)[0]\r\n\r\ninput = np.zeros((1, 10))\r\nsession = tf.Session()\r\nsplit = session.run(model_split, feed_dict = { model_input: input })\r\n\r\n# the size of the evaluated tensor does not match the size of the tensor\r\nassert split.shape == (1, 10)\r\nassert model_split.get_shape() == (1, 4)\r\n\r\nmodel_b = tf.zeros((1, 4))\r\n\r\n# this is okay adding things of the same shape\r\nsession.run(model_b+model_b, feed_dict = { model_input: input })\r\n\r\n# check that model_b and model_split have the same shape\r\nassert model_b.get_shape() == model_split.get_shape()\r\n\r\n# this line crashes because the shape of model_split + model_b are not compatible.\r\n# although the assert just checked that the sized match. This is the error message:\r\n#          InvalidArgumentError (see above for traceback): Incompatible shapes: [1,10] vs. [1,4]\r\nsession.run(model_split + model_b, feed_dict = { model_input: input })\r\n```\r\n\r\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Windows\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nSee #2 below\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nHow do I get this?\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n(C:\\Users\\television2\\Anaconda3) C:\\cygwin64\\home\\television2\\nn\\hierarchy>python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\n1.0.0\r\n\r\n", "comments": ["I found a work around. in the tf.split line split out the whole thing. tf.split(model_input, [4,6], axis=1)[0]", "@ekelsen this appears to be a bug.", "Fix in progress.  To be clear the line: `model_split = tf.split(model_input, [4], axis=1)[0]` shouldn't work; it would need to be `model_split = tf.split(model_input, [10], axis=1)[0]`.  You will now get an error either when the op is created if the dimensions are known or at runtime if they are not.", "You can specify an unknown value with -1, but you can't specify N-1 sizes if you expect N outputs.", "This got fixed by the referenced commit."]}, {"number": 8719, "title": "Correcting Cudnn to cuDNN in config script", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 8718, "title": "Incorrect reference for tf.learn in Linear Model tutorials", "body": "This [page](https://www.tensorflow.org/tutorials/linear) and [this](https://www.tensorflow.org/tutorials/wide) mentions `tf.learn` multiple times. \r\nHowever, looking at the links and source code, I believe that `tf.learn` should actually be either `tf.contrib` or `tf.contrib.learn`, not `tf.learn`.\r\n\r\n(I guess `tf.learn` was the old name, but got renamed but the tutorial is still outdated?)", "comments": ["Could you fix or delegate this @wolffg?", "Added a PR #10658 for that."]}, {"number": 8717, "title": "*** Error in `python': double free or corruption (!prev): 0x000000000167c080 ***", "body": "I run : python train.py --train_data_pattern='/vol/vssp/msos/yx/audioset/audioset_v1_embeddings/unbal_train/*.tfrecord' --frame_features=True --model=DbofModel --feature_names=\"audio_embedding\" --feature_sizes=\"128\" --train_dir=tmp_model/frame_level_DbofModel_unbal\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nINFO:tensorflow:/job:master/task:0: Tensorflow version: 1.0.1.\r\nINFO:tensorflow:/job:master/task:0: No checkpoint file found. Building a new model.\r\nINFO:tensorflow:Using batch size of 1024 for training.\r\n*** Error in `python': double free or corruption (!prev): 0x000000000167c080 ***\r\nAborted (core dumped)\r\n\r\n\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 14.04.5 LTS\r\nRelease:        14.04\r\nCodename:       trusty\r\n", "comments": ["Please fill in the full issue template.\r\nAlso include a minimal example to reproduce a problem.\r\n\r\nWithout knowing what you are running, we cannot really help you.", "We've observed similar crashes. I suspect incompatibility of TensorFlow official release with Ubuntu 14.04, something possibly related to jemalloc. The work-around is to do following:\r\n\r\n```\r\nsudo apt-get install google-perftools\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" \r\n\r\n```", "Looks like there is a workaround posted. Closing this issue.", "Thanks @yaroslavvb \r\nThe work-around works !", "I don't know why I send the command:\r\nbazel-bin/im2txt/train --input_file_pattern=\"${MSCOCO_DIR}/train-?????-of-00256\" --inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\" --train_dir=\"${MODEL_DIR}/train\" --train_inception=false --number_of_steps=1000000\r\n\r\nthere have a error:\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcufft.so.8.0. LD_LIBRARY_PATH:\r\nI tensorflow/stream_executor/cuda/cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n*** Error in `/usr/bin/python': double free or corruption (!prev): 0x000000000231f8e0 ***\r\nI don't know the error", " @yaroslavvb\r\nThanks\uff01\uff01\uff01\uff01\r\nit works !"]}, {"number": 8716, "title": "Fix tf.nn.softmax", "body": "Fixes #8667 ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\n@ilya-edrenkin any chance to take a look at the review comments?", "Jenkins, test this please."]}, {"number": 8715, "title": "RecordReader reads disk without buffer", "body": "tensorflow::io::RecordReader calls RandomAccessFile::Read() directly, without go through IoBuffer.\r\nAnd RandomAccessFile::Read() will call pread(2) or ReadFile OS API.\r\nIf there is an IoBuffer between them, it could reduce a lot of syscalls. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\nWindows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNone\r\n\r\nif installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\nc7b80d51da4fb6d51ea54a0bdf2601afa379d60c\r\n\r\n2. The output of `bazel version`\r\n(compiled by cmake and vs 2017)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nNone\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nNone\r\n\r\n### Logs or other output that would be helpful\r\n\r\n", "comments": ["That might be a good candidate for a PR if you're willing to contribute.\r\n\r\n@skye ", "Hi @drpngx , I'll take this.\r\n\r\nI just found the interface is a bit inconsistent.  For compressed file, it assumes that the file will be read sequentially and it will ignore the offset parameter in \r\n```\r\nclass RecordReader{\r\n  //...\r\n  Status ReadRecord(uint64* offset, string* record);\r\n}\r\n```\r\n\r\nSo,  if this class is designed for sequential reads, I want to remove the 'offset' argument, and use ZlibInputStream for zipped file, SnappyInputStream for snappy file, RandomAccessInputStream for uncompressed file.\r\n\r\nIs it ok?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8714, "title": "Fix typo at description of tf.nn.conv1d", "body": "As you see [description of tf.nn.conv1d](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/conv1d), `batch, out_width, out_channels` is linked, because markdown recognizes it as link. [batch, out_width, out_channels]\\(...\\)\r\n\r\nSo, add escape character to parentheses!", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please."]}, {"number": 8713, "title": "Configure should work offline", "body": "Currently, `configure` set some environment variables and invoke `bazel fetch`. When the fetch fails, due to network issue for example, the environment variables are also lost. I tried to `source ./configure`, but the shell died instantly. The only way I found to workaround this is to replace `bazel fetch` with `bash` in `configure`, then manually run `bazel fetch` a few times until it succeed. IMHO, `configure` should do the configuration separately instead of as part of `bazel fetch`.", "comments": ["As a workaround, you can simply comment out all calls to `bazel_clean_and_fetch` in configure script to achieve what you need.\r\n\r\n@damienmg Do we still need to run bazel fetch separtely during configure with bazel 0.4.5?\r\nCan bazel fetch be run automatically when we are running bazel build?", "Gunan: with the --action_env flag I added to the .bazelrc in #8637 we\nshould be good to remove bazel_clean_and_fetch and we can let the fetch\nhappens with the build command.\n\nOn Sat, Mar 25, 2017, 11:50 PM gunan <notifications@github.com> wrote:\n\n> As a workaround, you can simply comment out all calls to\n> bazel_clean_and_fetch in configure script to achieve what you need.\n>\n> @damienmg <https://github.com/damienmg> Do we still need to run bazel\n> fetch separtely during configure with bazel 0.4.5?\n> Can bazel fetch be run automatically when we are running bazel build?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8713#issuecomment-289244985>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf9w6d6GT6utjUu1Eh-61LP-Dw-zeks5rpZpPgaJpZM4MpD8c>\n> .\n>\n", "Thanks for the tip.\r\nThen I am sending #8761 to test it out and remove bazel fetch from configure."]}, {"number": 8712, "title": "major memory allocation/copying overhead in Android Inference Library", "body": "There seems to be a major overhead in [`TensorFlowInferenceInterface.feed(...)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java#L257)\r\nWhen running the feed-run-fetch cycle, I noticed that feeding took an unreasonable amount of time.\r\n\r\nSo I looked at the source code. I realized that I could pass a Buffer instead of an Array, because it would have been wrapped anyway. But either way a new Tensor object is created in [`Tensor.create(...)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L128) which includes first [allocating the memory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L204) and then copying over from the passed Buffer.\r\n\r\nAlthough this seems to be okay, when doing some Method Tracing, I discovered that when doing one cycle of feeding, running and fetching, it spends 99.99% with feeding, specifically 99.99% of that with creating the Tensor and 99.99% of that with copying over from the Buffer.\r\n\r\nThe chart is really packed with `FloatBuffer.put(...)`\r\n\r\nWhy isn't it possible to use the just newly created Buffer directly?\r\nOr why isn't the final Buffer filled directly from the Array?", "comments": ["@asimshankar Any ideas here?", "Could you elaborate a bit on the major overhead? The relative numbers would depend on the computation  being performed. For example, in [label image example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java), where the image fed is about 600KB (~150K 32-bit floats), creating the `Tensor` takes about ~100\u03bcs while executing the model takes about 30ms on my machine.\r\n\r\nThat said, yes, you're right in that the current feed API requires a copy. It should be precisely one copy (note that `FloatBuffer.wrap()` etc. should normally not involve an array copy) - from the Java `FloatBuffer` object to the underlying `Tensor` object in C memory. If you see more than one copy, do let us know.\r\n\r\nWe do not provide direct access to the C memory for safety reasons. For example, there is a private method ([`Tensor.buffer()`](https://github.com/tensorflow/tensorflow/blob/2acba51/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L472)) that provides a view of the underlying C memory as a Java `ByteBuffer`. However, that isn't safe for general use as bad things would happen if say one released native memory (via `Tensor.close()` perhaps) and before accessing the contents of the buffer (e.g., `ByteBuffer buf = t.buffer(); t.close(); doSomethignWith(buf);`)\r\n\r\nThe other option which I think you're asking about would be to avoid the C `Tensor` object allocating more memory and instead have it reference the Java `Buffer` object's underlying memory directly. That might be workable, modulo memory alignment requirements of the various TensorFlow kernels and some care to ensure that it is safe (for example, we wouldn't want the JVM's garbage collector from cleaning up the underlying memory while native code is executing a graph that requires it).\r\n\r\nThat might be worth investigating. I was expecting that in practice both the time and space costs of copying feed tensors would be dwarfed by the computation costs of executing models (except perhaps in toy models), like in the label image example. However, if that is not the case, this optimization might be a good idea to pursue. Do you have some more details that you might be able to share on your use case?\r\n\r\n", "Thanks for the elaboration!\r\n\r\nMy graph should look roughly like this:\r\n```\r\nplaceholder [None, 12593]\r\nreshape [None, 49, 257, 1]\r\nconv2d (1, 5, 5, 32) [None, 49, 257, 32]\r\nrelu [None, 49, 257, 32]\r\nmax_pool (1, 2, 4, 1) [None, 24, 64, 32]\r\nconv2d (1, 7, 7, 64) [None, 24, 64, 64]\r\nrelu [None, 49, 257, 32]\r\nmax_pool (1, 2, 4, 1) [None, 12, 16, 64]\r\ndense (1024) [None, 1024]\r\nrelu [None, 1024]\r\ndense (2) [None, 2]\r\n```\r\n\r\nbtw. batch size is 1 during inference", "Could you also share the profiling code you use to determine the 99.9% cost for feeding (and perhaps the absolute numbers of those times)? I'm surprised that the copies will be costing you 99% in such a graph.", "@asimshankar I just used the built-in \"Method Tracing\" functionality of ADB as it is directly integrated in Android Studio.\r\nhttps://developer.android.com/studio/profile/am-methodtrace.html\r\n\r\nIt captures each and every method call and provides exact numbers on how often methods are called and by whom and how much time is spent in each single one of time. It also differentiates between the time that is spent only directly in this method as well as in further calls down the stack trace.", "@Androbin @asimshankar I just gave this a shot on my own and got the same results. I think something about the method tracing in Android Manager is interfering with the operation of the inference thread, as I noticed the iterating speed slows down by more than an order of magnitude.\r\n\r\nIf I directly log the relevant sections while running Inception v1 (5h) on my Pixel with org.tensorflow.demo.env.SplitTimer, I get this:\r\n```\r\n03-27 16:56:47.036  6848  6961 I tensorflow: classifier: preprocessBitmap: cpu=1ms wall=2ms\r\n03-27 16:56:47.053  6848  6961 I tensorflow: classifier: feed: cpu=14ms wall=17ms\r\n03-27 16:56:47.389  6848  6961 I tensorflow: classifier: run: cpu=63ms wall=336ms\r\n03-27 16:56:47.390  6848  6961 I tensorflow: classifier: fetch: cpu=0ms wall=0ms\r\n```\r\n\r\nfeed might be something to take a look at optimizing if possible, but in reality it's only taking about 5% of the total walltime spent processing each frame.", "@asimshankar So, what would have the least overhead?\r\nFilling a previously allocated `float[]` and finally calling `FloatBuffer.wrap(float[])` each time\r\nor calling `FloatBuffer.put(float)` on a previously allocated direct buffer in native byte order?\r\nAnd for the last case: My pre-processing is parallel. Does it matter to the performance of `FloatBuffer.put(int, float)` if there are jumps in position?", "@Androbin : That is probably more suited as a question on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep github issues focused on TensorFlow bugs and feature requests.\r\n\r\nSince the original issue here seems to be some issue with the interaction with the Android Method Tracer, but not really a performance issue with the feed, I'm tempted to close this out. (If you get any more information about how this might be an issue with the TensorFlow Java API, please do create a new issue with as much detail as you can add).\r\n\r\nRegarding your question though, I believe all the following will be pretty much equivalent in terms of performance (they all involve one copy of the data from JVM-managed memory to native memory):\r\n\r\n```java\r\nTensor.create(shape, FloatBuffer.wrap(array));\r\nTensor.create(shape, floatBuffer);\r\n```\r\n\r\nMore details for the performance of various API calls of `java.nio.FloatBuffer` are probably better directed at Java/NIO experts.\r\n\r\nHope that helps.\r\nThanks!", "Okay, after applying some additional optimizations on the computation graph, the \"run\" section makes up for at least a fifth of the \"feed\" section and the \"fetch\" section didn't show up at all by now. Anyway, this still doesn't seem correct just jet.", "Can you provide more details, such as the device you are using, the input size, and the exact timings? I don't understand what you mean by \"run\" making up a fifth of the \"feed\" section. They are run independently and sequentially, and feed should be at least an order of magnitude smaller than run in most cases.\r\n\r\nIs this still using the Android Device Manager in Android Studio to trace code? I don't think you can trust the numbers it provides -- you'd be better off manually timing sections or using another tool until we figure out what's going on there.", "@andrewharp I am sorry, if I was too unclear.\r\nI am using a **Nexus 6P** as host device.\r\nInput size (**12593**) and model architecture are given by #issuecomment-289561458\r\nI will now measure the times myself and post the results.\r\n\r\nSorry for the confusion: I am referring to the (recently renamed) methods of [`TensorFlowInferenceInterface`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java). Previously, `feed` was the only one to be listed at all. After some changes to the computation graph, `run` is now said to account for about a fifth of the time that `feed` takes and `fetch` wasn't listed so far.\r\n\r\nYes, I know that they are independent from one another but they are usually called together.\r\nI am looking forward to my manual measuring results.\r\n\r\nYes, I am still using the Android Monitor over ADB to perform Method Tracing.", "Okay, `TimingLogger` delivers reasonable results:\r\nfeed: 3-4 ms\r\nrun: 250-270 ms\r\nfetch: 0-1 ms", "Right, there's a problem with the adb method tracing, so until we figure out what is going on there it seems you can't rely on the timings it gives."]}, {"number": 8711, "title": "adapted documentation formatting", "body": "Applied formatting of section of\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\r\nto responding section of\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8710, "title": "Random crashing in c2c fft", "body": "I'm having issues with the c2c fft. I would think this was more an issue with my code except that it seems to be non-deterministic. As a result I'm having a hard time getting a minimal example. I'm working on a recurrent neural net, in which the recurrent cell involves an fft (and an ifft), and the crash seems to occur more often as I increase the depth (number of recurrent cells chained together), or as I increase the size of the tensor being fft'd. The problem doesn't seem to scale with the product of these two, though, so I don't immediately think it's a memory issue: rather, it happens only if both are above some sort of weak cutoffs. (40 and 80 causes problems for me consistently.) This is on a GPU with 1.7GB of memory, and not much else in the computation graph, so I don't think I'm hitting constraints there. It's also very odd that the error is so random: I can typically run through 100 batches (each of 1 element) before it crashes, although this number seems random. I find the error message pretty inscrutable -- it reads:\r\n\r\n```\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:169] failed to create cuFFT batched plan:2\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:111] failed to run cuFFT routine cufftSetStream: 1\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: c2c fft failed : in.shape=[2,110]\r\n         [[Node: stack_wrapper_14/FFT = FFT[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack_wrapper_14/Pad/_535)]]\r\n         [[Node: stack_wrapper_34/shift_exp/_1147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2694_stack_wrapper_34/shift_exp\", tensor_type=DT_COMPLEX64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"stacknn_test.py\", line 300, in <module>\r\n    results = sess.run([merged, loss], feed_dict={xs: x, y_true: y})\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: c2c fft failed : in.shape=[2,110]\r\n         [[Node: stack_wrapper_14/FFT = FFT[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack_wrapper_14/Pad/_535)]]\r\n         [[Node: stack_wrapper_34/shift_exp/_1147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2694_stack_wrapper_34/shift_exp\", tensor_type=DT_COMPLEX64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'stack_wrapper_14/FFT', defined at:\r\n  File \"stacknn_test.py\", line 254, in <module>\r\n    (cell_output, state) = cell(inp, state)\r\n  File \"stacknn_test.py\", line 162, in __call__\r\n    tf.pad(state_stack, [[0,0], [0, self._stack_size]])\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 812, in fft\r\n    result = _op_def_lib.apply_op(\"FFT\", input=input, name=name)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\Timeroot\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): c2c fft failed : in.shape=[2,110]\r\n         [[Node: stack_wrapper_14/FFT = FFT[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](stack_wrapper_14/Pad/_535)]]\r\n         [[Node: stack_wrapper_34/shift_exp/_1147 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2694_stack_wrapper_34/shift_exp\", tensor_type=DT_COMPLEX64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nand I could only find one stack overflow question about this: http://stackoverflow.com/questions/41720751/ignoring-warning-unable-to-load-cudnn-dso-effecting-fft-computation which ended up only getting some comment about the cudnn DSO. I don't have the cudnn DSO either, but if /that's/ what's causing this problem, I would be surprised.\r\n\r\nUsing Tensorflow version 1.0.1, the Windows GPU build. Python version 3.5.3. The machine has a single GPU with compute capabilities 3.0. It has no problem loading cublas64_80.dll, cufft64_80.dll,  nvcuda.dll, and curand64_80.dll, but I don't have a cudnn dso.", "comments": ["@mrry, any knowledge of whether windows fft is known broken work?", "That looks like an out-of-memory error (return code 2\u2014from `failed to create cuFFT batched plan:2`\u2014is `CUFFT_ALLOC_FAILED`)."]}, {"number": 8709, "title": "gcc: error: unrecognized command line option '-fno-canonical-system-headers'", "body": "when I run :\r\n`bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nINFO: Found 1 target...\r\nERROR: /home/qs/.cache/bazel/_bazel_qs/081cd1dbca77dcff65c775e7e860e873/external/farmhash_archive/BUILD.bazel:12:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/qs/.cache/bazel/_bazel_qs/081cd1dbca77dcff65c775e7e860e873/execroot/tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64/:/usr/lib32/:/usr/lib/x86_64-linux-gnu/::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 \\\r\n    PATH=/bin:/usr/bin:/home/qs/mysoft/jdk1.8/bin \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.d '-frandom-seed=bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o' -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/farmhash_archive/src/farmhash.cc -o bazel-out/host/bin/external/farmhash_archive/_objs/farmhash/external/farmhash_archive/src/farmhash.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error: unrecognized command line option '-fno-canonical-system-headers'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.003s, Critical Path: 0.49s\r\n\r\nHow to solve this problem?", "comments": ["I use ubuntu14.04\r\n         cuda7.5\r\n         cudnn5.1\r\n         gcc4.7.3\r\n         bazel0.4.5\r\n         tensorflow r1.0", "@jart, could you take a look. Could it be too old of a version of gcc?\r\n", "Thank you for bringing this to our attention @Anida-qin. It seems you did your homework and discovered a closed duplicate https://github.com/tensorflow/tensorflow/issues/3618 although it helps when you bring that to our attention.\r\n\r\nIt seems the Bazel site [claims](https://bazel.build/versions/master/docs/install-ubuntu.html) Ubuntu 14.04 is out of their support matrix, which I find peculiar, because that's what Google installed on my workstation. It's not EOL until 2019. It is in the TensorFlow support matrix. (Attn: @damienmg)\r\n\r\n@Anida-qin GCC 4.8 is [available](http://packages.ubuntu.com/trusty/allpackages) on Ubuntu 14.04. If you were to apt-get install those, would it help?", "Ubuntu 14.04 is supported. That was an update error (adding 16.04 in the\nlist was the actual intent).\n\nOn Wed, Mar 29, 2017, 8:18 AM Justine Tunney <notifications@github.com>\nwrote:\n\n> Thank you for bringing this to our attention @Anida-qin\n> <https://github.com/Anida-qin>. It seems you did your homework and\n> discovered a closed duplicate #3618\n> <https://github.com/tensorflow/tensorflow/issues/3618> although it helps\n> when you bring that to our attention.\n>\n> It seems the Bazel site claims\n> <https://bazel.build/versions/master/docs/install-ubuntu.html> Ubuntu\n> 14.04 is out of their support matrix, which I find peculiar, because that's\n> what Google installed on my workstation. It's not EOL until 2019. It is in\n> the TensorFlow support matrix. (Attn: @damienmg\n> <https://github.com/damienmg>)\n>\n> @Anida-qin <https://github.com/Anida-qin> GCC 4.8 is available\n> <http://packages.ubuntu.com/trusty/allpackages> on Ubuntu 14.04. If you\n> were to apt-get install those, would it help?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8709#issuecomment-289993618>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHfzkj9HqIIbQdP_JariVpREnf4pL1ks5rqfeZgaJpZM4Mo_el>\n> .\n>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "What is the solution to this problem? I am running it on a CentOS 6 cluster and gcc 4.8.2 is installed on cluster and using bazel 0.4.5", "What is the solution to this problem? I am running it on a\r\n centos 6.7\r\nBazel 0.11.1\r\nCUDA 8.0,\r\n cuDNN 6.0", "+1", "As @jart mentioned, could you try updating to gcc 4.8 and try with that?\r\n", "I had a lot of similar weird compilation issues os OS X. I had to [completely uninstall `bazel`](https://github.com/bazelbuild/bazel/issues/838) and remove its cached files like this:\r\n\r\n    sudo rm -rf /private/var/tmp/_bazel_martin"]}, {"number": 8708, "title": "Permissions error when building with bazel", "body": "### Environment info\r\nOperating System:\r\nMacOS Sierra 10.12.3\r\nCUDA Version 8.0.62\r\nCUDNN version 5.1.10\r\nBazel Version: Build label: 0.4.5\r\n\r\ngit HEAD is c7b80d51da4fb6d51ea54a0bdf2601afa379d60c\r\n\r\n**Seems to be a permissions issue of some sort where it can't execute?**\r\n\r\nNot sure what permissions to change so just tried to rebuild with bazel clean but I don't think that's the issue. Looked up a bunch of similar cases but they are out of date.\r\n### Logs \r\nOriginal Command was \r\n```\r\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nError after running for 3556s was\r\n```\r\nERROR: /Users/Kevin/tensorflow/tensorflow/cc/BUILD:388:1: Executing genrule //tensorflow/cc:remote_fused_graph_ops_genrule failed: bash failed: error executing command\r\n  (cd /private/var/tmp/_bazel_Kevin/e120589e94215f409fa40a9ac20b2fce/execroot/tensorflow && \\\r\n  exec env - \r\n  ... \r\n  ...\r\n\r\n\r\n```", "comments": ["Our CI does not exhibit this problem. So I think this issue is specific to your machine setup.\r\nCould you paste the full error message?\r\nBazel usually also returns the output of the failed command. I cannot see the permission error you are referring to from your paste.", "@gunan I have pasted my error here. https://gist.github.com/kevihong/a9200bc4a2ff3286f390d3b52872b48c\r\nThe trials are in respective order, all I changed were some flags such as \r\n```\r\n--genrule_strategy=standalone --spawn_strategy=standalone\r\n```\r\nThank you for your assistance. ", "You seem to be running into this issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/6729\r\n\r\nPlease try the suggestions there.", "resolved."]}, {"number": 8707, "title": "Update Tensorboard readme with fixed links, fixes #8706", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8706, "title": "Broken links in TensorBoard README", "body": "In https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md,\r\nlinks for both \"TensorBoard: Visualizing Learning\" and \"TensorBoard: Graph Visualization\" lead to \"page not found\".  ", "comments": ["Just submitted a PR ( #8707 )"]}, {"number": 8705, "title": "Which is TensorFlow's best seq2seq? r1.0 or r1.1? Both are much different!!", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["We believe that we are making continuous improvements to the code, so a higher version is usually better. That said, if you are concerned about stability, a conservative approach is to use a version which has been released for a few months. \r\n\r\nNote that since `1.0`, we have committed to a compatibility promise. Everything that works in `1.0` will work in any `1.x` series. All in 1.x should also work in 1.y where `y > x`."]}, {"number": 8704, "title": "TensorFlow drops the first batch?", "body": "### Description\r\n\r\nI'm trying to understand how TF generates batches from `TFRecord` file format, how to implement the basic idea of evaluate the whole validation dataset after a full epoch so I've done a small experiment.\r\n\r\nBasically I've added an `image/id` key in the `TFRecord` file that looks like this:\r\n```\r\n'image/id': tf.FixedLenFeature(shape=[], dtype=tf.int64)\r\n'id': slim.tfexample_decoder.Tensor('image/id')\r\n```\r\n\r\nThe values of these ids are just numbers increasing from 0 to `num_samples - 1`. Then I'm reading the data like this:\r\n\r\n```\r\n...\r\ndata_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset, shuffle=False)\r\nraw_image, instance_id, label = data_provider.get(items=['image', 'id', 'label'])\r\n...\r\nnum_threads = 1\r\nimages, instance_ids, labels = tf.train.batch(tensors=[image, instance_id, label],\r\n                                                      batch_size=batch_size,\r\n                                                      num_threads=num_threads,\r\n                                                      capacity = batch_size,\r\n                                                      allow_smaller_final_batch=False)        \r\n...\r\n```\r\n\r\nI've used `shuffle=False`, `num_threads=1` and `capacity=batch_size` to ensure we are reading the instances in the validation set in order.\r\n\r\nI've then defined a `train_step_fn` that evaluates a mini-batch from the validation set after each step. I'm doing this to check and test we are not returning instances with duplicate ids in the same batch. The code looks like this:\r\n\r\n```\r\n    def train_step_fn(session, *args, **kwargs):\r\n        total_loss, should_stop = train_step(session, *args, **kwargs)\r\n        curr_global_step = tf.train.global_step(session, global_step)\r\n        curr_epoch = curr_global_step / validation_every_n_steps\r\n        image, val_ids, val_loss, accuracy = session.run([validation_images, validation_ids, total_validation_loss, validation_accuracy])\r\n        print(val_ids)\r\n        def float_formatter(x): return \"%.4f\" % x\r\n        tf.logging.info('after global step {} (epoch {}): validation loss = {}, validation accuracy = {}'\r\n                            .format(curr_global_step, curr_epoch, float_formatter(val_loss), float_formatter(accuracy)))\r\n        return [total_loss, should_stop]\r\n```\r\n\r\nMost of the output I'm seeing makes sense but the output from the first batch looks weird.\r\n\r\n```\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:global_step/sec: 0\r\nINFO:tensorflow:global step 1: loss = 4.1099 (3.70 sec/step)\r\n[17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]\r\nINFO:tensorflow:after global step 1 (epoch 0): validation loss = 4.6337, validation accuracy = 0.0000\r\nINFO:tensorflow:global step 2: loss = 4.2118 (0.52 sec/step)\r\n[33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48]\r\nINFO:tensorflow:after global step 2 (epoch 0): validation loss = 3.9538, validation accuracy = 0.0625\r\nINFO:tensorflow:global step 3: loss = 4.0593 (0.75 sec/step)\r\n[49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64]\r\nINFO:tensorflow:after global step 3 (epoch 0): validation loss = 4.0773, validation accuracy = 0.0000\r\nINFO:tensorflow:global step 4: loss = 3.7749 (0.64 sec/step)\r\n[65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80]\r\nINFO:tensorflow:after global step 4 (epoch 0): validation loss = 4.0847, validation accuracy = 0.0625\r\nINFO:tensorflow:global step 5: loss = 3.7973 (0.68 sec/step)\r\n[81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96]\r\nINFO:tensorflow:after global step 5 (epoch 0): validation loss = 3.5591, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 6: loss = 3.5584 (0.72 sec/step)\r\n[ 97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112]\r\nINFO:tensorflow:after global step 6 (epoch 0): validation loss = 3.6849, validation accuracy = 0.1250\r\nINFO:tensorflow:global step 7: loss = 3.5303 (0.70 sec/step)\r\n[113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128]\r\nINFO:tensorflow:after global step 7 (epoch 0): validation loss = 3.7812, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 8: loss = 3.2698 (0.85 sec/step)\r\n[129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\r\nINFO:tensorflow:after global step 8 (epoch 0): validation loss = 3.3004, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 9: loss = 3.4842 (0.94 sec/step)\r\nINFO:tensorflow:global_step/sec: 0.915008\r\n[145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\r\nINFO:tensorflow:after global step 9 (epoch 0): validation loss = 4.0377, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 10: loss = 3.5635 (1.48 sec/step)\r\n[178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193]\r\nINFO:tensorflow:after global step 10 (epoch 0): validation loss = 3.7854, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 11: loss = 3.1773 (0.65 sec/step)\r\n[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209]\r\n```\r\n\r\nI'm assuming the first mini-batch returned should be `[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]` instead of `[17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]`. Plus the fact that we are starting from 17 instead of 16 also bothers me a bit. It seems for some unknown reason TF is dropping a batch.\r\n\r\nI understand this probably won't affect the model training / validation process at all but I'm still pointing this out in case this is hiding a more serious root cause.", "comments": ["More logging output. You can see that there is a dropped batch before global step 1, after global step 10 and after global step 23.\r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from /home/derekhh/Downloads/inception_v1.ckpt\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:global_step/sec: 0\r\nINFO:tensorflow:global step 1: loss = 4.2185 (4.03 sec/step)\r\n[33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\r\n 58 59 60 61 62 63 64]\r\nINFO:tensorflow:after global step 1 (epoch 0): validation loss = 4.3316, validation accuracy = 0.0312\r\n2017-03-24 17:05:25.046604: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3439 get requests, put_count=3003 evicted_count=1000 eviction_rate=0.333 and unsatisfied allocation rate=0.446641\r\n2017-03-24 17:05:25.046640: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\nINFO:tensorflow:global step 2: loss = 4.0838 (0.35 sec/step)\r\n[65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89\r\n 90 91 92 93 94 95 96]\r\nINFO:tensorflow:after global step 2 (epoch 0): validation loss = 3.8756, validation accuracy = 0.0312\r\nINFO:tensorflow:global step 3: loss = 3.8911 (0.47 sec/step)\r\n[ 97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\r\n 115 116 117 118 119 120 121 122 123 124 125 126 127 128]\r\nINFO:tensorflow:after global step 3 (epoch 0): validation loss = 3.7271, validation accuracy = 0.1562\r\nINFO:tensorflow:global step 4: loss = 3.8750 (0.46 sec/step)\r\n[129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\r\n 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\r\nINFO:tensorflow:after global step 4 (epoch 0): validation loss = 3.8309, validation accuracy = 0.0938\r\nINFO:tensorflow:global step 5: loss = 3.7110 (0.43 sec/step)\r\n[161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\r\n 179 180 181 182 183 184 185 186 187 188 189 190 191 192]\r\nINFO:tensorflow:after global step 5 (epoch 0): validation loss = 3.7713, validation accuracy = 0.1250\r\nINFO:tensorflow:global step 6: loss = 3.6118 (0.47 sec/step)\r\n[193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210\r\n 211 212 213 214 215 216 217 218 219 220 221 222 223 224]\r\nINFO:tensorflow:after global step 6 (epoch 0): validation loss = 3.4648, validation accuracy = 0.1250\r\nINFO:tensorflow:global step 7: loss = 3.5667 (0.51 sec/step)\r\n[225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\r\n 243 244 245 246 247 248 249 250 251 252 253 254 255 256]\r\nINFO:tensorflow:after global step 7 (epoch 0): validation loss = 3.2306, validation accuracy = 0.1250\r\nINFO:tensorflow:global step 8: loss = 3.3802 (0.76 sec/step)\r\n[257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\r\n 275 276 277 278 279 280 281 282 283 284 285 286 287 288]\r\nINFO:tensorflow:after global step 8 (epoch 0): validation loss = 3.4580, validation accuracy = 0.0938\r\nINFO:tensorflow:global step 9: loss = 3.3113 (0.71 sec/step)\r\n[289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\r\n 307 308 309 310 311 312 313 314 315 316 317 318 319 320]\r\nINFO:tensorflow:after global step 9 (epoch 0): validation loss = 3.4953, validation accuracy = 0.2188\r\nINFO:tensorflow:global step 10: loss = 3.7665 (0.74 sec/step)\r\n[321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\r\n 339 340 341 342 343 344 345 346 347 348 349 350 351 352]\r\nINFO:tensorflow:after global step 10 (epoch 0): validation loss = 3.3358, validation accuracy = 0.1875\r\nINFO:tensorflow:global_step/sec: 1.048\r\nINFO:tensorflow:global step 11: loss = 3.5452 (0.81 sec/step)\r\n[17 18 19 20 21 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\r\n 43 44 45 46 47 48 49]\r\nINFO:tensorflow:after global step 11 (epoch 0): validation loss = 3.9426, validation accuracy = 0.0625\r\nINFO:tensorflow:global step 12: loss = 3.2217 (1.05 sec/step)\r\n[50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\r\n 75 76 77 78 79 80 81]\r\nINFO:tensorflow:after global step 12 (epoch 0): validation loss = 3.9321, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 13: loss = 2.7974 (0.65 sec/step)\r\n[ 82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\r\n 100 101 102 103 104 105 106 107 108 109 110 111 112 113]\r\nINFO:tensorflow:after global step 13 (epoch 0): validation loss = 3.5182, validation accuracy = 0.1875\r\nINFO:tensorflow:global step 14: loss = 2.7670 (0.65 sec/step)\r\n[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\r\n 132 133 134 135 136 137 138 139 140 141 142 143 144 145]\r\nINFO:tensorflow:after global step 14 (epoch 0): validation loss = 3.5026, validation accuracy = 0.2500\r\nINFO:tensorflow:global step 15: loss = 2.6559 (0.64 sec/step)\r\n[146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\r\n 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\r\nINFO:tensorflow:after global step 15 (epoch 0): validation loss = 3.6042, validation accuracy = 0.2812\r\nINFO:tensorflow:global step 16: loss = 2.7270 (0.44 sec/step)\r\n[178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195\r\n 196 197 198 199 200 201 202 203 204 205 206 207 208 209]\r\nINFO:tensorflow:after global step 16 (epoch 0): validation loss = 3.7182, validation accuracy = 0.1250\r\nINFO:tensorflow:global step 17: loss = 2.6056 (0.66 sec/step)\r\n[210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227\r\n 228 229 230 231 232 233 234 235 236 237 238 239 240 241]\r\nINFO:tensorflow:after global step 17 (epoch 0): validation loss = 3.1518, validation accuracy = 0.3125\r\nINFO:tensorflow:global step 18: loss = 2.4688 (0.80 sec/step)\r\n[242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\r\n 260 261 262 263 264 265 266 267 268 269 270 271 272 273]\r\nINFO:tensorflow:after global step 18 (epoch 0): validation loss = 2.8809, validation accuracy = 0.3438\r\nINFO:tensorflow:global step 19: loss = 2.7824 (0.86 sec/step)\r\n[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\r\n 292 293 294 295 296 297 298 299 300 301 302 303 304 305]\r\nINFO:tensorflow:after global step 19 (epoch 0): validation loss = 3.3889, validation accuracy = 0.2188\r\nINFO:tensorflow:global step 20: loss = 2.4856 (0.64 sec/step)\r\n[306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\r\n 324 325 326 327 328 329 330 331 332 333 334 335 336 337]\r\nINFO:tensorflow:after global step 20 (epoch 0): validation loss = 2.2776, validation accuracy = 0.5000\r\nINFO:tensorflow:global step 21: loss = 2.6202 (0.48 sec/step)\r\n[338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\r\n 356 357 358 359 360 361 362 363 364 365 366 367   0   1]\r\nINFO:tensorflow:after global step 21 (epoch 0): validation loss = 2.7438, validation accuracy = 0.3438\r\nINFO:tensorflow:global step 22: loss = 1.9660 (0.62 sec/step)\r\n[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\r\n 27 28 29 30 31 32 33]\r\nINFO:tensorflow:after global step 22 (epoch 0): validation loss = 2.0266, validation accuracy = 0.4062\r\nINFO:tensorflow:global step 23: loss = 2.3677 (0.77 sec/step)\r\n[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58\r\n 59 60 61 62 63 64 65]\r\nINFO:tensorflow:after global step 23 (epoch 0): validation loss = 2.3970, validation accuracy = 0.2812\r\nINFO:tensorflow:global step 24: loss = 2.0792 (0.74 sec/step)\r\n[ 98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\r\n 116 117 118 119 120 121 122 123 124 125 126 127 128 129]\r\nINFO:tensorflow:after global step 24 (epoch 0): validation loss = 2.2340, validation accuracy = 0.4375\r\nINFO:tensorflow:global_step/sec: 1.40001\r\nINFO:tensorflow:global step 25: loss = 2.0135 (1.37 sec/step)\r\n[130 131 132 133 134 135 137 138 139 140 141 142 143 144 145 146 147 148\r\n 149 150 151 152 153 154 155 156 157 158 159 160 161 162]\r\nINFO:tensorflow:after global step 25 (epoch 0): validation loss = 1.7657, validation accuracy = 0.6250\r\n2017-03-24 17:05:42.493672: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4038 get requests, put_count=4063 evicted_count=1000 eviction_rate=0.246124 and unsatisfied allocation rate=0.247152\r\n2017-03-24 17:05:42.493700: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281\r\nINFO:tensorflow:global step 26: loss = 2.0798 (0.61 sec/step)\r\n[163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\r\n 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\r\nINFO:tensorflow:after global step 26 (epoch 0): validation loss = 2.2265, validation accuracy = 0.4688\r\nINFO:tensorflow:global step 27: loss = 1.8638 (0.70 sec/step)\r\n[195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\r\n 213 214 215 216 217 218 219 220 221 222 223 224 225 226]\r\nINFO:tensorflow:after global step 27 (epoch 0): validation loss = 1.9837, validation accuracy = 0.5312\r\nINFO:tensorflow:global step 28: loss = 1.9316 (0.87 sec/step)\r\n[227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\r\n 245 246 247 248 249 250 251 252 253 254 255 256 257 258]\r\nINFO:tensorflow:after global step 28 (epoch 0): validation loss = 2.5838, validation accuracy = 0.4375\r\nINFO:tensorflow:global step 29: loss = 1.6807 (0.50 sec/step)\r\n[259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276\r\n 277 278 279 280 281 282 283 284 285 286 287 288 289 290]\r\nINFO:tensorflow:after global step 29 (epoch 0): validation loss = 1.6319, validation accuracy = 0.6250\r\nINFO:tensorflow:global step 30: loss = 1.5066 (0.50 sec/step)\r\n[291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\r\n 309 310 311 312 313 314 315 316 317 318 319 320 321 322]\r\nINFO:tensorflow:after global step 30 (epoch 0): validation loss = 1.8426, validation accuracy = 0.5625\r\nINFO:tensorflow:global step 31: loss = 1.6865 (0.74 sec/step)\r\n[323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\r\n 341 342 343 344 345 346 347 348 349 350 351 352 353 354]\r\nINFO:tensorflow:after global step 31 (epoch 0): validation loss = 1.8748, validation accuracy = 0.5312\r\nINFO:tensorflow:global step 32: loss = 1.3899 (0.65 sec/step)\r\n[355 356 357 358 359 360 361 362 363 364 365 366 367   0   1   2   3   4\r\n   5   6   7   8   9  10  11  12  13  14  15  16  17  18]\r\nINFO:tensorflow:after global step 32 (epoch 0): validation loss = 1.5526, validation accuracy = 0.5312\r\n````", "Well...I've found that I've added `validation_loss` to the list of summary tensors and this gets called every `save_summaries_secs` seconds. Setting `summary_op` as `None` in `slim.learning.train` fixes the problem.\r\n\r\nI'm sorry for this false alarm."]}, {"number": 8703, "title": "Debug tool not fully installed in Windows version", "body": "First of All: WINDOWS environment!\r\n\r\nI have installed the tensorflow 1.0 (Python 3.5, Cuda 8.0, Cudnn 5) in Windows using the official command pip3 install --upgrade tensorflow-gpu. Although there are some minor bugs, I have fixed them by myself, and the tensorflow mostly works OK. However, when using the tensorflow debug tool, I find it failed. \r\n\r\nThe reason is that the python files in folder tensorflow\\python\\debug are not generated from the most updated proto files. For example, in the generated \"debug_pb2.py\", there are no \"global_step\" for Message DebugOptions. I checked the most updated tensorflow codes, the debug.proto does have that \"global_step\". \r\n\r\nI have done some manual modifications of the debug_pb2.py to add necessary parameters. Now the error is \r\n\r\nFile \"C:\\Users\\XXX\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\debug\\lib\\debug_data.py\", line 519, in __init__\r\n    raise IOError(\"Dump root directory %s does not exist\" % dump_root)\r\nOSError: Dump root directory C:\\Users\\XXX\\AppData\\Local\\Temp\\tfdbg_8u5q2tve does not exist\r\n\r\nI have already run the codes under the administrator mode. \r\n\r\nMy questions are:\r\n1. Whether there is any method to just re-generate the debug tool related files using the most updated proto files stored in my local disk?\r\n2. Is there any way to rebuild the tensorflow using the online source codes, which seem to be for Linux?\r\n3. If not possible, is there anyway to manual fix the above error?\r\n\r\nThank you.\r\n", "comments": ["@ybsave TensorFlow Debugger (tfdbg) is known to have issues in version 1.0.x on Windows. See issue https://github.com/tensorflow/tensorflow/issues/7615. Can you try the latest release candidate, namely 1.1.0rc0?\r\n\r\nAlso beware that the debug_pb2.py file is not an essential feature of tfdbg. It is related to something still under development.", "The new version 1.1.0rc0 works well. The problem disappears. Thank you so much for your quick help!", "I have a ImportError: cannot import name 'debug_pb2' in ubuntu 16.04 when importing tensorflow\r\n", "\r\n@Charan-Karthikeyan, I tried the following commands, in the ubuntu:16.04 docker container and it seems to work correctly. Is it possible that your environment has some older versions of tensorflow causing some sort of interference? If so, can you try uninstalling them?\r\n\r\n```\r\ndocker run -it --rm docker:16.04\r\napt-get update\r\napt-get install python python-pip\r\npip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0rc0-cp27-none-linux_x86_64.whl\r\npython -m tensorflow.python.debug.examples.debug_mnist --debug\r\n\r\n```"]}, {"number": 8702, "title": "[bug] bazel cudnn-version-check is broken on python 2.7 / ubuntu 14.04", "body": "When I ./configure on /r1.1 or /r1.0, I get the following error:\r\n\r\n```\r\nAuto-Configuration Error: cuDNN version detected from /usr/local/cuda-8.0/include/cudnn.h (      5.      1. 5) does not match TF_CUDNN_VERSION (5.1.5)\r\n```\r\nFor what it is worth, I counted the space in cudnn.h, and they match the number of spaces in that error message above. Not sure if related.\r\n```\r\n#define CUDNN_MAJOR      5\r\n#define CUDNN_MINOR      1\r\n#define CUDNN_PATCHLEVEL 5\r\n```\r\n\r\n\r\n```\r\n$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] \r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n/usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.5\r\nPlease specify the location where cuDNN 5.1.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n..........\r\nERROR: package contains errors: tensorflow/core/kernels/cloud.\r\nERROR: error loading package 'tensorflow/core/kernels/cloud': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl\", line 815\r\n\t\t_create_cuda_repository(repository_ctx)\r\n\tFile \"/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl\", line 728, in _create_cuda_repository\r\n\t\t_get_cuda_config(repository_ctx)\r\n\tFile \"/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl\", line 584, in _get_cuda_config\r\n\t\t_cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n\tFile \"/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl\", line 311, in _cudnn_version\r\n\t\tauto_configure_fail(\"cuDNN version detected from %s ...))\r\n\tFile \"/mnt/6tb_internal/cnn_experiments/tensorflow_src/tensorflow/third_party/gpus/cuda_configure.bzl\", line 93, in auto_configure_fail\r\n\t\tfail(\"\r\n%sAuto-Configuration Error:%s ...))\r\n\r\nAuto-Configuration Error: cuDNN version detected from /usr/local/cuda-8.0/include/cudnn.h (      5.      1. 5) does not match TF_CUDNN_VERSION (5.1.5)\r\n```\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   558720 Sep 14  2016 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Sep 14  2016 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov  7 13:54 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n```\r\nr1.0, r1.1\r\n\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n```\r\n\r\n", "comments": ["This line seems to be missing an `rstrip()`, but the error happens elsewhere. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69/third_party/gpus/cuda_configure.bzl#L280", "Your diagnosis looks correct to me.\r\nWould you like to create a pull request?\r\nYou will also be able to verify your solution works for you locally.", "@gunan \r\nApparently, the bzl file contents are being run on some super-restricted subset of python, with questionable behavior. It can't seem to import, and is missing functions like `ord` and `print`. `lstrip` also has strange behavior. The only way I can get anything to print to console was to use the `auto_configure_fail` function.\r\n\r\nI added these lines as a diagnostic, and the results are very strange. (the lines were inserted right before the location I mentioned above, as bazel is trying to parse cudnn.h to figure out the installed cudnn versions).\r\nhttps://github.com/tensorflow/tensorflow/blob/ede5ebe54f0689a53eeaf0fb1c9a25f136ab3a69/third_party/gpus/cuda_configure.bzl#L280\r\n```\r\nspace_padded = lines[0].replace(define, \"\")  \r\nmanual = ''.join([c for c in space_padded if c in \"0123456789\"])\r\nauto_configure_fail(manual + \"/\" + lines[0] + \"/\" + space_padded + \"/\" + space_padded.lstrip()  )\r\n```\r\n\r\nThe output is \r\n```\r\nAuto-Configuration Error: 01315/#define CUDNN_MAJOR      5/      5/      5\r\n```\r\nIt appears that basic character iteration and `lstrip` are both broken. `ord` is not available so I couldn't print out hex-values easily for the string characters.\r\nI think there is something wrong with the python interpreter that is being used by bazel.", "bzl files are written in skylark. It is similar to python, but does not run under a python interpreter.\r\nYou can read more about it here:\r\nhttps://bazel.build/versions/master/docs/skylark/index.html", "@gunan thank you for that. I was so confused and frustrated earlier! I have submitted a fix for systems like mine.", "Should I be aiming for r1.1 instead of master with my pull request? https://github.com/tensorflow/tensorflow/pull/8784", "No, all pull requests should first go into master.\r\nThanks for the contribution!\r\n"]}, {"number": 8701, "title": "InvalidArgumentError using tf.learn and eval", "body": "See https://github.com/google/seq2seq/issues/103 for details and user logs.\r\n\r\nTLDR; I'm using tf.learn and for some people the evaluation fails with shape errors. This seems to be some kind of GPU memory sharing issue, as subsequent runs seem to consistently increase the shape size:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [1280,36240] and labels shape [6272]\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2304,36240] and labels shape [6272]\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [4480,36240] and labels shape [6272]\r\n```\r\n\r\nEvaluation works independently when there is no training in progress. It also doesn't happen when using the CPU only.\r\n\r\nI personally have run into similar issues before then multiple processes were trying to share the GPU, but that shouldn't be the case here.", "comments": ["@dennybritz, can you identify which line of python caused this error? My guess is this is related to the argument reorder on metrics... from the TensorFlow release notes.\r\n```\r\nChange arg order for {softmax,sparse_softmax,sigmoid}_cross_entropy_with_logits to be (labels, predictions), and force use of named args.\r\n```\r\nSo go find those and insert explicit keyword argument labels appropriately and it should fix it. Let me know if that fixes you problem. Thanks\r\n", "I don't think that is the issue. All arguments are named and it only happens to some users (I don't have this problem myself). Also, if you read the original issue you can see that some users managed to \"fix\" it by disabling bucketing or reducing the dev data.\r\n\r\nIt looks very much like some kind of GPU memory issue to me.", "I'm sorry I missed that. I've seen many examples of shape errors being caused by that change, so it is worth a shot.\r\n\r\n Since this issue links to a incomplete summary and then 4 more issues which are themselves just blobs of erros, it's pretty hard to follow what is going on. Let me ask a few clarifying questions, since you've spent a good deal of time looking at this it appears.\r\n\r\n1. What versions of TensorFlow are people using?  \"compiled from source from the master branch\" is not useful since master changes all the time.\r\n2. Did all of them manage to run it on CPU only successfully?\r\n3. What versions of CUDA are people using? Maybe there is a pattern.\r\n4. What is the command I can run to try this example? Where is the code? \r\n\r\n\r\n\r\n", "1. `1.0.0` or `1.0.1` - The code only works with that version.\r\n2. The people I've talked to, yes. But I will try to ask more.\r\n3. Not sure, will try to ask.\r\n4. The command in this tutorial: https://google.github.io/seq2seq/nmt\r\n\r\n", "So, this seems to be related to `train_and_evaluate` in tf.learn, see https://github.com/google/seq2seq/issues/103#issuecomment-293796457.\r\n\r\nIt seems like the only solution right now is to monkeypatch the Experiment class?", "This has the patch: https://github.com/google/seq2seq/pull/173", "Did you mean to reopen?", "Yes. I patched it in my code, but that doesn't seem right. I wonder what\nthe \"official\" solution is.\nOn Mon, Apr 17, 2017 at 3:59 PM drpngx <notifications@github.com> wrote:\n\n> Did you mean to reopen?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8701#issuecomment-294619545>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAYmvTlOCPdaVSYW6UmaSTRNSCvMiV88ks5rw-63gaJpZM4MoqzO>\n> .\n>\n", "OK, I wonder if it's best to use the seq2seq thread or this one. @sguada what do you think of this issue? It appears that it is related to `tf.learn`.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I meet the same problem.\r\n\r\nTF versio: 1.3.1\r\npython version: 3.5.2\r\ncode: https://gist.github.com/jinyu121/2c2e22f984a993a86f11f1be98f1d14c (Sorry for the Chinese comments... It's just my note)\r\ndata: the flower dataset used in tensorflow examples\r\ndata convert: create train and test TXT file with the format `image_path image_label`\r\nlog:  [log.log](https://github.com/tensorflow/tensorflow/files/1351702/log.log)\r\n"]}, {"number": 8700, "title": "Branch 151147354", "body": "", "comments": []}, {"number": 8699, "title": "Typo in README.md", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8698, "title": "Graph Transforms | quantize_nodes | Android TensorFlow Inference | native error", "body": "I am using the Graph Transforms Tool (built with Bazel) to prepare my graph for Android TensorFlow Inference.\r\n\r\nWhen using `--transforms='quantize_weights'` everything works just fine.\r\nBut with `--transforms='quantize_weights quantize_nodes'` I get this:\r\n\r\n```\r\nI/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\nI/TensorFlowInferenceInterface: TensorFlow native methods already loaded\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 22562 (obin.loremipsum)\r\n        \r\n        [ 03-24 19:19:10.194   375:  375 W/         ]\r\n        debuggerd: handling request: pid=22562 uid=10173 gid=10173 tid=22562\r\n```\r\n\r\nI don't have any clue on what has happened.\r\nI wasn't able to find anything related online.", "comments": ["Please specify your platform version and versions of TensorFlow @Androbin.\r\nAre there any usual pitfalls @petewarden that cause a segfault like this?", "I built TensorFlow from source (last update: yesterday) with CUDA 8.0 and cuDNN 5.1 on Ubuntu 16.04.2, downloaded the Inference Library from [nightly-android#79](https://ci.tensorflow.org/view/Nightly/job/nightly-android/79/) and running the inference on a Nexus 6P as host device.", "I experimented a bit with the graph transformation and discovered that the problem seems to be `merge_duplicate_nodes` which is called by `quantize_nodes` #8897", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Waiting for merge of probable fix:\r\nhttps://github.com/tensorflow/tensorflow/issues/11182#issuecomment-315771292"]}, {"number": 8697, "title": "Lower libc version on tensorflow:1.1.0-rc0-devel", "body": "Building libtensorflow.so with the Docker 1.10-devel-rc0 image results in a binary that requires glibc 2.23. Since that is aggressive for many current machines, might it make sense to reduce the dependency to eg 2.17? ", "comments": ["@craigcitro @gunan ", "I'm out of the loop, @gunan or @martinwicke seems more likely to have a good answer here.", "@jhseu @caisq FYI\r\n\r\nThe requirement is due to docker images being based off of ubuntu 16.04, which has a higher glibc version. Our prebuilt binaries are built on ubuntu 14.04, which does not have this requirement.", "@malmaud, you want to build a custom version of libtensorflow, right? \r\nOur release libtensorflow is still built with 14.04, e.g., http://ci.tensorflow.org/view/Release/job/release-libtensorflow/20/TYPE=cpu-slave/", "@yifeif FYI\r\n\r\nYou can edit the dockerfile to be `FROM ubuntu14.10`, which should fix this issue. We upgraded to ubuntu16 since 14 is getting seriously old.", "Also, we should probably remove Craig from the MAINTAINER line. ", "Sure, for my own purposes I can just edit the FROM line and rebuild. \r\n\r\nI think an argument could potentially be made that if the purpose of the developer images is primarily to enable building TensorFlow, it would make sense to try to configure those images in a way that the resulting binaries are compatible with as wide a collection of systems as possible, which isn't true if they are linked against libc 2.23. But if you guys feel differently, no problem and thanks for your help. ", "There are bugs when running on Ubuntu 14.04, so we'd prefer to keep any images where users might run TensorFlow on more recent versions of Ubuntu."]}, {"number": 8696, "title": "TensorFlow hangs during training while using with tf.device('/device:CPU:0'):", "body": "### Description\r\n\r\nI'm trying to fine-tune an Inception-V1 model and my latest implementation is based on [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py). I've noticed that sometimes TF hangs (might be similar to Issue [#2788](https://github.com/tensorflow/tensorflow/issues/2788)) and I'm trying to figure out why. \r\n\r\nThe following snippet is my `load_batch` function. I've also used this `session_config=tf.ConfigProto(operation_timeout_in_ms=60000)` to throw a `DeadlineExceededError` when things timed out.\r\n\r\nI've noticed that if I remove the `with tf.device('/device:CPU:0')` line, I can run the model for a whole night without any issue. If I add this line back I'll get a `DeadlineExceededError` fairly quickly (at around 1K steps with a batch size of 32) and I have a consistent repro.\r\n\r\n```\r\ndef load_batch(dataset, batch_size, height, width):\r\n    dataset_basename = os.path.basename(dataset.data_sources)\r\n    with tf.device('/device:CPU:0'):\r\n        with tf.name_scope(name=dataset_basename):\r\n            data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset)\r\n            raw_image, label = data_provider.get(items=['image', 'label'])\r\n            tf.summary.image('raw_image', tf.expand_dims(input=raw_image, axis=0))\r\n            image = tf.cast(raw_image, tf.float32) / 255.0\r\n            image = tf.image.resize_images(images=image, size=[height, width], align_corners=True)\r\n            tf.summary.image('resized_image', tf.expand_dims(input=image, axis=0))\r\n            # TensorFlow recommendation:\r\n            # min_after_dequeue + (num_threads + a small safety margin) * batch_size\r\n            # https://www.tensorflow.org/programmers_guide/reading_data\r\n            num_threads = 8\r\n            images, labels = tf.train.batch(tensors=[image, label],\r\n                                            batch_size=batch_size,\r\n                                            num_threads=num_threads,\r\n                                            capacity=(num_threads + 2) * batch_size,\r\n                                            allow_smaller_final_batch=True)\r\n    return images, labels\r\n```\r\n\r\nThe error message is:\r\n```\r\nINFO:tensorflow:global step 1040: loss = 0.2107 (0.17 sec/step)\r\n2017-03-24 10:09:57.915250: W tensorflow/core/kernels/queue_base.cc:294] _0_magazines_train.tfrecord/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.DeadlineExceededError'>, Timed out waiting for notification\r\nINFO:tensorflow:global step 1040: validation loss = 0.3864, validation accuracy = 97.66%\r\nINFO:tensorflow:Finished training! Saving model to disk.\r\nTraceback (most recent call last):\r\n  File \"fine-tune.py\", line 215, in <module>\r\n    run()\r\n  File \"fine-tune.py\", line 211, in run\r\n    session_config=tf.ConfigProto(operation_timeout_in_ms=60000))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 752, in train\r\n    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 960, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 788, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 786, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 994, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1044, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1064, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification\r\n```\r\n\r\nSince I have a consistent repro I'll be willing to provide more information to help debug this issue. Right now I'm not exactly sure what I should provide to help understand the root cause.\r\n\r\nThis is the TensorBoard information from `/batch/fraction_of_320_full`:\r\n\r\n<img width=\"643\" alt=\"image\" src=\"https://cloud.githubusercontent.com/assets/1497445/24305854/aa7c4da8-107b-11e7-864d-dfe1296b9503.png\">\r\n", "comments": ["Oh, it seems I forgot to provide more information about my current TF build and hardware config.\r\n\r\nTF: built from commit [c7b80d51da4fb6d51ea54a0bdf2601afa379d60c](c7b80d51da4fb6d51ea54a0bdf2601afa379d60c) with CUDA support but it still repros if I use the normal pip version.\r\n\r\nGPU: \r\n```\r\nFri Mar 24 10:24:51 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN X (Pascal)    Off  | 0000:05:00.0      On |                  N/A |\r\n| 23%   32C    P8    10W / 250W |    173MiB / 12186MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1194    G   /usr/lib/xorg/Xorg                             170MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nCPU:\r\n```\r\n\u276f lscpu\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                16\r\nOn-line CPU(s) list:   0-15\r\nThread(s) per core:    2\r\nCore(s) per socket:    8\r\nSocket(s):             1\r\nNUMA node(s):          1\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 79\r\nModel name:            Intel(R) Core(TM) i7-6900K CPU @ 3.20GHz\r\nStepping:              1\r\nCPU MHz:               1200.000\r\nCPU max MHz:           4100.0000\r\nCPU min MHz:           1200.0000\r\nBogoMIPS:              6399.59\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              20480K\r\nNUMA node0 CPU(s):     0-15\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\r\n```", "And BTW, I've just removed the `tf.device('/device:CPU:0')` line and at least I'm now into 9K steps:\r\n\r\n```\r\n...\r\nINFO:tensorflow:global step 9070: validation loss = 0.3390, validation accuracy = 95.31%\r\nINFO:tensorflow:global step 9071: loss = 0.2249 (0.22 sec/step)\r\nINFO:tensorflow:global step 9072: loss = 0.2727 (0.23 sec/step)\r\nINFO:tensorflow:global step 9073: loss = 0.1870 (0.21 sec/step)\r\nINFO:tensorflow:global step 9074: loss = 0.3343 (0.20 sec/step)\r\nINFO:tensorflow:global step 9075: loss = 0.2105 (0.20 sec/step)\r\nINFO:tensorflow:global step 9076: loss = 0.1722 (0.19 sec/step)\r\n...\r\n```\r\n\r\nHere is the `/batch/fraction_of_320_full` information for this run:\r\n\r\n<img width=\"1481\" alt=\"image\" src=\"https://cloud.githubusercontent.com/assets/1497445/24307582/ff2d512a-1081-11e7-9c6a-2a5593505fef.png\">\r\n", "A few guesses/checks that might help:\r\n\r\n* Checking the list of devices matches what you'd expect. The snippet below will print all the devices seen by TF and their names.\r\n```python\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n```\r\n\r\n* Thread management. Your error message seems to be more related to joining threads/coordinator. Not sure why this only shows with CPU as your device, but perhaps the following might help. I  don't see the use of a coordinator in your snippet. At the very least, it would help with debugging issues arising from coordinator/session. Usually the following is done when using queue runners and parallel threads.\r\n\r\n```python\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n# . . .  start your training, etc . . . \r\n# When done training and/or exception is caught:\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n```\r\n\r\n[Further reading on coord/threads](https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0#.a64trech2)\r\n\r\nFrom the [tf.train.batch](https://www.tensorflow.org/api_docs/python/tf/train/batch) documentation: \r\n\r\n> A QueueRunner for the queue is added to the current Graph's QUEUE_RUNNER collection.\r\n\r\nAs far as I know, that means the user should call ```start_queue_runners``` for everything to *start* properly, and then request stop and join when done for everything to *end* properly.\r\n\r\nIf none of this was news to you, then don't mind me. Best of luck!", "Thanks @mckinziebrandon for your kind reply! \r\n\r\nI've checked my return output for `list_local_devices` and it indeed says `/cpu:0`. For the `coord.join()` part. I'm using `slim.learning.train` for my training. The implementation includes [sv.start_queue_runners(sess)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L738).", "@mrry, could you comment on this issue please.", "One possible issue is the `tf.summary.image()` calls in your `load_batch()` function. Since (if I understand the code in `slim.learning.train` correctly) the merged summaries will be collected from a separate thread, this will cause certain images to be dequeued from the input pipeline (in order to make summaries) but never enqueued to the batch queue for training. \r\n\r\nIs your input finite or infinite? If it's finite, maybe running CPU-only means that your program completes steps more slowly (relative to the GPU version, and to the periodic summary thread) so it runs out of input data after 1000 steps? (If that's true, I'd expect the same fate to befall the GPU version, but it would complete more steps before that happens.)", "Thanks @mrry. I think what I'm observing is more or less the same as #2788 and I can also resume the training process by first suspending and then bringing the process back to foreground. \r\n\r\nWhat I don't quite understand is, even if running CPU only means it takes more time for the CPU to prepare the input data - why would that make the process stuck in there without doing anything?\r\n\r\nPS: When you say \"finite or infinite\"? Are you asking whether I'm setting something like the maximum number of steps or the number of epochs? I haven't set these parameters and the training process should be infinite.", "More information that might be helpful. CPU cores have 100% load when it hangs:\r\n\r\n```\r\n  1  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   5  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   9  [                                              0.0%]   13 [                                              0.0%]\r\n  2  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   6  [||||||||||||||||||||||||||||||||||||||||||||100.0%]   10 [||                                            1.3%]   14 [                                              0.0%]\r\n  3  [|                                             0.7%]   7  [                                              0.0%]   11 [||||||||||||||||||||||||||||||||||||||||||||100.0%]   15 [||||||||||||||||||||||||||||||||||||||||||||100.0%]\r\n  4  [                                              0.0%]   8  [                                              0.0%]   12 [||||||||||||||||||||||||||||||||||||||||||||100.0%]   16 [||||||||||||||||||||||||||||||||||||||||||||100.0%]\r\n```", "@mrry is asking whether your queues are empty -- that would be finite.\r\n\r\nSo @derekhh did you say that you `kill -SUSP` then `kill -COND` and it got the process to continue?", "@drpngx\r\n\r\nThanks for your reply. Later I've found even running train_image_classifier.py from TF-Slim hangs so I feel it should be related to a particular hardware / system config issue.\r\n\r\nI've been trying multiple ways to figure out the root cause in the past few weeks. I believe this is an issue unrelated to TensorFlow but is kind of specific to my motherboard. I'm using a similar motherboard as mentioned in Issue #1947 (ASUS X99-E WS 3.1).\r\n\r\nLater, I've followed this web page: https://www.pugetsystems.com/labs/hpc/Install-Ubuntu-16-04-or-14-04-and-CUDA-8-and-7-5-for-NVIDIA-Pascal-GPU-825/ and installed Ubuntu 14.04 LTS with some additional steps of configurations I didn't do before:\r\n\r\n1. **Setting \"pcie_aspm=off\" in the boot options.** [This seems to be the fix.]\r\n2. ~~Installed NVIDIA 375.39 driver and CUDA 8.0 separately instead of using the driver bundled with CUDA 8.0.~~ [I don't feel this is the fix since CUDA 8.0 is also bundled with NVIDIA 375.39.]\r\n3. Fixed a symbolic link issue related to \"libEGL.so.1\" as described in this post.\r\n\r\nI'm no longer seeing the issue right now. I still can't tell which of the aforementioned configuration steps actually fixed the issue. I'll test it more and update this thread so this might help some other people using similar hardware configs.", "Thanks for the update! Good to know.", "I had the same issue with my PC which has an ASUS motherboard.\r\nI updated the BIOS to the latest and turned off \"turbo mode\" and the issue disappeared. \r\n\r\nI have a more verbose version posted under issue https://github.com/tflearn/tflearn/issues/241"]}, {"number": 8695, "title": "Fix two compiler warnings", "body": "warning C4099: 'tensorflow::port::`anonymous-namespace'::CPUIDInfo': type name first seen using 'class' now seen using 'struct'", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please (Flaky infra)", "The failing test is a known flaky test.\r\nIm going ahead with the merge."]}]