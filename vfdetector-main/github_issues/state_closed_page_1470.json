[{"number": 8845, "title": "Tensorboard not showing data on Windows", "body": "I have a simple example of a MNIST classifier that works well for the dataset. However, Tensorboard is unable to read the data from the saved logs. I have inspected the data using `tensorboard --inspect --logdir...` and the files seem to be in order.\r\n\r\nI have tested this and a much simpler model on a different machine, and got the same results. Tensorboard runs and the interface is accessible, but no data is shown (I tested with Google Chrome and Edge).\r\n\r\nThe code is as follows:\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\n\r\nwith tf.name_scope('input'):\r\n    x = tf.placeholder(tf.float32, shape=[None, 784])\r\n    y_ = tf.placeholder(tf.float32, shape=[None, 10])\r\n\r\ndef variable_summaries(var):\r\n  with tf.name_scope('summaries'):\r\n    mean = tf.reduce_mean(var)\r\n    tf.summary.scalar('mean', mean)\r\n    with tf.name_scope('stddev'):\r\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\r\n    tf.summary.scalar('stddev', stddev)\r\n    tf.summary.scalar('max', tf.reduce_max(var))\r\n    tf.summary.scalar('min', tf.reduce_min(var))\r\n    tf.summary.histogram('histogram', var)\r\n\r\ndef weight_variable(shape):\r\n  initial = tf.truncated_normal(shape, stddev=0.1, name='weights')\r\n  return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n  initial = tf.constant(0.1, shape=shape, name='bias')\r\n  return tf.Variable(initial)\r\n\r\ndef conv2d(x, W):\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name='conv')\r\n\r\ndef max_pool_2x2(x):\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool')\r\n\r\nwith tf.name_scope('conv1'):\r\n    W_conv1 = weight_variable([5, 5, 1, 32])\r\n    b_conv1 = bias_variable([32])\r\n    variable_summaries(W_conv1)\r\n    variable_summaries(b_conv1)\r\n\r\n    x_image = tf.reshape(x, [-1, 28, 28, 1], name='reshape')\r\n    tf.summary.image('image', x_image, 3)\r\n\r\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, name='relu')\r\n    h_pool1 = max_pool_2x2(h_conv1)\r\n    variable_summaries(h_conv1)\r\n    variable_summaries(h_pool1)\r\n\r\nwith tf.name_scope('conv2x'):\r\n    W_conv2x = weight_variable([5, 5, 32, 64])\r\n    b_conv2x = bias_variable([64])\r\n    variable_summaries(W_conv2x)\r\n    variable_summaries(b_conv2x)\r\n\r\n    h_conv2x = tf.nn.relu(conv2d(h_pool1, W_conv2x) + b_conv2x, name='relu')\r\n    h_pool2x = max_pool_2x2(h_conv2x)\r\n    variable_summaries(h_conv2x)\r\n    variable_summaries(h_pool2x)\r\n\r\nwith tf.name_scope('conv2y'):\r\n    W_conv2y = weight_variable([3, 3, 32, 64])\r\n    b_conv2y = bias_variable([64])\r\n    variable_summaries(W_conv2y)\r\n    variable_summaries(b_conv2y)\r\n\r\n    h_conv2y = tf.nn.relu(conv2d(h_pool1, W_conv2y) + b_conv2y, name='relu')\r\n    h_pool2y = max_pool_2x2(h_conv2x)\r\n    variable_summaries(h_conv2y)\r\n    variable_summaries(h_pool2y)\r\n\r\nwith tf.name_scope('concat'):\r\n    h_pool2 = tf.concat([h_pool2x, h_pool2y], 3)\r\n    variable_summaries(h_pool2)\r\n\r\nwith tf.name_scope('fc1'):\r\n    W_fc1 = weight_variable([7 * 7 * (64*2), 1024])\r\n    b_fc1 = bias_variable([1024])\r\n    variable_summaries(W_fc1)\r\n    variable_summaries(b_fc1)\r\n\r\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * (64*2)])\r\n    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='relu')\r\n    variable_summaries(h_pool2_flat)\r\n    variable_summaries(h_fc1)\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name='dropout')\r\nvariable_summaries(h_fc1_drop)\r\n\r\nwith tf.name_scope('fc2'):\r\n    W_fc2 = weight_variable([1024, 10])\r\n    b_fc2 = bias_variable([10])\r\n    variable_summaries(W_fc2)\r\n    variable_summaries(b_fc2)\r\n\r\n    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\r\n    variable_summaries(y_conv)\r\n\r\nlearning_rate = tf.placeholder(tf.float32, shape=[])\r\ntf.summary.scalar('learning_rate', learning_rate)\r\n\r\nwith tf.name_scope('loss'):\r\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\r\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\r\n    variable_summaries(cross_entropy)\r\n\r\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\r\ntf.summary.scalar('accuracy', accuracy)\r\n\r\nmerged = tf.summary.merge_all()\r\ntrain_writer = tf.summary.FileWriter('summaries/train', sess.graph)\r\ntest_writer = tf.summary.FileWriter('summaries/test', sess.graph)\r\n\r\ntf.global_variables_initializer().run()\r\n\r\nfor i in range(101):\r\n  batch = mnist.train.next_batch(100)\r\n  summary, _, acc = sess.run([merged, train_step, accuracy], feed_dict={x:batch[0], y_: batch[1], learning_rate:0.02, keep_prob: 0.5})\r\n  train_writer.add_summary(summary, i)\r\n  #train_writer.add_graph(sess.graph, i)\r\n  print(\"step %d, training accuracy %g\" % (i, acc))\r\n  if i % 100 == 0:\r\n    #This is just a test model, the accuracy is not important\r\n    batch = mnist.test.next_batch(100)\r\n    summary, acc = sess.run([merged, accuracy], feed_dict={x:batch[0], y_: batch[1], learning_rate:0, keep_prob: 1.0})\r\n    test_writer.add_summary(summary, i)\r\n    print(\"step %d, test accuracy %g\" % (i, acc))\r\n\r\ntrain_writer.close()\r\ntest_writer.close()\r\n```\r\n\r\nThe logs are provided below.\r\n[summaries.zip](https://github.com/tensorflow/tensorflow/files/883033/summaries.zip)\r\n\r\nSystem information:\r\nWindows 10\r\nCUDA 8.0, cuDNN 5.0\r\nTensorflow 1.0.1\r\nPython 3.5 64-bit\r\n\r\n", "comments": ["Please include all the information required by the issue template.", "@gunan: I updated the issue description.", "I've verified that the train summary loads in tensorboard on Mac OS X. I don't have a windows machine to test. @mrry can you let us know if Windows tensorboard is expected to work?", "@aselle @IgorX2 I have managed to run your script and appear to be getting full Tensorboard functionality in in Windows. Your code appears to be coded fine.\r\n\r\nCan I please ask you for the following to help troubleshoot this.\r\n1. What command did you run for Tensorboard.\r\n2. Where on your disk do your summaries go? Are they in a folder called summaries below where your code is being executed or are they in a folder called summaries at root?\r\n3. Do you get anything in the graphs section?\r\n4. Can you please try a different directory, perhaps at root (i.e. C;/summaries) and try to fetch the logs from there?\r\n\r\nThis sounds like it could be a permissions error as I am unable to reproduce your error but instead get your full Tensorboard results. If not then perhaps it is in an error within your installation? I am happy to help you troubleshoot this.", "@jubjamie Thanks for looking into this! Were you also running with version 1.0.1?\r\n\r\n@aselle @IgorX2 TensorBoard is expected to work on Windows, although there are some wrinkles around (e.g.) TensorBoard options that are colon-delimited and get confused by Windows paths. Seeing the full command, as @jubjamie suggests, would help to diagnose this.", "@mrry Yes 1.0.1, I have a feeling that this is either permissions related or, more likely, to do with the actual file locations vs. whats typed in i.e. summaries/train =/= /summaries/train\r\n\r\nI see little other reason for Tensorboard to not show the data apart from a corrupt installation. @IgorX2 May I suggest that you see if you can copy & paste a Tensorboard tutorial and see if that works and make sure that you are able to read files correctly.", "I managed to get it working, but I still believe there is a bug.\r\n\r\nHere is what I tried:\r\n\r\n- Place the files in the project directory\r\n- Place the files in the root of D:\r\n- Place the files in the root of D: and use `/` instead of `\\` in path\r\n- Place the files in the root of C: (this is the only one that worked)\r\n\r\nAlso, using `tensorboard --inspect ...` on all three locations found the files and listed their contents.", "Thanks for  confirming this. The underlying problem is the same as #7856, which arises because TensorBoard uses a `:` to separate the run name from the log directory in its command-line flags, which is fine on Linux but on Windows it clashes with the use of `:` in the drive name. I suggested a workaround on the thread for #7856, and I'd encourage you to comment there if you have further problems with this.\r\n\r\n/cc @dandelionmane FYI."]}, {"number": 8844, "title": "Fix doc for svd", "body": "Error where \"left\" and \"right\" were reversed", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8843, "title": "Executor failed to create kernel for FFT even though using GPU", "body": "### Environment info\r\nOperating System:\r\nLinux\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n-rw-r--r-- 1 root root   556000 jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 jan 27 00:51 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   415432 jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 jan 27 00:48 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       18 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 root root 84163560 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 mar 29 10:04 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\n```\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsess = tf.InteractiveSession()\r\nb = tf.convert_to_tensor([[1.], [1.], [1.]], dtype=tf.float32)\r\ninput = tf.complex(b, tf.zeros_like(b))\r\nx = tf.fft(input)\r\nx.eval()\r\n```\r\n\r\n\r\n### Logs or other output that would be helpful\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 750 Ti\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.202\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)\r\nE tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Not found: No registered 'FFT' OpKernel for CPU devices compatible with node FFT = FFT[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](Complex)\r\n        .  Registered:  device='GPU'\r\n\r\n         [[Node: FFT = FFT[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](Complex)]]\r\n\r\n```\r\n", "comments": ["This does seem strange I can't even get a simple invocation like this to work\r\n\r\n```python\r\nsess = tf.Session()\r\nwith tf.Session() as sess:\r\n  with tf.device(tf.DeviceSpec(device_type='GPU')):\r\n    input = tf.constant([[1.,2.,3.,4.]], dtype=tf.complex64)\r\n    x = tf.fft(input)\r\n    x.eval()\r\n```", "@rmlarsen, could you take  a quick look?", "I have the same problem with Tensorflow version 1.0.1 when I try to use FFT2D on my GPU. ", "Hmm why does the error message \r\n\r\n\"No registered 'FFT' OpKernel for CPU devices compatible with node\"\r\n\r\ntalk about not finding a registered kernel for CPU devices? It is true the we don't support FFT on CPU yet.", "friendly ping to all involved: is this still an issue with 1.2? If so, who is a good person to assign this to? ", "I have updated my Tensorflow version 1.0.1 to version 1.2 and I don't have the error message anymore. The issue seems to have been settled.   "]}, {"number": 8842, "title": "AttributeError: 'Tensor' object has no attribute 'get' at DynamicRnnEstimator.fit(...). The same code works well with LinearRegressor.", "body": "TensorFlow version: **1.1**\r\n\r\nThe code:\r\n\r\n```\r\nimport random\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.learn.python.learn.estimators import constants\r\nfrom tensorflow.contrib.learn.python.learn.estimators.dynamic_rnn_estimator import PredictionType\r\n\r\nxData = []\r\nyData = []\r\nfor _ in range(10000):\r\n    x = random.random()\r\n    xData.append(x)\r\n    y = 2 * x\r\n    yData.append(y)\r\n\r\n\r\nxc = tf.contrib.layers.real_valued_column(\"\")\r\nestimator = tf.contrib.learn.DynamicRnnEstimator(problem_type = constants.ProblemType.LINEAR_REGRESSION,\r\n                                                 prediction_type = PredictionType.SINGLE_VALUE,\r\n                                                 sequence_feature_columns = [xc],\r\n                                                 context_feature_columns = None,\r\n                                                 num_units = 5,\r\n                                                 cell_type = 'lstm', \r\n                                                 optimizer = 'SGD',\r\n                                                 learning_rate = '0.1')\r\n\r\ndef get_train_inputs():\r\n  x = tf.constant(xData)\r\n  y = tf.constant(yData)\r\n\r\n  return x, y\r\n\r\nestimator.fit(input_fn=get_train_inputs, steps=1000) \r\n```\r\n\r\n> Got:  AttributeError: 'Tensor' object has no attribute 'get' here\r\n\r\n\r\nThe same code works for **LinearRegressor** instead of **DynamicRnnEstimator**.\r\n\r\n> \r\n> WARNING:tensorflow:From E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dynamic_rnn_estimator.py:724: regression_target (from tensorflow.contrib.layers.python.layers.target_column) is deprecated and will be removed after 2016-11-12.\r\n> Instructions for updating:\r\n> This file will be removed after the deprecation date.Please switch to third_party/tensorflow/contrib/learn/python/learn/estimators/head.py\r\n> WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\pavel\\AppData\\Local\\Temp\\tmpzy68t_iw\r\n> Traceback (most recent call last):\r\n>   File \"C:/Users/pavel/PycharmProjects/rnnEstimator/main.py\", line 31, in <module>\r\n>     estimator.fit(input_fn=get_train_inputs, steps=1000)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 281, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 430, in fit\r\n>     loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 927, in _train_model\r\n>     model_fn_ops = self._get_train_ops(features, labels)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1132, in _get_train_ops\r\n>     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1103, in _call_model_fn\r\n>     model_fn_results = self._model_fn(features, labels, **kwargs)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dynamic_rnn_estimator.py\", line 516, in _dynamic_rnn_model_fn\r\n>     sequence_length = features.get(sequence_length_key)\r\n> AttributeError: 'Tensor' object has no attribute 'get'", "comments": ["@alextp, could you take a quick look.", "Can I get a full stack trace?", "Try changing the input function to \r\n\r\n```\r\ndef get_train_inputs():\r\n  x = tf.constant(xData)\r\n  y = tf.constant(yData)\r\n\r\n  return {'': x} , y\r\n```", "@alextp Added original traceback.\r\n\r\n@jamieas  After I have added  _return {'': x} , y_ i got error: _NotImplementedError: No deep embedding lookup arguments for column _RealValuedColumn(column_name='', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)._\r\n\r\nTraceback:\r\n\r\n> WARNING:tensorflow:From E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dynamic_rnn_estimator.py:724: regression_target (from tensorflow.contrib.layers.python.layers.target_column) is deprecated and will be removed after 2016-11-12.\r\n> Instructions for updating:\r\n> This file will be removed after the deprecation date.Please switch to third_party/tensorflow/contrib/learn/python/learn/estimators/head.py\r\n> WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\pavel\\AppData\\Local\\Temp\\tmpndoa1_it\r\n> WARNING:tensorflow:sequence_input_from_feature_columns (from tensorflow.contrib.layers.python.layers.feature_column_ops) is experimental and may change or be removed at any time, and without warning.\r\n> **Traceback** (most recent call last):\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 165, in _input_from_feature_columns\r\n>     transformed_tensor)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py\", line 239, in _deep_embedding_lookup_arguments\r\n>     \"No deep embedding lookup arguments for column {}.\".format(self))\r\n> NotImplementedError: No deep embedding lookup arguments for column _RealValuedColumn(column_name='', dimension=1, default_value=None, dtype=tf.float32, normalizer=None).\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 180, in _input_from_feature_columns\r\n>     output_rank=output_rank))\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py\", line 1420, in _to_dnn_input_layer\r\n>     return _reshape_real_valued_tensor(input_tensor, output_rank, self.name)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column.py\", line 1333, in _reshape_real_valued_tensor\r\n>     raise ValueError(error_string)\r\n> ValueError: Error while processing column .Rank of input Tensor (1) should be the same as output_rank (3). For example, sequence data should typically be 3 dimensional (rank 3) while non-sequence data is typically 2 dimensional (rank 2).\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:/Users/pavel/PycharmProjects/rnnEstimator/main.py\", line 31, in <module>\r\n>     estimator.fit(input_fn=get_train_inputs, steps=1000)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 281, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 430, in fit\r\n>     loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 927, in _train_model\r\n>     model_fn_ops = self._get_train_ops(features, labels)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1132, in _get_train_ops\r\n>     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1103, in _call_model_fn\r\n>     model_fn_results = self._model_fn(features, labels, **kwargs)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dynamic_rnn_estimator.py\", line 519, in _dynamic_rnn_model_fn\r\n>     context_feature_columns)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dynamic_rnn_estimator.py\", line 194, in build_sequence_input\r\n>     scope=scope)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\framework\\experimental.py\", line 64, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 295, in sequence_input_from_feature_columns\r\n>     default_name='sequence_input_from_feature_columns')\r\n>   File \"E:\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\feature_column_ops.py\", line 183, in _input_from_feature_columns\r\n>     '{}, {}'.format(column.name, e, ee))\r\n> ValueError: Error creating input layer for column: .\r\n> Error while processing column .Rank of input Tensor (1) should be the same as output_rank (3). For example, sequence data should typically be 3 dimensional (rank 3) while non-sequence data is typically 2 dimensional (rank 2)., No deep embedding lookup arguments for column _RealValuedColumn(column_name='', dimension=1, default_value=None, dtype=tf.float32, normalizer=None).\r\n> ", "The relevant part of that trace is\r\n\r\n> Error while processing column .Rank of input Tensor (1) should be the same as output_rank (3). For example, sequence data should typically be 3 dimensional (rank 3) while non-sequence data is typically 2 dimensional (rank 2).\r\n\r\nSequence input should generally have rank 3, where the first two dimensions are batch size and sequence length. In the following example, the (not very useful) Estimator learns to predict the mean of a sequence:\r\n\r\n```python\r\nBATCH_SIZE = 32\r\nSEQUENCE_LENGTH = 16\r\n    \r\n\r\nxc = tf.contrib.layers.real_valued_column(\"\")\r\nestimator = tf.contrib.learn.DynamicRnnEstimator(problem_type = constants.ProblemType.LINEAR_REGRESSION,\r\n                                                 prediction_type = PredictionType.SINGLE_VALUE,\r\n                                                 sequence_feature_columns = [xc],\r\n                                                 context_feature_columns = None,\r\n                                                 num_units = 5,\r\n                                                 cell_type = 'lstm', \r\n                                                 optimizer = 'SGD',\r\n                                                 learning_rate = 0.1)\r\n\r\ndef get_train_inputs():\r\n  x = tf.random_uniform([BATCH_SIZE, SEQUENCE_LENGTH])\r\n  y = tf.reduce_mean(x, axis=1)\r\n  x = tf.expand_dims(x, axis=2)\r\n  return {\"\": x}, y\r\n\r\nestimator.fit(input_fn=get_train_inputs, steps=1000)\r\n```", "Thank you @jamieas it works! Now it is clear. \r\n\r\nMy personal opinion that It would be great if contrib.learn(as a high-level wrapper) will be a little more user-friendly with ranks and dimensions. Maybe some entity like DataAdapter that will take some set of common params as inputs like BATCH_SIZE, SEQUENCE_LENGTH, raw data input (not tensors) and will form appropriate output for  the estimator. IMHO. Thank you for your help!"]}, {"number": 8841, "title": "Gradient of reduce_prod not available on GPU", "body": "The following example fails to colocate the values:\r\n```python\r\nwith tf.device('/gpu:2'):\r\n    x = tf.placeholder(tf.float32, shape=[None, 100])\r\n    weight_dense_1 = tf.Variable(tf.zeros([100, 10]))\r\n    dense_1_out = tf.matmul(x, weight_dense_1)\r\n    y = tf.reduce_prod(tf.cast(tf.shape(dense_1_out), tf.float32))\r\n    grad = tf.gradients(y, [weight_dense_1], colocate_gradients_with_ops=True)\r\n```\r\nA bunch of warnings like this is displayed:\r\n```\r\nWARNING:tensorflow:Tried to colocate gradients_1/Prod_1_grad/Rank with an op Prod_1 that had a different device: /device:CPU:0 vs /device:GPU:2. Ignoring colocation property.\r\n```\r\nThe symptom is similar to #3397. Using CPU or specifying all input dimensions solves the problem. But the cause seems different.\r\n\r\nGradient of `Prod` operation is defined in [python/ops/math_grad.py](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/math_grad.py#L102-L143). There, the operation is forced to run on CPU (see 182fef1b55640906637e4bf0d205e508c24549e7), mentioning the `listdiff()` operation is CPU-only.\r\nI tried remove the forcing line and run this. It yields a kind explanation:\r\n```\r\nInvalidArgumentError: Cannot assign a device to node 'gradients/Prod_grad/range_1': Could not satisfy explicit device specification '/device:GPU:2' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nInvertPermutation: GPU CPU \r\nTranspose: GPU CPU \r\nConcatV2: GPU CPU \r\nPack: GPU CPU \r\nCumprod: GPU CPU \r\nListDiff: CPU \r\nShape: GPU CPU \r\n    ...(many GPU CPU ops)\r\nReshape: GPU CPU \r\nGather: CPU \r\n```\r\n\r\nTwo operations used here, namely [Gather](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/gather_op.cc) and [ListDiff](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/core/kernels/listdiff_op.cc), are defined on CPU-only. As some of the operations needed in calculating the gradient are CPU-only, by the colocation rule, they get grouped into CPU-only.\r\n\r\nThis also occurs when using `moments()` or [`sufficient_statistics()`](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/python/ops/nn_impl.py#L495-L541) (the former calls the latter). There, when some of the axes of the tensor are unknown (like batch size), the total number of values (which is needed for the mean and variance) is calculated by `reduce_prod()` on `shape()`.\r\nWhen the value of mean or variance is differentiated in some way (which is the case in batch normalization), a colocation between tensors named like `gradients/moments/sufficient_statistics/count_grad/Rank` and `moments/sufficient_statistics/count` fails.\r\n\r\nThough `listdiff()` is renamed later on Python interface to `setdiff1d()`, it's still named `ListDiff` internally.\r\n`gather()` operation is defined on GPU too, but only on float types.\r\n\r\nIt seems there hasn't been any issue on this. Would it mean that `Prod()` op is not differentiated in most of the cases?\r\nHow this can be solved? I'm not sure if the `setdiff1d()` operation is needed.\r\nFor me, this occured when using `moments()`, where the reciprocal of number of values is multiplicated to the sum of values. I think this is unnecessary, as it can be done with `reduce_mean()`. Is it right?\r\n\r\n### Environment info\r\nOperating System: **Ubuntu 16.04**.\r\nInstalled version of CUDA and cuDNN: **CUDA 8.0.61** / **cuDNN 5.1.10**.\r\npip3-installed `tensorflow-gpu==1.0.1`; all links here pointed to `r1.0`, but the problematic parts are the same as `master`.\r\n`python -c \"import tensorflow; print(tensorflow.__version__)\"` yields: `1.0.1`.", "comments": ["I suspect you are right, the reduce_prod is not used very often and certainly not often on the gpu. To successfully solve your problem, you'd need to make sure all ops used to implement reduce_prod had gpu implementations. We are likely not going to do this soon (@zheng-xq  might know more). Eventually, things like XLA should help allow more complete gpu implementation. Good luck!", "I have a need of locating this op on gpu as well", "I'm experimenting with the [Differentiable Neural Computer (DNC)](https://github.com/deepmind/dnc). And I found out that the model uses reduce_prod heavily in the RNN while loop, which causes very poor performance when training the model. Desperately hope that this feature could be heeded.", "Closing as this is resolved"]}, {"number": 8840, "title": "BasicDecoder error", "body": "Hi,\r\nI am trying to use BasicDecoder for a sequence-to-sequence translation model and I get error:\r\n`\r\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [1] [ but saw shape: ] [64] [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All/_2029, rnn/Assert/Assert/data_0, rnn/stack/_2031, rnn/Assert/Assert/data_2, rnn/Shape_1/_2033)]] [[Node: rnn/while/Identity_12/_2727 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:5\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4293_rnn/while/Identity_12\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:5\"](^_clooprnn/while/multi_rnn_cell/cell_5/lstm_cell/zeros/_208)]]`\r\nI can't figure it out, sequence_length tensor should be batch-sized and it is and error messege suggests that it should be [1]. This would be a case when a list of Tensors would be used instead of one fat tensor [time, batch_size, 1]\r\n\r\nMy target_vocab_size is 500K\r\nsize of RNN = 64, for testing\r\ndec_inp : <tf.Tensor 'embedded_inputs_1:0' shape=(100, ?, 64) dtype=float32>\r\ndecoder seq len is batch-sized (64) <tf.Tensor 'decoder_seq_len:0' shape=(?,) dtype=int32>\r\n\r\n>>> tf.__version__\r\n'1.1.0-rc0'\r\n>>>\r\n\r\n\r\n```\r\n\r\nW_target_emb = tf.Variable(tf.random_uniform([target_vocab_size, size], -1.0, 1.0), name=\"W_target_emb\")\r\n\r\nhalf = tf.constant(0.5)\r\ndec_inp = tf.cast(tf.stack(self.decoder_inputs), tf.float32)\r\ndec_inp = tf.reshape(dec_inp,[encoder_max_size, -1, size], name = \"embedded_inputs\")\r\nif not forward_only:\r\n\t#helper = seq2seq.TrainingHelper(inputs = target_embedded_chars, sequence_length = self.decoder_seq_len, time_major=True)\r\n\thelper = seq2seq.ScheduledEmbeddingTrainingHelper(inputs = dec_inp,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t  sequence_length = self.decoder_seq_len,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t embedding = W_target_emb,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t sampling_probability = half,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t time_major=True)\r\n\t\r\nelse:\r\n\thelper = seq2seq.GreedyEmbeddingHelper(dec_inp, \r\n\t\t\t\t\t\t\t\t\t\t   start_tokens=self.decoder_inputs[0],\r\n\t\t\t\t\t\t\t\t\t\t   end_token=data_utils.EOS_ID)\r\n\t\r\ndecoder_cell = LSTMBlockCell(num_units=size)\r\ndecoder_cell = MultiRNNCell([DeviceWrapper(ResidualWrapper(decoder_cell),device=\"/gpu:%d\" % i) for i in range(8) ])\r\n\r\nmy_decoder = seq2seq.BasicDecoder(\r\n\t\tcell=decoder_cell,\r\n\t\thelper=helper,\r\n\t\tinitial_state=encoder_final_state)\r\n\r\n\r\ndecoder_outputs, decoder_state = seq2seq.dynamic_decode(my_decoder, output_time_major=False, parallel_iterations=32,\r\n\t\t\t   swap_memory = True)\r\n```", "comments": ["This looks like your own custom code? If so, this question is better asked on StackOverflow. If you suspect a bug, please verify that any examples we provide don't work as well. ", "You are right, closing", "Did you finally get the decoder to work ?  I'm trying  to implement something similar with 1.1 but the helpers are a bit of a mystery to me right now , specifically where exactly do we use the training and output helpers. If you could share a sample of working code, it would be great !"]}, {"number": 8839, "title": "Fix resources section link", "body": "Updated the resources section link in the README file to point to the correct page on tensorflow.org", "comments": ["Can one of the admins verify this patch?", "PR merged. Thanks, @bhaprayan !", "Sure! @caisq "]}, {"number": 8838, "title": "Bazel test failure in TF  (Linking error ) on RHEL 7.3", "body": "### Environment info\r\nOperating System: Rhel:7.3 (ppc64le)\r\n\r\n####################################################\r\nI could able to build the TF (version 1.0.1) successfully  , however I am getting following error for the command ,` bazel test  -c opt //tensorflow/...`\r\n\r\n```\r\nERROR: /root/Sandip/Bazel/new/with_patches/tensorflow/tensorflow/compiler/xla/tests/BUILD:953:1: Linking of rule '//tensorflow/compiler/xla/tests:broadcast_simple_test_cpu_parallel' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/broadcast_simple_test_cpu_parallel '-Wl,-rpath,$ORIGIN/../../../../_solib_ppc/' -Lbazel-out/local-opt/bin/_solib_ppc -pthread ... (remaining 9 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n/usr/bin/ld: bazel-out/local-opt/bin/tensorflow/compiler/xla/tests/broadcast_simple_test_cpu_parallel: hidden symbol `pthread_atfork' in /usr/lib64/libpthread_nonshared.a(pthread_atfork.oS) is referenced by DSO\r\n/usr/bin/ld: final link failed: Bad value\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/compiler/xla/tests:broadcast_simple_test_cpu_parallel failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n____Elapsed time: 4.953s, Critical Path: 4.18s\r\n\r\n```\r\n\r\nAny solution  ?\r\n", "comments": ["We do not have official support on RHEL.\r\nYou might have quicker response using Stackoverflow."]}, {"number": 8837, "title": "Windows: Fix cuda_configure.bzl for Bazel build", "body": "1. Remove corresponding action_env before writing it\r\n2. Add quote to environment variables\r\n3. Return a dummpy value for gcc detection on Windows\r\n\r\n@gunan @damienmg ", "comments": ["This will help solve https://github.com/bazelbuild/bazel/issues/2753", "Since you have the sed_hyphen_i lines lower I think the assumption is that\nit is not. We strip down the --action_env flag from the file instead of\ndeleting the file.\n\nThe user might do custom change to it.\n\nOn Thu, Mar 30, 2017 at 1:52 PM Yun Peng <notifications@github.com> wrote:\n\n> *@meteorcloudy* commented on this pull request.\n> ------------------------------\n>\n> In configure\n> <https://github.com/tensorflow/tensorflow/pull/8837#discussion_r108905790>\n> :\n>\n> > @@ -9,6 +9,7 @@ SOURCE_BASE_DIR=`pwd -P`\n>  popd > /dev/null\n>\n>  # This file contains customized config settings.\n> +rm -f .bazelrc\n>\n> I am assuming ./configure is the only one who touchs .bazelrc, is that ok?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8837#discussion_r108905790>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf_MoWrmYY0PzKYfPr7kV4Eq2Nq56ks5rq5eDgaJpZM4MuPNi>\n> .\n>\n", "@damienmg I see, but we didn't actually use `sed_hyphen_i` on `action_env`, now I added the code to strip the flags. Please take a look again.", "https://github.com/tensorflow/tensorflow/pull/8637/files#diff-e2d5a00791bce9a01f99bc6fd613a39dR282\r\n\r\nApparently I missed that part when I rolled the change forward. You actually want to remove the action_env that you might set anywhere in that script unconditionally because if the user reconfigure he doesn't want to have stale entry in that file.\r\n\r\nMaybe we should just do an \"%import .action_env.bazelrc\" and wipe that file out and put all the action env in it. That would simplify the sed code.", "Sounds good, where should I add the `%import .action_env.bazelrc` ?", "top of the .bazelrc?\n\nOn Thu, Mar 30, 2017 at 3:34 PM Yun Peng <notifications@github.com> wrote:\n\n> Sounds good, where should I add the %import .action_env.bazelrc ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8837#issuecomment-290411561>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHfzVndfighoFylkO3RzTiroBJZ6juks5rq69VgaJpZM4MuPNi>\n> .\n>\n", "@damienmg Thanks for the hint, I pushed another change for this.", "Jenkins, test this please.", "Change LGTM, Damien, any remaining comments?", "I just added back bazel clean to configure script, so testing one more time to make sure.\r\nJenkins, test this please."]}, {"number": 8836, "title": "AttributeError: 'module' object has no attribute 'rnn'", "body": "I had a version of ptb_word_lm.py which worked in 0.11. Now I am trying to use the same code in version 1.0.1. I am facing the following error:\r\n\r\noutputs, state = tf.nn.rnn(cell, inputs,initial_state=self._initial_state)\r\nAttributeError: 'module' object has no attribute 'rnn'\r\n\r\nPlease help soon !!", "comments": ["That's because tf.nn.rnn doesn't exist anymore! Perhaps you are looking for [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) or the more experimental [tf.nn.raw_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn)?\r\n\r\nAlternativley there is the contrib repo of RNN function in [tf.contrib.rnn](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)", "So where is the original rnn function residing presently now?\r\n\r\nDoing this gave the following error:\r\n\r\noutputs, state = tf.contrib.rnn(cell, inputs,initial_state=self._initial_state)\r\nTypeError: 'module' object is not callable", "You are getting that error because tf.contrib.nn is not a function but a package of modules. Please make sure you _read the documentation_ before trying things in your code. I linked the documentation for you in my first comment for your convenience.\r\n\r\nRegarding the original rnn, i'm not sure as I never used it. Perhaps you need dynamic_rnn? I would read the documentation to find which functions satisfy your needs. You may need to alter your function arguments accordingly.\r\n\r\nAs this isn't a Tensorflow bug but rather a usage problem may I suggest that you open a Stack Overflow question to learn how to use RNN in Tensorflow as this issue may get closed.", "Did you solve it? I met the problem with you\r\n\r\nlstm_cell = tf.contrib.rnn.BasicLSTMCell(dimhidden, forget_bias=1.0)\r\n_LSTM_O, _LSTM_S = tf.contrib.rnn(lstm_cell, _Hsplit,dtype=tf.float32)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-54-a2b2c95b3fbd> in <module>()\r\n      2 x      = tf.placeholder(\"float\", [None, nsteps, diminput])\r\n      3 y      = tf.placeholder(\"float\", [None, dimoutput])\r\n----> 4 myrnn  = _RNN(x, weights, biases, nsteps, 'basic')\r\n      5 pred   = myrnn['O']\r\n      6 cost   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\r\n\r\n<ipython-input-53-6aff4bf02bd1> in _RNN(_X, _W, _b, _nsteps, _name)\r\n     16         scope.reuse_variables()\r\n     17         lstm_cell = tf.contrib.rnn.BasicLSTMCell(dimhidden, forget_bias=1.0)\r\n---> 18         _LSTM_O, _LSTM_S = tf.contrib.rnn(lstm_cell, _Hsplit,dtype=tf.float32)\r\n     19     # 6. Output\r\n     20     _O = tf.matmul(_LSTM_O[-1], _W['out']) + _b['out']\r\n\r\nTypeError: 'module' object is not callable\r\n", "@Ivan-Zhao tf.contrib.rnn is **NOT** a function. It is a collection of functions. This is why you are receiving that error. Please see it's [documentation here](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn).", "I was looking for the same thing, rnn.rnn -- a function. This thread helped me find rnn.dynamic_rnn which resembles the signature of the function I needed."]}, {"number": 8835, "title": "[windows 7] Not using CUDA. No errors or messages indicating dlls have been loaded in log. ", "body": "Issue: after upgrading from 0.12 to 1.0.1, I lost GPU support. On 0.12, there were log entries indicating successful loading of cuda/cudnn dlls and log entries if dlls weren't found. On 1.0.1, I don't see any log entries when importing tensorflow, but GPU is no longer listed amongst available devices. \r\n\r\nCan't provide much useful information without anything in the logs. Has logging been disabled, or is it getting written to a file now? How can I diagnose this?\r\n\r\n---\r\n\r\nOperating System: windows 7\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0 cuDNN 6.0\r\n\r\nOutput from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.1\r\n\r\nWhat other attempted solutions have you tried?\r\nReinstalled tf, reinstalled cudnn\r\n\r\n", "comments": ["Firstly, I noticed that you are using the brand new cuDNN 6.0. Tensorflow is built to use cuDNN 5.1 so this might be your problem. Could @gunan perhaps comment on whether 6.0 breaks Tensorflow/there is planned support for 6.0?\r\nIn the meantime, let's assume that it is **not** the issue and i'll give you my answer for users of cuDNN 5.1:\r\n \r\nWhen you run your session.run() call you should see that Tensorflow outputs device information.\r\nFor example, I see the following:\r\n```\r\n2017-03-30 14:06:33.577701: I c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:887] Found device 0 with properties:\r\nname: Tesla K40c\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.18GiB\r\nFree memory: 11.11GiB\r\n```\r\nThis appears in my console logs. Can you try calling a session.run(), perhaps your variable initialiser and see if you are able to find those logs. You might have to look between any _compilation instruction warnings_.\r\n\r\nYou could also try increasing the verbosity if you previously tweaked it.\r\nIf there's definitely nothing, ensure that it is being picked up by nvidia-smi\r\n", "Looks like I accidentally installed cudnn 6.0 side by side with 5.1 while attempting to fix the issue. This is not the cause of the problem though, as I was on a working setup with just cudnn 5.1 prior to upgrading to 1.0.1\r\n\r\nInvoking session.run() produces the following log output: https://gist.github.com/bicubic/c3edcc0b311437dd6888edc4bac30315\r\n\r\n`Device mapping: no known devices.` sounds ominous. \r\n\r\nThis is based on the gpu example which explicitly assigns the job to gpu0 which was working recently on 0.12. The example still seems to run fine, just not being executed on the gpu. ", "Strange. Is everything ok with nvidia-smi?", "nvidia-smi shows gpu is healthy and utilized, but not seeing any python/tf related processes in the utilization list.", "We have not verified with cudnn 6 yet. It may work, or it may not work, no guarantees.\r\n\r\n@bicubic I think somehow GPU device is invisible to your TF installation, but just to make sure could you try:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n```", "@gunan howdy. Yep, that's returning `False` so it looks like my GPU is somehow invisible after upgrading. Any suggestions how to proceed?", "@bicubic : How did you install TensorFlow? Is it possible that you used `pip3 install tensorflow` instead of `pip3 install tensorflow-gpu`?", "@asimshankar I won't have bandwidth to troubleshoot this issue further in the near future. Are you happy to close this ticket and I'll raise a new one and reference this one when I have more information?"]}, {"number": 8834, "title": "java tensorflow api :  Malformed TF_STRING tensor; too short to hold number of elements ", "body": "First, when i tried LabelImage [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java) with inception model ver 5, everything was good.\r\n\r\nThen i tried with an older inception model ( ver3 ) and i saw that both model have different input and output.\r\n\r\nIn ver5, our input tensor name is : \"Input\" , with dtype = FLOAT\r\nIn ver 3, our input tensor name is \"DecodeJpeg/contents\" , with dtype = STRING.\r\n\r\nSo i change LabelExample example with new name for both input and output : `Tensor result = s.runner().feed(\"input\", string_tensor_image).fetch(\"output\")` >> `s.runner().feed(\"DecodeJpeg/contents\", image).fetch(\"softmax\")` .\r\n\r\nAlso, i changed Image tensor to STRING type : \r\n\r\n    Tensor float_tensor_image = s.runner().fetch(output.op().name()).run().get(0);\r\n    byte[] bytes = new byte[res.numBytes()];\r\n    ByteBuffer buffer = ByteBuffer.wrap(bytes);\r\n    res.writeTo(buffer);\r\n    Tensor string_tensor_image = Tensor.create(DataType.STRING,res.shape(),buffer);\r\n\r\nIt looked good when i printed both tensor :\r\n \r\n    FLOAT tensor with shape [1, 224, 224, 3]\r\n    STRING tensor with shape [1, 224, 224, 3]\r\n\r\nBut after feeding to the graph, i get this error : \r\n`Exception in thread \"main\" java.lang.IllegalArgumentException: Malformed TF_STRING tensor; too short to hold number of elements.`\r\n\r\nI have tried everything i can, but no results. How can i fix it ?", "comments": ["well, pls help", "This question is probably better posed on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as we try to keep the github issues focused on bugs and feature requests.\r\n\r\nIf you do post there, probably also makes sense to provide more information (for example, a link to the exact model you're using if possible)\r\n", "@asimshankar this is model ver 5 : https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip .\r\n\r\nAnd this is model ver 3 : http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\n\r\nI download both from google. Also, i think it may be a bugs ", "By increasing number of bytes in array, it is solved, but then i get new error :\r\n \r\n    Tensor float_tensor = s.runner().fetch(output.op().name()).run().get(0);\r\n    byte[] bytes = new byte[float_tensor.numBytes()*64];\r\n    ByteBuffer buffer = ByteBuffer.wrap(bytes);\r\n    res.writeTo(buffer);\r\n    long[] shape = {};\r\n    Tensor string_tensor = Tensor.create(DataType.STRING, shape, buffer);\r\n    return string_tensor;\r\n\r\nthis is new string tensor : `STRING tensor with shape []`\r\n\r\nBut i get new error : \r\n\r\n    java.lang.IllegalArgumentException: Invalid JPEG data, size 0\r\n     [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\",                               fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]\r\n\r\nwell, i have no idea about how to fix it ? can someone suggest me ?", "As mentioned earlier, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\nHaving more complete code will help there - for example, it isn't clear what `res` and `float_tensor` are in your code and what you're feeding in. The `DecodeJpeg` operation consumes the file contents of a JPEG file as input, is that what you're feeding in? While in the other model, the `input` node consumes the normalized pixel values as floats. \r\n\r\nI strongly suggest you pursue this on stackoverflow as it most likely is an issue with your use of the new model and not a bug in TensorFlow"]}, {"number": 8833, "title": "DynamicAttentionWrapper expects own state on the 0-th step", "body": "Using Tensorflow 1.1rc0\r\nAFAIK the [DynamicAttentionWrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py) wraps an attention mechanism around the RNNCell. To do so, it passes around its own DynamicAttentionWrapperState in the __call__()\r\n\r\nHowever, I think that on the 0-th step, the network cannot pass such state as an argument, because the function gets called by a general decoder. (which doesnt know what cell it will encounter)\r\n\r\nI got the following error\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/trainer_class.py\", line 54, in __init__\r\n    self.model = Model(self.tok_chr.dim)\r\n  File \"/home/rob/Documents/frosha/analysis-service/src/analysis_parser/analysis_parser/models/rnn_seq2seq_tf.py\", line 164, in __init__\r\n    outputs, _ = dynamic_decode(decoder, impute_finished = True, maximum_iterations = max_sl)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\r\n    decoder_finished) = decoder.step(time, inputs, state)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\r\n    cell_outputs, cell_state = self._cell(inputs, state)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py\", line 530, in __call__\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\nAttributeError: 'tuple' object has no attribute 'attention'\r\n```\r\nThe error indicates that the current state has no attribute attention. But that's the final state of the encoder, which doesn't use this syntax\r\n\r\nThe following snippet shows the use of the wrapper. This was coded after [this](https://www.tensorflow.org/versions/r1.1/api_guides/python/contrib.seq2seq#Dynamic_Decoding) explanation\r\n\r\n```python\r\nencoder_outputs, encoder_state = core_rnn.static_rnn(\r\n                encoder_cell, encoder_inputs, dtype=dtype, sequence_length=self.SL_enc)\r\n\r\n#Some other code\r\n\r\nattention_size = 10\r\nattn_obj = BahdanauAttention(num_units=attention_size,\r\n                                             memory=attention_states,\r\n                                             memory_sequence_length=self.SL_enc,\r\n                                             normalize=True,\r\n                                             name='BahdanauAttentionObject')\r\n\r\nwrapped_cell = DynamicAttentionWrapper(cell_dec_fw,\r\n                                                       attn_obj,\r\n                                                       D,\r\n                                                       output_attention=False,\r\n                                                       name='DynAttnWrap')\r\n\r\nsampler = ScheduledEmbeddingTrainingHelper(decoder_inputs,\r\n                                                        sequence_length=self.SL_dec,\r\n                                                        embedding=embedding_in,\r\n                                                        sampling_probability=self.samp_prob)\r\ndecoder = BasicDecoder(wrapped_cell,\r\n                                       sampler,\r\n                                       encoder_state)\r\noutputs, _ = dynamic_decode(decoder)\r\n```", "comments": ["Where is DynamicAttentionWrapper? It does not seem to be in tf.contrib.seq2seq. Is AttentionWrapper now dynamic by default?", "The dynamic attention wrapper is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py)", "Ok I see the confusion. According to [this](https://github.com/tensorflow/tensorflow/commits/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py) it was renamed a few days ago from DynamicAttentionWrapper to AttentionWrapper. Many edits since, try to repull and test. I'll test and let you know.", "I apologize, but we typically cannot support the contrib directories of tensorflow unless they are underlying tensorflow bugs.\r\n\r\n@ebrevdo.\r\n", "That's right, the decoder needs a new, separate, type of state than what\r\nthe encoder emitted.\r\n\r\nAs of today I pushed a new method to AttentionWrapperState object called\r\n.clone().  You would use it thus:\r\n\r\n`initial_state = wrapper.zero_state(...).clone(cell_state=encoder_state)`\r\n\r\nand pass that initial_state to the decoder.", "(i may consider adding an initial_cell_state= argument when constructing the wrapper; but that approach mixes concerns.  we'll see)", "There's something I'm not getting, and it may be more theoretical. If the decoder's cell is an AttentionWrapper, which in turn attends to the output of the encoder RNN, why must the decoder's initial_state be an AttentionWrapperState?\r\n\r\nAlso I checked for the `.clone(...)` in `attention_wrapper.py` on master but couldn't find it. I'm probably blind though.  ", "Oh my bad, it's just a \"double\" initialization, initilization for the hidden states and for the attention. ", "Ok, nope. I tried to set the initial cell state and attention, based on the other attention's zero_state method, but no go.\r\n\r\nHere's what I tried\r\n```python    \r\n    # Attention Mechanisms. Bahdanau is additive style attention\r\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\r\n        num_units = mem_units, # depth of query mechanism\r\n        memory = attention_states, # hidden states to attend (output of RNN)\r\n        memory_sequence_length=seq_len_enc, # masks false memories\r\n        normalize=False, # normalize energy term\r\n        name='BahdanauAttention')\r\n\r\n    # Attention Wrapper: adds the attention mechanism to the cell\r\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n        cell = cell,# Instance of RNNCell\r\n        attention_mechanism = attn_mech, # Instance of AttentionMechanism\r\n        attention_size = attn_units, # Int, depth of attention (output) tensor\r\n        attention_history=False, # whether to store history in final output\r\n        name=\"attention_wrapper\")\r\n\r\n    # TrainingHelper does no sampling, only uses inputs\r\n    helper = tf.contrib.seq2seq.TrainingHelper(\r\n        inputs = x, # decoder inputs\r\n        sequence_length = seq_len_dec, # decoder input length\r\n        name = \"decoder_training_helper\")\r\n\r\n    # Decoder setup\r\n    batch_size = tf.shape(x)[0]\r\n    attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\r\n    init_state = tf.contrib.seq2seq.AttentionWrapperState(\\\r\n                cell_state=encoder_state,\r\n                attention=attn_zero,\r\n                time=0,\r\n                attention_history=())\r\n    decoder = tf.contrib.seq2seq.BasicDecoder(\r\n              cell = attn_cell,\r\n              helper = helper, # A Helper instance\r\n              initial_state = init_state, # initial state of decoder\r\n              output_layer = None) # instance of tf.layers.Layer, like Dense\r\n\r\n    # Perform dynamic decoding with decoder object\r\n    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\r\n```\r\n\r\nBut got an error on the `dynamic_decode` (below). Maybe you have instructions on how to properly set this up?\r\n\r\n```shell\r\n  File \"/home/andre/projects/seq2seq_drr/enc_dec.py\", line 232, in decoder_train_attn\r\n    outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\r\n    decoder_finished) = decoder.step(time, inputs, state)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\r\n    cell_outputs, cell_state = self._cell(inputs, state)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 532, in __call__\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 443, in <lambda>\r\n    lambda inputs, attention: array_ops.concat([inputs, attention], -1))\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1036, in concat\r\n    name=name)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\r\n    name=name)\r\n  File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 464, in apply_op\r\n    raise TypeError(\"%s that don't all match.\" % prefix)\r\nTypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\r\n```", "You want:\n\ninit_state = attn_zero.clone(cell_state=encoder_final_state)\n\n\n\nOn Apr 2, 2017 4:11 PM, \"Andre Cianflone\" <notifications@github.com> wrote:\n\n> Ok, nope. I tried to set the initial cell state and attention, based on\n> the other attention's zero_state method, but no go.\n>\n> Here's what I tried\n>\n>     # Attention Mechanisms. Bahdanau is additive style attention\n>     attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n>         num_units = mem_units, # depth of query mechanism\n>         memory = attention_states, # hidden states to attend (output of RNN)\n>         memory_sequence_length=seq_len_enc, # masks false memories\n>         normalize=False, # normalize energy term\n>         name='BahdanauAttention')\n>\n>     # Attention Wrapper: adds the attention mechanism to the cell\n>     attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n>         cell = cell,# Instance of RNNCell\n>         attention_mechanism = attn_mech, # Instance of AttentionMechanism\n>         attention_size = attn_units, # Int, depth of attention (output) tensor\n>         attention_history=False, # whether to store history in final output\n>         name=\"attention_wrapper\")\n>\n>     # TrainingHelper does no sampling, only uses inputs\n>     helper = tf.contrib.seq2seq.TrainingHelper(\n>         inputs = x, # decoder inputs\n>         sequence_length = seq_len_dec, # decoder input length\n>         name = \"decoder_training_helper\")\n>\n>     # Decoder setup\n>     batch_size = tf.shape(x)[0]\n>     attn_zero = attn_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n>     init_state = tf.contrib.seq2seq.AttentionWrapperState(\\\n>                 cell_state=encoder_state,\n>                 attention=attn_zero,\n>                 time=0,\n>                 attention_history=())\n>     decoder = tf.contrib.seq2seq.BasicDecoder(\n>               cell = attn_cell,\n>               helper = helper, # A Helper instance\n>               initial_state = init_state, # initial state of decoder\n>               output_layer = None) # instance of tf.layers.Layer, like Dense\n>\n>     # Perform dynamic decoding with decoder object\n>     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n>\n> But got an error on the dynamic_decode (below). Maybe you have\n> instructions on how to properly set this up?\n>\n>   File \"/home/andre/projects/seq2seq_drr/enc_dec.py\", line 232, in decoder_train_attn\n>     outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 278, in dynamic_decode\n>     swap_memory=swap_memory)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\n>     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\n>     pred, body, original_loop_vars, loop_vars, shape_invariants)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\n>     body_result = body(*packed_vars_for_body)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 231, in body\n>     decoder_finished) = decoder.step(time, inputs, state)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 140, in step\n>     cell_outputs, cell_state = self._cell(inputs, state)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 532, in __call__\n>     cell_inputs = self._cell_input_fn(inputs, state.attention)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 443, in <lambda>\n>     lambda inputs, attention: array_ops.concat([inputs, attention], -1))\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1036, in concat\n>     name=name)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n>     name=name)\n>   File \"/home/andre/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 464, in apply_op\n>     raise TypeError(\"%s that don't all match.\" % prefix)\n> TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-291021918>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim59Wb4MPNiKHa_gNxHRQyMIlwWW8ks5rsCsWgaJpZM4MuICf>\n> .\n>\n", "Awesome, finally got it to work. Your `clone` method only popped up today when I pulled, for whatever reason. ", "@ebrevdo I tried to use your previous code as explained. But If I put the encoder_state of the previous level (2 bidirectional encoders with GRU), I have a memory leak on GPU.\r\nMore precisely, here my code:\r\n```\r\n        W_embedding_output = tf.Variable(tf.random_uniform([decoder_output_dim, self.vocab_size], -0.01, 0.01), name=\"embedding_output\")\r\n        W_t = tf.transpose(W_embedding_output, [1, 0])\r\n        output_y_embedded = tf.nn.embedding_lookup(W_t, output_y)\r\n\r\n        # base cell to use for the decoder (same of inference, also for attention technique)\r\n        base_cell = DropoutWrapper(GRUCell(decoder_output_dim), self.dropout, 1.0, self.dropout)\r\n        attention_technique = BahdanauAttention(decoder_output_dim, encoded)\r\n        # helper to fit the previous state into decoder\r\n        helper = TrainingHelper(output_y_embedded, output_batch_length)\r\n        # cell to use, with attention method\r\n        cell = DynamicAttentionWrapper(\r\n            base_cell,\r\n            attention_technique,\r\n            self.encoder_dim,\r\n            output_attention=False\r\n        )\r\n        # initial state of the decoder, should be use the last encoder state as initial point?\r\n        zero_state = cell.zero_state(self.batch_size, tf.float32)\r\n\r\n        # zero_state = zero_state.clone(cell_state=encoder_state)\r\n\r\n        # decode\r\n        (outputs, index), final_state = dynamic_decode(\r\n            BasicDecoder(\r\n                cell,\r\n                helper,\r\n                zero_state)\r\n        )\r\n```\r\nSo, If I remove use the line **zero_state = zero_state.clone(cell_state=encoder_state)** during the session.run the program continuously allocate memory on GPU and finally it crashes due to device memory become full.\r\nHere the error.\r\n```\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[9024,1024]\r\n\t [[Node: gradients/tower2/Decoder/MatMul_grad/MatMul = MatMul[T=DT_FLOAT, _class=[\"loc:@tower2/Decoder/MatMul\"], transpose_a=false, transpose_b=true, _device=\"/job:localhost/replica:0/task:0/gpu:2\"](gradients/tower2/Decoder/output_grad/Reshape, tower2/Decoder/embedding_output/read)]]\r\n\t [[Node: gradients/Sub_2/_942 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:2\", send_device_incarnation=1, tensor_name=\"edge_9973_gradients/Sub_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^_cloopgradients/tower2/Decoder/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/cond/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/_828)]]\r\n```\r\nAm I using these classes in a wrong way? Could it be a bug?", "I'd need to see how you're calling session.run and the rest of your code to see if you're doing anything else wrong.  I suggest starting a thread on StackOverflow for this since it could be user error.  This thread is closed since the originally reported issue has been fixed in TensorFlow.", "`AttributeError: 'DynamicAttentionWrapperState' object has no attribute 'clone'`\r\n\r\n@proximacent Do you know why I'm getting this error or how I can fix it? It's specifically happening on my line:\r\n\r\n`init_state = attention_zero.clone(cell_state=encoder_final_states)`\r\n\r\n(I pluralize encoder final states because I'm using a bidirectional RNN. The two states are concatenated, though.)", "For anyone else coming along, I think I found the problem*. @ebrevdo 's comments are based on TensorFlow 1.2, not TensorFlow 1.1, so if you want to solve this problem, you'll need to switch. Then, you'll want to replace DynamicAttentionWrapper with AttentionWrapper.\r\n\r\n*if I'm wrong, anyone can feel free to correct me.", "@RylanSchaeffer , yes that's correct. At the time, ebrevdo's code was pre 1.2 but post 1.1. In the 1.2 release, some other naming has changed as well. For example AttentionWrapper's `attention_size` is now `attention_layer_size`. If you are using my above code, check the [AttentionWrapper page](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/seq2seq/AttentionWrapper) for the new `__init__` arguments. ", "@proximacent , I'm running into a new problem. Can I email you? There's not much (up-to-date) material online, and it sounds like you've already done something similar to what I'm working on.", "My problem is that somewhere inside my decoder, the following line produces this error: TypeError: 'Tensor' object is not iterable.\r\n\r\n`c, h = state`\r\n\r\nI've stepped through my code, and the failure seems to be generated when something causes the state variable to be a tensor (instead of a tuple) with the name \"define_model/define_decoder/decoder/while/Identity_3\"\r\n\r\ndefine_model() and define_decoder() are functions I wrote, but I don't know what inside decoder would be creating this Identity_3 and why it isn't a tuple.\r\n\r\nI know this isn't the right place for this. @ebrevdo , perhaps I could email you directly?", "@RylanSchaeffer, what's `state`, what is the output of `type(state)`? If I had to guess, maybe you changed from LSTM cell to GRU. Some functions, like tf.nn.dynamic_rnn, return a pair of (output, state), where `state` is LSTMStateTuple if you are using an LSTM cell, (which have `c` and `h`), but a Tensor if using a GRU cell.", "@proximacent , it depends on when I evaluate `type(state)`. For the first two times that `c, h = state` is evaluated, state has type `<class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'>`. Then, (just before my code breaks), state has type `<class 'tensorflow.python.framework.ops.Tensor'>`.\r\n\r\nI am using LSTM cells, not GRU cells (specifically `BasicLSTMCell` from `tensorflow.contrib.rnn`). Why and where would my code be switching RNN cell type?\r\n\r\nAlso, thanks for the help!", "@RylanSchaeffer , please paste some code. Should have `state = the.tensorflow.function`, the code where `type(state)` evaluates to LSTMStateTuple, Tensor, and code in between. ", "@proximacent , here's some of the code that I wrote:\r\n```\r\n            bahdanau_attention = BahdanauAttention(num_units=DECODER_NUM_UNITS,\r\n                                                   memory=encoder_outputs)\r\n\r\n            attention_cell = AttentionWrapper(cell=self._create_lstm_cell(),\r\n                                              attention_mechanism=bahdanau_attention)\r\n\r\n            attention_zero = attention_cell.zero_state(batch_size=self.x.shape[0], dtype=tf.float32)\r\n\r\n            init_state = attention_zero.clone(cell_state=encoder_final_states)\r\n\r\n            training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth\r\n                                             sequence_length=length)  # feed in sequence length\r\n\r\n            decoder = BasicDecoder(cell=attention_cell,\r\n                                   helper=training_helper,\r\n                                   initial_state=init_state)\r\n```\r\n\r\nThe error is raised in line 379 `c, h = state` of TensorFlow's rnn_cell_impl.py. If you want, I can paste that code.\r\n\r\nIf I set a breakpoint right before line 379, the first two times I reach the breakpoint, `type(state)` evaluates to `<class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'>`. The third time, `type(state)` evaluates to `<class 'tensorflow.python.framework.ops.Tensor'>`.", "I'm away this week but hope one of my collaborators can help with this\nissue.\n\nOn Jun 13, 2017 9:56 AM, \"Rylan Schaeffer\" <notifications@github.com> wrote:\n\n> Here's some of the code that I wrote:\n> `\n>\n>         bahdanau_attention = BahdanauAttention(num_units=DECODER_NUM_UNITS,\n>                                                memory=encoder_outputs)\n>\n>         attention_cell = AttentionWrapper(cell=self._create_lstm_cell(),\n>                                           attention_mechanism=bahdanau_attention)\n>\n>         attention_zero = attention_cell.zero_state(batch_size=self.x.shape[0], dtype=tf.float32)\n>\n>         init_state = attention_zero.clone(cell_state=encoder_final_states)\n>\n>         training_helper = TrainingHelper(inputs=self.y,  # feed in ground truth\n>                                          sequence_length=length)  # feed in sequence length\n>\n>         decoder = BasicDecoder(cell=attention_cell,\n>                                helper=training_helper,\n>                                initial_state=init_state)\n>\n> `\n>\n> The error is raised in line 379 c, h = state of TensorFlow's\n> rnn_cell_impl.py.\n>\n> If I set a breakpoint right before this line, the first two times I reach\n> the breakpoint, type(state) evaluates to <class\n> 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'>. The third time,\n> type(state) evaluates to <class 'tensorflow.python.framework.ops.Tensor'>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8833#issuecomment-308180696>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2rkcUu1rNuBoBGGn5y2g8W4sGDzks5sDr9LgaJpZM4MuICf>\n> .\n>\n", "Hi @RylanSchaeffer,\r\n\r\nLet me see if I can help you. What is the structure of your decoder? Is it a MultiRNNCell with several BasicLSTMCells? Can you print what is exactly in the encoder_final_states? (Is it something like (LSTMStateTuple, LSTMStateTuple, Tensor)?)", "@oahziur , thank you! My decoder is not a MultiRNNCell - it's just one BasicLSTMCell with an AttentionWrapper. Here is my method I use to create an LSTM cell:\r\n\r\n```\r\n@staticmethod  \r\ndef _create_lstm_cell():  \r\n    return BasicLSTMCell(LSTM_SIZE)\r\n```\r\n\r\nHere is my function call to wrap the cell with a Bahdanau Attention mechanism:\r\n\r\n```\r\nattention_cell = AttentionWrapper(cell=self._create_lstm_cell(),\r\n                           attention_mechanism=bahdanau_attention)\r\n```\r\n\r\nAnd finally here is how I pass the attention cell `BasicDecoder`:\r\n\r\n```\r\ndecoder = BasicDecoder(cell=attention_cell,\r\n                  helper=training_helper,\r\n                  initial_state=init_state)\r\n```\r\n\r\n\r\nMy `encoder_final_states` has type `<class 'tensorflow.python.framework.ops.Tensor'>` and `shape=(2, ?, 128)`; this makes sense as I'm using a Bidirectional RNN and my `LSTM_SIZE = 64`. `encoder_final_states` one of the two values returned by the following method:\r\n\r\n\r\n```\r\ndef _define_encoder(self):\r\n    with tf.name_scope('define_encoder'):\r\n        outputs, final_states = bidirectional_dynamic_rnn(cell_fw=self._create_lstm_cell(),\r\n                                              cell_bw=self._create_lstm_cell(),\r\n                                              inputs=self.x,\r\n                                              dtype=tf.float32)\r\n        outputs = tf.concat(outputs, axis=-1)\r\n        final_states = tf.concat(final_states, axis=-1)\r\n\r\n    return outputs, final_states\r\n```\r\n\r\nSorry about the formatting. I tried to clean it up best I could.", "@RylanSchaeffer , if your LSTM_SIZE=64, the final state should be (2, ?, 256) tensor if you set [`state_is_tuple=False` in BasicLSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell), since each state is 64, so a single state should have 128.\r\n\r\nI think you shouldn't do `final_states = tf.concat(final_states, axis=-1)` if `state_is_tuple=True` (the default behavior) since the return value of [bidirectional_dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn) will be a tuple of LSTMStateTuple instead of a tuple of Tensor. Concatenate c state and h state separately and then create a  `tf.contrib.rnn.LSTMStateTuple` to pass to the decoder should fix your problem.\r\n", "@oahziur , I'm a little confused. My understanding was that `final_states = tf.concat(final_states, axis=-1)` isn't concatenating the c state and the h state for an LSTM cell, but rather concatenating states of the two RNNs that together compose the bidirectional RNN i.e. c1 concatenated with c2, h1 concatenated with h2. Can you clarify?", "@RylanSchaeffer , I think the bidirectional_dynamic_rnn returns outputs and final states. \r\n\r\nYou have use `outputs = tf.concat(outputs, axis=-1)` to concat the outputs, which isn't a problem.\r\n\r\nIf you print final_states before `final_states = tf.concat(final_states, axis=-1)`, it should be (LSTMStateTuple, LSTMStateTuple) because you have one fw_cell and one bw_cell.", "@oahziur ok, that makes sense! I'm now confused by what you mean when you say, \"then create a tf.contrib.rnn.LSTMStateTuple to pass to the decoder should fix your problem.\" Can you explain more?", "@oahziur , my confusion stems from the fact that `BasicDecoder` accepts `initial_state: A (possibly nested tuple of...) tensors and TensorArrays.` Since I pass in `initial_state=init_state`, where `init_state = attention_zero.clone(cell_state=encoder_final_states)` and `encoder_final_states` is a 2-tuple of LSTMStateTuples, it doesn't seem like I should need to make any modifications.\r\n\r\nAnd yet `dynamic_decode(decoder=decoder)` now gives me the following error: `AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'`", "@RylanSchaeffer The problem is that you decoder only have 1 BasicLSTMCell so it can only accept a single LSTMStateTuple, but you have 2 LSTMStateTuples from the `bidirectional_dynamic_rnn`. \r\n\r\nIs this helpful?\r\n", "@oahziur , let me try to explain what my current understanding is, and you can tell me if I'm correct. A BasicDecoder accepts only one cell. One cell corresponds to one LSTMStateTuple (consisting of c and h). My problem is that I have two LSTMStateTuples. To reduce the two LSTMStateTuples down to one LSTMStateTuple, I need to separately concatenate the c states (i.e. c = c1 concat c2) and the h states (i.e. h = h1 concat h2), and then pass the newly created c and h states into a new LSTMStateTuple.\r\n\r\nIf that's correct, my next question is where I would use this new LSTMStateTuple? BasicDecoder accepts a cell, not a LSTMStateTuple. Would I pass the LSTMStateTuple in as the initial state? That makes the most sense. But then how do I do this? Do I change my code from `init_state = attention_zero.clone(cell_state=encoder_final_states)` to `init_state = attention_zero.clone(cell_state=new_lstm_state_tuple)`?", "@RylanSchaeffer Yes. your understanding is correct.\r\n\r\nJust a reminder, I believe your decoder lstm cell needs to have `2*LSTM_SIZE` in order to match the shape of the new concatenated state.", "Makes sense. @oahziur Thank you so much!", "@oahziur , I have an unrelated, general question about best practices. I'm currently trying to use RNNs to solve the problem I'm working on, but I'd also like to consider using fully convolutional models as well. How would you recommend storing and processing my data in a way conducive to training both types of models?\r\n\r\nCurrently, I've written a script to parse all of my data (originally in .csv files) into `tf.train.SequenceExample`s and write those to TFRecords. Then I use a TFRecordReader in conjunction with `tf.parse_single_sequence_example` to pipe data into my RNN models for training. However, I don't think that this will work for convolutional models.\r\n\r\nI don't know if asking here makes sense. Creating a new issue wouldn't make sense, but people on StackOverflow don't seem to respond to TensorFlow questions with the same alacrity (if at all).", "@RylanSchaeffer I am not very familiar with modeling sequence data with convolutional models and your use cases, but if you data is fixed length, I think you can always reshape the input Tensor to fit your convolutional model after you parse the example.", "@oahziur , can I try asking the question another way? Is there a protocol buffer other than `SequenceExample` that makes feeding data to a dynamic RNN easy?\r\n\r\nTo give more information, my data has the following shape: `[number of samples per minibatch, time steps per sample, number of features]`. Both `number of samples per minibatch` and `number of features` are fixed, but `time steps per sample` can vary (hence the use of a dynamic RNN). At least conceptually, it should be possible to reshape my data as `[number of samples per minibatch, time steps per sample, number of features, 1]` and treat this akin to a single-channel image (which can then be passed through convolutional layers). However, I'm confused by how I would parse data stored as a `SequenceExample` in a way that would let me do this reshaping.", "I spent more time reading up on `SequenceExample`, and I realized that I was incorrectly following a [tutorial](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/). His example uses sequences that have single features, but I can't find a tutorial that uses sequences with multiple features.\r\n\r\nI think the reason why I couldn't directly reshape my data is because I was improperly structuring my sequences in `SequenceExample`. How do I correctly structure sequential data with multiple features in a `SequenceExample`?", "Never mind. Everything just clicked. Thanks for the help!", "Funny, I was playing with my code and I think I got the exact error @RylanSchaeffer got. Basically I was switching back and forth between GRU and LSTM which don't have the same outputs from the `tf.nn.bidirectional_dynamic_rnn` function. The problem arises from:\r\n\r\n```\r\n      outputs, state = tf.nn.bidirectional_dynamic_rnn(\\\r\n                  cell_fw=cell_fw,\r\n                  cell_bw=cell_bw,\r\n                  inputs=x,\r\n                  sequence_length=seq_len,\r\n                  initial_state_fw=init_state_fw,\r\n                  initial_state_bw=init_state_bw,\r\n                  dtype=self.float_type)\r\n```\r\nIf your cells are LSTM, then `state` is a tuple of 2 LSTMStateTuple, and if your cells are GRU then `state` is a tuple of Tensor. As mentioned, for LSTMStateTuple you need to \"hack\" concat `state`. I also have to `set_shape` to the `state` Tensor otherwise you can get issues with decoding. The code below works for both GRU and LSTM:\r\n\r\n```\r\n      # If LSTM cell, then \"state\" is not a tuple of Tensors but an\r\n      # LSTMStateTuple of \"c\" and \"h\". Need to concat separately then new\r\n      if \"LSTMStateTuple\" in str(type(state[0])):\r\n        c = tf.concat([state[0][0],state[1][0]],axis=1)\r\n        h = tf.concat([state[0][1],state[1][1]],axis=1)\r\n        state = tf.contrib.rnn.LSTMStateTuple(c,h)\r\n      else:\r\n        state = tf.concat(state,1)\r\n        state.set_shape([None, bi_encoder_size])\r\n```\r\nTo me this seems a little hackish, but works for now. I think `tf.nn.bidirectional_dynamic_rnn` should have an optional \"concat\" arg which checks in the background what type of cell was passed and concats accordingly.\r\n\r\nCheers", "@proximacent @oahziur @ebrevdo \r\n\r\nI'm trying to train a sequence to sequence model, but the training times are monotonically increasing. I posted on [StackOverflow](https://stackoverflow.com/questions/44706150/tensorflow-seq2seq-training-time-per-minibatch-monotonically-increases), but I thought I'd ask in case any of you have an idea of what I might be doing wrong."]}, {"number": 8832, "title": "[tensorflow.org] Wrong footer position", "body": "In a few pages of tensorflow.org (like https://www.tensorflow.org/install/install_windows), the footer appears at the wrong place.\r\n\r\nThe fix is to move \r\n```\r\n<footer class=\"devsite-utility-footer\">...</footer>\r\n<footer class=\"devsite-footer-linkboxes nocontent devsite-footer-linkboxes-all-backup\">...</footer>\r\n```\r\nafter\r\n```\r\n<div class=\"devsite-main-content clearfix\">...</div>\r\n```", "comments": ["@wolffg could you reassign appropriately?", "@gunan Have you considered open sourcing the docs as well?", "all the docs are already opensource.\r\nHere is the document in question:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_windows.md\r\n\r\n", "The footer location is not available to edit externally.\n\nIn this case (install_windows), there is an unclosed <table> tag which is\ncausing issues.  I will fix this and republish.  Thanks for the note, and\nif you find other pages with weird footers, please report them.\n\nOn Thu, Mar 30, 2017 at 6:34 PM, gunan <notifications@github.com> wrote:\n\n> all the docs are already opensource.\n> Here is the document in question:\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/docs_src/install/install_windows.md\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8832#issuecomment-290591416>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1YDtpFQ2HOxp5XF359ohLALldGZyjFks5rrFgNgaJpZM4MuHJ1>\n> .\n>\n", "@wolffg  Ah, I see where is the trailing `<table>` now, do you want me to send a PR for this?\r\n\r\nAlso, I think `>>>` in the following snippet (same page) should be removed, the markdown rendering engine used by tensorflow.org confuses it with blockquote (Github's is fine with it though).\r\n\r\n```md\r\n```python\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n>>> print(sess.run(hello))\r\n```\r\n", "Don't worry about it.  I already submitted something; it should be\npublished soon.\n\nOn Tue, Apr 4, 2017 at 2:53 AM, Loo Rong Jie <notifications@github.com>\nwrote:\n\n> @wolffg <https://github.com/wolffg> Ah, I see where is the trailing\n> <table> now, do you want me to send a PR for this?\n>\n> Also, I think >>> in the following snippet (same page) should be removed,\n> the markdown rendering engine used by tensorflow.org confuses it with\n> blockquote (Github's is fine with it though).\n>\n> ```python\n> >>> import tensorflow as tf\n> >>> hello = tf.constant('Hello, TensorFlow!')\n> >>> sess = tf.Session()\n> >>> print(sess.run(hello))\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8832#issuecomment-291451510>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1YDt-UC6MUYx-09b9gm5iQXF-rC4N3ks5rshMmgaJpZM4MuHJ1>\n> .\n>\n", "Closed, see #8960 "]}, {"number": 8831, "title": "Add CMake v3.8 as working version", "body": "While CMake 3.7 was causing build errors with GPU ON. I was able to compile Tensorflow (master) with CMake 3.8 with GPU enabled perfectly fine", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @KhabarlakKonstantin!\r\n\r\nfyi @gunan.  Also looks like cmake 3.8 is still in RC4?", "@yifeif Yes, CMake 3.8 is in RC4\r\nIt looks like it has started compiling with CMake 3.7.2. Probably, someone has fixed the issue\r\nLet me recheck it once more and I'll update PR, if it works", "If the build also works with 3.7.2, let's edit the message to 3.5 or later, but leave a note saying 3.7.0 and 3.7.1 have a bug that breaks our GPU build.\r\n\r\n@mrry @guschmue FYI.", "I made a change in CMakeLists.txt some time ago and the define that was breaking gpu builds should be gone (there was a define passed to nvcc which broke nvcc but that define was only used to generate version.cc) so I moved it and its no longer passed to nvcc.\r\nAll cmake versions better than 3.6 should work now. I use 3.8 on my box and its fine. ", "Jenkins, test this please."]}, {"number": 8830, "title": "Couldn't open CUDA library libcupti.so.8.0", "body": "Hi,\r\n\r\nI've installed 1.0 on my ubuntu 16.04 machine and deployed tf-1.0 with success, at least before I run the example shipped with tf, e.g, [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py)\r\n\r\nWhen running mnist_with_summaries.py, I got the very problem posted in #5282. The discussion is mainly about the missing `libcupti.so` and most of them suggested to install or symlink the lib. But both approach didn't work for me.... Since the `so` file exists when installing CUDA-8.0 under the directory `/usr/local/cuda/extra/CUPTI/lib64/`, so I thought what I need to do is just link the library under the directory `/usr/local/cuda/lib64/`. However, problem still exists after the linking. And the `libcupti-dev` in `apt-get` is old (v7.5), which fails also. \r\n\r\nAnyone could give me some advice? Thanks in advance. \r\n\r\n@drpngx @gunan @K-Wu", "comments": ["Did you try adding `/usr/local/cuda/extras/CUPTI/lib64/` to your `LD_LIBRARY_PATH`", "Thanks for your timely reply! @gunan \r\n\r\nDo you mean [here](https://github.com/tensorflow/tensorflow/blob/v1.0.0/tensorflow/tools/docker/Dockerfile.devel-gpu#L90)? \r\n\r\nI installed tf 1.0.0 from `pip` repo. I've no idea about where to check this....", "PIP has no relation to the environment variable.\r\nBased on your reply, I assume you did not set it.\r\nYou can try this before you run `mnist_with_summaries.py`\r\nIt will have to be run in the same terminal, right before you run TF.\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n```\r\n\r\n", "Thanks very much for pointing out this! @gunan \r\n\r\nThought I need to learn more on OS, especially linux....", "Note @gunan had a typo in his first suggestion (\"extra\" should be \"extras\") as it is, correctly, in his second comment.", "Thanks for pointing this out @ShariqM !\r\nI edited my first comment to avoid any confusion.", "> PIP has no relation to the environment variable.\r\n> Based on your reply, I assume you did not set it.\r\n> You can try this before you run `mnist_with_summaries.py`\r\n> It will have to be run in the same terminal, right before you run TF.\r\n> \r\n> ```\r\n> export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n> ```\r\n\r\nIt's working ! Thanks !\r\n", "What if i donot have the cuda folder in /usr/local ? I'm trying to run it on cloud and I use module to load cuda. ", "If you use a module, you have to find where your module is and use the path of the cuda version inside. For me it was:\r\n`export LD_LIBRARY_PATH=/opt/cuda-9.0_cudnn-7.4.2/extras/CUPTI/lib64:$LD_LIBRARY_PATH`", "Thanks for that comment ! I found my module at **/opt/apps/cuda/9.0/extras/CUPTI/lib64/** , \r\nadding it here just incase someone else stumbles upon the same problem. Goodluck !", "Check your libcupti.so.8.0 file in \"/usr/local/cuda/extra/CUPTI/lib64/\" path. If exists, copy the whole libcupti.* file in LD_LIBRARY_PATH\r\n\r\n>> cd /usr/local/cuda/extra/CUPTI/lib64/\r\n>> sudo cp /usr/local/cuda/extra/CUPTI/lib64/libcupti* /directory/of/LD/LIBRARY/PATH/\r\n\r\nHope this will work fine, for me It worked.\r\nGood luck !!", "I confirm that the path of libcupti has been added to LD_LIBRARY_PATH, but the program still reminds that can not load libcupti.so. Then, I execute the following commands:\r\n`sudo cp /usr/local/cuda-10.1/extras/CUPTI/lib64/libcupti.so /usr/local/lib/libcupti.so && sudo ldconfig\r\nsudo cp /usr/local/cuda-10.1/extras/CUPTI/lib64/libcupti.so.10.1 /usr/local/lib/libcupti.so.10.1 && sudo ldconfig`.\r\nIt works."]}, {"number": 8829, "title": "Tensorboard smoothing is not smooth at all.", "body": "I have Tensorflow v1.0.1 installed on Linux.\r\n\r\n\"Smoothed\" Tensorboard looks not smooth at all compared to the original graph. For some reason it manages to find some interesting loss function behavior features that simply do not exist in the original loss data.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/1829149/24485892/f269a388-14bb-11e7-8b4c-b21d54e4c4be.png)\r\n\r\n", "comments": ["I've found tensorboard to sometimes get a bit upset with smoothing and that I need to change the smoothing level. Can you post some screenshots of the data with 0, 0.5 and 1 smoothing?", "Agreed. I don't know how the smoothing is being done under the hood, but it definitely doesn't \"look right\".\r\n\r\n![image](https://cloud.githubusercontent.com/assets/4053170/24511336/5cf5ef34-1531-11e7-8be4-938527377aa7.png)\r\n(Smoothing = 1)", "I believe that it uses an IIR filter as seen in the [Tensorboard Code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/vz_line_chart/vz-line-chart.ts#L455)\r\n", "Hmm. Does that make sense for non-periodic data? This may be too simplified but I feel like just a moving average would be better...\r\n\r\nEDIT\r\n\r\nLooking at the code more it seems like it *is* just a recursive average. Then I wonder why we're seeing these artifacts. I don't see how a recursive average could create the plot I showed (or the OP plot for that matter). I need to investigate further.", "@dandelionmane, could you provide some insight, please?", "Note that the referenced sources are in master branch, where the behaviour has much changed. In TensorFlow 1.0.1, a window average is used https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/tensorboard/components/vz_line_chart/vz-line-chart.ts#L432 , causing the artifacts on the images.\r\n\r\nHowever, an exponential moving average has been merged 4 days ago in pull request #8363, resulting in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/vz_line_chart/vz-line-chart.ts#L455.", "It would also be nice if TensorBoard could somehow ignore outliers not just in the chart scaling but also in the smoothing.  And it would be nice if it could nonetheless let you see what those outliers are.  Since there seems to be no way to zoom in sufficiently to go step by step in TensorBoard, there is no way to find out exactly when outliers occurred and what their values are.\r\n\r\nHowever, I know the long term plan for TensorBoard involves making it more pluggable and user-extensible, so it might be that the core team should just be spending its effort on making this happen so that users can supply whatever smoothing function they like or even replacements for the whole scalar display widget.", "Hi all,\r\n\r\nI posit that no smoothing is perfect, and there are always going to be some edge cases where it doesn't behave quite like you imagine it should. \r\n\r\nI'm closing this issue since we're migrating to a new repo, and because it looks like the new exponential smoothing solves a lot of complaints here. If you still want smoothing changes, please open a new issue at https://github.com/tensorflow/tensorboard, with the following two things:\r\n\r\n1. a new algorithm that you are proposing\r\n2. examples of how the new algorithm behaves on some data sets, compared to how the current algorithm behaves"]}, {"number": 8828, "title": "Suport cuDNN v6.0", "body": "cuDNN v6.0 has been released. There are some cool new features:\r\n* Dilated Convolutions: Dilated Convolutions are now supported in cuDNN without a\r\nchange in API.\r\n* `cudnnConvolutionBiasActivationForward` allows for the execution of a single kernel fusing convolution, bias and activation operations\r\n\r\nFull release notes:\r\n>Dilated Convolutions: Dilated Convolutions are now supported in cuDNN without a\r\nchange in API. Previously unused \u201cupscale\u201d fields in the Convolution Descriptor\r\nhave been repurposed to allow user specification of dilation factors along each\r\ndimension. Support for dilation is present in the following code paths :\r\nForward : CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\r\nBackward Data: CUDNN_CONVOLUTION_BWD_DATA_ALGO_0 and\r\nBackward Filter: CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0\r\n\uf0b7 The new API cudnnConvolutionBiasActivationForward allows for the execution of\r\na single kernel fusing convolution, bias and activation operations. At present, only\r\nper channel bias and RELU activation are supported.\r\n\uf0b7 Inference on 8 bit integer data is now supported, leveraging the 4 element dot\r\nproduct instruction (IDP4A) of Pascal GPUs with CUDA capabilities 6.1. Two tensor\r\nlayouts are supported for this feature: CUDNN_TENSOR_NHWC with INT8 data\r\ntype and CUDNN_TENSOR_NCHW_VECT with INT8x4 data type.\r\n\uf0b7 RNN now supports 3 algorithms\r\no CUDNN_RNN_ALGO_STANDARD :\r\n\uf0a7 Same functionality as in CUDNN v5.1\r\no CUDNN_RNN_ALGO_PERSIST_STATIC :\r\n\uf0a7 This algorithm relies on the usage of persistent CUDA kernels which\r\nare pre-compiled to fit different GPUs.\r\n\uf0a7 This algorithm is available only on Pascal GPUs.\r\no CUDNN_RNN_ALGO_PERSIST_DYNAMIC :\r\nThis algorithm also relies on the usage of persistent CUDA kernels\r\nbut these kernels are compiled at runtime using nvrtc. In some cases\r\nthis results in a significant performance benefit.\r\n\uf0a7 This algorithm is also available only on Pascal GPUs and is supported\r\nonly on Linux and Windows.\r\n\uf0b7 Support for 1D-FFT convolutions has been added\r\n\uf0b7 New API routine cudnnReduceTensor has been added, supporting 8 reduction\r\noperations\r\n\uf0b7 Activation mode CUDNN_ACTIVATION_ELU is now supported.\r\n\uf0b7 A deterministic max pooling mode CUDNN_POOLING_MAX_DETERMINISTIC\r\nhas been added.\r\n\uf0b7 Significant performance improvement for softmax layers for mode\r\nCUDNN_SOFTMAX_MODE_CHANNEL has been achieved when low batch\r\nnumber is used.\r\n\uf0b7 Significant performance improvements have been added for cudnnAddTensor\r\nwhen spatial dimensions are set to 1.", "comments": ["We base our builds off of nvidia/cuda docker images: https://hub.docker.com/r/nvidia/cuda/\r\nOnce cudnn 6 appears here, we will try to update our builds very quickly.", "@gunan not sure if you were referring to this, but part of this ticket is about not just new docker images, but also updating the TF kernels to take advantage of the new features in cuDNN 6. For example, [pyTorch uses cuDNN 6 for dilated convolutions](https://github.com/pytorch/pytorch/blob/8aa1cefed880e3ad1a839bdbdc9d1fb9d52f5af3/torch/nn/_functions/conv.py#L81-L82). ", "I understand, however moving our build infra to cudnn6 will have to be the first step.\r\nFor updating ops, and a timeline for that, @zheng-xq should comment.", "Please note that the new Docker images will probably break your build because of #8264, see the details on GitLab:\r\nhttps://gitlab.com/nvidia/cuda/issues/2#note_26563587", "okay @gunan. https://hub.docker.com/r/nvidia/cuda/ now has cudnn6 images.", "Is there an ETA for this issue?", "Hello, \r\n\r\nAbout the cudnn 6.0 compatibility, is it already working? I am trying tensorflow installation from source and I have got the error:\r\n\r\n> Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nInvalid path to cuDNN 6.0.21 toolkit. Neither of the following two files can be found:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/libcudnn.so\r\n\r\nThe OS is ubuntu 16.04, CUDA 8.0 and used anaconda for python 2.7\r\n", "@gnujimmik \r\nSeems like you have installed CUDA but not cuDNN.\r\nPlease refer to: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\r\n(As far as I know cuDNN isn't shipped with CUDA directly)", "@Androbin @gnujimmik actually, `libcudnn.so` might be in the standard install path:\r\n```\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so\r\n```", "@Androbin it's not shipped with CUDA indeed, but you can install cuDNN in two ways on Ubuntu:\r\n1. Extract an archive anywhere on your filesystem.\r\n2. Use our machine learning deb package repository and `apt-get install -y libcudnn6`\r\n\r\nOn CentOS, you only have option 1.\r\n", "I solved it. I think the cuda runtime docker images was broken. Using a tag, cuda-8.0-devel-cudnn6-ubuntu16-04 was working well. Thank you for helping. ", "@gnujimmik have you tried cudnn 6.0 with tensorflow 1.2-rc0 ?", "@danielbaak yes. ubuntu 16.04 + cuda 8.0 + cudnn 6.0 + tensorflow 1.2-rc0. ", "@gnujimmik hi,what about cuDNN6.0  with tensorflow 1.1? Does it work?", "@gnujimmik cuDNN6.0 with cuda 8.0 is givinig the same error for tensorflow 1.1\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcudnn.so.5: cannot open shared object file: No such file or directory\r\n\r\nPlease can anyone help. \r\n", "@sounakdey how did you install tensorflow? you must build it from scratch", "@sandersk as he mentioned, you will need to build it from source. The default support for the tensorflow is cuda 8.0 and cudnn 5.", "yes i tried to built it from source but it gives an error like this\r\ncuda 8.0.61 cudnn 6\r\nERROR: /path/to/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 130 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::GetBlasGemmAlgorithms(std::vector<long long int>*)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:9: error: 'CUBLAS_GEMM_ALGO5' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:28: error: 'CUBLAS_GEMM_ALGO6' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                            ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:47: error: 'CUBLAS_GEMM_ALGO7' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\nany help will be appreciated", "@sounakdey This might be #9002", "I saw this but still having the cuda 8.0.61 is giving the same error while building both for r1.1 and r1.2. ", "@gunan - for windwos cudnn6 is fine, builds and all unit tests are passing. \r\nAll that is needed is to point cmake to the cudnn6 location\r\n\r\n```\r\nset CUDNN_HOME=C:\\local\\cudnn-8.0-windows10-x64-v6.0\\cuda\r\ncmake -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=%CUDNN_HOME% \r\n```\r\n", "Thanks for verification @guschmue !\r\nWe are working on installing cudnn on our machines. Once that is done, we will upgrade to cudnn6 accross the board.", "@Mahmoud Yes i built it from scratch but it still never worked while\ncompiling\n\nOn Fri, Jun 23, 2017 at 8:21 PM, gunan <notifications@github.com> wrote:\n\n> Thanks for verification @guschmue <https://github.com/guschmue> !\n> We are working on installing cudnn on our machines. Once that is done, we\n> will upgrade to cudnn6 accross the board.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8828#issuecomment-310737667>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIPZyzNOto02xiqVbPc_vbum17Di6EcOks5sHAI2gaJpZM4MtzHY>\n> .\n>\n", "2 days ago, virtualenv option install of tensorflow (tf) on Ubuntu 16.04 LTS, CUDNNv6, when import tensorflow, gave error ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory so it was still looking for CUDNN v5, but when I   \r\n\r\n```  \r\npip install --upgrade tensorflow\r\n``` \r\n(do that with tensorflow-gpu as well) \r\nthen it now works with CUDNNv6 successfully.  So be sure to pip install --upgrade ... (i.e. update pip) and make sure to have that latest 1.2.1 version of tf, with the pip or virtualenv pip install option.\r\n\r\n![tensorflowshortscreenshot from 2017-07-10 03-06-07](https://user-images.githubusercontent.com/3122259/28013512-1b997186-651e-11e7-9357-7dc2b6877ee0.png)\r\n\r\n", "I guess that:\r\n`pip install --upgrade tensorflow`\r\nInstalls the non-gpu version, so it has no issues with CUDDN 6, logically.", "yes\npip install --upgrade tensorflow-gpu\nis still not working with CUDDN 6\n\nOn Thu, Jul 13, 2017 at 11:54 AM, Carles Mateu <notifications@github.com>\nwrote:\n\n> I guess that:\n> pip install --upgrade tensorflow\n> Installs the non-gpu version, so it has no issues with CUDDN 6, logically.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8828#issuecomment-315030252>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIPZy-VtXWmmC_N8fhfeFRfkZc1gJmgOks5sNelkgaJpZM4MtzHY>\n> .\n>\n", "cudnn v6 support in prebuilt binaries is announced for next release in 1.2 release notes.\r\nPlease wait for 1.3 release for it to work out of the box.", "@gunan Should we wait for the full release of 1.3 or should the RC's support v6? Nothing in the rc0 release notes so curious! Ty", "1.3 RC should be built with cudnn 6.\r\n@av8ramit Could you add cuDNN 6 support to 1.3 release notes?", "@gunan reckon that's related to #11645 ", "Just FYI, I've added cuDNN 6 to the 1.3 release notes.", "Cheers. Guess this might as well be closed now @cancan101 ?", "This can probably be closed though I would ask is cuDNN merely linked against or is TF using the features offered (main ones being dilated convolution and fused conv + bias + activation)?", "It seems that the 1.3.0rc0 is just linked with the cuDNN 6, because I didn't noticed any performance improvement using dilated convolutions with 1.3.0rc0 and cuDNN 6.", "Correct, we moved the linked library to cudnn 6, but we are still not using the full power of it.\r\n@zheng-xq @tfboyd may have more to say on the timeline for using the new cudnn 6 features.", "@gunan @cancan101 @perone HI, I upgraded to tensorflow1.3 and now tensorflow couldn't find my GPU. I reckon it's due to CuDNN5.1. is this assumption true?\r\nHow can I upgrade CUDNN5 to CuDNN6 or 7? is it gonna work fine for tensorflow r1.3?\r\nI was only able to download deb file for CuDNN6 and I'm wondering if I have to use the DEB or Pow DEB file. Also, can anybody guide in how to do this upgrade from CuDNN5 to 6 or 7..\r\nThanks", "@collawolley CUDA and cuDNN are installs that are fairly simple compared to the driver.  The way I do my installed from scratch are as follows and I have a document [here](https://github.com/tfboyd/tf-tools/blob/master/install/aws_ubuntu16_04.md).  The versions of the tools change but the steps are the same.\r\n\r\n**GPU Drivers**\r\nI install these with apt-get nvidia-xxx because it is just easier and the drivers that come with the CUDA package are often old and that method seems to be harder.\r\n\r\n**CUDA**\r\nCheck out my instructions above.  Download the package and you extract it to the folder and ONLY run the CUDA install not the driver install.  My document also covers putting the paths into your ldconfig which I find easier than LD_LIBRARY_PATH although there are advantages to LD_LIBRARY_PATH if you test a lot of different versions of CUDA.  \r\n\r\n**cuDNN**\r\nThis one might make you laugh.  I do this the cheapest way possible and dump the files in with the CUDA install. Below is the script I use to swap cuDNN versions.  You need to download the cuDNN files first from the NVIDIA site (you have to signup which is 'free') and then you are copying files (maybe 5-10 total) into the CUDA include and lib64 directories.  \r\n\r\nYou have to change the directories, but this is the general idea and I am sharing it to show that you do not need a package manager and can 100% do this on your own.  That way you \"own\" it and know exactly what is happening.  \r\n\r\n```bash\r\n# Remove current cuDNN.  Commented out for safety uncomment or run manually after finding\r\n# the files\r\n#sudo rm /usr/local/cuda/include/cudnn.h\r\n#sudo rm /usr/local/cuda/lib64/libcudnn*\r\n\r\n# Add this version of cuDNN (6.0)\r\nsudo cp -P include/cudnn.h /usr/local/cuda-8.0_cuDNN_7/include/\r\nsudo cp -P lib64/libcudnn* /usr/local/cuda-8.0_cuDNN_7/lib64/\r\nsudo chmod a+r /usr/local/cuda-8.0_cuDNN_7/lib64/libcudnn*\r\nsudo chmod a+r /usr/local/cuda-8.0_cuDNN_7/include/cudnn.h\r\n\r\n# assuming you are using the ldconfig approach\r\nsudo ldconfig\r\n\r\n```\r\n\r\nGood luck.\r\n\r\n", "Dilated Convolutions support is in progress.  The link below is for me:\r\nb/38417408\r\n\r\nAdd dilation rate support in Conv2D, Conv3D, and their grad operators.  Here are some of the early numbers for the change.  My guess is this is on a local Titan but possibly something else.  I have a goal to get share information faster and that means data dumps with minimal explanation.  As far as timeline the change is in review and has been in active progress for some time.\r\n\r\n```bash\r\n1D convolution benchmark:\r\ninput_shape     filter_shape    dilation        padding new_runtime     old_runtime     speedup\r\n===============================================================================\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,2,1]\tVALID\t0.00005758\t0.00005961\t1.03526x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,2,1]\tSAME\t0.00006395\t0.00008794\t1.37514x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,4,1]\tVALID\t0.00005672\t0.00008252\t1.45487x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,4,1]\tSAME\t0.00005449\t0.00008841\t1.6225x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,8,1]\tVALID\t0.00004836\t0.00012469\t2.57837x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,8,1]\tSAME\t0.00005121\t0.00009303\t1.81664x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,16,1]\tVALID\t0.00006055\t0.00008979\t1.48291x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,16,1]\tSAME\t0.00005921\t0.00008144\t1.37544x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,32,1]\tVALID\t0.00006061\t0.00008116\t1.33905x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,32,1]\tSAME\t0.00005695\t0.00009377\t1.64653x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,64,1]\tVALID\t0.00005906\t0.00008995\t1.52303x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,64,1]\tSAME\t0.00006263\t0.00008729\t1.39374x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,128,1]\tVALID\t0.00006235\t0.00008463\t1.35734x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,128,1]\tSAME\t0.00005383\t0.00008037\t1.49303x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,256,1]\tVALID\t0.00006219\t0.00009497\t1.52709x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,256,1]\tSAME\t0.00006209\t0.00009128\t1.47012x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,512,1]\tVALID\t0.00005178\t0.00008769\t1.69351x\r\n[1,1,2000,1]\t[1,3,1,2]\t[1,1,512,1]\tSAME\t0.00005198\t0.00008554\t1.64563x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,2,1]\tVALID\t0.00006688\t0.00011240\t1.68062x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,2,1]\tSAME\t0.00006801\t0.00010370\t1.52478x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,4,1]\tVALID\t0.00006562\t0.00008887\t1.35431x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,4,1]\tSAME\t0.00005807\t0.00009184\t1.58154x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,8,1]\tVALID\t0.00007196\t0.00009882\t1.37326x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,8,1]\tSAME\t0.00005872\t0.00008156\t1.38896x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,16,1]\tVALID\t0.00005951\t0.00011199\t1.88187x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,16,1]\tSAME\t0.00005866\t0.00009291\t1.58387x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,32,1]\tVALID\t0.00006804\t0.00009703\t1.42607x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,32,1]\tSAME\t0.00007159\t0.00008727\t1.21903x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,64,1]\tVALID\t0.00006002\t0.00010006\t1.66711x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,64,1]\tSAME\t0.00006001\t0.00009324\t1.55374x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,128,1]\tVALID\t0.00006543\t0.00010054\t1.5366x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,128,1]\tSAME\t0.00006640\t0.00009464\t1.4253x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,256,1]\tVALID\t0.00007116\t0.00011236\t1.57898x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,256,1]\tSAME\t0.00006039\t0.00012486\t2.06756x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,512,1]\tVALID\t0.00006599\t0.00009332\t1.41415x\r\n[1,1,2000,4]\t[1,3,4,8]\t[1,1,512,1]\tSAME\t0.00006419\t0.00008190\t1.2759x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,2,1]\tVALID\t0.00006921\t0.00009786\t1.41396x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,2,1]\tSAME\t0.00006978\t0.00006958\t0.997134x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,4,1]\tVALID\t0.00006206\t0.00011811\t1.90316x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,4,1]\tSAME\t0.00006378\t0.00010154\t1.59204x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,8,1]\tVALID\t0.00006083\t0.00010898\t1.79155x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,8,1]\tSAME\t0.00005911\t0.00009920\t1.67823x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,16,1]\tVALID\t0.00005304\t0.00009534\t1.79751x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,16,1]\tSAME\t0.00006025\t0.00010284\t1.70689x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,32,1]\tVALID\t0.00005720\t0.00012036\t2.1042x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,32,1]\tSAME\t0.00006837\t0.00011353\t1.66052x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,64,1]\tVALID\t0.00006880\t0.00009820\t1.42733x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,64,1]\tSAME\t0.00006790\t0.00010152\t1.49514x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,128,1]\tVALID\t0.00006641\t0.00011005\t1.65713x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,128,1]\tSAME\t0.00006811\t0.00011749\t1.725x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,256,1]\tVALID\t0.00006754\t0.00011810\t1.74859x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,256,1]\tSAME\t0.00007047\t0.00011071\t1.57102x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,512,1]\tVALID\t0.00006006\t0.00010844\t1.80553x\r\n[1,1,2000,16]\t[1,3,16,32]\t[1,1,512,1]\tSAME\t0.00006161\t0.00009654\t1.56695x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,2,1]\tVALID\t0.00005919\t0.00009413\t1.5903x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,2,1]\tSAME\t0.00006383\t0.00006682\t1.04684x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,4,1]\tVALID\t0.00006381\t0.00009762\t1.52985x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,4,1]\tSAME\t0.00006399\t0.00013848\t2.16409x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,8,1]\tVALID\t0.00005774\t0.00010584\t1.83304x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,8,1]\tSAME\t0.00007597\t0.00008368\t1.10149x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,16,1]\tVALID\t0.00006298\t0.00010941\t1.73722x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,16,1]\tSAME\t0.00008816\t0.00010191\t1.15597x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,32,1]\tVALID\t0.00007067\t0.00009408\t1.33126x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,32,1]\tSAME\t0.00006937\t0.00008838\t1.27404x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,64,1]\tVALID\t0.00006999\t0.00011198\t1.59994x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,64,1]\tSAME\t0.00006322\t0.00009393\t1.48576x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,128,1]\tVALID\t0.00007443\t0.00011252\t1.51176x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,128,1]\tSAME\t0.00006478\t0.00009993\t1.54261x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,256,1]\tVALID\t0.00008803\t0.00011139\t1.26536x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,256,1]\tSAME\t0.00007771\t0.00011038\t1.42041x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,512,1]\tVALID\t0.00006616\t0.00010212\t1.54353x\r\n[1,1,2000,32]\t[1,3,32,64]\t[1,1,512,1]\tSAME\t0.00005772\t0.00011225\t1.94473x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,2,1]\tVALID\t0.00006751\t0.00009870\t1.46201x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,2,1]\tSAME\t0.00006244\t0.00008257\t1.32239x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,4,1]\tVALID\t0.00006573\t0.00010224\t1.55545x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,4,1]\tSAME\t0.00006055\t0.00010280\t1.69777x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,8,1]\tVALID\t0.00005795\t0.00009868\t1.70285x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,8,1]\tSAME\t0.00005254\t0.00008278\t1.57556x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,16,1]\tVALID\t0.00005476\t0.00010069\t1.83875x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,16,1]\tSAME\t0.00005648\t0.00011307\t2.00195x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,32,1]\tVALID\t0.00006231\t0.00007961\t1.27764x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,32,1]\tSAME\t0.00006199\t0.00008299\t1.33876x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,64,1]\tVALID\t0.00005809\t0.00008556\t1.47289x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,64,1]\tSAME\t0.00005712\t0.00008388\t1.46849x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,128,1]\tVALID\t0.00007150\t0.00008308\t1.16196x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,128,1]\tSAME\t0.00007550\t0.00009056\t1.19947x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,256,1]\tVALID\t0.00007656\t0.00008883\t1.16027x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,256,1]\tSAME\t0.00007246\t0.00008632\t1.19128x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,512,1]\tVALID\t0.00006864\t0.00008891\t1.29531x\r\n[1,1,4000,1]\t[1,3,1,2]\t[1,1,512,1]\tSAME\t0.00006079\t0.00008307\t1.36651x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,2,1]\tVALID\t0.00007227\t0.00009067\t1.2546x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,2,1]\tSAME\t0.00006243\t0.00009022\t1.44514x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,4,1]\tVALID\t0.00006956\t0.00012247\t1.76064x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,4,1]\tSAME\t0.00004862\t0.00009418\t1.93706x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,8,1]\tVALID\t0.00006702\t0.00009292\t1.38645x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,8,1]\tSAME\t0.00006085\t0.00009907\t1.6281x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,16,1]\tVALID\t0.00006899\t0.00009074\t1.31526x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,16,1]\tSAME\t0.00006986\t0.00009899\t1.41698x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,32,1]\tVALID\t0.00006411\t0.00009018\t1.40664x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,32,1]\tSAME\t0.00007178\t0.00009497\t1.32307x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,64,1]\tVALID\t0.00006592\t0.00009528\t1.44539x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,64,1]\tSAME\t0.00006156\t0.00009386\t1.52469x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,128,1]\tVALID\t0.00006765\t0.00009771\t1.44435x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,128,1]\tSAME\t0.00005723\t0.00008964\t1.56631x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,256,1]\tVALID\t0.00007014\t0.00009486\t1.35244x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,256,1]\tSAME\t0.00006361\t0.00009410\t1.47933x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,512,1]\tVALID\t0.00006537\t0.00009733\t1.48891x\r\n[1,1,4000,4]\t[1,3,4,8]\t[1,1,512,1]\tSAME\t0.00007170\t0.00010448\t1.45718x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,2,1]\tVALID\t0.00006054\t0.00009855\t1.62785x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,2,1]\tSAME\t0.00005908\t0.00009097\t1.53978x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,4,1]\tVALID\t0.00006879\t0.00009551\t1.38843x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,4,1]\tSAME\t0.00006928\t0.00009561\t1.38005x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,8,1]\tVALID\t0.00006048\t0.00009533\t1.57622x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,8,1]\tSAME\t0.00006103\t0.00009489\t1.55481x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,16,1]\tVALID\t0.00006214\t0.00008976\t1.44448x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,16,1]\tSAME\t0.00007023\t0.00010309\t1.46789x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,32,1]\tVALID\t0.00006579\t0.00009250\t1.40599x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,32,1]\tSAME\t0.00006850\t0.00009546\t1.39358x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,64,1]\tVALID\t0.00006841\t0.00009931\t1.45169x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,64,1]\tSAME\t0.00007368\t0.00009232\t1.25299x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,128,1]\tVALID\t0.00006102\t0.00009517\t1.55965x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,128,1]\tSAME\t0.00007004\t0.00009404\t1.34266x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,256,1]\tVALID\t0.00005881\t0.00009558\t1.62523x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,256,1]\tSAME\t0.00007120\t0.00009117\t1.28048x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,512,1]\tVALID\t0.00006448\t0.00009539\t1.47937x\r\n[1,1,4000,16]\t[1,3,16,32]\t[1,1,512,1]\tSAME\t0.00005708\t0.00009181\t1.60844x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,2,1]\tVALID\t0.00006209\t0.00009588\t1.54421x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,2,1]\tSAME\t0.00006653\t0.00009741\t1.46415x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,4,1]\tVALID\t0.00006567\t0.00009922\t1.51089x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,4,1]\tSAME\t0.00006993\t0.00009407\t1.3452x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,8,1]\tVALID\t0.00006089\t0.00009559\t1.56988x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,8,1]\tSAME\t0.00006255\t0.00010266\t1.64125x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,16,1]\tVALID\t0.00006349\t0.00010176\t1.60277x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,16,1]\tSAME\t0.00006249\t0.00010165\t1.62666x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,32,1]\tVALID\t0.00006386\t0.00010494\t1.64328x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,32,1]\tSAME\t0.00006792\t0.00010760\t1.58422x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,64,1]\tVALID\t0.00007513\t0.00010506\t1.39838x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,64,1]\tSAME\t0.00006888\t0.00008726\t1.26684x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,128,1]\tVALID\t0.00006989\t0.00011145\t1.59465x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,128,1]\tSAME\t0.00006776\t0.00011847\t1.74838x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,256,1]\tVALID\t0.00007142\t0.00011892\t1.66508x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,256,1]\tSAME\t0.00006216\t0.00009617\t1.54714x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,512,1]\tVALID\t0.00005241\t0.00011139\t2.12536x\r\n[1,1,4000,32]\t[1,3,32,64]\t[1,1,512,1]\tSAME\t0.00006162\t0.00009327\t1.51363x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,2,1]\tVALID\t0.00004589\t0.00008756\t1.90804x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,2,1]\tSAME\t0.00004903\t0.00008673\t1.76892x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,4,1]\tVALID\t0.00005829\t0.00008076\t1.38549x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,4,1]\tSAME\t0.00004904\t0.00008209\t1.67394x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,8,1]\tVALID\t0.00005378\t0.00008569\t1.59334x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,8,1]\tSAME\t0.00006029\t0.00009073\t1.50489x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,16,1]\tVALID\t0.00005733\t0.00008718\t1.52067x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,16,1]\tSAME\t0.00005971\t0.00008678\t1.45336x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,32,1]\tVALID\t0.00005822\t0.00008479\t1.45637x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,32,1]\tSAME\t0.00005659\t0.00009635\t1.7026x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,64,1]\tVALID\t0.00005671\t0.00008650\t1.5253x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,64,1]\tSAME\t0.00006220\t0.00011187\t1.79855x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,128,1]\tVALID\t0.00006788\t0.00008434\t1.24249x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,128,1]\tSAME\t0.00007366\t0.00008984\t1.21966x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,256,1]\tVALID\t0.00006150\t0.00008328\t1.35415x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,256,1]\tSAME\t0.00005964\t0.00009823\t1.64705x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,512,1]\tVALID\t0.00006135\t0.00010238\t1.66879x\r\n[1,1,8000,1]\t[1,3,1,2]\t[1,1,512,1]\tSAME\t0.00005900\t0.00008919\t1.51169x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,2,1]\tVALID\t0.00007488\t0.00009541\t1.27417x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,2,1]\tSAME\t0.00008420\t0.00009904\t1.17625x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,4,1]\tVALID\t0.00007031\t0.00010654\t1.51529x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,4,1]\tSAME\t0.00008041\t0.00010912\t1.35705x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,8,1]\tVALID\t0.00006782\t0.00011182\t1.64878x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,8,1]\tSAME\t0.00008019\t0.00011079\t1.38159x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,16,1]\tVALID\t0.00006540\t0.00010130\t1.54893x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,16,1]\tSAME\t0.00006462\t0.00009408\t1.4559x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,32,1]\tVALID\t0.00006994\t0.00010283\t1.47026x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,32,1]\tSAME\t0.00007176\t0.00010054\t1.40106x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,64,1]\tVALID\t0.00005334\t0.00009050\t1.69666x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,64,1]\tSAME\t0.00007829\t0.00009921\t1.26721x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,128,1]\tVALID\t0.00007156\t0.00008490\t1.18642x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,128,1]\tSAME\t0.00007064\t0.00009414\t1.33267x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,256,1]\tVALID\t0.00008280\t0.00010062\t1.21522x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,256,1]\tSAME\t0.00008573\t0.00009233\t1.07699x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,512,1]\tVALID\t0.00007640\t0.00009932\t1.3x\r\n[1,1,8000,4]\t[1,3,4,8]\t[1,1,512,1]\tSAME\t0.00006977\t0.00009816\t1.40691x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,2,1]\tVALID\t0.00006087\t0.00009866\t1.62083x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,2,1]\tSAME\t0.00005479\t0.00009548\t1.74265x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,4,1]\tVALID\t0.00007926\t0.00009564\t1.20666x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,4,1]\tSAME\t0.00008294\t0.00009277\t1.11852x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,8,1]\tVALID\t0.00007563\t0.00009144\t1.20904x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,8,1]\tSAME\t0.00006585\t0.00009198\t1.39681x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,16,1]\tVALID\t0.00005924\t0.00009398\t1.58643x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,16,1]\tSAME\t0.00007066\t0.00009238\t1.30739x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,32,1]\tVALID\t0.00007358\t0.00009095\t1.23607x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,32,1]\tSAME\t0.00007154\t0.00010677\t1.49245x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,64,1]\tVALID\t0.00008058\t0.00008823\t1.09494x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,64,1]\tSAME\t0.00005409\t0.00009674\t1.7885x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,128,1]\tVALID\t0.00007926\t0.00007920\t0.999243x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,128,1]\tSAME\t0.00006786\t0.00008257\t1.21677x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,256,1]\tVALID\t0.00005607\t0.00007784\t1.38826x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,256,1]\tSAME\t0.00006557\t0.00009491\t1.44746x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,512,1]\tVALID\t0.00006331\t0.00009373\t1.48049x\r\n[1,1,8000,16]\t[1,3,16,32]\t[1,1,512,1]\tSAME\t0.00006038\t0.00009216\t1.52633x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,2,1]\tVALID\t0.00010448\t0.00011861\t1.13524x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,2,1]\tSAME\t0.00010089\t0.00011988\t1.18822x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,4,1]\tVALID\t0.00010399\t0.00011929\t1.14713x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,4,1]\tSAME\t0.00010054\t0.00011913\t1.1849x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,8,1]\tVALID\t0.00010199\t0.00011886\t1.16541x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,8,1]\tSAME\t0.00010086\t0.00011846\t1.1745x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,16,1]\tVALID\t0.00010123\t0.00011891\t1.17465x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,16,1]\tSAME\t0.00010026\t0.00011828\t1.17973x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,32,1]\tVALID\t0.00010027\t0.00011974\t1.19418x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,32,1]\tSAME\t0.00010076\t0.00010936\t1.08535x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,64,1]\tVALID\t0.00009921\t0.00010883\t1.09697x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,64,1]\tSAME\t0.00010124\t0.00011203\t1.10658x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,128,1]\tVALID\t0.00009819\t0.00011696\t1.19116x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,128,1]\tSAME\t0.00010087\t0.00012151\t1.20462x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,256,1]\tVALID\t0.00009623\t0.00011365\t1.18102x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,256,1]\tSAME\t0.00010067\t0.00012391\t1.23085x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,512,1]\tVALID\t0.00009502\t0.00012072\t1.27047x\r\n[1,1,8000,32]\t[1,3,32,64]\t[1,1,512,1]\tSAME\t0.00009995\t0.00012979\t1.29855x\r\n\r\n2D convolution benchmark:\r\ninput_shape        filter     dilation  padding new(s)     old(s)      delta\r\n===============================================================================\r\n[1,2000,2000,1]    [3,3,1,2]   [2.2]     VALID 0.00231346  0.00303571  23.7918%\r\n[1,2000,2000,1]    [3,3,1,2]   [2.2]     SAME  0.00228869  0.00302117  24.2449%\r\n[1,2000,2000,1]    [3,3,1,2]   [4,4]     VALID 0.00226216  0.00378979  40.3091%\r\n[1,2000,2000,1]    [3,3,1,2]   [4,4]     SAME  0.00226940  0.00383298  40.7928%\r\n[1,2000,2000,1]    [3,3,1,2]   [8,8]     VALID 0.00224240  0.00380246  41.0277%\r\n[1,2000,2000,1]    [3,3,1,2]   [8,8]     SAME  0.00227775  0.00385336  40.8892%\r\n[1,2000,2000,1]    [3,3,1,2]   [16,16]   VALID 0.00220352  0.00352770  37.5366%\r\n[1,2000,2000,1]    [3,3,1,2]   [16,16]   SAME  0.00227510  0.00367454  38.0848%\r\n[1,2000,2000,1]    [3,3,1,2]   [32,32]   VALID 0.00213600  0.00262561  18.6475%\r\n[1,2000,2000,1]    [3,3,1,2]   [32,32]   SAME  0.00227678  0.00282214  19.3243%\r\n[1,2000,2000,1]    [3,3,1,2]   [64,64]   VALID 0.00200663  0.00219924  8.75803%\r\n[1,2000,2000,1]    [3,3,1,2]   [64,64]   SAME  0.00229604  0.00249575  8.002%\r\n[1,2000,2000,1]    [3,3,1,2]   [128,128] VALID 0.00174985  0.00195712  10.5906%\r\n[1,2000,2000,1]    [3,3,1,2]   [128,128] SAME  0.00228872  0.00247906  7.67791%\r\n[1,2000,2000,1]    [3,3,1,2]   [256,256] VALID 0.00128284  0.00171598  25.2416%\r\n[1,2000,2000,1]    [3,3,1,2]   [256,256] SAME  0.00228398  0.00281686  18.9175%\r\n[1,2000,2000,1]    [3,3,1,2]   [512,512] VALID 0.00056654  0.00089643  36.8004%\r\n[1,2000,2000,1]    [3,3,1,2]   [512,512] SAME  0.00227736  0.00332520  31.5121%\r\n[1,2000,2000,4]    [3,3,4,8]   [2.2]     VALID 0.00621930  0.00720891  13.7276%\r\n[1,2000,2000,4]    [3,3,4,8]   [2.2]     SAME  0.00625408  0.00715719  12.6182%\r\n[1,2000,2000,4]    [3,3,4,8]   [4,4]     VALID 0.00621946  0.00749500  17.0185%\r\n[1,2000,2000,4]    [3,3,4,8]   [4,4]     SAME  0.00627438  0.00715426  12.2987%\r\n[1,2000,2000,4]    [3,3,4,8]   [8,8]     VALID 0.00616579  0.00867273  28.906%\r\n[1,2000,2000,4]    [3,3,4,8]   [8,8]     SAME  0.00625862  0.00875359  28.5022%\r\n[1,2000,2000,4]    [3,3,4,8]   [16,16]   VALID 0.00610005  0.00808240  24.5267%\r\n[1,2000,2000,4]    [3,3,4,8]   [16,16]   SAME  0.00627068  0.00836015  24.9932%\r\n[1,2000,2000,4]    [3,3,4,8]   [32,32]   VALID 0.00591817  0.00744971  20.5584%\r\n[1,2000,2000,4]    [3,3,4,8]   [32,32]   SAME  0.00628644  0.00796382  21.0625%\r\n[1,2000,2000,4]    [3,3,4,8]   [64,64]   VALID 0.00559814  0.00646511  13.41%\r\n[1,2000,2000,4]    [3,3,4,8]   [64,64]   SAME  0.00629064  0.00707921  11.1392%\r\n[1,2000,2000,4]    [3,3,4,8]   [128,128] VALID 0.00491748  0.00555902  11.5405%\r\n[1,2000,2000,4]    [3,3,4,8]   [128,128] SAME  0.00628739  0.00662156  5.0467%\r\n[1,2000,2000,4]    [3,3,4,8]   [256,256] VALID 0.00373130  0.00516305  27.7307%\r\n[1,2000,2000,4]    [3,3,4,8]   [256,256] SAME  0.00627354  0.00846297  25.8707%\r\n[1,2000,2000,4]    [3,3,4,8]   [512,512] VALID 0.00193695  0.00419297  53.8048%\r\n[1,2000,2000,4]    [3,3,4,8]   [512,512] SAME  0.00623270  0.00983202  36.6081%\r\n[1,2000,2000,16]   [3,3,16,32] [2.2]     VALID 0.02008603  0.02596919  22.6544%\r\n[1,2000,2000,16]   [3,3,16,32] [2.2]     SAME  0.02028466  0.02592769  21.7645%\r\n[1,2000,2000,16]   [3,3,16,32] [4,4]     VALID 0.02027318  0.02616444  22.5163%\r\n[1,2000,2000,16]   [3,3,16,32] [4,4]     SAME  0.02056549  0.02615826  21.3805%\r\n[1,2000,2000,16]   [3,3,16,32] [8,8]     VALID 0.02030271  0.02589026  21.5817%\r\n[1,2000,2000,16]   [3,3,16,32] [8,8]     SAME  0.02066941  0.02676808  22.7834%\r\n[1,2000,2000,16]   [3,3,16,32] [16,16]   VALID 0.02005850  0.02607086  23.0616%\r\n[1,2000,2000,16]   [3,3,16,32] [16,16]   SAME  0.02066449  0.02675757  22.7714%\r\n[1,2000,2000,16]   [3,3,16,32] [32,32]   VALID 0.01961490  0.02569756  23.6702%\r\n[1,2000,2000,16]   [3,3,16,32] [32,32]   SAME  0.02073576  0.02729048  24.0183%\r\n[1,2000,2000,16]   [3,3,16,32] [64,64]   VALID 0.01856925  0.02516450  26.2085%\r\n[1,2000,2000,16]   [3,3,16,32] [64,64]   SAME  0.02077435  0.02641020  21.3397%\r\n[1,2000,2000,16]   [3,3,16,32] [128,128] VALID 0.01645060  0.02294116  28.2922%\r\n[1,2000,2000,16]   [3,3,16,32] [128,128] SAME  0.02074842  0.02668388  22.2436%\r\n[1,2000,2000,16]   [3,3,16,32] [256,256] VALID 0.01256535  0.01881233  33.2068%\r\n[1,2000,2000,16]   [3,3,16,32] [256,256] SAME  0.02069790  0.03215248  35.6258%\r\n[1,2000,2000,16]   [3,3,16,32] [512,512] VALID 0.00672528  0.01579534  57.4224%\r\n[1,2000,2000,16]   [3,3,16,32] [512,512] SAME  0.02051509  0.04023022  49.0058%\r\n[1,2000,2000,32]   [3,3,32,64] [2.2]     VALID 0.06883289  0.06289514  -9.44071%\r\n[1,2000,2000,32]   [3,3,32,64] [2.2]     SAME  0.06931305  0.06275911  -10.443%\r\n[1,2000,2000,32]   [3,3,32,64] [4,4]     VALID 0.06910991  0.06272004  -10.1879%\r\n[1,2000,2000,32]   [3,3,32,64] [4,4]     SAME  0.06964424  0.06283241  -10.8413%\r\n[1,2000,2000,32]   [3,3,32,64] [8,8]     VALID 0.06883455  0.06198412  -11.0519%\r\n[1,2000,2000,32]   [3,3,32,64] [8,8]     SAME  0.06975165  0.06324264  -10.2921%\r\n[1,2000,2000,32]   [3,3,32,64] [16,16]   VALID 0.06753337  0.06148470  -9.83768%\r\n[1,2000,2000,32]   [3,3,32,64] [16,16]   SAME  0.06958027  0.06326196  -9.98753%\r\n[1,2000,2000,32]   [3,3,32,64] [32,32]   VALID 0.06529795  0.06071445  -7.54927%\r\n[1,2000,2000,32]   [3,3,32,64] [32,32]   SAME  0.06935539  0.06462759  -7.31545%\r\n[1,2000,2000,32]   [3,3,32,64] [64,64]   VALID 0.06140354  0.05971524  -2.82725%\r\n[1,2000,2000,32]   [3,3,32,64] [64,64]   SAME  0.06933038  0.05810705  -19.3149%\r\n[1,2000,2000,32]   [3,3,32,64] [128,128] VALID 0.05387654  0.05367116  -0.382664%\r\n[1,2000,2000,32]   [3,3,32,64] [128,128] SAME  0.06925929  0.05933531  -16.7253%\r\n[1,2000,2000,32]   [3,3,32,64] [256,256] VALID 0.04047116  0.04335156  6.64428%\r\n[1,2000,2000,32]   [3,3,32,64] [256,256] SAME  0.06909944  0.07228471  4.40656%\r\n[1,2000,2000,32]   [3,3,32,64] [512,512] VALID 0.02030520  0.02902289  30.0373%\r\n[1,2000,2000,32]   [3,3,32,64] [512,512] SAME  0.06882550  0.08068930  14.7031%\r\n```  ", "Hello. Can you guys help me ? \r\n\r\nI have an error when I do 'import tensorflow as tf' \r\n\r\npython3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cudnnConvolutionBiasActivationForward\r\n\r\nHow can I fix it?. Thanks...", "@DonghunP what version of TensorFlow are you using? Maybe you are hitting https://github.com/tensorflow/tensorflow/issues/12326", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I think this is now obsolete, as we are looking into cudnn 7.", " Wasn't this issue being used to track the status of cudnn dilated convolution support ? See previous comments.", "Very old issue but I wanted to add that dilated convs are now supported.  I was searching for issues for another purpose and found this one.  Most/all of you likely already knew this but if not now you do.  :-)  @perone ", "That's **amazing**, I wasn't aware, thanks a lot for letting us know @tfboyd."]}, {"number": 8827, "title": "bugs with cifar10 while doing evaluation", "body": "I slightly modify the cifar10 code, which is modifying length of width and height not to have same length.\r\n\r\nheight = 16\r\nwidth = 32\r\n\r\nWhile I did eval, it showed an error log cifar10_input.py at \"  float_image.set_shape([height, width, 3])\" this line.\r\nI debugged it, I found a strange thing. Above two line from this, I found this \"  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,**width, height**)\". \r\nThe reason why I felt strange is the position of second and third param of resize_image_with_crop_or_pad is not same to compare any other functions. (normally height is second and width is third. or first and second.) I mean the order is strange.\r\n\r\nSo I think that \"  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, **height, width**)\" may be correct.\r\n\r\n", "comments": ["I'll just leave it here, https://www.tensorflow.org/api_docs/python/tf/image/resize_image_with_crop_or_pad", "@nealwu , this seems to be a problem. Could you take a look?", "It looks like you're talking about [this file](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_input.py) in the models repository? That was fixed previously by this pull request: https://github.com/tensorflow/models/pull/1011. You should pull the repository again."]}, {"number": 8826, "title": "Java: Formatting tweaks to READMEs", "body": "They looked funny when rendered on github.", "comments": []}, {"number": 8825, "title": "Branch 151636533", "body": "", "comments": []}, {"number": 8824, "title": "Update examples/android/README.md", "body": "Remove line breaks between images so they show on the same row. Apparently something changed recently in github's presentation of .md files that caused the line breaks to be interpreted literally..", "comments": []}, {"number": 8823, "title": "GRU cell error in sequence to sequence models tutorial", "body": "Hello everyone,\r\nI got the code from tensorflow website for this tutorial. It processed for a long while and got the following errors. Installing tensorflow from sources also did not work for me as it gave me many errors. so I deleted everything installed tensorflow on my Ubuntu 16.04 LTS. Please please help me in resolving these errors as it is an emergency. I am a beginner to tensorflow and its been 2 months already I have been seeing this error\r\n\r\nThanks in advance\r\n![seq](https://cloud.githubusercontent.com/assets/20130992/24481010/92cc0f3e-149c-11e7-8244-7b20f513efdd.png)\r\n", "comments": ["Closing this as a duplicate of [#8821](https://github.com/tensorflow/tensorflow/issues/8821)"]}, {"number": 8822, "title": "AttributeError: 'module' object has no attribute 'GRUCell'", "body": "Hello everyone,\r\nI got the code from tensorflow website for this tutorial. It processed for a long while and got the following errors. Installing tensorflow from sources also did not work for me as it gave me many errors. so I deleted everything installed tensorflow on my Ubuntu 16.04 LTS. Please please help me in resolving these errors as it is an emergency. I am a beginner to tensorflow and its been 2 months already I have been seeing this error\r\n\r\nThanks in advance\r\n![seq](https://cloud.githubusercontent.com/assets/20130992/24480978/67188b74-149c-11e7-89a5-63d69c3a8412.png)\r\n", "comments": ["Closing this as a duplicate of #8821 "]}, {"number": 8821, "title": "GRU cell error- sequence to sequence models tutorial", "body": "Hello everyone,\r\nI got the code from tensorflow website for this tutorial. It processed for a long while and got the following errors. Installing tensorflow from sources also did not work for me as it gave me many errors. so I deleted everything installed tensorflow on my Ubuntu 16.04 LTS. Please please help me in resolving these errors as it is an emergency. I am a beginner to tensorflow and its been 2 months already I have been seeing this error\r\n\r\nThanks in advance\r\n![seq](https://cloud.githubusercontent.com/assets/20130992/24480912/0820eb52-149c-11e7-8734-c5861c14d73e.png)\r\n", "comments": []}, {"number": 8819, "title": "Does tf.train.MonitoredTrainingSession() support GRPC?", "body": "I am not sure if the feature for `MonitoredTrainingSession()` working with `grpc` is existing or not. If not, can we have a feature request here? I also posted details on StackOverflow: http://stackoverflow.com/questions/43103763/does-tf-train-monitoredtrainingsession-support-grpc. Thanks!\r\n", "comments": ["[Answered on Stack Overflow](http://stackoverflow.com/a/43120699/3574081)."]}, {"number": 8818, "title": "Bug when using `tf.contrib.metrics.streaming_mean_iou`", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\nls -l /usr/local/cuda-8.0/lib/libcud*\r\n-rw-r--r-- 1 root root   560184 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 root root   394472 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root   737516 Sep  5  2016 /usr/local/cuda-8.0/lib/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn.so -> libcudnn.so.5\r\n-rwxr-xr-x 1 root root 61453024 Nov 13 12:10 /usr/local/cuda-8.0/lib/libcudnn.so.4\r\n-rwxr-xr-x 1 root root 61453024 Nov 13 12:10 /usr/local/cuda-8.0/lib/libcudnn.so.4.0.7\r\nlrwxrwxrwx 1 root root       17 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 78065952 Jan 12 11:03 /usr/local/cuda-8.0/lib/libcudnn.so.5.0.5\r\n-rwxr-xr-x 1 root root 79337624 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Jan 17 13:52 /usr/local/cuda-8.0/lib/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n`0.12.1`\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n\r\nI'm getting a strange error when trying to compute the intersection over union using tensorflows tf.contrib.metrics.streaming_mean_iou.\r\nThis was the code I was using before which works perfectly fine\r\n\r\n```\r\nimport tensorflow as tf\r\nlabel = tf.image.decode_png(tf.read_file('/path/to/label.png'),channels=1)\r\nlabel_lin = tf.reshape(label, [-1,])\r\nweights = tf.cast(tf.less_equal(label_lin, 10), tf.int32)\r\nmIoU, update_op = tf.contrib.metrics.streaming_mean_iou(label_lin, label_lin,num_classes = 11,weights = weights)\r\ninit = tf.local_variables_initializer()\r\nsess.run(init)\r\nsess.run([update_op])\r\n```\r\nHowever when I use a mask like this\r\n\r\n```\r\nmask = tf.image.decode_png(tf.read_file('/path/to/mask_file.png'),channels=1)\r\nmask_lin = tf.reshape(mask, [-1,])\r\nmask_lin = tf.cast(mask_lin,tf.int32)\r\nmIoU, update_op = tf.contrib.metrics.streaming_mean_iou(label_lin, label_lin,num_classes = 11,weights = mask_lin)\r\ninit = tf.local_variables_initializer()\r\nsess.run(init)\r\nsess.run([update_op])\r\n```\r\nIt keeps on failing after an irregular number of iterations showing this error:\r\n\r\n`*** Error in `/usr/bin/python': corrupted double-linked list: 0x00007f29d0022fd0 ***`\r\n\r\nI checked the shape and data type of both mask_lin and  weights. They are the same, so I cannot really see what is going wrong here.\r\nAlso the fact that the error comes after calling update_op an irregular number of times is strange. Maybe TF empties the mask_lin object after calling several sess.run()'s ?\r\nOr is this some TF bug? But then again why would it work with weights...", "comments": ["Is there anything special about how you prepared the png files. Could you run \"file <png>\" on them to show their specs. How did you create them? What tool? Try replacing those file read called with creation of tensorflow constants and see if it works ok.\r\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8817, "title": "Conditionally trainable variables and stochastic depth neural networks", "body": "I came across with a task where I would like to apply stochastic depth _regularization_ technique using Tensorflow (https://arxiv.org/pdf/1603.09382.pdf). Tensorflow doesn't provide enough settings to implement this one. I found closed issue  #1784 which is similar to this request, where guys finished the discussion with claim that [ `tf.cond` | `tf.select` ] primitives are enough for this task. But if you carefully read the paper it says that during training the depth changes for both directions: forward and backward propagation steps. Therefore number of tranable W parameters of the network changes too. The core conception of the Tensorflow is building computation graph before session of training is run. Currently, I can not create dynamic computation graph, so that depending on a boolean value W parameters of a layer were not engaged in optimisation process.\r\n\r\nIf `tf.Variable` accepted `trainable` parameter as a boolean tensor apart from built-in boolean value it would solve the problem. In this case, it would mean that Tensorflow operates natively with dynamic computational graphs, which in fact very powerful tool.\r\n\r\nI would appreciate any suggestions and ideas, so that this question was closed for good and all.\r\n\r\n@vrv, @martinwicke, @aselle", "comments": ["I am adding this to our list of models that we would like to make easier in TensorFlow. I don't have any personal knowledge of the paper, but as far as your comment about having a trainable flag. It seems like you could multiply by a vector of 0's or 1' to mask the variable dynamically to achieve the same effect. Let me know if that would be sufficient? Thanks!", "Thank you @aselle,\r\nI implemented cancelling in a residual block using multiplication by zero and `tf.stop_gradient` as well, and it only prevents the contribution of this particular gradient of residual block to the parent layers. But as far as I understood the parameters which are marked as trainable will be always updated regardless of procedures mentioned above.\r\nJust clarify the idea a bit, let's say we have a graph `a -> b -> c`, and `a`, `b`, `c` trainable layers (tensors) are included in the list of `tf.graph.TRAINABLE_VARIABLES`. Some layers may disappear randomly from trainable list during training according to stochastic depth nets, but Tensorflow does not allow it in any case now.", "I see. I think you could probably implement this using a custom optimizer that controls the update vector and disables it using knowledge of variables and their position in layers. This may not be easy, but it may be possible.\r\n", "Closing due to inactivity. I'll reopen this issue if @awav indicates the previous suggestion was not sufficient.\r\n\r\n*Note: We have an on-call rotation for triaging issue. When filing issues, please let us take care of tagging team members for you.*", "@awav - correct me if I'm missing something, but is the goal to simply not update Variables that aren't used due to a conditional? TensorFlow already zeros out these gradients. Here's some sample code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\n\r\na = tf.Variable(10.0)\r\nb = tf.Variable(10.0)\r\nswitch = tf.placeholder(tf.bool)\r\nres = tf.cond(switch, lambda: tf.mul(2.0, a), lambda: tf.square(b))\r\nopt = tf.train.GradientDescentOptimizer(0.05)\r\ngrads = opt.compute_gradients(res)\r\ntrain = opt.apply_gradients(grads)\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print(sess.run(grads, {switch: True}))\r\n```\r\n\r\nIf you adjust the `{switch: True}` dict to be set to false, you'll see that the gradient values will flip depending on which path is taken.\r\n\r\nWhen ` {switch: True}`:\r\n\r\n```python\r\n[(2.0, 10.0), (0.0, 10.0)]\r\n```\r\n\r\nWhen ` {switch: False}`:\r\n\r\n```python\r\n[(0.0, 10.0), (20.0, 10.0)]\r\n```\r\n\r\nFor completeness, if you apply the gradients with different switches set, you only update one or the other:\r\n\r\n ` {switch: True}`:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(train, {switch: True})\r\n    print(sess.run([a, b]))\r\n\r\n>>> [9.8999996, 10.0]\r\n```\r\n\r\n` {switch: False}`:\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(train, {switch: False})\r\n    print(sess.run([a, b]))\r\n\r\n>>> [10.0, 9.0]\r\n```\r\n\r\nI think the most likely problem that might occur when trying to implement stochastic depth is that you may not see the reduced computation due to the less-lazy way `tf.cond` executes ([see the last paragraph of the documentation before the \"args\" section](https://www.tensorflow.org/api_docs/python/tf/cond)).", "The conditional statement does not seem to cut it for me. In my case, I have a model of the form data -> encoder -> intermediate result -> decoder -> result. I would like to be able to set the variables in the decoder and encoder as trainable during training by passing a boolean tensor. Is it possible to do it using 'tf.cond'? When I pass a boolean tensor as tf.get_variable(...,trainable = boolTensor) I get a TypeError.", "I have same problem. In my case, I have the model input -> features -> decode1 -> loss1\r\n                                                                                                      |-------> decode2 -> loss2.\r\nLoss1 and Loss2 are different loss function. When I minimize the loss1, I want to fix the weights in decode2. Also, when I minimize the loss2, I want to fix the weights in features and decode1. In training process, I need to iterative training two losses. I need the trainable flag to determine which part to train and which loss is needed to minimize. One of the solution online is to save and restore the weights every time. But it is not efficient at all.\r\n"]}, {"number": 8816, "title": "Thread-safe implementations of MKL kernels and rerolling changes lost in previous merge", "body": "- Changes to mkl_matmul BUILD rule so it is built as part of the matmul kernel library\r\n- Bug fixes to MKL kernels for thread-safety\r\n- Adding a TF to MKL dimension ordering map to track dimension order of MKL tensors.\r\n- Rerolling changes to MKL layout pass, mkl_util.h, nn_ops.cc and BUILD files that got lost with parallel merges \r\n", "comments": ["Can one of the admins verify this patch?", "@jbobba thanks for the PR. Could you resolve the conflicts? @rmlarsen mind taking a look?", "I moved the if_mkl into tf_cc_test_mkl. So that part of the change shouldn't be necessary. ", "@martinwicke do you have that change coming in from a different branch or should I remove the if_mkl in this PR?", "@martinwicke never mind. I checked the definition of tf_cc_test_mkl", "Can we start the tests on this?", "Jenkins, test this please.", "One more thing related to the build.", "Jenkins, test this please!", "Can we merge this in?"]}, {"number": 8815, "title": "tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform", "body": "I built tensorflow from source on Ubuntu 14.04 LTS.\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nubuntu@ip-10-0-1-152:~/tensorflow$ ls -l /usr/local/cuda\r\nlrwxrwxrwx 1 root root 19 Mar 27 16:02 /usr/local/cuda -> /usr/local/cuda-7.0\r\n\r\nIf installed from binary pip package, provide:\r\n\r\nubuntu@ip-10-0-1-152:~/tensorflow$ ls -l /home/ubuntu/cuda\r\ntotal 8\r\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Mar 21 20:34 include\r\ndrwxrwxr-x 2 ubuntu ubuntu 4096 Mar 21 20:34 lib64\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\nubuntu@ip-10-0-1-152:~/tensorflow$ git rev-parse HEAD\r\nbaa85cbf5e51a21f58bc28ef9eedc122e6118eb8\r\n2. The output of `bazel version`\r\nubuntu@ip-10-0-1-152:~/tensorflow$ bazel version\r\n...........................................................................................................\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 3522.680s, Critical Path: 2077.85s\r\nubuntu@ip-10-0-1-19:~/tensorflow$ \r\nubuntu@ip-10-0-1-19:~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \r\nWed Mar 29 17:58:57 UTC 2017 : === Using tmpdir: /tmp/tmp.fQQIv2RgvY\r\n~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/tensorflow\r\n~/tensorflow\r\n/tmp/tmp.fQQIv2RgvY ~/tensorflow\r\nWed Mar 29 17:58:58 UTC 2017 : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\n~/tensorflow\r\nWed Mar 29 17:59:29 UTC 2017 : === Output wheel file is in: /tmp/tensorflow_pkg\r\n\r\n\r\nubuntu@ip-10-0-1-19:~/tensorflow$ sudo -H pip install --upgrade /tmp/tensorflow_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl \r\ntensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform.\r\n\r\nIs this a clue?\r\n\r\nsudo -H pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl \r\nprotobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\r\n", "comments": ["```\r\nubuntu@ip-10-0-1-152:~$ python --version\r\nPython 2.7.11 :: Anaconda custom (64-bit)\r\nubuntu@ip-10-0-1-152:~$ conda --version\r\nconda 4.3.14\r\nubuntu@ip-10-0-1-152:~$ pip --version\r\npip 9.0.1 from /home/ubuntu/anaconda/lib/python2.7/site-packages (python 2.7)\r\n```", "Can you report the output of `sudo pip --version` as well? I suspect it might be different from the `pip` in your user account's `$PATH`.", "ubuntu@ip-10-0-1-152:~/tensorflow$ sudo pip --version \npip 9.0.1 from /usr/local/lib/python3.4/dist-packages/pip-9.0.1-py3.4.egg (python 3.4)\n\nUmmm.  python 3.4.  Bad!\n> On Mar 29, 2017, at 2:36 PM, Derek Murray <notifications@github.com> wrote:\n> \n> sudo pip --version \n\n", "Thanks for confirming my suspicion! I'd advise following the [virtualenv installation instructions](https://www.tensorflow.org/install/install_linux#installing_with_virtualenv) if you can, since they don't require you to use `sudo`.", "I\u2019m able to use TensorFlow, however, \u2018nvidia-smi\u2019 reports that the GPU is not being used.\nE.g. - No running processes found   \n\nubuntu@ip-10-0-1-48:~$ nvidia-smi\nThu Mar 30 05:45:26 2017       \n+------------------------------------------------------+                       \n| NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\n| N/A   35C    P0    38W / 125W |     10MiB /  4095MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nubuntu@ip-10-0-1-48:~$ \n\n1. Does TensorFlow w/GPU run on Ubuntu 14.04 LTS?  Amazon support seemed to think it only runs on 16.04.\n2. This could be a similar issue:\n\nhttps://github.com/tensorflow/tensorflow/issues/8734\n\n> On Mar 29, 2017, at 5:16 PM, Derek Murray <notifications@github.com> wrote:\n> \n> Thanks for confirming my suspicion! I'd advise following the virtualenv installation instructions <https://www.tensorflow.org/install/install_linux#installing_with_virtualenv> if you can, since they don't require you to use sudo.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/8815#issuecomment-290264317>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AC9i24zlIEC9CBGF7r44xBsn0i5-3Iyhks5rqvRGgaJpZM4MtYdP>.\n> \n\n", "btw - ./configure never asked me if I wanted to use the GPU.  It only asked about openCL and CUDA:\n\nubuntu@ip-10-0-1-48:~/tensorflow$ ./configure\nPlease specify the location of python. [Default is /home/ubuntu/anaconda/bin/python]: \nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \nDo you wish to use jemalloc as the malloc implementation? [Y/n] y\njemalloc enabled\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] y\nHadoop File System support will be enabled for TensorFlow\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\nXLA JIT support will be enabled for TensorFlow\nFound possible Python library paths:\n  /home/ubuntu/anaconda/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/home/ubuntu/anaconda/lib/python2.7/site-packages]\n\nUsing python library path: /home/ubuntu/anaconda/lib/python2.7/site-packages\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\nNo OpenCL support will be enabled for TensorFlow\nDo you wish to build TensorFlow with CUDA support? [y/N] y\nCUDA support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/ubuntu/cuda\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n...........\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n\nI was under the impression that I would see this output (https://alliseesolutions.wordpress.com/2016/09/08/install-gpu-tensorflow-from-sources-w-ubuntu-16-04-and-cuda-8-0-rc/):\n\nSetting up Cuda include\nSetting up Cuda lib\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n\n\n\n> On Mar 30, 2017, at 8:16 AM, David Laxer <davidl@softintel.com> wrote:\n> \n> I\u2019m able to use TensorFlow, however, \u2018nvidia-smi\u2019 reports that the GPU is not being used.\n> E.g. - No running processes found   \n> \n> ubuntu@ip-10-0-1-48:~$ nvidia-smi\n> Thu Mar 30 05:45:26 2017       \n> +------------------------------------------------------+                       \n> | NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \n> |-------------------------------+----------------------+----------------------+\n> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n> |===============================+======================+======================|\n> |   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\n> | N/A   35C    P0    38W / 125W |     10MiB /  4095MiB |      0%      Default |\n> +-------------------------------+----------------------+----------------------+\n>                                                                                \n> +-----------------------------------------------------------------------------+\n> | Processes:                                                       GPU Memory |\n> |  GPU       PID  Type  Process name                               Usage      |\n> |=============================================================================|\n> |  No running processes found                                                 |\n> +-----------------------------------------------------------------------------+\n> ubuntu@ip-10-0-1-48:~$ \n> \n> 1. Does TensorFlow w/GPU run on Ubuntu 14.04 LTS?  Amazon support seemed to think it only runs on 16.04.\n> 2. This could be a similar issue:\n> \n> https://github.com/tensorflow/tensorflow/issues/8734 <https://github.com/tensorflow/tensorflow/issues/8734>\n> \n>> On Mar 29, 2017, at 5:16 PM, Derek Murray <notifications@github.com <mailto:notifications@github.com>> wrote:\n>> \n>> Thanks for confirming my suspicion! I'd advise following the virtualenv installation instructions <https://www.tensorflow.org/install/install_linux#installing_with_virtualenv> if you can, since they don't require you to use sudo.\n>> \n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/8815#issuecomment-290264317>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AC9i24zlIEC9CBGF7r44xBsn0i5-3Iyhks5rqvRGgaJpZM4MtYdP>.\n>> \n> \n\n", "Hi,\r\nI am facing similar issue my pip is 2.7 \r\n```\r\n$ sudo pip --version\r\npip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)\r\n```", "Got it fixed when\r\nI downloaded get-pip.py from https://bootstrap.pypa.io/get-pip.py\r\nand then ran `python2.7 get-pip.py` for installing pip2.7"]}]