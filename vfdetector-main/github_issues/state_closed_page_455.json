[{"number": 40180, "title": "TFLite cross-compiling arm64 build error?", "body": "As I am currently trying to Tensorflow lite for ARM64 architecture, I just try to follow the instructions from below: https://www.tensorflow.org/lite/guide/build_arm64#cross-compile_for_arm64\r\n\r\nBut I simply get a compilation error: tensorflow/lite/tools/make/downloads/ruy/ruy/cpuinfo.cc:9:21: fatal error: cpuinfo.h: No such file or directory\r\n\r\nI am surprised this starting build not working out of the box.\r\n\r\nBtw, I am trying to do the above in Ubuntu 16.04 VM.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:From Git clone\r\n-   **TensorFlow version (use command below)**:latest\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Solved now. You need to explicity install Bazel for this. The tensorflow-lite dependencies script does not do this. Not sure why, should have been part of this."]}, {"number": 40179, "title": "TF-TRT test FusedBatchNorm op converter", "body": "Add test for the FusedBatchNorm op converter in implicit batch, explicit batch and dynamic shape mode. Tagging @bixia1 for review.", "comments": ["@bixia1 Can you please review this PR ? Thanks!", "@tfeher Can you please check @bixia1's comments and keep us posted. Thanks!", "@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "Thanks @bixia1, I have made the requested changes."]}, {"number": 40178, "title": "Always exceeds 10% of free system memory.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu 16.04 LTS\r\n- TensorFlow installed from (source or binary):\r\nconda 4.8.3\r\n- TensorFlow version (use command below):\r\ntensorflow-gpu v2.2.0\r\n- Python version:\r\npython 3.7.7\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n- CUDA/cuDNN version:\r\nCUDA Version 10.1.168\r\n- GPU model and memory:\r\n2 * GeForce RTX 2080 Ti 11GB\r\n\r\n**Describe the current behavior**\r\nI can use the same input data and run the same model correctly on the tensorflow-gpu-v1.14, but it doesn't work when I use the tensorflow-gpu-v2.0 or tensorflow-gpu-v2.2.\r\n\r\nAfter excute the fit function, it will raise an error:\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]\r\n\r\nAlso, when I change the enviroment to tensorflow-cpu-v2.0 or v2.2, it will work correctly but using CPU, that will be very slow.\r\n\r\n**Describe the expected behavior**\r\nIt should run as well as when I used tensorflow-gpu-v1.14.\r\nI'm sure the memory of GPU and CPU is enough. I tried many ways to solve it but not worked:\r\n- `\r\ngpus = tf.config.experimental.list_physical_devices('GPU')`\r\n`tf.config.experimental.set_memory_growth(gpus[0], True)`\r\n`tf.config.experimental.set_memory_growth(gpus[1], True)\r\n`\r\n- `os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'`\r\nit will just hide the error message and not help.\r\n- decrease the batch_size for fit, but it still invalid.\r\n- reboot my computer, still not work.\r\n- remove the enviroment and install again, still not work.\r\n\r\nIs there anything wrong?\r\n\r\n**Standalone code to reproduce the issue**\r\n- x_train.shape: (1254521, 56, 40)\r\n- y_train.shape: (1254521, 2)\r\n\r\nMy simple train function:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef train(file_path):\r\n    print('Loading data ...')\r\n    x_train = np.float32(np.random.randint(-8, 8, size=[1254521, 56, 40]))\r\n    y_train = [[1, 0] for _ in range(x_train.shape[0])]\r\n    y_train = np.float32(y_train)\r\n\r\n    print(x_train.shape, y_train.shape)\r\n\r\n    print('Create model ...')\r\n    data = tf.keras.Input(shape=[56, 40])\r\n    x = tf.keras.layers.Reshape([56, 40, 1])(data)\r\n\r\n    cnn_out = tf.keras.layers.Conv2D(8, (20, 5), strides=(1, 2))(x)\r\n    cnn_output = tf.keras.layers.Reshape([37, 18 * 8])(cnn_out)\r\n\r\n    gru_out = tf.keras.layers.GRU(20)(cnn_output)\r\n\r\n    outputs = tf.keras.layers.Dense(2)(gru_out)\r\n\r\n    my_model = tf.keras.Model(inputs=data, outputs=outputs)\r\n\r\n    my_model.compile()\r\n\r\n    print(my_model.summary())\r\n\r\n    my_model.fit(x_train, y_train, epochs=100, batch_size=1024, validation_split=0.1)\r\n    my_model.save(file_path)\r\n\r\n\r\nif __name__ == '__main__':\r\n    train(\"test.h5\")\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nFull traceback of error:\r\n\r\n```\r\nLoading data ...\r\n(1254521, 56, 40) (1254521, 2)\r\nCreate model ...\r\n2020-06-05 16:26:08.953877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-05 16:26:08.997184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:08.997775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-05 16:26:08.997841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:08.998395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-05 16:26:08.998559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-05 16:26:08.999723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-05 16:26:09.000912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-05 16:26:09.001118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-05 16:26:09.002557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-05 16:26:09.003395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-05 16:26:09.006865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-05 16:26:09.007010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.007707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.008324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.008891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.009430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-05 16:26:09.009771: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-06-05 16:26:09.042664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3192000000 Hz\r\n2020-06-05 16:26:09.043165: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615ceb22100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-05 16:26:09.043193: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-05 16:26:09.182570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.182998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-05 16:26:09.183062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.183455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.635GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-05 16:26:09.183493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-05 16:26:09.183504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-05 16:26:09.183513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-05 16:26:09.183523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-05 16:26:09.183532: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-05 16:26:09.183541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-05 16:26:09.183551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-05 16:26:09.183586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.184053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.184672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.185309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.185863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-05 16:26:09.185958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-05 16:26:09.187361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-05 16:26:09.187379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \r\n2020-06-05 16:26:09.187385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N N \r\n2020-06-05 16:26:09.187389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   N N \r\n2020-06-05 16:26:09.187506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.187922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.188338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.188891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.189454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9860 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-06-05 16:26:09.189759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.190357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-05 16:26:09.190762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10203 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\n2020-06-05 16:26:09.192135: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615cfe5f110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-05 16:26:09.192150: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-06-05 16:26:09.192155: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 56, 40)]          0         \r\n_________________________________________________________________\r\nreshape (Reshape)            (None, 56, 40, 1)         0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 37, 18, 8)         808       \r\n_________________________________________________________________\r\nreshape_1 (Reshape)          (None, 37, 144)           0         \r\n_________________________________________________________________\r\ngru (GRU)                    (None, 20)                9960      \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2)                 42        \r\n=================================================================\r\nTotal params: 10,810\r\nTrainable params: 10,810\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n2020-06-05 16:26:10.028753: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 11240508160 exceeds 10% of free system memory.\r\n2020-06-05 16:26:24.748619: W tensorflow/core/common_runtime/bfc_allocator.cc:434] Allocator (GPU_0_bfc) ran out of memory trying to allocate 10.47GiB (rounded to 11240508160)\r\nCurrent allocation summary follows.\r\n2020-06-05 16:26:24.748646: I tensorflow/core/common_runtime/bfc_allocator.cc:934] BFCAllocator dump for GPU_0_bfc\r\n2020-06-05 16:26:24.748653: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (256):   Total Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 248B client-requested in use in bin.\r\n2020-06-05 16:26:24.748658: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (512):   Total Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 480B client-requested in use in bin.\r\n2020-06-05 16:26:24.748663: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2020-06-05 16:26:24.748668: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (2048):  Total Chunks: 2, Chunks in use: 1. 6.2KiB allocated for chunks. 3.2KiB in use in bin. 3.1KiB client-requested in use in bin.\r\n2020-06-05 16:26:24.748675: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (4096):  Total Chunks: 2, Chunks in use: 1. 9.5KiB allocated for chunks. 4.8KiB in use in bin. 4.7KiB client-requested in use in bin.\r\n2020-06-05 16:26:24.748680: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748685: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748691: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (32768):         Total Chunks: 2, Chunks in use: 1. 91.8KiB allocated for chunks. 33.8KiB in use in bin. 33.8KiB client-requested in use in bin.\r\n2020-06-05 16:26:24.748695: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748700: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (131072):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748705: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748710: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748716: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (1048576):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748721: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748726: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748731: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748736: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748741: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748746: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748751: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748756: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (268435456):     Total Chunks: 1, Chunks in use: 0. 9.63GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-06-05 16:26:24.748761: I tensorflow/core/common_runtime/bfc_allocator.cc:957] Bin for 10.47GiB was 256.00MiB, Chunk State: \r\n2020-06-05 16:26:24.748768: I tensorflow/core/common_runtime/bfc_allocator.cc:963]   Size: 9.63GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 33.8KiB | Requested Size: 33.8KiB | in_use: 1 | bin_num: -1\r\n2020-06-05 16:26:24.748773: I tensorflow/core/common_runtime/bfc_allocator.cc:970] Next region of size 10339342080\r\n2020-06-05 16:26:24.748780: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000000 of size 1280 next 1\r\n2020-06-05 16:26:24.748784: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000500 of size 256 next 5\r\n2020-06-05 16:26:24.748789: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000600 of size 256 next 8\r\n2020-06-05 16:26:24.748793: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000700 of size 256 next 10\r\n2020-06-05 16:26:24.748797: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000800 of size 256 next 11\r\n2020-06-05 16:26:24.748802: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000900 of size 256 next 14\r\n2020-06-05 16:26:24.748806: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000a00 of size 256 next 16\r\n2020-06-05 16:26:24.748811: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000b00 of size 512 next 17\r\n2020-06-05 16:26:24.748815: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000d00 of size 256 next 20\r\n2020-06-05 16:26:24.748820: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000e00 of size 256 next 21\r\n2020-06-05 16:26:24.748824: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e000f00 of size 256 next 22\r\n2020-06-05 16:26:24.748828: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e001000 of size 256 next 23\r\n2020-06-05 16:26:24.748833: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e001100 of size 256 next 2\r\n2020-06-05 16:26:24.748837: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e001200 of size 256 next 3\r\n2020-06-05 16:26:24.748841: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e001300 of size 256 next 4\r\n2020-06-05 16:26:24.748845: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e001400 of size 256 next 9\r\n2020-06-05 16:26:24.748850: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e001500 of size 256 next 18\r\n2020-06-05 16:26:24.748854: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 7f2a7e001600 of size 3072 next 6\r\n2020-06-05 16:26:24.748858: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e002200 of size 3328 next 7\r\n2020-06-05 16:26:24.748863: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 7f2a7e002f00 of size 4864 next 19\r\n2020-06-05 16:26:24.748867: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e004200 of size 4864 next 15\r\n2020-06-05 16:26:24.748872: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 7f2a7e005500 of size 59392 next 13\r\n2020-06-05 16:26:24.748876: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 7f2a7e013d00 of size 34560 next 12\r\n2020-06-05 16:26:24.748881: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 7f2a7e01c400 of size 10339226368 next 18446744073709551615\r\n2020-06-05 16:26:24.748885: I tensorflow/core/common_runtime/bfc_allocator.cc:995]      Summary of in-use Chunks by size: \r\n2020-06-05 16:26:24.748891: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 15 Chunks of size 256 totalling 3.8KiB\r\n2020-06-05 16:26:24.748895: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 512 totalling 512B\r\n2020-06-05 16:26:24.748900: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 1280 totalling 1.2KiB\r\n2020-06-05 16:26:24.748905: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 3328 totalling 3.2KiB\r\n2020-06-05 16:26:24.748909: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 4864 totalling 4.8KiB\r\n2020-06-05 16:26:24.748914: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 34560 totalling 33.8KiB\r\n2020-06-05 16:26:24.748918: I tensorflow/core/common_runtime/bfc_allocator.cc:1002] Sum Total of in-use chunks: 47.2KiB\r\n2020-06-05 16:26:24.748923: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] total_region_allocated_bytes_: 10339342080 memory_limit_: 10339342080 available bytes: 0 curr_region_allocation_bytes_: 20678684160\r\n2020-06-05 16:26:24.748929: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] Stats: \r\nLimit:                 10339342080\r\nInUse:                       48384\r\nMaxInUse:                   109824\r\nNumAllocs:                      56\r\nMaxAllocSize:                34560\r\n\r\n2020-06-05 16:26:24.748939: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *___________________________________________________________________________________________________\r\nTraceback (most recent call last):\r\n  File \"question.py\", line 35, in <module>\r\n    train(\"test.h5\")\r\n  File \"question.py\", line 30, in train\r\n    my_model.fit(x_train, y_train, epochs=100, batch_size=1024, validation_split=0.1)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 797, in fit\r\n    shuffle=False))\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1338, in train_validation_split\r\n    functools.partial(_split, indices=train_indices), arrays)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 617, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/nest.py\", line 617, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1335, in _split\r\n    return array_ops.gather_v2(t, indices)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 4541, in gather_v2\r\n    batch_dims=batch_dims)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 4524, in gather\r\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3755, in gather_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/fzy/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]\r\n\r\n\r\n```", "comments": ["@hdu-fzy \r\n\r\nCan you Please modify the code on colab and share the gist. Also, request you provide supporting files to eproduce the issue in our environment.It helps us in localizing the issue faster.Thanks! ", "I think the problems are consistent with [this](https://github.com/tensorflow/tensorflow/issues/39574).", "@fzyai  Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40178\">No</a>\n", "Does anyone has the solution to this issue? I'm facing this issue with the configuration below:\r\nOS: ubuntu20.04.03\r\nTF version: 2.4.0"]}, {"number": 40177, "title": "[tf.data] Return unknown cardinality when preserve_cardinality is false", "body": "Fix bug mentioned in #40107 .\r\n\r\ncc @aaudiber \r\n\r\nThank you for your time on reviewing this pr.", "comments": ["@aaudiber \r\nThank you for your quick review! As for the test, could you tell me is there a way to separate the test for tf1 and tf2? Because the unknown cardinality will only appear in tf1. ", "Sorry, I linked to the wrong test file - it's better to update https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/kernel_tests/cardinality_test.py since `cardinality` has graduated out of `/experimental`.\r\n\r\nYou can write something like the below to test v1-only functionality:\r\n\r\n```python\r\n\r\n  def _v1_only_test_combinations():\r\n     ...  # similar to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/kernel_tests/cardinality_test.py#L30\r\n\r\n  @combinations.generate(\r\n      combinations.times(combinations.combine(tf_api_version=1, mode=[\"eager\", \"graph\"]),\r\n              _v1_only_test_combinations())\r\n  def testCardinalityV1Only(self, dataset_fn, expected_result):\r\n    dataset = dataset_fn()\r\n    self.assertEqual(self.evaluate(dataset.cardinality()), expected_result)\r\n```", "@aaudiber \r\nThank you for teaching me how to add test! Could you take another look?", "@aaudiber \r\nThe build failure on ubuntu is due to some test for cardinality in c++. I will soon push updates to them.", "@aaudiber \r\nThe newly updated tests should be able to fix the building failure. Could you have another look?"]}, {"number": 40176, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[1] does not have value with aarch64", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS Linux release 7.6.1810 (AltArch)  4.14.0-115.el7a.0.1.aarch64\r\n- TensorFlow installed from (source or binary):\r\ncompiled from source \r\n- TensorFlow version (use command below):\r\n1.13.1\r\n- Python version:\r\n3.7.6\r\n- Bazel version (if compiling from source):\r\n0.20.0\r\n- GCC/Compiler version (if compiling from source):\r\n5.5.0\r\n- CUDA/cuDNN version:\r\nno\r\n- GPU model and memory:\r\nno\r\n\r\nI use multilayers' lstm to generate texts. The network :\r\ndef buildModel(wordNum, gtX, hidden_units=128, layers=2, embedding=True):  \r\n    \"\"\"build rnn\"\"\"\r\n    with tf.variable_scope(\"embedding\"):  # embedding\r\n        if embedding is True:\r\n            embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\r\n            inputbatch = tf.nn.embedding_lookup(embedding, gtX)\r\n\r\n        else:\r\n            inputbatch = tf.one_hot(gtX, wordNum)\r\n        print('shape: ', inputbatch.shape)\r\n    inputbatch = tf.layers.batch_normalization(inputbatch, 0)\r\n    basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units, state_is_tuple = True)\r\n    droped_cell = tf.contrib.rnn.DropoutWrapper(basicCell, output_keep_prob=0.5)\r\n    stack_cells = []\r\n    for _ in range(layers):\r\n        stack_cells.append(droped_cell)\r\n#     stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\r\n    stackCell = tf.contrib.rnn.MultiRNNCell(stack_cells)\r\n    initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\r\n    outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\r\n    # outputs = tf.concat(outputs, 1)\r\n    outputs = tf.reshape(outputs, [-1, hidden_units])\r\n    print('outpus:', outputs.shape)\r\n    with tf.variable_scope(\"softmax\"):\r\n        w = tf.get_variable(\"w\", initializer=tf.truncated_normal([hidden_units, wordNum], stddev=0.1))\r\n        b = tf.get_variable(\"b\", initializer=tf.zeros(wordNum))\r\n        logits = tf.matmul(outputs, w) + b\r\n\r\n    probs = tf.nn.softmax(logits)\r\n    return logits, probs, stackCell, initState, finalState\r\nAnd the loss function is tf.contrib.legacy_seq2seq.sequence_loss_by_example.\r\nThe following error appears  occasionally, sometimes in one epoch, sometimes in a few epochs. I am very confused about it. What is going on? I have  tried to add dropout , BN and fix the length of input, etc. But they are useless. Please help, tks a lot!\r\n All error log:\r\nTraceback (most recent call last):\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[1] does not have value\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/ddhome/src/lstm_debug/HCIA-IP/exp-lstm.py\", line 224, in <module>\r\n    train()\r\n  File \"/ddhome/src/lstm_debug/HCIA-IP/exp-lstm.py\", line 203, in train\r\n    a, loss, gStep = sess.run([trainOP, cost, addGlobalStep], feed_dict={gtX: x, gtY: y})\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/ddhome/.pyenv/versions/anaconda3-5.3.1/envs/py3.7.6-tf1.13.1/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[1] does not have value\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@jxfruit,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nAlso, is there any specific reason you are using TF v1.13? Please update TensorFlow to 2.x and let us know if you are facing the same issue. Thanks!", "@amahendrakar \r\nsorry, I cannot update tensorflow from v1.13 to 2.x in my project. All codes are following:\r\n\r\n```python\r\nimport os\r\nimport collections\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pickle\r\n\r\nin_dir = \"./exp-14\"\r\n\r\ncur_pth = os.getcwd()\r\n# root_pth = '/'.join(cur_pth.split('/')[:-1])\r\n# out_dir = os.path.join(root_pth, 'exp-14')\r\nout_dir = \"./exp-14\"\r\nif not os.path.exists(out_dir):\r\n    os.makedirs(out_dir)\r\n    print(out_dir)\r\nelse:\r\n    print(\"The directionary exists\")\r\n\r\npoem_file = os.path.join(in_dir, 'poem.txt')\r\nprint(in_dir, out_dir)\r\n\r\npoems = []\r\nwith open(poem_file, \"r\", encoding='UTF-8') as f:\r\n    for line in f:\r\n        try:\r\n            # line = line.decode('UTF-8')\r\n            line = line.strip(u'\\n')\r\n            title, content = line.strip(u' ').split(u':')\r\n            content = content.replace(u' ', u'')\r\n            if u'_' in content or u'(' in content or u'\uff08' in content or u'\u300a' in content or u'[' in content:\r\n                continue\r\n            if len(content) < 5 or len(content) > 79:\r\n                continue\r\n            content = u'[' + content + u']'\r\n            poems.append(content)\r\n        except Exception as e:\r\n            pass\r\n\r\npoems = sorted(poems, key=lambda line: len(line))\r\nprint('total poems: ', len(poems))\r\nprint(poems[:100])\r\n\r\nall_words = []\r\nfor poem in poems:\r\n    all_words += [word for word in poem]\r\ncounter = collections.Counter(all_words)\r\n\r\ncount_pairs = sorted(counter.items(), key=lambda x: -x[1])\r\n\r\nwords, _ = zip(*count_pairs)\r\nwords = words[:len(words)] + (' ',)\r\nprint(words[:20])\r\n\r\nword_num_map = dict(zip(words, range(len(words))))\r\nto_num = lambda word: word_num_map.get(word, len(words))\r\npoems_vector = [list(map(to_num, poem)) for poem in poems]\r\nprint(poems[:10])\r\nprint(poems_vector[:10])\r\n\r\nw2v_file = os.path.join(out_dir, 'w2v.data')\r\nwith open(w2v_file, \"wb\") as fw:\r\n    pickle.dump([words, word_num_map, poems_vector], fw)\r\n\r\nbatch_size = 64\r\nn_chunk = len(poems_vector) // batch_size\r\nprint(n_chunk)\r\n\r\n\r\nclass DataSet(object):\r\n    def __init__(self, data_size):\r\n        self._data_size = data_size\r\n        self._epochs_completed = 0\r\n        self._index_in_epoch = 0\r\n        self._data_index = np.arange(data_size)\r\n\r\n    def next_batch(self, batch_size):\r\n\r\n        start = self._index_in_epoch\r\n\r\n        if start + batch_size > self._data_size:\r\n            np.random.shuffle(self._data_index)\r\n            self._epochs_completed = self._epochs_completed + 1\r\n            self._index_in_epoch = batch_size\r\n            full_batch_features, full_batch_labels = self.data_batch(0, batch_size)\r\n            return full_batch_features, full_batch_labels\r\n\r\n        else:\r\n            self._index_in_epoch += batch_size\r\n            end = self._index_in_epoch\r\n\r\n            full_batch_features, full_batch_labels = self.data_batch(start, end)\r\n\r\n            return full_batch_features, full_batch_labels\r\n\r\n    def data_batch(self, start, end):\r\n        batches = []\r\n        for i in range(start, end):\r\n            batches.append(poems_vector[self._data_index[i]])\r\n\r\n        length = max(map(len, poems_vector))\r\n        xdata = np.full((end - start, length), word_num_map[' '], np.int32)\r\n\r\n        for row in range(end - start):\r\n            xdata[row, :len(batches[row])] = batches[row]\r\n\r\n        ydata = np.copy(xdata)\r\n        ydata[:, :-1] = xdata[:, 1:]\r\n        return xdata, ydata\r\n\r\n\r\ndef neural_network(batch_size=32, model='lstm', rnn_size=128, num_layers=2, is_training=True):\r\n    tf.reset_default_graph()\r\n    input_data = tf.placeholder(tf.int32, [batch_size, None])\r\n    if is_training:\r\n        output_targets = tf.placeholder(tf.int32, [batch_size, None])\r\n    if model == 'rnn':\r\n        cell_fun = tf.contrib.rnn.BasicRNNCell\r\n    elif model == 'gru':\r\n        cell_fun = tf.contrib.rnn.GRUCell\r\n    elif model == 'lstm':\r\n        cell_fun = tf.contrib.rnn.BasicLSTMCell\r\n\r\n    basicCell = cell_fun(rnn_size, state_is_tuple=True)\r\n    droped_cell = tf.contrib.rnn.DropoutWrapper(basicCell, output_keep_prob=0.5)\r\n    cells = []\r\n    for _ in range(num_layers):\r\n        cells.append(droped_cell)\r\n    cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\r\n    # cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\r\n\r\n    initial_state = cell.zero_state(batch_size, tf.float32)\r\n\r\n    with tf.variable_scope('rnnlm'):\r\n\r\n        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, len(words)])\r\n        softmax_b = tf.get_variable(\"softmax_b\", [len(words)])\r\n\r\n        with tf.device(\"/cpu:0\"):\r\n            embedding = tf.get_variable(\"embedding\", [len(words), rnn_size])\r\n\r\n            inputs = tf.nn.embedding_lookup(embedding, input_data)\r\n\r\n    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, scope='rnnlm')\r\n\r\n    output = tf.reshape(outputs, [-1, rnn_size])\r\n    logits = tf.matmul(output, softmax_w) + softmax_b\r\n    probs = tf.nn.softmax(logits)\r\n\r\n    if is_training:\r\n        return logits, last_state, probs, cell, initial_state, input_data, output_targets\r\n    else:\r\n        return logits, last_state, probs, cell, initial_state, input_data\r\n\r\n\r\ndef load_model(sess, saver, ckpt_path):\r\n    latest_ckpt = tf.train.latest_checkpoint(ckpt_path)\r\n    if latest_ckpt:\r\n        print('resume from', latest_ckpt)\r\n        saver.restore(sess, latest_ckpt)\r\n        return int(latest_ckpt[latest_ckpt.rindex('-') + 1:])\r\n    else:\r\n        print('building model from scratch')\r\n        sess.run(tf.global_variables_initializer())\r\n        return -1\r\n\r\n\r\nmodel_read_dir = os.path.join(in_dir, 'model')\r\nmodel_read_path = os.path.join(model_read_dir, 'poem.module')\r\nmodel_save_dir = os.path.join(out_dir, 'model')\r\nmodel_save_path = os.path.join(model_save_dir, 'poem.module')\r\n\r\n\r\ndef train_neural_network():\r\n    logits, last_state, _, _, _, input_data, output_targets = neural_network(batch_size)\r\n\r\n    targets = tf.reshape(output_targets, [-1])\r\n\r\n    with tf.name_scope('sequence_loss'):\r\n        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\r\n                                                                  [tf.ones_like(targets, dtype=tf.float32)], len(words))\r\n        cost = tf.reduce_mean(loss)\r\n        tf.summary.scalar(\"loss_value\", cost)\r\n\r\n    learning_rate = tf.Variable(0.0, trainable=False)\r\n\r\n    tvars = tf.trainable_variables()\r\n\r\n    #     tf.clip_by_global_norm(\r\n    #     t_list,\r\n    #     clip_norm,\r\n    #     use_norm=None,\r\n    #     name=None\r\n    #     )\r\n    # t_list[i] * clip_norm / max(global_norm, clip_norm)\r\n    # global_norm = sqrt(sum([t**2 for t in t_list]))\r\n\r\n    grad_vars = tf.gradients(cost, tvars)\r\n    grads, gn = tf.clip_by_global_norm(grad_vars, 1, name='gradients_clip')\r\n\r\n    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1e-4)\r\n\r\n    # train_op = optimizer.minimize(cost)\r\n    train_op = optimizer.apply_gradients(zip(grads, tvars))\r\n\r\n    Session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\r\n\r\n    Session_config.gpu_options.allow_growth = True\r\n\r\n    trainds = DataSet(len(poems_vector))\r\n\r\n    summary_merge_op = tf.summary.merge_all()\r\n    with tf.Session(config=Session_config) as sess:\r\n        with tf.device('/gpu:0'):\r\n            sess.run(tf.global_variables_initializer())\r\n\r\n            saver = tf.train.Saver(tf.all_variables())\r\n\r\n            # last_epoch = load_model(sess, saver, model_read_dir)\r\n            last_epoch = 0\r\n            writer = tf.summary.FileWriter('logs/', sess.graph)\r\n            for epoch in range(last_epoch + 1, 30):\r\n\r\n                sess.run(tf.assign(learning_rate, 0.002 * (0.3 ** epoch)))\r\n                # sess.run(tf.assign(learning_rate, 0.01))\r\n\r\n                all_loss = 0.0\r\n\r\n                for batche in range(n_chunk):\r\n\r\n                    x, y = trainds.next_batch(batch_size)\r\n\r\n                    # sess.run(train_op, feed_dict={input_data: x, output_targets: y})\r\n                    train_loss, _, _, summary_info = sess.run([cost, last_state, train_op, summary_merge_op],\r\n                                                              feed_dict={input_data: x, output_targets: y})\r\n\r\n                    writer.add_summary(summary_info, epoch)\r\n\r\n                    grad = sess.run(grads, feed_dict={input_data: x, output_targets: y})\r\n                    all_loss = all_loss + train_loss\r\n                    # print(grad)\r\n                    # print('epoch:', epoch, 'batch:', batche, 'lr:', 0.002 * (0.97 ** epoch), 'tl:', train_loss)\r\n                    if batche % 50 == 1:\r\n                        #     #print(epoch, batche, 0.01,train_loss)\r\n                        print('epoch:', epoch, 'batch:', batche, 'lr:', 0.002 * (0.3 ** epoch), 'tl:', train_loss)\r\n\r\n                saver.save(sess, model_save_path, global_step=epoch)\r\n\r\n                print('epoch:', epoch, ' Loss: ', all_loss * 1.0 / n_chunk)\r\n\r\n\r\ntrain_neural_network() \r\n```\r\nThe demo data are following:\r\n\"\"\"\r\n\u8d8a\u4e2d\u72c2\u751f\u9898\u65d7\u4ead:\u65e5\u65e5\u8349\u91cd\u751f\uff0c\u60a0\u60a0\u508d\u7d20\u57ce\u3002\u8bf8\u4faf\u9010\u5154\u767d\uff0c\u590f\u6ee1\u955c\u6e56\u5e73\u3002\r\n\u6e05\u6cf0\u4e09\u5e74\u6b4c:\u4e19\u7533\u5e74\uff0c\u6570\u5728\u4e94\u697c\u524d\u3002\u4f46\u770b\u516b\u4e5d\u6708\uff0c\u80e1\u864f\u4e71\u4e2d\u539f\u3002\r\n\u8700\u738b\u6c0f\u8c36\u6587:\u674e\u795c\u897f\u738b\u9022\u5409\u660c\uff0c\u4e0a\u5fb7\u5151\u5174\u4e39\u83ab\u5f53\u3002\r\n\u9ec4\u4e07\u795c\u9898\u8700\u5bab\u58c1:\u83ab\u4ea4\u7275\u52a8\u9752\u732a\u8db3\uff0c\u52a8\u5373\u708e\u708e\u4e0d\u53ef\u6251\u3002\u9e37\u517d\u4e0d\u6b32\u4e24\u5934\u9ec4\uff0c\u9ec4\u5373\u5176\u5e74\u5929\u4e0b\u54ed\u3002\r\n\u5b5f\u8700\u4e10\u8005\u8bed:\u4e0d\u5f97\u767b\uff0c\u767b\u4fbf\u5012\u3002\r\n\u5b5f\u8700\u6843\u7b26\u8bd7:\u65b0\u5e74\u7eb3\u4f59\u5e86\uff0c\u5609\u8282\u53f7\u957f\u6625\u3002\r\n\u4e0a\u84dd\u548c\u5c1a\u664b\u6c49\u4e8c\u4ee3\u8c36:\u77f3\u69b4\u82b1\u53d1\u77f3\u69b4\u5f00\u3002\r\n\u53c8\u9057\u949f\u4f20\u5048:\u4f46\u770b\u6765\u5e74\u4e8c\u4e09\u6708\uff0c\u67f3\u6761\u582a\u4f5c\u6253\u949f\u69cc\u3002\r\n\u53c8\u62a5\u738b\u5ba1\u77e5\u5341\u5b57\u8c36:\u4e0d\u6015\u7f8a\u5165\u5c4b\uff0c\u53ea\u6015\u94b1\u5165\u8179\u3002\r\n\u94b1\u5904\u58eb\u674e\u6c0f\u8c36:\u4eff\u4f5b\u4e4b\u95f4\u4e00\u500d\u6768\u3002\r\n\u5b59\u54b8\u9898\u5e90\u5c71\u795e\u5e99\u8bd7:\u72ec\u5165\u7384\u5bab\u793c\u81f3\u771f\uff0c\u711a\u9999\u4e0d\u4e3a\u8d31\u8d2b\u8eab\u3002\u79e6\u6dee\u4e24\u5cb8\u6c99\u57cb\u9aa8\uff0c\u6e53\u6d66\u5343\u5bb6\u8840\u67d3\u5c18\u3002\u5e90\u961c\u70df\u971e\u8c01\u662f\u4e3b\uff0c\u864e\u6eaa\u98ce\u6708\u5c5e\u4f55\u4eba\u3002\u4e5d\u6c5f\u592a\u5b88\u52e4\u738b\u5ba4\uff0c\u597d\u653e\u5929\u5175\u6e21\u8981\u6d25\u3002\r\n\u5357\u5510\u6c5f\u5dde\u98ce\u5760\u8bd7:\u7531\u6765\u79c9\u8282\u4e16\u65e0\u53cc\uff0c\u72ec\u5b88\u5b64\u57ce\u6b7b\u4e0d\u964d\u3002\u4f55\u4f3c\u77e5\u673a\u65e9\u56de\u9996\uff0c\u514d\u6559\u6d41\u8840\u6ee1\u957f\u6c5f\u3002\r\n\u676d\u5dde\u8fd8\u4e61\u548c\u5c1a\u5531:\u8fd8\u4e61\u5bc2\u5bc2\u6773\u65e0\u8e2a\uff0c\u4e0d\u6302\u5f81\u5e06\u6c34\u9646\u901a\u3002\u8e4b\u5f97\u6545\u4e61\u56de\u5730\u7a33\uff0c\u66f4\u65e0\u5357\u5317\u4e0e\u897f\u4e1c\u3002\r\n\u798f\u5dde\u8bb0:\u6f6e\u6c34\u6765\uff0c\u5ca9\u5934\u6ca1\u3002\u6f6e\u6c34\u53bb\uff0c\u77e2\u53e3\u51fa\u3002\r\n\u9ec4\u6d85\u69c3\u8c36:\u5148\u6253\u5357\uff0c\u540e\u6253\u5317\uff0c\u7559\u53d6\u6e05\u6e90\u4f5c\u4f5b\u56fd\u3002\r\n\u9648\u667a\u5e7f\u8c36:\u9a91\u9a6c\u6765\uff0c\u9a91\u9a6c\u53bb\u3002\r\n\u53c8\u8c36:\u529f\u4e0b\u7530\uff0c\u529b\u4ea4\u8fde\u3002\u4e95\u5e95\u5750\uff0c\u4e8c\u5341\u5e74\u3002\r\n\u50e7\u7f04\u793a\u738b\u5904\u539a:\u5468\u58eb\u540c\u6210\uff0c\u4e8c\u738b\u6b8a\u540d\u3002\u738b\u5c45\u4e00\u7109\uff0c\u767e\u65e5\u4e3a\u7a0b\u3002\r\n\u4efb\u53df\u4e66\u6388\u5218\u751f:\u627f\u6b32\u5f80\u6881\u5b8b\uff0c\u6881\u5b8b\u707e\u65b9\u91cd\uff0c\u65e6\u5915\u4e3a\u4eba\u8bbc\u3002\u627f\u6b32\u8bbf\u90d1\u751f\uff0c\u90d1\u751f\u5c06\u6709\u5384\u3002\u5373\u4e3a\u5343\u91cc\u5ba2\uff0c\u517c\u4ea6\u53d8\u886b\u8272\u3002\r\n\u660c\u660e\u91cc\u4e2d\u8c36:\u6b32\u77e5\u4fee\u7eed\u8005\uff0c\u811a\u4e0b\u662f\u751f\u6bdb\u3002\r\n\u5b8b\u5584\u5a01\u8bd7:\u6708\u843d\u4e09\u682a\u6811\uff0c\u65e5\u6620\u4e5d\u91cd\u5929\u3002\u826f\u591c\u6b22\u5bb4\u7f62\uff0c\u6682\u522b\u5e9a\u7533\u5e74\u3002\r\n\u7530\u627f\u55e3\u8bf3\u674e\u5b9d\u81e3\u4f2a\u8c36:\u4e8c\u5e1d\u540c\u529f\u52bf\u4e07\u5168\uff0c\u5c06\u7530\u4f5c\u4f34\u5165\u5e7d\u71d5\u3002\r\n\u76ae\u65e5\u4f11\u9020\u9ec4\u5de2\u8c36:\u6b32\u77e5\u5723\u4eba\u59d3\uff0c\u7531\u516b\u4e8c\u5341\u4e00\u3002\u6b32\u77e5\u5723\u4eba\u540d\uff0c\u679c\u5934\u4e09\u5c48\u5f8b\u3002\r\n\u9644\uff1a\u4e0a\u5143\u521d\u5d69\u5c71\u77f3\u8bb0:\u6728\u5b50\u5f53\u5929\u4e0b\uff0c\u6b62\u6208\u9f99\u3002\u674e\u4ee3\u4ee3\u4e0d\u79fb\u5b97\uff0c\u4e2d\u9f0e\u663e\u771f\u5bb9\uff0c\u57fa\u5343\u4e07\u5c81\u3002\r\n\u4e0a\u9633\u94dc\u5668\u7bc6:\u957f\u5b9c\u5b50\u5b59\u3002\r\n\u6c38\u5b89\u6e20\u77f3\u94ed:\u767e\u5e74\u4e3a\u5e02\u540e\u4e3a\u6c60\u3002\r\n\u6f33\u6cc9\u5206\u5730\u795e\u7bc6:\u6f33\u6cc9\u4e24\u5dde\uff0c\u5206\u5730\u592a\u5e73\u3002\u6c38\u5b89\u9f99\u6eaa\uff0c\u5c71\u9ad8\u6c14\u6e05\u3002\u5343\u5e74\u4e0d\u60d1\uff0c\u4e07\u53e4\u4f5c\u7a0b\u3002\r\n\u542b\u5143\u6bbf\u4e39\u77f3\u9690\u8bed:\u5929\u6c49\u4e8c\u5e74\uff0c\u8d64\u5149\u751f\u6817\u3002\u6728\u4e0b\u6709\u5b50\uff0c\u4f24\u5fc3\u9047\u9177\u3002\r\n\u957f\u5b89\u7a7a\u5b85\u94ed\u7bc6:\u590f\u5929\u5b50\u7d2b\u91d1\u4e09\u5341\u65a4\uff0c\u8d50\u6709\u5fb7\u8005\u3002\r\n\u8386\u7530\u77f3\u8bb0:\u77f3\u6562\u5f53\uff0c\u9547\u767e\u9b3c\uff0c\u538b\u707e\u6b83\u3002\u5b98\u540f\u798f\uff0c\u767e\u59d3\u5eb7\u3002\u98ce\u6559\u76db\uff0c\u793c\u4e50\u660c\u3002\r\n\u7b26\u79bb\u6811\u7a74\u4e2d\u77f3\u7bc6:\u65c1\u6709\u6c34\uff0c\u4e0a\u6709\u9053\uff0c\u516b\u767e\u5e74\u4e2d\u9022\u6832\u6833\u3002\r\n\u6dee\u897f\u6c60\u6fe0\u77f3\u94ed:\u4e95\u5e95\u4e00\u7aff\u7af9\uff0c\u7af9\u8272\u6df1\u7eff\u7eff\u3002\u9e21\u672a\u80a5\uff0c\u9152\u672a\u719f\uff0c\u969c\u8f66\u513f\u90ce\u4e14\u987b\u7f29\u3002\r\n\u7f57\u6c60\u77f3\u523b:\u9f99\u57ce\u67f3\uff0c\u795e\u6240\u5b88\u3002\u9a71\u5389\u9b3c\uff0c\u5c71\u5de6\u9996\u3002\u798f\u571f\u6c13\uff0c\u5236\u4e5d\u4e11\u3002\r\n\"\"\"\r\n", "@jxfruit,\r\nOn running the code with TF v1.15, I am facing a different error stating `ZeroDivisionError: float division by zero`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dcedc3c02106b6a486c9a20ea0bedbb6/40176.ipynb#scrollTo=vhm2uGOU0uHG). \r\n\r\nCould you please upgrade to TF v1.15 and let us know if the issue is resolved. Thanks!", "@amahendrakar thanks a lot for your reply. I update TF v1.13 to TF v1.15, but I still get the same error, which is different from yours. I am so confused. By the way, our cpu is kunpeng 920 based on aarch64. I am not sure whether our chip is something wrong or not. And I cannot open your website,  may you provide a file with it. tks again!", "@jxfruit,\r\nValue of the variable `n_chunk` was 0, hence I was facing the `ZeroDivisionError`. \r\n\r\nOn replacng the line \r\n`print('epoch:', epoch, ' Loss: ', all_loss * 1.0 / n_chunk)` with \r\n`print('epoch:', epoch, ' Loss: ', all_loss * 1.0 / 1)`, I was able to run the code without any issues. \r\n\r\nPlease take a look at the Python notebook attached below. \r\n[40176.zip](https://github.com/tensorflow/tensorflow/files/4768727/40176.zip)\r\n\r\nThanks!", "@amahendrakar After checking the code, I found  the problem. There were only 33 poems, namely 33 data, because of the following code block:\r\n```python\r\nbatch_size=64\r\nn_chunk = len(poems_vector) // batch_size\r\nprint(n_chunk)\r\n```\r\nObviously, n_chunk will got 0. After changing batch_size from 64 to 8 will be ok.\r\nAnd I found the data is not enough, that could not get the error. When my dataset is more than 10Mb, the error can be got. Sadly, I could not upload the whole dataset. Could you produce more data to evaluate it, for example, repeating the data 1000 times?\r\ntks!\r\n", "@jxfruit,\r\nEven after repeating the data, I did not face any issues while running the code. Please check the attached zip file for the Python notebook. Thanks!\r\n\r\n[40176.zip](https://github.com/tensorflow/tensorflow/files/4772659/40176.zip)\r\n", "@amahendrakar hi\uff0ccould I  know your experiment environment ? Mine is based HUAWEI KUNPENG920. Is this causing the problem?", "@jxfruit,\r\nI have used [Google Colab](https://colab.research.google.com/) to run the code. ", "@amahendrakar @gowthamkpr Is your env  based on arm64 cpu? On x86-64 cpu, my codes is fine.", "@jxfruit,\r\n> I have used [Google Colab](https://colab.research.google.com/) to run the code.\r\n\r\nThe platform I was running on was based on x86_64 architecture.", "@amahendrakar It runs good on x86_64 architecture, and the problem is on aarch64 architecture, tks", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40176\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40176\">No</a>\n"]}, {"number": 40175, "title": "tf.summary.flush segfaults when writer is not a valid tf.summary.SummaryWriter object", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0 & v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.summary.flush` doesn't have input validity check to ensure `writer` is a `tf.summary.SummaryWriter`;\r\ngive it a ndarray as `writer` would make it segfault.\r\n**Describe the expected behavior**\r\nThe function shouldn't segfault and should have a proper input checking.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.summary.flush(writer=np.random.rand(2,2)) # causes segfaults\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate this issue, please find [gist here](https://colab.sandbox.google.com/gist/Saduf2019/7c8032ff3f91a11e944dfc953c68909a/untitled217.ipynb) the colab crashes when i runt he code.", "This should be expected, after all you are calling functionality which does not exist. That is, this is not really a fault of TF.", "The `flush()` op has legacy behavior where callers were passing in the raw handle to the resource rather than the SummaryWriter python object:\r\n\r\nhttps://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/summary_ops_v2.py;l=964;drc=5a07c487f666d2edecea59ba7e3a215a2a03fe9d\r\n\r\nIn particular this is used by TPUEstimator here:\r\n\r\nhttps://cs.opensource.google/tensorflow/estimator/+/master:tensorflow_estimator/python/estimator/tpu/tpu_estimator.py;l=522;drc=079a14eb925c3320c87baa0874ec7167e1b23242\r\n\r\nSo while I'd like to get rid of that undocumented fallback it's a bit hard to do so.  Perhaps instead there is a way we can at least restrict it to actual resource handle tensors (rather than arbitrary objects), although I'm still not sure that would completely avoid segfaults if passing in a resource handle that did not correspond to a summary writer resource.", "Oh, then this is something that we need to fix. Didn't know this part of history.", "@leeyeetonn  I tried to reproduce the issue in TF 2.5 and Nightly, I am getting different error but colab doesnt  crashes when i executed the code. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/a847f0e86a4dd752dc9319110ff5eceb/untitled82.ipynb).Thanks!", "@leeyeetonn Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40175\">No</a>\n", "The different error in colab is our fix to this issue. This is working now.", "@saikumarchalla @mihaimaruseac  I believed the crash is gone and is replaced with an error message. Thank you!"]}, {"number": 40174, "title": "[INTEL MKL] Fix Tensorflow MKL 'set-build-env.py' script syntax error", "body": "Thanks for catching this @kkasravi", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40174) for more info**.\n\n<!-- need_author_consent -->", "@kkasravi would you please provide your consent here like this:\r\n\r\n@googlebot I consent\r\n\r\nThanks.", "/assign @penpornk ", "@ashahba Thank you for your contribution. Can you please sign CLA? Thanks!", "@gbaned me and @kkasravi both have CLA's and I'm waiting for him to provide consent.\r\nIf I don't hear from him by EOD tomorrow, I'll go ahead and do another identical minor PR \ud83d\ude42 ", "@googlebot I consent", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40174) for more info**.\n\n<!-- ok -->", "Thanks @kkasravi \r\n@gbaned The `CLA` issue is resolved now.", "@mihaimaruseac this one is ready to go."]}, {"number": 40173, "title": "[ROCm] Fix for the ROCm CSB breakage - 200604", "body": "This PR addresses 2 unit test failures introduced by the following commit\r\nhttps://github.com/tensorflow/tensorflow/commit/bd20260350de4d0bc870fdc2bda39e62844a3e7b\r\n\r\nfailing unit tests\r\n\r\n```\r\n//tensorflow/core/common_runtime/gpu:gpu_device_test\r\n//tensorflow/core/common_runtime/gpu:gpu_device_unified_memory_test\r\n```\r\n\r\nIt is different from PR #40164, which fixes the build error on the ROCm platform.\r\n\r\nThe commit above adds unit tests that check the assignment of priority values to GPU streams. Because ROCm support for assigning priority values to GPU streams is missing, those unit tests fail.\r\n\r\nThis PR/commit adds the missing ROCm support, and updates the unit test to work with AMD GPUs too. The valid priority value range seems to be different for AMD GPUs (0,2) as compared to NVidia GPUs (-1, 0), and hence the changes requrired in the testcases too.\r\n\r\n\r\n-------------------------------------\r\n\r\n/cc @whchung @chsigg  @cheshire @nvining-work ", "comments": ["Thank you.\n\nOn Fri, Jun 5, 2020, 2:45 AM Deven Desai <notifications@github.com> wrote:\n\n> This PR addresses 2 unit test failures introduced by the following commit\n> bd20260\n> <https://github.com/tensorflow/tensorflow/commit/bd20260350de4d0bc870fdc2bda39e62844a3e7b>\n>\n> failing unit tests\n>\n> //tensorflow/core/common_runtime/gpu:gpu_device_test\n> //tensorflow/core/common_runtime/gpu:gpu_device_unified_memory_test\n>\n> It is different from PR #40164\n> <https://github.com/tensorflow/tensorflow/pull/40164>, which fixes the\n> build error on the ROCm platform.\n>\n> The commit above adds unit tests that check the assignment of priority\n> values to GPU streams. Because ROCm support for assigning priority values\n> to GPU streams is missing, those unit tests fail.\n>\n> This PR/commit adds the missing ROCm support, and updates the unit test to\n> work with AMD GPUs too. The valid priority value range seems to be\n> different for AMD GPUs (0,2) as compared to NVidia GPUs (-1, 0), and hence\n> the changes requrired in the testcases too.\n> ------------------------------\n>\n> /cc @whchung <https://github.com/whchung> @chsigg\n> <https://github.com/chsigg> @cheshire <https://github.com/cheshire>\n> @nvining-work <https://github.com/nvining-work>\n> ------------------------------\n> You can view, comment on, or merge this pull request online at:\n>\n>   https://github.com/tensorflow/tensorflow/pull/40173\n> Commit Summary\n>\n>    - [ROCm] Fix for the ROCm CSB breakage - 200604\n>\n> File Changes\n>\n>    - *M* tensorflow/core/common_runtime/gpu/gpu_device.cc\n>    <https://github.com/tensorflow/tensorflow/pull/40173/files#diff-3780f0ef44936240abc76c4c42541532>\n>    (23)\n>    - *M* tensorflow/core/common_runtime/gpu/gpu_device_test.cc\n>    <https://github.com/tensorflow/tensorflow/pull/40173/files#diff-516a8549cd3c43b29838bca09b3e5a2f>\n>    (59)\n>    - *M* tensorflow/stream_executor/rocm/rocm_driver.cc\n>    <https://github.com/tensorflow/tensorflow/pull/40173/files#diff-c6e1d2c79c0d9405582bc65c21546247>\n>    (15)\n>    - *M* tensorflow/stream_executor/rocm/rocm_driver_wrapper.h\n>    <https://github.com/tensorflow/tensorflow/pull/40173/files#diff-36ca8343b632e87a61106e534f499576>\n>    (1)\n>\n> Patch Links:\n>\n>    - https://github.com/tensorflow/tensorflow/pull/40173.patch\n>    - https://github.com/tensorflow/tensorflow/pull/40173.diff\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/40173>, or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525W6ESPNKB2ACL7XURTRVA53XANCNFSM4NTDG47Q>\n> .\n>\n", "@chsigg gentle ping"]}, {"number": 40172, "title": "ROCm build broken", "body": "TensorFlow cannot build AMD GPUs using ROCm at head right now. The compilation error is:\r\n\r\n```\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:68:5: note: in instantiation of function template specialization 'tensorflow::GpuAtomicMin<long long, long long>' requested here\r\n    GpuAtomicMin(out, val);\r\n    ^\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:118:9: note: in instantiation of member function 'tensorflow::(anonymous namespace)::LeftUpdate<long long, tensorflow::scatter_nd_op::UpdateOp::MIN>::operator()' requested here\r\n        update(out + i + si, ldg(updates + (index * slice_size + si)));\r\n        ^\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:156:33: note: in instantiation of function template specialization 'tensorflow::ScatterNdOpKernel<long long, int, tensorflow::scatter_nd_op::UpdateOp::MIN, 1>' requested here\r\n    TF_CHECK_OK(GpuLaunchKernel(ScatterNdOpKernel<T, Index, op, IXDIM>,\r\n                                ^\r\n/opt/rocm/hip/include/hip/hcc_detail/hip_atomic.h:141:5: note: candidate function not viable: no known conversion from 'long long *' to 'int *' for 1st argument\r\nint atomicMin(int* address, int val)\r\n    ^\r\n/opt/rocm/hip/include/hip/hcc_detail/hip_atomic.h:147:14: note: candidate function not viable: no known conversion from 'long long *' to 'unsigned int *' for 1st argument\r\nunsigned int atomicMin(unsigned int* address, unsigned int val)\r\n             ^\r\n/opt/rocm/hip/include/hip/hcc_detail/hip_atomic.h:153:20: note: candidate function not viable: no known conversion from 'long long *' to 'unsigned long long *' for 1st argument\r\nunsigned long long atomicMin(\r\n \r\n```\r\n\r\nThe problem seems to be in our code, our templates need gpuAtomicMin for also signed types, but they are not available for signed types in AMD code.\r\n\r\n@chsigg FYI\r\n@whchung could you take a look\r\n", "comments": ["@deven-amd Would you mind take this ticket?\r\n\r\n@gunan I'm mostly spending my time on MLIR code generation now and I'd like @deven-amd to help look into this. It seems I couldn't assign the ticket to him though.", "We can't assign this issue to @deven-amd because he is not part of the TF org.", "I already filed PR #40164 yesterday to fix this issue ", "Thanks Deven for the quick fix. I'm working on getting it merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40172\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40172\">No</a>\n"]}, {"number": 40171, "title": "Memory leak when repeatedly loading and deleting keras models", "body": "If a Keras model is saved using `tf.saved_model.save` and then repeatedly loaded with `tf.saved_model.load` and deleted with `del` it becomes apparent that there is a slow memory leak.  `keras.backend.clear_session` does not resolve this issue.  See attached gist for an example that reproduces this issue in TensorFlow 2.2 on Google Colab.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI have attached a custom repro case, but this appears to happen for various types of typical keras models.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCan reproduce in Google Colab and Docker RedHat images\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nnot tested\r\n- TensorFlow installed from (source or binary):\r\nbinary (from pip)\r\n- TensorFlow version (use command below):\r\n('2.2.0', 'v2.2.0-0-g2b96f3662b')\r\n- Python version:\r\n3.6.9 (google colab)\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\ndefault in google colab\r\n- GPU model and memory:\r\ndefault in google colab\r\n\r\n**Describe the current behavior**\r\nWhen Keras models are saved / loaded repeatedly, memory usage gradually continues to grow over time.  For dynamic model servers that load and unload models over time, this may eventually lead to a crash due to memory exhaustion.\r\n\r\n**Describe the expected behavior**\r\nAll memory should be recovered after a keras model instance is deleted with `del` and the garbage collector is run with `gc.collect()`.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe following GitHub gist demonstrates the issue (can also be run in Colab):\r\nhttps://gist.github.com/idfah/dff83de8d2a6406c9b92221e6282a8d6", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/46ec357442d36c1616a9805df7407a4a/40171-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/c6767cb79aa7908c57e75b3615969b67/40171-tf-nightly.ipynb). Please find the attached gist. Thanks!", "verified that the same behavior is present in `2.0`, `2.1` and `2.2`", "Hello, \r\nI have some more info to add to this issue. I have run a bunch of experiments and have observed the following:\r\n- There is a small but persistent memory leak whenever models are loaded multiple times. \r\n- The leak seems to depend on the number of layers. \r\n- The leak seems to be independent of number of variables/parameters. \r\n\r\nI have uploaded my experiments [here](https://github.com/kmh4321/tf2_memory_leak). I have 3 models having roughly the same number of parameters but spread across different layers. The memory leak seems to be the least when there is only 1 hidden layer. There is almost a 10x memory leak increase when I increase the layers from 1 hidden layer to 15 hidden layers. All while keeping the # of parameters/variables (roughly) fixed. With just one layer I observed a memory usage increase of 210KB per load cycle v/s 1.7MB per load cycle with multiple layers/\r\n\r\nMaybe there is something in the way Keras instantiates layers that is causing this issue?\r\n\r\ncc: @frreiss ", "Preliminary results from profiling the example program @kmh4321 provided with `pympler` and `gperftools`:\r\n* The leak does not appear to be in the Python heap (or at least the portion of the Python heap that `pympler` can see).\r\n* There's a pretty substantial amount of memory leakage of objects allocated from various subroutines of `TF_GraphImportGraphDefWithResults`. I'm guessing that some part of the Python code is leaking handles to `TF_Graph` objects located on the C++ heap.\r\n* The memory that `gperftools`' profiler can see adds up to about 75% of the actual memory usage of my test process. That's probably good enough for this case.\r\n* The stack traces I'm getting from `pprof --text --stacks` are kind of garbled right now, probably because I'm using TensorFlow 2.2.0 binaries from PyPI. I'll try building a version of TensorFlow with debug symbols overnight.\r\n", "Here's the output of `pprof --text --stacks` in case anyone else can make more sense out of it:[pprof.out.txt](https://github.com/tensorflow/tensorflow/files/4817217/pprof.out.txt)\r\n", "I've identified the cause of this leak. While loading functions from the SavedModel file, `function_deserialization.load_function_def_library()` executes these two lines:\r\n```\r\n    if context.executing_eagerly():\r\n      func.add_to_graph(ops.get_default_graph())\r\n```\r\nSee source file [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/saved_model/function_deserialization.py#L318).\r\n\r\nThese lines create a second copy of the body of each function and stick that second copy into a dictionary hanging off a `tf.Graph` that is in turn hanging off a global variable. That second copy of the function is never deleted.\r\n\r\nThis problem appears to have been patched on the master branch four months ago; see [this commit](https://github.com/tensorflow/tensorflow/commit/3421416220f5dd65340f03332ff1d474de69c052).\r\n\r\n**It would be a really good idea to backport this fix to the 2.2 branch.** I'm not sure why it wasn't backported in the first place. TensorFlow 2.2.0 came out almost two months after this fix was pushed into the master branch.\r\n\r\nIt would also be a good idea to backport this fix to the 2.1 branch.\r\n", "I've identified a workaround that cuts this memory leakage by about 80%: Load the model from a temporary background thread.\r\n\r\nHere's some code to copy and paste:\r\n``` python\r\ndef load_model_hack(saved_model_path: str):\r\n  \"\"\"\r\n  Load a SavedModel from a background thread so that most of the garbage \r\n  that saved_model.load() leaves on the calling thread's environment will\r\n  be collected. Still leaks memory, but at a lower rate.\r\n  \"\"\"\r\n  import threading\r\n  from typing import Any, Dict\r\n  def callback(path: str, result_holder: Dict[str, Any]) -> None:\r\n    try:\r\n      result_holder[\"model\"] = tf.saved_model.load(path)\r\n    except Exception as e:\r\n      result_holder[\"error\"] = e\r\n\r\n  # Call saved_model.load() in a background thread \r\n  thread_results = {}\r\n  t = threading.Thread(target=callback, args=[saved_model_path, thread_results])\r\n  t.start()\r\n  t.join()\r\n\r\n  # Forward any exceptions thrown by the background thread\r\n  if \"error\" in thread_results:\r\n    raise thread_results[\"error\"]\r\n  return thread_results[\"model\"]\r\n```\r\n\r\nAfter putting this workaround in place, the test case described above still leaks memory on TensorFlow 2.2.0. However, the leakage occurs at a much slower rate. I suspect that there is a second memory leak in TensorFlow 2.2.0's model loading path, and that this second leak has also been patched on the master branch without backporting the patch to the 2.2 branch.", "@k-w-w would it be possible to add the fix from the current master branch to the 2.2 branch?\r\n\r\n> I've identified the cause of this leak. While loading functions from the SavedModel file, `function_deserialization.load_function_def_library()` executes these two lines:\r\n> \r\n> ```\r\n>     if context.executing_eagerly():\r\n>       func.add_to_graph(ops.get_default_graph())\r\n> ```\r\n> \r\n> See source file [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/saved_model/function_deserialization.py#L318).\r\n> \r\n> These lines create a second copy of the body of each function and stick that second copy into a dictionary hanging off a `tf.Graph` that is in turn hanging off a global variable. That second copy of the function is never deleted.\r\n> \r\n> This problem appears to have been patched on the master branch four months ago; see [this commit](https://github.com/tensorflow/tensorflow/commit/3421416220f5dd65340f03332ff1d474de69c052).\r\n> \r\n> **It would be a really good idea to backport this fix to the 2.2 branch.** I'm not sure why it wasn't backported in the first place. TensorFlow 2.2.0 came out almost two months after this fix was pushed into the master branch.\r\n> \r\n> It would also be a good idea to backport this fix to the 2.1 branch.\r\n\r\n", "**Some updates:** \r\n* I have tracked down the second memory leak.\r\n* I have identified a workaround for the second leak.\r\n* There is a third leak.\r\n\r\nDetails follow.\r\n\r\nThe root cause of the second memory leak is the lines immediately before the lines I pointed out in my previous comment:\r\n\r\n(`python/saved_model/function_deserialization.py`:\r\n```python\r\n316    func = function_lib.ConcreteFunction(func_graph)\r\n317    func.add_to_graph()\r\n318    if context.executing_eagerly():\r\n319      func.add_to_graph(ops.get_default_graph())\r\n```\r\n\r\nLine 316 creates a `ConcreteFunction` object. The initializer for `ConcreteFunction` adds the function to the current eager execution context and creates an ` _EagerDefinedFunctionDeleter` callback object to that will remove the function from the context when the `ConcreteFunction` goes out of scope.\r\n\r\nThen line 317 calls `add_to_graph()` on the `ConcreteFunction`, implicitly passing `None` for the `g` (graph) parameter. `ConcreteFunction.add_to_graph()` passes its `g` argument down to `_EagerDefinedFunction.add_to_graph()`. And `_EagerDefinedFunction.add_to_graph()` will call `TFE_ContextAddFunctionDef` if its `g` parameter is `None`. `TFE_ContextAddFunctionDef` is actually a thin wrapper around `TFE_ContextAddFunction`.\r\n\r\nSo every function in the SavedModel is passed **twice** to `TFE_ContextAddFunction`.\r\n\r\nWhen `TFE_ContextAddFunction` is called with a function definition whose name happens to match an existing function in the context, `TFE_ContextAddFunction` ignores the function definition that was passed to it (a disastrous design flaw IMO -- what if the second copy of the function isn't the same as the first one?) and increments the existing function's reference count.\r\n\r\nSo every function in the SavedModel ends up instantiated in the current context **with a reference count of 2**.\r\n\r\nWhen the model's surrogate Python object goes out of scope, the `ConcreteFunction` objects under it also go out of scope, which causes the `_EagerDefinedFunctionDeleter` objects attached to them to go out of scope, which triggers a **single** call to `TFE_ContextRemoveFunction` for each function. In spite of its name, the `TFE_ContextRemoveFunction` API call does not actually remove the function whose name is passed to it unless the reference count of that function is 1. And every function in the SavedModel has a reference count of 2.\r\n\r\nThis second memory leak is also fixed in commit 3421416220f5dd65340f03332ff1d474de69c052\r\n\r\nAfter that commit, line 317 is replaced with:\r\n``` python\r\n  func.add_to_graph(graph)\r\n```\r\nwhere `graph` is a temporary `Graph` object. Having the `g` parameter of `ConcreteFunction.add_to_graph()` set to a value other than `None` prevents the leaky code in `_EagerDefinedFunction.add_to_graph()` from running.\r\n\r\n\r\nHere's an updated version of my workaround that corrects for both memory leaks:\r\n\r\n```python\r\ndef load_model_hack(saved_model_path: str):\r\n  \"\"\"\r\n  Load a SavedModel without leaking as much memory as usual.\r\n  \r\n  This function applies two workarounds: \r\n  * Load the model from a temporary background thread so that \r\n    `saved_model.load()` won't leave garbage hanging off of the global \r\n    default graph\r\n  * Unpin functions that `saved_model.load()` pins twice, so that the \r\n    garbage collection logic in `ConcreteFunction` will correctly remove\r\n    these functions when the restored model goes out of scope..\r\n  \"\"\"\r\n  import threading\r\n  from typing import Any, Dict\r\n  def callback(path: str, result_holder: Dict[str, Any]) -> None:\r\n    \"\"\"Callback function to be executed in a background thread\"\"\"\r\n    try:\r\n      result_holder[\"model\"] = tf.saved_model.load(path)\r\n\r\n      # Every function that was pinned twice (and hopefully exactly those\r\n      # functions) should now be in the background thread's global default\r\n      # graph. Unpin these functions once.\r\n      default_graph = tf.compat.v1.get_default_graph()\r\n      for function_name in default_graph._functions.keys():\r\n        tf.python.context.remove_function(function_name)\r\n    except Exception as e:\r\n      result_holder[\"error\"] = e\r\n\r\n  # Call saved_model.load() in a background thread \r\n  thread_results = {}\r\n  t = threading.Thread(target=callback, args=[saved_model_path, thread_results])\r\n  t.start()\r\n  t.join()\r\n\r\n  # Forward any exceptions thrown by the background thread\r\n  if \"error\" in thread_results:\r\n    raise thread_results[\"error\"]\r\n  return thread_results[\"model\"]\r\n```\r\n\r\nThis workaround further reduces the amount of memory that my test program leaks. However **it does not completely eliminate memory leakage.** \r\n\r\n\r\n", "Updates:\r\n\r\n* I have found the third leak.\r\n* The patch for the first two leaks also patched the third leak.\r\n* I have updated my workaround to cover all three leaks.\r\n* There is a fourth leak.\r\n\r\n\r\nDetails follow.\r\n\r\n\r\n`Loader._load_nodes()` walks through the graph, reconstituting each part of the graph from its serialized Protocol Buffers representation.\r\n\r\n(See line 264 in `tensorflow/python/saved_model/load.py`)\r\n``` python\r\n258    # Re-create everything except slot variables.\r\n259    for node_id, proto in enumerate(self._proto.nodes):\r\n260      if node_id in slot_variable_node_ids:\r\n261        # Defer recreating slot variables so we can use the public Optimizer\r\n262        # interface.\r\n263        continue\r\n264      node, setter = self._recreate(proto, node_id)   <<<<<<<<<<<\r\n265      nodes[node_id] = node\r\n266      node_setters[node_id] = setter\r\n```\r\n\r\nThe `proto` variable here is a surrogate object for a `SavedObject` message:\r\n\r\n(in `tensorflow/core/protobuf/saved_object_graph.proto`)\r\n``` proto\r\nmessage SavedObject {\r\n  // Objects which this object depends on: named edges in the dependency\r\n  // graph.\r\n  //\r\n  // Note: currently only valid if kind == \"user_object\".\r\n  repeated TrackableObjectGraph.TrackableObject.ObjectReference children = 1;\r\n\r\n  // Removed when forking SavedObject from TrackableObjectGraph.\r\n  reserved \"attributes\";\r\n  reserved 2;\r\n\r\n  // Slot variables owned by this object. This describes the three-way\r\n  // (optimizer, variable, slot variable) relationship; none of the three\r\n  // depend on the others directly.\r\n  //\r\n  // Note: currently only valid if kind == \"user_object\".\r\n  repeated TrackableObjectGraph.TrackableObject.SlotVariableReference\r\n      slot_variables = 3;\r\n\r\n  oneof kind {\r\n    SavedUserObject user_object = 4;\r\n    SavedAsset asset = 5;\r\n    SavedFunction function = 6;\r\n    SavedVariable variable = 7;\r\n    SavedBareConcreteFunction bare_concrete_function = 8;\r\n    SavedConstant constant = 9;\r\n    SavedResource resource = 10;\r\n  }\r\n}\r\n```\r\n\r\n`Loader._recreate()` is an unnecessarily-complex switch statement.  Here are the relevant snippets:\r\n\r\n(in `tensorflow/python/saved_model/load.py`)\r\n``` python\r\n353  def _recreate(self, proto, node_id):\r\n354    \"\"\"Creates a Python object from a SavedObject protocol buffer.\"\"\"\r\n355    factory = {\r\n356        \"user_object\": (\r\n357            lambda: self._recreate_user_object(proto.user_object, node_id)),\r\n358        \"asset\": lambda: self._recreate_asset(proto.asset),\r\n359        \"function\": lambda: self._recreate_function(proto.function),\r\n360        \"bare_concrete_function\": functools.partial(\r\n361            self._recreate_bare_concrete_function,\r\n362            proto.bare_concrete_function),\r\n363        \"variable\": lambda: self._recreate_variable(proto.variable),\r\n364        \"constant\": lambda: self._recreate_constant(proto.constant),\r\n365        \"resource\": lambda: self._recreate_resource(proto.resource),\r\n366    }\r\n367    kind = proto.WhichOneof(\"kind\")\r\n368    if kind not in factory:\r\n369      raise ValueError(\"Unknown SavedObject type: %r\" % kind)\r\n360    return factory[kind]()\r\n\r\n...\r\n\r\n396  def _recreate_function(self, proto):\r\n397    return function_deserialization.recreate_function(\r\n398        proto, self._concrete_functions), setattr\r\n```\r\n\r\nIn the case of this memory leak, the `SavedObject` message in the variable `proto` has its `kind` field set to `bare_concrete_function`. So the above translates into:\r\n``` python\r\nif proto.kind is bare_concrete_function:\r\n    node = function_deserialization.setup_bare_concrete_function(proto,\r\n        self._concrete_functions)\r\n    setter = getattr\r\n    return node, setter\r\nelse ... # code that doesn't execute for this case\r\n```\r\n\r\n(The reference to `getattr` feeds another bit of complex code elsewhere in `load.py`.)\r\n\r\n`function_deserialization.setup_bare_concrete_function()` looks up the  already-deserialized `ConcreteFunction` object, then calls that object's `add_to_graph()` method (line 172 below).\r\n\r\n(in `tensorflow/python/saved_model/function_deserialization.py`)\r\n``` python\r\n159 def setup_bare_concrete_function(saved_bare_concrete_function,\r\n160                                 concrete_functions):\r\n161  \"\"\"Makes a restored bare concrete function callable.\"\"\"\r\n160  # Bare concrete functions accept only flat lists of Tensors with unique\r\n163  # names.\r\n164  concrete_function = concrete_functions[\r\n165      saved_bare_concrete_function.concrete_function_name]\r\n166  # pylint: disable=protected-access\r\n167  concrete_function._arg_keywords = (\r\n168      saved_bare_concrete_function.argument_keywords)\r\n169  concrete_function._num_positional_args = (\r\n170      saved_bare_concrete_function.allowed_positional_arguments)\r\n171  # pylint: enable=protected-access\r\n172  concrete_function.add_to_graph()     <<<<<<<<<<\r\n173  return concrete_function\r\n```\r\n\r\nOf course, the code that created the `ConcreteFunction` object in the first place has already added it to the graph twice.\r\n\r\nAnd due to same the root cause as leaks 1 and 2, the `ConcreteFunction`'s graph pointer is set to `None`, so `ConcreteFunction.add_to_graph()` calls `_EagerDefinedFunction.add_to_graph()`, which adds the function to the eager execution context a third time.\r\n\r\n\r\nUnfortunately, the `Loader` class discards all information about what functions it has given this special treatment to, so a workaround along the lines of what I posted in my previous comment is not going to work. Here's my new workaround, which involves live-patching the function deletion callbacks in the background thread's global default graph:\r\n\r\n``` python\r\nclass DeleteWithExtremePrejudice(object):\r\n  \"\"\"\r\n  A version of _EagerDefinedFunctionDeleter (see\r\n  tensorflow/python/eager/function.py) that keeps deleting the target function\r\n  until an InvalidArgumentError exception is thrown.  Checking for that\r\n  exception is the only way to ensure that a function really has been deleted\r\n  and is not, in fact, still taking up memory.\r\n  \"\"\"\r\n\r\n  def __init__(self, func_name):\r\n    self.func_name = func_name\r\n\r\n  def __del__(self):\r\n    MAX_ATTEMPTS = 10\r\n    try:\r\n      for i in range(MAX_ATTEMPTS):\r\n        tf.python.context.remove_function(self.func_name)\r\n      # If we get here, removal did *not* fail as expected.\r\n      print(f\"WARNING: Failed to remove function \"\r\n            f\"'{self.func_name}' after {MAX_ATTEMPTS} attempts. \"\r\n            f\"This problem may result in a memory leak.\")\r\n    except tf.errors.InvalidArgumentError:\r\n      # tf.python.context.remove_function() throws this exception when\r\n      # you try to remove a function that has already been removed.\r\n      # In the case of this `try` clause, such behavior is \"normal\".\r\n      pass\r\n    except Exception as e:\r\n      print(f\"WARNING: {e} thrown when attempting to delete function \"\r\n            f\"'{self.func_name}'. This problem may result in a memory leak.\")\r\n  \r\n\r\ndef load_model_hack(saved_model_path: str):\r\n  \"\"\"\r\n  Load a SavedModel without leaking memory.\r\n  \r\n  This function applies two workarounds: \r\n  * Load the model from a temporary background thread so that \r\n    `saved_model.load()` won't leave garbage hanging off of the global \r\n    default graph\r\n  * Patch the garbage collection callbacks of all `ConcreteFunction`s\r\n    in the returned model so that these functions will be properly removed\r\n    when the restored model goes out of scope.\r\n  \"\"\"\r\n  import threading\r\n  from typing import Any, Dict\r\n  def callback(path: str, result_holder: Dict[str, Any]) -> None:\r\n    \"\"\"Callback function to be executed in a background thread\"\"\"\r\n    try:\r\n      model = tf.saved_model.load(path)\r\n\r\n      # Every function that was pinned two or more times should now be\r\n      # in the current thread's global default graph variable.\r\n      # Replace the deletion callbacks of these functions with a more\r\n      # effective version.\r\n      default_graph = tf.compat.v1.get_default_graph()\r\n      for func in default_graph._functions.values():\r\n        func._function_deleter = DeleteWithExtremePrejudice(func.name)\r\n        # NOTE: This assignment will trigger the previous deletion callback.\r\n        # That's ok, because every function in this list has been pinned \r\n        # at least twice.\r\n\r\n      result_holder[\"model\"] = model\r\n\r\n    except Exception as e:\r\n      result_holder[\"error\"] = e\r\n\r\n  # Call saved_model.load() in a background thread \r\n  thread_results = {}\r\n  t = threading.Thread(target=callback, args=[saved_model_path, thread_results])\r\n  t.start()\r\n  t.join()\r\n```\r\n\r\nAfter applying this workaround, my test program leaks significantly less memory than before, **but it still leaks memory.**", "@idfah Recently there were some updates to reduce the memory leak. I ran your code with recent `tf-nightly` and see a loss of ~12 MB over 500 iterations. Can you please check the [this gist](https://colab.research.google.com/gist/jvishnuvardhan/dd1d0a740af1aeb00d3c31b4a496006f/40171-tf-nightly.ipynb) and let us know what you think. \r\n\r\nPlease verify and close the issue If this was resolved for you. Thanks!", "@jvishnuvardhan, I think this issue needs to be kept open a while longer.\r\n\r\nSo far, we have verified that there is a serious memory leak in `tf.saved_model.load()` in TensorFlow 2.0.x, 2.1.x, and 2.2.x.\r\n\r\nI would categorize this leak as a blocker for any application that needs to cycle models in and out of memory -- for example, to process a corpus of documents that span multiple languages; or to serve multiple versions of the same model. Our simple test program \"only\" leaks a few megabytes each time the model is loaded, but larger models with weights embedded in their graphs can leak hundreds of megabytes per load/unload cycle.\r\n\r\nThe leak is actually three leaks, all of which were patched in master back in March (in commit 3421416220f5dd65340f03332ff1d474de69c052). However, the fix was not included in the May release of TensorFlow 2.2.0.  As of today, five months later, the fix has not been ported to the 2.2.x, 2.1.x, or 2.0.x branches of TensorFlow.\r\n\r\nTensorFlow 2.3.0 includes the fix for these three memory leaks.  However, fixing this bug in 2.3.0 does not resolve this issue for us.  My colleagues are currently using TensorFlow 2.2.x.\r\n\r\nIn addition to the three large leaks, there is a fourth leak that is not currently patched in the master branch.  You can see the presence of this fourth leak in the output of the notebook linked in your previous comment:\r\n![memory_leak](https://user-images.githubusercontent.com/12436991/89954357-c35f0700-dbe5-11ea-86f8-d47b62d8831f.png)\r\n\r\nWith the simple 2-layer model in your example notebook, each call to `saved_model.load()` leaks about 25kb of memory.  Larger models leak more, probably a megabyte or two for a medium-sized deep learning model.  This level of memory leakage is something that one could plausibly work around with periodic reboots; but I would submit that `tf.saved_model.load()` ought not to leak any memory at all.  Authors of long-running applications should be able to load and unload TensorFlow models without worrying about running out of memory.\r\n\r\nI have tracked the root cause of the fourth leak to a problem in TensorFlow's mechanism for caching kernels. \r\n\r\nIn addition to creating graphs, `tf.saved_model.load()` executes operations in those graphs, primarily for the purpose of restoring variable values. The code that executes these operations is `EagerExecute()`, which calls `EagerLocalExecute()`, which calls `GetOrCreateKernelAndDevice()`, which asks the current `EagerContext` to look for the kernel for each operation in its kernel cache.\r\n\r\nThe `EagerContext` class maintains a cache of kernel instances:\r\n(in `tensorflow/core/common_runtime/eager/context.h`, direct link [here](https://github.com/tensorflow/tensorflow/blob/deeeffefe317a0728b70262b9177712cadd77df1/tensorflow/core/common_runtime/eager/context.h#L612))\r\n```C++\r\n612  std::unordered_map<Fprint128, core::RefCountPtr<KernelAndDevice>,\r\n613                     Fprint128Hasher>\r\n614      kernel_cache_ TF_GUARDED_BY(cache_mu_);\r\n```\r\n\r\nThe kernel cache does not have a size limit. There is an API call to clear the cache, but the Python side of TensorFlow only uses that API call when resetting the global random seed.\r\n\r\nEach entry in the cache is parameterized by the \"fingerprint\" of the associated operation. This \"fingerprint\" is a hash value computed from multiple parameters of the operation, including all of the operation's attributes.\r\n\r\n`saved_model.load()` restores variables through a process that involves invoking multiple instances of the `VarHandleOp` operation. Due to the graph rewriting that happens during model loading, each of these operations has a unique value in the `shared_name` field if its attributes.  These unique values cause the operations to have unique fingerprints, even across multiple load operations on the same model. Each unique fingerprint causes the creation of a new entry in the cache.  The cache is of unlimited size and is never cleared, so memory usage for these cached kernels grows in an unbounded fashion.\r\n\r\nThe best workaround I've found for this problem is to have your Python application periodically clear the cache via the internal API. Here's some Python code to do so:\r\n\r\n```python\r\nimport tensorflow.python as tf_internals\r\ncontext_handle = tf_internals.eager.context.context()._context_handle\r\nif context_handle is not None:\r\n  tf_internals.pywrap_tfe.TFE_ContextClearCaches(context_handle)\r\n```\r\n\r\nA more permanent fix would be to evict stale entries from the cache following a least recently used policy. **I'm working on a PR to apply such a fix.**\r\n\r\nThe above workaround reduces but **does not eliminate the memory leakage** of my test program.  Before the workaround, each call to `saved_model.load()` leaked about 125kb on TensorFlow 2.3.0 and 115kb on tf-nightly.  After the workaround, each call leaks about 30kb and 20kb on TensorFlow 2.3.0 and nightly, respectively. I haven't checked whether this remaining leakage is constant or whether it scales with model size. However, since the leakage appears to be coming from a graph rewrite, I would expect the amount of memory leaked to scale with model size.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Not sure why tensorflow-butler thinks this issue hasn't had recent activity. There is an open pull request for part of the problem described here.", "@frreiss regarding the backport, we typically dont backport bug fixes into previous releases. Is it possible for you to use a later version like TF 2.3.0 which has the fixes ?\r\nRegarding the 4th leak, I dont see any pull request that is open currently, is this something you are working on ?", "Hi @goldiegadde . I see from the release notes that TensorFlow 2.3.1 includes 25 bug fixes, and TensorFlow 2.2.1 includes 19 bug fixes. Perhaps you meant to say that you typically don't backport fixes for memory leaks?\r\n\r\nMoving to 2.3.x is the only viable option for us at this point, and my colleagues will be doing so in spite of the disruption that this entails.\r\n\r\nThe fourth leak is currently low on my priority list because https://github.com/tensorflow/tensorflow/issues/33412 is a much more severe problem for us. Hopefully that fourth leak fixes itself.", "I just tested the above [colab notebook](https://colab.research.google.com/gist/idfah/dff83de8d2a6406c9b92221e6282a8d6/tf2_load_mem_leak.ipynb#scrollTo=O6kkoWvXnz_6) with the latest tf-nightly (version 2.5.0-dev20201214) and this memory increase with iterations looks reduced by a fair bit. Its unclear to me if this is fully fixed, but perhaps the fourth memory leak you mentioned has been fixed.\r\n\r\n![memory_leak_TF2.5.0-dev20201214](https://user-images.githubusercontent.com/45604577/102146437-c5e49e00-3e1d-11eb-8f56-81c9e1d62856.jpg)\r\n\r\nEdit:\r\nAlso want to note this appears fixed in TF 2.4.0, so no need to grab a tf-nightly\r\n\r\n![memory_leak_TF2.4.0](https://user-images.githubusercontent.com/45604577/103597322-dc2dc900-4eb4-11eb-8365-8940766fb19c.jpg)\r\n", "I think this issue is as fixed as it's going to be. What do you think, @idfah ?", "@frreiss the [commit](https://github.com/tensorflow/tensorflow/commit/3421416220f5dd65340f03332ff1d474de69c052) you referred to has been cherrypicked into [2.2.2](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.2) as well. I am closing this bug for now if any other issues linger can you please open a new one?\r\nThanks!", "> Updates:\r\n> \r\n> * I have found the third leak.\r\n> * The patch for the first two leaks also patched the third leak.\r\n> * I have updated my workaround to cover all three leaks.\r\n> * There is a fourth leak.\r\n> \r\n> Details follow.\r\n> \r\n> `Loader._load_nodes()` walks through the graph, reconstituting each part of the graph from its serialized Protocol Buffers representation.\r\n> \r\n> (See line 264 in `tensorflow/python/saved_model/load.py`)\r\n> \r\n> ```python\r\n> 258    # Re-create everything except slot variables.\r\n> 259    for node_id, proto in enumerate(self._proto.nodes):\r\n> 260      if node_id in slot_variable_node_ids:\r\n> 261        # Defer recreating slot variables so we can use the public Optimizer\r\n> 262        # interface.\r\n> 263        continue\r\n> 264      node, setter = self._recreate(proto, node_id)   <<<<<<<<<<<\r\n> 265      nodes[node_id] = node\r\n> 266      node_setters[node_id] = setter\r\n> ```\r\n> \r\n> The `proto` variable here is a surrogate object for a `SavedObject` message:\r\n> \r\n> (in `tensorflow/core/protobuf/saved_object_graph.proto`)\r\n> \r\n> ```protobuf\r\n> message SavedObject {\r\n>   // Objects which this object depends on: named edges in the dependency\r\n>   // graph.\r\n>   //\r\n>   // Note: currently only valid if kind == \"user_object\".\r\n>   repeated TrackableObjectGraph.TrackableObject.ObjectReference children = 1;\r\n> \r\n>   // Removed when forking SavedObject from TrackableObjectGraph.\r\n>   reserved \"attributes\";\r\n>   reserved 2;\r\n> \r\n>   // Slot variables owned by this object. This describes the three-way\r\n>   // (optimizer, variable, slot variable) relationship; none of the three\r\n>   // depend on the others directly.\r\n>   //\r\n>   // Note: currently only valid if kind == \"user_object\".\r\n>   repeated TrackableObjectGraph.TrackableObject.SlotVariableReference\r\n>       slot_variables = 3;\r\n> \r\n>   oneof kind {\r\n>     SavedUserObject user_object = 4;\r\n>     SavedAsset asset = 5;\r\n>     SavedFunction function = 6;\r\n>     SavedVariable variable = 7;\r\n>     SavedBareConcreteFunction bare_concrete_function = 8;\r\n>     SavedConstant constant = 9;\r\n>     SavedResource resource = 10;\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> `Loader._recreate()` is an unnecessarily-complex switch statement. Here are the relevant snippets:\r\n> \r\n> (in `tensorflow/python/saved_model/load.py`)\r\n> \r\n> ```python\r\n> 353  def _recreate(self, proto, node_id):\r\n> 354    \"\"\"Creates a Python object from a SavedObject protocol buffer.\"\"\"\r\n> 355    factory = {\r\n> 356        \"user_object\": (\r\n> 357            lambda: self._recreate_user_object(proto.user_object, node_id)),\r\n> 358        \"asset\": lambda: self._recreate_asset(proto.asset),\r\n> 359        \"function\": lambda: self._recreate_function(proto.function),\r\n> 360        \"bare_concrete_function\": functools.partial(\r\n> 361            self._recreate_bare_concrete_function,\r\n> 362            proto.bare_concrete_function),\r\n> 363        \"variable\": lambda: self._recreate_variable(proto.variable),\r\n> 364        \"constant\": lambda: self._recreate_constant(proto.constant),\r\n> 365        \"resource\": lambda: self._recreate_resource(proto.resource),\r\n> 366    }\r\n> 367    kind = proto.WhichOneof(\"kind\")\r\n> 368    if kind not in factory:\r\n> 369      raise ValueError(\"Unknown SavedObject type: %r\" % kind)\r\n> 360    return factory[kind]()\r\n> \r\n> ...\r\n> \r\n> 396  def _recreate_function(self, proto):\r\n> 397    return function_deserialization.recreate_function(\r\n> 398        proto, self._concrete_functions), setattr\r\n> ```\r\n> \r\n> In the case of this memory leak, the `SavedObject` message in the variable `proto` has its `kind` field set to `bare_concrete_function`. So the above translates into:\r\n> \r\n> ```python\r\n> if proto.kind is bare_concrete_function:\r\n>     node = function_deserialization.setup_bare_concrete_function(proto,\r\n>         self._concrete_functions)\r\n>     setter = getattr\r\n>     return node, setter\r\n> else ... # code that doesn't execute for this case\r\n> ```\r\n> \r\n> (The reference to `getattr` feeds another bit of complex code elsewhere in `load.py`.)\r\n> \r\n> `function_deserialization.setup_bare_concrete_function()` looks up the already-deserialized `ConcreteFunction` object, then calls that object's `add_to_graph()` method (line 172 below).\r\n> \r\n> (in `tensorflow/python/saved_model/function_deserialization.py`)\r\n> \r\n> ```python\r\n> 159 def setup_bare_concrete_function(saved_bare_concrete_function,\r\n> 160                                 concrete_functions):\r\n> 161  \"\"\"Makes a restored bare concrete function callable.\"\"\"\r\n> 160  # Bare concrete functions accept only flat lists of Tensors with unique\r\n> 163  # names.\r\n> 164  concrete_function = concrete_functions[\r\n> 165      saved_bare_concrete_function.concrete_function_name]\r\n> 166  # pylint: disable=protected-access\r\n> 167  concrete_function._arg_keywords = (\r\n> 168      saved_bare_concrete_function.argument_keywords)\r\n> 169  concrete_function._num_positional_args = (\r\n> 170      saved_bare_concrete_function.allowed_positional_arguments)\r\n> 171  # pylint: enable=protected-access\r\n> 172  concrete_function.add_to_graph()     <<<<<<<<<<\r\n> 173  return concrete_function\r\n> ```\r\n> \r\n> Of course, the code that created the `ConcreteFunction` object in the first place has already added it to the graph twice.\r\n> \r\n> And due to same the root cause as leaks 1 and 2, the `ConcreteFunction`'s graph pointer is set to `None`, so `ConcreteFunction.add_to_graph()` calls `_EagerDefinedFunction.add_to_graph()`, which adds the function to the eager execution context a third time.\r\n> \r\n> Unfortunately, the `Loader` class discards all information about what functions it has given this special treatment to, so a workaround along the lines of what I posted in my previous comment is not going to work. Here's my new workaround, which involves live-patching the function deletion callbacks in the background thread's global default graph:\r\n> \r\n> ```python\r\n> class DeleteWithExtremePrejudice(object):\r\n>   \"\"\"\r\n>   A version of _EagerDefinedFunctionDeleter (see\r\n>   tensorflow/python/eager/function.py) that keeps deleting the target function\r\n>   until an InvalidArgumentError exception is thrown.  Checking for that\r\n>   exception is the only way to ensure that a function really has been deleted\r\n>   and is not, in fact, still taking up memory.\r\n>   \"\"\"\r\n> \r\n>   def __init__(self, func_name):\r\n>     self.func_name = func_name\r\n> \r\n>   def __del__(self):\r\n>     MAX_ATTEMPTS = 10\r\n>     try:\r\n>       for i in range(MAX_ATTEMPTS):\r\n>         tf.python.context.remove_function(self.func_name)\r\n>       # If we get here, removal did *not* fail as expected.\r\n>       print(f\"WARNING: Failed to remove function \"\r\n>             f\"'{self.func_name}' after {MAX_ATTEMPTS} attempts. \"\r\n>             f\"This problem may result in a memory leak.\")\r\n>     except tf.errors.InvalidArgumentError:\r\n>       # tf.python.context.remove_function() throws this exception when\r\n>       # you try to remove a function that has already been removed.\r\n>       # In the case of this `try` clause, such behavior is \"normal\".\r\n>       pass\r\n>     except Exception as e:\r\n>       print(f\"WARNING: {e} thrown when attempting to delete function \"\r\n>             f\"'{self.func_name}'. This problem may result in a memory leak.\")\r\n>   \r\n> \r\n> def load_model_hack(saved_model_path: str):\r\n>   \"\"\"\r\n>   Load a SavedModel without leaking memory.\r\n>   \r\n>   This function applies two workarounds: \r\n>   * Load the model from a temporary background thread so that \r\n>     `saved_model.load()` won't leave garbage hanging off of the global \r\n>     default graph\r\n>   * Patch the garbage collection callbacks of all `ConcreteFunction`s\r\n>     in the returned model so that these functions will be properly removed\r\n>     when the restored model goes out of scope.\r\n>   \"\"\"\r\n>   import threading\r\n>   from typing import Any, Dict\r\n>   def callback(path: str, result_holder: Dict[str, Any]) -> None:\r\n>     \"\"\"Callback function to be executed in a background thread\"\"\"\r\n>     try:\r\n>       model = tf.saved_model.load(path)\r\n> \r\n>       # Every function that was pinned two or more times should now be\r\n>       # in the current thread's global default graph variable.\r\n>       # Replace the deletion callbacks of these functions with a more\r\n>       # effective version.\r\n>       default_graph = tf.compat.v1.get_default_graph()\r\n>       for func in default_graph._functions.values():\r\n>         func._function_deleter = DeleteWithExtremePrejudice(func.name)\r\n>         # NOTE: This assignment will trigger the previous deletion callback.\r\n>         # That's ok, because every function in this list has been pinned \r\n>         # at least twice.\r\n> \r\n>       result_holder[\"model\"] = model\r\n> \r\n>     except Exception as e:\r\n>       result_holder[\"error\"] = e\r\n> \r\n>   # Call saved_model.load() in a background thread \r\n>   thread_results = {}\r\n>   t = threading.Thread(target=callback, args=[saved_model_path, thread_results])\r\n>   t.start()\r\n>   t.join()\r\n> ```\r\n> \r\n> After applying this workaround, my test program leaks significantly less memory than before, **but it still leaks memory.**\r\n\r\nWow when reading the third leak, I thought it would be hilarious if there is a fourth (though I think there would be a fifth), by any chance, have you documented the thinking and the tools used to track down these leaks?", "> Wow when reading the third leak, I thought it would be hilarious if there is a fourth (though I think there would be a fifth), by any chance, have you documented the thinking and the tools used to track down these leaks?\r\n\r\nMy main piece of advice in terms of tools is that the tools lie. TensorFlow is a very complex program with multiple heaps (Python heap, C heap, TensorFlow's own memory manager, and CUDA) and global data structures that can look like leaks at first glance. You need to use several leak checkers and cross-reference their outputs. For this particular issue, I used `tcmalloc`, `pprof`, Python's `gc` package, the PyCharm debugger, and some additional Python code to walk the heap from inside the program under test."]}, {"number": 40170, "title": "build failed on CentOS-7 @flatbuffers//:flatc' #1660 ", "body": "## Bug Report\r\nGetting the following error while building.\r\n\r\n```\r\nERROR: /build/external/flatbuffers/BUILD.bazel:60:1: \r\nLinking of rule '@flatbuffers//:flatc' failed (Exit 1)\r\n\r\n```\r\n\r\nRe: #1563\r\nthis bug also reported the same issue but with building docker. Here, I am not building any docker.\r\n\r\n\r\n\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., CentOS Linux 7)**:\r\n- **TensorFlow repo (source v2.2.0)**:\r\n\r\n### Describe the problem\r\nGetting the above build error\r\n### Exact Steps to Reproduce\r\nclone and checkout the tag v2.2.0\r\n./configure\r\n<accept all defaults>\r\n`bazel --output_base=$build_dir build --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Source code / logs\r\n```\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nERROR: /build/external/swig/BUILD.bazel:5:1: Linking of rule '@swig//:swig' failed (Exit 1)\r\nbazel-out/host/bin/external/swig/_objs/swig/allocate.o:allocate.cxx:function Allocate::~Allocate(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/swig/_objs/swig/contract.o:contract.cxx:function Contracts::~Contracts(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/swig/_objs/swig/lang.o:lang.cxx:function Language::~Language(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/swig/_objs/swig/module.o:module.cxx:function Swig_register_module(char const*, Language* (*)()) [clone .cold.0]: error: undefined reference to 'operator delete(void*, unsigned long)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 87.968s, Critical Path: 69.22s\r\nINFO: 4631 processes: 4631 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["@cheers00 \r\nPlease refer to this issue and let us know if it helps:\r\n#36170 ", "@Saduf2019 thank you.\r\nI tried following build options mentioned in #36170 but I still get the same error.\r\n```\r\n--linkopt=-lm  \r\n--host_linkopt=-lm\r\n```\r\n\r\nBTW, this is happening in CentOS only. I could build without any problem on Ubuntu 20.04.", "I get the same issue on centos 7. My gcc version is 7.3, and I am using the master branch for the build. I added flags for mkl and avx enablement, and --host-linkopt=-lm & --linkopt=-lm option, but that didn't help either.", "@cheers00,\r\n\r\nHere's a similar [issue](https://github.com/bazelbuild/bazel/issues/10575#issuecomment-592647616) where they explained the cause may be due to incompatible version of different tools in build environment. We recommend you build the latest stable version of tensorflow i.e 2.6.0 using tested configurations in this [guide](https://www.tensorflow.org/install/source#tested_build_configurations) and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40170\">No</a>\n"]}, {"number": 40169, "title": "ensure model initialized on ANY trackable attr set", "body": "In particular, empty tuples should not trigger this.", "comments": ["Mine"]}, {"number": 40168, "title": "linking error while building TensorFlow Lite C++ image classification demo for Android", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite/examples/label_image:label_image\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /home/ambuj/try/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/ambuj/try/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/ambuj/try/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/ambuj/try/env/bin/python3 --action_env PYTHON_LIB_PATH=/home/ambuj/try/env/lib/python3.6/site-packages --python_path=/home/ambuj/try/env/bin/python3 --config=xla --action_env ANDROID_NDK_HOME=/opt/android-ndk --action_env ANDROID_NDK_API_LEVEL=27 --action_env ANDROID_BUILD_TOOLS_VERSION=29.0.2 --action_env ANDROID_SDK_API_LEVEL=28 --action_env ANDROID_SDK_HOME=/opt/android-sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/ambuj/try/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/ambuj/try/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm64 in file /home/ambuj/try/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /home/ambuj/try/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/ambuj/.cache/bazel/_bazel_ambuj/9eebf77bd6558b95e6f902cd1dc311ce/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - <builtin>\r\n - /home/ambuj/.cache/bazel/_bazel_ambuj/9eebf77bd6558b95e6f902cd1dc311ce/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /home/ambuj/try/tensorflow/WORKSPACE:37:1\r\nINFO: Analyzed target //tensorflow/lite/examples/label_image:label_image (0 packages loaded, 1926 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/ambuj/try/tensorflow/tensorflow/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/lite/examples/label_image:label_image' failed (Exit 1)\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/experimental/delegates/hexagon/builders/libop_builder.a(transpose_conv_2d_builder.o): In function `tflite::delegates::hexagon::TransposeConv2dOpBuilder::PopulateSubGraph(TfLiteIntArray const*, TfLiteIntArray const*, TfLiteContext*)':\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/transpose_conv_2d_builder.cc:90: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/transpose_conv_2d_builder.cc:90: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/transpose_conv_2d_builder.cc:147: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/transpose_conv_2d_builder.cc:147: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/transpose_conv_2d_builder.cc:200: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/experimental/delegates/hexagon/builders/libop_builder.a(transpose_conv_2d_builder.o):/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/transpose_conv_2d_builder.cc:200: more undefined references to `tflite::delegates::hexagon::OpBuilder::kScalarShape' follow\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/libarguments.a(arguments.o): In function `char const* std::__ndk1::__search_substring<char, std::__ndk1::char_traits<char> >(char const*, char const*, char const*, char const*)':\r\n/proc/self/cwd/external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/__string:(.text+0x230c): undefined reference to `tflite::gpu::cl::Arguments::kArgsPrefix'\r\n/proc/self/cwd/external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/__string:(.text+0x2310): undefined reference to `tflite::gpu::cl::Arguments::kArgsPrefix'\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/libarguments.a(arguments.o): In function `std::__ndk1::char_traits<char>::compare(char const*, char const*, unsigned long)':\r\n/proc/self/cwd/external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/__string:250: undefined reference to `tflite::gpu::cl::Arguments::kArgsPrefix'\r\n/proc/self/cwd/external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/__string:250: undefined reference to `tflite::gpu::cl::Arguments::kArgsPrefix'\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/libarguments.a(arguments.o): In function `char const* std::__ndk1::__search_substring<char, std::__ndk1::char_traits<char> >(char const*, char const*, char const*, char const*)':\r\n/proc/self/cwd/external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/__string:(.text+0x2b40): undefined reference to `tflite::gpu::cl::Arguments::kArgsPrefix'\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/gpu/cl/libarguments.a(arguments.o):/proc/self/cwd/external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/__string:(.text+0x2b44): more undefined references to `tflite::gpu::cl::Arguments::kArgsPrefix' follow\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/experimental/delegates/hexagon/builders/libop_builder.a(pack_builder.o): In function `tflite::delegates::hexagon::PackOpBuilder::PopulateSubGraph(TfLiteIntArray const*, TfLiteIntArray const*, TfLiteContext*)':\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/pack_builder.cc:47: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/pack_builder.cc:47: undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/pack_builder.cc:(.text+0x2d4): undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/pack_builder.cc:(.text+0x2d8): undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\n/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/pack_builder.cc:(.text+0x35c): undefined reference to `tflite::delegates::hexagon::OpBuilder::kScalarShape'\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/experimental/delegates/hexagon/builders/libop_builder.a(pack_builder.o):/proc/self/cwd/tensorflow/lite/experimental/delegates/hexagon/builders/pack_builder.cc:(.text+0x360): more undefined references to `tflite::delegates::hexagon::OpBuilder::kScalarShape' follow\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/lite/examples/label_image:label_image failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.811s, Critical Path: 0.31s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@snsvis this is not a label_image problem. I can reproduce the problem. It also happens when building benchmark with `bazel build --config android_arm64 //tensorflow/lite/tools/benchmark:benchmark_model`. A quick workaround is\r\n```diff\r\ndiff --git a/tensorflow/lite/delegates/gpu/cl/arguments.h b/tensorflow/lite/delegates/gpu/cl/arguments.h\r\nindex 453ffcb56b..2b24d7abf9 100644\r\n--- a/tensorflow/lite/delegates/gpu/cl/arguments.h\r\n+++ b/tensorflow/lite/delegates/gpu/cl/arguments.h\r\n@@ -32,6 +32,8 @@ namespace tflite {\r\n namespace gpu {\r\n namespace cl {\r\n \r\n+static constexpr char kArgsPrefix[] = \"args.\";\r\n+\r\n class Arguments {\r\n  public:\r\n   Arguments() = default;\r\n@@ -95,8 +97,6 @@ class Arguments {\r\n                           const std::vector<std::string>& member_names,\r\n                           std::string* code);\r\n \r\n-  static constexpr char kArgsPrefix[] = \"args.\";\r\n-\r\n   struct IntValue {\r\n     int value;\r\n \r\ndiff --git a/tensorflow/lite/experimental/delegates/hexagon/builders/op_builder.h b/tensorflow/lite/experimental/delegates/hexagon/builders/op_builder.h\r\nindex e130e6d2ef..7b7d25d46b 100644\r\n--- a/tensorflow/lite/experimental/delegates/hexagon/builders/op_builder.h\r\n+++ b/tensorflow/lite/experimental/delegates/hexagon/builders/op_builder.h\r\n@@ -49,6 +49,7 @@ struct OpNode {\r\n \r\n class GraphBuilder;\r\n \r\n+static constexpr int kScalarShape[] = {1, 1, 1, 1};\r\n // Represents a single Op in the TFLite graph.\r\n // For each op in TFLite there should be an OpBuidler, this builder is\r\n // responsible for constructing equivalent node(s) in the hexagon graph. A\r\n@@ -61,7 +62,6 @@ class GraphBuilder;\r\n class OpBuilder {\r\n  public:\r\n   // Const representing the shape of a scalar value.\r\n-  static constexpr int kScalarShape[] = {1, 1, 1, 1};\r\n \r\n   OpBuilder(GraphBuilder* graph_builder, int hexagon_op_type)\r\n       : graph_builder_(graph_builder) {\r\n\r\n```\r\n+@karimnosseir who made the changes according git log.\r\n\r\n@karimnosseir FYI. Someting weird happened. I don't know why, but the two class variables `Arguments::kArgsPrefix` and `OpBuilder::kScalarShape` are removed by the compiler/linker. So there are messages shown by @snsvis.", "Fix on the way. Sorry for the trouble", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40168\">No</a>\n"]}, {"number": 40167, "title": "GPU delegate not as accurate as NNAPI reference  on 16float models on Style Transfer example. tensorflow-lite", "body": "I was comparing the output of the transfer style app (while using the floating point models).\r\n\r\nAnd found the following results\r\n\r\nI/Style: Difference on average between CPU and  CPU: 8.049963811796872E-7\r\nI/Style: Difference on average between CPU and  GPU: 0.00804935239782874\r\nI/Style: Difference on average between CPU and  NNAPIREFERENCE: 8.941092843719466E-7\r\nI/Style: Difference on average between CPU and  NNAPIDEFAULT: 7.434606152762919E-7\r\nI/Style: Difference on average between CPU and  NNAPIGPU: 8.056053260209931E-7\r\nI/Style: Difference on average between CPU and  NNAPIDSP: 7.446350522072008E-7\r\nI/Style: Difference on average between CPU and  NNAPIHTA: 7.532546634042985E-7\r\n\r\n\r\nNNAPI Reference is much more accurate than GPU through gpu delegate. What exactly is the threshold or standard for correctness? \r\n\r\n\r\n**How to reproduce results.**\r\n\r\nI run the same noise through each interpreter, accelerated as specified.\r\n\r\nthe model produces an output of float[1][384][384][3]. I, basically, just take the difference between each float value and average them (sum and divide). I use the results from the CPU as the baseline, and compare.\r\n\r\nThe models used are attached. \r\n\r\n[style_float_models.zip](https://github.com/tensorflow/tensorflow/files/4732809/style_float_models.zip)\r\n\r\n\r\n", "comments": ["ping.", "Sorry, I started taking a look at your network but then it dropped off my radar.\r\n\r\nThis network structure of doing a mean, squared diff, rsqrt (instance norm) is quite sensitive to lower precision.  The errors in small numbers bubbles up quickly with this sequence.  This happens usually only in FP16.  Things may be okay if you use FP32.\r\n\r\nI remember having solved this problem by fixing some accumulators to FP32.  A fix should have gone out around March IIRC.  Are you still seeing this issue at HEAD?", "@amitDaMan Could you please refer as per the above [comment](https://github.com/tensorflow/tensorflow/issues/40167#issuecomment-683131276) and let us know if this issue still persists ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40167\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40167\">No</a>\n", "@impjdi I am recently facing the same issue on my model which contains the numbers of the sequence you mentioned above (mean, squarred diff, instance norm). \r\nThese are the results for the output diff tool. As you can see from the result, fp16 avg_error is 0.334995. But fp32 average error is much smaller with 6.36335e-06. Can you provide me any other workarounds I can solve this issue? How can I fix the accumulators to FP32? \r\nI still need fp16 because inference speed is much faster than fp32. Thanks in advance.\r\n\r\n```\r\nadb shell /data/local/tmp/run_eval \\\r\n    --model_file=/data/local/tmp/model.tflite \\\r\n    --delegate=gpu \\\r\n    --num_runs=5 \\\r\n    --gpu_precision_loss_allowed=true\r\nGPU delegate created.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Replacing 622 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nnative : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.\r\nnative : lite/tools/evaluation/stages/tflite_inference_stage.cc:128 \r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Replacing 388 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 336 partitions.\r\nnative : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.\r\nNum evaluation runs: 5\r\nReference run latency: avg=399408(us), std_dev=32415(us)\r\nTest run latency: avg=51941.7(us), std_dev=6460(us)\r\nOutputDiff[0]: avg_error=0.334995, std_dev=0.0490164\r\n```\r\n\r\n```\r\n\u279c  ~ adb shell /data/local/tmp/run_eval \\\r\n    --model_file=/data/local/tmp/model.tflite \\\r\n    --delegate=gpu \\\r\n    --num_runs=5 \\\r\n    --gpu_precision_loss_allowed=false  \r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nGPU delegate created.\r\nINFO: Replacing 622 node(s) with delegate (TfLiteGpuDelegateV2) node, yielding 1 partitions.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nnative : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.\r\nnative : lite/tools/evaluation/stages/tflite_inference_stage.cc:128 \r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Replacing 388 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 336 partitions.\r\nnative : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.\r\nNum evaluation runs: 5\r\nReference run latency: avg=405939(us), std_dev=40977(us)\r\nTest run latency: avg=112061(us), std_dev=3014(us)\r\nOutputDiff[0]: avg_error=6.36335e-06, std_dev=3.32635e-06\r\n```\r\n"]}, {"number": 40165, "title": "AttributeError: 'PerReplica' object has no attribute 'begin'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: Python 2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to distributed evaluation on 2 GPUs on my local dev server using Mirrored Strategy. But I'm getting errors as follows:\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 517, in _actual_eval\r\n    return _evaluate()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 508, in _evaluate\r\n    output_dir=self.eval_dir(name))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1609, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/evaluation.py\", line 269, in _evaluate_once\r\n    session_creator=session_creator, hooks=hooks) as session:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 713, in __init__\r\n    h.begin()\r\n**AttributeError: 'PerReplica' object has no attribute 'begin'**\r\n```\r\n\r\nI also noticed I'm having \"not used by distribute strategy\" error in the log (which I don't have in distributed training using mirrored strategy):\r\n```\r\n2020-06-04 18:40:02.149347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 14006 MB memory) -> physical GPU (device: 0, name: Quadro RTX 5000, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2020-06-04 18:40:02.149933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 14039 MB memory) -> physical GPU (device: 1, name: Quadro RTX 5000, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n**INFO: tensorflow: Device is available but not used by distribute strategy: /device:CPU:0**\r\n```\r\n\r\nMy distributed model evaluation is as follows:\r\n```\r\nstrategy = tf.compat.v1.distribute.MirroredStrategy()\r\n\r\n self.run_config = tf.estimator.RunConfig(\r\n        model_dir=self.job_dir,\r\n        save_summary_steps=self.save_summary_steps,\r\n        session_config=session_config,\r\n        eval_distribute=strategy,\r\n)\r\n\r\nestimator = tf.estimator.Estimator(\r\n    model_fn=eval_model_fn,\r\n    config=self.run_config\r\n)\r\n\r\neval_result = estimator.evaluate(\r\n    input_fn=input_tf_dataset,\r\n    steps=eval_steps,\r\n    name=self.eval_name,\r\n    hooks=hooks,\r\n    checkpoint_path=weights_path\r\n)          \r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI expected distributed evaluation shall work, because I also have distributed training + evaluation working properly as follows:\r\n```\r\nself.run_config = tf.estimator.RunConfig(\r\n    model_dir=self.job_dir,\r\n    save_checkpoints_steps=self.save_checkpoints_steps,\r\n    save_checkpoints_secs=self.save_checkpoints_secs,\r\n    keep_checkpoint_max=self.keep_checkpoint_max,\r\n    save_summary_steps=self.save_summary_steps,\r\n    session_config=session_config,\r\n    train_distribute=distributed_strategy,\r\n)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(\r\n    input_fn=train_input_tf_dataset\r\n    max_steps=max_steps,\r\n    hooks=train_hooks,\r\n)\r\neval_spec = tf.estimator.EvalSpec(\r\n    input_fn=eval_input_tf_dataset\r\n    steps=eval_steps,\r\n    name=self.eval_name,\r\n    hooks=eval_hooks\r\n)\r\n\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nI don't have standalone code to reproduce the issue right now.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI also found this issue: https://github.com/tensorflow/tensorflow/issues/28018. But for my case, even if I remove hooks, I still get the similar error.", "comments": ["This issue is a duplicate of https://github.com/tensorflow/tensorflow/issues/28018. \r\n\r\nThe hooks passed into evaluate() function does cause any issue.\r\n```\r\neval_result = estimator.evaluate(\r\n    input_fn=input_tf_dataset,\r\n    steps=eval_steps,\r\n    name=self.eval_name,\r\n    hooks=hooks,\r\n    checkpoint_path=weights_path\r\n)          \r\n```\r\n\r\nIt's eval_hooks in EstimatorSpec causing the issue. If I comment out \"evaluation_hooks\", it's working fine.\r\n```\r\ndef eval_model_fn:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=ModeKey.EVAL,\r\n            evaluation_hooks=eval_hooks,\r\n        )    \r\n\r\nestimator = tf.estimator.Estimator(\r\n    model_fn=eval_model_fn,\r\n    config=self.run_config\r\n)\r\n```", "This has been fixed (https://github.com/tensorflow/estimator/commit/131f54a62ae9ded9057aeb0eb1243d9516373b14). Please test with TF nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40165\">No</a>\n"]}, {"number": 40164, "title": "[ROCm] Fix for the build error in ROCm CSB - 200604", "body": "The folllowing commit introduced build errors on the ROCm platform\r\nhttps://github.com/tensorflow/tensorflow/commit/cf30d41ded3b5adae5c3bf3c7e05ce8bb7066a9c\r\n\r\nThose initial build errors are addressed by the following commit, but the build is still broken\r\nhttps://github.com/tensorflow/tensorflow/commit/a1e26e4298aac041e56c856c40bcb0c29fbc3f83\r\n\r\nError we get after the second commit\r\n\r\n```\r\nIn file included from tensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:24:\r\nIn file included from ./tensorflow/core/util/gpu_kernel_helper.h:25:\r\n./tensorflow/core/util/gpu_device_functions.h:754:10: error: no matching function for call to 'atomicMax'\r\n  return atomicMax(ptr, value);\r\n         ^~~~~~~~~\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:61:5: note: in instantiation of function template specialization 'tensorflow::GpuAtomicMax<long long, long long>' requested here\r\n    GpuAtomicMax(out, val);\r\n    ^\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:118:9: note: in instantiation of member function 'tensorflow::(anonymous namespace)::LeftUpdate<long long, tensorflow::scatter_nd_op::UpdateOp::MAX>::operator()' requested here\r\n        update(out + i + si, ldg(updates + (index * slice_size + si)));\r\n        ^\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc:156:33: note: in instantiation of function template specialization 'tensorflow::ScatterNdOpKernel<long long, int, tensorflow::scatter_nd_op::UpdateOp::MAX, 1>' requested here\r\n    TF_CHECK_OK(GpuLaunchKernel(ScatterNdOpKernel<T, Index, op, IXDIM>,\r\n                                ^\r\n/opt/rocm-3.3.0/hip/include/hip/hcc_detail/hip_atomic.h:178:5: note: candidate function not viable: no known conversion from 'long long *' to 'int *' for 1st argument\r\nint atomicMax(int* address, int val)\r\n    ^\r\n/opt/rocm-3.3.0/hip/include/hip/hcc_detail/hip_atomic.h:184:14: note: candidate function not viable: no known conversion from 'long long *' to 'unsigned int *' for 1st argument\r\nunsigned int atomicMax(unsigned int* address, unsigned int val)\r\n             ^\r\n/opt/rocm-3.3.0/hip/include/hip/hcc_detail/hip_atomic.h:190:20: note: candidate function not viable: no known conversion from 'long long *' to 'unsigned long long *' for 1st argument\r\nunsigned long long atomicMax(\r\n                   ^\r\n```\r\n\r\nThis PR fixes the build by adding the following type specializations for the `long long` type\r\n * `GpuAtomicCasHelper`\r\n * `GpuAtomicMax`\r\n * `GpuAtomicMin`\r\n\r\n\r\n------------------------\r\n\r\n/cc @whchung @ekuznetsov139 @chsigg @cheshire @nvining-work ", "comments": []}, {"number": 40163, "title": "Save and load custom optimizers for continued training", "body": "My question is essentially the exact same as that specified [here](https://stackoverflow.com/questions/49503748/save-and-load-model-optimizer-state) but without using the Keras backend. Namely, how does one save and restore custom optimizers to their last state in TensorFlow (e.g. [`L-BFGS-B`](http://tensorflow.biotecan.com/python/Python_1.8/tensorflow.google.cn/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface.html), Adam) when continuing training? Is this currently supported?\r\n\r\nAs per the solution [here](https://stackoverflow.com/questions/43243527/python-tensorflow-how-to-restart-training-with-optimizer-and-import-meta-graph) for the Adam optimizer specifically, it appears one approach is to use `tf.add_collection` and `tf.get_collection`, but that appears to not work if I need to restore the optimizer in a new session/shell. I have written a simple test code below where I am able to to save and restore the neural network itself after training, *but* the optimizers and global counter variable are not starting from where they last ended if I launch the script from a new session/shell. Therefore, insights into restoring optimizers when continuing training via this script are sought. The goal is to save the optimizers and properly continue training from where the optimizers last ended when the code is launched again.\r\n```\r\nimport numpy as np \r\nimport tensorflow as tf\r\n\r\npath_save = '/home/mathewsa/stored_models/' #custom path to save network\r\nsave_model = str(path_save)+\"test_save.ckpt\"\r\nend_it = 1000 #number of iterations\r\nfrac_train = 1.0 #randomly sampled fraction of data to create training set\r\nfrac_sample_train = 0.01 #randomly sampled fraction of data from training set to train in batches\r\nlayers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\r\n\r\n#Generate training data\r\nlen_data = 10000\r\nx_x = np.array([np.linspace(0.,1.,len_data)])\r\nx_y = np.array([np.linspace(0.,1.,len_data)]) \r\ny_true = np.array([np.linspace(-1.,1.,len_data)])\r\n\r\nN_train = int(frac_train*len_data)\r\nidx = np.random.choice(len_data, N_train, replace=False)\r\n\r\nx_train = x_x.T[idx,:]\r\ny_train = x_y.T[idx,:] \r\nv1_train = y_true.T[idx,:] \r\n\r\nsample_batch_size = int(frac_sample_train*N_train)\r\n\r\nnp.random.seed(1234)\r\ntf.set_random_seed(1234)\r\nimport logging\r\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\nclass NeuralNet:\r\n    def __init__(self, x, y, v1, layers):\r\n        X = np.concatenate([x, y], 1)  \r\n        self.lb = X.min(0)\r\n        self.ub = X.max(0)\r\n        self.X = X\r\n        self.x = X[:,0:1]\r\n        self.y = X[:,1:2] \r\n        self.v1 = v1 \r\n        self.layers = layers \r\n        self.weights_v1, self.biases_v1 = self.initialize_NN(layers) \r\n        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=False,\r\n                                                     log_device_placement=False)) \r\n        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\r\n        self.y_tf = tf.placeholder(tf.float32, shape=[None, self.y.shape[1]]) \r\n        self.v1_tf = tf.placeholder(tf.float32, shape=[None, self.v1.shape[1]])  \r\n        self.v1_pred = self.net(self.x_tf, self.y_tf) \r\n        self.loss = tf.reduce_mean(tf.square(self.v1_tf - self.v1_pred)) \r\n        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\r\n                                                                var_list=self.weights_v1+self.biases_v1,\r\n                                                                method = 'L-BFGS-B',\r\n                                                                options = {'maxiter': 50,\r\n                                                                           'maxfun': 50000,\r\n                                                                           'maxcor': 50,\r\n                                                                           'maxls': 50,\r\n                                                                           'ftol' : 1.0 * np.finfo(float).eps})\r\n        self.optimizer_Adam = tf.train.AdamOptimizer()\r\n        self.train_op_Adam_v1 = self.optimizer_Adam.minimize(self.loss, var_list=self.weights_v1+self.biases_v1) \r\n        self.saver = tf.train.Saver()\r\n        init = tf.global_variables_initializer()  \r\n        self.sess.run(init)\r\n    def initialize_NN(self, layers):\r\n        weights = []\r\n        biases = []\r\n        num_layers = len(layers)\r\n        for l in range(0,num_layers-1):\r\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\r\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\r\n            weights.append(W)\r\n            biases.append(b) \r\n        return weights, biases\r\n    def xavier_init(self, size):\r\n        in_dim = size[0]\r\n        out_dim = size[1]\r\n        xavier_stddev = np.sqrt(2/(in_dim + out_dim)) \r\n        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\r\n    def neural_net(self, X, weights, biases):\r\n        num_layers = len(weights) + 1\r\n        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\r\n        for l in range(0,num_layers-2):\r\n            W = weights[l]\r\n            b = biases[l]\r\n            H = tf.tanh(tf.add(tf.matmul(H, W), b))\r\n        W = weights[-1]\r\n        b = biases[-1]\r\n        Y = tf.add(tf.matmul(H, W), b) \r\n        return Y\r\n    def net(self, x, y): \r\n        v1_out = self.neural_net(tf.concat([x,y], 1), self.weights_v1, self.biases_v1)\r\n        v1 = v1_out[:,0:1]\r\n        return v1\r\n    def callback(self, loss):\r\n        global Nfeval\r\n        print(str(Nfeval)+' - Loss in loop: %.3e' % (loss))\r\n        Nfeval += 1\r\n    def fetch_minibatch(self, x_in, y_in, v1_in, N_train_sample):  \r\n        idx_batch = np.random.choice(len(x_in), N_train_sample, replace=False)\r\n        x_batch = x_in[idx_batch,:]\r\n        y_batch = y_in[idx_batch,:] \r\n        v1_batch = v1_in[idx_batch,:] \r\n        return x_batch, y_batch, v1_batch\r\n    def train(self, end_it): \r\n        saver = tf.train.Saver()\r\n        print('Stage 4.20')\r\n        try:\r\n            saver.restore(self.sess, save_model) \r\n            print('Using previous model')\r\n        except:\r\n            self.Nfeval = 1\r\n            print('No previous model') \r\n        it = 0\r\n        while it < end_it: \r\n            x_res_batch, y_res_batch, v1_res_batch = self.fetch_minibatch(self.x, self.y, self.v1, sample_batch_size) # Fetch residual mini-batch\r\n            tf_dict = {self.x_tf: x_res_batch, self.y_tf: y_res_batch,\r\n                       self.v1_tf: v1_res_batch}\r\n            self.sess.run(self.train_op_Adam_v1, tf_dict)\r\n            self.optimizer.minimize(self.sess,\r\n                                    feed_dict = tf_dict,\r\n                                    fetches = [self.loss],\r\n                                    loss_callback = self.callback) \r\n            it = it + 1\r\n        self.save_path = saver.save(self.sess, save_model)\r\n        print('Finishing up training and saving as: ') \r\n        print(save_model) \r\n    def restore_model(self, path_full_saved):\r\n        saver = tf.train.Saver()\r\n        print('Stage 4.20')\r\n        try:\r\n            saver.restore(self.sess, str(path_full_saved))\r\n            print('Using previous model')\r\n        except:\r\n            print('No previous model')\r\n    def predict(self, x_star, y_star): \r\n        tf_dict = {self.x_tf: x_star, self.y_tf: y_star}\r\n        v1_star = self.sess.run(self.v1_pred, tf_dict)  \r\n        return v1_star\r\n\r\nmodel = NeuralNet(x_train, y_train, v1_train, layers)\r\n \r\nNfeval = 1\r\nmodel.train(end_it)\r\n```", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 40162, "title": "Can't build the binary for Sparkfun Edge Examples", "body": "I'm new to this but after following the instructions to flash the SparkFun Edge with the hello world example it can't create the binary file. After entering \r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin\r\n\r\nThese errors appear after downloading the dependencies:\r\n\r\n1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: ELF: not found\r\ntensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: 1: tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin//arm-none-eabi-g++: Syntax error: Unterminated quoted string\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:300: tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main.o] Error 2\r\n\r\nThis has also happened with the other examples. The folders are created (/tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world) but not the hello_world.bin file. \r\n\r\nThank you.\r\n\r\nI'm using Rapbian on Pi4", "comments": ["Just realised it is likely not able to run on raspberry pi (ARM) so will close."]}, {"number": 40161, "title": "<tf.Variable 'Variable:0' shape=(1, 56, 56, 256) dtype=float32, numpy= array([[[[0.        , 3.6259902 , 3.3980963 , ...,        nan,                  nan,        nan],", "body": "Does anyone ever experience like this? Please give enlightenment, thank you\r\n\r\n![image](https://user-images.githubusercontent.com/58981061/83787982-cce86200-a6be-11ea-86be-93c587a5e45a.png)\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nCan you please elaborate about the issue & the context.Will it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "\u0e09\u0e31\u0e19\u0e2d\u0e49\u0e2d\u0e19\u0e27\u0e2d\u0e19 \u0e14\u0e49\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e40\u0e21\u0e15\u0e32 \u0e09\u0e31\u0e19\u0e44\u0e21\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e23\u0e38\u0e49\u0e21\u0e32\u0e01\u0e43\u0e19\u0e23\u0e30\u0e1a\u0e1a\u0e42\u0e0b\u0e40\u0e0a\u0e35\u0e22\u0e25 \u0e09\u0e31\u0e19\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e38\u0e19\u0e2a\u0e33\u0e23\u0e2d\u0e07\u0e40\u0e25\u0e35\u0e49\u0e22\u0e07\u0e1a\u0e38\u0e15\u0e23\u0e41\u0e25\u0e30\u0e40\u0e23\u0e34\u0e48\u0e21\u0e01\u0e34\u0e08\u0e01\u0e32\u0e23\u0e40\u0e25\u0e49\u0e01\u0e46\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 \u0e43\u0e19\u0e20\u0e39\u0e21\u0e34\u0e20\u0e32\u0e04\u0e34\u0e17\u0e35\u0e48\u0e09\u0e31\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e40\u0e15\u0e47\u0e1a\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22\u0e01\u0e25\u0e25\u0e27\u0e07\u0e08\u0e19\u0e09\u0e31\u0e19\u0e41\u0e25\u0e30\u0e04\u0e23\u0e2d\u0e1a\u0e04\u0e23\u0e31\u0e27\u0e25\u0e33\u0e1a\u0e32\u0e01\u0e2b\u0e21\u0e14\u0e17\u0e32\u0e07\u0e41\u0e01\u0e49\u0e44\u0e02 \u0e09\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e2b\u0e21\u0e49\u0e32\u0e22 \u0e41\u0e15\u0e48\u0e09\u0e31\u0e19\u0e21\u0e35\u0e07\u0e32\u0e19\u0e17\u0e35\u0e48\u0e14\u0e35 \u0e2a\u0e38\u0e08\u0e23\u0e34\u0e15\u0e41\u0e25\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e42\u0e25\u0e01\u0e42\u0e0b\u0e40\u0e0a\u0e35\u0e22\u0e25 \u0e09\u0e31\u0e19\u0e04\u0e07\u0e40\u0e02\u0e49\u0e32\u0e23\u0e30\u0e1a\u0e1a\u0e1c\u0e34\u0e14 \u0e16\u0e39\u0e01\u0e1a\u0e49\u0e32\u0e07 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e02\u0e49\u0e32\u0e16\u0e36\u0e07\u0e01\u0e23\u0e30\u0e40\u0e1b\u0e4b\u0e32\u0e40\u0e07\u0e34\u0e19\u0e41\u0e25\u0e30\u0e2a\u0e34\u0e19\u0e40\u0e0a\u0e37\u0e48\u0e2d \u0e02\u0e2d\u0e1a\u0e04\u0e38\u0e13 \u0e41\u0e25\u0e30\u0e01\u0e25\u0e48\u0e32\u0e27\u0e02\u0e2d\u0e42\u0e17\u0e29    \u0e14\u0e49\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e19\u0e31\u0e1a\u0e16\u0e37\u0e2d  Wiphawee", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40161\">No</a>\n"]}, {"number": 40160, "title": "Unable to call \"image_dataset_from_directory\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): conda install tensorflow \r\n- TensorFlow version:- version 2.1.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\nAm following the commands on the tensir.io site ( https://keras.io/api/preprocessing/image/#image_dataset_from_directory-function )\r\nand when i try to do the imports of the required functions at the start of my program i get an error.  I have reviewed the directories and the image_dataset_from_directory is not in the folder so it didn't download as part of the package.  how can i get it or has it been discontinued?\r\n\r\nInitial commands:\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import preprocessing\r\nfrom tensorflow.keras.preprocessing.image import image_dataset_from_directory\r\n\r\nError: \r\n\r\nfrom tensorflow.keras.preprocessing.image import image_dataset_from_directory\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-11-7fae7ea40691>\", line 1, in <module>\r\n    from tensorflow.keras.preprocessing.image import image_dataset_from_directory\r\n\r\nImportError: cannot import name 'image_dataset_from_directory' from 'tensorflow.keras.preprocessing.image' (d:\\anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow_core\\python\\keras\\api\\_v2\\keras\\preprocessing\\image\\__init__.py)\r\n", "comments": ["The `tf.keras.preprocessing.image.image_dataset_from_directory` function is currently only available on the master branch. It is not yet a part of TF 2.2.\r\n\r\nIf you require this extra functionality in the code, consider using `tf-nightly` builds which can be installed using:\r\n\r\n```sh\r\npip install tf-nightly\r\n```\r\n\r\nRefer: https://github.com/keras-team/keras-io/issues/12#issuecomment-626220119\r\n\r\nThanks. ", "@Cillinc,\r\nAs per the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) of `tf.keras.preprocessing.image_dataset_from_directory\r\n` the API is new and is only available in TF-nightly. \r\n\r\nCould you please install TensorFlow nightly and check if you are facing the same issue. Please take a look at [this gist](https://colab.research.google.com/gist/amahendrakar/aed4310c0e8b98ab585a318a520533c1/40160.ipynb) for reference. Thanks!", "thanks i have upgraded pip to version 20.1.1 and used it to do a `pip install tf-nightly`  it completed correctly (had got an error ob the lower version of pip and i am still getting the same results - errors.\r\nfrom tensorflow.keras.preprocessing.image import image_dataset_from_directory\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-7fae7ea40691>\", line 1, in <module>\r\n    from tensorflow.keras.preprocessing.image import image_dataset_from_directory\r\n\r\nImportError: cannot import name 'image_dataset_from_directory' from 'tensorflow.keras.preprocessing.image' (d:\\anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\keras\\preprocessing\\image\\__init__.py)\r\n\r\n\r\n\r\n![Capture - tf-nightly 1](https://user-images.githubusercontent.com/33866780/83802569-e8de0a80-a6a2-11ea-92a6-034f9978369b.PNG)\r\n![Capture - pip upgrade and tf-nightly 2](https://user-images.githubusercontent.com/33866780/83802578-eb406480-a6a2-11ea-8bd1-0af12f9ed70c.PNG)\r\n![Capture keras_preprocessing dir](https://user-images.githubusercontent.com/33866780/83802580-ed0a2800-a6a2-11ea-86ab-bd9eaed906a6.PNG)\r\n\r\n\r\n", "think i found the solution on one of your github postings - well concealed...\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import preprocessing\r\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\r\n\r\nlooks like the text on keras.io where i got the script might need a slight adjustment", "> think i found the solution on one of your github postings - well concealed...\r\n> \r\n> import tensorflow as tf\r\n> from tensorflow import keras\r\n> from tensorflow.keras import preprocessing\r\n> from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n> \r\n> looks like the text on keras.io where i got the script might need a slight adjustment\r\n\r\ndon'T know who you are but thank you so much for this info", "> think i found the solution on one of your github postings - well concealed...\r\n> \r\n> import tensorflow as tf\r\n> from tensorflow import keras\r\n> from tensorflow.keras import preprocessing\r\n> from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n> \r\n> looks like the text on keras.io where i got the script might need a slight adjustment\r\n\r\nThis also wont work. you have to use tf-nightly only.", "> > think i found the solution on one of your github postings - well concealed...\r\n> > import tensorflow as tf\r\n> > from tensorflow import keras\r\n> > from tensorflow.keras import preprocessing\r\n> > from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n> > looks like the text on keras.io where i got the script might need a slight adjustment\r\n> \r\n> This also wont work. you have to use tf-nightly only.\r\n\r\nTry import it like this: - \r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n\r\nThen, use it like this: - \r\ntrain_images = ImageDataGenerator().flow_from_directory(train_path,target_size=(width,height))", "I solved using:  \"tf.keras.preprocessing.image_dataset_from_directory\"\r\nObs.: import tensorflow as tf"]}, {"number": 40158, "title": "[compiler] Fix segmentation fault in segment graph", "body": "Signed-off-by: Gaurav Singh <gaurav1086@gmail.com>\r\n\r\nThe function: graph->FindNodeId(i) can return a nullptr for invalid i (node_id)\r\n```\r\n  SimpleNode* FindNodeId(int node_id) {\r\n    if (node_id < 0 || node_id > static_cast<int>(nodes_.size())) {\r\n      return nullptr;\r\n    }    \r\n    return nodes_[node_id];\r\n  }\r\n```\r\nThe returned node is dereferenced multiple times in the caller thereby causing segmentation fault. To fix this, add the null check and if null/invalid, continue/skip the node right away", "comments": ["@bixia1 Can you please review this PR ? Thanks!"]}, {"number": 40157, "title": "TFLite: Cannot run inference on TF Lite Model: \"Regular TensorFlow ops are not supported by this interpreter.\" ", "body": "**System information**\r\n- OSX\r\n- TF 2.3.0-dev20200602\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nConversion code:\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(curr_dir + \"saved_model\")\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the TF Lite model.\r\n    with tf.io.gfile.GFile(curr_dir + '/model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\n\r\nInference code:\r\n```\r\n    # Compare Inference\r\n    import tensorflow as tf\r\n\r\n    # Load the TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n```\r\nThe model I'm trying to convert to tflite and run inference on is SSDLite_MobileNetV2, obtained rom the Model Zoo:\r\n\r\nhttp://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\r\n\r\n**Failure details**\r\n\r\nConversion is successful, however I cannot run inference: Here is the error that I run into:\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. \r\nMake sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.\r\n```\r\n\r\nI've been playing around with converter settings with no luck\r\ni.e. combinations of: \r\n```    \r\n    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    #                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nWith none of the settings above set, or the supported_ops set, I can convert the model but cannot run inference, with a similar error as above.\r\nWith optimizations set to default, it gives me an error in conversion", "comments": ["Using flex delegate in python is not yet supported https://www.tensorflow.org/lite/guide/ops_select#python_pip_package\r\nHowever, the feature will be landed really soon, might be sometimes next week, so please wait a little bit.", "Thanks for the information. I'll keep a watch ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157\">No</a>\n", "Is there an approximate ETA on Python support for flex delegate?", "The PR got all required approval. I think it won't take too long.", "The feature is delivered at the HEAD of master. aselva-eb you can try it now.", "@thaink Would you please post a sample code of how to use this in ```Interpreter```? ", "Thank you @thaink for continuing to give me updates on this - I appreciate it very much!\r\n\r\nI tried to update my Tensorflow (using tf-nightly to get latest HEAD of master) and run inference on a model with both ops - however I still run into the same error. Perhaps there's a flag in Interpreter that needs to be set to enable flex ops?\r\n\r\nHere's the code I'm using for inference:\r\n\r\n```\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n```\r\n\r\nError happens on `allocate_tensors()` function:\r\nHere is the error:\r\n```Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.```", "Hi aselva-eb,\r\nThere was a regression with the PR so it got rolled back and just re-submitted yesterday.\r\nCan you give it a try again?\r\nThere should be no additional step to use Flex delegate.", "@thaink still no luck. Was it rolled-back again by any chance? I'm on: `tf-nightly-2.3.0.dev20200619`", "aselva-eb,\r\nThe PR was not rolled back anymore. \r\nCould you send me your converted model so I can do a check.", "This is absolutely fabulous! First tests on tf-nightly 06-22  seem to work perfectly. I have not evaluated the results of the converted models, but FlexDelegate loads automatically for models that need it.", "Great to hear that.", "@thaink - I've attached the converted model:\r\n\r\n[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/4819404/model.tflite.zip)\r\n", "> Thank you @thaink for continuing to give me updates on this - I appreciate it very much!\r\n> \r\n> I tried to update my Tensorflow (using tf-nightly to get latest HEAD of master) and run inference on a model with both ops - however I still run into the same error. Perhaps there's a flag in Interpreter that needs to be set to enable flex ops?\r\n> \r\n> Here's the code I'm using for inference:\r\n> \r\n> ```\r\n> # Load the TFLite model and allocate tensors.\r\n> interpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\n> interpreter.allocate_tensors()\r\n> \r\n> # Get input and output tensors.\r\n> input_details = interpreter.get_input_details()\r\n> output_details = interpreter.get_output_details()\r\n> ```\r\n> \r\n> Error happens on `allocate_tensors()` function:\r\n> Here is the error:\r\n> `Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.`\r\n\r\nhave you fixed it yet? I have the same issue :((((", "In my test today. It is working.\r\n\r\n[test_flex.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4829629/test_flex.ipynb.zip)\r\n", "> In my test today. It is working.\r\n> \r\n> [test_flex.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4829629/test_flex.ipynb.zip)\r\n\r\nHi @thaink \r\nI tried your notebook with a fresh environment and wasn't able to get it to work. Still get the same issue:\r\n\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.\r\n```\r\n\r\nVersion of tf-nightly: `2.3.0-dev20200625` running with Python 3.7\r\n\r\nThis very odd...\r\n\r\nCan you re-try with a fresh environment and let me know? ", "Hi @thaink just wanted to follow up with ^. Are you able to reproduce your results with a fresh environment?", "Hi aselva-eb,\r\nI tried a fresh environment with venv today and it is still working fine.\r\nCan you try VenV? Are you getting the error at interpreter.allocate_tensors()?\r\n\r\n", "> Hi aselva-eb,\r\n> I tried a fresh environment with venv today and it is still working fine.\r\n> Can you try VenV? Are you getting the error at interpreter.allocate_tensors()?\r\n\r\nOh really??? No I'm using Conda.. \r\nYes, my error is on allocate_tensors()", "I installed tf-nightly with Venv and run the following script:\r\nimport tensorflow as tf\r\ntf.__version__\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\ninterpreter.allocate_tensors()\r\nprint(\"all ok\")\r\n", "> I installed tf-nightly with Venv and run the following script:\r\n> import tensorflow as tf\r\n> tf.**version**\r\n> \r\n> interpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\n> interpreter.allocate_tensors()\r\n> print(\"all ok\")\r\n\r\nI tried the above with Venv and got the following:\r\n```\r\n2.5.0-dev20200629\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 7, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/Users/abc/test/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 243, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3 (FlexTensorArrayV3) failed to prepare.\r\n```\r\n\r\nWhat version of Python are you running? OS?\r\nI'm on Python 3.7.7 with Mac OS Mojave", "Oh, Interesting.\r\nI just tried on a linux docker container, and here's what I get:\r\n\r\n```\r\n2020-06-30 16:58:39.637247: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-06-30 16:58:39.637334: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2.5.0-dev20200629\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-06-30 16:58:41.132576: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-06-30 16:58:41.140496: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2208000000 Hz\r\n2020-06-30 16:58:41.141281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x464c640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-30 16:58:41.141333: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-30 16:58:41.147742: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-06-30 16:58:41.147801: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-06-30 16:58:41.147832: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (beb39de6fc37): /proc/driver/nvidia/version does not exist\r\nINFO: TfLiteFlexDelegate delegate: 28 nodes delegated out of 233 nodes with 6 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 6 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 6 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 8 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 7 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 99 nodes delegated out of 3383 nodes with 3 partitions.\r\n\r\nall ok\r\n```\r\n\r\nMust be something funny between the two platforms installation of tf_nightly. (OSX Mojave vs Linux)\r\n\r\nAre those errors at the top concerning? Looks like it's just trying to run on GPU when the drivers are not accessible.\r\n\r\n\r\nI tried running the invoke command, but here's the error I get now:\r\n\r\n```\r\n2020-06-30 17:01:47.430400: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 19, in <module>\r\n    interpreter.invoke()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 524, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n\t (while executing 'TensorArrayScatterV3' via Eager)Node number 233 (TfLiteFlexDelegate) failed to invoke.\r\n```\r\n\r\n", "I wonder if the above invoke error is as result of an TF1 model that is incompatible with TF2.", "> I wonder if the above invoke error is as result of an TF1 model that is incompatible with TF2.\r\n\r\nI don't think so. Let me ask another what is different between python package of Linux and MacOS.", "@terryheo \r\nHi Terry, Is there any differences between pip package for Linux and MacOS?\r\nThe flex delegate works on linux but seems to fail on MacOS.", "It's currently only works with Linux.\r\nMacOS and Windows support isn't ready yet. (I have a plan to do it)", "That explains it. Thanks for your help, all. :)\r\n", "@thaink , Although I can get a few steps farther with using a Linux Docker Image instead of MacOS, I still have issues when running invoke on the model I downloaded from the Tensorflow Object Detection Model Zoo.. \r\nCould it be a model compatibility issue? Or does this look like something specific to Flex delegate support?  \r\n(See log here: https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-651923086)", "> @thaink , Although I can get a few steps farther with using a Linux Docker Image instead of MacOS, I still have issues when running invoke on the model I downloaded from the Tensorflow Object Detection Model Zoo..\r\n> Could it be a model compatibility issue? Or does this look like something specific to Flex delegate support?\r\n> (See log here: [#40157 (comment)](https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-651923086))\r\n\r\nI get the same error and I want to know that is this because TFLite has not supported it now?", "@aselva-eb is the python script the same? Or can you explain the steps to reproduce this?\r\nFlex delegate should only use CPU. Seems like it is trying to use Cuda here.", "Yes @thaink, the python script is the same.\r\n\r\nI'll paste it again below:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\ninterpreter.allocate_tensors()\r\nprint(\"all ok\")\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```", "@thaink \r\n\r\nI'm on tf-nightly with the tensorflow docker image. Upon `import tensorflow as tf` this is the stdout:\r\n\r\n```\r\n2020-07-08 23:30:21.138117: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-07-08 23:30:21.138183: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nThe rest of the stdout is related to the loading of the model/invocation using the script above:\r\n\r\n```\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-07-08 23:28:36.616600: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-08 23:28:36.623904: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2208000000 Hz\r\n2020-07-08 23:28:36.624427: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a83282f820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-08 23:28:36.624461: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-08 23:28:36.630149: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-07-08 23:28:36.630210: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-07-08 23:28:36.630312: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (beb39de6fc37): /proc/driver/nvidia/version does not exist\r\nINFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 149 nodes with 1 partitions.\r\n\r\nall ok\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 19, in <module>\r\n    interpreter.invoke()\r\n  File \"/root/miniconda3/envs/python3p7/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 524, in invoke\r\n    self._interpreter.Invoke()\r\n```\r\n\r\nBecause this error happens upon even importing tensorflow, I wonder if there is some default set in the image/container for the GPU. \r\n\r\n", "Alright. Let me check it on my machine.", "In my test, it fail on both MacOS and Linux. We haven't test flex delegate on docker container before, Let me try to get more log.", "Hi aselva-eb,\r\nAfter doing some more tests, it turn out that the problem is with TensorArrayScatterV3 only.\r\nThat op fails with flex delegate. is there a way to avoid that op in your model?", "Hi @thaink ,\r\nPotentially - but in the mean time will there be dev work to support that op? Estimated ETA for a fix?", "I'll need to investigate more about the failure before estimating time for a fix, thanks.", "TF ops support (Flex delegate) is now enabled for MacOS.\r\nYou can try nightly (tf_nightly-2.4.0.dev20200721-cp36-cp36m-macosx_10_9_x86_64.whl or later)\r\n\r\nLet me know if you still have an issue here.", "Hello,\r\nI know this issue is open for MacOS but as already discussed this also happens on Linux and I am having a very similar problem if not the same with Linux. \r\n\r\nI just tried tf_nightly-2.4.0.dev20200721-cp37-cp37m-manylinux2010_x86_64.whl getting similar results.\r\n\r\nRunning on a GCP Notebook \r\n\r\n- Python 3.7.6\r\n\r\n- tf_nightly-2.4.0.dev20200721\r\n\r\n- Linux\r\n\r\n\r\nThe stdout is slightly different but the same error, I am executing the same script \r\n```\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-07-22 09:10:53.192648: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions i\r\nn performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-22 09:10:53.202567: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\r\n2020-07-22 09:10:53.202919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5640cc7d71e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-22 09:10:53.202961: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-22 09:10:53.210611: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such fil\r\ne or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-07-22 09:10:53.210659: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-07-22 09:10:53.210692: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-2): /proc/driver/nvidia/version does not exi\r\nst\r\nINFO: TfLiteFlexDelegate delegate: 55 nodes delegated out of 432 nodes with 15 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 15 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 15 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 11 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 9 nodes delegated out of 114 nodes with 3 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 17 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 10 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 11 nodes delegated out of 167 nodes with 3 partitions.\r\n\r\n2020-07-22 09:10:53.296992: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per\r\n_step_0/_tensor_arraysTensorArrayV3_0)\r\nTraceback (most recent call last):\r\n  File \"untitled.py\", line 22, in <module>\r\n    interpreter.invoke()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 524, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n         (while executing 'TensorArrayScatterV3' via Eager)Node number 432 (TfLiteFlexDelegate) failed to invoke.\r\n```\r\n\r\nThanks", "> TF ops support (Flex delegate) is now enabled for MacOS.\r\n> You can try nightly (tf_nightly-2.4.0.dev20200721-cp36-cp36m-macosx_10_9_x86_64.whl or later)\r\n> \r\n> Let me know if you still have an issue here.\r\n\r\nCool! Thanks - Will try it out and let you know!", "@SergioPN could you share your model? (or some simple step to reproduce?)", "Sure,\r\n\r\nI was trying to use this frozen_inference_graph in particular\r\nhttps://github.com/kwea123/fish_detection/tree/master/fish_inception_v2_graph2/frozen_inference_graph.pb\r\n\r\nYou also can check the saved_model\r\n\r\nThanks @terryheo \r\n\r\n", "@terryheo  and @SergioPN \r\nThe TensorArrayScatterV3 input is resource, which is not well supported now.\r\nIt is the problem with the op alone, not the problem of flex mechanism on MacOS.", "@terryheo  \r\nare you planning to add support for Flex delegate on windows also? I am using tf.signal.stft in my model and i am able to convert and run it on linux but not on windows. ", "@Shubham3101  Flex delegate is supported on Windows. Did you use tf-nightly ?", "@terryheo No i am using TF(r2.3). which tf-nightly should i use?\r\n", "The feature will be enabled with TF r2.4. For now, tf-nightly only supports it.", "Thanks, it is working with tf-nightly", "thank all of you\uff0cI read a lot of handbook , blog ,material but not deal with it in python.So before it supports, just use java,tf-nightly.", "@terryheo  Does this works with tflite_runtime, I am facing same issues with tflite_runtime\r\n\r\n```\r\n    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 1 (FlexTensorArrayV3) failed to prepare.\r\n\r\n\r\n```", "@terryheo  Same issue even with  2.4.0-dev20200803 on linux \r\n```\r\nall ok\r\n2020-08-03 18:52:27.116473: W tensorflow/core/framework/op_kernel.cc:1772] OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\nTraceback (most recent call last):\r\n  File \"test_local.py\", line 25, in <module>\r\n    interpreter.invoke()\r\n  File \"/home/tiru/anaconda3/envs/tflite_converter/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 525, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n\t (while executing 'TensorArrayScatterV3' via Eager)Node number 462 (TfLiteFlexDelegate) failed to invoke.\r\n\r\n\r\n```", "same issue on 2.4.0-dev20200811\r\n\r\n java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n    \t (while executing 'TensorArrayScatterV3' via Eager)\r\n    Node number 285 (TfLiteFlexDelegate) failed to invoke.", "I use tensorflow==2.3.0\r\nand \r\n```\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS\r\n  ,tf.lite.OpsSet.SELECT_TF_OPS\r\n  ]\r\n```\r\nand this problem is solved. \r\n\r\nBut, I have another error when I run \"interpreter.invoke()\". There is nothing happen, it looks like the process is aborted.\r\nI run my code on google colab. I find out that \"SELECT_TF_OPS\" require [delegating](https://www.tensorflow.org/lite/performance/delegates)\r\nTensorflow Lite has tutorial for delegating with android code. \r\n\r\nMy question is can I delegate to gpu on colab ? I found nothing about delegating to colab gpu.\r\nSorry for my bad English.", "We used the same library version on android, tensorflow=2.3.0 and founded the same issue as described in my previous comment, in our case this seems to be replicated on the 2.3.0 version of tensorflow android library.", "Did this break again as I am unable to run the inference on the lastest tf-nightly-2.4.0dev20200626\r\nI am using the ssd_mobilenet_v2_coco_2018_03_29 to do some object detection with the OD api.\r\nThe inference code is same as aselva-eb gave here: https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-655807893\r\nthis is the error:\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n\t (while executing 'TensorArrayScatterV3' via Eager)Node number 244 (TfLiteFlexDelegate) failed to invoke.\r\n", "@Raphaeal19 did you convert tflite with SELECT_TF_OPS?\r\nhttps://www.tensorflow.org/lite/guide/ops_select#converting_the_model", "Yes, this is the code for it @terryheo \r\n\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/workspace/exported_graph/saved_model',signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('model_36k_egohands.tflite', 'wb') as f:\r\n  f.write(tflite_model)", "Oh I see. According to https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-663306760, TensorArrayV3 is not supported with SELECT_TF_OPS", "@tiru1930 tflite_runtime doesn't support TensorFlow ops since it doesn't contain TF kernel implementation.", "oh I see. Could you kindly direct me how to resolve this issue with regard to Object detection API @terryheo ? I am just starting out in this.\r\nShould i change my model completely?", "Also, i just checked https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/allowlisted_flex_ops.cc.\r\nallowlisted_flex_ops has the op TensorArrayV3 as well as TensorArrayScatterV3 in it. @terryheo ", "@Raphaeal19 Since the ops is not working. I'll remove the op from the allowedlist.\r\n", "@thaink can you give me an eta on the removal of the ops?", "@Raphaeal19 Removing is a simple cl. So it'll be removed soon.", "i tried adding the --input_shape arg during the exporting of inference graph to the command and it worked for me.\r\n`!python /content/models/research/object_detection/export_inference_graph.py \\\r\n    --input_type image_tensor \\\r\n    **--input_shape 1,300,300,3 \\**\r\n    --pipeline_config_path /content/workspace/models/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\r\n    --trained_checkpoint_prefix /content/workspace/training/model.ckpt-36676 \\\r\n    --output_directory /content/workspace/exported_graph/`\r\n", "> @thaink can you give me an eta on the removal of the ops?\r\n\r\nFlex-ops only models can support this ops so it will not be removed.", "@thaink @terryheo \r\nHi \r\ni convert my frozen graph to tflite. when inference it, the inference code is \r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n\r\ni get the below error:\r\n\r\n RuntimeError: Container __per_step_0 does not exist. (Could not find resource:__per_step_0/_tensor_arraysbidirectional_rnn/bw/bw/dynamic_rnn/input_0_1)\r\n (while executing 'TensorArrayScatterV3' via Eager)Node number 91 (TfLiteFlexDelegate) failed to invoke.\r\n\r\ncan you help me ?\r\n\r\n\r\n", "@saeedkhanehgir TensorArrayScatterV3 does not works with flex delegate in most case.\r\nFor now, you'll need to remove it from the mode.", "@thaink \r\nthanks. For your answer\r\nCould you please explain further where I should delete it?", "@saeedkhanehgir I think you have to replace it in the training code. Then train the model and convert it again.", "@thaink \r\nDo I have no other solution?", "@thaink \r\ncan i build a custom TensorArrayScatterV3 op?", "@thaink @terryheo \r\nI think this error was because of lstm layer . if lstm layer was created by keras, this error was solved.", "@saeedkhanehgir Great info. Thanks for letting us know.", "Hi @thaink,\r\n\r\nI train and save .pb model with TensorFlow 1.15.0 and  try to convert .pb file to .tflite file with tf-nightly\r\n\r\nMain codes:\r\ncoverter = tf.compat.v1.lite.TFliteConverter.from_frozen_graph()\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT]\r\nconverter.experimental_new_converter = True\r\n\r\nError:\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. Node  number 3 (FlexSign) failed to prepare.\r\n\r\nSeems like it's the problem with tf.sign and I DO have tf.sign in my model and there is BiLSTM layer in my model. So does it mean I have to implement tf.sign op on my own or are there other solutions? \r\n\r\nThank you.\r\n", "@le8888e Sign is supported by flex delegate so the conversion must be fine.\r\nI think the problem here is a typo.\r\ncould you use tf.lite.OpsSet.SELECT_TF_OPS instead of tf.lite.OpsSet.SELECT \r\n", "Hi @thaink,\r\n\r\nI am using tf.lite.OpsSet.SELECT_TF_OPS, sorry for my mistake. And I'm trying tf-nightly 2.4.0 for  flex delegate.\r\n\r\nAnother question,  I'm using tf 15.0 to define and train the model. In the step of defining model, is 'tf.enable_control_flow_v2()' a must?\r\nWithout this, there are errors like \r\n\u2018converting unsupported op Enter and TensorArrayV3\u2019\r\n\r\nThank you", "@le8888e I don't think define the model in a version and convert an a different version is a good practice.\r\nCould you try to define the model with nightly? You can just try to convert it, no need to train.", "@thaink OK, I will give it a try. Is tf-nightly 1.* available and where can I get them? Since code between tf 1.* and 2.* is a little different, I want to try tf-nightly 1.*\r\nThank you\r\n ", "I don't think we have nightly for 1*\r\nYou can try disable_v2_behavior on nightly and see if it is what you expect.", "Hi @saeedkhanehgir,\r\n\r\nI came up the same problem with you. In my case there are\r\ntf.contirb.cnn.BasicLSTMCell()\r\nand\r\ntf.nn.bidirectional_dynamic_rnn() \r\nin my model and there are other layers. Do I only need to implement these two layers by keras and keep other layers unchange\uff1f The tf version I\u2018m using is tf 1.15.0\r\n\r\nThank you.", "Hi  @thaink,\r\n\r\nAfter implementing BiLSTM layer by Keras, I successfully convert and inference .tflite model.\r\nI want to serve .tflite model by tf-serving on PC with python api. Is this feasible and is there any guidance for this?\r\n\r\nTHANK YOU", "@le8888e You implemented the BiLSTM for Tensorflow?\r\nIs it in C++ or python?", "@thaink Just replace TF layer declaration by Keras\r\nOriginal:\r\noutputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedded_chars, dtype=tf.float32)\r\nNew:\r\noutputs = tf.keras.layers.Birdirectional(lstm_cell, merge_mode=\"concat\")(embedded_chars)\r\n\r\nThen errors are gone.", "@le8888e Are you able to run the tflite model?\r\n", "@thaink Yes, I can run .tflite model by\r\ninterpreter = tf.lite.Interpreter(model_path)\r\ninterpreter.allocate_tensors()\r\n...\r\n...\r\ninterpreter.invoke()\r\n\r\nThe outputs of .pb and .tflite are exactly the same.\r\n\r\nI'm wondering if I can serve .tflite model with TF Serving.\r\n\r\nThank you ", "There is an on-going effort of supporting tflite on TF Serving.\r\nhttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/tflite_session.cc\r\nIt's not ready yet but it'll be available soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40157\">No</a>\n", "Hi @terryheo @thaink ,\r\n\r\nI was building tf-nightly viac pip. Would XNNPACK built by default? And how can I check it?\r\n\r\nThank you ", "It's not enabled by default.\r\nYou can enable it with \"--use_xnnpack=true\" option.\r\n\r\nhttps://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html\r\n\r\nYou can see \"Created TensorFlow Lite XNNPACK delegate for CPU.\" from log.", "Hi. I got the same error. I dont understand. Which version of tf-nightly do you use to make it work.\r\n\r\n", "TensorArray need resources type support, which is currently in progress.", "In the TF-nightly version or TF 2.5 version (not tflite-runtime 2.5 version), this problem has resolved already.", "Hello @thaink & @abattery \r\n\r\nI am running in this error with NonMaxSuppression operation while running inference from TFlite model. \r\n(tf.image.non_max_suppression is one of the final layers of my model)\r\nI tested using both TF v2.3.2 and v2.4.1\r\n\r\n```\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. \r\nERROR: Node number 368 (FlexNonMaxSuppressionV3) failed to prepare.\r\n```\r\n\r\nIs this operation supported under any recent Tensorflow releases?\r\nThe discussion [here](https://github.com/tensorflow/tensorflow/issues/32004#issuecomment-559760072) says I need to build TFLite using tensorflow/lite/delegates/flex:delegate as dependency.\r\n\r\nI would be glad if you could elaborate how to do this. \r\n\r\nThanks.", "@suraj-maniyar You can find our guide here: https://www.tensorflow.org/lite/guide/ops_select#run_inference\r\nHow to use it depends on what environment you are running with.", "@thaink \r\nI am using C++ API on CentOS 8 machine and I get the above error on model loading.", "@suraj-maniyar Are you using bazel? How did you install the dependency for TFLite?", "@thaink Yes, I am using Bazel (v3.7.1)\nI followed all the instructions for building TFLite C API from source from [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/c#building-the-c-api).", "I see. You can have two options:\r\n- add tensorflow/lite/delegates/flex:delegate to the list of your dependency\r\n- build //tensorflow/lite/delegates/flex: tensorflowlite_flex with --config=monolithic and add the .so file to your project", "Hello @thaink \r\nI tried your 2 approaches.\r\n\r\nFor first approach : \r\nI updated this file :\r\n```\r\n<tensorflow checkout>/tensorflow/lite/BUILD\r\n```\r\n\r\nI changed the tflite_cc_shared_object to add the flex delegete dependency like so ([reference](https://github.com/tensorflow/tensorflow/issues/33980#issue-517305280)) :\r\n```\r\ntflite_cc_shared_object(\r\n    name = \"tensorflowlite\",\r\n    # Until we have more granular symbol export for the C++ API on Windows,\r\n    # export all symbols.\r\n    features = [\"windows_export_all_symbols\"],\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-Wl,-z,defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)\",\r\n        ],\r\n    }),\r\n    per_os_targets = True,\r\n    deps = [\r\n        \":framework\",\r\n        \":tflite_exported_symbols.lds\",\r\n        \":tflite_version_script.lds\",\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\",\r\n    ],\r\n)\r\n```\r\n\r\nAnd built tensorflowlite using this command :\r\n```\r\nbazel build -c opt //tensorflow/lite/c:tensorflowlite_c --jobs 16\r\n```\r\n\r\nThis built successfully, but during runtime, I still get error in model loading that contains NonMaxSuppression layer.\r\n\r\nFor the second approach I get this error : \r\n```\r\nERROR: Skipping '//tensorflow/lite/delegates/flex:tensorflow_flex': no such target '//tensorflow/lite/delegates/flex:tensorflow_flex': target 'tensorflow_flex' not declared in package 'tensorflow/lite/delegates/flex' defined by /root/TensorFlow-dev/2.3.2/tensorflow/tensorflow/lite/delegates/flex/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/lite/delegates/flex:tensorflow_flex': target 'tensorflow_flex' not declared in package 'tensorflow/lite/delegates/flex' defined by /root/TensorFlow-dev/2.3.2/tensorflow/tensorflow/lite/delegates/flex/BUILD\r\nINFO: Elapsed time: 0.083s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nwhen I try to build tflite with flex delegate. \r\n```\r\nbazel build //tensorflow/lite/delegates/flex: tensorflow_flex --config=monolithic --jobs 16\r\n```\r\n\r\nPlease let me know if there is anything that I am probably doing the wrong way here. I am new to using TensorFlowLite.\r\nThanks.", "@thaink \r\nAny updates on this issue?", "For the first approach, you are building the wrong target: It should be //tensorflow/lite:tensorflowlite\r\nFor the second one, You should sync to our master branch and the name is tensorflowlite_flex not tensorflow_flex", "@thaink \r\nThank you very much for the correction. I tried building tensorflowlite using the first approach.\r\nIt gave me the following error:\r\n```\r\nERROR: /root/TensorFlow-dev/2.3.2/tensorflow/tensorflow/lite/BUILD:644:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/e9dcbb52a816a0ad322c991a68a41fbc/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/root/CMake/cmake-3.20.0-rc4/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib64/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_ENABLE_XLA=1 \\\r\n  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nstderr (/root/.cache/bazel/_bazel_root/e9dcbb52a816a0ad322c991a68a41fbc/execroot/org_tensorflow/bazel-out/_tmp/actions/stderr-2) exceeds maximum size of --experimental_ui_max_stdouterr_bytes=1048576 bytes; skipping\r\nTarget //tensorflow/lite:tensorflowlite failed to build\r\nINFO: Elapsed time: 143.768s, Critical Path: 143.48s\r\nINFO: 2 processes: 2 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nIs this also supposed to be built under master branch? I am building it under v2.3.2\r\n\r\n\r\n", "Can you add --experimental_ui_max_stdouterr_bytes=1000048576 or a bigger number of necessary. And don't forget to add --config=monolithic ", "Thanks a lot @thaink. This built successfully!\r\nI have one additional question: I tried building TF Lite v2.4.1 with flex delegate dependency using cmake and again got the unsupported operator error.\r\nIs flex delegate not supported in v2.4.1 under CMake?\r\nIf so, could you give an estimate when the support would be added?", "@terryheo can you update about the cmake build?", "We don't have a plan to support Flex delegate with CMake. But I'm wondering why you can't use Bazel for the purpose.", "@thaink I use tf.image.combined_non_max_suppression in my model. I can transfer it to tflite. But when I use it in Android or use python tf.lite.interpreter, i got the issus.\r\n![image](https://user-images.githubusercontent.com/54882489/118105343-d752c600-b40e-11eb-9810-5a12f18e1ce3.png)\r\nHow can I resolve it ", "\r\n\r\nI use the code to convert my model. \r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('model')\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\n```\r\n", "@mao381332619 Please upload a new issue. We would like to keep each issue focused.", "ok. thanks @abattery ", "It works for me now. with Tensorflow == 2.3.0. Merci a lot.\n\n<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>\nGaranti\nsans virus. www.avast.com\n<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>\n<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>\n\nLe jeu. 13 mai 2021 \u00e0 10:01, mao381332619 ***@***.***> a\n\u00e9crit :\n\n> ok. thanks @abattery <https://github.com/abattery>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40157#issuecomment-840454586>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFUFGTBWB2D5G2SHYCK4EYDTNOPO5ANCNFSM4NSYL2OQ>\n> .\n>\n\n\n-- \n*Franck MIGONE*\n*Ing\u00e9nieur Statisticien*\n*+225 77 55 34 45 / +225 40 48 92 27*\n*Chef Services Ressources et Formation, ENSEA Junior Services*\n", "> We don't have a plan to support Flex delegate with CMake. But I'm wondering why you can't use Bazel for the purpose.\r\n\r\n@thaink @terryheo \r\nActually, I was able to use the Bazel build for my project. Thanks a lot for your help.\r\n", "@thaink @terryheo \r\nNot sure if this falls under a different topic, but I am interested in running inference using flex delegate on single thread.\r\nFor that I configured my interpreter like this : \r\n```\r\ntflite::ops::builtin::BuiltinOpResolver builtins;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter, 1);\r\ninterpreter->SetNumThreads(1);\r\nthread_interpreter.interpreter_.reset(interpreter.release());\r\n```\r\n\r\nUpon doing that I get an error in destructor for Flex delegate : \r\n```\r\n(gdb) bt\r\n#0  0x00007ffff5e314c0 in __pthread_timedjoin_ex () from /lib64/libpthread.so.0\r\n#1  0x00007ffff3686661 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#2  0x00007ffff3696137 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#3  0x00007fffeb67dd69 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#4  0x00007fffeb67e1b1 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#5  0x00007fffeb640353 in tflite::flex::DelegateData::~DelegateData() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#6  0x00007fffeb639134 in tflite::FlexDelegate::~FlexDelegate() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#7  0x00007fffee762e3f in tflite::TfLiteDelegateFactory::DeleteSimpleDelegate(TfLiteDelegate*) ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#8  0x00007fffeb61799e in tflite::impl::Interpreter::~Interpreter() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#9  0x00007ffff72bbdc7 in boost::checked_delete<tflite::impl::Interpreter> (x=0xe83960)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/checked_delete.hpp:34\r\n#10 0x00007ffff72bc998 in boost::detail::sp_counted_impl_p<tflite::impl::Interpreter>::dispose (this=0x129b190)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_impl.hpp:78\r\n#11 0x00007ffff70af8b6 in boost::detail::sp_counted_base::release (this=0x129b190)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:146\r\n#12 0x00007ffff70af949 in boost::detail::shared_count::~shared_count (this=0x100bd88, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/shared_count.hpp:371\r\n#13 0x00007ffff72b962c in boost::shared_ptr<tflite::impl::Interpreter>::~shared_ptr (this=0x100bd80, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/shared_ptr.hpp:328\r\n#14 0x00007ffff72bca60 in TensorflowInferenceCore::ThreadInterpreter::~ThreadInterpreter (this=0x100bd80,\r\n    __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/src-main/deep_learning_inference/tensorflow_lite/TensorflowInferenceCore.h:17\r\n```\r\n\r\nIs there a way I can confine inference with flex delegate to run on single thread?\r\nI also built tensorflow with xnnpack support but it did not use single thread.\r\n", "@suraj-maniyar It should be a different topic. Can you file a new issue for it?", "@thaink I opened a new issue [here](https://github.com/tensorflow/tensorflow/issues/49266). Could you please take a look?\r\nThanks.", "I'm using tflite_runtime 2.5, I converted my model using tensorflow 2.5 but when I'm trying to do inference using that model with tflite_runtime in my environment I'm getting below error \r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexAbs) failed to prepare.", "@nolanding the guide to use Select TF ops is in https://www.tensorflow.org/lite/guide/ops_select#run_inference", "@thaink  I followed the same, but I don't find anything for python. \r\nCan you suggest something?", "@nolanding Can you try a newer version of tflite_runtime? It is backward-compatibility so it should work with older model.", "@thaink  I've tensorflow data type 13 which doesnot work on lower versions.\r\ngetting below error for lower versions \r\nUnsupported data type 13 in tensor", "@thaink  is there some way to define flex ops during inference? ", "I mean tflite_runtime 2.6 or higher. There shouldn't be any additional step to run the the delegate.", "@terryheo can you take a look at the recent question?", "> I mean tflite_runtime 2.6 or higher. There shouldn't be any additional step to run the the delegate.\r\n\r\nI'll try it and get back to you.", "@thaink converting it to tflite didn't work for me, was stuck on dynamic RNNs conversion. Switched to onnx, that helped.", "Is this solved ?\r\n\r\n"]}, {"number": 40156, "title": "Tensorflow 2.0 Object detection GPU fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version: Tensorflow-gpu 2.0.2\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 10.0/7.6.0\r\n- GPU model and memory: GeForce 930MX\r\n**Describe the current behavior**\r\nHi.\r\nI'm trying to do an object detection tutorial from [this](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb), however I get the following output without success:\r\n`(tfflask) C:\\Users\\DELL\\Downloads\\models-master\\research\\object_detection>python test_object.py\r\n2020-06-04 21:02:11.627763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-06-04 21:02:14.271146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-04 21:02:14.688585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce 930MX major: 5 minor: 0 memoryClockRate(GHz): 1.0195\r\npciBusID: 0000:03:00.0\r\n2020-06-04 21:02:14.694154: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-06-04 21:02:14.700750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2020-06-04 21:02:16.474139: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-04 21:02:16.555975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce 930MX major: 5 minor: 0 memoryClockRate(GHz): 1.0195\r\npciBusID: 0000:03:00.0\r\n2020-06-04 21:02:16.567918: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-06-04 21:02:16.576144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-06-04 21:04:34.299345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-04 21:04:34.303521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-06-04 21:04:34.305504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-06-04 21:04:34.311931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: GeForce 930MX, pci bus id: 0000:03:00.0, compute capability: 5.0)\r\n2020-06-04 21:04:41.840234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-06-04 21:04:42.605329: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2020-06-04 21:04:43.557938: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-06-04 21:04:43.829741: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-06-04 21:04:44.020239: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2020-06-04 21:04:44.422733: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n`\r\n\r\n\r\nIs the memory overloading? I tried inserting:\r\n`physical_devices = tf.compat.v1.config.experimental.list_physical_devices('GPU') \r\n\r\ntf.compat.v1.config.experimental.set_memory_growth(physical_devices[0], True)`\r\nbut to no avail.", "comments": ["Never mind , I figured it out. All i had to do was update the version of CUDA to 10.1, tensorflow-gpu to tensorflow-gpu-2.2 and keep the CudNN as is. Installing. Installing Microsoft Visual Studio Redistributable helped tensorflow-gpu 2.2 to get the required .dll's which are required after upgrading to tensorflow-gpu >2.0. I'm closing the issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40156\">No</a>\n"]}, {"number": 40155, "title": "TFLite: Cannot run inference on TF Lite Model: \"Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 3", "body": "**System information**\r\n- OSX Mojave\r\n- TF 2.3.0-dev20200602\r\n- Python 3.7.7\r\n- TF installed using Conda\r\n\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(curr_dir + \"saved_model\")\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the TF Lite model.\r\n    with tf.io.gfile.GFile(curr_dir + '/model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@aselva-eb \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version \r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "> @aselva-eb\r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\nI have my OS and TF version listed at the top. Is there something else in particular that I should add?", "@aselva-eb\r\nPlease share a simple stand alone code to replicate the error faced or share the colab gist with error faced for us to analyse", "Sorry! - just realized how empty this ticket is - not sure how I ended up posting this. \r\nThe real ticket with the info is:\r\nhttps://github.com/tensorflow/tensorflow/issues/40155"]}, {"number": 40154, "title": "GPU delegate gives different result from CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution:Linux Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device: Pixel 3 XL\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r2.2.0 and master\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source): default\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nGPU delegate gives very different result from CPU. \r\nI was able to hard-code the source (in tensorflow/lite/delegates/gpu/common/model_builder.cc) to allow some operations to be delegated to GPU. Out of 270+ operations: \r\n- Delegating just one Conv_2d will produce very similar result as the one by CPU only. \r\n- Delegating a few more operations seem to produce bigger difference. \r\n- Delegating just the first MUL operation will produce very different result. \r\n\r\n**Describe the expected behavior**\r\nresult should be close\r\n\r\n**Standalone code to reproduce the issue**\r\nI personally hijacked tflite tools/benchmark code and give an sample image as deterministic input, instead of random input. \r\nI would love to provide my code change if it helps. \r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe tflite model was converted from InsightFace/ArcFace MXNet model\r\nhttps://github.com/deepinsight/insightface/wiki/Model-Zoo (3.2 model)\r\nlink to download the tflite\r\nhttps://drive.google.com/file/d/1pJX2I8btskVy-QHiF-mcUF7HF6ZFaQuf/view?usp=sharing\r\n\r\nAlso attached the graph of above model:\r\n[visualized_official_arcface_no_sub.zip](https://github.com/tensorflow/tensorflow/files/4731201/visualized_official_arcface_no_sub.zip)\r\n", "comments": ["Can confirm that I see output diff errors for GPU vs CPU.\r\n\r\n@ShiyongL , for future ease, feel free to check out our [inference_diff tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) to compare raw delegate outputs with CPU, and modify the [delegate node partitioner](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils.cc#L125) to select particular nodes by their index in their execution plan (which can be visualized with [this tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py)) :-).\r\n\r\nLooking into this...", "The culprit (atleast partially) seems to be the GPU delegate's [low-precision mode](setQuantizedModelsAllowed), which is enabled by default in our benchmark tooling.\r\n\r\nAFAIK, the delegate uses fp16 (instead of fp32) precision in that case, and that seems to be the cause of your output diffs. I could not pinpoint a single node/nodes that cause the output errors; they seem to be increasing with number of delegate nodes as you suggested.\r\n\r\nSee the outputs with [inference_diff](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff) with the mode enabled/disabled:\r\n\r\n```\r\n$ adb shell /data/local/tmp/run_eval   --model_file=/data/local/tmp/gpu_debug_model.tflite --delegate=gpu --num_runs=5 --gpu_precision_loss_allowed=true\r\n...\r\nOutputDiff[0]: avg_error=nan, std_dev=nan\r\nOutputDiff[1]: avg_error=0, std_dev=0\r\n```\r\n\r\nvs \r\n\r\n```\r\n$ adb shell /data/local/tmp/run_eval   --model_file=/data/local/tmp/gpu_debug_model.tflite --delegate=gpu --num_runs=5 --gpu_precision_loss_allowed=false\r\n...\r\nOutputDiff[0]: avg_error=0.0824622, std_dev=0.000453001\r\nOutputDiff[1]: avg_error=0, std_dev=0\r\n```\r\n\r\nThere are still some errors in the latter case, but they are lesser than the earlier one. (Note that the latency goes to ~80ms from ~50ms on a Pixel 3, when the mode is disabled).\r\n\r\nIn our Java API, the mode can be disabled by doing the following instead of [the usual]():\r\n```\r\nGpuDelegate delegate = new GpuDelegate(new GpuDelegate.Options().setPrecisionLossAllowed(false));\r\n```\r\n\r\nCC'ing Juhyun who might know if such output diffs are expected - I guess Convolutions & Fully-Connecteds can amplify previous error.\r\n", "Usually, it's either the model's fault of numerical instability (e.g. using squared diff on an error that is below a very small threshold such as 1e-6, often visible in instance norms of X - MEAN followed by SQUARE_DIFF, and then doing math based on those values) or a bug in our shaders' accumulators, i.e. some accumulators needs to be in FP32 regardless of the precision mode such as SOFTMAX.  Unfortunately, I don't think I will have time to look into this, it would be nice if you could pinpoint what the issue is.", "@srjoglekar246 @impjdi Thanks a lot for looking into this! The inference_diff tool is super for debugging the issue. Here are what I found so far:\r\n\r\n- Following @srjoglekar246's lead, I tried the inference_diff and got slightly different errors on my Pixel 3XL. And I used tensorflow master branch and r2.2 branch and both produced different results. So my first question is: which branch I should use for Android deployment? I notice most users are using `org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly` arr, but there is also `org.tensorflow:tensorflow-lite-gpu:2.2.0` arr. I tried both in my app, the results are very different on GPU delegation but same on CPU. \r\n\r\n- I also tried to allow GPU delegation to each single operation. Surprisingly, the first MUL operation already produced some error. I trimmed out all the operations but the first MUL operation and convert this tiny model to TFLite. Running on inference_diff and I got same error already, using  `gpu_precision_loss_allowed=true or false`. \r\n\r\n`output_errors {\r\n      output_errors {\r\n      max_value: 0.0021719106\r\n      min_value: 0.00214503519\r\n      avg_value: 0.0021564289927482605\r\n      std_deviation: 1.0854229e-05\r\n    }`\r\n\r\n- Furthermore, I noticed that the inference_diff tool uses random inputs of [-1, 1] while my original input is [-127, 127]. If I hardcoded the inputs to all -127, I got even much bigger error with GPU delegation. Here is one line of code change in  [https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/lite/tools/evaluation/stages/inference_profiler_stage.cc#L51] \r\n\r\n`    data->push_back(static_cast<T>(-127.0f));\r\n    //data->push_back(static_cast<T>(rand_float));`\r\n\r\nHere is the error I got: \r\n`output_errors {\r\n      max_value: 0.661458313\r\n      min_value: 0.661458313\r\n      avg_value: 0.6614583015441895\r\n      std_deviation: 0\r\n    }`\r\n\r\nThe tiny model of a single MUL operation is attached\r\n[MUL_only.zip](https://github.com/tensorflow/tensorflow/files/4765345/MUL_only.zip)\r\nAnd my number was produced on branch r2.2. \r\n\r\nIt would be greatly appreciated if you could take another look. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40154\">No</a>\n", "Sorry I was accidently closed this issue but reopened it again. ", "@ShiyongL for the AAR, it depends on whether you would prefer to use the TF nightly branch or the stabler TF 2.2 master branch for your environment. TF2.2 will probably be safer, but not contain any recent patches you might need. If the results are very different, can you explain how? Are the results with the 2.2 branch more correct? Then the issue might have been introduced recently.\r\n\r\nAbout the 1-op model diff: In most delegates, there will be some diff v/s CPU, mainly because of the accumulators/optimizations used. For example, even with MobileNet v1 (where the GPU gives comparable top-k accuracy), there is a diff of ~2e-5 for the output tensor.\r\n\r\nDo you have a way to check the task-oriented 'accuracy' of the model? (for example, how well your app recognises objects for an object detection model). That might help us see how bad the actual performance is (maybe with/without the gpu_precision_loss param)", "> @ShiyongL for the AAR, it depends on whether you would prefer to use the TF nightly branch or the stabler TF 2.2 master branch for your environment. TF2.2 will probably be safer, but not contain any recent patches you might need. If the results are very different, can you explain how? Are the results with the 2.2 branch more correct? Then the issue might have been introduced recently.\r\n> \r\n> About the 1-op model diff: In most delegates, there will be some diff v/s CPU, mainly because of the accumulators/optimizations used. For example, even with MobileNet v1 (where the GPU gives comparable top-k accuracy), there is a diff of ~2e-5 for the output tensor.\r\n> \r\n> Do you have a way to check the task-oriented 'accuracy' of the model? (for example, how well your app recognises objects for an object detection model). That might help us see how bad the actual performance is (maybe with/without the gpu_precision_loss param)\r\n\r\n@srjoglekar246 for the 1-op model, TF2.2 and TF nightly branch(I assume it is the Master branch) produced slightly results:\r\nTF nightly:\r\n`OutputDiff[0]: avg_error=0.00215609, std_dev=1.08443e-05, gpu_precision_loss = false`\r\n`OutputDiff[0]: avg_error=0.00215643, std_dev=1.08542e-05, gpu_precision_loss = true`\r\nTF 2.2 (exactly same results for gpu_precision_loss = true or false ):\r\n`output_errors { output_errors { max_value: 0.0021719106 min_value: 0.00214503519 avg_value: 0.0021564289927482605 std_deviation: 1.0854229e-05 }`\r\n- The diff is 2e-3, which much larger than ~2e-5 by MobileNet v1. \r\n- When the input is 100x larger (change input from -1, 1 to -127, 127), the error is 100x larger. If you could reproduce the error using the single op model, it should help @impjdi for further debugging. \r\n\r\n\r\nTo answer your second question, I am using the official ArcFace model from InsigntFace [https://github.com/deepinsight/insightface](url) to recognize faces. The input is (112x112x3) image, output is a 512 tensor. \r\n- With original model (has the first MUL operation), I was getting all NAN at my output. \r\n- After removing the first MUL operation (which I do the MUL first then passing the result to CNN model), I was getting more reasonable results but still quite different (for a sample image):\r\nOutputs from CPU: [0.7939907, 0.5848467, -0.94000, -0.9900000, ..., 0.3934663, 0.93600059]\r\nOutputs from GPU: [0.7978515, 0.3818359, -0.96875, -0.9086914, ..., 0.1318359, 0.77441406] (with gpu_precision_loss_allowed=false)  TF2.2 and TF nightly produced exactly same results here (CPU or GPU.)\r\n\r\nI would be very happy if the error is ~e-2. \r\nThank again for your support!\r\n\r\n\r\n", "The output differences between TF2.2 & nightly seem too tiny to show any change in behavior between the two (~1e-7). So we can assume that nothing has been broken since TF2.2\r\n\r\nAbout the MUL op:\r\n1. The MUL op provided by @ShiyongL has an input tensor being multiplied by a scalar value `0.007812`. The avg diff w/ CPU does rise as the input sampling range is increased from {-1, 1} to {-100, 100} or {-1000, 1000} (proportionately). However, the std deviation of the diff values  remains ~5-e6, which indicates that the problem might just be accumulation errors that increase as the input values grow. I audited the OpenCL kernel on the GPU delegate, and it seems a straight-forward multiplication.\r\n\r\n2. Even when I run inference_diff on the arcface model by rejecting all MUL ops (leading to delegation of only 5 ops somewhere in the middle of the graph), I still see an avg error of ~0.02. So MUL might not be the only issue.\r\n\r\nA couple of clarifying questions from my side:\r\n\r\n1. Is -127, 127 the expected input range for the model?\r\n\r\n2. I am not sure what the 512-element output tensor represents, so looking at raw diffs might be misleading. Have you tried plugging in the model into an application that post-processes the output as needed? And does the precision with GPU fall a lot even after post-processing? A simple example I know is the SSD MobileNet, which has >1 raw diffs (CPU vs the DSP delegate) when we use inference_diff directly, but the COCO metrics remain good enough.", "I agree with @srjoglekar246 that MUL is not the only issue. Since it is the first operation, it is easier to trim out the rest and focusing on this particular operation. Like @srjoglekar246 said, multiplication should be a straightforward operation, the difference should not be that big between CPU and GPU. It is super interesting to see the reason. \r\n\r\nRe 1: -127, 127 was the expected input. Multiply by 0.007812 will normalize the input to -1, 1, which might be a better input for numerical stability reason. As I have shown the a previous comment, the difference between CPU and GPU is still quite large. \r\nRe2: the 512 output tensor is a 512 vector. ArcFace would inference two images and get 2 512 vectors. Then sure the distance between the two vectors. If the two vectors are similar, the two images are considered to be matched. I am not sure what you meant by >1 raw diffs for SSD MobileNet, which has multiple outputs like class, score and bboxes. I agree we should not compare just the raw values but the error ratio should be relatively low, like less than 1% due to floating point precision. \r\n\r\nI reported this issue when I got NAN as the output. Now we figured out the MUL is causing a big issue. After removing the MUL operation, I could get some more reasonable results, though the error is still not small and I am not totally convinced that this is caused by numerical stability. I will take the results anyhow and run the evaluation tomorrow. Will report back. \r\n\r\nThanks a lot!\r\n\r\n", "@srjoglekar246 The errors between CPU and GPU are so large that it deteriorates the evaluation performance, when the inference results are used to determine whether one face image is similar to another. \r\nIt would be greatly appreciated if you or other engineers could look into it!", "@ShiyongL , I have asked the GPU folks internally to take a look at this model/issue. Will keep you posted on the details :-).\r\n\r\nOn that note, did you try any other delegates for this? (for example, XNNPack?)", "\r\n\r\n\r\n> @ShiyongL , I have asked the GPU folks internally to take a look at this model/issue. Will keep you posted on the details :-).\r\n> \r\n> On that note, did you try any other delegates for this? (for example, XNNPack?)\r\n\r\n\r\nThanks a lot @srjoglekar246 ! \r\n\r\nI haven't tried XNNPack. Will check it out.", "@ShiyongL we just fixed this on our end (will probably take a day or so to reflect in nightly). There were a couple of small issues with some GPU graph optimizations & model parsing which are now fixed.\r\n\r\nThe error for input in [-127, 127] is now ~0.02 with precision loss, and ~1e-6 with precision loss disabled (the precision loss issues are likely due to numerical instability w/ FP16). Could you do check the inference results again and close the issue if things work now?", "Perfect! \r\nI are using org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly ARR. Do you know how long does it take for the fix to be available to the ARR, for example at [JCenter](https://bintray.com/google/tensorflow/tensorflow-lite)? \r\nOn the other hand, do you mind sharing a link to a PR so that I could make sure to have the fixed code?  \r\nThank you very much!\r\n", "@ShiyongL it should reflect within 24 hours or so. ([This](https://github.com/tensorflow/tensorflow/commit/225bdf60f3c4f51ab5568a53d31b0799369a1b89) is the commit after which the model should work fine).\r\n\r\nNote: You should have cleared the gradle cache and/or use the change=true flag", "Managed to validate the fix. Thank you very much!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40154\">No</a>\n"]}, {"number": 40153, "title": "Add Init for TF_Filesystem GCS", "body": "@mihaimaruseac \r\nThis PR add the `init` function for `filesystem_ops`", "comments": ["@mihaimaruseac \r\nCan you merge this PR so I can continue to work. Thank you", "Working on it. There are internal errors that require manual fixing/investigation.", "@mihaimaruseac \r\nhttps://github.com/tensorflow/tensorflow/pull/40351\r\nI have made a PR that updates `google-cloud-cpp` to `1.14.0`. It also contains the explanations for your reviews. ", "@mihaimaruseac \r\n`repo_mapping` seems good. Please take a look. Thank you !", "@mihaimaruseac \r\nHere is a PR for `repo_mapping` only https://github.com/tensorflow/tensorflow/pull/40389", "I was thinking to move the other 2 packages in `workspace.bzl` to a separate PR. We can file a concurrent PR for that.", "@mihaimaruseac \r\nHere is a PR to add 2 packages to `workspace.bzl`\nhttps://github.com/tensorflow/tensorflow/pull/40390", "Now we just need to rebase this back on master and then we should be able to merge. I'll probably still need to do some manual import, but it's going to be easier this time and less risky. Maybe by tomorrow we can merge it", "@mihaimaruseac I have rebased", "Needs manual import, working on it right now", "@mihaimaruseac \nI see. Could you give me some idea how internal import work so I can avoid errors in the next PRs ? Like why there are errors in this PR ?", "I don't think it's your fault now. Basically internally some parts of the code have different names, some dependencies are in different location. Everytime you have an `@dep` in a `BUILD` file, internally that one is a normal `//path/to/dep` dependency.\r\n\r\nThere is a script that does all of these conversions but when new things are added the script also needs to be modified. It can be modified only via doing a manual import and fix. That is the reason I suggested splitting this into several PRs, so that we can localize exactly which places need to be changed to satisfy the script and so that it is easy to revert if we did a mistake/broke some build.", "I see. Thank you", "The Linux GPU failure is unrelated, we are rolling back the change causing the error there. So far public tests seem to be ok, there are a few more internal tests that are running now but everything looks good", "After this PR is merged, I will send you a PR for WritableFile"]}, {"number": 40152, "title": "TFLite: ELU activation not supported on GPU delegate", "body": "Models with ELU activation op cannot run on GPU delegate, getting the following error:\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\nELU: Operation is not supported.\r\n```\r\n\r\nCan support for this op be added to GPU delegate?\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\nHere's a code to generate a minimal example model:\r\n\r\n```\r\nimport tensorflow as tf\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.Input(shape=(32,32)),\r\n    tf.keras.layers.ELU()\r\n])\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter=False\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\ntflite_model = converter.convert()\r\nwith open('elu.tflite', \"wb\") as f:\r\n    f.write(tflite_model)\r\n```\r\n", "comments": ["Model generated by the code above, which fails on TFLite GPU delegate:\r\n[elu.tflite.zip](https://github.com/tensorflow/tensorflow/files/4730972/elu.tflite.zip)\r\n", "Having the same issue on iOS with the `MetalDelegate`. Would be awesome if this could get solved!", "@ltxitai It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40152\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40152\">No</a>\n"]}, {"number": 40151, "title": "Initiating inputs on functional model API doesn't work as expected", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  Unknown (not installed)\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: Python 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 440.33.01  CUDA Version: 10.2 \r\n- GPU model and memory: RTX 2080 TI 11016MiB\r\n\r\n\r\n**Describe the current behavior**\r\nThe current behavior creates two \"parallel\" models when provided with multiple outputs within the same model and if this model was defined with custom inputs as in my example below. \r\n\r\n**Describe the expected behavior**\r\nYou can run the same code with a pre-built model. \r\n```python\r\nres=tf.keras.applications.ResNet50()\r\ngrad_model = tf.keras.models.Model(\r\n    [res.inputs], [res.layers[-5].output, res.output]\r\n)\r\ngrad_model.summary()\r\n```\r\n**Standalone code to reproduce the issue**\r\n```python\r\nclass LeNet(tf.keras.Model):\r\n    def __init__(self):\r\n        super(LeNet, self).__init__()\r\n        self.conv1 = Conv2D(32, 3, activation='relu')\r\n        self.conv2 = Conv2D(64, 3, activation='relu')\r\n        self.conv3 = Conv2D(128, 3, activation='relu')\r\n        self.flatten = Flatten()\r\n        self.d1 = Dense(128, activation='relu')\r\n        self.d2 = Dense(64)\r\n        self.out = Dense(10, activation='softmax')\r\n\r\n    def call(self, x):\r\n        if(len(x.shape)==3):\r\n            x=tf.expand_dims(x,axis=0)\r\n        \r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        x = self.conv3(x)\r\n        x = self.flatten(x)\r\n        x = self.d1(x)\r\n        x = self.d2(x)\r\n        return self.out(x)\r\nlenet=LeNet()\r\ninputs=tf.keras.layers.Input((28,28,1))\r\nlenet(inputs)\r\nlenet.summary()\r\ngrad_model = tf.keras.models.Model(\r\n    [lenet.inputs], [lenet.layers[-5].output, lenet.output]\r\n)\r\ngrad_model.summary()\r\n```\r\n**Other info / logs** \r\n\r\nI think the issue is that the inputs to the model when lenet(inputs) is run, is not updating the inbound nodes for all layers or replacing the existing ones. \r\n", "comments": ["@fostiropoulos \r\nI ran the code shared by you and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/b4d9d18be40f4c4e170d856a769ecc42/untitled205.ipynb). Please provide with complete stand alone code to replicate the issue or if possible share a colab gist for us to analyse the issue faced.", "You need the imports\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten\r\nfrom tensorflow.keras.models import Sequential\r\nclass LeNet(tf.keras.Model):\r\n    def __init__(self):\r\n        super(LeNet, self).__init__()\r\n        self.conv1 = Conv2D(32, 3, activation='relu')\r\n        self.conv2 = Conv2D(64, 3, activation='relu')\r\n        self.conv3 = Conv2D(128, 3, activation='relu')\r\n        self.flatten = Flatten()\r\n        self.d1 = Dense(128, activation='relu')\r\n        self.d2 = Dense(64)\r\n        self.out = Dense(10, activation='softmax')\r\n\r\n    def call(self, x):\r\n        if(len(x.shape)==3):\r\n            x=tf.expand_dims(x,axis=0)\r\n        \r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        x = self.conv3(x)\r\n        x = self.flatten(x)\r\n        x = self.d1(x)\r\n        x = self.d2(x)\r\n        return self.out(x)\r\nlenet=LeNet()\r\ninputs=tf.keras.Input((28,28,1))\r\nlenet(inputs)\r\nlenet.summary()\r\ngrad_model = tf.keras.models.Model(\r\n    [inputs], [lenet.layers[-5].output, lenet.output]\r\n)\r\ngrad_model.summary()\r\n```\r\n\r\nTry changing inputs to lenet.inputs, strangely enough I get a different error in collab than my local enviroment. ", "Another simpler example: https://colab.research.google.com/gist/Saduf2019/b4d9d18be40f4c4e170d856a769ecc42/untitled205.ipynb", "@fostiropoulos \r\nFor the \"conv2D error, please refer to this [link](https://stackoverflow.com/questions/44131295/keras-cannot-import-name-conv2d). I have resolved them by using\r\n \"from keras.layers.convolutional import Conv2D\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Dense\" imports.\r\n\r\nFor the code shared, i ran it and do not face any error, please find [gist here](https://colab.sandbox.google.com/gist/Saduf2019/7f0aa4e2b51bb44790c82ca67d011299/untitled217.ipynb)\r\nPlease share the error log for us to analyse.\r\n", "The error is logical, not python based. ", "@fostiropoulos This is intended behavior. When you have multi-output (two output in your case), you will certainly have two branches in the model. For multi-output model you also need to use multiple loss functions (two in your case). What are you expecting? Can you please give us more details about your use-case and expected result. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/15a45cf6db7ab64cb5657421450c86dc/copy-of-untitled217.ipynb) with another example. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40151\">No</a>\n"]}, {"number": 40150, "title": "tf.TensorArray.stack() fails inside tf.function when dtype is tf.uint32 or tf.uint64", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen used in a tf.function, calling stack() in a tf.TensorArray object fails with the following error when the dtype is tf.uint32 or tf.uint64.\r\n\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Unsupported type in DataTypeToPrimitiveType: 'variant'\r\n\t [[node TensorArrayV2Stack/TensorListStack (defined at ta_error.py:21) ]]\r\n  (1) Invalid argument:  Unsupported type in DataTypeToPrimitiveType: 'variant'\r\n\t [[node TensorArrayV2Stack/TensorListStack (defined at ta_error.py:21) ]]\r\n\t [[TensorArrayV2Stack/TensorListStack/_6]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_test_12]\r\n\r\n**Describe the expected behavior**\r\nShould not fail, but return a stacked tensor. Works correctly when the dtype is not tf.uint32 or tf.uint64. Also works correctly when executed in eager mode.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n@tf.function\r\ndef test():\r\n  ta = tf.TensorArray(tf.uint64, 1)\r\n  ta = ta.write(0, 123)\r\n  return ta.stack()\r\n```\r\n", "comments": ["I have tried in colab with TF version 2.2 ,nightly version(`2.3.0-dev20200604`) and was able to reproduce the issue.Please,find the gist [here](https://colab.research.google.com/gist/ravikyram/7ec03b405f875dce2d345b2f22e5aec5/untitled50.ipynb).Thanks!", "I think this issue is now fixed with tf-nightly. Closing issue. Please reopen if still there is a problem", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40150\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40150\">No</a>\n"]}, {"number": 40149, "title": "libtensorflowlite.so crash on Android (NDK r18b)", "body": "[<em>Please](url) make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android Phone \r\n- TensorFlow installed from (source or binary): 2.1.1\r\n- TensorFlow version:\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: None\r\n- Bazel version (if compiling from source): 0.27.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI built the libtensorflowlite.so for both armeabi-v7a and arm64-v8a using the build command from doc file:\r\n\r\n```\r\nbazel build -c opt --config=android_arm //tensorflow/lite:libtensorflowlite.so\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\nHere is the bazel configuration:\r\n```\r\nbuild --host_force_python=PY2\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --action_env ANDROID_NDK_HOME=\"/mnt/1tb-drive/software/Android/ndk/android-ndk-r18b\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"28.0.3\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"28\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/mnt/1tb-drive/software/Android\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\nHowever, when using this in a jnilibs (along with NDK r18b), the generated .so file has segmentation fault. How could I fix this problem?\r\n\r\n```\r\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 20155 (Thread-3), pid 20112\r\n```\r\n\r\nTo be clear, I was able to load model succesfully:\r\n\r\n```\r\nmodelStftSpectrogram = tflite::FlatBufferModel::BuildFromFile(path.c_str());\r\n\r\n    if(!model){\r\n        exit(0);\r\n    }\r\n    \r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n    TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n    TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n```\r\n\r\nThen the segmentation fault happens exactly at this line:\r\n\r\n```\r\n    float *tflite_input = interpreter->typed_input_tensor<float>(0);\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["If you're using Android, you'd better build it with dbg.\r\n```\r\nbazel build -c dbg --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n```\r\nThen you can use ndk-stack to find where it crashed with Android crash dump on logcat.\r\nhttps://developer.android.com/ndk/guides/ndk-stack", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40149\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40149\">No</a>\n"]}]