[{"number": 20014, "title": "TensorFlow master build failure on s390x", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nTensorFlow Master build is failing on s390x due to BoringSSL (which is not supported on s390x).\r\nSeems like error comes while compiling gRPC. \r\nChanges causing build failure can be checked [here](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/126/)\r\n\r\n@gunan , Could you please help in redirecting this to the relevant person?\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/js/.cache/bazel/_bazel_jenkins/14d9bef57f8e4d2a0eef0de174c4144b/external/grpc/BUILD:1628:1: C++ compilation of rule '@grpc//:alts_frame_protector' failed (Exit 1)\r\nIn file included from external/boringssl/src/include/openssl/bio.h:60:0,\r\n                 from external/grpc/src/core/tsi/alts/crypt/aes_gcm.cc:23:\r\nexternal/boringssl/src/include/openssl/base.h:114:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^\r\n```", "comments": ["Looks like boringssl  crept back in.\r\nI am having difficulty running bazel query, so debugging is taking longer than usual.\r\nBut I will get to the bottom of this.", "I am pretty certain the root cause is this commit: https://github.com/tensorflow/tensorflow/commit/ba9422a8adba18fc97cc1923002b7db8ca63dcfe\r\n\r\n@saeta Can we switch between secure/insecure grpc conditionally, to avoid breaking builds on big endian systems?\r\nBoring ssl does not support big endian, and they expressed that they have no intention to do so.\r\nSo in ppc64 systems we have to build without boring ssl.", "Thanks @gunan & @saeta.", "Thanks for merging so quickly @gunan and thank you @namrata-ibm for such a fast turnaround on helping us test these things out.\r\n\r\nSo you're aware, there are a couple more changes in-flight that might re-break the build. If I do end up re-breaking things, I apologize in advance!"]}, {"number": 20013, "title": "//tensorflow/contrib/distributions:matrix_inverse_tril_test fails on ppc64le", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source\r\n- **TensorFlow version (use command below)**:\r\n      TF master\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n     bazel-0.11.1\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n `bazel test -c opt --jobs 1 -k --cache_test_results=no --test_output=errors  //tensorflow/contrib/distributions:matrix_inverse_tril_test`\r\n\r\n### Describe the problem\r\nGetting error `InvalidArgumentError: assertion failed: [Input must be lower triangular.] [Condition x == y did not hold element-wise:]`.\r\nNot sure what exactly causing this issue, need to investigate.\r\n\r\n### Source code / logs\r\n```\r\nE.......\r\n======================================================================\r\nERROR: testBatch (__main__.MatrixInverseTriLBijectorTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 625, in decorated\r\n    f(self, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/matrix_inverse_tril_test.py\", line 110, in testBatch\r\n    y_, x_back_, fldj_, ildj_ = self.evaluate([y, x_back, fldj, ildj])\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 864, in evaluate\r\n    return sess.run(tensors)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: assertion failed: [Input must be lower triangular.] [Condition x == y did not hold element-wise:] [x (matrix_inverse_tril_1/inverse/MatrixBandPart:0) = ] [[[[0 2.77555756e-17][0]]]...] [y (matrix_inverse_tril_1/inverse/zeros_like:0) = ] [[[[0 0][0]]]...]\r\n         [[Node: matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_0, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_2, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_4, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_2)]]\r\n\r\nCaused by op u'matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert', defined at:\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/matrix_inverse_tril_test.py\", line 190, in <module>\r\n    test.main()\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 64, in main\r\n    return _googletest.main(argv)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 100, in main\r\n    benchmark.benchmarks_main(true_main=main_wrapper)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py\", line 344, in benchmarks_main\r\n    true_main()\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 99, in main_wrapper\r\n    return app.run(main=g_main, argv=args)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 70, in g_main\r\n    return unittest_main(argv=argv)\r\n  File \"/usr/lib/python2.7/unittest/main.py\", line 95, in __init__\r\n    self.runTests()\r\n  File \"/usr/lib/python2.7/unittest/main.py\", line 232, in runTests\r\n    self.result = testRunner.run(self.test)\r\n  File \"/usr/lib/python2.7/unittest/runner.py\", line 151, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\r\n    test(result)\r\n  File \"/usr/lib/python2.7/unittest/case.py\", line 393, in __call__\r\n    return self.run(*args, **kwds)\r\n  File \"/usr/lib/python2.7/unittest/case.py\", line 329, in run\r\n    testMethod()\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 625, in decorated\r\n    f(self, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/matrix_inverse_tril_test.py\", line 106, in testBatch\r\n    x_back = inv.inverse(x_inv_)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/distributions/bijector_impl.py\", line 800, in inverse\r\n    return self._call_inverse(y, name)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/distributions/bijector_impl.py\", line 779, in _call_inverse\r\n    mapping = mapping.merge(x=self._inverse(y, **kwargs))\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/ops/bijectors/matrix_inverse_tril.py\", line 80, in _inverse\r\n    return self._forward(y)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/ops/bijectors/matrix_inverse_tril.py\", line 74, in _forward\r\n    with ops.control_dependencies(self._assertions(x)):\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/ops/bijectors/matrix_inverse_tril.py\", line 138, in _assertions\r\n    message=\"Input must be lower triangular.\")\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/check_ops.py\", line 382, in assert_equal\r\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/util/tf_should_use.py\", line 118, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py\", line 151, in Assert\r\n    guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py\", line 2049, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py\", line 1890, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py\", line 149, in true_assert\r\n    condition, data, summarize, name=\"Assert\")\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_logging_ops.py\", line 51, in _assert\r\n    name=name)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 3206, in create_op\r\n    op_def=op_def)\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1701, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): assertion failed: [Input must be lower triangular.] [Condition x == y did not hold element-wise:] [x (matrix_inverse_tril_1/inverse/MatrixBandPart:0) = ] [[[[0 2.77555756e-17][0]]]...] [y (matrix_inverse_tril_1/inverse/zeros_like:0) = ] [[[[0 0][0]]]...]\r\n         [[Node: matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_0, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_2, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_4, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_2)]]\r\n\r\n\r\n----------------------------------------------------------------------\r\nRan 8 tests in 3.177s\r\n```", "comments": ["@jvdillon , can you take a look?", "I take the failing example, and run it thru numpy's linalg inverse method the inverse of 0 doesn't remain as 0 like on x86, it is populated with some non-logical value. This is breaking the method.\r\n\r\nRecreatable testcase:\r\n\r\n```\r\ndocker run -idt --name $USER-temp ubuntu:18.04 bash\r\ndocker attach $USER-temp\r\n\r\napt update\r\napt install vim python python-pip\r\npip install numpy\r\nvi test.py\r\npython test.py\r\n\r\n\r\n\r\nwhere test.py is:\r\n\r\nimport numpy as np\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    x_ = np.array([[[[1., 0.],\r\n                     [2., 3.]]],\r\n                   [[[4., 0.],\r\n                     [5., -6.]]]], dtype=np.float32)\r\n    print(x_)\r\n    x_inv_ = np.linalg.inv(x_)\r\n    print(x_inv_)\r\n\r\n\r\n\r\nx86:\r\n\r\nroot@08d7b342534c:/# python test.py\r\n[[[[ 1.  0.]\r\n   [ 2.  3.]]]\r\n\r\n\r\n [[[ 4.  0.]\r\n   [ 5. -6.]]]]\r\n[[[[ 1.          0.        ]\r\n   [-0.6666667   0.33333334]]]\r\n\r\n\r\n [[[ 0.25        0.        ]\r\n   [ 0.20833333 -0.16666667]]]]\r\n\r\n\r\n\r\n\r\nppc64le: \r\n\r\nroot@68c0b6023e55:/# python test.py\r\n[[[[ 1.  0.]\r\n   [ 2.  3.]]]\r\n\r\n\r\n [[[ 4.  0.]\r\n   [ 5. -6.]]]]\r\n[[[[ 1.0000000e+00  2.7755576e-17]\r\n   [-6.6666669e-01  3.3333334e-01]]]\r\n\r\n\r\n [[[ 2.5000000e-01  1.1102230e-17]\r\n   [ 2.0833333e-01 -1.6666667e-01]]]]\r\nroot@68c0b6023e55:/#\r\n\r\n\r\n```", "The inverse of 0 is technically undefined, right?", "I opened an issue against numpy on this: https://github.com/numpy/numpy/issues/11445", "@jvdillon , would you be able to review the PR #20477 to fix this issue?\r\n\r\nI took your suggestion from the numpy issue.", "Nagging Assignee @jvdillon: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The fix for this is in https://github.com/tensorflow/probability and will probably never be merged into tensorflow/tensorflow as that code is now deprecated.\r\n\r\n@sandipmgiri - Can this issue be closed ? "]}, {"number": 20012, "title": "//tensorflow/python:cluster_test test fails on ppc64le", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source\r\n- **TensorFlow version (use command below)**:\r\n      TF master\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n     bazel-0.11.1\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n `bazel test -c opt --jobs 1 -k --cache_test_results=no --test_output=errors  //tensorflow/python:cluster_test`\r\n\r\n### Describe the problem\r\nThis test is passing on X86 , however it's failing on ppc64le.\r\n\r\nThe test fails at line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/grappler/cluster_test.py#L84 i.e. ` self.assertEqual(52, peak_usage)`.\r\nOn x86 we are getting peak_usage=52 and test is passing.However on ppc64le peak_usage=48 hence test fails.\r\n\r\nCurrently I'm looking into source code to find the root cause. \r\nNeed to understand, how they are calculated peak_usage in this test.\r\nAny comments/suggestions appreciated.Thanks! \r\n\r\n### Source code / logs\r\n```\r\n======================================================================\r\nFAIL: testMemoryEstimates (__main__.ClusterTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/cluster_test.runfiles/org_tensorflow/tensorflow/python/grappler/cluster_test.py\", line 84, in testMemoryEstimates\r\n    self.assertEqual(52, peak_usage)\r\nAssertionError: 52 != 48L\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.030s\r\n\r\nFAILED (failures=1)\r\n```", "comments": ["@sandipmgiri , this test is not failing for me on ppc64le.\r\n\r\nThis is how I run the entire unit test bucket:\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\nscreen -L\r\nexport TF_BUILD_CONTAINER_TYPE=\"CPU.PPC64LE\"\r\nexport TF_BUILD_PYTHON_VERSION=\"PYTHON2\"\r\nexport TF_BUILD_IS_OPT=\"OPT\"\r\nexport TF_BUILD_IS_PIP=\"NO_PIP\"\r\nexport TF_BUILD_APPEND_ARGUMENTS=\"--test_tag_filters=-no_gpu,-no_oss,-oss_serial --color no\"\r\n./tensorflow/tools/ci_build/ci_parameterized_build.sh\r\n\r\nAfterwards , I can use the docker image created (tf_ci.cpu.ppc64le) to run bazel test on a single testcase as you have above.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Test passed successfully on high end vm. Closing this issue."]}, {"number": 20011, "title": "More documentation for keras adagrad and adadelta", "body": "Replicating PR https://github.com/keras-team/keras/pull/10410 from `keras`, as suggested by @fchollet ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@googlebot  I signed the CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "cool. can we merge this then?", "@fchollet ok, fixed. good to go.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20010, "title": "unclear doc: precision_at_k, recall_at_k in metric function (for estimator.add_metrics)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  1.7\r\n- **Python version**: 3\r\n- **GPU model and memory**: GPU:0 (not using GPU)\r\n- **Exact command to reproduce**:\r\ntf.metrics.precision_at_k(labels=labels, predictions=predictions, k=5)\r\ntf.metrics.precision_at_k(labels=labels, predictions=predictions['logits'], k=5)\r\n\r\n### Describe the problem\r\nMay I ask if it is possible to have clearer documentation on what is expected for the metric_fn for prebuilt metrics? (perhaps there is documentation that I have missed out, or perhaps I just did not understand them). For example, the link below demonstrates that auc can be calculated from predictions['logistic'] obtained from the estimator.\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/estimator/add_metrics\r\n`  def my_auc(labels, predictions):\r\n    return {'auc': tf.metrics.auc(labels, predictions['logistic'])}\r\n\r\n  estimator = tf.estimator.DNNClassifier(...)\r\n  estimator = tf.contrib.estimator.add_metrics(estimator, my_auc)`\r\n\r\nHowever, when I try to add precision_at_k and recall_at_k, I could not get the desired result.\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/metrics_impl.py\r\n`predictions: Float `Tensor` with shape [D1, ... DN, num_classes] where\r\n      N >= 1. Commonly, N=1 and predictions has shape [batch size, num_classes].\r\n      The final dimension contains the logit values for each class. [D1, ... DN]\r\n      must match `labels``\r\n\r\nI tried the following metric functions:\r\nCase A.\r\n`def precision_at_5(labels, predictions):\r\n      labels = tf.to_int64(labels)\r\n      return {'precision_at_5': tf.metrics.precision_at_k(labels=labels, predictions=predictions, k=5)}\r\n`\r\nCase B.\r\n`def precision_at_5(labels, predictions):\r\n      labels = tf.to_int64(labels)\r\n      return {'precision_at_5': tf.metrics.precision_at_k(labels=labels, predictions=predictions['logits'], k=5)} #or predictions['probabilities']\r\n`\r\n\r\n\r\n### Source code / logs\r\nThe estimator that uses the metric function above is as follows\r\n`estimator = tf.estimator.DNNLinearCombinedClassifier(\r\n        model_dir=model_dir,\r\n        linear_feature_columns=wide_columns,\r\n        dnn_feature_columns=deep_columns,\r\n        dnn_hidden_units=hidden_units,\r\n        config=run_config)\r\n    estimator = tf.contrib.estimator.add_metrics(estimator, precision_at_5)\r\n    estimator = tf.contrib.estimator.add_metrics(estimator, recall_at_5)\r\n    estimator = tf.contrib.estimator.add_metrics(estimator, average_precision_at_5)\r\n    return estimator`\r\n\r\nErrors for the 3 examples are as follows:\r\nCase A.\r\n`File \"wide_deep.py\", line 165, in precision_at_5\r\n    return {'precision_at_5': tf.metrics.precision_at_k(labels=labels, predictions=predictions, k=5)}\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/metrics_impl.py\", line 3405, in precision_at_k\r\n    _, top_k_idx = nn.top_k(predictions, k)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/nn_ops.py\", line 2354, in top_k\r\n    return gen_nn_ops.top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7631, in top_kv2\r\n    \"TopKV2\", input=input, k=k, sorted=sorted, name=name)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 513, in _apply_op_helper\r\n    raise err\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1040, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py\", line 235, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py\", line 214, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py\", line 522, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'logistic': <tf.Tensor 'head/predictions/logistic:0' shape=(?, 1) dtype=float32>, 'logits': <tf.Tensor 'add:0' shape=(?, 1) dtype=float32>, 'classes': <tf.Tensor 'head/predictions/str_classes:0' shape=(?, 1) dtype=string>, 'probabilities': <tf.Tensor 'head/predictions/probabilities:0' shape=(?, 2) dtype=float32>, 'class_ids': <tf.Tensor 'head/predictions/ExpandDims:0' shape=(?, 1) dtype=int64>}. Consider casting elements to a supported type.`\r\n\r\nCase B.\r\n`File \"wide_deep.py\", line 161, in precision_at_5\r\n    return {'precision_at_5': tf.metrics.precision_at_k(labels=labels, predictions=predictions['logits'], k=5)}\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/metrics_impl.py\", line 3405, in precision_at_k\r\n    _, top_k_idx = nn.top_k(predictions, k)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/nn_ops.py\", line 2354, in top_k\r\n    return gen_nn_ops.top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7631, in top_kv2\r\n    \"TopKV2\", input=input, k=k, sorted=sorted, name=name)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 3292, in create_op\r\n    compute_device=compute_device)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 3332, in _create_op_helper\r\n    set_shapes_for_outputs(op)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2496, in set_shapes_for_outputs\r\n    return _set_shapes_for_outputs(op)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2469, in _set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2399, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/Users/jinyunsoo/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\", line 691, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: input must have last dimension >= k = 5 but is 1 for 'precision_at_5/TopKV2' (op: 'TopKV2') with input shapes: [?,1], [] and with computed input tensors: input[1] = <5>.`\r\n\r\n\r\nThank you very much!", "comments": ["This was intended for precision based on Top K recommendations (Top K highest probability of being class 1 returned). I realise now that the nn.top_k used within tf.metrics.precision_at_k means something different, and so k has to be <= last dimension of the predictions. I'll have to write a custom metrics function."]}, {"number": 20009, "title": "tf.get_variable(reuse=tf.AUTO_REUSE) is not working in the eager execution mode.", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDarwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen calling get_variable() function in tensorflow, the behavior of the \"reuse\" flag is defined in the [tensorflow api doc](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to be AUTO_REUSE:\r\n\r\n> reuse: True, None, or tf.AUTO_REUSE; ... **When eager execution is enabled, this argument is always forced to be tf.AUTO_REUSE**.\r\n\r\nHowever when I run the demo code as suggested in the api doc:\r\n\r\n```\r\ntf.enable_eager_execution()\r\ndef foo():\r\n  with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\r\n    v = tf.get_variable(\"v\", [1])\r\n  return v\r\nv1 = foo()  # Creates v.\r\nv2 = foo()  # Gets the same, existing v.\r\nassert v1 == v2\r\n```\r\n\r\nIt fails. (It passes if the first line is removed, as expected.)\r\n\r\nNot sure whether it is a bug or expected behavior, but at least the api doc seems kind of misleading. If it's expected, then what's the correct way to reuse a variable in eager mode? (and can this be included in the api doc?)", "comments": ["This is sadly working as intended and we need to update the documentation. We don't support variable reuse in eager other than inside templates."]}, {"number": 20008, "title": "//tensorflow/python:cost_analyzer_test test fails on ppc64le", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source\r\n- **TensorFlow version (use command below)**:\r\n      TF master\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n     bazel-0.11.1\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n `bazel test -c opt --jobs 1 -k --cache_test_results=no --test_output=errors  //tensorflow/python:cost_analyzer_test`\r\n\r\n### Describe the problem\r\nFAIL: testBasicMemory (__main__.CostAnalysisTest)\r\nMake sure arguments can be passed correctly.\r\nAssertionError: False is not true\r\n\r\nI have started looking into this. \r\nAny comments/suggestions appreciated.Thanks! \r\n### Source code / logs\r\n```\r\nINFO: Analysed target //tensorflow/python:cost_analyzer_test (0 packages loaded).\r\nINFO: Found 1 test target...\r\nFAIL: //tensorflow/python:cost_analyzer_test (see /root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/python/cost_analyzer_test/test.log)\r\nINFO: From Testing //tensorflow/python:cost_analyzer_test:\r\n==================== Test output for //tensorflow/python:cost_analyzer_test:\r\n2018-06-14 09:10:09.134980: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-06-14 09:10:09.135207: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-06-14 09:10:09.136755: I tensorflow/core/grappler/clusters/single_machine.cc:349] Cleaning up previous session\r\n2018-06-14 09:10:09.138122: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-06-14 09:10:09.147635: I tensorflow/core/grappler/clusters/single_machine.cc:349] Cleaning up previous session\r\n2018-06-14 09:10:09.148782: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n.2018-06-14 09:10:09.157410: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-06-14 09:10:09.157507: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\nF2018-06-14 09:10:09.352496: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-06-14 09:10:09.352692: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-06-14 09:10:09.359070: I tensorflow/core/grappler/clusters/single_machine.cc:349] Cleaning up previous session\r\n2018-06-14 09:10:09.360080: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-06-14 09:10:09.566104: I tensorflow/core/grappler/clusters/single_machine.cc:349] Cleaning up previous session\r\n2018-06-14 09:10:09.569427: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n\r\nTotal time measured in ns (serialized):                           5000\r\nTotal time measured in ns (actual):                             281002\r\nTotal time analytical in ns (upper bound):        -9223372036854775000\r\nTotal time analytical in ns (lower bound):                           0\r\nOverall efficiency (analytical upper/actual):             -3.28232e+13\r\nOverall efficiency (analytical lower/actual):                        0\r\n\r\n                                 Op,          Count,  Measured time (ns),    Time percent,     Acc percent,    Analytical upper,    Analytical lower,      Overall eff      Compute eff       Memory eff\r\n                               AddN,              1,                5000,          1e+02%,          1e+02%,-9223372036854775000,                   0,       -1.8e+17%,       -1.8e+17%,              0%,\r\n\r\nBelow is the per-node report summary:\r\n                                 Op,  Measured time (ns),   Compute time (ns),    Memory time (ns),     Compute eff,      Memory eff,    Inputs\r\n                               AddN,                5000,-9223372036854775000,                   0,       -1.8e+17%,           -inf%,    []\r\n\r\n\r\nPeak usage for device /job:localhost/replica:0/task:0/device:CPU:0: 12 bytes\r\n  a:0 uses 4 bytes\r\n  b:0 uses 4 bytes\r\n  c:0 uses 4 bytes\r\n\r\n\r\nTotal time measured in ns (serialized):                        8460000\r\nTotal time measured in ns (actual):                            5328500\r\nTotal time analytical in ns (upper bound):        -9223372036854519848\r\nTotal time analytical in ns (lower bound):        -9223372036854655544\r\nOverall efficiency (analytical upper/actual):             -1.73095e+12\r\nOverall efficiency (analytical lower/actual):             -1.73095e+12\r\n\r\n                                 Op,          Count,  Measured time (ns),    Time percent,     Acc percent,    Analytical upper,    Analytical lower,      Overall eff      Compute eff       Memory eff\r\n                             MatMul,              3,             2160000,             26%,             26%,-9223372036854676232, 9223372036854773384,       -4.3e+14%,        4.3e+14%,            4.7%,\r\n                          ApplyAdam,              4,             1422000,             17%,             42%,              128232,              125000,              9%,           0.23%,            8.8%,\r\n               Conv2DBackpropFilter,              1,             1040000,             12%,             55%, 9223372036854775000, 9223372036854775000,        8.9e+14%,        8.9e+14%,              0%,\r\n                           ReluGrad,              1,              958000,             11%,             66%,-9223372036854767616, 9223372036854775000,       -9.6e+14%,        9.6e+14%,           0.94%,\r\n                             Conv2D,              1,              604000,            7.1%,             73%,-9223372036854773616, 9223372036854775000,       -1.5e+15%,        1.5e+15%,            0.5%,\r\n                         VariableV2,             14,              537000,            6.3%,             79%,                   0,                   0,              0%,              0%,              0%,\r\n                                Sum,              4,              513000,            6.1%,             86%,               12232,                9000,            2.4%,           0.63%,            1.8%,\r\n                              Const,             10,              465000,            5.5%,             91%,                   0,                   0,              0%,              0%,              0%,\r\n                                Mul,              6,              415000,            4.9%,             96%,               -4848,               -4848,           -1.2%,           -1.2%,              0%,\r\n                                Add,              2,              205000,            2.4%,             98%,                4384,               -1616,            2.1%,          -0.79%,            2.9%,\r\n                               Relu,              1,               35000,           0.41%,             99%,-9223372036854770616, 9223372036854775000,       -2.6e+16%, .2018-06-14 09:10:09.738459: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-06-14 09:10:09.738574: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-06-14 09:10:09.739464: I tensorflow/core/grappler/clusters/single_machine.cc:349] Cleaning up previous session\r\n2018-06-14 09:10:09.740389: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-06-14 09:10:09.747818: I tensorflow/core/grappler/clusters/single_machine.cc:349] Cleaning up previous session\r\n2018-06-14 09:10:09.749965: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n..\r\n======================================================================\r\nFAIL: testBasicMemory (__main__.CostAnalysisTest)\r\nMake sure arguments can be passed correctly.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/cost_analyzer_test.runfiles/org_tensorflow/tensorflow/python/grappler/cost_analyzer_test.py\", line 152, in testBasicMemory\r\n    in report)\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 5 tests in 0.639s\r\n\r\nFAILED (failures=1)\r\n       2.6e+16%,             17%,\r\n                         Reciprocal,              1,               25000,            0.3%,             99%, 9223372036854775000, 9223372036854775000,        3.7e+16%,        3.7e+16%,              0%,\r\n                           Identity,              6,               24000,           0.28%,             99%,                   0,                   0,              0%,              0%,              0%,\r\n                            Reshape,              6,               22000,           0.26%,          1e+02%,                   0,                   0,              0%,              0%,              0%,\r\n                             Assign,              2,               10000,           0.12%,          1e+02%,                1616,                   0,             16%,             16%,              0%,\r\n                            Softmax,              1,                9000,           0.11%,          1e+02%,-9223372036854775000,                   0,         -1e+17%,         -1e+17%,              0%,\r\n                               Tile,              1,                7000,          0.083%,          1e+02%,-9223372036854775000,                   0,       -1.3e+17%,       -1.3e+17%,              0%,\r\n                                Sub,              1,                5000,          0.059%,          1e+02%, 9223372036854775000, 9223372036854775000,        1.8e+17%,        1.8e+17%,              0%,\r\n                               NoOp,              2,                4000,          0.047%,          1e+02%,                   0,                   0,              0%,              0%,              0%,\r\n\r\n\r\n\r\nTotal time measured in ns (serialized):                           5000\r\nTotal time measured in ns (actual):                             149014\r\nTotal time analytical in ns (upper bound):        -9223372036854775000\r\nTotal time analytical in ns (lower bound):                           0\r\nOverall efficiency (analytical upper/actual):              -6.1896e+13\r\nOverall efficiency (analytical lower/actual):                        0\r\n\r\n                                 Op,          Count,  Measured time (ns),    Time percent,     Acc percent,    Analytical upper,    Analytical lower,      Overall eff      Compute eff       Memory eff\r\n                               AddN,              1,                5000,          1e+02%,          1e+02%,-9223372036854775000,                   0,       -1.8e+17%,       -1.8e+17%,              0%,\r\n\r\nBelow is the full per-node report:\r\nop_performance {\r\n  op {\r\n    op: \"AddN\"\r\n    attr {\r\n      key: \"N\"\r\n      value {\r\n        i: 2\r\n      }\r\n    }\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_INT32\r\n      }\r\n    }\r\n    inputs {\r\n      shape {\r\n        unknown_rank: true\r\n      }\r\n      value {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n        }\r\n        int_val: 20\r\n      }\r\n    }\r\n    inputs {\r\n      shape {\r\n        unknown_rank: true\r\n      }\r\n    }\r\n    device {\r\n      type: \"CPU\"\r\n      model: \"0\"\r\n      num_cores: 4\r\n      environment {\r\n        key: \"cpu_instruction_set\"\r\n        value: \"VSX\"\r\n      }\r\n      environment {\r\n        key: \"eigen\"\r\n        value: \"3.3.90\"\r\n      }\r\n      l1_cache_size: 16384\r\n      l2_cache_size: 524288\r\n      l3_cache_size: 524288\r\n      memory_size: 993132544\r\n    }\r\n  }\r\n  compute_cost: 5000\r\n  compute_efficiency: -1844674407370955\r\n  node: \"d\"\r\n  compute_time: -9223372036854775000\r\n  memory_efficiency: -inf\r\n  op_memory {\r\n    output_memory: 4\r\n    persistent_memory: 4\r\n  }\r\n}\r\n\r\n================================================================================\r\nTarget //tensorflow/python:cost_analyzer_test up-to-date:\r\n  bazel-bin/tensorflow/python/cost_analyzer_test\r\nINFO: Elapsed time: 1.835s, Critical Path: 1.36s\r\nINFO: Build completed, 1 test FAILED, 2 total actions\r\n//tensorflow/python:cost_analyzer_test                                   FAILED in 1.4s\r\n```\r\n", "comments": ["/CC @gunan, how should ppc64le issues be handled?", "Out of 5 sub-tests only one is failing i.e. testBasicMemory .  \r\n```\r\ndef testBasicMemory(self):\r\n    \"\"\"Make sure arguments can be passed correctly.\"\"\"\r\n    with test_util.device(use_gpu=False):\r\n      a = constant_op.constant(10, name=\"a\")\r\n      b = constant_op.constant(20, name=\"b\")\r\n      c = math_ops.add_n([a, b], name=\"c\")\r\n      d = math_ops.add_n([b, c], name=\"d\")\r\n      train_op = ops.get_collection_ref(ops.GraphKeys.TRAIN_OP)\r\n      train_op.append(d)\r\n      mg = meta_graph.create_meta_graph_def(graph=ops.get_default_graph())\r\n\r\n    report = cost_analyzer.GenerateMemoryReport(mg)\r\n\r\n    # Print the report to make it easier to debug\r\n    print(\"{}\".format(report))\r\n\r\n    # Check the report\r\n    self.assertTrue(\r\n        \"Peak usage for device /job:localhost/replica:0/task:0/device:CPU:0: \"\r\n        \"16 bytes\"\r\n        in report)\r\n    self.assertTrue(\"  a:0 uses 4 bytes\" in report)\r\n    self.assertTrue(\"  b:0 uses 4 bytes\" in report)\r\n    self.assertTrue(\"  c:0 uses 4 bytes\" in report)\r\n    self.assertTrue(\"  d:0 uses 4 bytes\" in report)\r\n```\r\nThe test is failing due to incorrect peak usage on ppc64le :\r\n```\r\nPeak usage for device /job:localhost/replica:0/task:0/device:CPU:0: 12 bytes\r\n  a:0 uses 4 bytes\r\n  b:0 uses 4 bytes\r\n  c:0 uses 4 bytes\r\n```\r\n  \r\nHowever on X86 , we are getting peak usage as below and test is passing :\r\n```\r\nPeak usage for device /job:localhost/replica:0/task:0/device:CPU:0: 16 bytes\r\n  a:0 uses 4 bytes\r\n  b:0 uses 4 bytes\r\n  d:0 uses 4 bytes\r\n  c:0 uses 4 bytes\r\n```\r\n  \r\nLooks like last 4 bytes are missing on ppc64le. Looking further...  \r\n\r\nRelevant log for ppc64le : \r\n```\r\nF.\r\n======================================================================\r\nFAIL: testBasicMemory (__main__.CostAnalysisTest)\r\nMake sure arguments can be passed correctly.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/cost_analyzer_test.runfiles/org_tensorflow/tensorflow/python/grappler/cost_analyzer_test.py\", line 61, in testBasicMemory\r\n    in report)\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.020s\r\n\r\nFAILED (failures=1)\r\nPeak usage for device /job:localhost/replica:0/task:0/device:CPU:0: 12 bytes\r\n  a:0 uses 4 bytes\r\n  b:0 uses 4 bytes\r\n  c:0 uses 4 bytes\r\n\r\n================================================================================\r\nTarget //tensorflow/python:cost_analyzer_test up-to-date:\r\n```", "Adding @wdirons as he is also fixing a lot of ppc64le issues.", "On both the platform (x86 and ppc64le) getting same \"mg\"(metagraph) value , see below :\r\n\r\n```\r\nmeta_info_def {\r\n  stripped_op_list {\r\n    op {\r\n      name: \"AddN\"\r\n      input_arg {\r\n        name: \"inputs\"\r\n        type_attr: \"T\"\r\n        number_attr: \"N\"\r\n      }\r\n      output_arg {\r\n        name: \"sum\"\r\n        type_attr: \"T\"\r\n      }\r\n      attr {\r\n        name: \"N\"\r\n        type: \"int\"\r\n        has_minimum: true\r\n        minimum: 1\r\n      }\r\n      attr {\r\n        name: \"T\"\r\n        type: \"type\"\r\n        allowed_values {\r\n          list {\r\n            type: DT_FLOAT\r\n            type: DT_DOUBLE\r\n            type: DT_INT32\r\n            type: DT_UINT8\r\n            type: DT_INT16\r\n            type: DT_INT8\r\n            type: DT_COMPLEX64\r\n            type: DT_INT64\r\n            type: DT_QINT8\r\n            type: DT_QUINT8\r\n            type: DT_QINT32\r\n            type: DT_BFLOAT16\r\n            type: DT_UINT16\r\n            type: DT_COMPLEX128\r\n            type: DT_HALF\r\n            type: DT_UINT32\r\n            type: DT_UINT64\r\n            type: DT_VARIANT\r\n          }\r\n        }\r\n      }\r\n      is_aggregate: true\r\n      is_commutative: true\r\n    }\r\n    op {\r\n      name: \"Const\"\r\n      output_arg {\r\n        name: \"output\"\r\n        type_attr: \"dtype\"\r\n      }\r\n      attr {\r\n        name: \"value\"\r\n        type: \"tensor\"\r\n      }\r\n      attr {\r\n        name: \"dtype\"\r\n        type: \"type\"\r\n      }\r\n    }\r\n  }\r\n  tensorflow_version: \"1.9.0-rc0\"\r\n  tensorflow_git_version: \"v1.8.0-3287-g5ae5ab4\"\r\n}\r\ngraph_def {\r\n  node {\r\n    name: \"a\"\r\n    op: \"Const\"\r\n    device: \"/device:CPU:0\"\r\n    attr {\r\n      key: \"_output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"dtype\"\r\n      value {\r\n        type: DT_INT32\r\n      }\r\n    }\r\n    attr {\r\n      key: \"value\"\r\n      value {\r\n        tensor {\r\n          dtype: DT_INT32\r\n          tensor_shape {\r\n          }\r\n          int_val: 10\r\n        }\r\n      }\r\n    }\r\n  }\r\n  node {\r\n    name: \"b\"\r\n    op: \"Const\"\r\n    device: \"/device:CPU:0\"\r\n    attr {\r\n      key: \"_output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"dtype\"\r\n      value {\r\n        type: DT_INT32\r\n      }\r\n    }\r\n    attr {\r\n      key: \"value\"\r\n      value {\r\n        tensor {\r\n          dtype: DT_INT32\r\n          tensor_shape {\r\n          }\r\n          int_val: 20\r\n        }\r\n      }\r\n    }\r\n  }\r\n  node {\r\n    name: \"c\"\r\n    op: \"AddN\"\r\n    input: \"a\"\r\n    input: \"b\"\r\n    device: \"/device:CPU:0\"\r\n    attr {\r\n      key: \"N\"\r\n      value {\r\n        i: 2\r\n      }\r\n    }\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_INT32\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  node {\r\n    name: \"d\"\r\n    op: \"AddN\"\r\n    input: \"b\"\r\n    input: \"c\"\r\n    device: \"/device:CPU:0\"\r\n    attr {\r\n      key: \"N\"\r\n      value {\r\n        i: 2\r\n      }\r\n    }\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_INT32\r\n      }\r\n    }\r\n    attr {\r\n      key: \"_output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  versions {\r\n    producer: 26\r\n  }\r\n}\r\ncollection_def {\r\n  key: \"train_op\"\r\n  value {\r\n    node_list {\r\n      value: \"d:0\"\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\nHowever , `report = cost_analyzer.GenerateMemoryReport(mg)` line returns incorrect result on ppc64le :\r\n```\r\nPeak usage for device /job:localhost/replica:0/task:0/device:CPU:0: 12 bytes\r\n  a:0 uses 4 bytes\r\n  b:0 uses 4 bytes\r\n  c:0 uses 4 bytes\r\n```\r\n  \r\nExpected :\r\n```\r\nPeak usage for device /job:localhost/replica:0/task:0/device:CPU:0: 16 bytes\r\n  a:0 uses 4 bytes\r\n  b:0 uses 4 bytes\r\n  d:0 uses 4 bytes\r\n  c:0 uses 4 bytes\r\n  \r\n```\r\n\r\nI have looked into the `GenerateMemoryReport() `function in file `tensorflow/python/grappler/cost_analyzer.py`, and got to know that the `peak_usage = cluster.DeterminePeakMemoryUsage(item) `line returning incorrect value.\r\n\r\nNow looking into the DeterminePeakMemoryUsage() code.\r\n  \r\n", "@sandipmgiri , this test is not failing for me on ppc64le.\r\n\r\nThis is how I run the entire unit test bucket:\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\nscreen -L\r\nexport TF_BUILD_CONTAINER_TYPE=\"CPU.PPC64LE\"\r\nexport TF_BUILD_PYTHON_VERSION=\"PYTHON2\"\r\nexport TF_BUILD_IS_OPT=\"OPT\"\r\nexport TF_BUILD_IS_PIP=\"NO_PIP\"\r\nexport TF_BUILD_APPEND_ARGUMENTS=\"--test_tag_filters=-no_gpu,-no_oss,-oss_serial --color no\"\r\n./tensorflow/tools/ci_build/ci_parameterized_build.sh\r\n\r\nAfterwards , I can use the docker image created (tf_ci.cpu.ppc64le) to run bazel test as you have above.", "@wdirons Tried to execute the steps as you mentioned , however I'm getting the below error :  \r\n\r\n```\r\n./tensorflow/tools/ci_build/ci_parameterized_build.sh\r\nParameterized build starts at: Wed Jul  4 12:25:36 UTC 2018\r\n\r\nRequired build parameters:\r\n  TF_BUILD_CONTAINER_TYPE=cpu.ppc64le\r\n  TF_BUILD_PYTHON_VERSION=python2\r\n  TF_BUILD_IS_OPT=opt\r\n  TF_BUILD_IS_PIP=no_pip\r\nOptional build parameters:\r\n  TF_BUILD_DRY_RUN=\r\n  TF_BUILD_MAVX=\r\n  TF_BUILD_APPEND_CI_DOCKER_EXTRA_PARAMS=\r\n  TF_BUILD_APPEND_ARGUMENTS=--test_tag_filters=-no_gpu,-no_oss,-oss_serial --color no\r\n  TF_BUILD_BAZEL_TARGET=\r\n  TF_BUILD_BAZEL_CLEAN=\r\n  TF_BUILD_TEST_TUTORIALS=\r\n  TF_BUILD_INTEGRATION_TESTS=\r\n  TF_BUILD_RUN_BENCHMARKS=\r\n  TF_BUILD_OPTIONS=\r\nExporting CI_DOCKER_EXTRA_PARAMS: -v /tmp/tmp.VAbHWZouts_ci_parameterized_build.sh:/tmp/tf_build.sh\r\nExecuting final command (/tmp/tmp.VAbHWZouts_ci_parameterized_build.sh)...\r\n==========================================\r\n#!/usr/bin/env bash\r\nbazel test --test_output=errors -c opt --test_tag_filters=-no_gpu,-no_oss,-oss_serial,-benchmark-test --color no --distinct_host_configuration=false -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/... //tensorflow/contrib/lite:context_test //tensorflow/contrib/lite:framework //tensorflow/contrib/lite:interpreter_test //tensorflow/contrib/lite:model_test //tensorflow/contrib/lite/toco:toco //tensorflow/contrib/lite:simple_memory_arena_test //tensorflow/contrib/lite:string_util_test //tensorflow/contrib/lite/kernels:activations_test //tensorflow/contrib/lite/kernels:add_test //tensorflow/contrib/lite/kernels:basic_rnn_test //tensorflow/contrib/lite/kernels:concatenation_test //tensorflow/contrib/lite/kernels:conv_test //tensorflow/contrib/lite/kernels:depthwise_conv_test //tensorflow/contrib/lite/kernels:embedding_lookup_test //tensorflow/contrib/lite/kernels:embedding_lookup_sparse_test //tensorflow/contrib/lite/kernels:fully_connected_test //tensorflow/contrib/lite/kernels:hashtable_lookup_test //tensorflow/contrib/lite/kernels:local_response_norm_test //tensorflow/contrib/lite/kernels:lsh_projection_test //tensorflow/contrib/lite/kernels:lstm_test //tensorflow/contrib/lite/kernels:l2norm_test //tensorflow/contrib/lite/kernels:mul_test //tensorflow/contrib/lite/kernels:pooling_test //tensorflow/contrib/lite/kernels:reshape_test //tensorflow/contrib/lite/kernels:resize_bilinear_test //tensorflow/contrib/lite/kernels:skip_gram_test //tensorflow/contrib/lite/kernels:softmax_test //tensorflow/contrib/lite/kernels:space_to_depth_test //tensorflow/contrib/lite/kernels:svdf_test\r\n==========================================\r\n\r\nWORKSPACE: /root/tensorflow\r\nCI_DOCKER_EXTRA_PARAMS: -v /tmp/tmp.VAbHWZouts_ci_parameterized_build.sh:/tmp/tf_build.sh\r\nCOMMAND: /tmp/tf_build.sh\r\nCI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured cpu.ppc64le\r\nCONTAINER_TYPE: cpu.ppc64le\r\nBUILD_TAG: tf_ci\r\n  (docker container name will be tf_ci.cpu.ppc64le)\r\n\r\nBuilding container (tf_ci.cpu.ppc64le)...\r\nSending build context to Docker daemon 390.7 kB\r\nStep 1 : FROM ubuntu:16.04\r\n ---> a267250427b3\r\nStep 2 : LABEL maintainer \"William Irons <wdirons@us.ibm.com>\"\r\n ---> Using cache\r\n ---> f470162bd3d1\r\nStep 3 : COPY install/*.sh /install/\r\n ---> Using cache\r\n ---> 59704b8bc034\r\nStep 4 : RUN /install/install_bootstrap_deb_packages.sh\r\n ---> Using cache\r\n ---> 82a4ac2345c4\r\nStep 5 : RUN add-apt-repository -y ppa:openjdk-r/ppa\r\n ---> Using cache\r\n ---> ac7be5fc321f\r\nStep 6 : RUN /install/install_deb_packages.sh\r\n ---> Using cache\r\n ---> b30410c2d510\r\nStep 7 : RUN apt-get update && apt-get install -y libopenblas-dev\r\n ---> Using cache\r\n ---> 65b05a51b4a7\r\nStep 8 : RUN /install/install_hdf5_ppc64le.sh\r\n ---> Using cache\r\n ---> b0c5826da7fc\r\nStep 9 : RUN /install/install_pip_packages.sh\r\n ---> Using cache\r\n ---> 21d9e8b7e9bf\r\nStep 10 : RUN /install/install_bazel_from_source.sh\r\n ---> Using cache\r\n ---> e4b783fec046\r\nStep 11 : RUN /install/install_proto3.sh\r\n ---> Using cache\r\n ---> b90d0afd44f5\r\nStep 12 : RUN /install/install_buildifier_from_source.sh\r\n ---> Using cache\r\n ---> bbbe9d957269\r\nStep 13 : RUN /install/install_auditwheel.sh\r\n ---> Using cache\r\n ---> 7c86f6b0e0bb\r\nStep 14 : RUN /install/install_golang_ppc64le.sh\r\n ---> Using cache\r\n ---> e452271dfd67\r\nStep 15 : COPY install/.bazelrc /etc/bazel.bazelrc\r\n ---> Using cache\r\n ---> 44e52aaaa7fa\r\nSuccessfully built 44e52aaaa7fa\r\nRunning '/tmp/tf_build.sh' inside tf_ci.cpu.ppc64le...\r\ndocker: Error response from daemon: oci runtime error: exec: \"./tensorflow/tools/ci_build/builds/with_the_same_user\": stat ./tensorflow/tools/ci_build/builds/with_the_same_user: no such file or directory.\r\ntime=\"2018-07-04T12:25:36Z\" level=error msg=\"error getting events from daemon: net/http: request canceled\"\r\n```", "@sandipmgiri , I'll work with you offline. I don't want to confused this issue trying to debug the unit test process.", "Test passed successfully on high end vm. Closing this issue."]}, {"number": 20007, "title": "non random operation should not change the graph random state(1.5 cpu python 2.7\uff09", "body": "Not sure why the random state changed for non random operation like tf.constant\r\n\r\n`pip freeze|grep \"python\\|tensorflow\"`\r\npython-dateutil==2.7.2\r\ntensorflow==1.5.0\r\ntensorflow-tensorboard==1.5.1\r\n\r\ncat test.py\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nseed = 0\r\ndef test1():\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        tf.set_random_seed(seed)\r\n        aaa = tf.random_uniform([1])\r\n    sess = tf.Session(graph = graph)\r\n    print(sess.run(aaa))\r\n    print(sess.run(aaa))\r\n\r\ndef test2():\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        tf.set_random_seed(seed)\r\n        bbb = tf.constant([1,2])\r\n        aaa = tf.random_uniform([1])\r\n    sess = tf.Session(graph = graph)\r\n    print(sess.run(aaa))\r\n    print(sess.run(aaa))\r\nif __name__ == \"__main__\":\r\n    test1()\r\n    test1()\r\n    test2()\r\n```\r\npython test.py\r\n/home/tom/pyenv/anti/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n1.5.0\r\n2018-06-14 16:06:47.429664: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n[0.3206401]\r\n[0.012326]\r\n[0.3206401]\r\n[0.012326]\r\n[0.2773143]\r\n[0.92967796]\r\n", "comments": ["duplication of #14675 ?", "Looks the same.\r\n\r\nBut be noticed this is in CPU(according to my experience even same graph in GPU can not reproduce the same due to GPU internal mechanism)\r\n\r\nI still think non random operation should not change the random state(whatever it is a bigger or small change of the graph). Do not know why it is hard for tensorflow.\r\n\r\nI did a little change of my graph without random operation and the performance changed about 1 percent. It will be more friendly to users if tensorflow can generate the same result."]}, {"number": 20006, "title": "C++ load and running Tensorflow Model Crash", "body": "I use C++ load and running tensorflow model , but it`s crash.\r\nthe core is :\r\n![304826260213283160](https://user-images.githubusercontent.com/19491069/41392235-15e627ce-6fd2-11e8-98c8-461d430ce05d.png)\r\nplease tell me how to solve it\r\nThinks!!!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I compile tensorflow source code and use it on two different servers\r\n**compile:**\r\n- Linux version 2.6.32-431.el6.x86_64(redhat 4.4.7)\r\n- gcc4.8.5\r\n- bazel0.9.0\r\n- TensorFlow installed from github(https://github.com/tensorflow/tensorflow/releases?after=v1.5.1)\r\n- TensorFlow 1.4.0\r\n- CUDA Version 8.0.61  /cuDNN: CUDNN_MAJOR  5\r\n- GPU:Tesla K40m  Memory:11439MiB\r\n\r\n**use:**\r\n- Linux version 2.6.32-279.el6.x86_64(redhat 4.4.6)\r\n- gcc4.4.6 (but i use it with libstdc++.so.6)\r\n\r\nI write a dome that just create a session and run it. After my datas have been recycled many times, the program crashed. I supervised the memory and it is stable.\r\nThe crash happened in the libtensorflow_framework.so. I don't konw how it works.\r\n", "Sorry I have difficulty understand your question. \r\n\r\nIn generalThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20005, "title": "tf lite demo throw the error: Op builtin_code out or range: 59", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: tensorflow 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)** : 6.4\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: GeForce GTX 750\r\n- **Exact command to reproduce**: run mtcnn pnet model in tflite android demo of tflitecamerademo.\r\n\r\n### Describe the problem\r\nTflite generate is ok.\r\nBut run in tflitecamerademo error:\r\n\"Op builtin_code out or range: 59. Are you using old TFLite binary with newer model?Registration failed.\"\r\nFor the version of 3daa07aa2dde379388beb2a557a78bc5dd1b86ba on June 6.\r\nIt is strange that running demo ok on June 6. \r\non June 11, I download new code to another folder to run the demo,\r\nit was error as below log.\r\nThen, I use the demo of June 6, \r\nthe error appeared.\r\nI checked the jar file, it is not changed.\r\nSo now I will reproduce the error both the version of June 6, and June 11.\r\nI don't know how the June 6 be changed, \r\nand how to resolve the error  about \"Op builtin_code out or range: 59\"\r\n\r\n### Source code / logs\r\nThe error log in adb as below:\r\n   \r\n    Caused by: java.lang.IllegalArgumentException: Cannot create interpreter: Op builtin_code out or range: 59. Are you using old TFLite binary with newer model?Registration failed.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:90)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:96)\r\n        at com.example.android.tflitecamerademo.ImageClassifierFloatInception.<init>(ImageClassifierFloatInception.java:50)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:335)\r\n        at android.app.Fragment.performActivityCreated(Fragment.java:2077)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:917)\r\n        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1072)\r\n        at android.app.BackStackRecord.run(BackStackRecord.java:852)\r\n        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:1478)\r\n        at android.app.Activity.performStart(Activity.java:6107)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2491)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2608)\u00a0\r\n        at android.app.ActivityThread.access$800(ActivityThread.java:178)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1470)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:111)\u00a0\r\n        at android.os.Looper.loop(Looper.java:194)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:5637)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:959)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:754)\u00a0\r\n", "comments": ["I rebuilt libtensorflowlite_jni.so, But I can't find the path to replace.\r\nI delete all the libtensorflowlite_jni.so files in my computer, \r\nIt can run mobilenet module OK ...\r\nBefore June 4, I can replace the libtensorflowlite_jni.so, I can add my log to the jni so.\r\nI guess the error of \"Op builtin_code out or range: 59 \" is caused by the jni so file is not update.", "After compile with 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nthe error of \"Op builtin_code out or range: 59 \" disappeared.\r\nNow it is OK.\r\n\r\nNow I just don't know how to replace libtensorflowlite_jni.so in my enviroment.\r\nBut it is not important now.", "@andrehentz any hints about the libtensorflowlite_jni.so file?", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\nOriginal issue solved. "]}, {"number": 20004, "title": "New loss function for Neural Networks based on 2018 paper from Cornell ", "body": "Zhilu Zhang and Mert R. Sabuncu of Cornell University (New York, United States) has submitted on May 20, 2018 a [paper](https://arxiv.org/abs/1805.07836)  proposing a new error function for `Neural Networks` (including Deep Neural Networks) aimed at classification with `softmax` function in the output layer,  specially in the case of noisy labels. \r\n\r\nThis new loss function has joined characteristics of `mean absolute error (MAE)` with `cross entropy loss`. \r\n\r\nAccording to the tests carried out, it generates a significant improvement in problems of binary classification or multiclassification with noisy labels, compared to cross entropy loss or mean absolute error (MAE) \r\n\r\nBy e-mail, we exchanged ideas for some time, where I has clarified some points of this paper. \r\n\r\nBecause of the promising results obtained, I've agreed with them that I would incorporate this new function into the repertoire of loss functions in `TensorFlow` to make it available to the community.\r\n\r\nSo I've read the `contribution guidelines` and write Python code for new  function (that I've called  `gen_crossentropy`) and also write the testing code, following the standards of another loss error functions. \r\n\r\nIn my local repository, the test have run 100% well. \r\n\r\nIt's my first collaboration with this fantastic project, so any observations or guidance will be welcome.", "comments": ["I'm going to travel today and I'm back at July-10. In case of some interaction in this pull request, I'm only answer after that date. ", "Nagging Reviewer @fchollet: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "I'm back and waiting ... ", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Thank you for the PR.\r\n\r\nAfter review, we have decided not to make this addition to the Keras API.\r\n\r\nEvery feature has a cost that goes beyond the initial PR: maintenance cost, documentation cost, and cognitive cost for users. In the Keras API, every new feature has to be maintained in perpetuity, and has to be replicated in every implementation of the Keras API (which includes tf.keras, multi-backend Keras, tensorflow.js, keras-mxnet, and others).\r\n\r\nAs, such, our criteria for adding a new feature in the API is the following:\r\n\r\n- It should be broadly useful to Keras users, not a niche feature. Niche features should be maintained independently by those who need them (e.g. by extending the API via subclassing), as third-party Keras add-on packages.\r\n- It should be broadly recognized as a deep learning best practice. We don\u2019t add new layers/etc that were recently published to ArXiv.org, we only add new objects that are already commonly used in the deep learning community.\r\n- It should have an owner committed to maintaining it in the long term."]}, {"number": 20003, "title": "Make it possible to disable bazel symbol stripping", "body": "Recent changes added build --strip=always to the .tf_configure.bazelrc file which is overriding command-line options for stripping. User has to manually edit the file and remove the flag in order to be able to control strip rules. This PR adds a check that if BAZEL_NO_STRIP is defined in environment, it doesn't add strip=always flag to bazelrc file.", "comments": ["Gentle ping @yifeif ", "@yifeif,\r\n\r\nIs this better?", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for this! I originally did the --strip=always fix for an older version but didnt get around to adding a configure flag for it.\r\n\r\nIsn't it cleaner to put the check inside set_build_strip_flag()?\r\nalso the fix commits should be squashed down i think.\r\n\r\nsomething like:\r\n```\r\ndiff --git a/configure.py b/configure.py\r\nindex f97bf8a668..884e50d741 100644\r\n--- a/configure.py\r\n+++ b/configure.py\r\n@@ -1399,7 +1399,8 @@ def set_grpc_build_flags():\r\n \r\n \r\n def set_build_strip_flag():\r\n-  write_to_bazelrc('build --strip=always')\r\n+  if environ_cp.get(\"BAZEL_STRIP\") != \"0\":\r\n+    write_to_bazelrc('build --strip=always')\r\n \r\n \r\n def set_windows_build_flags(environ_cp):\r\n```", "I submitted an alternative to this in https://github.com/tensorflow/tensorflow/pull/21373", "@perfinion,\r\n\r\nI was wondering why always strip? Why bazel default is not good? i.e. strip in opt and fastbuild and not strip in debug. What was the rationale behind strip=always?\r\n\r\nThanks.\r\n", "@samikama its from https://github.com/tensorflow/tensorflow/pull/19599\r\nOriginally the linker flags had \"-s\" in the linkopts in the BUILD files. I removed those cuz things shouldn't be stripped. Martin said they had build issues without it so the compromise was to remove the -s linker flags and add in build --strip=always instead. Sed'ing out the line in one place was much easier to deal with than having patches I'd need to keep resolving conflicts for every release.\r\n\r\nI got that PR merged right before the branch so I didn't want to make too big of a change last minute and then forgot to add the configuration option later on.", "@perfinion could you pull rebase and push again?", "@drpngx PR 21373 has my other version of this. That one is ready to merge so once its in this can be closed.", "@perfinion,\r\n\r\nTo be honest, instead of this hack, I would prefer to keep build system flexibility in place. there should be a <name>.stripped option automatically generated by bazel to strip debug symbols. Dependencies could be updated to use stripped binaries instead of this hack.\r\n\r\nI would be very much in favor of *not* adding --strip=always to bazelrc \r\n", "@samikama yeah, I'd rather not have it too. But lets see what @martinwicke says, he asked for it initially.", "@samikama Sorry for the delay in working on this. Can you resolve the conflict in this PR so we can get it merged? Thanks.", "@caisq,\r\n\r\nLets wait @martinwicke to answer. It is better if we can have a proper fix instead of hacks and workaround hacks.", "What happens to the build if we disable `strip=always`? will it still build? What is the size of the resulting binaries? Assuming it's all fine (lots of things have changed since the list time we tried this), then I'm fine removing the strip. One way to find out would be to make a PR (maybe this one) which just removes the stripping, and carefully check the sizes of the artifacts (and their build times). That may mean adding an `ls -l` to the build script to make sure we see the size.", "@martinwicke, \r\nAccording to the https://docs.bazel.build/versions/master/user-manual.html#flag--strip, unless it is told to do so or build with dbg flag it should strip debug symbols with default settings. To me leaving defaults and modifying command-line options seems fine.", "@martinwicke fwiw, I have disabled stripping on the Gentoo packages since I first packaged it and have not had any reports from users about problems with it.", "Do you know what happens to binary size? We're at the limit of what we can put on pypi. If all is as expected, we should already be stripping, and there's no problem. Let's try it?", "@martinwicke @samikama \r\nThey correspond to nothing in .bazelrc, --strip=never, and --strip=always\r\n```\r\n$ ls -l /tmp/dst-*strip/*.whl\r\n-rw-r--r--. 1 jason users 50544207 Aug 14 20:51 /tmp/dst-neverstrip/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n-rw-r--r--. 1 jason users 50544200 Aug 14 20:45 /tmp/dst-nostrip/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n-rw-r--r--. 1 jason users 50544200 Aug 14 20:48 /tmp/dst-strip/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\nInterestingly, I went back and looked at 1039ff9ee8c8c7ed09f9bb106131a50285866dd4 which was the commit where I removed the `-s` flags. Those were only ever on libtensorflow{,_cc}.so. Not on libtensorflow_framework.so. The wheels only contain the _framework.so. The .so and _cc.so are not distributed on PyPI so shouldn't actually have made a difference anyway.\r\n\r\nThe *really* heavy handed way to strip is `--linkopt=-Wl,--strip-all`, but even that one only saves ~3.8MB on the entire wheel so also isn't worth the downsides. ", "@samikama do you want to re-do this PR to remove the stripping? I'll drop the BAZEL_STRIP commit from my other PR then since its pointless.", "Heh, if you really want to drop the wheel size, unbundling so far drops >10MB.\r\n```\r\n$ ls -l /tmp/dst-*/*.whl\r\n-rw-r--r--. 1 jason users 50544207 Aug 14 20:51 /tmp/dst-neverstrip/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n-rw-r--r--. 1 jason users 50544200 Aug 14 20:45 /tmp/dst-nostrip/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n-rw-r--r--. 1 jason users 50544200 Aug 14 20:48 /tmp/dst-strip/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n-rw-r--r--. 1 jason users 46485258 Aug 14 21:18 /tmp/dst-stripall/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n-rw-r--r--. 1 jason users 39503309 Aug 14 21:43 /tmp/dst-unbundle/tensorflow-1.10.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\n\r\n@martinwicke If you are having issues with PyPI another thing to look into would be building the releases with \"-Os\" ie optimize for size, I haven't tried but wouldn't be surprised if it saves a fair amount.\r\n", "Ok. Can you make this PR (or a new one if you prefer) entirely remove the forced stripping? Then we run the tests and if they're good, we can merge this. If it works, I prefer it over adding another config option. \r\n\r\ncc/ @gunan ", "Should we remove `set_build_strip_flag()` entirely?", "@martinwicke,\r\nDone", "@yifeif @caisq I think we can merge this. The tests before the final commit looked good. ", "@aselle (or someone you tell me knows) could this tflite profiler failure be related to changes in binary size, or differences in when stripping is performed during the build? Or is it a flake? Time measurements always smell like a flake, but who knows..."]}, {"number": 20002, "title": "Make GCS ops work in open source", "body": "Fixes a few issues with the GCS ops in open source.", "comments": ["cc @case540 ", "Note: from the log of the one failing build, it looks to be unrelated (some bfloat16 issue):\r\n\r\n```\r\n2018-06-13 18:15:45.848632: F tensorflow/python/lib/core/bfloat16.cc:664] Check failed: PyBfloat16_Type.tp_base != nullptr\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 344.441s, Critical Path: 116.60s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n"]}, {"number": 20001, "title": "[WIP] Add canned multi label head", "body": "Work in Progress.\r\n\r\nmulti label head was provided in tf.contrib.learn, but not in the canned head_lib", "comments": ["close this, since the contrib version is finalized "]}, {"number": 20000, "title": "Add IBM ppc64le GPU build status to community supported build in README", "body": "The build status should be ready to be added but currently the build depends on 2 cherry-picks  #20102 #19930.", "comments": ["Both cherry-picks have now been merged into the master branch, so the ppc64le build now runs completely from the master level. Can we get the readme updated to show the build status?\r\n\r\n@gunan - Could you please review this change ?", "I am sorry, I completely missed this PR. The change looks good. Thanks for the contribution!", "Thank you @gunan , I appreciate all the reviews you have been doing for the power specific changes. "]}, {"number": 19999, "title": "r1.9-rc1 cherry-pick request: bugfix/memory leak fix", "body": "", "comments": []}, {"number": 19998, "title": "Install Tensorflow x86 with pip", "body": "Can i install tensorflow for python x86 with pip?", "comments": ["Hi @Fabioluxx. By x86, do you mean Python running on Windows? \r\n\r\nTensorFlow can be downloaded from pip in general. If you have pip installed, you should be able to download TensorFlow just fine by running `pip install tensorflow` regardless of platform. \r\n\r\nHope this helps.", "By the way, if pip isn't added to your path you can run it instead with `python -m pip install tensorflow`. ", "But it worked only when i uninstalled python x86 and installed python x64, otherwise a erro occurs like no version of tensorflow found and if i try to use install `pip3 install tensorflow== ` to see what versions are available, is empty", "According to the documentation, it appears that TensorFlow should work on almost any Windows system. They only support and test on 64-bit, x86 systems that are Windows 7 or later however. \r\n\r\nI'm unsure of why it didn't work for you.  Did you read the documentation here?: [Install TensorFlow on Windows](https://www.tensorflow.org/install/install_windows)", "Yes, i am using windows 10 x64", "Conclusion: It only supports x64\r\n\r\nAfter some search, here is the repository: https://pypi.org/project/tensorflow/#files\r\nthat only have x64 builds", "Yes, you are correct @Fabioluxx. TensorFlow only support 64-bit Python. See [TensorFlow not found using pip](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip). \r\n\r\nIf this issue is resolved for you, please close it."]}, {"number": 19997, "title": "r1.9-rc1 cherry-pick request: Keras save_weights fix", "body": "Cherry-pick the fix for  the Keras model.save_weights method.\r\nIs currently broken with default settings (graph mode, no manual session, tf-format)\r\n\r\nCherry-picked: e20ccaab7a85d729f37ad4b7b90188e97e2124fa\r\nCL/200088574", "comments": []}, {"number": 19996, "title": "Feature Request: untruncated normal in variance scaling initializer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\nCurrently the \"normal\" mode of `variance_scaling_initializer` actually produces truncated normal distribution. \r\nDespite that I saw Google suggest in several places that a truncated normal might be better than a true normal distribution, truncated normal is NOT what is used in the initialization literature such as:\r\n\r\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256. 2010.\r\n[2] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. 2015.\r\n\r\nI found that this difference is affecting some of my experiments. To be more consistent with literature, I would request a new mode in variance_scaling_initializer that produces untruncated normal distribution. I'm OK to implement it If someone can give it a good name.", "comments": ["(For API review.) We agree this should be fixed. I think the best path forward is:\r\n\r\n* We change the legal values in https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/init_ops.py#L443\r\nto \"normal\", \"uniform\", \"truncated_normal\", \"untruncated_normal\"\r\n\r\n* We change the default here: https://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/init_ops.py#L435\r\nfrom \"normal\" to \"truncated_normal\" (matching current behavior, but clearer)\r\n\r\n* We change the implementation here:\r\nhttps://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/init_ops.py#L465\r\nto treat \"normal\" & \"truncated_normal\" as the same (required for backwards-compatibility) and add a new implementation for \"untruncated_normal\"\r\n\r\n* In the documentation here:\r\nhttps://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/python/ops/init_ops.py#L421\r\nupdate the text to document the options as \"truncated_normal\", \"untruncated_normal\", and \"uniform\", and that \"normal\" is a deprecated alias for \"truncated_normal\"\r\n\r\n* I will make a note that at 2.0, when we can break backwards compatibility, we should change \"normal\" to be an alias for \"untruncated_normal\".", "Would that solve your problem? If so, please send a PR and reference this issue. Thank you!", "That's exactly the solution I was hoping for."]}, {"number": 19995, "title": "Branch 200416472", "body": "", "comments": []}, {"number": 19994, "title": "Update tensorboard dependency to 1.9.x", "body": "TensorBoard 1.9.0 has been released to PyPI: https://pypi.org/project/tensorboard/1.9.0/", "comments": ["So what I've typically done for these PRs (e.g. #18844 for 1.8) is sending them straight to the release branch because they're only really meaningful for that branch.  The dep here only gets used for releases - there's a separate `tb-nightly` dep that `tf-nightly` uses which I update in a separate PR to master.\r\n\r\nIf it's best to do it via master + a separate cherrypick I can do it that way (should I close this PR then?) but it's a little extra work so just explaining the rationale.", "Ill merge this once tests pass. But in future, probably go ahead and submit change to master first even if it doesnt affect master branch.", "Errr, probably dont even need to bother waiting on tests for this change.", "Ok, in the future I will send the PR to master first, thanks."]}, {"number": 19993, "title": "Fix build issue on mac with python-2.7.10 and clang-9.1.0", "body": "While building tensorflow on mac with `python 2.7.10` and `llvm 9.1.0` (`macOS High Sierra 10.15.5`), the following compilation errors surface:\r\n```\r\nIn file included from tensorflow/python/lib/core/py_util.cc:20:\r\nIn file included from ./tensorflow/core/lib/core/errors.h:19:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/sstream:174:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ostream:138:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:492:15: error: C++ requires a type specifier for all declarations\r\n    char_type toupper(char_type __c) const\r\n              ^\r\nbazel-out/host/genfiles/external/local_config_python/python_include/pyport.h:731:29: note: expanded from macro 'toupper'\r\n...\r\n...\r\n```\r\n\r\nThe error is related to the issue in `pyport.h`.\r\nThe build error could be fixed by including `#include <locale>`\r\nbefore including `#include <Python.h>`.\r\n\r\nThe changes in this PR allows the build to succeed.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19992, "title": "The DepthwiseConv2dNative() function ignores the dilations argument", "body": "Hi!\r\n\r\nI am using Tensorflow v1.7.0.\r\n\r\nI am invoking the DepthwiseConv2dNative() function with a dilations argument that is [1, 2, 2, 1]. Despite of this, the dilations value is being ignored.\r\n\r\nLooking at the tensorflow source code, it is evident that the dilations argument is presumably being disregarded (depthwise_conv_op.cc, around line 400) in both CUDNN and non-CUDNN scenarios. Yet, there is no mention of this in the documentation.\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@chsigg -- is @PavelPr 's assessment correct? Is it expected that dilation is ignored? If so, can we document or fix?", "Hi!\r\n\r\nApologies for not including the required information to begin with. I am using TensorFlow under Windows 10, in CPU mode. Bazel version is 0.14.1. \r\n\r\nWhile I am fully aware of the fact that Windows is not an officially supported platform, reading through the TensorFlow source code creates an impression that this is a yet-to-be-implemented feature. Still, the documentation bears no trace of this (to the contrary, the \"dilations\" argument is even being described in detail; see: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/depthwise-conv2d-native).\r\n\r\nHave a look at line 403 of depthwise_conv_op.cc (under tensorflow/core/kernels/):\r\n\r\n      // TODO(yangzihao): Send in arbitrary dilation rates after the dilated\r\n      // conv is supported.\r\n      launcher_(context, use_cudnn_, cudnn_use_autotune_, input,\r\n                reshaped_filter, /*row_dilation=*/1, /*col_dilation=*/1,\r\n                stride_, stride_, padding_, output, data_format_);\r\n\r\nWhile this particular block pertains to CUDNN, there is no trace of dilation information being passed in the CPU variant as well, the code of which is right below that block.\r\n\r\nThanks!", "I have found the same problem\uff0c with tf.nn.depthwise_conv2d_native\uff08\uff09\uff0c the parameter 'dilations' do not take effect\uff0cdilations=[1, 2, 2, 1] will have the same result with dilations=[1, 1, 1, 1].  @PavelPr  had pointed the bug in tensorflow's  implementation.", "@chsigg @yzhwang -- are there current plans to add dilation parameters to DepthwiseCov2dNative? If not, should the docs be updated?", "This definitely sounds like a problem that can reduce the accuracy of your models, but it might go unnoticed by some.\r\n\r\nIf the cudnn path does not support the dilated convolution in this case yet, maybe it would be a good idea to just use the native path if the dilation rate is > 1?", "Meet the same issue. Any update? If this can not be supported in short term, I think at least the doc should be updated first, it took me much time to figure out there is a misalignment between doc and real behavior.", "@PavelPr  Is this still an issue with the latest TF1.13 and TF2.0? Thanks!", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=19992\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=19992\">No</a>\n", "@jvishnuvardhan The issue still exists in v1.13.1.", "@weichiche Could you create new issue with the issue details and context. Thanks!", "I just stumbled on this thread while looking at [`tf.nn.depthwise_conv2d`](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d). I assume `tf.nn.depthwise_conv2d` is a wrapper for `tensorflow::ops::DepthwiseConv2dNative`, and I will share my experiments in hope it is useful for someone.\r\n\r\nI'm on Linux, testing on CPU mode with tensorflow 1.14.0(https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.14.0-cp37-cp37m-linux_x86_64.whl), and dilation seems to work. Below is a self-contained script for testing. \r\n```python\r\n%reload_ext autoreload\r\n%autoreload 2\r\n%matplotlib inline\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nx = np.random.rand(32,64,3);x.shape\r\n\r\nplt.figure()\r\nplt.title('X')\r\nplt.imshow(x)\r\n\r\nnum_channels = 3\r\nbox_filter = np.ones([2, 2, 3, 1]) / 4\r\ndilation_size = 2\r\n\r\ny_with_dilation = tf.nn.depthwise_conv2d(\r\n    np.expand_dims(x, axis=0),\r\n    box_filter,  # [filter_height, filter_width, in_channels, out_channels]\r\n    dilations=[dilation_size, dilation_size],\r\n    strides=[1,1,1,1],\r\n    padding=\"VALID\",   # No padding\r\n    data_format=\"NHWC\"\r\n)\r\n\r\ny_without_dilation = tf.nn.depthwise_conv2d(\r\n    np.expand_dims(x, axis=0),\r\n    box_filter,  # [filter_height, filter_width, in_channels, out_channels]\r\n    strides=[1,1,1,1],\r\n    padding=\"VALID\",   # No padding\r\n    data_format=\"NHWC\"\r\n)\r\n\r\ndef plot_y(y, title=''):\r\n    y = tf.squeeze(y)\r\n    plt.figure()\r\n    plt.title(title)\r\n    plt.imshow(y)\r\n    \r\nplot_y(y_with_dilation, 'With dilation')\r\nplot_y(y_without_dilation, 'Without dilation')\r\n```\r\n![x](https://user-images.githubusercontent.com/5738363/60093996-8458c280-9753-11e9-851c-cd3e516f1142.png)\r\n![1](https://user-images.githubusercontent.com/5738363/60093999-8753b300-9753-11e9-9e49-f58661bdac99.png)\r\n![2](https://user-images.githubusercontent.com/5738363/60094005-89b60d00-9753-11e9-8112-4c1d590ccb71.png)\r\n\r\nPlease don't ask me why I'm using a uniform filter :)\r\n"]}, {"number": 19990, "title": "Remove duplicate import in linear_equations.py", "body": "The line `from tensorflow.python.ops import linalg_ops`\r\nin linear_equations.py is a duplicate from the previous\r\nline. This fix removes the duplicate import.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19989, "title": "S3 backend request fails on small object (at least) against Minio S3 storage implementation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9 (Stretch)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-5-gcfd0ea3bfb 1.8.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516 / clang version 4.0.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nAt least when using [minio](https://minio.io) for S3 storage, small object requests initiated from TFRecordDataset() fail with following error:\r\n2018-06-13 17:02:04.857500: W tensorflow/core/platform/s3/aws_logging.cc:57] Encountered Unknown AWSError 'InvalidRange': The requested range is not satisfiable\r\nReason is that TF is requesting the object with buffer size Range header and when the object is smaller than the buffer (256 kB) the system is not satisfied with the smaller response size, although response HTTP header clearly indicates that the object was received in full. Instead there's an attempt to try to fetch next chunk with Range that begins at the end of object and reaches further beyond end of the object. Naturally this request will fail. This will cause the entire request fail.\r\n\r\n### Source code / logs\r\n2018-06-13 17:02:04.857500: W tensorflow/core/platform/s3/aws_logging.cc:57] Encountered Unknown AWSError 'InvalidRange': The requested range is not satisfiable\r\n", "comments": ["Seeing the same issue but with files of size 500Kb to 1000Kb. There also seems to have been a significant slowdown between TF 1.9 and TF 1.11 with S3 Storage, possibly pertaining to this error having to be raised on every object.\r\n\r\n@case540 @yongtang ", "Nagging Assignee @jart: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@jlaako , Please check with r1.12.0 for this AWS S3 error on the debian releases. Thanks.", "I'm not working at Intel anymore or on Tensorflow things in general. But maybe someone else who has the same problem can help checking? @joeyearsley ", "The author has indicated that this issue can now be closed, until some resubmits a similar issue. Thanks."]}, {"number": 19987, "title": "Cast size() to int to avoid implementation-defined conversion", "body": "`selected.size()` is unsigned and originally 0.\r\nThe conversion from `(size_t)0 - 1` to int is implementation defined: \r\n\r\nFrom https://en.cppreference.com/w/cpp/language/implicit_conversion:\r\n\r\n> If the destination type is signed, the value does not change if the source integer can be represented in the destination type. Otherwise the result is implementation-defined. (Note that this is different from signed integer arithmetic overflow, which is undefined).\r\n\r\nThis fixes #19578 ", "comments": ["@caisq Seems like a CI issue again.", "@caisq a gentle ping", "@cy89 Can you help review this PR or recommend someone to review it? Thanks.", "@caisq he is only random assigned to the original issue and looks like he hardly respond to anything on github: https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+involves%3Acy89\r\n@rmlarsen reviewed my PR on the NMS op last time", "Resolved the merge conflict.", "Any updates? \r\nResolving the merge conflicts has unfortunately invalidated the review and tests..", "Nagging Assignee @caisq: It has been 50 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "ping @rmlarsen @caisq "]}, {"number": 19986, "title": "Caused by: java.lang.IllegalArgumentException: Expects arg[0] to be uint8 but float is provided", "body": "Caused by: java.lang.IllegalArgumentException: Expects arg[0] to be uint8 but float is provided\r\n\r\nI am developping on Android Studio and the model input is : Found 1 possible inputs: (name=image_tensor, type=uint8(4), shape=[?,?,?,3])\r\n\r\nIn my android code what I am trying to get is the array float of the values of pixels of an image stored as Bitmap and then I am trying to feed the model. I am getting that error .\r\nPlease anyone could help me how I can fix this?\r\nI have tried to convert each float into array of byte[4] and then send the bytes but it seems I do not get the good images as a result.\r\nTensorflow version: 1.8.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Dude, did you solve this error?\r\nIf you did, very appreciated would be if you shared your inisghts ;)", "@azemZejnil Unfortunately I did not. It seemed lot of work to do, like for example use the jni library to use c++ within java. Which means that you have to change it before training the model, so that the input should be float and not uint8.", "@japer21 Do you remember where exactly do you change it before training the model, so that the input is float?\r\nI was looking some of the .config files and couldn't figure out anything particular.\r\nThanks", "@azemZejnil unfortunately it was not me who was charged of training the model. So that is why I just omitted this part for android. Sorry.  ", "Thank you anyways, you are very kind :)", "@japer21, if you still have the problem, can you please fill out the issue template? Or @azemZejnil if you have this problem, can you file a new issue with the issue template filled out? I'm unsure what the context is here.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I still haven't solved the problem\n\nuto, 24. srp 2018. 15:02 Alfred Sorten Wolf <notifications@github.com> je\nnapisao:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19986#issuecomment-407397474>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AROI6cKOIX-abPydmg906CbCrZxS0WRXks5uJxr2gaJpZM4UmGIJ>\n> .\n>\n", "@azemZejnil Zdravo, treba da pretvorish od float vo byte niza, za sekoj float imash niza od 3 bytes, za RGB, to je neshto shto napravija mojot team.\r\n\r\nHello, sorry I saw your language familiar to me, bosnian or croatian, so that is why I wrote on my language, macedonian, but never mind. I hope the solution I provided below, that was as much as I was able to do, will help you clarify some things, I know there is lot of work to do. ", "@japer21 Hello. I understand most of what you are saying, but not completely. which language is that?\r\nCan you ask your colleagues for a bit more info, please?\r\nWhere exactly do they change float to byte?\r\nThanks a lot", "@azemZejnil So, somewhere in your code you can have this\r\n\r\n```\r\n Bitmap result= BitmapFactory.decodeByteArray(decodedBytes, 0, decodedBytes.length);\r\n int [] pixelsInt = getIntPixels(result);\r\n byte[] pixelsBytes = new byte[pixelsInt.length*4];\r\n for(int i=0;i<pixelsInt.length;i++){\r\n     byte [] intToByte = toByteArray(pixelsInt[i]);\r\n     for(int j=0;j<intToByte.length;j++){\r\n         pixelsBytes[(i*4)+j]=intToByte[j];\r\n     }\r\n  }\r\n```\r\n\r\nLet's say we have the image as Bitmap, in our case it's the result variable.\r\nWhat I do next is I am collecting the integer values of each pixel of the bitmap variable result and then getting the bytes of it, below you can find the implementation of the 2 helper functions  :\r\n\r\n```\r\nprivate int[] getIntPixels(Bitmap bitmap) {\r\n        int[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];\r\n        int[] intResults = new int[bitmap.getWidth() * bitmap.getHeight()*3];\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        for (int i = 0; i < intValues.length; ++i) {\r\n            final int val = intValues[i];\r\n            intResults[i * 3 + 2] = Color.red(val);\r\n            intResults[i * 3 + 1] = Color.green(val);\r\n            intResults[i * 3] = Color.blue(val);\r\n        }\r\n        return intResults;\r\n    }\r\n\r\nprivate byte[] toByteArray(int value) {\r\n        return new byte[] {\r\n                (byte)(value >> 24),\r\n                (byte)(value >> 16),\r\n                (byte)(value >> 8),\r\n                (byte)value};\r\n    }\r\n```\r\nThen, I create my byte array, `pixelsBytes` and therefore `pixelsBytes`. Finally what I am sending is the array `pixelsBytes`.", "```\r\n//.....\r\nprivate TensorFlowInferenceInterface inferenceInterface;\r\n//.....\r\ninferenceInterface = new TensorFlowInferenceInterface(assetManager, MODEL_FILE);\r\n//.....\r\n//here you call the tensorflow inference\r\ninferenceInterface.feed(INPUT_NODE,pixelsBytes,1,result.getWidth()*2,result.getHeight()*2,3);\r\n```", "Damn, didn't expect that detailed answer,\r\nthank you very much.\r\nI will try it as soon as I get to and I will let you know how it went.\r\n\r\nBut in the conclusion, we are changing stuff in the Android Studio.\r\nI was wondering if there is a way to change input type in the training", "@azemZejnil Yeah, I think it's impossible to change the input type in the training, I think it depends on what tensorflow model you are using for training your model, it seems complicated. Or I do not know, maybe there is a way of customisation of the input, but I have never done it, yet :). You are welcome. ", "Closing due to inactivity. If this is still an issue, please file a new issue with the issues template filled out."]}, {"number": 19985, "title": "Fixed compilation error (undefined reference to LLVMInitializePowerPC\u2026", "body": "\u2026TargetMC) on ppc64le when XLA is enabled\r\n\r\nBuilding master branch tensorflow on ppc64le with XLA enabled gives compilation error - undefined reference to LLVMInitializePowerPCTargetMC. Fixed the error by adding powerpc related code generator and disassembler targets.", "comments": []}, {"number": 19984, "title": "Specify endianness in expected_result array to fix #15767.", "body": "", "comments": ["Gentle ping? @rmlarsen @benoitsteiner ", "Nagging Assignee @caisq: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19983, "title": "Fix for compilation failure in gdr_server_lib.cc", "body": "", "comments": []}]