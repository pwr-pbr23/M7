[{"number": 30091, "title": "Cannot reuse the self.Dense() defined in __init__(self) in call(self, x)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version:  Python 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I define the following:\r\n```\r\nclass MNISTModel(Model):\r\n    def __init__(self):\r\n        super(MNISTModel, self).__init__()\r\n        self.conv1 = Conv2D(32, (3, 3), input_shape=(28, 28, 1))\r\n        self.activ = Activation('relu')\r\n        self.flatt = Flatten()\r\n        self.dense = Dense(200) #Here I want to reuse this Dense() layer\r\n        self.dense1 = Dense(200) #However, I need to re-define the same layer again to use in call()\r\n        self.dens2 = Dense(10)\r\n```\r\n**Describe the expected behavior**\r\nI think that we only need to define the self.Dense(200) only once, but we can reuse it in the  def call(self, x)\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.data import Dataset\r\nfrom tensorflow.keras import datasets, Model, losses, optimizers, metrics\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Activation, MaxPooling2D\r\n\r\n\r\nclass MNISTModel(Model):\r\n    def __init__(self):\r\n        super(MNISTModel, self).__init__()\r\n        self.conv1 = Conv2D(32, (3, 3), input_shape=(28, 28, 1))\r\n        self.activ = Activation('relu')\r\n        self.conv2 = Conv2D(32, (3, 3))\r\n        self.maxpo = MaxPooling2D(pool_size=(2, 2))\r\n        self.conv3 = Conv2D(64, (3, 3))\r\n        self.flatt = Flatten()\r\n        self.dense = Dense(200)\r\n        self.dense1 = Dense(200)\r\n        self.dens2 = Dense(10)\r\n\r\n    def call(self, x):\r\n        x = self.conv1(x)\r\n        x = self.activ(x)\r\n        x = self.conv2(x)\r\n        x = self.activ(x)\r\n        x = self.maxpo(x)\r\n\r\n        x = self.conv3(x)\r\n        x = self.activ(x)\r\n        x = self.conv3(x)\r\n        x = self.activ(x)\r\n        x = self.maxpo(x)\r\n\r\n        x = self.flatt(x)\r\n        x = self.dense(x)\r\n        x = self.activ(x)\r\n        x = self.dense1(x)\r\n        x = self.activ(x)\r\n        return self.dens2(x)\r\n\r\n\r\n(train_data, train_labels), (test_data, test_labels) = datasets.mnist.load_data()\r\ntrain_data, test_data = train_data / 255.0, test_data / 255.0\r\ntrain_data = train_data[..., tf.newaxis]\r\ntest_data = test_data[..., tf.newaxis]\r\nprint(train_data.shape, test_data.shape, type(train_data))\r\ntrain_data = Dataset.from_tensor_slices(\r\n    (train_data, train_labels)).shuffle(60000).batch(128)\r\ntest_data = Dataset.from_tensor_slices((test_data, test_labels)).batch(128)\r\n\r\n\r\nmodel = MNISTModel()\r\nloss_object = losses.SparseCategoricalCrossentropy()\r\noptimizer = optimizers.Adam()\r\n\r\ntrain_loss = metrics.Mean(name='train_loss')\r\ntrain_accuracy = metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\ntest_loss = metrics.Mean(name='test_loss')\r\ntest_accuracy = metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n\r\n\r\n@tf.function\r\ndef train_step(images, labels):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(images)\r\n        loss_value = loss_object(labels, logits)\r\n    grads = tape.gradient(loss_value, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    train_loss(loss_value)\r\n    train_accuracy(labels, logits)\r\n\r\n\r\n@tf.function\r\ndef test_step(images, labels):\r\n    logits = model(images)\r\n    tloss_value = loss_object(labels, logits)\r\n    test_loss(tloss_value)\r\n    test_accuracy(labels, logits)\r\n\r\n\r\nEPOCHS = 5\r\n\r\nfor epoch in range(EPOCHS):\r\n    for images, labels in train_data:\r\n        train_step(images, labels)\r\n\r\n    for test_images, test_labels in test_data:\r\n        test_step(test_images, test_labels)\r\n\r\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n    print (template.format(epoch + 1, train_loss.result(), train_accuracy.result()\r\n                           * 100, test_loss.result(), test_accuracy.result() * 100))\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@MyRespect After executing the above code on Colab with Tensorflow 2.0.0.beta1 I got the following error \r\n`ValueError: Input 0 of layer conv2d_5 is incompatible with the layer: expected axis -1 of input shape to have value 32 but received input with shape [128, 10, 10, 64]`. \r\n", " @gadagashwini \r\nYes, you reproduced my bug. This is because I use \r\n`        x = self.conv3(x)\r\n        x = self.activ(x)\r\n        x = self.conv3(x)`\r\nin the __call__(), but if you define \r\n`self.conv3_1 = Conv2D(64, (3, 3))` \r\nin the __init__(), and use it in the __call__():\r\n`        x = self.conv3(x)\r\n        x = self.activ(x)\r\n        x = self.conv3_1(x)`\r\nyou can avoid the problem. So here I think it is a bug to be fixed.", "@MyRespect Thanks for the bug! You can reuse layers, you just have to make sure that the shapes of your second inputs are compatible with the shapes of your first inputs to the layer, in this case it looks like that is not true for your model", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30091\">No</a>\n"]}, {"number": 30090, "title": "support max_profiling_buffer_entries", "body": "```\r\nbazel build --config android_arm64 \\\r\n//tensorflow/lite/examples/label_image:label_image \\\r\n--copt=-DTFLITE_PROFILING_ENABLED\r\n```\r\ndoesn't work after fbda4a1c5540b81dc6fd4f720e5e1353e08c44e6, which changed `profiling::Profiler` constructor", "comments": []}, {"number": 30089, "title": "TypeError: Generator object is not an iterator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64bit\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Anaconda 3.7\r\n- GPU model and memory: GTX 1050 2GB\r\nEdit: With tensorflow CPU (tf version: 1.14), same issue occurs.\r\n\r\nWhen I'm trying to do model.fit(), memoryerror occurs, resulting in using fit_generator. \r\nFor fit_generator, since my input is a custom 512D embedding from a previous model, I wrote my custom generator function, but while running it, following error occurs:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"neural.py\", line 244, in <module>\r\n    model.fit_generator(mygenerator(neural_network_x_1, neural_network_x_2, neural_network_y), steps_per_epoch= 25,epochs=100)\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1418, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\engine\\training_generator.py\", line 181, in fit_generator\r\n    generator_output = next(output_generator)\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 709, in get\r\n    six.reraise(*sys.exc_info())\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 685, in get\r\n    inputs = self.queue.get(block=True).get()\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\multiprocessing\\pool.py\", line 670, in get\r\n    raise self._value\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\multiprocessing\\pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"C:\\Users\\chinm\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 626, in next_sample\r\n    return six.next(_SHARED_SEQUENCES[uid])\r\nTypeError: 'mygenerator' object is not an iterator\r\n\r\n```\r\nThe generator class mentioned below is in the order as mentioned in: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\r\n\r\n```\r\nfrom tensorflow.python.keras.utils.data_utils import Sequence\r\nclass mygenerator(Sequence):\r\n    def __init__(self, x_set_1, x_set_2, y_set, batch_size = 16):\r\n        self.x1, self.x2, self.y = x_set_1, x_set_2, y_set\r\n        self.batch_size = batch_size\r\n        self.len_data = 0\r\n\r\n    def __len__(self):\r\n        return int(np.ceil(len(self.y) / float(self.batch_size)))\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x1 = self.x1[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n        batch_x2 = self.x2[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n\r\n        # read your data here using the batch lists, batch_x and batch_y\r\n        xi = [item for item in batch_x1] \r\n        xj = [item for item in batch_x2] \r\n        yi = [item for item in batch_y]\r\n        return np.asarray([xi, xj], yi)\r\n\r\nmodel.fit_generator(mygenerator(neural_network_x_1, neural_network_x_2, neural_network_y), steps_per_epoch= 25,epochs=100)\r\n\r\n```", "comments": ["I am seeing model is not defined when reproducing the issue.Please provide us complete code to reproduce the issue.Thanks", "Hi @ravikyram , here's the complete code to reproduce the issue:\r\n\r\n```\r\n\r\nimport numpy as np\r\nno_element = 10\r\nx1 = np.random.rand(no_element ,512)\r\nx2 = np.random.rand(no_element ,512)\r\nx3 = np.random.randint(2, size=no_element)\r\n\r\nfrom tensorflow.python.keras.utils.data_utils import Sequence\r\n\r\nclass mygenerator(Sequence):\r\n    def __init__(self, x_set_1, x_set_2, y_set, batch_size = 16):\r\n        self.x1, self.x2, self.y = x_set_1, x_set_2, y_set\r\n        self.batch_size = batch_size\r\n        self.len_data = 0\r\n\r\n    def __len__(self):\r\n        return int(np.ceil(len(self.y) / float(self.batch_size)))\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x1 = self.x1[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n        batch_x2 = self.x2[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n\r\n        # read your data here using the batch lists, batch_x and batch_y\r\n        xi = [item for item in batch_x1] \r\n        xj = [item for item in batch_x2] \r\n        yi = [item for item in batch_y]\r\n        return np.asarray([xi, xj], yi)\r\n\r\n\r\nfrom keras import Sequential\r\nfrom keras import Input\r\nfrom keras.layers import Dense, Concatenate\r\nfrom keras import Model\r\n# from keras import optimizers\r\nfrom keras.optimizers import Adam\r\n\r\n# define two sets of inputs\r\ninputA = Input(shape=(512,))\r\ninputB = Input(shape= (512,))\r\n\r\n# the first branch operates on the first input\r\nx = Dense(128, activation=\"relu\")(inputA)\r\nx = Model(inputs=inputA, outputs=x)\r\n\r\n# the second branch opreates on the second input\r\ny = Dense(128, activation=\"relu\")(inputB)\r\ny = Model(inputs=inputB, outputs=y)\r\n\r\n# combine the output of the two branches\r\ncombined = Concatenate()([x.output, y.output])\r\n\r\n# apply a FC layer and then a regression prediction on the\r\n# combined outputs\r\nz = Dense(16, activation=\"relu\")(combined)\r\nz = Dense(4, activation=\"relu\")(combined)\r\nz = Dense(2, activation=\"linear\")(z)\r\n\r\nmodel = Model(inputs=[x.input, y.input], outputs=z)\r\nopt = Adam(lr=1e-3, decay=1e-3 / 200)\r\nmodel.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\r\n\r\n\r\nmodel.fit_generator(mygenerator(x1, x2, x3), steps_per_epoch= 25,epochs=100)\r\n\r\n```", "I have reproduced the issue in colab with TF version 1.12.0.Thanks", "I am facing a different issues, please find [gist here](https://colab.research.google.com/gist/Saduf2019/1fc5cd922e06200db7087126abfeb690/untitled.ipynb), the issue reported is resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30089\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30089\">No</a>\n"]}, {"number": 30088, "title": "When I run ./minimal, the error message is minimal <tflite model>. Where is the tflite model", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:armv7l platform\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:remotes/origin/r1.13\r\n- Python version:N/A\r\n- Installed using virtualenv? pip? conda?:git\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):arm-linux-gnueabihf-g++\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have compiled tensoflow-lite on ubuntu and successfully generated libtensorflow-lite.a and minimal, how should I test them on the Raspberry Pi? When I run ./minimal, the error message is minimal <tflite model>. Where is the tflite model, please help me, 3Q\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n# ./minimal \r\nminimal <tflite model>\r\n\r\n# ./minimal benchmark\r\nbenchmark-lib.a  benchmark_model  \r\nroot@am57xx-evm:~/app_virtual# ./minimal benchmark_model \r\nModel provided has model identifier '\u0001\u0001\u0001\u0003', should be 'TFL3'\r\n\r\nError at tensorflow/lite/examples/minimal/minimal.cc:49\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nAs above...", "comments": ["any valid TensorFlow Lite model works, e.g., you can get some models [here](https://www.tensorflow.org/lite/guide/hosted_models)", "> any valid TensorFlow Lite model works, e.g., you can get some models [here](https://www.tensorflow.org/lite/guide/hosted_models)\r\n\r\nWebsite can't open\u3002\uff0c", "I downloaded a model (mobilenet_v1_1.0_224.tflite) from [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models), then modified the Makefile to generate the BIN file label_image, and then run the command ./label_image -v 1 -m ./mobilenet_v1_1.0_224.tflite -i ./grace_hopper. bmp -l ./labels.txt, test is ok..", "Looks like its resolved. Can we close this issue? ", "> Looks like its resolved. Can we close this issue?\r\n\r\n"]}, {"number": 30086, "title": "How to make the Checkpoint store moments and other relevant variables in tf.train Optimizers.", "body": "I encountered a problem when my code stopped for some reason on my machine, so I had to restart my code and continue the training process by loading the latest checkpoint file. I found that the performance is not consistent before and after the checkpoint that I loaded and the performance dropped a lot. So, since my code uses tf.train.AdamOptimizer, I guess that the checkpoint doesn't store the moment vectors and the gradients in the previous steps, and when I load the checkpoint the moment vectors are initialized as zeros. Am I correct? Is there any method that can help store relevant vectors for the Adamopotimizer in the checkpoints so that if my machine is down again, restarting from the latest checkpoint will not influence anything? Thanks! ", "comments": ["Which APIs are you using to save and restore? `tf.train.Saver` and `tf.train.Checkpoint` will both save optimizer slot variables ([the guide has a diagram including slot variables](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/checkpoints.ipynb) under \"Loading Mechanics\").", "@allenlavoie Thanks for your reply! I am using saver = tf.train.Saver() API and use saver.save(), saver.restore() to save and restore models. But could you explain why the performance obviously dropped when I restored the model from the latest checkpoint and went for more epoches? The \"latest checkpoint\" that I restored is just like a watershed in terms of performance.  ", "Can you please help us with Tensorflow version and full code snippet to reproduce the issue.Thanks.", "One common issue is creating the Saver too early; by default it picks up variables from the global collection, so if the Optimizer's minimize call hasn't run yet then it won't pick up those variables."]}, {"number": 30085, "title": "Default values not listed for hyperparameters in tfp ExponentiatedQuadratic class", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/probability/api_docs/python/tfp/positive_semidefinite_kernels/ExponentiatedQuadratic\r\n\r\n## Description of issue (what needs changing):\r\n\r\nList default values for hyperparameters `amplitude` and `length_scale` that are used when a kernel is created with default values, as suggested [here](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/GaussianProcess#examples):\r\n\r\n```\r\ntfd = tfp.distributions\r\npsd_kernels = tfp.positive_semidefinite_kernels\r\n\r\n# Define a kernel with default parameters.\r\nkernel = psd_kernels.ExponentiatedQuadratic()\r\n```\r\n\r\n### Parameters defined\r\n\r\nDefault values for `amplitude` and `length_scale` are not listed (apart from a keyword 'None').", "comments": ["@alinaselega This is more related to `tfp` repo. Please post it in tfp repo [here](https://github.com/tensorflow/probability/issues) and close this issue. Thanks!", "OK, thanks!"]}, {"number": 30084, "title": "[XLA] Change the VLOG level of a Status message traceback to 2", "body": "Some people use Status messages a a method of flow control.  This happens in many places across the code base.\r\n\r\nProducing a backtrace whenever one of these is done produces a lot of visual clutter in a VLOG 1 trace.  I have moved it to VLOG 2.", "comments": ["thanks.  i did know about vmodule logging, but sometimes there are just too many files that i am interested in to use that feature.  it is great feature of logging though.", "ps - merging person - i seriously doubt that the MacOS contrib failure is due to this change.", "@kokoro-team Hi - i think the test failure was not related to this merge.  can you re-run the tests again please?\r\n"]}, {"number": 30083, "title": "[XLA] Include sort in ReplaceComputations check", "body": "Sort has a subcomputation, and therefore should be included in this check.  The check is in ReplaceComputations, and is checking all operations which have a subcomputation.\r\n", "comments": ["sure - i will add one.\r\n\r\nthis code path is never exercised because i believe that no backends other than the GC one use the subcomputation unifier, and that is the only thing that uses replace computation.", "hi- i am definitely looking at this - it isn't too high priority at our end so it might take me a little time to get around to it.", "@jlebar test added."]}, {"number": 30082, "title": "[XLA] Make the bfloat16 part of the iota test optional", "body": "For backends which do not support BF16, allow parts of the iota test to be disabled.\r\n", "comments": []}, {"number": 30081, "title": "[XLA] Split NextAfter test into a sepatate test instance", "body": "Split the NextAfter test into a separate test to allow it to be independently disabled using the test manifest.\r\n\r\n", "comments": []}, {"number": 30080, "title": "[XLA] Add resource arg count and indices to module config", "body": "The GraphCore backend, and maybe others, benefits from knowing which inputs to the HloModule are associated with resource variables, and which are standard inputs.\r\n\r\nThere are 4 situations:\r\n1) a standard input or output\r\n2) a resource variable which is updated.  this appears as both an input and an output\r\n3) a resource variable which is created.  this appears as only an output\r\n4) a resource variable which is not updated.  this appears as only an input\r\n\r\n\r\nThis change introduces 4 useful pieces of information into the HloModuleConfig:\r\n\r\n- The number of arguments to the original TF operation\r\nthis is useful because the original arguments are not always present in the HLO, and yet the update mapping refers to positions in the original op\r\n\r\n- The number of inputs which are resource variables\r\nthis is important for telling which inputs are resource variables\r\n\r\n- The number of outputs which are updates of resource variables\r\nthis is important for telling which outputs are resource variables\r\n\r\n- Mapping of inputs from XLA computation to TF operation\r\nthis is important for telling which actual resources were updated, if any\r\n", "comments": ["@jpienaar this is the resource variable thing i was mentioning.\r\n", "> The GraphCore backend, and maybe others, benefits from knowing which inputs to the HloModule are associated with resource variables\r\n\r\nCan you share how it benefits from this information?  If the benefit is that the backend can share buffers between the input and output, then maybe `HloInputOutputAliasConfig` is the right mechanism?\r\n\r\nCC @yunxing ", "the backend can know which outputs are resources, which can be left on device, and which are inputs/outputs to be streamed on and off.\r\n\r\nthe HloInputOutputAliasConfig does not contain this information, and also isn't created by the standard TF -> XLA jit mechanism.  it appears to be something for standalone XLA users.", "> the backend can know which outputs are resources, which can be left on device, and which are inputs/outputs to be streamed on and off.\r\n\r\nIMO it would be better to tell XLA about this directly (which outputs can be left on device) instead of telling it which input/outputs are resource variables.  Right now XLA does not know about resource variables at all and it is a bit weird to introduce the concept to XLA solely to change buffer management behavior.", "fair enough.  will do \ud83d\udc4d \r\n\r\n", "If I do it - is the HloInputOutputAliasConfig the right place to put the output->input mapping?  i am not really clear what it is meant to be, although it does seem to have the right name.  assuming that the JIT client op can create such a thing.\r\n\r\nI would still have to put the more info in somewhere, because there are effectively 4 types of input and output as described in the first comment.", "> If I do it - is the HloInputOutputAliasConfig the right place to put the output->input mapping? i am not really clear what it is meant to be, although it does seem to have the right name. assuming that the JIT client op can create such a thing.\r\n\r\nIt looks like `XlaBuilder` can access the alias config: https://github.com/tensorflow/tensorflow/blob/d5544d3d7dd936fcf55ba37f1c780ef602b9d9c4/tensorflow/compiler/xla/client/xla_builder.cc#L376\r\n\r\nBtw, I think we should rename `HloInputOutputAliasConfig` to `HloInputOutputMemoryConfig` to make it more obvious that it is more general than aliasing.  You could also create a class like `HloInputOutputMemoryConfig` and add it to `HloModule`.", "I can see the benefits of having input output aliasing for updated variables. Is there any other benefits of knowing if a variable is created/not updated? ", "> Is there any other benefits of knowing if a variable is created/not updated?\r\n\r\nSee https://github.com/tensorflow/tensorflow/pull/30080#issuecomment-505309245\r\n\r\n(The backend may want to leave the variables on the device.)\r\n\r\n@DavidNorman : now that I think of it, maybe it is best to do this at the layer that calls into the XLA executable?  Or are you going to generate different code to access buffers that are streamed into the device vs. buffers that are left on the device?", "yes.  we generate different code for streaming vs retained variables.  we need to know, at compile time, which inputs/outputs are resources (retained buffers) or not.\r\n\r\nI will have a go at passing the info through an HloInputOutputMemoryConfig structure instead.  Sounds like a better way of passing the info then using the HloModuleConfig.", "@yunxing  If a variable is created by a graph (this isn't very common, but it does happen rarely), it is worth retaining on the device.  This means you need to know which outputs are 'created', instead of 'updated'.  With resources, you need to know which inputs are not updated, because when you only have counts of variables you have to work with the assumption that the last 'N' inputs and last 'M' outputs are retained resources. \r\n\r\nIn general N != M != number of vars used by XalRun op, because the actual XLA graph doesn't necessarily consume all of the variables or update them all.  \r\n\r\nBut - i can see that an HloInputOutputMemoryConfig would be able to make the whole thing a lot more explicit, removing many of the issues that I have faced with the counting scheme in this PR.\r\n\r\n", "Can one of the admins verify this patch?", "@DavidNorman Is this an active PR or are you working on a different solution to your problem?", "@DavidNorman Could you please check reviewer comments and keep us posted. Thanks!", "I'll close this off.  Our branch and release works fine with the changes as they are, and it is possible that MLIR will overtake this particular interface."]}, {"number": 30079, "title": "[XLA] Disable test for backends with no BF16 support", "body": "Some tests instantiate multiple typed versions for a set of data types.  Some backends do not support BF16 as a data type.  This change instantiates the tests, but doesn't run the ones which are for BF16.  ", "comments": []}, {"number": 30078, "title": "TensorFlow Keras API not importing properly", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Linux Ubuntu 18.0\r\n- TensorFlow installed from (source or binary): binary (from pip)\r\n- TensorFlow version: I've tried 1.5, 1.10, 1.14 & 2.0 `tensorflow` and `tensorflow-gpu` binaries\r\n- Python version: 3.6.8\r\n- GPU model and memory: GTX 1060 3GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to import `tf.keras` or `tf.python.keras`, I get `AttributeError: module 'tensorflow' has no attribute 'keras'`. I have tried various versions of tensorflow and tensorflow-gpu installed via pip. Using version 1.14 of tensorflow-gpu, it worked, however none of the submodules were available; I checked using `dir(keras)` and the module was completely empty.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should import fine, I've copied code directly from the official tensorflow keras getting started guides.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python3\r\n# works fine\r\nimport tensorflow as tf\r\n# none of the following work; they all produce variants of the aforementioned error\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.keras import layers\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python import keras\r\n```", "comments": ["Running in a docker container with py3 and tf 1.14.0 everything looks right:\r\n```\r\neduardofv@ttmagpie:~$ sudo docker run -it --rm tensorflow/tensorflow:latest-py3 \r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nWARNING: You are running this container as root, which can cause new files in\r\nmounted volumes to be created as the root user on your host machine.\r\n\r\nTo avoid this, run the container by specifying your user's userid:\r\n\r\n$ docker run -u $(id -u):$(id -g) args...\r\n\r\nroot@e29cb559d2b0:/# python\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.14.0'\r\n>>> from tensorflow import keras\r\n>>> from tensorflow.keras import layers\r\n>>> exit\r\nUse exit() or Ctrl-D (i.e. EOF) to exit\r\n>>> \r\nroot@e29cb559d2b0:/# uname -a\r\nLinux e29cb559d2b0 4.18.0-21-generic #22~18.04.1-Ubuntu SMP Thu May 16 15:07:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n``` ", "That's really strange, it's a problem with my Ubuntu/python3/pip3 installation then I guess...\r\n\r\nI've looked into it more and for some reason all of my tensorflow installations are totally empty. `dir(tf)` just has `__name__` etc, no actual modules. Before I upgraded to Ubuntu 18.04 from 16.04 TF worked fine, so it must have been a software issue. No idea how this has happened though.\r\n\r\nAnyway, I've installed conda for python3 and installed tensorflow-gpu 1.13 with cuda-toolkit 9.0 through that, and it seems to work fine (at least, I can import tf modules!)."]}, {"number": 30077, "title": "[XLA] Add half types to the new batchnorm op", "body": "The new Batchnorm operations should allow a backend to operate at half precision as well as full float32 precision.\r\n", "comments": ["a particular backend does not need to support any specific value in any of the fields in its kernal registration.\r\n\r\nI take it you are referring to the values of the reserved_fields, which are presumably a hack to get intermediate values from the fwd to the bwd pass.\r\n\r\nperhaps it would be better if my change modified the type of the mean/scale/variance/offset parameters to 'T', which is really what I was trying to achieve?\r\n\r\n", "> perhaps it would be better if my change modified the type of the mean/scale/variance/offset parameters to 'T', which is really what I was trying to achieve?\r\n\r\nOr you could create a separate attribute `S` that allows narrow types and have the `mean`/`scale`/`variance`/`offset` parameters be of this type.  I don't know if there is way to make this backward compatible though -- we'd want `S` to default to the value of `T`, whatever that is.\r\n\r\nBtw, OOC, why can't you have these parameters be `float`?  Can't you cast them back to `half` / `bfloat` in the implementation?", "Btw, Reed is probably the best person to review this.", "Is there a purpose of allowing `mean`/`scale`/`variance`/`offset` to be float16 or bfloat16? Nvidia observed these values should be float32 for convergence, and there seems to be no upside to having them be a lower precision.", "we have observed that things converge fine with stochastic rounded float16.", "i don't understand why you would limit the definition of the general operation, when the registration of the device specific kernel operation can place per device type limitations.", "regarding 'Btw, OOC, why can't you have these parameters be float? Can't you cast them back to half / bfloat in the implementation?', surely the logical conclusion of this is that all floating point numbers  should be specified as double precision.  ", "/CC @alextp, what do you think of this change? Is there precedent to allowing a type in the OpDef, but not implementing the change in any kernel? I would recommend to users to always keep `mean`/`scale`/`variance`/`offset` in float32, but users may decide they would rather have them in float16.\r\n\r\nI think this will also make the error messages worse if a user does pass a float16 value to `mean`/`scale`/`variance`/`offset`.", "Incidentally, the reason I did this was because the old FusedBatchNormV2 was recently replaced by BatchNormV3.  \r\n\r\nBatchNormV2 has the following signature:\r\n\r\n```\r\nREGISTER_OP(\"FusedBatchNormV2\")\r\n    .Input(\"x: T\")\r\n    .Input(\"scale: U\")\r\n    .Input(\"offset: U\")\r\n    .Input(\"mean: U\")\r\n    .Input(\"variance: U\")\r\n    .Output(\"y: T\")\r\n    .Output(\"batch_mean: U\")\r\n    .Output(\"batch_variance: U\")\r\n    .Output(\"reserve_space_1: U\")\r\n    .Output(\"reserve_space_2: U\")\r\n    .Attr(\"T: {half, bfloat16, float}\")\r\n    .Attr(\"U: {half, bfloat16, float}\")\r\n    .Attr(\"epsilon: float = 0.0001\")\r\n    .Attr(GetConvnetDataFormatAttrString())\r\n    .Attr(\"is_training: bool = true\")\r\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\r\n```", "@reedwm I don't think it should be in the opdef's business to specify the dtypes too narrowly. It's best to leave that to kernel registrations. Many ops technically support dtypes for which we don't have kernels on some devices (say, Add which supports string dtype but not on the gpu)", "> I don't think it should be in the opdef's business to specify the dtypes too narrowly\r\n\r\nIn that case, this changes looks good. I just have one comment.", "thanks guys.  will correct the bfloat thing. ", "i hope all the tests pass.  we don't run the GPU tests in our CI suite, so I can't tell ahead of time if they will break.  I'm guessing not, given that the public python part of the batchnorm interface will still create float32 stats variables.\r\n", "I would be keen to hear about the long term plan with regard to this commit.  We have the same issue, in the sense that our device-side library uses inv-sqrt as its stats outputs (apparently it is something to do with dynamic ranges).\r\n\r\n```\r\ncommit 4b2be43f63b3b11e6d8d5a6648330e42bf5600cb\r\nAuthor: Sanjoy Das <sanjoy@google.com>\r\nDate:   Fri Feb 1 15:39:18 2019 -0800\r\n\r\n    Fix TF/XLA lowering for FusedBatchnorm training on GPU\r\n    \r\n    The FusedBatchnorm TensorFlow operation has two outputs, reserve_space_{1|2},\r\n    that contain unspecified results; in practice on CPUs the they contain the mean\r\n    and the variance and on GPUs they contain the mean the *inverse sqrt of the\r\n    variance* (what cudnn returns).  This is unfortunate for XLA because without a\r\n    spec describing what these outputs are, we can't soundly replace these outputs\r\n    with XLA computed values.\r\n    \r\n    For now we rely on the in-practice values returned in reserve_space_{1|2} and\r\n    add a test case checking that these values are indeed correct.  In the future we\r\n    may consider some of the cleaner fixes outlined in the tracking bug.\r\n    \r\n    PiperOrigin-RevId: 232050515\r\n```\r\n", "> I would be keen to hear about the long term plan with regard to this commit. We have the same issue, in the sense that our device-side library uses inv-sqrt as its stats outputs (apparently it is something to do with dynamic ranges).\r\n\r\nNobody is working on this at the moment, but there are the two other ideas we discussed (from the internal bug report):\r\n\r\n@timshen91 had this interesting idea: we could change the contract of `reserve_space_{1|2}` to be that it either contains the cudnn implementation defined values or a sentinel value (e.g a scalar 0).  Then in the cudnn implementations of `FusedBatchNorm{Grad}` training we could check the shape of the `reserve_space_{1|2}` and act accordingly.\r\n\r\nWe could even go one step further and have `FusedBatchNorm{Grad}` consume/return an additional host side integer that tells us how to interpret `reserve_space_{1|2}`.  The cudnn kernel for `FusedBatchNormGrad` would then check if this integer was, say, `kCudnnBuffer` and only then pass it to the cudnn backwards batch norm.", "I will remove the extra limitation on the format, because it is failing the Ubuntu CC tests.", "In retrospect, i have not changed the data format line at all.  Is this is an error in the Ubuntu CC test where it is comparing an earlier registration to the current code base?\r\n\r\nps.  I think I have addressed all the change requests now.", "This is a bizarre error. It looks like ops_history.v1.pbtxt incorrectly does not currently list the allowed_values for the data_format attribute in FusedBatchNormGradV2. Not sure how this happened.\r\n\r\nI'll try to fix ops_history.v1.pbtxt. Sorry for the delay.", "Try merging past 4c7e2edfea75200d2e4c20e32c73a8a7fb7f764b and running the tests again.", "cheers - doing that now", "done.", "Ugh, now the compatibility test is complaining that for the FusedBatchNormGradV2 and FusedBatchNormGradV3 ops, the type of \"scale\" is changed from float to U. I don't think this actually breaks backwards compatibility, as previously, U could only be float anyway, so any old GraphDefs will not be broken.\r\n\r\n/CC @alextp, what's the best approach here? Creating a new FusedBatchNormGradV4 op? (This is the best approach IMO). Force submitting? Trying to fix the test? (Although it looks like fixing the test is tricky and I currently don't have time).", "@reedwm the solution is not reuse U, and instead make a new dtype parameter for the things which used to be float. You can still register kernels which only support U=new_parameter.", "@alextp the problem with adding a new dtype parameter is that then, `scale` could be a different dtype from the rest of the parameters like `reserve_space_1`, which is not allowed.\r\n\r\nFor example, currently, the op looks like this (I removed some unnecessary lines):\r\n\r\n```\r\nREGISTER_OP(\"FusedBatchNormGradV3\")\r\n    .Input(\"scale: float\")\r\n    .Input(\"reserve_space_1: U\")\r\n    .Attr(\"U: {float}\")\r\n```\r\n\r\nThis PR changes the op to look like this:\r\n\r\n```\r\nREGISTER_OP(\"FusedBatchNormGradV3\")\r\n    .Input(\"scale: U\")\r\n    .Input(\"reserve_space_1: U\")\r\n    .Attr(\"U: {half, bfloat16, float}\")\r\n```\r\n\r\nIn both cases, `scale` and `reserve_space` have the same dtype, which is a requirement. But If we introduce a new dtype parameter, `scale` and `reserve_space` could be different, which is undesired:\r\n\r\n```\r\nREGISTER_OP(\"FusedBatchNormGradV3\")\r\n    .Input(\"scale: V\")\r\n    .Input(\"reserve_space_1: U\")\r\n    .Attr(\"V: {half, bfloat16, float}\")\r\n    .Attr(\"U: {half, bfloat16, float}\")\r\n```", "@reedwm I understand the issue. This is why I encourage you to relax the opdef restriction (by adding another attribute) and instead enforce this property by not registering kernels for the invalid combinations.\r\n\r\nDoing so preserves backward compatibility, in that no existing graph becomes invalid, which is the property that the backwards compatibility test tries to enforce but fails here.", "@alextp thank you for the clarifcation.\r\n\r\n@DavidNorman, can you update `FusedBatchNormGradV2` and `FusedBatchNormGradV3` to add a new attribute for `scale`, as @alextp suggested?", "sure thing.  that makes perfect sense.", "done :)", "@DavidNorman can you please check this link https://source.cloud.google.com/results/invocations/75f4a655-9bd0-4115-baf5-1bfa96838209/log\r\nfor logs , where ubuntu CC builds are failing.", "i see.  it needs a default type.  i'll add that right away", "done", "@rthadur done", "ok - i can see that i'm going to have to look into the data format restriction - make it back into no restriction.  and also fix up this default value thing.", "@reedwm  @rthadur \r\n\r\nI've added the type constraint to the kernels for CPU and GPU in the hope that the grappler GPU test will be ok with that.  The other failures are the return of the data_format issue which  isn't something that has been touched by this pull request.", "@DavidNorman thank you, there are few more tests failing for ubuntu CC here : https://source.cloud.google.com/results/invocations/2499aa6a-de8d-4f0c-b2a1-e3a1a03f2b91/log\r\ncan you please check ", "@rthadur this failing test is testing something that I have not changed.  It is complaining that the batchnorm data format is more constrained than for the current head.  however, this is not true.  the data format has not changed.  i think that the test needs to be checked.  perhaps it is testing against the release version of the framework, not the master version.  i do not think that I should be removing the constraint that someone else has added just to pass the test.", "@DavidNorman will try to pull this in. ", "@DavidNorman this is failing internally with same errors , @reedwm any thoughts how we proceed with this ?", "Whoops, I thought I fixed this in 4c7e2edfea75200d2e4c20e32c73a8a7fb7f764b, but I only fixed it for the gradient ops. Didn't realize it affected the non-gradient ops.\r\n\r\nI will fix the bug preventing the tests here from working properly.", "@reedwm  thank you , will keep merging this on hold.", "5c7b8ea9b23a60efbbe20d3bb3b679a6be381924 should fix this.", "Can one of the admins verify this patch?", "Correct me if I am wrong, but from what i understand, this change would allow the stats variables to be `half` or `bfloat` in addition to `float`. In that case what is the canonical form in the tf2xla bridge? The GPU backend expects these variables to be in fp32 for mixed precision computation. Do all backends now need to add passes to cast the stat variables according to their requirements? If not, then is the canonical form in the bridge still fp32? In that case the bridge needs to handle non-float types by inserting coverts which the backend can eventually choose to eliminate. Sorry if this has already been discussed earlier and  I missed it.", "I am unfamiliar with the tf2xla bridge details. /CC @sanjoy.\r\n\r\nNote even with this change, a BatchNormalization layer will still keep stats variables in float32, and nearly all models would still have the stats variables in float32. ", "@AyanmoI This change redefines the acceptable types of the stats variables.  It does not change the types which are actually registered by each backend, so the GPU registration remains fp32 only, and the GPU will fail if the user attempts to provide an fp16 stat.   This was true before, just the failure was because of a mismatched op registration, not kernel registration.\r\n\r\nThe tf2xla bridge can do what it likes.  As it happens, it does the right thing with non-fp32 types,  but if it was fixed at fp32 for some mad reason, then we could override the tf2xla implementation with a custom one for our backend.\r\n\r\n", "> @AyanmoI This change redefines the acceptable types of the stats variables. It does not change the types which are actually registered by each backend, so the GPU registration remains fp32 only, and the GPU will fail if the user attempts to provide an fp16 stat. This was true before, just the failure was because of a mismatched op registration, not kernel registration.\r\n\r\nI agree. However, IMO with this change, we don't even need to error out. All we need to do is decide on a canonical representation of the batchnorm op at tf2xla level. If we leave it as it is, then backends like GPU can insert converts through a pass if it sees the stats variables in (say) fp16. Or, we can have converts to fp32 in tf2xla and backends which want the stats variables in `Half` can insert another convert in their backend and `algebraic_simplifier` would eliminate the convert pair eventually.\r\n> The tf2xla bridge can do what it likes. As it happens, it does the right thing with non-fp32 types, but if it was fixed at fp32 for some mad reason, then we could override the tf2xla implementation with a custom one for our backend.\r\n\r\nThis PR in itself is fine but I believe we can make changes to the bridge or backends such that no errors occur. We shouldn't throw an error anyway since {fp16,bfloat}->fp32 is an upcast and it should just work from the users perspective. \r\n", "We've decided to abandon this and do something clever lower down.  there are too many other things higher up in the python code which would need to be modified too."]}, {"number": 30076, "title": "[XLA] Add half as supported type for TruncatedNormal op definition", "body": "Add the half type as a supported type for the truncated normal random op.  Backends can still decide on a per-backend basis whether they support that type or not. \r\n", "comments": ["Do you mind adding a test case to tensorflow/compiler/tests/random_ops_test.py ?", "sure thing", "Hi,\r\n\r\nactually it already does test half types:\r\n\r\n```\r\n  def _random_types(self):\r\n    return set(self.numeric_types) - set(\r\n        self.complex_types) - {np.uint64, np.int64, np.uint8, np.int8}\r\n```\r\n", "> Hi,\r\n> \r\n> actually it already does test half types:\r\n\r\nIf there was a test it would have been failing. :)\r\n\r\nI think you only need to change this line (and remove the TODO): https://github.com/tensorflow/tensorflow/blob/b1d7145be23670dc412659f36d62996d024ef709/tensorflow/compiler/tests/random_ops_test.py#L124", "you are absolutely right.  I had checked the tests like:\r\n\r\n```\r\n  def testRandomNormalVariance(self):\r\n    for dtype in self._random_types() & self.float_types:\r\n\r\n```\r\n\r\nbut the `testTruncatedNormalIsInRange` is different.  will patch.", "obviously i've only tested on the CPU and IPU backends, it could be that allowing all float types in that test will cause the GPU or TPU to get upset.  hopefully not.", "i see. the 'truncated normal is not constant' doesn't test all types either. you are quite right - there were no tests at all.\r\n", "done :)", "@gbaned  I think I have sorted all of the requested changes here.", "Hi,\r\n\r\ni had to resolve an issue with this test changing slightly.  it seems that someone has added double support, so that is now in, but i have left in the checks for CPU/GPU w.r.t. halves.  \r\n\r\ni left the TODO(sanjoy) in, even though there is a separate TODO now for getting the test working with halves.", "@DavidNorman Could you please check Ubuntu Sanity errors? Thanks!", "@gbaned done :)", "i've had to abandon the test changes.  there is someone inside google who is also working on getting half formats into the random test, and it keeps conflicting.\r\n\r\nplease consider merging this without the test changes.  \r\n\r\nthis is the code: b/34339814\r\n", "I've realised I can achieve the same result as this by overriding the TF2XLA implementation of truncated normal and specifying a backend-specific implementation of truncated normal.\r\n\r\nclosing this as no longer necessary."]}, {"number": 30075, "title": "[XLA] Remove dependency on having a GPU to run jit tests", "body": "We would like to be able to build the XLA backend without the GPU JIT compiled into it.  We put the xla_gpu_device behind a check for CUDA, and the update the 2 tests which explicitly target the GPU.", "comments": ["sure thing.", "sorry about taking my time over this.", "It turns out that some other GPU things were added to this test since we first put in this compilation fix, and so we don't actually run it any more.  So, I'm going to revert all of these changes in our repo, and abandon this change.\r\n\r\n"]}, {"number": 30074, "title": "model = MyModel()  model1 = model works fine but model1 = MyModel() notworking ( means model and model1are not same)!!", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["```python\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    #self.conv1 = Conv2D(32, 3, activation='relu')\r\n    self.conv1 = Conv2D(32, kernel_size=3, activation='relu')\r\n    self.flatten = Flatten()\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10, activation='softmax')\r\n\r\n  def call(self, x):\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\n\r\nmodel = MyModel()\r\nmodel1 = model\r\n# above is creating model1 with same number of layers of model.\r\n# but in the following, model1 is coming up with zero layer and model is having 3 layers.\r\nmodel1 = MyModel()\r\n```\r\n\r\nThanks in advance  jk", "<img width=\"1167\" alt=\"Screenshot 2019-06-24 at 2 49 52 PM\" src=\"https://user-images.githubusercontent.com/23133817/60007135-5c079000-968f-11e9-8ee2-9941ceb50426.png\">\r\n\r\nI am not able to replicate your issue. Please provide more details. Works fine at my end.", " Thank you for your response.\n  In the following  model = MyMddel()   ..this is working fine\n I am trying to create  2\n model = MyMddel()\n model1 = MyMddel()\nwhere model is used in training with Data set and weights are stored in\n\"model\"\n\n[image: image.png]\n\n Storing weights done in the following ... it is working fi\n[image: image.png]\n\nAfter the above step, i did try Load it on \"model1\"\n\nit is not going through for  \"  model = MyModel()   model1= MyModel()\"\n[image: image.png]\n[image: image.png]\n\n it is going though   for  \"  model = MyModel()   model1=model\"\n\n[image: image.png]\n\nThanks a lot in advance\nRegards\njk\nfrom BLR\n\n\nOn Mon, Jun 24, 2019 at 2:57 PM Kumar Nityan Suman <notifications@github.com>\nwrote:\n\n> [image: Screenshot 2019-06-24 at 2 49 52 PM]\n> <https://user-images.githubusercontent.com/23133817/60007135-5c079000-968f-11e9-8ee2-9941ceb50426.png>\n>\n> I am not able to replicate your issue. Please provide more details. Works\n> fine at my end.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30074?email_source=notifications&email_token=ABNM6UBFJKQZO2WXCBF6SQDP4CHQHA5CNFSM4H24P532YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYMJ7DA#issuecomment-504930188>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABNM6UCRFAM7ZNP2MBCJJPLP4CHQHANCNFSM4H24P53Q>\n> .\n>\n", "Cannot see images.", "\r\n[fnmistJune23v2.pdf](https://github.com/tensorflow/tensorflow/files/3322056/fnmistJune23v2.pdf)\r\nThank you for your response.\r\n\r\n  In the following\r\n     model = MyMddel()   \\\\..this is working fine\r\n\r\n I am trying to create  2\r\n model = MyMddel()\r\n model1 = MyMddel()\r\n\r\nwhere model is used in training with Data set and weights are stored in\r\n\"model\"\r\nStoring weights done in the following ... it is working fine\r\n\r\n   model.save_weights('initial_weights-jk1.h5')\r\n\r\nAfter the above step, i did try Load it on \"model1\"\r\n\r\nit is not going through for  \"  model = MyModel()   model1= MyModel()\"\r\n\r\n  model1.load_weights('initial_weights-jk1.h5')\r\n\r\n it is going though   for  \"  model = MyModel()   model1=model\"\r\n\r\n     model1.load_weights('initial_weights-jk1.h5')\r\n\r\nBest regards\r\njk\r\n\r\n\r\nOn Mon, Jun 24, 2019 at 3:46 PM Kumar Nityan Suman <notifications@github.com>\r\nwrote:\r\n\r\n> Cannot see images.\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/30074?email_source=notifications&email_token=ABNM6UGNMWMUWZNJN66EF2LP4CNGDA5CNFSM4H24P532YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYMOFBA#issuecomment-504947332>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ABNM6UB5MSDKPQLFZZ2Q4GDP4CNGDANCNFSM4H24P53Q>\r\n> .\r\n>\r\n", "Duplicate of #30073 ?", "Can you please help us with .py file, so it will be easy for us to reproduce the issue.Thanks!", "Hello Ravi\r\n  pls find  fnmistJune23v2.ipynb\r\nBest Regards\r\njk\r\n[fnmistJune23v2.ipynb.zip](https://github.com/tensorflow/tensorflow/files/3325629/fnmistJune23v2.ipynb.zip)\r\n\r\n\r\n", "> Can you please help us with .py file, so it will be easy for us to reproduce the issue.Thanks!\r\n\r\nhi ..gm..i did send  ipynb file in the above zip file.. pls use it ...", "I have tried executing the code & i got the following result.Please, let us know is this is the expected output?.\r\n![mynodel](https://user-images.githubusercontent.com/51902062/60318287-10efc480-9990-11e9-950d-61fe5ca80879.png)\r\n\r\n", "> I have tried executing the code & i got the following result.Please, let us know is this is the expected output?.\r\n> ![mynodel](https://user-images.githubusercontent.com/51902062/60318287-10efc480-9990-11e9-950d-61fe5ca80879.png)\r\n\r\nHi Ravi\r\n![SaveLoadv1](https://user-images.githubusercontent.com/5951312/60320097-4a2b3300-9996-11e9-8a3c-4245d00b9ce4.png)\r\n\r\n    model is used to training time  and store weights  in model (file)\r\n    model1 is used to load weights from file\r\n\r\n      issue is during load..\r\n\r\n", "Looks like weights have been saved in the model and loading it from model 1. from the above screenshot. Since the weights has not saved to the model1 ,Please try saving the weights to model1 .Please,check. and let me know if my understanding is correct.Thanks!", "> Looks like weights have been saved in the model and loading it from model 1. from the above screenshot. Since the weights has not saved to the model1 ,Please try saving the weights to model1 .Please,check. and let me know if my understanding is correct.Thanks!\r\n\r\ngood.. model1 also same class ..  it is another new variable of MyModel.\r\nLoading time, there should be provision to load new variable ( which is model1) of same class MyModel\r\n//jk", "> > Looks like weights have been saved in the model and loading it from model 1. from the above screenshot. Since the weights has not saved to the model1 ,Please try saving the weights to model1 .Please,check. and let me know if my understanding is correct.Thanks!\r\n> \r\n> good.. model1 also same class .. it is another new variable of MyModel.\r\n> Loading time, there should be provision to load new variable ( which is model1) of same class MyModel\r\n> //jk\r\n\r\nYes you are right model1 is also same object class as model. But When we save weights explicitly to model, model1 will not have provision to load of the weights of model. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30073, "title": "model = MyModel() #model1 = MyModel() model1 = model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Duplicate of #30074?", "\r\n[fnmistJune23v2.pdf](https://github.com/tensorflow/tensorflow/files/3321535/fnmistJune23v2.pdf)\r\n#30074? and #30073  are same...  I do not know how #30074 generated.. but  in #30073 , i did try find why  option A working and option B is not working. ?\r\n\r\n Option A\r\n   model =  MyModel()\r\n   model1 = model\r\n\r\nOption B\r\n   model =  MyModel()\r\n   model1 = MyModel()\r\n\r\n\r\n\r\n   ", "Closing this as duplicate, as the other one has more content\r\n\r\nAlso, please use triple back-quotes to format your code blocks: https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown"]}, {"number": 30072, "title": "Can build tflite with xcode ?", "body": "I build tflite with https://www.tensorflow.org/lite/guide/build_ios and set not optimizing code for debug code in tflite \r\nThough can hit breakpoint and step by step to debug sources but has some messy so i want build tflite from source with xcode\r\n", "comments": ["@weinixuehao : Thanks for reaching out to us. Can you please help us to understand more about the issue you are facing as in which TensorFlow version you are using, which step you are stuck in. It will help us to resolve the issue faster. Thanks!", "@achandraa I built tflite binary with debug symbols by -g option but debug experience is not better. some variable could not view value and step by step debuging has some messy so i wanted build source directly with xcode but this way need to know which files to compile ", "@weinixuehao Could you please let us know if you still need help on this issue ? please refer this **[document](https://www.tensorflow.org/lite/guide/build_ios)** and similar issue **[link](https://stackoverflow.com/questions/52030130/how-to-build-and-run-the-tensorflow-lite-ios-examples)** . if it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30072\">No</a>\n"]}, {"number": 30071, "title": "Classifying Handwritten Digits with TF.Learn - Machine Learning(jupyter notebook on docker)", "body": "---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nInvalidArgumentError: tensor_name = linear//weight; shape in shape_and_slice spec [1,10] does not match the shape stored in checkpoint: [784,10]\r\n\t [[{{node save/RestoreV2}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)\r\n   1275         sess.run(self.saver_def.restore_op_name,\r\n-> 1276                  {self.saver_def.filename_tensor_name: save_path})\r\n   1277     except errors.NotFoundError as err:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n\r\nInvalidArgumentError: tensor_name = linear//weight; shape in shape_and_slice spec [1,10] does not match the shape stored in checkpoint: [784,10]\r\n\t [[node save/RestoreV2 (defined at <ipython-input-56-0354ba381638>:2) ]]\r\n\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\r\n    self.run()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\r\n    return runner(coro)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-56-0354ba381638>\", line 2, in <module>\r\n    print (\"Predicted %d, Label: %d\" % (classifier.predict(test_data[0]), test_labels[0]))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 539, in predict\r\n    as_iterable=as_iterable)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 574, in predict_classes\r\n    as_iterable=as_iterable)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 670, in predict\r\n    iterate_batches=iterate_batches)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 974, in _infer_model\r\n    config=self._session_config))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 934, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1122, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1127, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 805, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 562, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 217, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 604, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\r\n    self.build()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 844, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 507, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 385, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\r\n    name=name)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): tensor_name = linear//weight; shape in shape_and_slice spec [1,10] does not match the shape stored in checkpoint: [784,10]\r\n\t [[node save/RestoreV2 (defined at <ipython-input-56-0354ba381638>:2) ]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-56-0354ba381638> in <module>\r\n      1 # here's one it gets right\r\n----> 2 print (\"Predicted %d, Label: %d\" % (classifier.predict(test_data[0]), test_labels[0]))\r\n      3 display(0)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    572                   func.__module__, arg_name, arg_value, 'in a future version'\r\n    573                   if date is None else ('after %s' % date), instructions)\r\n--> 574       return func(*args, **kwargs)\r\n    575 \r\n    576     doc = _add_deprecated_arg_value_notice_to_docstring(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    572                   func.__module__, arg_name, arg_value, 'in a future version'\r\n    573                   if date is None else ('after %s' % date), instructions)\r\n--> 574       return func(*args, **kwargs)\r\n    575 \r\n    576     doc = _add_deprecated_arg_value_notice_to_docstring(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py in predict(self, x, input_fn, batch_size, outputs, as_iterable)\r\n    537           input_fn=input_fn,\r\n    538           batch_size=batch_size,\r\n--> 539           as_iterable=as_iterable)\r\n    540     return super(LinearClassifier, self).predict(\r\n    541         x=x,\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    572                   func.__module__, arg_name, arg_value, 'in a future version'\r\n    573                   if date is None else ('after %s' % date), instructions)\r\n--> 574       return func(*args, **kwargs)\r\n    575 \r\n    576     doc = _add_deprecated_arg_value_notice_to_docstring(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py in predict_classes(self, x, input_fn, batch_size, as_iterable)\r\n    572         batch_size=batch_size,\r\n    573         outputs=[key],\r\n--> 574         as_iterable=as_iterable)\r\n    575     if as_iterable:\r\n    576       return _as_iterable(preds, output=key)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in predict(self, x, input_fn, batch_size, outputs, as_iterable, iterate_batches)\r\n    668         outputs=outputs,\r\n    669         as_iterable=as_iterable,\r\n--> 670         iterate_batches=iterate_batches)\r\n    671 \r\n    672   def get_variable_value(self, name):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _infer_model(self, input_fn, feed_fn, outputs, as_iterable, iterate_batches)\r\n    972               checkpoint_filename_with_path=checkpoint_path,\r\n    973               scaffold=infer_ops.scaffold,\r\n--> 974               config=self._session_config))\r\n    975       if not as_iterable:\r\n    976         with mon_sess:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py in __init__(self, session_creator, hooks, stop_grace_period_secs)\r\n    932     super(MonitoredSession, self).__init__(\r\n    933         session_creator, hooks, should_recover=True,\r\n--> 934         stop_grace_period_secs=stop_grace_period_secs)\r\n    935 \r\n    936 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py in __init__(self, session_creator, hooks, should_recover, stop_grace_period_secs)\r\n    646         stop_grace_period_secs=stop_grace_period_secs)\r\n    647     if should_recover:\r\n--> 648       self._sess = _RecoverableSession(self._coordinated_creator)\r\n    649     else:\r\n    650       self._sess = self._coordinated_creator.create_session()\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py in __init__(self, sess_creator)\r\n   1120     \"\"\"\r\n   1121     self._sess_creator = sess_creator\r\n-> 1122     _WrappedSession.__init__(self, self._create_session())\r\n   1123 \r\n   1124   def _create_session(self):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py in _create_session(self)\r\n   1125     while True:\r\n   1126       try:\r\n-> 1127         return self._sess_creator.create_session()\r\n   1128       except _PREEMPTION_ERRORS as e:\r\n   1129         logging.info('An error was raised while a session was being created. '\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py in create_session(self)\r\n    803       \"\"\"Creates a coordinated session.\"\"\"\r\n    804       # Keep the tf_sess for unit testing.\r\n--> 805       self.tf_sess = self._session_creator.create_session()\r\n    806       # We don't want coordinator to suppress any exception.\r\n    807       self.coord = coordinator.Coordinator(clean_stop_exception_types=[])\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py in create_session(self)\r\n    569         init_op=self._scaffold.init_op,\r\n    570         init_feed_dict=self._scaffold.init_feed_dict,\r\n--> 571         init_fn=self._scaffold.init_fn)\r\n    572 \r\n    573 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\r\n    279         wait_for_checkpoint=wait_for_checkpoint,\r\n    280         max_wait_secs=max_wait_secs,\r\n--> 281         config=config)\r\n    282     if not is_loaded_from_checkpoint:\r\n    283       if init_op is None and not init_fn and self._local_init_op is None:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py in _restore_checkpoint(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\r\n    193 \r\n    194     if checkpoint_filename_with_path:\r\n--> 195       saver.restore(sess, checkpoint_filename_with_path)\r\n    196       return sess, True\r\n    197 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)\r\n   1310       # We add a more reasonable error message here to help users (b/110263146)\r\n   1311       raise _wrap_restore_error_with_msg(\r\n-> 1312           err, \"a mismatch between the current graph and the graph\")\r\n   1313 \r\n   1314   @staticmethod\r\n\r\nInvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\ntensor_name = linear//weight; shape in shape_and_slice spec [1,10] does not match the shape stored in checkpoint: [784,10]\r\n\t [[node save/RestoreV2 (defined at <ipython-input-56-0354ba381638>:2) ]]\r\n\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 539, in run_forever\r\n    self._run_once()\r\n  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1775, in _run_once\r\n    handle._run()\r\n  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 781, in inner\r\n    self.run()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\r\n    return runner(coro)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-56-0354ba381638>\", line 2, in <module>\r\n    print (\"Predicted %d, Label: %d\" % (classifier.predict(test_data[0]), test_labels[0]))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 539, in predict\r\n    as_iterable=as_iterable)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 574, in predict_classes\r\n    as_iterable=as_iterable)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 670, in predict\r\n    iterate_batches=iterate_batches)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 974, in _infer_model\r\n    config=self._session_config))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 934, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1122, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1127, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 805, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 562, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 217, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 604, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\r\n    self.build()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 844, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 507, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 385, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\r\n    name=name)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\ntensor_name = linear//weight; shape in shape_and_slice spec [1,10] does not match the shape stored in checkpoint: [784,10]\r\n\t [[node save/RestoreV2 (defined at <ipython-input-56-0354ba381638>:2) ]]\r\n", "comments": ["\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30070, "title": "AttributeError: module 'tensorflow' has no attribute 'set_random_seed'", "body": "tf2.0\r\n\r\n    tf.set_random_seed(self._seed)\r\nAttributeError: module 'tensorflow' has no attribute 'set_random_seed'", "comments": ["Moved to `tf.random.set_seed()`\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/set_seed\r\n", "@Jie-Yuan Were you able to resolve this issue? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30068, "title": "Device getting slower when running tensorflow c++", "body": "Dear all, I have encountered a problem that tensorflow inference time is getting slower. \r\n\r\nSituation:\r\n\r\nOur device is used in the production line and tensorflow prediction is one of the module in the whole project. Since in production line the device is operating for a very long time (24 hours non-stop), after a few days (probably 3-5 days) we can feel that the device is getting slower and tensorflow inference time is about 2-3 times as usual. We suspect it was caused by GPU memory leak or some other issues. May I ask has anyone encountered such problem? How should I do with this situation?\r\n\r\nEnvironment:\r\n\r\nOS: Windows 10 (64 bits)\r\nCPU: I7 - 6th Generation\r\nGPU: Nvidia GTX 1050 Ti\r\nTF Version: r1.4\r\nCMake build with VS 2015\r\n", "comments": ["BTW, if I want to reset GPU memory during runtime (maybe use session->Close() ), what should I do or how should I modify the source code? Seems currently session->Close() will not free GPU memory. Thanks!", "To reset gpu memory allocation; See https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-367948448", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30067, "title": "make rocm compile", "body": "3e3b9156 changed `CHECK_ERR()` to `PCHECK()`.", "comments": ["Oops, it's fixed by  https://github.com/tensorflow/tensorflow/pull/30030 already. \r\nClose this."]}, {"number": 30066, "title": " How to configure c++ tensorflow1.13 environment under vs2015 compiler?", "body": " how to configure c++ tensorflow1.13 environment under vs2015 compiler?", "comments": ["In win10 environment, using bazel has successfully compiled the version of tensorflow 1.13, how to configure c++ tensorflow1.13 environment under vs2015 compiler? Help, thank you!"]}, {"number": 30065, "title": "No such package 'tensorflow/contrib/android': BUILD file not found on package path", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: last version from Github\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.26\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: Nivida Geforce 840m\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI try to use this tool for building .so file using this command line:\r\n> bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so  --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=-std=c++11  --cpu=armeabi-v7a\r\n\r\nBut I get this error :\r\n> ERROR: Skipping '//tensorflow/contrib/android:libtensorflow_inference.so': no such package 'tensorflow/contrib/android': BUILD file not found on package path\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'tensorflow/contrib/android': BUILD file not found on package path\r\nINFO: Elapsed time: 0.543s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nHow can I solve it?\r\nThanks", "comments": ["Were you able to solve this issue?", "I did not continue with this command line,\r\nI followed other instruction to generate tflite file model", "Closing this issue since you were able to get a workaround. Feel free to reopen if have further questions. Thanks!", "Why you closed this issue?\r\nYou did not give me any solution to my problem.\r\n", "I'm not able to repro the problem using the BUILD rule @ https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/BUILD#L61. It's possible there are issues with building on a Windows host, though I don't see an immediate reason why. In any case, TensorFlow Mobile (which contrib/android supports) has been deprecated in favor of TensorFlow Lite. We'll be sure to update the documentation accordingly.", "I have used python tflite converter and I get tflite model. So for now I want to know how can I use tflite model and how can I extract features from this file?", "For general TensorFlow Lite usage please see the guide @ https://www.tensorflow.org/lite/guide. There are complete steps for using inference after you've successfully converted your model.", "this documentation didn't have instruction how it works the extraction of features from the output nodes!", "Extracting output features is model-dependent. The output tensors are simply typed buffers. Here is some [additional guidance](https://www.tensorflow.org/lite/guide/faq#how_do_i_determine_the_inputsoutputs_for_graphdef_protocol_buffer) for better understanding model outputs.", "Thanks, I will seet it", "You're getting this error because there is no contrib module in the master branch. Switch to a release branch and you'll get contrib there. "]}, {"number": 30064, "title": "Deploy .pb frozen model in Android mobile", "body": "Hello,\r\nI have worked on iris detection ( in the human eye ) in the real-time project using deep learning. So I have used a CNN model based on facial landmarks detection using Tensorflow GPU version with regression predict and I have modified the number of points to get only the iris region, \r\nI have now .ckpt files trained the model with 146 000 steps and I have tried to freeze the model but I'm not sure that the process of the freeze is correct or no.\r\nI get a .pb file with size 28 Mo.\r\nI want to deploy and use this .pb file into an Android application to detect the iris region by drawing the landmarks on it.\r\nHow can I use a .pb file for Android application?\r\nHow can I extract the prediction from .pb file in the Android application?\r\nThanks\r\n", "comments": ["@abdou31 This [document](https://www.tensorflow.org/lite/convert/cmdline_examples) will guide you how to convert frozen graph to lite model. Let us know if that helps you. Thanks!", "Thanks @gadagashwini , I will see it", "@abdou31 Can we close this issue.Since it is not bug/performance or build/install issue. Thanks! ", "I have successfully converted .pb frozen model to tflite model but the problem I didn't know that this is correct or no.\r\nDo you have idea how can I know that the file generated ( tflite ) is corrected?\r\nI write this because I didn't have any about output and input nodes setted are correct or no?", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Ok, but can you tell me where can I post this issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). Thanks!", "@abdou31 Have look on [tensorflow lite guide](https://www.tensorflow.org/lite/guide/android) for android app. Thanks!", "Actually, I ask this question in StackOverflow but I didn't get any answer.\r\nI don't know why but I think that the community didn't give any importance to questions related to IA (Deep learning  and Machine learning).", "Closing this issue since the original title of the issue is resolved. Please post a new issue if facing additional problems unrelated to original issue. This will help us to keep track of the topics discussed. Thanks!", "This problem is solved.\r\nTo deploy a model into Android Studio. it should using a tflite model not a pb model", "@abdou31 have u able to deploy the model into Android?\r\nIf yes can u tell me how did u test  tflite model?\r\n", "@shbnm21, \r\nPlease take a look at [android guide doc](https://www.tensorflow.org/lite/guide/android). ", "@gadagashwini I will look into that.\r\nI have used a pretrained model (GAN model) for face attribute manipulation .converted into .pb file.The size of .pb file is 166mb.The model is based on tensorflow 1.15 .I have googled about deployment of the model on Android.some blogs says to use .pb file into Android and some says to use tflite .\r\nNow I am confused can u provide any documentation on how to convert .pb file into tflite and to test it's accuracy.\r\nAlso I want to know whether tflite can be used to convert GAN models.", "@shbnm21,\r\nPlease find the documentation on how to convert pb to lite model in this [link](https://www.tensorflow.org/lite/convert/cmdline_examples#convert_a_tensorflow_graphdef_). Thanks!", "@gadagashwini thank you", "You are right there are some apps [like this](https://play.google.com/store/apps/details?id=com.bgstudio.auto.removebg) which using direct .pb file in their app. I don't know how but I found they are using .pb (Frozen model) in their app! So, its possible there is some technique must be to use it!", "@nihkilft123 \r\nDo u know anything how faceapp has deployed their model on android?\r\nMy understanding is they have created API and process the image on server and send back the results on Android.\r\n\r\n", "Yes if you can find their model.pb than definitely you can put it into your app! Here I have code which I implemented:\r\n\r\n    var tensorFlow: TensorFlowInferenceInterface? = null\r\n\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n     ......\r\n        tensorFlow = TensorFlowInferenceInterface(assets, TF_OD_API_MODEL_FILE)\r\n\r\n        //I have model maximum capacity of 512x512 size look your own also\r\n        val height = 512\r\n        val width = 512\r\n        val bm: Bitmap = ImageUtils.resizeBitmapFitAspect(width, height, bitmap) // This method converts the bitmap into maximum size limit\r\n\r\n        val i: Int = width * height\r\n        val inputArray = IntArray(i)\r\n        val inputBytes = ByteArray(i * 3)\r\n        val outputArray = IntArray(i)\r\n        bm.getPixels(inputArray, 0, width, 0, 0, width, height)\r\n            for (i2 in inputArray.indices) {\r\n                val i3 = inputArray[i2]\r\n                val i4 = i2 * 3\r\n                inputBytes[i4 + 0] = (i3 shr 16 and 255).toByte()\r\n                inputBytes[i4 + 1] = (i3 shr 8 and 255).toByte()\r\n                inputBytes[i4 + 2] = (i3 and 255).toByte()\r\n            }\r\n        tensorFlow?.feed(\"ImageTensor\", inputBytes, 1, width.toLong(), height.toLong(), 3)\r\n\r\n        val outputNode = \"SemanticPredictions\"\r\n        val outputNodes = arrayOf(outputNode)\r\n        tensorFlow?.run(outputNodes, true)\r\n        tensorFlow?.fetch(outputNode, outputArray)\r\n\r\n        val dArr = DoubleArray(256)\r\n        for (i5 in 0 until height) {\r\n            for (i6 in 0 until width) {\r\n                val i7: Int = i5 * width + i6\r\n                if (outputArray[i7] != 0) {\r\n                    val i8 = outputArray[i7]\r\n                    dArr[i8] = dArr[i8] + 1.0\r\n                }\r\n            }\r\n        }\r\n        for (i9 in height / 2 - 10 until height / 2 + 10) {\r\n            for (i10 in width / 2 - 10 until width / 2 + 10) {\r\n                val i11: Int = i9 * width + i10\r\n                dArr[outputArray[i11]] = dArr[outputArray[i11]] * 1.04\r\n            }\r\n        }\r\n        var d2 = 0.0\r\n        var i12 = 0\r\n        for (i13 in 1..255) {\r\n            if (d2 < dArr[i13]) {\r\n                d2 = dArr[i13]\r\n                i12 = i13\r\n            }\r\n        }\r\n        for (i14 in 0 until height) {\r\n            for (i15 in 0 until width) {\r\n                val i16: Int = i14 * width + i15\r\n                outputArray[i16] = if (outputArray[i16] == i12) 1 else 0\r\n            }\r\n        }\r\n\r\n        val result = getBitmapFromOutput(outputArray, bm)\r\n    }\r\n\r\n    private fun getBitmapFromOutput(iArr: IntArray, bitmap: Bitmap): Bitmap {\r\n        val width = bitmap.width\r\n        val height = bitmap.height\r\n        val createBitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)\r\n        val iArr2 = java.lang.reflect.Array.newInstance(Int::class.javaPrimitiveType, width, height) as Array<IntArray>\r\n        val iArr3 = java.lang.reflect.Array.newInstance(Int::class.javaPrimitiveType, width, height) as Array<IntArray>\r\n        for (i in 0 until height) {\r\n            for (i2 in 0 until width) {\r\n                val iArr4 = iArr2[i2]\r\n                val iArr5 = iArr3[i2]\r\n                val i3 = iArr[i * width + i2]\r\n                iArr5[i] = i3\r\n                iArr4[i] = i3\r\n            }\r\n        }\r\n        for (i4 in 0 until height) {\r\n            for (i5 in 0 until width) {\r\n                createBitmap.setPixel(i5, i4, if (iArr3[i5][i4] == 1 && iArr2[i5][i4] == 1) bitmap.getPixel(i5, i4) else 0)\r\n            }\r\n        }\r\n        return createBitmap\r\n    }\r\n\r\nImageUtils:\r\n\r\n    fun resizeBitmapFitAspect(width: Int, height: Int, bitmap: Bitmap): Bitmap {\r\n            val background = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)\r\n            val originalWidth = bitmap.width.toFloat()\r\n            val originalHeight = bitmap.height.toFloat()\r\n            val canvas = Canvas(background)\r\n            val scale: Float\r\n            var xTranslation = 0.0f\r\n            var yTranslation = 0.0f\r\n            if (originalWidth > originalHeight) {\r\n                scale = width / originalWidth\r\n                yTranslation = (height - originalHeight * scale) / 2.0f\r\n            } else {\r\n                scale = height / originalHeight\r\n                xTranslation = (width - originalWidth * scale) / 2.0f\r\n            }\r\n            val transformation = Matrix()\r\n            transformation.postTranslate(xTranslation, yTranslation)\r\n            transformation.preScale(scale, scale)\r\n            val paint = Paint()\r\n            paint.isFilterBitmap = true\r\n            canvas.drawBitmap(bitmap, transformation, paint)\r\n            return background\r\n        }\r\n        \r\nWell this code perfectly get output from `.pb` model but if want to use it than you must have info about that model. I found app which is removing background from people photo and use. So, just pretty steal that model and use it!", "> @nihkilft123\r\n> Do u know anything how faceapp has deployed their model on android?\r\n> My understanding is they have created API and process the image on server and send back the results on Android.\r\n\r\nNot all apps are working with APIs some also directly using tensorflow model in their app! According to my deep research and knowledge!", "@nihkilft123 can u share what is your app all about?\r\n\r\n", "@nihkilft123 is that function tensorflow.feed is similar to feed_dict in tensorflow machine learning\r\n\r\n", "> @nihkilft123 can u share what is your app all about?\r\n\r\nI think app link in my first comment may told you all!", "Ok got it.thanks...\r\n", "@nihkilft123 can u explain me your code pls.the output of tensorflow is byte array and you are converting it into bitmap but after tenssorflow.fetch what is happening ?\r\nsorry if my question is stupid."]}, {"number": 30062, "title": "Expose tf.keras.preprocessing.text.tokenizer_from_json to match Keras API", "body": "This fix tries to address the issue raised in #30061 where `tf.keras.preprocessing.text.tokenizer_from_json` is not available, while in keras, `keras.preprocessing.text.tokenizer_from_json` is available.\r\n\r\nThis fix expose `tf.keras.preprocessing.text.tokenizer_from_json` and alias to `keras.preprocessing.text.tokenizer_from_json`, like all other similar APIs in the same keras_proprocessing/text.py\r\n\r\nThis fix fixes #30061.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This broke a lot of builds and is now rolled back in c205be168295893f7f49be3ed8c9d5d14e0307a4\r\n\r\n```python\r\n  File \".../tensorflow/python/keras/preprocessing/text.py\", line 30, in <module>\r\n    tokenizer_from_json = text.tokenizer_from_json\r\nAttributeError: 'module' object has no attribute 'tokenizer_from_json'\r\n```", "Any update on this issue?", "It has never been rolled forward :(\r\n\r\nI'll attempt something later today", "This cannot be done before we update the keras-preprocessing dependency to a newer version. As of now, `text` module doesn't contain `tokenizer_from_json`\r\n\r\nI think we might try this next week, please ping me if this gets delayed more.", "is there an ETA for this fix?  If not, is there a temporary workaround (besides going back to tf1)?", "Please ping me after TF2.0-RC0 is released to check.", "@mihaimaruseac @mike-edmonds Since 2.0.0RC0 has been released, I created a PR #31946 to re-apply the changes.", "> Please ping me after TF2.0-RC0 is released to check.\r\n\r\nCould you tell when this will be fixed.  ", "Ping\uff0c the fix is not part of rc1 yet.", "We cannot release it in 2.0 as it depends on us changing some setup on our release infrastructure (as it requires newer keras-preprocessing). So we have to wait until 2.0 is released and then the fix will land in master in 1-2 days. Sorry about this.", "@mihaimaruseac Is there any update on this issue? ", "Blocked on a 1.15 release pending. We cannot update the release infrastructure while a release is pending. Sorry about this", "This is fixed now"]}, {"number": 30061, "title": "Method mentioned in documentation is missing: tf.keras.preprocessing.text.tokenizer_from_json", "body": "**System information**\r\n- TensorFlow version: v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n\r\n**Describe the current behavior**\r\nError raised when attempting to call `tf.keras.preprocessing.text.tokenizer_from_json`:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n----> 1 tf.keras.preprocessing.text.tokenizer_from_json\r\n\r\nAttributeError: module 'tensorflow.python.keras.api._v2.keras.preprocessing.text' has no attribute 'tokenizer_from_json'\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect this method to be callable per the documentation of `tf.keras.preprocessing.text.Tokenizer.to_json` (below), which says we can use `keras.preprocessing.text.tokenizer_from_json(json_string)` to load a tokenizer.\r\n\r\nOtherwise, there's no obvious way to use the output of `Tokenizer.to_json` to restore a tokenizer.\r\n\r\n```\r\nSignature: tf.keras.preprocessing.text.Tokenizer.to_json(self, **kwargs)\r\nDocstring:\r\nReturns a JSON string containing the tokenizer configuration.\r\nTo load a tokenizer from a JSON string, use\r\n`keras.preprocessing.text.tokenizer_from_json(json_string)`.\r\n\r\n# Arguments\r\n    **kwargs: Additional keyword arguments\r\n        to be passed to `json.dumps()`.\r\n\r\n# Returns\r\n    A JSON string containing the tokenizer configuration.\r\n```\r\n**Code to reproduce the issue**\r\n```python3\r\njson_string = tf.keras.preprocessing.text.Tokenizer().to_json()\r\ntf.keras.preprocessing.text.tokenizer_from_json(json_string)\r\n```", "comments": ["Added a PR #30062 for the fix."]}, {"number": 30060, "title": "arm-none-eabi-g++: error: tensorflow/lite/experimental/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o: No such file or directory tensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:372: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech' failed make: *** [tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech] Error 1", "body": "arm-none-eabi-g++: error: tensorflow/lite/experimental/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o: No such file or directory\r\ntensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:372: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech] Error 1", "comments": ["arm-none-eabi-g++: error: tensorflow/lite/experimental/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o: No such file or directory\r\ntensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:372: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech] Error 1", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Hi Saptech,\r\n\r\nCould you provide a list of steps int order to reproduce this issue? It looks like the download didn't compete. Could you delete tensorflow/lite/experimental/micro/tools/make/downloads folder and try again?\r\n\r\nThanks,", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30059, "title": "Eager execution gradients for Grad CAM", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise 7, Windows 10 Education\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.13.1\r\n- **Python version**: 3.7.3\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 10.1, 7.4\r\n- **GPU model and memory**: RTX 2080, 8GB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have been struggling with this for a few days and finally decided this was an issue worthy of submission here because it deals directly with documentation. I am trying to use TensorFlow eager execution to visualize gradient class activation maps for images, but I'm absolutely stuck on how to compute the gradient between two layers of a sequential model.\r\n\r\nThe core of the question is how to implement [this](http://www.hackevolve.com/where-cnn-is-looking-grad-cam/) in TensorFlow with eager execution. This seems like it should be simple to do (considering it's almost verbatim an example from _Deep Learning with Python_  by Fran\u00e7ois Chollet), but I cannot find any documentation regarding it.\r\n\r\nI posted this to StackOverflow in the hopes that somebody might be able to point me in the right direction, but so far no luck: https://stackoverflow.com/questions/56711640/tensorflow-eager-execution-compute-gradient-between-two-layers-of-a-sequential\r\n\r\nOne line, in particular, has me stumped:\r\n\r\n```python\r\ngrads = K.gradients(class_output, last_conv_layer.output)[0]\r\n```\r\n\r\nI understand that it is finding the gradients between the last convolutional layer and the output for the particular class. However, I cannot figure out how to accomplish this using `GradientTape`, since (a) both are tensors and not variables, and (b) one is not directly derived from the other (their feature maps already exist, so without a graph they are effectively independent).\r\n\r\nThe obvious steps are reproducing the first part with Eager execution.\r\n\r\n```python\r\nimport numpy as np\r\nimport cv2\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nmodel = tf.keras.models.load_model(\"model.h5\")\r\nprint(type(model))\r\n# tensorflow.python.keras.engine.sequential.Sequential\r\n\r\nfrom dataset import prepare_dataset\r\n_, ds, _, _, _, _ = prepare_dataset() # ds is a tf.data.Dataset\r\nprint(type(ds))\r\n# tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter\r\n\r\nit = train_ds.make_one_shot_iterator()\r\nimg, label = it.get_next()\r\nprint(type(img), img.shape)\r\n# <class 'tensorflow.python.framework.ops.EagerTensor'> (192, 192, 3)\r\n\r\nprint(type(label), label.shape)\r\n# <class 'tensorflow.python.framework.ops.EagerTensor'> (2,)\r\n\r\nimg = np.expand_dims(img, axis=0)\r\nprint(img.shape)\r\n# (1, 192, 192, 3)\r\n\r\npredictions = model.predict(img)\r\nprint(predictions)\r\n# array([[0.9711799 , 0.02882008]], dtype=float32)\r\n\r\nclass_idx = np.argmax(predictions[0])\r\nprint(class_idx)\r\n# 0\r\n\r\nclass_output = model.output[:, class_idx]\r\nprint(model.output, class_output)\r\n# Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32) Tensor(\"strided_slice_5:0\", dtype=float32)\r\n\r\nlast_conv_layer = model.get_layer('conv2d_33') # the last conv layer\r\n\r\n\"\"\"\r\nNow, the fun part: how do I compute the gradient of class_output with respect to\r\nthe output of the last convolutional layer?\r\n\"\"\"\r\n```\r\n\r\nOne attempt is using reduce_sum and multiply to get the desired gradient (ignore the `class_output` step):\r\n\r\n```python\r\nwith tf.GradientTape() as tape: \r\n    print(label)\r\n    # tf.Tensor([1. 0.], shape=(2,), dtype=float32)\r\n    y_c = tf.reduce_sum(tf.multiply(model.output, label))\r\n    print(y_c)\r\n    # Tensor(\"Sum_4:0\", shape=(), dtype=float32)\r\n    last_conv_layer = model.get_layer('activation_6')\r\n\r\ngrad = tape.gradient(y_c, last_conv_layer.output)\r\n```\r\n\r\nHowever, `grad` is `None` in this setup.\r\n\r\nI am unclear on where to go from here. Any help is appreciated.", "comments": ["Can you please provide me model.h5 file to reproduce the issue.Thanks.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]