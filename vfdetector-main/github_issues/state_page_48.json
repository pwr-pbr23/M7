[{"number": 43773, "title": "Saving and loading Keras model with RNN layer that uses both multiple input and constants will result in ValueError when using model.predict()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.3.0\r\n- Python version:3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen using a RNN layer that has multiple inputs, as well as contants input, saving and loading it to disk using the H5 file format will produce the following when using model.predict:\r\n\r\n`tensorflow\\python\\keras\\engine\\input_spec.py:155 assert_input_compatibility\r\n        raise ValueError('Layer ' + layer_name + ' expects ' +\r\n\r\n    ValueError: Layer rnn expects 3 inputs, but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 5, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 5, 5) dtype=float32>]`\r\n\r\n**Describe the expected behavior**\r\nI expect equal results (and thus no ValueError) when calling model.predict() on a certain set of inputs, and doing it again but after saving and loading the model\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\nclass RNNCellWithConstants(keras.layers.Layer):\r\n\r\n  def __init__(self, units, constant_size, **kwargs):\r\n    self.units = units\r\n    self.state_size = units\r\n    self.constant_size = constant_size\r\n    super(RNNCellWithConstants, self).__init__(**kwargs)\r\n\r\n  def build(self, input_shape):\r\n    self.input_kernel = self.add_weight(\r\n        shape=(input_shape[0][1], self.units),\r\n        initializer='uniform',\r\n        name='kernel')\r\n    self.recurrent_kernel = self.add_weight(\r\n        shape=(self.units, self.units),\r\n        initializer='uniform',\r\n        name='recurrent_kernel')\r\n    self.constant_kernel = self.add_weight(\r\n        shape=(self.constant_size, self.units),\r\n        initializer='uniform',\r\n        name='constant_kernel')\r\n    self.built = True\r\n\r\n  def call(self, inputs, states, constants):\r\n    [x1, _] = inputs\r\n    [prev_output] = states\r\n    [constant] = constants\r\n    h_input = keras.backend.dot(x1, self.input_kernel)\r\n    h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\r\n    h_const = keras.backend.dot(constant, self.constant_kernel)\r\n    output = h_input + h_state + h_const\r\n    return output, [output]\r\n\r\n  def get_config(self):\r\n    config = {'units': self.units, 'constant_size': self.constant_size}\r\n    base_config = super(RNNCellWithConstants, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\nx1 = keras.Input((None, 5))\r\nx2 = keras.Input((None, 5))\r\nc = keras.Input((3,))\r\ncell = RNNCellWithConstants(32, constant_size=3)\r\nlayer = keras.layers.RNN(cell)\r\ny = layer((x1,x2), constants=c)\r\n\r\nmodel = keras.models.Model([x1, x2, c], y)\r\nmodel.compile(\r\n    optimizer='rmsprop',\r\n    loss='mse')\r\nmodel.train_on_batch(\r\n    [np.zeros((6, 5, 5)), np.zeros((6, 5, 5)), np.zeros((6, 3))],\r\n    np.zeros((6, 32))\r\n)\r\n\r\n# Test basic case \r\nx1_np = np.random.random((6, 5, 5))\r\nx2_np = np.random.random((6, 5, 5))\r\nc_np = np.random.random((6, 3))\r\ny_np = model.predict([x1_np, x2_np, c_np]) \r\n\r\nmodel.save(\"test.h5\")\r\nloaded_model = keras.models.load_model(\"test.h5\", custom_objects={\"RNNCellWithConstants\":RNNCellWithConstants})\r\nloaded_y_np = loaded_model.predict([x1_np, x2_np, c_np]) \r\nassert np.array_equal(y_np, loaded_y_np)\r\n```\r\n\r\n**Other info / logs**\r\nI also figured out why this happens: Python's standard json encoder converts list and tuples into array, and the decoder always turns array's into list. When the RNN __call__ function gets called with a list instead of tuple, the following line\r\n````\r\n        full_input_spec = generic_utils.to_list(\r\n            nest.map_structure(lambda _: None, inputs)) + additional_specs\r\n````\r\nWill map that input differently than if it were a tuple.\r\n\r\nI will be submitting a pr to fix this\r\n", "comments": ["I have tried in colab with TF version 2.3 [gist ](https://colab.research.google.com/gist/ravikyram/c9faf64717b07cdfcea9f94a58656de3/untitled414.ipynb), nightly version [gist](https://colab.research.google.com/gist/ravikyram/cb845f973d39417bf741a1549ee57f01/untitled415.ipynb) and was able to reproduce the issue.Thanks!", "I am able to replicate the issue on tf 2.5, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f0494cbaca40f524522c55b8d8506726/untitled597.ipynb).", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210528, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4ee2d78e2e1e900d0330a9ee3bb01faa/43773.ipynb). Thanks!"]}, {"number": 43768, "title": "Dataset.from_generator() returns a Dataset which freezes execution upon iteration", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab, running Ubuntu 18.04.5 LTS (Bionic Beaver)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **None**\r\n- TensorFlow installed from (source or binary): **Source** (according to [this link](https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=8UvRkm1JGUrk))\r\n- TensorFlow version (use command below): **v2.3.0-0-gb36436b087 2.3.0**\r\n- Python version: **Python 3.6.9**\r\n\r\n**running !bash tf_env_collect.sh threw an error, could not collect the following parameters:**\r\n- Bazel version (if compiling from source): ?\r\n- GCC/Compiler version (if compiling from source): ?\r\n- CUDA/cuDNN version: ?\r\n- GPU model and memory: ? (dynamically allocated by Google colab)\r\n\r\n**Describe the current behavior**\r\nAccording to the docs, tf.data.Dataset has a static method called from_generator(), constructing a dataset from a given iterable. I performed the following actions:\r\n\r\n```\r\n1) downloaded a text file from a link\r\n2) parsed the file using a TextLineDataset (called raw_lines), then limited the size using dataset.take(100)\r\n\r\n3) Created a new dataset (called after_generator_a) using an identity generator on the lines of raw_lines\r\n4) used take(1) to obtain a subset dataset (from after_generator_a), containing a single record\r\n5) iterated over the dataset to print the only record. The record was successfully printed\r\n\r\n6) Created a new dataset (called after_generator_b) using an identity generator on the lines of after_generator_a\r\n7) used take(1) to obtain a subset dataset (from after_generator_a), containing a single record\r\n8) iterated over the dataset to print the only record. Nothing prints, and the Google Colab environment no longer responds\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe code in 8) should print the value successfully, and not crash the Colab environment\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/11ILZUYpb-8Solip6xTbdV8elvPGOpngo?usp=sharing\r\n\r\n**Other info / logs** \r\nNone\r\n", "comments": ["I ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a65df5c1151a2d6185384c0200d4d266/untitled418.ipynb)", "@yonatankarimish This looks like an issue with the Python global interpreter lock (GIL). The function passed to `from_generator` will be executed in Python, which requires holding the GIL. If the function iterates over a second `from_generator` dataset, it will deadlock because the second `from_generator` dataset executes in a different thread, and that thread will also need to acquire the GIL.\r\n\r\nCan you share the goal of using `from_generator` this way? We may be able to find simpler way to achieve the same thing.", "@aaudiber I would like to begin by mentioning I am relatively new to Python, and definitely new to Tensorflow.\r\n\r\n1. The real-life code I was writing was an adaptation of mine to one of the official Keras tutorials, which involved some preprocessing of text. Considering that any medium-sized (and above) text files would probably cause an out-of-memory error if held in memory, I wrote a generator function to extract values from a text file into a Tensorflow `dataset`. Then used nearly identical code to extract values from the first dataset using a second generator into a second dataset.\r\n\r\n(Of course, the first dataset is already held in-memory, so no memory usage improvements by using the second generator)\r\n\r\n2. I didn't really wait for the issue to be resolved and just abandoned the `tf.data.Datasets` altogether. I got the code to work using standard python lists, but I was lucky my dataset was small enough to be held in memory. If my text files were too big to be held in memory, I'd have needed to persist the data to text files between generators, and then calling `from_generator` multiple times would have been a real issue.\r\n\r\n3. I'd expect that if I perform an assignment of the form `my_dataset = tf.data.Dataset.from_generator()`, then by the next line of code the variable my_dataset will contain all my required data, with the generator function yielding all it's values. Therefore - i'd expect the global interpreter lock to be released by the thread running the generator. Am I right making that assumption? if not, why would a thread that has nothing more to execute maintain a lock on the GIL?\r\n\r\n4. Whatever the answer to 3) might be, the bug still persists. I think it's up to you guys to consider if you want to fix it. If you decide not to do so, I think it would have made things more clear if the documentation mentioned you cannot invoke the `from_dataset` function twice in a row.", "I agree this is a bug and would like to see it fixed. \r\n\r\nRegarding (3), it's important to understand that `tf.data.Dataset` objects don't perform any computation or contain any data. Datasets act like blueprints for creating iterators, which do the actual computation to produce data. When you assign `my_dataset = tf.data.Dataset.from_generator(...)`, the generator function will only be executed when you iterate through the dataset. Identity generators are not necessary to create dataset copies. You can reuse the same dataset instead. For example:\r\n\r\n```\r\ndataset = tf.data.Dataset.range(4)\r\n\r\ntimes_two = dataset.map(lambda x: x*2)  \r\nprint(list(times_two.as_numpy_iterator()))  # prints [0, 2, 4, 6]\r\n\r\nplus_one = dataset.map(lambda x: x+1)\r\nprint(list(plus_one.as_numpy_iterator()))  # prints [1, 2, 3, 4]\r\n\r\nfirst_element = dataset.take(1)\r\nprint(list(first_element.as_numpy_iterator()))  # prints [0]\r\n```", "@aaudiber Thanks for the clarification, I think I understand the bug now. ", "While trying to reproduce your issue in Tf Nightly 2.6 faced different error, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/6d8b5b0754747b67a886ba303bfc8a7d/43768.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43768\">No</a>\n", "@aaudiber I might be running this same issue but not entirely sure. Any insights into [50637](https://github.com/tensorflow/tensorflow/issues/50637) - model.fit with dataset generator results in deadlock/hang. Appreciate any thoughts. Thanks"]}, {"number": 43755, "title": "training.tracking.data_structures.List is not properly serialized / deserialized", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nWeights loaded from a layer containing sublayers in a `tracking.data_structures.List` are not properly listed in the `.weights` attributes. This will not only cause confusion but will also sabotage any further optimization or modification of weights. They are properly restored though.\r\n\r\nThe issue is not present when `ListWrapper` is used.\r\n\r\n**Describe the expected behavior**\r\nVariables of sublayers contained in a `List` should be contained in `.weights` when the model is loaded again\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nclass TestLayer(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, **kwargs):\r\n        super(TestLayer, self).__init__(**kwargs)\r\n\r\n        self.static_layer = tf.keras.layers.Dense(128)\r\n        self.my_layers = tf.python.training.tracking.data_structures.List()\r\n        for i in range(4):\r\n            layer = tf.keras.layers.Dense(128)\r\n            self.my_layers.append(layer)\r\n\r\n\r\n    def call(self, x):\r\n        x = self.static_layer(x)\r\n\r\n        for layer in self.my_layers:\r\n            x = layer(x)\r\n\r\n        return x\r\n\r\n\r\n    def get_config(self):\r\n        return super(TestLayer, self).get_config()\r\n\r\n\r\nmodel = tf.keras.Sequential([TestLayer()])\r\n\r\nx = tf.constant(42.0, shape=[1,1])\r\ny1 = model(x)\r\n\r\nmodel.save('my_test_model', save_format='tf')\r\n\r\nmodel_loaded = tf.keras.models.load_model('my_test_model')\r\ny2 = model_loaded(x)\r\n\r\n# output\r\nmodel.summary()\r\nmodel_loaded.summary()\r\nprint('n vars: ', len(model.weights), ' ', len(model_loaded.weights))\r\nprint('diff: ', tf.norm(y1-y2))   \r\n```\r\n\r\n**Output**\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ntest_layer (TestLayer)       (None, 128)               66304     \r\n=================================================================\r\nTotal params: 66,304\r\nTrainable params: 66,304\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ntest_layer (TestLayer)       (None, 128)               256       \r\n=================================================================\r\nTotal params: 256\r\nTrainable params: 256\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nn vars:  10   2\r\ndiff:  tf.Tensor(0.0, shape=(), dtype=float32)\r\n```", "comments": ["I have tried in colab with TF version 2.3 and was able to reproduce the issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/edd95a5ea2b8e20653c85f4f7562e4cf/untitled418.ipynb). However i am seeing the below error message in TF nightly version(`AttributeError: module 'tensorflow' has no attribute 'python'`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/9656de67353c9853fb48a58c0aaeb151/untitled419.ipynb).Thanks!", "I am able to reproduce the issue with `tf-nightly`. [here](https://colab.research.google.com/gist/jvishnuvardhan/adf87ee193ca261778bd1ce9050814ad/untitled419.ipynb) is a gist for our reference. Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210528, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/e68f8be30386637f23f47bb79ace6bd0/43755.ipynb). Thanks!"]}, {"number": 43744, "title": "Variable and Slow InfeedEnqueueTuple on TPUv3-8", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP VM with tf-version nightly\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: TPUv3-8\r\n\r\n**Describe the current behavior**\r\n[InfeedEnqueueTuple](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/tpu/kernels/infeed_ops.h) has variable performance and 33% slowdown. Infeed cannot keep TPUv3-8 saturated even with heavy caching due to stragglers in infeed operator. All input processing time is reported spent in \"while/body/_1/InfeedQueue/enqueue/*\".\r\nI am attaching profiler results with infeed greater than 45ms. Note that each thread has variable time per Enqueue.\r\n![slow_step](https://user-images.githubusercontent.com/12423239/94967869-ee6f2400-04cd-11eb-8c1c-e0a1c2df1a5e.png)\r\n![infeed_slow](https://user-images.githubusercontent.com/12423239/94967461-232eab80-04cd-11eb-8e30-c291098dff33.png)\r\n\r\n**Describe the expected behavior**\r\nInfeedEnqueueTuple has consistent and fast performance (less than 45ms per step). I sometimes am able to observe periods of fast operation, where each infeed is consistently 45ms (image attached). However, infeed eventually regresses to ~60ms per step, as above.\r\n![infeed_fast](https://user-images.githubusercontent.com/12423239/94967617-6be66480-04cd-11eb-828f-035298cd4fb0.png)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nI am using MLPerfv0.7 ResNet [code](https://github.com/mlperf/training_results_v0.7/tree/master/Google/benchmarks/resnet/implementations/resnet-research-TF-tpu-v4-16) with ResNet-18. Issue persists even if dataset cache() (e.g., take(1).cache().repeat()) is placed right before prefetch().\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Same issue with [tpu repository](https://github.com/tensorflow/tpu/tree/master/models/official/resnet) ", "Hi Micheal,\r\n\r\nThanks for reporting. When you mention \"infeed eventually regresses to ~60ms per step, as above.\", is the 60ms consistent and never reduces later? Have you tried if the issue is reproducible on fake data?", "Hi @rxsang,\r\nThe issue persists and the 60ms delays are the normal behavior. I have been unable to get 45ms consistently---it's something that appears at the beginning of training epochs. It might have to do with infeed queueing and the possibility that the queue can get prefetched in during eval time or the start of training (e.g., initialization code), giving an appearance of no bottleneck while the queue is draining.\r\n\r\nYou can reduce the infeed delay by lowering the resolution of the input size e.g., reduce from 224x224 to 112x112 (this reduces the infeed volume), though I still see jitter in the new step time (which is overall reduced by a factor of ~2-3x due to lower image resolution).\r\n\r\nFor fake data: in short, yes it should be easily reproducible. I believe you can just run the tpu repository with the flags `python3 resnet_main.py --tpu=$TPU_NAME --resnet_depth=18 --use_cache=False --model_dir=$MODEL_DIR`. You will also need to add `dataset = dataset.take(1).cache().repeat()` [before the final prefetch](https://github.com/tensorflow/tpu/blob/5144289ba9c9e5b1e55cc118b69fe62dd868657c/models/official/resnet/imagenet_input.py#L217).\r\n\r\nLong answer: I have tried two implementations of synthetic tensors, and they have the same step time issue as above. I see the same issue on MLPerf when I materialize fake tensors of batch_sizex224x224x3 data in the input pipeline (before prefetch). Likewise, on TPU repository, I have tried using empty data_dir, which has the same logical effect, though the step time is ~115ms due to subsequent computational operations (batching). When I `dataset = dataset.take(1).cache().repeat()` before prefetch on this pipeline, the compute goes away and I am left with the 60-70ms step time (as above).  I haven't tried making on-TPU tensors."]}, {"number": 43732, "title": "Batch Renormalization via Keras Layer not working with TF 2+ and tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.3.0-54-gfcc4b966f1 2.3.1`\r\n- Python version: `3.6.2`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nPassing a Tensor derived from a `tf.keras.Input` to the `renorm_clipping` parameter of `tf.keras.layers.BatchNormalization` fails with an error like this:\r\n\r\n```\r\n_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'RealDiv_7:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'AddV2_2:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'RealDiv_8:0' shape=(None, 1) dtype=float32>]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nSince `renorm_clipping` expects Tensors, I would expect any Tensor to work, including those derived from `tf.keras.Input`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[This notebook](https://gist.github.com/georgwiese/12d341bd131e2b16c8d4bb1e275289a6) shows a small example with TF 2.3.1. It creates a model with a step as input, so that batch renorm parameters can be computed from it. Please let me know if this is not the way the step should be passed to the model.\r\n\r\nThe model is then used in a `tf.function` (originally as part of a custom training loop, but simply running the forward pass reproduces the issue).\r\n\r\n**Other info / logs**\r\nSee attached notebook for traceback.\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version (`2.4.0-dev20201004`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/34b820e2c0bd51504ef91dfca447f3f4/untitled421.ipynb). Thanks!", "Hi @georgwiese, have you looked at the error message that occurs in nightly? It provides more information as to what is going on. Specifically `in compilation of <function predict at 0x7f9442407ae8>, found return value of type <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>, which is not a Tensor.` You can see [in the docs here](https://www.tensorflow.org/api_docs/python/tf/keras/Input#returns) that 'Note that even if eager execution is enabled, Input produces a symbolic tensor (i.e. a placeholder)' \r\n\r\nAs a workaround, using `model.predict_step(x)` in your tf.function instead seems to work. Have you tried that? You might also want to look into writing a custom layer.", "@nikitamaia Thanks for your quick response!\r\n\r\n`model.predict_step(x)` works because it runs the model in inference mode (in my original code, if I set `training=False`, I also get no error; same if the `BatchNormalization` parameters do not depend on `step`).\r\n\r\nFor the actual use case, I want to train the model and hence need to run the model in training mode. It is trained using a custom training loop, but I could also reproduce the same error using `tf.keras.Model.fit()` and `tf.keras.Model.train_step()`. Please see [this Notebook](https://gist.github.com/georgwiese/eaa9f048e2a85d93f0bec89c91949cef).\r\n\r\nSo, is there any way to currently use batch renormalization with a dynamic schedule without re-implementing it as a custom layer?", "Ah yes, makes sense why I didn't see the error message with `model.predict_step(x)`!\r\nHowever, I think this question is a better fit for Stack Overflow since there's a bigger community to help. Github is best for bugs/performance issues.", "I [have posted this on Stack Overflow](https://stackoverflow.com/questions/64155323/batch-renormalization-with-tensorflow-2-and-tf-function), but I'm becoming convinced that this is a bug, or at least a documentation issue. It looks like there is no way to use batch renormalization in the recommended way (i.e., with a dynamic schedule).\r\n\r\nOr am I doing something unconventional in the notebook linked above?"]}, {"number": 43708, "title": "Masking support for multiple values.", "body": "Is there a way to mask more than one value in input before sending to Keras layers in TensorFlow? I have to pad in-between too with 0 to make the data balanced in addition to post padding. But my Seq2Seq model is unable to distinguish the in-between paddings and the paddings at the end and during testing, the output from decoder stops as soon as the first in-between padding occurs.\r\n\r\nConsider an input tensor to be,\r\n\r\n    [[1,2,3,4], [5,6,7]] \r\n\r\nBelow is the way I am padding the above tensor. Each sub-list must be of size 6 and the total number of sub-lists should be 3.\r\n\r\n    [ 3,4,5,6,0,0  , 7,8,9,0,0,0  , 0,0,0,0,0,0 ]\r\n\r\nI  want my tensor to be masked this way, where 2 is also masked along with 0, before sending to subsequent layers like embedding and RNN layers.\r\n\r\n    [ 3,4,5,6,2,2  , 7,8,9,2,2,2,  0,0,0,0,0,0 ]\r\n\r\nCould some one also point out a possible solution for preventing the decoder output from stopping at the end of processing the first sub-list of an input?", "comments": ["@vyshnavigutta369\r\nI am not sure whether I have understood you right or not, but maybe two consecutive masking layers (one with mask_value=0. and the other with mask_value=2.) is what you are looking for."]}, {"number": 43706, "title": "ParseTensor (tf.io.parse_tensor) is not vectorized - Vectorizing via tf.vectorized_map uses while_loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 **2.3.0**\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: TITAN V, 12G VRAM\r\n\r\n**Describe the current behavior**\r\nSee the code below. I'm serializing a list of tensors and then attempt to parse them using (1) naive, single-record parsing and (2) batch-parsing using vectorized_map, which I expect to yield a significant performance increase. \r\n\r\nBut: `tf.io.parse_tensor` appear to be not implemented for vectorized parsing, as I'm getting a `WARNING:tensorflow:Using a while_loop for converting ParseTensor` message and see little to no performance increase!\r\n\r\nI find it very surprising that such an essential operation is not vectorized.. how else would I parse no-scalar features from e.g. a TFRecord file? Meanwhile, `tf.io.parse_example` is vectorized. \r\n\r\n**Describe the expected behavior**\r\nI would expect that using a vectorized version of `tf.io.parse_tensor` to yield significant performance increase. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport time\r\n\r\n# This would normaly come from some data stream, e.g. stream of TFRecords\r\nsome_tensor_list = [np.zeros(shape=(5,5), dtype=np.int32)]*100000\r\nsome_tensor_list_serialized = [tf.io.serialize_tensor(x) for x in some_tensor_list]\r\n\r\n# Feed to tf.data\r\ndataset = tf.data.Dataset.from_tensor_slices(some_tensor_list_serialized)\r\n\r\n# Parsing whole batch back to tensors\r\ndef parse_batch(b):\r\n    return tf.vectorized_map(lambda x: tf.io.parse_tensor(x, out_type = tf.int32), b)\r\n\r\n# Parsing single record back to tensors\r\ndef parse_single(x):\r\n    return tf.io.parse_tensor(x, out_type = tf.int32)\r\n\r\n# Compare speed\r\ndef exaust_iterable(it):\r\n    t = time.time()\r\n    for _ in it:\r\n        pass\r\n    print(f'{time.time() - t}s')\r\n\r\n# naive\r\ndataset_naive = dataset.map(parse_single)\r\n\r\nexaust_iterable(dataset_naive)\r\n\r\n# vectorized over batch\r\ndataset_vec = dataset.batch(32)\r\ndataset_vec = dataset_vec.map(parse_batch)\r\n\r\nexaust_iterable(dataset_vec)\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5ebb056a8b89acfeaa0ed82ffa1593c2/43706-tf-nightly.ipynb). Thanks!", "Just came across the same issue, I'm trying to do the following\r\n\r\n1. Load my tfrecord dataset\r\n2. Apply `batch`\r\n3. Parse the batch of `tf.train.Example`s\r\n4. Parse the batch of tensors\r\n\r\nI'm unable to do `4.` since `tf.io.parse_tensor` only accepts a single tensor and not a batch of tensors.\r\n\r\nHere's my code\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nds = tf.data.TFRecordDataset('ds.tfrecords')\r\nds = ds.batch(24) # Without this the code works fine\r\n\r\ndescription = {\r\n    'c_encoded': tf.io.FixedLenFeature([], tf.string)\r\n}\r\n\r\nfor examples in ds:\r\n    # Parse examples in batch\r\n    parsed_example = tf.io.parse_example(examples, description)\r\n    # Fails here since it can only parse one tensor at a time\r\n    c_encoded = tf.io.parse_tensor(parsed_example['c_encoded'], out_type=tf.float32)\r\n```\r\n\r\nMaybe I'm missing something but what's the point of being able to parse a batch of `Example`s if you can't deserialize that batch of tensors? My options are to either loop through the batch and parse each tensor individually but that's less efficient or to batch my data prior to building my  tfrecords but that's less flexible if I want to quickly change the batch size.\r\n\r\nWould be great to have a version of ` tf.io.parse_tensor` that accepts a batch.", "@mhorlacher,\r\nUsing [**`tf.function`**](https://www.tensorflow.org/guide/function) has resulted in significant performance increase. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/7157864b1c7e2ba77a848ee339027f04/43706-tf-nightly.ipynb). Thanks!", "@Jephthia \r\n\r\nYou can use a for-loop to deal with the batch problem of tf.io.parse_tensor, like this(using your code):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nds = tf.data.TFRecordDataset('ds.tfrecords')\r\nds = ds.batch(24) # Without this the code works fine\r\n\r\ndescription = {\r\n    'c_encoded': tf.io.FixedLenFeature([], tf.string)\r\n}\r\n\r\nfor examples in ds:\r\n    # Parse examples in batch\r\n    parsed_example = tf.io.parse_example(examples, description)\r\n    # Fails here since it can only parse one tensor at a time\r\n    # c_encoded = tf.io.parse_tensor(parsed_example['c_encoded'], out_type=tf.float32)\r\n\r\n    # A for-loop\r\n    c_encoded = []\r\n    for i in range(24): \r\n        c_encoded.append(tf.io.parse_tensor(parsed_example['c_encoded'][i], out_type=tf.float32))\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I am aware that I could loop over the batch and parse each tensor individually. I was hoping that I wouldn't need to do that and could simply call `tf.io.parse_tensor` on a batch of tensors. It seems weird to me that you can parse a batch of examples with `tf.io.parse_example` but when it comes to parsing these tensors you need to loop over each tensor because it can't handle a batch.\r\n\r\nIs there a good reason why `tf.io.parse_example` is vectorized but not `tf.io.parse_tensor` ?", "@Jephthia I agree, that was exactly why I opened the issue - seems like a missed opportunity here. Currently, my models are not IO bottlenecked because the individual samples are quite big, but I would be really happy to see this resolved in future versions at some point. ", "@mhorlacher , could you re-open this issue? This deserves proper attention. This issue prevents users from decoding tensors in batched tf.data pipelines", "To summarize the request, please include a vectorized version of tf.io.parse_tensor", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I agree with @sjang92 - this is still very much needed, specially for shallow models where IO is more of a bottleneck. ", "I am also interested in a vectorized version of tf.io.parse_tensor", "I have same issue. I also hope verctorzied version of tf.io.parse_tensor."]}, {"number": 43699, "title": "Multiple instances of custom metrics don't behave as expected with multiple output model when evaluating.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution: **Ubuntu 20.04.1 LTS**\r\n- TensorFlow installed from (source or binary): `pip install`\r\n- TensorFlow version (use command below): `tensorflow==2.3.0`\r\n- Python version: **Python 3.6.9 :: Anaconda, Inc.**\r\n\r\n**Current behavior**\r\nCustom metrics behave unexpectedly when applied on a model / dataset with multpiple outputs.\r\nBelow is a snippet to reproduce.\r\n\r\n**Describe the expected behavior**\r\nResults should be consistent whatever the number of outputs.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef build_dataset(n_outputs):\r\n    \"\"\"\r\n    Build a random dataset.\r\n    Target is a dictionnary of `n_output` elements with keys as [\"out0\", \"out1\", ...]\r\n    \"\"\"\r\n    input_data = np.random.normal(size=(1024, 5)).astype(\"float32\")\r\n    target_data = {\r\n        f\"out{n}\": np.random.normal(size=(1024,)).astype(\"float32\")\r\n        for n in range(n_outputs)\r\n    }\r\n    input_dataset = tf.data.Dataset.from_tensor_slices(input_data)\r\n    target_dataset = tf.data.Dataset.from_tensor_slices(target_data)\r\n    dataset = tf.data.Dataset.zip((input_dataset, target_dataset)).batch(16)\r\n    return dataset\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    \"\"\"Model with `n_outputs` outputs\"\"\"\r\n    def __init__(self, n_outputs):\r\n        tf.keras.Model.__init__(self)\r\n        self.hidden = tf.keras.layers.Dense(16)\r\n        self.out = {f\"out{n}\": tf.keras.layers.Dense(1) for n in range(n_outputs)}\r\n\r\n    def call(self, inputs):\r\n        hidden = self.hidden(inputs)\r\n        return {out_name: out_layer(hidden) for out_name, out_layer in self.out.items()}\r\n\r\n\r\nclass DumbMetric(tf.keras.metrics.Metric):\r\n    \"\"\"This metrics takes a param at __init__ time and then always return param as result\"\"\"\r\n    def __init__(self, name=\"Dumb\", param=0., **kwargs):\r\n        tf.keras.metrics.Metric.__init__(self, name=name, **kwargs)\r\n        self.param = float(param)\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight):\r\n        pass\r\n\r\n    def result(self):\r\n        return self.param\r\n\r\n\r\nfor n in (1, 2):\r\n    # For 1 output, metrics behave as expected:\r\n    #   - metric1 returns 1\r\n    #   - metric2 returns 2\r\n    #\r\n    # For 2 outputs, metrics don't behave as expected:\r\n    #   - metric1 returns 0 (default `param` value)\r\n    #   - metric2 returns 0 (default `param` value)\r\n\r\n    model = Model(n)\r\n    model.compile(\r\n        metrics=[\r\n            DumbMetric(name=\"metric1\", param=1.),\r\n            DumbMetric(name=\"metric2\", param=2.)\r\n        ]\r\n    )\r\n    metrics = model.evaluate(build_dataset(n), return_dict=True, verbose=0)\r\n    print(f\"For {n} output(s): \", metrics)\r\n```\r\n\r\n**Actual output:**\r\n```\r\nFor 1 output(s):  {'loss': 0.0, 'metric1': 1.0, 'metric2': 2.0}\r\nFor 2 output(s):  {'loss': 0.0, 'out0_metric1': 0.0, 'out0_metric2': 0.0, 'out1_metric1': 0.0, 'out1_metric2': 0.0}\r\n```\r\n\r\n**Excpected output:**\r\n```\r\nFor 1 output(s):  {'loss': 0.0, 'metric1': 1.0, 'metric2': 2.0}\r\nFor 2 output(s):  {'loss': 0.0, 'out0_metric1': 1.0, 'out0_metric2': 2.0, 'out1_metric1': 1.0, 'out1_metric2': 2.0}\r\n```", "comments": ["Hi again,\r\n\r\nAfter further investigation, overriding the following method in `DumbMetric` fixes the issue:\r\n\r\n```python\r\n    def get_config(self):\r\n        return {\r\n            **tf.keras.metrics.Metric.get_config(self),\r\n            \"param\": self.param\r\n        }\r\n```\r\n\r\nThis is due because metric objects are being copied [through this method](https://github.com/tensorflow/tensorflow/blob/7ea86e9de8a5b6426e7291e0e5477ddaee83ba88/tensorflow/python/keras/engine/compile_utils.py#L487-L489) when multiple outputs are [out there](https://github.com/tensorflow/tensorflow/blob/7ea86e9de8a5b6426e7291e0e5477ddaee83ba88/tensorflow/python/keras/engine/compile_utils.py#L92-L94).\r\n\r\nIt wasn't clear to me this should be done when subclassing `tf.keras.metrics.Metric`. I may have missed some hint in the documentation. Though, wouldn't it be nice to pass `__init__` `kwargs` by default to `tf.keras.metrics.Metric.get_config` in case user defines custom parameters that should be passed again when \"copiying\" metrics ?\r\n\r\nThanks in advance.", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ed1d48fcc45a609c9bd0512ab39d31b3/43699.ipynb). Thanks!", "Having `tf.keras.metrics.Metric.get_config` as an `abc.abstractmethod` with an explicit mention in the docstring / documentation is the simplest \"fix\" I can think of.\r\n\r\nWhat do you think ?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210528, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/aa03a1cdcaa690e7ef609624433dba81/43699.ipynb). Thanks!"]}, {"number": 43697, "title": "More details and examples for tf.image.generate_bounding_box_proposals", "body": "\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/generate_bounding_box_proposals\r\n\r\n## Description of the issue (what needs changing):\r\nThe documentation for this function isn't clear. The meanings of the function's arguments aren't given. There are no usage examples. Seeing the code linked in the documentation doesn't make anything clearer. \r\n\r\n### Clear description\r\nThis function can be used to greatly simplify the making of object detection models, as it implements the proposal generation part of the code.\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\nYes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\nNo, the parameter definitions aren't available\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\nReturns are defined\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nNo, the errors raised are not defined\r\n\r\n\r\n### Usage example\r\n\r\nNo\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\nThere are no visuals available now. However, it will be helpful to add image examples as this is a function from the image module. The bounding box proposals generated by the function can be verified easily with images. \r\n\r\n### Submit a pull request?\r\nI would be happy to fix this issue, but I would need more information about the parameters used. I am unsure what they mean.\r\n\r\nAre you planning to also submit a pull request to fix the issue See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["I will take this issue by adding all the required arguments and their definitions, I will need to change the [file](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/ops/image_ops_impl.py#L5001-L5027) posted here with the docstrings I suppose.\r\n\r\nI checked the docs repo but  I believe api docs are generated from docstrings so I will need to modify the changes accordingly in the source file mentioned here itself.\r\n\r\n@ymodak Let me know if I am misinterpreting it. Thanks.", "@lamberta  I followed the first instruction regarding change in the docstring only instructions given [here](https://www.tensorflow.org/community/contribute/docs_ref#test_on_your_local_machine) yet I am still facing the issue\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tensorflow/tools/docs/tf_doctest.py\", line 32, in <module>\r\n    from tensorflow.tools.docs import tf_doctest_lib\r\nImportError: cannot import name 'tf_doctest_lib' from 'tensorflow.tools.docs' (/usr/local/lib64/python3.8/site-packages/tensorflow/tools/docs/__init__.py)\r\n```\r\n\r\nI checked the directory it does not indeed contain the file : \r\n![Screenshot from 2020-10-03 21-22-29](https://user-images.githubusercontent.com/23502062/94995848-95cb8400-05be-11eb-80a8-040ed77110b5.png)\r\n\r\n\r\nThe Tensorflow version seems to be the nightly version too.\r\n\r\nI have also installed tf_nightly over the already installed tensorflow. How do I solve this?\r\n#41279 is nearly the same issue it appears so.", "```\r\n[this@that ~]$ python tf_doctest.py /home/that/Github/tensorflow/tensorflow/python/ops/image_ops_impl.py \r\n2020-10-03 22:35:40.483100: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-10-03 22:35:40.483230: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n/usr/local/lib/python3.8/site-packages/absl/flags/_validators.py:407: UserWarning: Flag --module has a non-None default value. That does not make sense with mark_flags_as_mutual_exclusive, which checks whether the listed flags have a value other than None.\r\n  warnings.warn(\r\n2020-10-03 22:36:01.245212: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n2020-10-03 22:36:01.245355: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n2020-10-03 22:36:01.245374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:31] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nRunning tests under Python 3.8.5: /usr/bin/python\r\n[ RUN      ] unittest.loader._FailedTest.Github\r\n[  FAILED  ] unittest.loader._FailedTest.Github\r\n======================================================================\r\nERROR: Github (unittest.loader._FailedTest)\r\n----------------------------------------------------------------------\r\nAttributeError: module '__main__' has no attribute 'Github'\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.000s\r\n\r\nFAILED (errors=1)\r\n\r\n\r\n\r\n[this@that ~]$ python tf_doctest.py --file /home/that/Github/tensorflow/tensorflow/python/ops/image_ops_impl.py \r\n2020-10-03 22:38:06.502343: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-10-03 22:38:06.502381: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n/usr/local/lib/python3.8/site-packages/absl/flags/_validators.py:407: UserWarning: Flag --module has a non-None default value. That does not make sense with mark_flags_as_mutual_exclusive, which checks whether the listed flags have a value other than None.\r\n  warnings.warn(\r\n2020-10-03 22:38:08.391603: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n2020-10-03 22:38:08.391731: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n2020-10-03 22:38:08.391751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:31] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nRunning tests under Python 3.8.5: /usr/bin/python\r\nFATAL Flags parsing error: flags module=[], file=/home/that/Github/tensorflow/tensorflow/python/ops/image_ops_impl.py: At most one of (module, file) must have a value other than None.\r\nPass --helpshort or --helpfull to see help on flags.\r\n[aphadke@DESKTOP-TP18Q4P ~]$ \r\n\r\n\r\n\r\n```\r\n\r\nI am getting two different results .", "@NitishaS-812k\r\nIs this still an issue", "> @NitishaS-812k\r\n> Is this still an issue\r\n\r\nsome usage examples would be great too, other functions in tf.image have a lot of useful examples."]}, {"number": 43696, "title": "Remove CMSIS Core include from all Makefiles", "body": "Once PR #41860 is merged, the CMSIS include path is added in cmsis.inc. Specifying the include path in all Makefiles is no longer needed. This is a reminder to remove lines like\r\n  INCLUDES +=  isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ \r\n\r\nSee discussion in the original post:\r\n\r\nOriginally posted by @advaitjain in https://github.com/tensorflow/tensorflow/pull/41860#discussion_r497863693", "comments": []}, {"number": 43693, "title": "Add Ubuntu 20.04 installation instructions", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101\r\n\r\n## Description of issue (what needs changing):\r\n\r\nTensorflow installation instructions for Ubuntu 20.04 with GPU support is needed\r\n\r\n### Clear description\r\n\r\nWith Ubuntu 20.04 around for 6 months, please provide installation instructions for Tensorflow with GPU support. It makes sense to have new development environments set up on Ubuntu 20.04\r\n\r\n", "comments": ["What is missing from https://www.tensorflow.org/install/pip ?", "GPU support seems to be specific for Ubuntu 18.04 and Ubuntu 16.04 as seen on https://www.tensorflow.org/install/gpu#install_cuda_with_apt", "@lamberta / @yashk2810 can we add the Ubuntu 20.04 instructions? Or they don't differ significantly from the existing ones to need additional edits?", "> they don't differ significantly from the existing ones to need additional edits?\r\n\r\nI have no idea. Someone needs to try those instructions out on a 20.04 VM and then edit the page.", "Currently the [install](https://www.tensorflow.org/install) page lists the requirements as \"Ubuntu 16.04 or later\" so should make this part f the official support.\r\n\r\ncc: @sanjoy @pkanwar23 for GPU support\r\n", "Under the [Nvidia CUDA Toolkit 10.1 Archive](https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu) page there is no official supported version for Ubuntu 20.04.\r\nWith the release of the new RTX GPUs and soon TF 2.4 I think it would be better to update the [GPU support](https://www.tensorflow.org/install/gpu) page to include instructions for installation of CUDA 11.1 for Ubuntu 20.04. #45308 currently is in the process of addressing this issue.\r\n\r\nI think instead of providing the NVIDIA package installation instructions on the TensorFlow GPU support page, we should redirect users to the appropriate Nvidia CUDA Toolkit installation page.\r\n\r\n\r\n\r\n", "@pmeckoni Could you please let us know if this is still an issue and  Please have  a look at the similar[ issue](https://github.com/tensorflow/tensorflow/issues/45308) ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Under the [Nvidia CUDA Toolkit 10.1 Archive](https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu) page there is no official supported version for Ubuntu 20.04. With the release of the new RTX GPUs and soon TF 2.4 I think it would be better to update the [GPU support](https://www.tensorflow.org/install/gpu) page to include instructions for installation of CUDA 11.1 for Ubuntu 20.04. #45308 currently is in the process of addressing this issue.\r\n\r\nCUDA is officially supported for Ubuntu 20.04 here https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/\r\n\r\nI think the packages in apt are named similarly but idk if named exactly the same\r\n\r\n", "@pmeckoni Could you please refer to the above [comment](https://github.com/tensorflow/tensorflow/issues/43693#issuecomment-964606622) and let us know if it helps?Thanks!", "No response. I have moved on and no longer have access to the computer which had nvidia. So there is no way I can use this now.", "I'm getting a Document Not Found error when trying to train a gradient boosted machine. It says the URI does not point to a valid page. Any help?"]}, {"number": 43688, "title": "[TransformGraph & TF2 SavedModel] - Missing API", "body": "### Issue\r\n\r\nWhen using the latest TF2 release and latest available container (`2.3.1` at the time of writing), it appears that the following API is missing:\r\n\r\n```python\r\nfrom tensorflow.tools.graph_transforms import TransformGraph  # ModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\nfrom tensorflow.compat.v1.tools.graph_transforms import TransformGraph  # ModuleNotFoundError: No module named 'tensorflow.compat.v1.tools'\r\n```\r\n\r\nWhich used to be available in TF1 (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms).\r\n\r\nThough it doesn't make much of a sense since Tensorflow 2 ship with the C++ library associated:\r\n```bash\r\nls -al /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_transform_graph.so \r\n-rwxr-xr-x 1 root staff 1274144 Sep 29 00:25 /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_transform_graph.so\r\n```\r\n\r\n### Motivation\r\n\r\nThis function is very useful to pre-process and \"clean\" the graph before inference when using a SavedModel (namely for TF-TRT, could also maybe concerns TF Serving or TF Lite, I didn't check).\r\n\r\nAny chance to have this API (or a replacement) back in the TF API ? I understand that TF2 is eager mode oriented, though the SavedModel format is a graph format ... So, would makes sense to keep this API in the mix since the SavedModel format is still actively used and supported in TF2.\r\n\r\nCC: @sanjoy @reedwm ", "comments": ["@av8ramit do you know why the `TransformGraph` class is not in the pip package anymore? The class is defined [here](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/graph_transforms/__init__.py;l=26;drc=00983e6b0700f9a34e0df980669172b6ea85dad0).", "Thanks for filing the issue @DEKHTIARJonathan. Taking a look and will get back to you.", "So I believe that API was removed from the 2.0 pip package. It appears the recommended alternative is to [build it from source](https://github.com/tensorflow/tensorflow/issues/30865#issuecomment-662528252) using bazel. @annarev do you perhaps have more context into alternatives or why it was removed from the 2.0 APIs?", "@av8ramit  Building from source might be an issue. In principle, I have nothing against it. However in this case we would it in one of the core APIs of Tensorflow.\r\n\r\nWe would like to use it in the TF-TRT APIs (`tensorflow/python/compiler/tensorrt`), so we would need to have this available by default in Tensorflow2.\r\n\r\nAnd we can't really ask each TF-TRT user to build TF from source ...\r\n\r\n-----------\r\n\r\nBtw. Kuddos to the team who implemented this API, very nicely put together ;) ", "Graph transforms were removed and meant to be replaced by Grappler according to this RFC:\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n\r\n@petewarden, @rmlarsen do you know if there is an alternative for this usecase (pre-process graph before inference with SavedModel)?", "Sorry @DEKHTIARJonathan I am not the best person to answer this (I was looped in in the event this was a pybind export issue), but I'll leave it to Pete and Rasmus to hopefully shed some more light. ", "@annarev let me give you a crude of what I could be interested in doing if that could help:\r\n\r\n```python\r\ntransforms = [\r\n        \"remove_nodes(op=Identity, op=CheckNumerics, op=StopGradient, op=Placeholder)\",\r\n        \"sort_by_execution_order\",\r\n        \"remove_attribute(attribute_name=_XlaSeparateCompiledGradients)\",\r\n        \"remove_attribute(attribute_name=_XlaCompile)\",\r\n        \"remove_attribute(attribute_name=_XlaScope)\",\r\n        \"sort_by_execution_order\",\r\n        'merge_duplicate_nodes',\r\n        \"sort_by_execution_order\",\r\n        'strip_unused_nodes',\r\n        \"sort_by_execution_order\",\r\n        'fold_constants(ignore_errors=true)',\r\n        \"sort_by_execution_order\",\r\n        'fold_batch_norms',\r\n        \"sort_by_execution_order\",\r\n    ]\r\n\r\n    print(\"Applying Graph Transformations ...\")\r\n    return TransformGraph(\r\n        graph_def,\r\n        input_node_names,  # an array of the input node(s)\r\n        output_node_names,  # an array of output nodes\r\n        transforms\r\n    )\r\n```\r\n\r\nThe order of execution is important. Is there any way to order grappler optimization passes in case I find a way to use the grappler ?", "then how to use the transforms in grappler optimization?", "Has any progress been made on this? Anybody find a fix?\r\n", "@av8ramit @reedwm any update ?", "@rohan100jain, should these files be added back in the pip package?", "@DEKHTIARJonathan I no longer work on this project, unfortunately. Good luck!"]}, {"number": 43687, "title": "XLA Compilation does not work with Embeddings Layer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Default on colab\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla P4 (8gb)\r\n\r\n\r\n**Describe the current behavior**\r\nI'm using a simple model with Embeddings which works with normal `tf.function` but does not work with `tf.function(experimental_compile=True)`. I suspect it is because Embeddings are kept on the CPU while the other model layers are on the GPU, however, moving the tensors explicitly to the right device did not help (as demonstrated in the script).\r\n\r\n**Describe the expected behavior**\r\nThe function should run without errors just like with a normal `tf.function`.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1dDjBiks335If3zqBleL_CPrNZ8CIanPP?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@n2cholas \r\n\r\nI have tried in TF-nightly version (2.4.0-dev20200930) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8dc30c27c6cc9afe663c07be997868e8/untitled406.ipynb#scrollTo=l0HX6JYYzAVW). Thanks!", "@ravikyram Thanks! I was able to get it running on colab, but I still get the error on my local machine even with tf-nightly. Here are the specifications:\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip (so binary)\r\n- TensorFlow version (use command below): 2.4.0-dev20200930\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Titan RTX (24gb)", "@n2cholas \r\n\r\nCan you please share the error log.Thanks!", "@ravikyram here's all the output from running it:\r\n\r\n```\r\n2020-10-05 13:56:02.579136: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-05 13:56:07.217818: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-05 13:56:07.218509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-05 13:56:07.284812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:17:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-10-05 13:56:07.284838: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-05 13:56:07.286588: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-10-05 13:56:07.287851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-05 13:56:07.288096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-05 13:56:07.289287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-05 13:56:07.289664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-05 13:56:07.289776: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-05 13:56:07.290790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-10-05 13:56:07.291078: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-05 13:56:07.291830: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-05 13:56:07.293287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:17:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.78GiB deviceMemoryBandwidth: 607.97GiB/s\r\n2020-10-05 13:56:07.293321: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-05 13:56:07.293346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-10-05 13:56:07.293363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-05 13:56:07.293378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-05 13:56:07.293399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-05 13:56:07.293416: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-05 13:56:07.293434: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-05 13:56:07.299268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-10-05 13:56:07.299335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-05 13:56:07.749866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-05 13:56:07.749896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-10-05 13:56:07.749901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-10-05 13:56:07.751331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10910 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:17:00.0, compute capability: 7.0)\r\nWARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\n2020-10-05 13:56:11.378588: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1be075d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-05 13:56:11.378619: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN V, Compute Capability 7.0\r\n2020-10-05 13:56:11.927927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-10-05 13:56:20.457126: I tensorflow/compiler/jit/xla_compilation_cache.cc:325] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n2020-10-05 13:56:21.618453: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_ops.cc:231 : Invalid argument: Trying to access resource _AnonymousVar10 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 83, in <module>\r\n    forward(train_data[:batch_size], train_labels[:batch_size])\r\n  File \"/local/nvadivelu/jax-paper/tmp/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 795, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/local/nvadivelu/jax-paper/tmp/env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 822, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/local/nvadivelu/jax-paper/tmp/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2947, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/local/nvadivelu/jax-paper/tmp/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1927, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/local/nvadivelu/jax-paper/tmp/env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 556, in call\r\n    outputs = execute.execute(\r\n  File \"/local/nvadivelu/jax-paper/tmp/env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar10 located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_forward_16492]\r\n```\r\n\r\nHere is the output of `pip freeze`:\r\n```\r\nabsl-py==0.10.0\r\nastunparse==1.6.3\r\ncachetools==4.1.1\r\ncertifi==2020.6.20\r\nchardet==3.0.4\r\nflatbuffers==1.12\r\ngast==0.3.3\r\ngoogle-auth==1.22.1\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.32.0\r\nh5py==2.10.0\r\nidna==2.10\r\nKeras==2.4.3\r\nKeras-Preprocessing==1.1.2\r\nMarkdown==3.2.2\r\nnumpy==1.18.5\r\noauthlib==3.1.0\r\nopt-einsum==3.3.0\r\nprotobuf==3.13.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\nPyYAML==5.3.1\r\nrequests==2.24.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.6\r\nscipy==1.5.2\r\nsix==1.15.0\r\ntb-nightly==2.4.0a20201005\r\ntensorboard-plugin-wit==1.7.0\r\ntermcolor==1.1.0\r\ntf-estimator-nightly==2.4.0.dev2020100501\r\ntf-nightly-gpu==2.4.0.dev20201005\r\ntyping-extensions==3.7.4.3\r\nurllib3==1.25.10\r\nWerkzeug==1.0.1\r\nwrapt==1.12.1\r\n```", "@n2cholas \r\n It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "@Saduf2019 the issue still persists with 2.5.0. With 2.6.0, the code works there is a different issue.  Consider the following snippet (directly from the colab):\r\n\r\n```py\r\ndef to_cpu(x):\r\n    with tf.device('/CPU:0'): return tf.identity(x)\r\n \r\ndef to_gpu(x):\r\n    with tf.device('/GPU:0'): return tf.identity(x)\r\n \r\nx = tf.convert_to_tensor([1, 2, 3])\r\nprint(x.device)\r\nprint(to_gpu(x).device)\r\nprint(to_cpu(to_gpu(x)).device)\r\n```\r\nThis prints the following in 2.5.0 (which is correct):\r\n```\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\nBut in 2.6.0, it prints:\r\n```\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\nThe second line should be `GPU`. Why is this happening? is this a bug?", "I met the same problem, did you know how to solve it now? "]}, {"number": 43667, "title": "tf.keras.layers.experimental.preprocessing.CategoryEncoding: is it possible to apply it to a nD tensor?", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**: currently, tf.keras.layers.experimental.preprocessing.CategoryEncoding layers can only be called with a 2D tensor of shape (samples, \"timesteps\"). It would be very useful to allow calling with a nD tensor, for example when working with sequences.. As for \"Normalization\" Layer, an \"axis\" parameter could be added to the layer parameters.\r\nFor instance, I would like to use a CategoryEncoding Layer on sequence-input data with the following shape (samples, timesteps, nb_features).\r\nWhen nb_features= 1 Application of CategoryEncoding Layer to such tensor would return a tensor of shape  (samples, timesteps, nb_classes_in_the_first_feature).\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wishes to apply Categorical preprocessing such as one-hot encoding to categorical features of sequence / Timeserie data\r\n\r\n**Any Other info.**\r\n", "comments": ["Contributions welcome!", "Can I work on this? Or maybe @scd75 is already working on it?", "Hi, I was planning to take a look but did not start yet, so please go ahead.\r\nPlus I am not a coding expert per se, so I guess you would produce a better result!", "Is there anyone still working on this feature? i would like to take on the challange.", "Sounds good", "Is this already implemented? There are an explaination about multi-hot encoding with this API on this [docs ](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding?version=nightly). Maybe someone already working on this.", "Hey can I work on this seems interesting\r\n@scd75 can you post any sample data in the type you mentioned", "Hello!\nI am newbie to open source contribution and really interested to contribute to this project!! . But I have very very less knowledge about it . Can anyone help me please?", "@hfahrudin @scd75 \r\nI think the documentation is talking different things. Also the documentation is out of date.\r\nDocument says:\r\n```\r\n>>>layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=4)\r\n>>>layer([[0, 1], [0, 0], [1, 2], [3, 1]])\r\n\r\n<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\r\narray([[1., 1., 0., 0.],\r\n       [1., 0., 0., 0.],\r\n       [0., 1., 1., 0.],\r\n       [0., 1., 0., 1.]], dtype=float32)>\r\n```\r\n\r\nIssues want\r\n```\r\n>>>layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=2, axis = 2)\r\n>>>new_data = np.array([[[1], [3]],\r\n                     [[2], [3]],\r\n                     [[1], [2]]])\r\n>>>print(new_data.shape)\r\n(3, 2, 1)\r\n>>>print(layer(new_data))\r\n<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\r\narray([[[1., 0.], [1., 1.]],\r\n           [[0., 1.], [1., 1.]],\r\n           [[1., 0.], [0., 1.]]], dtype=float32)>\r\n```\r\nBut now it gave\r\n\r\n```\r\nInvalidArgumentError: Shape must be at most rank 2 but is rank 3 [Op:DenseBincount]\r\n```\r\n\r\nSo If I understand it correctly. I could contribute on this question. But need to check with your current progress. Let me know if there is a green light."]}, {"number": 43665, "title": "IteratorGetNext: unsupported op: No registered 'IteratorGetNext' OpKernel for XLA_GPU_JIT devices compatible with node {{node IteratorGetNext}} \t.  Registered:  device='XLA_GPU'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n   ```\r\n    #Related info to xs:\r\n    names = [name for name in os.listdir(cdir + xname) if name.endswith('.png')]\r\n    xs = [np.array(Image.open(cdir + sat + name).convert('RGB'),dtype=\"float32\") for name in names]\r\n    xs = [x/255.0 for x in xs]\r\n    xs = np.asarray(xs)\r\n\r\n    with tf.device(\"/device:XLA_GPU:0\"):\r\n    history = model.fit(xs,\r\n                    ys,\r\n                    validation_split = 0.1,\r\n                    epochs=EPOCHS,\r\n                    batch_size = BATCH_SIZE,\r\n                       )\r\n    ```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  ```\r\n  Distributor ID:\tUbuntu\r\n  Description:\tUbuntu 18.04.5 LTS\r\n  Release:\t18.04\r\n  Codename:\tbionic\r\n  ```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n   No\r\n- TensorFlow installed from (source or binary):\r\n   ```\r\n    binary \r\n    pip3.6 install tensorflow==2.1.0 tensorflow-gpu==2.1.0\r\n    pip3.7 install tensorflow==2.1.0 tensorflow-gpu==2.1.0\r\n   ```\r\n- TensorFlow version (use command below):\r\n   ```\r\n   2.1.0\r\n   ```\r\n- Python version:\r\n   ```\r\n   import platform\r\n   import sys\r\n   print(platform.python_version()) #jupyter notebook\r\n   print(sys.version)\r\n   print(sys.version_info)\r\n   3.6.9\r\n   3.6.9 (default, Jul 17 2020, 12:50:27) \r\n   [GCC 8.4.0]\r\n   sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\r\n   ```\r\n- Bazel version (if compiling from source):\r\n   No\r\n- GCC/Compiler version (if compiling from source):\r\n    No\r\n- CUDA/cuDNN version:\r\n   ```\r\n   nvcc --version\r\n   nvcc: NVIDIA (R) Cuda compiler driver\r\n   Copyright (c) 2005-2019 NVIDIA Corporation\r\n   Built on Sun_Jul_28_19:07:16_PDT_2019\r\n   Cuda compilation tools, release 10.1, V10.1.243\r\n   ```\r\n- GPU model and memory:\r\n```\r\nnvidia-smi\r\nWed Sep 30 13:07:10 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce 920M        On   | 00000000:03:00.0 N/A |                  N/A |\r\n| N/A   43C    P8    N/A /  N/A |    118MiB /  2004MiB |     N/A      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nAfter following all steps from here - https://www.tensorflow.org/install/gpu and installing all these GPU doesn't work\r\n```\r\n# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\nsudo apt-get install --no-install-recommends nvidia-driver-450\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-1 \\\r\n    libcudnn7=7.6.5.32-1+cuda10.1  \\\r\n    libcudnn7-dev=7.6.5.32-1+cuda10.1\r\n\r\n\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\nsudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\r\n    libnvinfer-dev=6.0.1-1+cuda10.1 \\\r\n    libnvinfer-plugin6=6.0.1-1+cuda10.1\r\n\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nShould be able to train on GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\n\r\n```\r\nlocal_device_protos = device_lib.list_local_devices()\r\nfor x in local_device_protos:\r\n    print(x)\r\n    \r\n#print(tf.test.is_gpu_available)\r\nname: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 5085277874134328407\r\n\r\nname: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 5803642393619994138\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n\r\nname: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 6332427526805909660\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n```\r\n```\r\nwith tf.device(\"/device:XLA_GPU:0\"):\r\n    history = model.fit(xs,\r\n                    ys,\r\n                    validation_split = 0.1,\r\n                    epochs=EPOCHS,\r\n                    batch_size = BATCH_SIZE,\r\n                       )\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTrain on 921 samples, validate on 103 samples\r\nEpoch 1/10\r\n 16/921 [..............................] - ETA: 4:43\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-18-40038c56b2dd> in <module>()\r\n      4                     validation_split = 0.1,\r\n      5                     epochs=EPOCHS,\r\n----> 6                     batch_size = BATCH_SIZE,\r\n      7                        )\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/home/maifee/.local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Function invoked by the following node is not compilable: name: \"__inference_distributed_function_7577\" op: \"__inference_distributed_function_7577\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" attr { key: \"_XlaCompile\" value { b: true } } attr { key: \"config_proto\" value { s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\" } } attr { key: \"executor_type\" value { s: \"\" } }.\r\nUncompilable nodes:\r\n\tIteratorGetNext: unsupported op: No registered 'IteratorGetNext' OpKernel for XLA_GPU_JIT devices compatible with node {{node IteratorGetNext}}\r\n\t.  Registered:  device='XLA_GPU'\r\n  device='XLA_CPU'\r\n  device='GPU'\r\n  device='CPU'\r\n\r\n\tStacktrace:\r\n\t\tNode: __inference_distributed_function_7577, function: \r\n\t\tNode: IteratorGetNext, function: __inference_distributed_function_7577\r\n [Op:__inference_distributed_function_7577]\r\n\r\n\r\n```", "comments": ["Info related `xs` and `ys` are :\r\n```\r\nnames = [name for name in os.listdir(cdir + xname) if name.endswith('.png')]\r\nxs = [np.array(Image.open(cdir + sat + name).convert('RGB'),dtype=\"float32\") for name in names]\r\nxs = [x/255.0 for x in xs]\r\nxs = np.asarray(xs)\r\n# Same for ys\r\n```", "@maifeeulasad,\r\nCould you please run the below commands and share the output with us\r\n```\r\ncat /usr/local/cuda/version.txt\r\n\r\ncat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n```\r\n\r\nAlso, please upgrade TensorFlow to v2.3 and make sure you have all the compatible dependencies as per the [tested build configurations](https://www.tensorflow.org/install/source#gpu). Thanks!", "@amahendrakar , here is the outcome of those two command : \r\n```\r\nmaifee@maifee-ubuntu:~$ cat /usr/local/cuda/version.txt\r\ncat: /usr/local/cuda/version.txt: No such file or directory\r\nmaifee@maifee-ubuntu:~$ cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 5\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n\r\n#include \"driver_types.h\"\r\nmaifee@maifee-ubuntu:~$ \r\n```", "@amahendrakar But this returns some output about cuda version : \r\n```\r\nmaifee@maifee-ubuntu:~$ cat /usr/local/cuda-10.1/version.txt\r\nCUDA Version 10.1.243\r\n```", "@amahendrakar and I have updated to tf2.3 and tf-gpu2.3 too\r\n\r\nsame error in both version", "I am having the same error message, but on the CPU. Has this issue been resolved or a work around been found?"]}, {"number": 43655, "title": "A new feature in tf.roll", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\nTensorflow 2.1.0\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, tensorflow only allows for same shift in the same dimension. I want different shift ability\r\n\r\n[[1,2,3,4],\r\n[1,2,3,4],\r\n[1,2,3,4]] \r\ncan be only turned into \r\n[[2,3,4,1],\r\n[2,3,4,1],\r\n[2,3,4,1]] in the current iteration. \r\nI need a roll function that can get this without using any for loop, I also need it for any dimension tensor , i.e. 4 dimension that I work on. \r\n[[2,3,4,1],\r\n[3,4,1,2],\r\n[4,1,2,3]]\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes by adding a property. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople who work on system biology using tensorflow for transcription data. A great deal of this will be used in this paper which I authored. I want to expand it to make it more flexible. \r\n\r\nhttps://academic.oup.com/bioinformatics/article/36/Supplement_1/i499/5870526?itm_content=bioinformatics&itm_source=trendmd-widget&itm_campaign=trendmd-pilot&itm_medium=sidebar\r\n\r\n**Any Other info.**\r\nLast time, it was closed using a replace method but it only solve the example problem I give, not the full problem. ", "comments": ["@yiliu9090,\r\n[tf.roll](https://www.tensorflow.org/api_docs/python/tf/roll) now provides a facility to specify **`Multiple Axes`**. Can you PTAL? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry about the delay. \r\nThis is not what I really wanted for tf.roll. What I mean is that if there are multiple \r\nCurrently, tensorflow only allows for same shift in the same dimension. I want different shift for the same axis just different roll. \r\nFor Example \r\n[[1,2,3,4],\r\n[1,2,3,4],\r\n[1,2,3,4]]\r\ncan be only turned into\r\n[[2,3,4,1],\r\n[2,3,4,1],\r\n[2,3,4,1]] in the current iteration.\r\nI need a roll function that can get this without using any for loop, I also need it for any dimension tensor , i.e. 4 dimension that I work on.\r\n[[2,3,4,1],\r\n[3,4,1,2],\r\n[4,1,2,3]]\r\nHere note that shift all happens at dimension 1, but the shift is 1,2,3 for different rows.  \r\nSo this works a little differently from the what I initial thought. \r\n\r\nWhat you have is still same axis same level, so it is impossible to do this. (If I am wrong please tell me how to do this in current iteration)\r\n\r\nThanks so much. ", "I could use this feature too, and I am happy to contribute to it.\r\n\r\nGiven a tensor of shape (n, y, x) \r\n\r\nMy current solution is to break the matrix into a list:\r\n`list = [list[item] for item in range(n)]`\r\n\r\nAnd then use another loop to roll each list tensor:\r\n`for iter in range(len(list)):` \r\n`    xShift = (iter % xShift)`\r\n`    yShift = int(iter / yShift)`\r\n`    list[iter] = tf.roll(list[iter], shift= [yShift, xShift], axis= [0, 1])`\r\n\r\nThe list can then be converted back to a tensor using:\r\n`list = tf.convert_to_tensor(list)`\r\n\r\nThis is inefficient, but it works for my use case. My tensor is actually 6-dimensional, and rolling this way was much more efficient\r\nthan rolling individually.\r\n\r\nEDIT: I am/was not aware of how to write a correct comment on git.\r\n\r\nTensorflow version 2.5"]}, {"number": 43650, "title": "Custom loss function is not working", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.3\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ\r\n\r\n**Describe the behavior**\r\nI am implementing the PPO algorithm using Keras but encountered the following issue related to the custom loss function in Keras.\r\n\r\n**Error message:**\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  u/tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: old_prediction_input:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 426, in <module>\r\n    agent.train()\r\n  File \"train.py\", line 370, in train\r\n    actor_loss = self.actor.fit(\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 840, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1843, in _filtered_call\r\n    return self._call_flat(\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1923, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 545, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\dhyey\\Desktop\\Train-ml-agents\\python-envs\\offline_training\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 72, in quick_execute\r\n    raise core._SymbolicException(\r\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'old_prediction_input:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'advantage_input:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'reward_input:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'value_input:0' shape=(None, 1) dtype=float32>]\r\n```\r\n\r\n**Check the custom loss function [here on Colab](https://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ#scrollTo=DIoqurd3uDb1)**\r\n\r\n**Check the actor model [here on Colab](https://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ#scrollTo=rTrPW6F50TbY)**\r\n\r\nSo after searching I found one work around i.e to add `run_eagerly=True` to the model.compile() method as: `actor_model.compile(... , run_eagerly=True)`\r\n\r\nBut after applying run_eagerly to true, I am getting 0 loss value from `actor.history['loss']` and to debug this I am not able to print the total_loss value in the ppo_loss(...) function because it gives the `AttributeError: 'Tensor' object has no attribute 'numpy'`.\r\n\r\n", "comments": ["@Dhyeythumar \r\n\r\nI have tried in colab with TF version 2.3 and i am seeing different error message( `AttributeError: 'Tensor' object has no attribute 'numpy`').Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/096aceb3b1dce841a22508c3691af553/untitled404.ipynb). Thanks!", "Yes, I have also mentioned this in the above issue. So when I try to get the value from a tensor in ppo_loss() function I am getting `AttributeError: 'Tensor' object has no attribute 'numpy'` error. Basically, I am doing this to check why I am getting 0 in that ppo_loss function. \r\n\r\nFor now, I have updated the code on colab so we won't get  `AttributeError: 'Tensor' object has no attribute 'numpy'`.", "Hi @Dhyeythumar, this is a lot of custom code to look through. But often a cause of this error message is trying to use numpy ops in graph mode, or relying on python side effects (like appending to lists) instead of using TensorArrays in graph mode. I am not very familiar with gym so I can't point to the exact issue in your code. However, I would suggest taking a look at the [tf.function guide](https://www.tensorflow.org/guide/function) to get a sense of common pitfalls in graph mode. To isolate where the issue is, you might want to try running your code with a standard loss and see if it runs without this error. \r\n\r\nSetting `run_eagerly=True` will get rid of the error message, but it's not a great solution as it can really affect the performance of your code. This should really only be used for debugging purposes.\r\n\r\nAdditionally, you should update your import statements so you're using `tf.keras` instead of `keras`\r\neg `from tensorflow.keras.layers import Input, Dense` etc", "Hi @nikitamaia, I did apply the standard loss function and, it works. So I think the problem occurs when I pass extra inputs to the loss function with the help of the Input layer (these values are not included in the Dense layers, but to use these values while calculating losses, I am passing them through Input layers).\r\n\r\nAnd sure, I will update the code according to your suggestions and mention the progress here.", "@Dhyeythumar,\r\nCan you please let us know if you have updated the code according to @nikitamaia's suggestions? Thanks!  ", "I have already updated the colab notebook with a standard loss function and, it works, so definitely, there is a problem with the custom loss function. I have also updated imports like `keras` to `tensorflow.python.keras`. Still, the error remains.", "While trying to reproduce the issue, I'm facing different error in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/06f288fe16749ef985eb85e63ca26c33/43650.ipynb), Thanks!", "Hi @sachinprasadhs, You have forgotten to install few packages shown below:\r\n```\r\n!pip install box2d box2d-py\r\n```", "## Complete Report on Errors\r\n> Errors are generated from custom loss functions only.\r\n\r\n#### \u2705 Works with TensorFlow 1.15.0:\r\nhttps://colab.research.google.com/drive/1CdTzQZ-2mUeNebAv9qV-kW8WoAHX61lV\r\n\r\n---\r\n\r\n#### \u274c Error from TensorFlow 2.3.0:\r\n```\r\n_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'old_prediction_input:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'advantage_input:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'reward_input:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'value_input:0' shape=(None, 1) dtype=float32>]\r\n```\r\n**Standalone code to reproduce the issue with Tensorflow 2.3.0**\r\nhttps://colab.research.google.com/drive/1wa93OGIfXVY66GHO6wI1O9PnZyxXB-KJ\r\n\r\n---\r\n\r\n#### \u274c Error from TensorFlow 2.5.0:\r\n```\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n```\r\n**Standalone code to reproduce the issue with Tensorflow 2.5.0**\r\nhttps://colab.research.google.com/drive/1dqVHrnqUmD5ZipzMwAJa0XcnZE1fh-HG\r\n\r\n**Similar error as mentioned in the below issues:**\r\n- [TypeError: Cannot convert a symbolic Keras input/output to a numpy array.](https://github.com/tensorflow/tensorflow/issues/47311)\r\n- [custom loss function using Keras input layer](https://github.com/keras-team/keras/issues/14651)", "## Complete Report on Causes\r\n\r\n### From TensorFlow 2.3.0:\r\n```python\r\ndef ppo_loss(oldpolicy_probs, advantage, reward, value):\r\n    def loss(y_true, y_pred):\r\n        print(oldpolicy_probs) # Tensor(\"old_prediction_input:0\", shape=(None, 2), dtype=float32)\r\n        print(advantage)       # Tensor(\"advantage_input:0\", shape=(None, 1), dtype=float32)\r\n        print(reward)          # Tensor(\"reward_input:0\", shape=(None, 1), dtype=float32)\r\n        print(value)           # Tensor(\"value_input:0\", shape=(None, 1), dtype=float32)\r\n\r\n        print(y_true)     # Tensor(\"IteratorGetNext:5\", shape=(128, 2), dtype=float32)\r\n        print(y_pred)     # Tensor(\"functional_1/policy/Tanh:0\", shape=(128, 2), dtype=float32)\r\n        \r\n        # ... Compute Loss ...\r\n    return loss\r\n```\r\n\r\n---\r\n\r\n### From TensorFlow 2.5.0:\r\nIn custom loss function some of the data is in **KerasTensor form** and others in **Tensor form**.\r\n```python\r\ndef ppo_loss(oldpolicy_probs, advantage, reward, value):\r\n    def loss(y_true, y_pred):\r\n        print(oldpolicy_probs) # KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name='old_prediction_input'), name='old_prediction_input', description=\"created by layer 'old_prediction_input'\")\r\n        print(advantage)       # KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='advantage_input'), name='advantage_input', description=\"created by layer 'advantage_input'\")\r\n        print(reward)          # KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='reward_input'), name='reward_input', description=\"created by layer 'reward_input'\")\r\n        print(value)           # KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='value_input'), name='value_input', description=\"created by layer 'value_input'\")\r\n\r\n        print(y_true)     # Tensor(\"IteratorGetNext:5\", shape=(128, 2), dtype=float32)\r\n        print(y_pred)     # Tensor(\"model/policy/Tanh:0\", shape=(128, 2), dtype=float32)\r\n        \r\n        # ... Compute Loss ...\r\n    return loss\r\n```\r\n\r\n**So what to do in this case? How to pass the values to the custom loss functions so they are also in the Tensor form?**", "> Correct me if I am wrong.\r\n\r\n**The custom loss function will only work when a `Tensor` is returned and not a `Symbolic KerasTensor` or `Symbolic Tensor`.**\r\n\r\nSo the main issue here is that custom loss function is returning a `Symbolic KerasTensor` and not a `Tensor`. And this is happening because inputs to the custom loss function are in `Symbolic KerasTensor` form.\r\n\r\nSo the question is:\r\n**How to pass the extra parameters to the custom loss function in `Tensor` form instead of `Symbolic KerasTensor`?**", "Was able to reproduce your issue in Tf Nightly 2.6.0 , please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/036c354539585bd24414891688686dd6/43650.ipynb). Thanks!", "I'm struggling with this too (passing extra arguments to loss function, which are keras inputs).  Code that used to work no longer does with TF 2.5.", "Was able to reproduce your issue in TF 2.6 , please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/43cc8aa80fc3f17f8648c01ea4ae6f65/43650.ipynb#scrollTo=rLB-JECfuJKh). Thanks!", "Just as FYI, you can use model.add_loss() to do similar things as with the \"function in a function\" approach that is causing trouble here.  That is, if you are trying to get other tensors such as the input into your loss."]}, {"number": 43648, "title": "LookupError when computing nested gradient with UpSampling2D", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA GeForce GTX 1070 8GB VRAM\r\n\r\n**Describe the current behavior**\r\nThe provided code fails with the following error (see Colab link for full stacktrace):\r\n`LookupError: gradient registry has no entry for: ResizeNearestNeighborGrad`\r\n\r\n**Describe the expected behavior**\r\nThe GradientTape.gradient method should be able to compute the gradient.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Google Colab standalone code](https://colab.research.google.com/drive/1eGMvXJxVvY7zb8OISkS1H3D90N0J380d?usp=sharing)\r\n\r\n**Other info / logs**\r\nI encountered the error while developing a GAN. I used Conv2DTranspose for upsampling at first but encountered artifacts. After changing Conv2DTranspose to a combination of UpSampling2D and Conv2D (as is common in GAN's) the inner gradient stopped working.", "comments": ["@cookiehunter \r\nFrom the code shared you are using random data and generator is unable to get a point of reference, you can declare a watch before you write  tf.GradientTape() as inner Tape.(your indentation is incorrect as well)\r\nI cannot see the @tf.function wrapper, this does not seem like a bug from tf end. \r\nplease refer to these links: [link](https://www.programcreek.com/python/example/112480/tensorflow.compat.v2.GradientTape)\r\nand create an issue on [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) as this is not a bug/feature request for tf.", "@Saduf2019 \r\nEssentially what i am trying to do is measure the influence of inputB on the output of the model inside the loss calculation and use that influence as part of the loss.\r\nMy random inputs (inputA and inputB) are just placeholders for my real data in this example.\r\nI am computing the gradient of the model output wrt inputB (innerTape). That gradient takes part in the final loss calculation (variable e). I have added a watch for inputB on the innerTape. This tape is supposed to only track that variable. This error is not related to autograph and happens with and without @tf.function. The indentations are - as far as i can tell - intentional and correct for the task at hand.\r\nAs i said this code does work perfectly fine when using Conv2DTranspose instead of UpSampling2D + Conv2D.\r\nI don't know tensorflow internally but from the error message i think it is possible, that the second order derivative of UpSampling2D is just not defined in tensorflow right now.\r\nAnyway, i don't see anything wrong with what i am doing from a mathematical standpoint.", "@cookiehunter \r\nPlease provide with complete code  or colab gist with complete code as mentioned in above comment.", "@Saduf2019 \r\nMy full source code is outside the scope of colab as it is quite large.\r\nI have boiled down the code even further. I got rid of keras completely, just plain tensorflow code.\r\nThe new code involves essentially 1 number only (4 if you count the nearest neighbor resize).\r\nIn this simple example, I can now calculate the gradient by hand, but still tensorflow is unable to.\r\n[New code](https://colab.research.google.com/drive/12ztDqi2CRQPEI8KABQAzd0BHA3RfJWy_?usp=sharing) (I included the values and tensor shapes that i calculated by hand as comments)\r\nTensorflow should definitely be able to calculate the gradient there. As i said, from the stack trace it's clear, that it's probably missing a (very basic) gradient definition.", "I just found a workaround. Instead of using tensorflows UpSampling operation I defined my own version using tf.repeat along both dimensions. It can obviously only perform nearest neighbor upsampling as opposed to the actual UpSampling2D that has the option to use bilinear sampling. But for nearest neighbor it works exactly like UpSampling2D and does not have the gradient bug. It is a little slow though:\r\n\r\n```\r\nclass CustomUpSampling2D(tf.keras.layers.Layer):\r\n  def __init__(self, size):\r\n    super(CustomUpSampling2D, self).__init__()\r\n    if type(size) is not tuple and type(size) is not list:\r\n        size = (size, size)\r\n    self.size = size\r\n\r\n  def build(self, input_shape):\r\n    pass\r\n\r\n  def call(self, input):\r\n      return tf.repeat(tf.repeat(input, self.size[0], axis=1), self.size[1], axis=2)\r\n```", "@cookiehunter \r\nPlease move this issue to closed status if resolved.", "@Saduf2019 \r\nThe issue is not resolved. I just found a workaround and wanted to share it in case it is useful for someone else until the bug is fixed. I would really like to use the proper UpSampling2D layer instead, as it is also a lot faster than the workaround.", "@jvishnuvardhan  \r\nI ran the latest code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/534b4f47371db75af4de4fd53f2b5ee8/untitled418.ipynb)", "I met the same problem at tensorflow 2.4.\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow installed from (source or binary): binary (pip)\r\nTensorFlow version (use command below): 2.4\r\nPython version: 3.6\r\nCUDA/cuDNN version: 11.0,", "@jvishnuvardhan Is this issue resolved? I could not find the solution for this anywhere else. I know that the problem is that the gradient (grad function) for the **'ResizeBilinearGrad'** op is not implemented in TF, which is the back-end op called during the execution of Keras **Upsampling2D** function. This is  true for the \"nearest resize\" option as well. I also found that we can register the gradient for this op using _@tf.register_gradient_ But I don't know how to implement what the decorated function would return. It would greatly benefit my research if I this is solved, So can you please help me regarding this?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210528, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/f0bde0d4ee90716e864cacc9e93d82f0/43648.ipynb). Thanks!", "@cookiehunter Was able to replicate the issue on colab using TF v2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5aca69933b39428ac71eba3186ed4271/43648.ipynb#scrollTo=wrLulRCTqyrk) for reference.Thanks!", "Still I am getting this error. Tensorflow 2.8."]}, {"number": 43634, "title": "k hot encoding for tf.keras.preprocessing.image.DirectoryIterator", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\ntf.keras.preprocessing.image.DirectoryIterator\r\n\r\n**System information**\r\n- TensorFlow version (you are using):latest\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the function `tf.keras.preprocessing.image.DirectoryIterator` doesn't have any option for multiple classes for a single image. Currently only a single class can be assigned to an image.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAnyone\r\n\r\n**Any Other info.**\r\n", "comments": ["This class is not meant to support k hot encoding, given it requires the directory to be split by class", "I 'm not able to understand why it can't be done. The subdirs can have same images and they will have same name. In case of k-hot encoding, all that's to be done is to create is a n-dim vector for each entry.\r\nExample if,\r\nA, B, C and D are the subdirs and A and B have a common file abc.png then corresponding to \"abc.png\" a label of array([1, 1, 0, 0]) can be made. I 'm unable to see what problems can be faced. \r\n  \r\n\r\n  ", "Also, I w'd like to contribute if I can get a little bit of support because I haven't contributed so far yet.\r\n", "> Also, I w'd like to contribute if I can get a little bit of support because I haven't contributed so far yet.\r\n\r\nHow would you like to propose it be done?", "> \r\n> \r\n> I 'm not able to understand why it can't be done. The subdirs can have same images and they will have same name. In case of k-hot encoding, all that's to be done is to create is a n-dim vector for each entry.\r\n> Example if,\r\n> A, B, C and D are the subdirs and A and B have a common file abc.png then corresponding to \"abc.png\" a label of array([1, 1, 0, 0]) can be made. I 'm unable to see what problems can be faced.\r\n\r\n@tanzhenyu  As said here.", "> > I 'm not able to understand why it can't be done. The subdirs can have same images and they will have same name. In case of k-hot encoding, all that's to be done is to create is a n-dim vector for each entry.\r\n> > Example if,\r\n> > A, B, C and D are the subdirs and A and B have a common file abc.png then corresponding to \"abc.png\" a label of array([1, 1, 0, 0]) can be made. I 'm unable to see what problems can be faced.\r\n> \r\n> @tanzhenyu As said here.\r\n\r\nThat seems do-able, I was thinking something else. \r\nI will mark this as contribution welcome", "Thank you.\r\n", "I would like to know how to approach this problem? and contribute to it. Please guide me @tanzhenyu ", "@tanzhenyu I was starting with making it but it felt like [flow from dataframe](https://keras.io/api/preprocessing/image/#flowfromdataframe-method) might be the most suitable method based on data that one gets on Kaggle. Please share your opinions. This way one would just need to prepare a pandas dataframe of file paths and labels, like currently I 'm having this. \r\n![image](https://user-images.githubusercontent.com/40588378/97090992-16c5ea80-1656-11eb-8f30-ff4319e748fa.png)\r\nCreating such a dataframe would not be tough in pandas for any user. \r\n", "@tanzhenyu \r\nI want to propose a possible solution. Suppose following is the directory structure\r\n```\r\nimages\r\n-feature1\r\n---img1.jpg\r\n---img2.jpg\r\n---img3.jpg\r\n-feature2\r\n---img1.jpg\r\n---img3.jpg\r\n---img4.jpg\r\n---img5.jpg\r\n```\r\nWe could calculate the hash of all images in all directories, as the names can be the same, and store them in a dict (say).\r\nStructure of this dict:\r\n```python\r\n{\r\n  'feature1':{\r\n                    'img1.jpg':1234,\r\n                    'img2.jpg':1235\r\n                     ....  },\r\n  'feature2':{\r\n                   'img1.jpg':1234,\r\n                    'img3.jpg':1234\r\n                     ....  },\r\n}\r\n\r\n```\r\n Then make a `tf.dataDataset` of all unique images (no labels in that dataset). Then  iterate through all images in all feature dicts to find if it is present under any other feature, and create a many-hot vector in this fashion. Of course, this is a brute-force method. Please suggest improvements on the same. \r\n", "@XtremeGood \r\nEven for this? ", "Actually no, keras does not provide enough good api like tensorflow, it rather requires you to load all the data in memory at once. So an efficient approach like storing the paths in a dataframe and making the iterator again and again would be required.", "I was thinking on similar lines, using `tf.data.Dataset` more efficiently ", "sklearn has MultiLabelBinarizer. Intending to do something similar"]}, {"number": 43626, "title": "Feature request: MCUXpresso IDE integration", "body": "The [NXP MCUXpresso IDE](https://www.nxp.com/design/software/development-software/mcuxpresso-software-and-tools-/mcuxpresso-integrated-development-environment-ide:MCUXpresso-IDE) is probably the most widely used development environment for NXP micro-controllers. It is free to use and available for Linux, macOS and Windows. It seems like there is no support for it in TensorFlow Lite for Microcontrollers yet. Would be great to have support for it.", "comments": []}, {"number": 43625, "title": "Feature request: STM32CubeIDE integration", "body": "The [STM32CubeIDE](https://www.st.com/en/development-tools/stm32cubeide.html) is probably the most widely used development environment for STM32 micro-controllers. It is free to use and available for Linux, macOS and Windows. It seems like there is no support for it in *TensorFlow Lite for Microcontrollers* yet. Would be great to have support for it.", "comments": []}, {"number": 43616, "title": "Compiling Person Detection Model for zephyr_riscv", "body": "Hi,\r\nI was able to compile both the example applications(Hello World and Magic wand) for zephyr_riscv. \r\n\r\nHow to compile the other TFLite Models(Person detection and Image Recognition) for zephyr_riscv? What are the changes that need to be made in order to compile it for zephyr_riscv?\r\n\r\nNote: **make -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv person_detection_bin** I used this command for Person Detection and I am getting the output as: **make: Nothing to be done for 'person_detection_bin'**.\r\n\r\nBest Regards,\r\nDarshan", "comments": []}, {"number": 43608, "title": "Debugger V2 not working.  Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26", "body": "### System information\r\n\r\n-  I have used the test example from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/examples/v2/debug_mnist_v2.py)\r\n-   OS: Windows 10\r\n-   Tensorflow 2.3.1 (installed with pip):\r\n-   Python 3.6\r\n-   CUDA 10.1\r\n-   nVidia GeForce GTX 1050\r\n\r\nI cannot make the example work with Debugger V2.\r\n\r\nBy executing the example from the link above I get the following output:\r\n\r\n```\r\nD:\\src\\ai\\visualthing\\venv\\Scripts\\python.exe \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\pydevd.py\" --multiproc --qt-support=auto --client 127.0.0.1 --port 50790 --file D:/src/ai/visualthing/debug_mnist_v2.py --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH\r\n\r\npydev debugger: process 8484 is connecting\r\n\r\nConnected to pydev debugger (build 192.5728.105)\r\n2020-09-27 20:31:08.451881: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\nINFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir, tensor debug mode: FULL_HEALTH)\r\nI0927 20:31:11.284601  1260 dumping_callback.py:871] Enabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir, tensor debug mode: FULL_HEALTH)\r\n2020-09-27 20:31:11.557685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-27 20:31:11.584474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-09-27 20:31:11.584652: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-27 20:31:11.588047: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-27 20:31:11.591169: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-27 20:31:11.592204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-27 20:31:11.595773: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-27 20:31:11.597733: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-27 20:31:11.605092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-27 20:31:11.605244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-27 20:31:11.605644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-27 20:31:11.614513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f4c545b410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-27 20:31:11.614778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-27 20:31:11.615119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-09-27 20:31:11.615425: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-27 20:31:11.615585: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-27 20:31:11.615691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-27 20:31:11.615830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-27 20:31:11.615921: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-27 20:31:11.616011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-27 20:31:11.616099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-27 20:31:11.616214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-27 20:31:12.188255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-27 20:31:12.188425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-27 20:31:12.188484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-27 20:31:12.188686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2987 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-09-27 20:31:12.191306: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f4e366a9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-27 20:31:12.191431: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1\r\n2020-09-27 20:31:13.537229: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\pydevd.py\", line 2060, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\pydevd.py\", line 2054, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\pydevd.py\", line 1405, in run\r\n    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\pydevd.py\", line 1412, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2019.2\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"D:/src/ai/visualthing/debug_mnist_v2.py\", line 238, in <module>\r\n    absl.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"D:/src/ai/visualthing/debug_mnist_v2.py\", line 223, in main\r\n    y = model(x_train)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1933, in _call_flat\r\n    cancellation_manager=cancellation_manager)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 138, in execute_with_callbacks\r\n    tensors = quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n  File \"D:\\src\\ai\\visualthing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26\r\n\t [[{{node StatefulPartitionedCall/MatMul/ReadVariableOp/DebugNumericSummaryV2}}]]\r\n\t [[x/_1]]\r\n  (1) Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:26\r\n\t [[{{node StatefulPartitionedCall/MatMul/ReadVariableOp/DebugNumericSummaryV2}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__forward_model_324]\r\n\r\nFunction call stack:\r\nmodel -> model\r\n\r\nINFO:tensorflow:Disabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir)\r\nI0927 20:31:55.200698  1260 dumping_callback.py:895] Disabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir)\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nI have also tried to build my own example with no success, same error: \r\n`DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53)`\r\n\r\n\r\n\r\n", "comments": ["@jaimeff \r\nPlease share stand alone code or if possible share a colab gist with error reported.", "@Saduf2019 \r\nI have used the exact same example as in this repo [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/examples/v2/debug_mnist_v2.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/examples/v2/debug_mnist_v2.py)\r\n\r\nyou can run it by executing:\r\n\r\n`python debug_mnist_v2.py  --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH`\r\n\r\nIn my specific case, I have run it with the following command:\r\n\r\n`D:\\src\\ai\\visualthing\\venv\\Scripts\\python.exe D:/src/ai/visualthing/debug_mnist_v2.py --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH`\r\n\r\n", "Same problem here. Searched all over for a solution and can't find one. Any help would be appreciated.\r\n", "@Saduf2019 Does the approach suggested by @jaimeff solve your problem? I'm not able to reproduce your issue with either the latest tf-nightly (2.4.0-dev20201007) or tf 2.3.1. I'm using the command \r\n\r\n```\r\npython -m tensorflow.python.debug.examples.v2.debug_mnist_v2 \\\r\n    --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH\r\n```", "I get the following using that command:\r\n\r\n```\r\n(venv) D:\\src\\ai\\visualthing>python -m tensorflow.python.debug.examples.v2.debug_mnist_v2 --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH\r\n2020-10-13 15:02:57.350397: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\nD:\\src\\ai\\visualthing\\venv\\Scripts\\python.exe: Error while finding module specification for 'tensorflow.python.debug.examples.v2.debug_mnist_v2' (ModuleNotFoundError: No module named 'tensorflow.python.debug.examples')\r\n```\r\n\r\nThis is the output of  'pip list' that is related to tensorflow:\r\n\r\n```\r\ntensorboard              2.3.0\r\ntensorboard-plugin-wit   1.7.0\r\ntensorflow               2.3.1\r\ntensorflow-addons        0.11.2\r\ntensorflow-estimator     2.3.0\r\n```\r\nSo apparently, tensorflow package 2.3.1 installed with pip doesn't have DebugV2 support? \r\nIs there any other package that I'm missing?\r\n", "@jaimeff  This may be an operating system-specific issue. I see you are using Windows. I failed to reproduce the issue on Linux. \r\n\r\nCan you try running this Python script directly (instead of using `python -m ...`) and see what happens? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/examples/v2/debug_mnist_v2.py", "Yes, that's exactly what I did when I saw I didn't have the file. So I copied it and ran it. \r\n\r\nThe full output is in the first message [(click here)](https://github.com/tensorflow/tensorflow/issues/43608#issue-709794642) (the one that I posted to open this issue).\r\n\r\nTo summarize I'm getting the error message `DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53)`\r\n", "I got the same error. You have got a fix?", "> I solved my problem by removing this line:\r\n> `tf.debugging.experimental.enable_dump_debug_info(path, tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)`\r\n> \r\n> **AND** restarting my Kernel after removing this line.\r\n> \r\n> System information\r\n> \r\n> ```\r\n> OS: Windows 10\r\n> Tensorflow 2.3.0 (installed with pip):\r\n> Python 3.8\r\n> CUDA 10.1\r\n> nVidia GeForce GTX 1050\r\n> ```\r\n\r\nYes, but now you don't have debugging information. Am I right?\r\n\r\nThe problem is that we cannot use Debugger V2 on Windows 10. The whole purpose of this ticket is to figure out how to make it work. Of course, if you disable it the problem is gone :-D\r\n", "I can still reproduce this bug on tf 2.4.1 on windows.\r\ntf.debugging.experimental.enable_dump_debug_info still results in the mentioned exception", "I ran into the same issue on Windows 10 with tf 2.3.0 ", "I played around with the parameters. It seems that the debugger runs with the defaults. i.e. \r\ntf.debugging.experimental.enable_dump_debug_info(        \"tfdbg_logs\",tensor_debug_mode=\"NO_TENSOR\" ). But other options for the parameter `tensor_debug_mode` fail.  \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:4156\r\n\t [[node functional_1/batch_normalization_8/FusedBatchNormV3/ReadVariableOp_1/DebugNumericSummaryV2 (defined at C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:119) ]]\r\n\t \r\n[[broadcast_weights_1/assert_broadcastable/is_valid_shape/else/_486/broadcast_weights_1/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/then/_1492/broadcast_weights_1/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/concat/_2860]]\r\n  (1) Invalid argument:  DebugNumericSummaryV2Op requires tensor_id to be less than or equal to (2^53). Given tensor_id:4156\r\n\t [[node functional_1/batch_normalization_8/FusedBatchNormV3/ReadVariableOp_1/DebugNumericSummaryV2 (defined at C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packag\r\n```es\\wandb\\integration\\keras\\keras.py:119) ]]\r\n", "Same problem here. Has anyone found the solution?", "on Win10 TF 2.4.1 with the same error message", "Getting same issue in TF 2.5.0 in Windows 10, any work around?", "As I mention only the NO_TENSOR debug_mode works. However, it not really a workaround if you want to get details on your tensors. Other than that you could try to set up WSL or use a Linux distro and enjoy the journey of setting up another TensorFlow environment.", "Same issues TF 2.5.1 on Windows 10."]}, {"number": 43607, "title": "Example of inferencing a Tensorflow lite model with parsing_serving_input_receiver_fn using C++ API", "body": "I have followed the Tensorflow2 documentation to convert my trained  tf.estimator model to tflite model; in order to convert my model, first I had to save my model in saved_model format with a input_receiver_fn and then convert it with SELECT_OPS flag:\r\n\r\n```\r\nclassifier = tf.estimator.LinearClassifier(n_classes=2, model_dir = classifier_dir, feature_columns=features)\r\nclassifier.train(input_fn = lambda: trian_fn(features = train_datas, labels = trian_labels))\r\n\r\nserving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(tf.feature_column.make_parse_example_spec(features))\r\n\r\nclassifier.export_saved_model(classifier_dir+\"\\saved_model\", serving_input_fn)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir = saved_model_dir , signature_keys=['serving_default']) \r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nI wanted to run my tflite model on an ARM device without python support so I built the C++ interpreter shared libs with Bazel as it is explained in the documentation :\r\n [Cross-compile for armhf with Bazel](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_armhf_with_bazel)\r\n[Select TensorFlow operators C++\r\n](https://www.tensorflow.org/lite/guide/ops_select#c)\r\n\r\nMy model has 3 input features but when I try to use the following [guide](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c) for inferencing I get a segmentation fault.\r\n\r\nI used the following code to extract my model details:\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"./model.tflite\")\r\ninterpreter.allocate_tensors()\r\nprint(\"all ok\")\r\n# Print input shape and type\r\ninputs = interpreter.get_input_details()\r\nprint('{} input(s):'.format(len(inputs)))\r\nfor i in range(0, len(inputs)):\r\n    print('{} {}'.format(inputs[i]['shape'], inputs[i]['dtype']))\r\n\r\n# Print output shape and type\r\noutputs = interpreter.get_output_details()\r\nprint('\\n{} output(s):'.format(len(outputs)))\r\nfor i in range(0, len(outputs)):\r\n    print('{} {}'.format(outputs[i]['shape'], outputs[i]['dtype']))\r\n```\r\nI got the following output:\r\n\r\n```\r\nall ok\r\n1 input(s):\r\n[1] <class 'numpy.bytes_'>\r\n\r\n2 output(s):\r\n[1 2] <class 'numpy.bytes_'>\r\n[1 2] <class 'numpy.float32'>\r\n```\r\nfirst few lines of the output of   `tflite::PrintInterpreterState(interpreter.get())`  are:\r\n```\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 25 nodes with 1 partitions.\r\n\r\nInterpreter has 54 tensors and 26 nodes\r\nInputs: 0\r\nOutputs: 38 34\r\n\r\nTensor   0 input_example_tensor kTfLiteString  kTfLiteDynamic          0 bytes ( 0.0 MB)  1\r\n```\r\n\r\nThe output illustrates that the input shape is not the same as the original model, also the input type is <class 'numpy.bytes_'> but the Tensorflow 2 model inputs are [numpy.float32, numpy.float32, numpy.float32].\r\nmy input dictionary for prediction in TF2 model is something like : {'feature0' : data0, 'feature1' : data1, 'feature2' : data2}\r\n[here is the Google Colab link to the project](https://colab.research.google.com/drive/1fkj8zM2FM-xd6cajWkStcasmzliZWF9s?usp=sharing)\r\n\r\nI tried to fill the input buffer with a vector of zeros but it was without success. Here is my C++ code to load a tflite model and feed it inputs for prediction. can someone please point me to the right direction since I could not find any examples or related documentation for feeding inputs to converted tf.estimator with a serving_input_fn.\r\n\r\n\r\n```\r\n#include <cstdio>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"\r\n\r\nint main()\r\n{\r\n  // Load model\r\n      std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(\"model.tflite\");\r\n      \r\n  // Build the interpreter with the InterpreterBuilder.\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n  \r\n  // Allocate tensor buffers.\r\n  interpreter->AllocateTensors();\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Fill input buffers\r\n  std::vector<float> tensor(3, 0);\t//Vector of zeros\r\n  int input = interpreter->inputs()[0];\r\n  float* input_data_ptr = interpreter->typed_input_tensor<float>(input);\r\n  for(int i = 0; i < 3; ++i)\r\n  {\r\n  \t*(input_data_ptr) = (float)tensor[i];\r\n  \tinput_data_ptr++;\r\n  }\r\n  // Run inference\r\n  interpreter->Invoke();\r\n  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n  \r\n  return 0;\r\n}\r\n```\r\n", "comments": ["The original model uses an example proto as input internally, which is used by the TF estimator API. So, the converted TFLite model needs one input, example proto as a byte array. Could you create an example proto, in a form of byte array, with three features in your inference code?", "@abattery  Thank you for the comment, I have no previous knowledge about example protos. Are there any documentation references, or can you provide a simple example for just one input feature? ", "Here is the example code in Python to generate an Example protobuf byte.\r\n\r\n\r\n```\r\n    features = collections.OrderedDict()\r\n    features[\"feature_one\"] = ...\r\n    features[\"feature_two\"] = ...\r\n    features[\"feature_third\"] = ...\r\n\r\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\r\n    input_tensor = tf_example.SerializeToString()\r\n```", "@abattery Thanks, are the methods for generating  Example protobuf bytes available in C++ API, or should I implement my own? because I don't have python on the platform that I'm using to interpret tflite model.", "The example proto is a part of Tensorflow core project. If you can link to Tensorflow core project, you may use the following methods for example protobuf object creation. Or it is possible to create an example proto based on protobuf library.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature_util.h\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto", "@abattery I will try to feed an example proto to my model using C++ and I will post the results. thank you.\r\n", "@abattery I read about proto buffers and example protos in Tensorflow, I decided to first verify my model in python and then write the C++ interpreter, I created my own example protobuf:\r\n```\r\ndef _float_feature(value):\r\n  \"\"\"Returns a float_list from a float / double.\"\"\"\r\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\ndef serialize_example(feature0, feature1, feature2):\r\n  feature={'feature0': _float_feature(feature0),\r\n            'feature1': _float_feature(feature1),\r\n            'feature2' : _float_feature(feature2)}\r\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n  return example_proto.SerializeToString()\r\nserialized_example = serialize_example(0,0,0)\r\nserialized_example\r\n```\r\nthe output is : `b'\\nB\\n\\x14\\n\\x08feature0\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n\\x14\\n\\x08feature1\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n\\x14\\n\\x08feature2\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00'`\r\nI tried to feed the example to model as you mentioned:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\ninterpreter.set_tensor(input_details[0]['index'], serialized_example)\r\n\r\n```\r\nbut I get the following error:\r\n`ValueError: Cannot set tensor: Dimension mismatch. Got 0 but expected 1 for input 0.\r\n`\r\nshould I change the input shape?", "Could you try `interpreter.resize_tensor_input` before assigning the value?", "@abattery Sorry for the late feedback, Tested it and got a runtime reset; is there a way to change the input function so it accepts three feature inputs as an array with shape [1 1 1] instead of using a serving input? it's more efficient and befitting for my application and it saves a lot of trouble.  In the documentation I only encountered a guide which used a serving_input_fn to convert a TF2 estimator to a tflite model. I don't know if there are any alternative ways to create an input function"]}, {"number": 43592, "title": "WorkerTrainingState Test and Bugfix", "body": "When trying to backup and restoring from the same location on multiple workers, delete_backup has a bug where get_matching_files_v2 fails because another worker has already previously deleted the folder.\r\n\r\nThis has been fixed because of this [issue](https://github.com/tensorflow/tensorflow/issues/43789) still keeping the test there.", "comments": ["@sboshin  Can you please resolve conflicts? Thanks!", "@sboshin Can you please resolve conflicts? Thanks!", "@sboshin  Any update on this PR? Please. Thanks!", "Sorry it seems using multiprocessing is inheriting the eager context when running the test. Will Fix", "Thanks for the PR. Is it possible to not rely on multiprocessing module for unit tests? multiprocessing module in general does not work well with our testing infrastructure and unit tests without it will make it much smoother."]}, {"number": 43583, "title": "Tensorflow lite(ssd_mobilenet_v2_fpnlite_640*640)  Android APP detect nothing", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Linux Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  **Motorola**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **Tf-nightly 2.4.0**\r\n- Python version: **3.6.10**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **CUDA 10.2 cuDNN**\r\n- GPU model and memory: **GeForce GTX 1080 8G**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n1. Use my data to retrain the model **ssd_mobilenet_v2_fpnlite_640*640**, [link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)\r\n2. train command: **model_main_tf2.tf** [,link](https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py)\r\n    export command: **export_tflite_graph_tf2.py**, [link](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_graph_tf2.py)\r\n\r\n**model convert :**\r\n`model = tf.saved_model.load(\"./saved_model\")\r\nmodel.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 640, 640, 3])\r\ntf.saved_model.save(model, \"saved_model_updated\", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='./saved_model', signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"./ssd_resnet50.tflite\", \"wb\").write(tflite_model)`\r\n\r\n3. Android app, [link](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)\r\n4. main lines in **DetectActivity.java** \r\n\r\n` private static final int TF_OD_API_INPUT_SIZE = 640;\r\n\r\n  private static final boolean TF_OD_API_IS_QUANTIZED = true;\r\n\r\n  private static final String TF_OD_API_MODEL_FILE = \"detect.tflite\";\r\n\r\n  private static final String TF_OD_API_LABELS_FILE = \"labelmap.txt\";\r\n\r\n  private static final DetectorMode MODE = DetectorMode.TF_OD_API;\r\n\r\n  // Minimum detection confidence to track a detection.\r\n  private static final float MINIMUM_CONFIDENCE_TF_OD_API = 0.5f;\r\n\r\n  private static final boolean MAINTAIN_ASPECT = false;\r\n\r\n  private static final Size DESIRED_PREVIEW_SIZE = new Size(640, 480);`\r\n\r\n**Describe the expected behavior**\r\n**Android app should  detect objects  with their names location** \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n**When run app, errs are like this:**\r\n\r\n`2020-09-26 10:13:37.689 21845-21845/org.tensorflow.lite.examples.detection E/tensorflow: CameraActivity: Exception!\r\n    java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.assertZipFile(MetadataExtractor.java:325)\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.getAssociatedFile(MetadataExtractor.java:165)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:116)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:106)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1175)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6695)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:772)`\r\n\r\n**By the way i tested the converted tflite and it's ok.**\r\n\r\n", "comments": ["I have the same problem, anyone with suggestions?", "I followed this guide below with tf1 `ssd_mobilenetv2_oidv4` model and got similar error on [Android sample](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android). \r\n\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md\r\n\r\n\r\nMy model conversion script with `tensorflow 1` following the guide,\r\n\r\nhttps://nbviewer.jupyter.org/github/quickgrid/CodeLab/blob/master/tensorflow/TFlite_custom_object_detection_model_Export_TF1.ipynb?flush_cache=True\r\n\r\n\r\nLabel map as mentioned in the guide,\r\n\r\n[labelmap.txt](https://github.com/tensorflow/tensorflow/files/5315227/labelmap.txt)\r\n\r\n\r\nExported `detect.tflite` properties,\r\n\r\n![model - Copy](https://user-images.githubusercontent.com/1857293/94872821-71ad5d00-046f-11eb-8a19-75748d60bd48.jpg)\r\n\r\n\r\nPartial error message,\r\n```\r\nProcess: org.tensorflow.lite.examples.detection, PID: 6129\r\n    java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.assertZipFile(MetadataExtractor.java:325)\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.getAssociatedFile(MetadataExtractor.java:165)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:116)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:105)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\n```", "**UPDATE 2:**  **Packing metadata `labelmap.txt` into tflite works with current metadata based [Android example](https://github.com/tensorflow/examples/tree/72b5e41c423038c72650eb12fb3fb9693b533787).** It is working for both quantized and unquantized float model. Here is my sample working solution for `tf1` and `tf2` based `SSD MobileNet v2`.\r\n\r\nhttps://github.com/quickgrid/CodeLab/blob/master/tensorflow/TFlite_Object_Detection_Custom_Model_Export_With_Metadata_TF1.ipynb\r\n\r\nhttps://github.com/quickgrid/CodeLab/blob/master/tensorflow/TFlite_Object_Detection_Custom_Model_Export_With_Metadata_TF2.ipynb\r\n\r\n\r\n<hr>\r\n\r\n**UPDATE:** The problem seems indeed with missing `metadata` conversion instruction documentation. Previously it read labels from file,\r\n\r\n```\r\nString actualFilename = labelFilename.split(\"file:///android_asset/\")[1];\t\r\nInputStream labelsInput = assetManager.open(actualFilename);\r\nBufferedReader br = new BufferedReader(new InputStreamReader(labelsInput));\r\n```\r\n\r\nHere, is the commit where it was changed,\r\n\r\nhttps://github.com/tensorflow/examples/commit/de42482b453de6f7b6488203b20e7eec61ee722e#diff-0cf980453f18b3cc1368f8eb16a803f9L113\r\n\r\n<hr>\r\n\r\nSeems like the problem is maybe missing instructions for android example code. It did not mention about metadata,\r\n\r\nhttps://github.com/tensorflow/examples/blob/e13e7cc1d2df1c006f14a931906f3f409e91c97d/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L113 \r\n\r\n\r\nThe sample code uses `metadata` implementation,\r\n\r\nhttps://github.com/tensorflow/examples/blob/e13e7cc1d2df1c006f14a931906f3f409e91c97d/lite/examples/object_detection/android/app/build.gradle#L48\r\n\r\nRelated doc,\r\n\r\nhttps://www.tensorflow.org/lite/convert/metadata#read_the_metadata_in_java\r\n\r\n\r\nHere it mentions about `tflite` becoming a `zip`, but still keeping `tflite` extension,\r\n\r\n> The associated files can now be bundled with the model through the metadata Python library. The new TensorFlow Lite model becomes a zip file that contains both the model and the associated files. It can be unpacked with common zip tools. This new model format keeps using the same file extension, .tflite. It is compatible with existing TFLite framework and Interpreter.\r\n\r\nhttps://www.tensorflow.org/lite/convert/metadata#pack_the_associated_files\r\n\r\n\r\nI was able get `tensorflow 2` based `SSD MobileNet v2 320x320` working with python tflite inference api on colab. Bounding boxes seem correct, labels seem to be wrong.\r\n\r\n\r\n[Code ](https://github.com/tensorflow/examples/blob/e13e7cc1d2df1c006f14a931906f3f409e91c97d/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L113 ) from sample,\r\n\r\n```\r\nMappedByteBuffer modelFile = loadModelFile(assetManager, modelFilename);\r\nMetadataExtractor metadata = new MetadataExtractor(modelFile);\r\nBufferedReader br =\r\n    new BufferedReader(new InputStreamReader(metadata.getAssociatedFile(labelFilename)));\r\nString line;\r\nwhile ((line = br.readLine()) != null) {\r\n  LOGGER.w(line);\r\n  d.labels.add(line);\r\n}\r\nbr.close();\r\n```\r\n\r\nI commented and added this below above code in `TFLiteObjectDetectionAPIModel.java`. It should work by replacing code below with reading `labelmap.txt` line by line and adding to `d`.\r\n\r\n```\r\nfor(int i = 0; i < 91; i++){\r\n  d.labels.add(\"label: \" + i);\r\n}\r\n```\r\n\r\nSeems if the model does not have metadata then `labelmap.txt` can be read into `d`. My takeaway is either the model should be  zipped with metadata or that code in java must be replaced to make the sample work.\r\n\r\nhttps://github.com/tensorflow/examples/blob/e13e7cc1d2df1c006f14a931906f3f409e91c97d/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java#L111\r\n\r\n", "@quickgrid hmm sorry\r\nI'am using ssd mobilenet V2 coco 300x300 trained using tensorflow 2 following this [tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md#running-our-model-on-android)\r\nalready try change TF_OD_API_IS_QUANTIZED = false; TF_OD_API_IS_QUANTIZED = true;\r\nand its not working\r\n\r\nthis is my error\r\n`2020-10-13 01:44:23.049 19435-19435/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 19435\r\n    java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.assertZipFile(MetadataExtractor.java:325)\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.getAssociatedFile(MetadataExtractor.java:165)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:118)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\n        at android.view.TextureView.getTextureLayer(TextureView.java:400)\r\n        at android.view.TextureView.draw(TextureView.java:349)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22062)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22048)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22048)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.draw(View.java:23190)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22062)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1246)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.draw(View.java:23190)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22062)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22048)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22048)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22048)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22048)\r\n        at android.view.View.draw(View.java:22917)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:5230)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4987)\r\n        at android.view.View.draw(View.java:23190)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:1154)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:22062)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:588)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:594)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:667)\r\n2020-10-13 01:44:23.049 19435-19435/org.tensorflow.lite.examples.detection E/AndroidRuntime:     at android.view.ViewRootImpl.draw(ViewRootImpl.java:4296)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:4080)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3348)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2225)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:9126)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:999)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:797)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:732)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:984)\r\n        at android.os.Handler.handleCallback(Handler.java:883)\r\n        at android.os.Handler.dispatchMessage(Handler.java:100)\r\n        at android.os.Looper.loop(Looper.java:237)\r\n        at android.app.ActivityThread.main(ActivityThread.java:8167)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:496)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1100)\r\n2020-10-13 01:44:24.749 19435-21998/org.tensorflow.lite.examples.detection W/System: A resource failed to call close. \r\n`\r\n\r\nIts crashing when the compile finished\r\nI already read your answer\r\nbut can you make it step by step what should I do, because I am new about this\r\nso i cant understand what really I need to do with your answer recently\r\n\r\nplease help me :)\r\n\r\n\r\n\r\njust read [this](https://towardsdatascience.com/detecting-pikachu-on-android-using-tensorflow-object-detection-15464c7a60cd) articles in the conclusion part he tells about his crash.\r\nI don't know about this, any idea?", "@KevinCS17 Seems code was updated and many things changed, https://github.com/tensorflow/examples/commit/bac70589c2b206614f7f95ca84b624a9d6e9449e. You can try with latest one.", "@KevinCS17 I use below code  in aTFLiteObjectDetectionAPIModel.java : \r\n`InputStream is = assetManager.open(labelFilename);\r\n    BufferedReader br = new BufferedReader(new InputStreamReader(is));`\r\n\r\ninstead\r\n\r\n` BufferedReader br =\r\n        new BufferedReader(new InputStreamReader(metadata.getAssociatedFile(labelFilename)));`\r\n\r\n", "> @KevinCS17 Seems code was updated and many things changed, [tensorflow/examples@bac7058](https://github.com/tensorflow/examples/commit/bac70589c2b206614f7f95ca84b624a9d6e9449e). You can try with latest one.\r\n\r\nYup it just updated last night (1 am in my country), and I work in 12 - 4 am.. dont know if that just updated.. I must try the new one..", "> @KevinCS17 I use below code in aTFLiteObjectDetectionAPIModel.java :\r\n> `InputStream is = assetManager.open(labelFilename); BufferedReader br = new BufferedReader(new InputStreamReader(is));`\r\n> \r\n> instead\r\n> \r\n> ` BufferedReader br = new BufferedReader(new InputStreamReader(metadata.getAssociatedFile(labelFilename)));`\r\n\r\nThank you for the help and fast responds..\r\nI'll try that one later as soon as possible.. and report its work or not.. \ud83d\ude4f\ud83d\ude4f", "ITS WORKKKK\r\nthe label map a little but problem i think\r\n\r\n`???\r\nperson\r\ncar\r\nbicycle\r\nmotorcycle\r\nstop sign`\r\n\r\nits already as same as the class\r\nbut\r\nwhen detecting person it say \"???\"\r\ndetecting car it say \"person\"\r\nuntil stop sign it saya \"motorcycle\"\r\n\r\nso the class = class + 1\r\n\r\nso I'll already fix it with\r\nchanging the labelmap.txt to\r\n`person\r\ncar\r\nbicycle\r\nmotorcycle\r\nstop sign\r\n`\r\n", "So what I did is using \r\n\r\n> @KevinCS17 Seems code was updated and many things changed, [tensorflow/examples@bac7058](https://github.com/tensorflow/examples/commit/bac70589c2b206614f7f95ca84b624a9d6e9449e). You can try with latest one.\r\n\r\nthis new github\r\n\r\nchanging this \r\n\r\n> @KevinCS17 I use below code in aTFLiteObjectDetectionAPIModel.java :\r\n> `InputStream is = assetManager.open(labelFilename); BufferedReader br = new BufferedReader(new InputStreamReader(is));`\r\n> \r\n> instead\r\n> \r\n> ` BufferedReader br = new BufferedReader(new InputStreamReader(metadata.getAssociatedFile(labelFilename)));`\r\n\r\nfrom\r\n`    try (BufferedReader br =\r\n        new BufferedReader(\r\n            new InputStreamReader(\r\n                metadata.getAssociatedFile(labelFilename), Charset.defaultCharset()))) {\r\n      String line;\r\n      while ((line = br.readLine()) != null) {\r\n        Log.w(TAG, line);\r\n        d.labels.add(line);\r\n      }\r\n    }`\r\n\r\nto this\r\n\r\n`try (InputStream is = assetManager.open(labelFilename); BufferedReader br = new BufferedReader(new InputStreamReader(is))) {\r\n      String line;\r\n      while ((line = br.readLine()) != null) {\r\n        Log.w(TAG, line);\r\n        d.labels.add(line);\r\n      }\r\n    }`\r\n\r\nI dont use quantized models so changing this line in DetectorActivity.java\r\n`private static final boolean TF_OD_API_IS_QUANTIZED = true;`\r\nto\r\n`private static final boolean TF_OD_API_IS_QUANTIZED = false;`", "> try (InputStream is = assetManager.open(labelFilename); BufferedReader br = new BufferedReader(new InputStreamReader(is))) { String line; while ((line = br.readLine()) != null) { Log.w(TAG, line); d.labels.add(line); } }\r\n\r\nnot worked for me: i have error \"error: cannot find symbol InputStream\"\r\n*Repository https://github.com/tensorflow/examples downloaded 15.10.2020*", "> > try (InputStream is = assetManager.open(labelFilename); BufferedReader br = new BufferedReader(new InputStreamReader(is))) { String line; while ((line = br.readLine()) != null) { Log.w(TAG, line); d.labels.add(line); } }\r\n> \r\n> not worked for me: i have error \"error: cannot find symbol InputStream\"\r\n> _Repository https://github.com/tensorflow/examples downloaded 15.10.2020_\r\n\r\nhave you already alt + enter to import the new class on Android Studio?\r\n\r\nif it doesnt work , try to see your own model.tflite models in [Netron](https://lutzroeder.github.io/netron/)\r\njust drag and drop your model file to the Netron browser\r\ncompare it with the detect.tflite in the example\r\nthe example using ssd_mobilnet_v1 I think... (but I am not sure about the models)", "> > > try (InputStream is = assetManager.open(labelFilename); BufferedReader br = new BufferedReader(new InputStreamReader(is))) { String line; while ((line = br.readLine()) != null) { Log.w(TAG, line); d.labels.add(line); } }\r\n> > \r\n> > \r\n> > not worked for me: i have error \"error: cannot find symbol InputStream\"\r\n> > _Repository https://github.com/tensorflow/examples downloaded 15.10.2020_\r\n> \r\n> have you already alt + enter to import the new class on Android Studio?\r\n> \r\n> if it doesnt work , try to see your own model.tflite models in [Netron](https://lutzroeder.github.io/netron/)\r\n> just drag and drop your model file to the Netron browser\r\n> compare it with the detect.tflite in the example\r\n> the example using ssd_mobilnet_v1 I think... (but I am not sure about the models)\r\n\r\nThanks! Alt+enter resolve error ", "> Thanks! Alt+enter resolve error\r\n\r\nVery well.. happy to hear that too :)", "@KevinCS17 \r\nWhen I tested it on the new github, it worked as follows.\r\n\r\n\r\n    AssetManager am = context.getAssets();\r\n    InputStream is = am.open(labelFilename);\r\n\r\n    MappedByteBuffer modelFile = loadModelFile(context.getAssets(), modelFilename);\r\n    MetadataExtractor metadata = new MetadataExtractor(modelFile);\r\n    try (BufferedReader br =\r\n        new BufferedReader(\r\n            new InputStreamReader(is))) {\r\n      String line;\r\n      while ((line = br.readLine()) != null) {\r\n        Log.w(TAG, line);\r\n        d.labels.add(line);\r\n      }\r\n    }\r\nI hope it helps\r\nSorry for not speaking English well :(", "@Jangbyeongwook Thank you so much!!!! you are really savior of my life. This solve all my problems after a week unable to sleep very well :)", "Working for me with https://github.com/tensorflow/examples/compare/master...cachvico:darren/fix-od ", "![Screenshot from 2021-01-28 17-46-08](https://user-images.githubusercontent.com/48323383/106164365-8afeb580-6192-11eb-87a7-71ab0655d6d2.png)\r\n\r\n\r\n\r\n\r\nI trained a tensorflow model (SSD MOBILE NET V2) and converted to tflite to detect on android using tensorflow object detection demo app but the model does not detect any object. Please help!", "> @Jangbyeongwook Thank you so much!!!! you are really savior of my life. This solve all my problems after a week unable to sleep very well :)\r\n\r\nhi, I'm facing the same problem. please help", "> @KevinCS17 Seems code was updated and many things changed, [tensorflow/examples@bac7058](https://github.com/tensorflow/examples/commit/bac70589c2b206614f7f95ca84b624a9d6e9449e). You can try with latest one.\r\n\r\nI cloned the updated repo but still getting the same error. The app runs but does not detect object", "> @Jangbyeongwook Thank you so much!!!! you are really savior of my life. This solve all my problems after a week unable to sleep very well :)\r\n\r\nI am still getting this issue kindly help.\r\njava.lang.IllegalArgumentException: Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:0) with shape [1, 4] to a Java object with shape [1, 4, 4].\r\n\r\n"]}, {"number": 43568, "title": "TensorBoard callback doesn't update step properly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: 18.04.1-Ubuntu\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: libcudart.so.10.1\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nIf using `tf.summary` operations while an `tf.keras.callbacks.TensorBoard` callback is active, the default `step` is always `0`.\r\nThis does not affect the step value of logged metrics, because `tf.keras.Model` does not rely on the default step, but directly uses `_train_counter`.\r\n\r\nThe reason for this is in https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L2075 which evaluates the value of the `step` variable, i.e `self._train_step` aka `self.model._train_counter` once when `on_train_begin()` or a similar method is called, but not when a new batch starts.\r\n\r\n**Describe the expected behavior**\r\nThe default step value, i.e the value returned by `tf.summary.experimental.get_step()`, should reflect the current step. As `self._train_step` is a variable it should be sufficient to just pass it directly to `tf.summary.experimental.set_step()`.\r\n\r\n**Standalone code to reproduce the issue**\r\nlog an arbitrary scalar with `tf.summary.scalar` during training but don't provide a step argument. Make sure that you created a `tf.keras.callbacks.TensorBoard` and set the `update_freq` parameter.", "comments": ["I wrote a quick hotfix which works for me:\r\n\r\n```\r\nclass TensorBoardFix(tf.keras.callbacks.TensorBoard):\r\n    \"\"\"\r\n    This fixes incorrect step values when using the TensorBoard callback with custom summary ops\r\n    \"\"\"\r\n\r\n    def on_train_begin(self, *args, **kwargs):\r\n        super(TensorBoardFix, self).on_train_begin(*args, **kwargs)\r\n        tf.summary.experimental.set_step(self._train_step)\r\n\r\n\r\n    def on_test_begin(self, *args, **kwargs):\r\n        super(TensorBoardFix, self).on_test_begin(*args, **kwargs)\r\n        tf.summary.experimental.set_step(self._val_step)\r\n```", "/cc @omalleyt12 Do you see any side effect for a PR like this?", "If you are modifying the code you might want to consider updating the documentation as well. It's currently undocumented that the `TensorBoard` callback sets the default writer. I stumbled upon this issue initially after I tried to hijack the writer from the TensorBoard callback, realizing that the class already does what I wanted.", "@sehoffmann \r\nCould you please verify with latest versions of tf and let us know if you still face the problem.", "This problem still exists in TensorFlow 2.5.0. When I use `tf.summary.*`  operations to record scalars and images I want, their steps are always 0 in TensorBoard. And the wrapper class `TensorBoardFix` mentioned by @sehoffmann works as a temporary solution with TF 2.5.0.", "> @sehoffmann\r\n> Could you please verify with latest versions of tf and let us know if you still face the problem.\r\n\r\nThis problem still exists in TensorFlow 2.6.0 , too . Can I push a PR which contains the hotfix to ```keras``` to fix it ?"]}, {"number": 43547, "title": "Cannot use Hexagon delegate in Samsung S20 ultra. Failed to fetch Hexagon NN version. ", "body": "\r\n2020-09-25 00:49:30.103 3562-3562/org.tensorflow.lite.examples.detection E/Zygote: isWhitelistProcess - Process is Whitelisted\r\n2020-09-25 00:49:30.103 3562-3562/org.tensorflow.lite.examples.detection E/Zygote: accessInfo : 1\r\n2020-09-25 00:49:30.105 3562-3562/org.tensorflow.lite.examples.detection I/mples.detectio: Late-enabling -Xcheck:jni\r\n2020-09-25 00:49:30.115 3562-3562/org.tensorflow.lite.examples.detection E/mples.detectio: Unknown bits set in runtime_flags: 0x8000\r\n2020-09-25 00:49:30.120 3562-3562/org.tensorflow.lite.examples.detection D/ActivityThread: setConscryptValidator\r\n2020-09-25 00:49:30.120 3562-3562/org.tensorflow.lite.examples.detection D/ActivityThread: setConscryptValidator - put\r\n2020-09-25 00:49:30.161 3562-3562/org.tensorflow.lite.examples.detection W/ActivityThread: Application org.tensorflow.lite.examples.detection is waiting for the debugger on port 8100...\r\n2020-09-25 00:49:30.164 3562-3562/org.tensorflow.lite.examples.detection I/System.out: Sending WAIT chunk\r\n2020-09-25 00:49:31.166 3562-3562/org.tensorflow.lite.examples.detection I/System.out: Debugger has connected\r\n2020-09-25 00:49:31.166 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:31.367 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:31.568 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:31.769 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:31.969 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:32.174 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:32.375 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:32.576 3562-3562/org.tensorflow.lite.examples.detection I/System.out: waiting for debugger to settle...\r\n2020-09-25 00:49:32.777 3562-3562/org.tensorflow.lite.examples.detection I/System.out: debugger has settled (1432)\r\n2020-09-25 00:49:32.782 3562-3562/org.tensorflow.lite.examples.detection D/Proxy: setHttpProxySystemPropertyInternal for uid 10283 The host value is null the port value is null\r\n2020-09-25 00:49:32.796 3562-3562/org.tensorflow.lite.examples.detection I/mples.detectio: The ClassLoaderContext is a special shared library.\r\n2020-09-25 00:49:33.272 3562-3562/org.tensorflow.lite.examples.detection W/SemFloatingFeature: You called API `String getString(String tag, String defaultValue)` with feature [SEC_FLOATING_FEATURE_MESSAGE_CONFIG_PACKAGE_NAME].It has been deprecated after android Q. Instead, please Use `String getString(String tag)`\r\n2020-09-25 00:49:33.347 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onCreate org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:33.509 3562-3562/org.tensorflow.lite.examples.detection I/MultiWindowDecorSupport: [INFO] isPopOver = false\r\n2020-09-25 00:49:33.510 3562-3562/org.tensorflow.lite.examples.detection I/MultiWindowDecorSupport: updateCaptionType >> DecorView@4870239[], isFloating: false, isApplication: true, hasWindowDecorCaption: false, hasWindowControllerCallback: true\r\n2020-09-25 00:49:33.512 3562-3562/org.tensorflow.lite.examples.detection D/MultiWindowDecorSupport: setCaptionType = 0, DecorView = DecorView@4870239[]\r\n2020-09-25 00:49:33.632 3562-3562/org.tensorflow.lite.examples.detection W/mples.detectio: Accessing hidden method Landroid/view/View;->computeFitSystemWindows(Landroid/graphics/Rect;Landroid/graphics/Rect;)Z (greylist, reflection, allowed)\r\n2020-09-25 00:49:33.634 3562-3562/org.tensorflow.lite.examples.detection W/mples.detectio: Accessing hidden method Landroid/view/ViewGroup;->makeOptionalFitsSystemWindows()V (greylist, reflection, allowed)\r\n2020-09-25 00:49:33.722 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[800x128], ColorType=4, SampleSize=1\r\n2020-09-25 00:49:33.777 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[60x18], ColorType=4, SampleSize=1\r\n2020-09-25 00:49:34.189 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[81x81], ColorType=4, SampleSize=1\r\n2020-09-25 00:49:34.195 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[81x81], ColorType=4, SampleSize=1\r\n2020-09-25 00:49:34.223 3562-3562/org.tensorflow.lite.examples.detection D/skia: ImageDecoder : EncodedFormat=4, Size=[71x48], ColorType=4, SampleSize=1\r\n2020-09-25 00:49:34.319 3562-3562/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Connecting to camera service\r\n2020-09-25 00:49:34.325 3562-3562/org.tensorflow.lite.examples.detection D/VendorTagDescriptor: addVendorDescriptor: vendor tag id 3854507339 added\r\n2020-09-25 00:49:34.345 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 0 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client org.tensorflow.lite.examples.detection API Level 2\r\n2020-09-25 00:49:34.350 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 1 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.356 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 2 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.362 3562-15963/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 20 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.367 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 21 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.373 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 23 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.380 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 3 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.385 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 4 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.390 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 40 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.396 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 41 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.401 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 5 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.407 3562-15178/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 52 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.413 3562-15156/org.tensorflow.lite.examples.detection I/CameraManagerGlobal: Camera 80 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\n2020-09-25 00:49:34.560 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraActivity: Camera API lv2?: true\r\n2020-09-25 00:49:34.639 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onStart org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:34.650 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onResume org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:34.689 3562-15846/org.tensorflow.lite.examples.detection D/NativeCustomFrequencyManager: [NativeCFMS] BpCustomFrequencyManager::BpCustomFrequencyManager()\r\n2020-09-25 00:49:34.695 3562-3562/org.tensorflow.lite.examples.detection D/ViewRootImpl@e74cbb7[DetectorActivity]: ThreadedRenderer.create() translucent=false\r\n2020-09-25 00:49:34.696 3562-3562/org.tensorflow.lite.examples.detection E/ViewRootImpl@e74cbb7[DetectorActivity]: shouldSkipPokeDrawLockIfNeeded, Surface is not valid.\r\n2020-09-25 00:49:34.715 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: setView = com.android.internal.policy.DecorView@4870239 TM=true MM=false\r\n2020-09-25 00:49:34.729 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onPause org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:34.749 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: stopped(true) old=false\r\n2020-09-25 00:49:34.753 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onStop org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:34.931 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: Relayout returned: old=(0,0,1080,2400) new=(0,0,1080,2009) req=(0,0)8 dur=21 res=0x100001 s={false 0} ch=false\r\n2020-09-25 00:49:34.953 3562-3562/org.tensorflow.lite.examples.detection E/ViewRootImpl@e74cbb7[DetectorActivity]: shouldSkipPokeDrawLockIfNeeded, Surface is not valid.\r\n2020-09-25 00:49:38.128 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: Relayout returned: old=(0,0,1080,2009) new=(0,0,1080,2009) req=(0,0)4 dur=8 res=0x100001 s={false 0} ch=false\r\n2020-09-25 00:49:38.130 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: stopped(false) old=true\r\n2020-09-25 00:49:38.131 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onStart org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:38.133 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: stopped(false) old=false\r\n2020-09-25 00:49:38.133 3562-3562/org.tensorflow.lite.examples.detection D/tensorflow: CameraActivity: onResume org.tensorflow.lite.examples.detection.DetectorActivity@5dbc894\r\n2020-09-25 00:49:38.171 3562-3562/org.tensorflow.lite.examples.detection D/Surface: Create surface, surface=0x7d32b36000\r\n2020-09-25 00:49:38.172 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: Relayout returned: old=(0,0,1080,2009) new=(0,0,1080,2009) req=(1080,2009)0 dur=7 res=0x100007 s={true 537721528320} ch=true\r\n2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection D/OpenGLRenderer: createReliableSurface : 0x7c9517b1c0(0x7d32b36000)\r\n2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: QUALCOMM build                   : 48175c6, I33ebe07a9a\r\n    Build Date                       : 07/27/20\r\n    OpenGL ES Shader Compiler Version: EV031.29.00.09\r\n    Local Branch                     : \r\n    Remote Branch                    : refs/tags/AU_LINUX_ANDROID_LA.UM.8.12.C1.10.00.00.649.113\r\n    Remote Branch                    : NONE\r\n    Reconstruct Branch               : NOTHING\r\n2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: Build Config                     : S P 8.0.12 AArch64\r\n2020-09-25 00:49:38.173 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: Driver Path                      : /vendor/lib64/egl/libGLESv2_adreno.so\r\n2020-09-25 00:49:38.173 3562-3562/org.tensorflow.lite.examples.detection D/ViewRootImpl@e74cbb7[DetectorActivity]: mThreadedRenderer.initialize() mSurface={isValid=true 537721528320} hwInitialized=true\r\n2020-09-25 00:49:38.178 3562-15846/org.tensorflow.lite.examples.detection I/AdrenoGLES-0: PFP: 0x016dd087, ME: 0x00000000\r\n2020-09-25 00:49:38.187 3562-15846/org.tensorflow.lite.examples.detection D/OpenGLRenderer: makeCurrent EglSurface : 0x0 -> 0x0\r\n2020-09-25 00:49:38.190 3562-15846/org.tensorflow.lite.examples.detection D/OpenGLRenderer: eglCreateWindowSurface : 0x7c95175d00\r\n2020-09-25 00:49:38.201 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: [DrawPending] drawPending(1) 1 android.view.ViewRootImpl.reportNextDraw:9955 android.view.ViewRootImpl.performTraversals:3332 android.view.ViewRootImpl.doTraversal:2225 \r\n2020-09-25 00:49:38.201 3562-3562/org.tensorflow.lite.examples.detection I/ViewRootImpl@e74cbb7[DetectorActivity]: [DrawPending] performDraw() Waiting asnyc report\r\n2020-09-25 00:49:38.206 3562-3562/org.tensorflow.lite.examples.detection D/BufferQueueProducer: Create producer, producer=0x7c9af71000\r\n2020-09-25 00:49:38.206 3562-3562/org.tensorflow.lite.examples.detection D/Surface: Create surface, surface=0x7ca3b80000\r\n2020-09-25 00:49:38.246 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480\r\n2020-09-25 00:49:38.248 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Valid preview sizes: [4000x3000, 4000x2252, 4000x1800, 2992x2992, 2400x1080, 1920x864, 1920x824, 3840x2160, 1920x1080, 1440x1080, 1088x1088, 1280x720, 960x720, 720x480, 640x480]\r\n2020-09-25 00:49:38.248 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Rejected preview sizes: [640x360, 352x288, 320x240, 256x144, 176x144]\r\n2020-09-25 00:49:38.248 3562-3562/org.tensorflow.lite.examples.detection I/tensorflow: CameraConnectionFragment: Exact size match found.\r\n2020-09-25 00:49:44.005 3562-3562/org.tensorflow.lite.examples.detection I/org.tensorflow.lite.examples.detection: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2022: fastrpc_apps_user_init done\r\n2020-09-25 00:49:44.006 3562-3562/org.tensorflow.lite.examples.detection W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n2020-09-25 00:49:44.006 3562-3562/org.tensorflow.lite.examples.detection I/tflite: Hexagon Delegate is not supported.\r\n", "comments": ["Are you using v1.20? See https://www.tensorflow.org/lite/performance/hexagon_delegate", "Yes. Tried on both v1.17 and 1.20. ", "Can you verify that the shared libraries are actually pushed to the device as expected ?\r\nCan you capture the full logcat and share it ?\r\n\r\nThanks", "verified with the below logs..\r\n LOGGER.i(\"Lib location: %s\", this.getApplicationInfo().nativeLibraryDir); \r\nLibs are present in the path\r\n\r\nAlso below step output in S20 Ultra is (Please see if this is hard coded)\r\n\r\nVerify if your device indeed has a supported SoC. Run adb shell cat /proc/cpuinfo | grep Hardware and see if it returns something like \"Hardware : Qualcomm Technologies, Inc MSMXXXX\".\r\n\r\nHardware : Qualcomm Technologies, Inc Kona\r\n\r\n\r\n\r\nFull logs attached\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5319679/log.txt)", "Thanks\r\nIt looks like some SElinux policy that disable the access to the DSP. \r\n\r\nLet us verify, can you check the below questions\r\n\r\nA few more things:\r\n- Which android version and phone model you're using in the latest run\r\n- In the initialize line for the delegate\r\n(e.g.) hexagonDelegate = new HexagonDelegate(activity);\r\nCan you print\r\nactivity.getApplicationInfo().nativeLibraryDir\r\nand check that the shared lib files are on the device under the same path.\r\n\r\nadb shell ls -al <PATH_PRINTED_FROM_ABOVE>\r\n\r\nIf you can paste the results in the reply will be good.\r\n", "\r\nWhich android version and phone model you're using in the latest run\r\n\r\n- Android 10\r\n\r\n\r\nIn the initialize line for the delegate\r\n(e.g.) hexagonDelegate = new HexagonDelegate(activity);\r\nCan you print\r\nactivity.getApplicationInfo().nativeLibraryDir\r\n\r\n- This is already verified. Shared lib files are present in the path LOGGER.i(\"Lib location: %s\", this.getApplicationInfo().nativeLibraryDir);\r\nLibs are present in the path\r\n", "I have the exact same behaviour on OnePlus 7t after the latest android 10 update (before that the hexagon delegate worked fine). If this can't be resolved with Qualcomm and/or OEMs, the hexagon delegate becomes pretty useless, which would be a pity after all the excellent work that has been put into it.", "@neeraj-partha @aki65  Thanks for verifying that the issue happens with android 10. Will check and update the issue.\r\nThanks", "@karimnosseir Just to avoid any misunderstandings: The problem did not come with android 10 (I had that before), but with the latest OnePlus update from October 2020 (they update every two months). So it may well be related to some exaggerated security measures because of the DSP \"vulnerabilities\" detected lately.", "Thanks a lot @aki65  for confirming. We will check and update the issue.\r\nThanks a lot for the update and more information.", "Looks like some OEMs might have locked down DSP access after the recent vulnerabilities as you figured. There are plans underway to mitigate some of this going forward, but we haven't got a timeline yet.\r\n\r\nFor the time being, does using the NNAPI delegate work for your use case?", "@neeraj-partha To confirm your case, did the Hexagon delegate stop working on your Samsung S20 *after* some software/OS update, or it never worked from the beginning?", "> @neeraj-partha To confirm your case, did the Hexagon delegate stop working on your Samsung S20 *after* some software/OS update, or it never worked from the beginning?\n\nIt never worked for me from the beginning", "OEMs and the chipset manufacturers are fooling around.\n\nDSP is accessible to only OEMs from their apps..i can see QC DLC models are loading and executing on DSP from say camera app.\n\nEntire definition of OS- resource manager is to be reviewed :). \nI think google should ban such OEMs who bypass the underlying hardware resources and keep it for themselves. Not exposing to third party developers", "@srjoglekar246 I already tried running my model with the NNAPI delegate (using benchmark_model):\r\n1) If I choose \"nnapi-reference\" as accelerator, it works (but of course very slow), so I assume there is no error in the model or on the tensorflow side\r\n2) But if I choose \"qti-dsp\" as accelerator, it fails with ANEURALNETWORKS_OP_FAILED\r\n\r\nThis occured already before the critical OS update, so can't be caused by any DSP locking. Since my model is very simple (only a few convolutions and fully-connected layers), my guess is that Qualcomm's NNAPI driver can't handle per-channel quantizations. But I can't do without them, since they are the only way to get the necessary accuracy (great work of you guys, by the way).\r\n\r\nIn my opinion my use case is really common: simple model, straightforward post-training quantization ... but currently no way to run it on a Qualcomm device (other than on cpu). Considering Qualcomm's market share I think that this DSP issue is really critical for android AI development.", "Hey @miaowang14 , does the NNAPI delegate not support per-channel quantized tensors? Please see the comment(s) above for context/use-case.", "Any updates on this critical issue ?", "@aki65 We are waiting for some updates from partners. Meanwhile, could you point me to which post-training quantization you are using? Your model and/or code to convert the model would be helpful.", "@srjoglekar246 The model was quantized by:\r\n...\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_int8_model = converter.convert()\r\n...\r\nwith a suitably defined representative_dataset_gen. The attached zip contains the .tflite file.\r\n[sample.zip](https://github.com/tensorflow/tensorflow/files/5539328/sample.zip)\r\n\r\n", "@aki65 That seems like a standard per-channel quantized model.\r\nOut of curiosity, have you tried using the GPU backend? Or does it not work for your use-case for some reason?", "@srjoglekar246 I also tried the GPU backend. It works, but is even slower than running the model on CPU (not totally unexpected, since the GPU can't profit from the quantization).", "I believe qti-dsp supports per-channel quantization since Android 10 (driver version 1.2+), as long as the model is fully quantized.\r\n\r\nThe model use float input and output, and has quantize and dequantize ops which causes the qti-dsp to reject the model.\r\nCould you try NNAPI delegate without specifying any accelerator?  (if no accelerator specified, NNAPI runtime will try to partition the model and accelerate as much ops as possible on available accelerators.)", "@miaowang14 If I don't specify an accelerator, it seems to use nnapi-reference, at least the inference time is the same: around 1050 ms. For comparison: The inference time without NNAPI (i.e. running on cpu) is 17 ms.", "@miaowang14 For further information, here is the debug output when running the model with accelerator=qti-dsp\r\n[logcat.txt](https://github.com/tensorflow/tensorflow/files/5542717/logcat.txt)\r\n\r\n", "Thanks for the error logs. It looks like a driver issue on the device you are using.\r\nDo you have the NNAPI driver version? (you can obtain it by \"adb logcat | grep nnhal\" immediately after rebooting the device)\r\n\r\nOn Pixel 4a (5G) running Android 11, I am seeing the following results\r\n\r\n> $ adb shell /data/local/tmp/benchmark_tflite_model --graph=/data/local/tmp/sample.tflite --use_nnapi=true  --nnapi_accelerator_name=qti-dsp\r\ncan't determine number of CPU cores: assuming 4\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [/data/local/tmp/sample.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [qti-dsp]\r\nNNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,nnapi-reference]\r\nLoaded model /data/local/tmp/sample.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nExplicitly applied NNAPI delegate, and the model graph will be completely executed by the delegate.\r\nThe input model file size (MB): 10.6132\r\nInitialized session in 371.513ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=40 first=12629 curr=12843 min=10196 max=16003 avg=12749 std=1191\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=77 first=11070 curr=12770 min=11070 max=16088 avg=12999.5 std=942\r\nInference timings in us: Init: 371513, First inference: 12629, Warmup (avg): 12749, Inference (avg): 12999.5\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=4.94141 overall=4.94141\r\n", "@miaowang14 Output is:\r\n\r\n11-15 18:42:11.995   882   882 I android.hardware.neuralnetworks@1.2-service: nnhal main 1.2-09", "@aki65, thanks. Looking at the partner feedback, this seems to be a known issue and has been fixed in 1.2-18 or newer drivers. ", "@miaowang14 Thank you very much for investigating this. But the existence of a fix doesn't really help if it never reaches OEMs. After all, OnePlus is one of the most eager companies regarding updates. And indeed, the last OS update for my device is only a few weeks old. If it contains such an outdated driver, will I ever get the fixed version ?", "@aki65, I completely agree that updatability of vendor bits is a major pain point. The NNAPI team is currently focused on solving this for developers and hopefully we can share more details early next year.\r\n\r\nThat being said, unfortunately we may have to wait for OEMs to roll-out the driver updates (potentially with Android 11 update) at this moment, or limit the NNAPI offload to only 1.2-18 drivers or higher with [ANeuralNetworksDevice_getVersion](https://developer.android.com/ndk/reference/group/neural-networks#aneuralnetworksdevice_getversion).", "The analysis of this NNAPI problem clearly highlights the importance of the hexagon delegate as it can be updated by the user and it can bypass driver issues (until their solution is found and distributed). So this leads back to the question originating this thread: Why is the hexagon delegate not working anymore on up-to-date Samsung and OnePlus devices (and maybe others), and what can be done about this ?", "@srjoglekar246 \r\n\r\n> We are waiting for some updates from partners ...\r\n\r\nStill no updates on the hexagon delegate issue ?\r\n\r\n\r\n"]}, {"number": 43535, "title": "Problem with custom grad with multiple external variables", "body": "**Describe the current behavior**\r\n\r\nThe \"variables\" argument in custom grad seems to be buggy. For example this code, which is a complexified version of the \"custom_grad\" example in the doc:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nm=1000\r\n\r\nvarr=tf.Variable(200.0)\r\nvarr3=tf.Variable(200.0)\r\nvarr2=tf.constant(1.0)\r\n\r\n@tf.function\r\n@tf.custom_gradient\r\ndef log1pexp(x1, x2):\r\n  #some basic function fluff \r\n  e=tf.exp(x1)\r\n  e=tf.math.log(1 + e)\r\n  for i in range(m):\r\n  \te += 0.0*tf.exp(x1) + 1.0\r\n\r\n  #here comes the problem\r\n  print(\"tracing\")\r\n  def grad(dy, dz, variables=[varr, varr3]):\r\n    return [dy * (1 - 1 / (1 + e)) + 0 * varr2, tf.constant(0.0)], [0.0 * varr, 0.0*varr3]\r\n\r\n  return [e+varr, varr3], grad#\r\n\r\nx = tf.constant(1.0)\r\nwith tf.GradientTape() as g:\r\n  g.watch(x)\r\n  y=log1pexp(x,0.0)\r\n\r\ndy_dx = g.gradient(y, x)\r\nprint (dy_dx)\r\n```\r\n\r\nLeads to the error:\r\n``ValueError: Must return gradient for each variable from @custom_gradient grad_fn.``\r\n\r\nWhich i seem to be doing (log1pexp has 2 inputs, outputs and two external variables, so the return size should be  list of size 2 each ?)\r\n\r\nHowever if I change line 23 to \r\n```    return [dy * (1 - 1 / (1 + e)) + 0 * varr2, tf.constant(0.0)], [0.0 * varr] ```\r\n\r\nThen it doesn't give an error, which doesn't make sense to me since in that case, grad_vars only contain gradient info for ONE of the external variables I have registered in the variables parameters.\r\n\r\nAlso it doesn't care about varr2 at all ...\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code above should not give error?\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee above\r\n\r\n**Other info / logs** \r\n\r\nBasic ubuntu 20. error with tf 2.2 and 2.3", "comments": ["The issue is that `actual_grad_fn` receive only 1 `variable` (in this case `e+varr`) but two `variable_grads`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e798106686903c077d6f082d637f0971da31f20c/tensorflow/python/ops/custom_gradient.py#L507-L508", "Are you telling me it's a tf problem or a problem in my code ?\r\n\r\nI'm not sure what I should read in the two lines: it seems logiclal. According to my code the two \"variables\" actual_grad_fn receive should be **varr** and **varr3**, (i'm not sure why you said it only receive \"e+varr\" since it's an output of fn) and indeed there 's only one i guess", "Yes It was just a typo I meant `varr`. \r\nEdit this file locally and try to add `print(variables)` before the mentioned condition", "OK so I've inspected with the code:\r\n```\r\n  variables_in_tape = frozenset([\r\n    v.ref() for v in variable_watcher.watched_variables()\r\n  ])\r\n```\r\nand I found something:\r\n\r\nIn the above code, varr3 **is not registered in variables_in_tape** seemingly because **it's used as an \"identity function\" and there's no operation involving it**\r\n\r\nFor example if I change my function to return this (just adding 0.0 to varr3 so the function is essentially the same) :\r\n```return [e+varr, 0.0+varr3], grad ```\r\n\r\nThen tensorflow will register varr3, and as expected will require me to provide gradient for both varr and varr3.\r\n\r\nI'm pretty sure this is not expected behavior from the tensorflow team ... not sure how to escalate but you guys should check this out.\r\n\r\nHere's code to test it\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nm=1000\r\n\r\nvarr=tf.Variable(200.0)\r\nvarr3=tf.Variable(200.0)\r\nvarr2=tf.constant(1.0)\r\n\r\n@tf.function\r\n@tf.custom_gradient\r\ndef log1pexp(x1, x2):\r\n  \r\n  e=tf.exp(x1)\r\n  e=tf.math.log(1 + e)\r\n  \r\n  for i in range(m):\r\n  \te += 0.0*tf.exp(x1) + 1.0\r\n\r\n  print(\"tracing\")\r\n  def grad(dy, dz, variables=[varr, varr3]):\r\n    return [dy * (1 - 1 / (1 + e)) + 0 * varr2, tf.constant(0.0)], [0.0 * varr, 0.0 * varr3]#[0.0 * varr, 0.0 * varr3]\r\n\r\n  return [e+varr, 0.0+varr3], grad#[e+varr, varr3]\r\n\r\n\r\n\r\n#Debug.. works only if tf.function and tf.custom_grad are commented\r\n#change with the commentaed changes above to see the difference...\r\ncustom_gradd=True\r\nif not custom_gradd:\r\n  from tensorflow.python.util import tf_inspect\r\n  from tensorflow.python.eager import tape as tape_lib\r\n  with tape_lib.VariableWatcher() as variable_watcher:\r\n    result, grad_fn = log1pexp(0.0,0.0)\r\n\r\n  variables_in_tape = frozenset([\r\n    v.ref() for v in variable_watcher.watched_variables()\r\n  ])\r\n\r\n  print (\"************\")\r\n  print (variables_in_tape)\r\n  print (\"************\")\r\n  print(tf_inspect.getfullargspec(log1pexp(0.0,0.0)[1]))\r\n\r\n\r\nx = tf.constant(1.0)\r\n\r\n#t0 = time.time()\r\n#print(log1pexp(x,0.0))\r\n#t1 = time.time()\r\n#print (\"Tracing run:\", t1-t0)\r\n#\r\n#t0 = time.time()\r\n#print(log1pexp(x,0.0))\r\n#t1 = time.time()\r\n#print (\"Traced run:\", t1-t0)\r\n#\r\n#m=2000\r\n#varr.assign(400.0)\r\n#t0 = time.time()\r\n#print(log1pexp(x,0.0))\r\n#t1 = time.time()\r\n#print (\"Modified Traced run:\", t1-t0)\r\n\r\n\r\nwith tf.GradientTape() as g:\r\n  g.watch(x)\r\n  y=log1pexp(x,0.0)\r\n\r\ndy_dx = g.gradient(y, x) # Will compute to 6.0\r\nprint (dy_dx)\r\n\r\n#x = tf.constant(3.0)\r\n#with tf.GradientTape() as g:\r\n#  g.watch(x)\r\n#  y = x * x\r\n#dy_dx = g.gradient(y, x) # Will compute to 6.0\r\n```\r\n\r\n\r\n", "/cc @rohan100jain", "@RochMollero \r\nI ran your code on tf 2.3 and nightly, i do not face the error reported above, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b5bdc253d11a598e40dd13c8ab36a8bf/untitled418.ipynb).", "@Saduf2019 The user is asking something different.\n\n /cc @jaingaurav @wangpengmit can you give a feedback here?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @RochMollero\r\n> I ran your code on tf 2.3 and nightly, i do not face the error reported above, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b5bdc253d11a598e40dd13c8ab36a8bf/untitled418.ipynb).\r\n\r\nprecisely because you ran the working version only. the not working version is when you set \r\n\r\n return [e+varr, varr3],\r\n\r\n(without the 0.0*)", "You're right here. We have some logic that detects what variables are used within a custom_gradient function and that only works when there is actual variable read (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/pywrap_tfe_src.cc#L3294) and not when a variable is simply passed through to the output. Right now we don't have plans to fix this immediately, so I'd recommend just doing 1.0*varr or something so that a read is triggered. \r\n\r\nOn the other hand, contributions are welcome to fix this! One suggestion would be to look at the outputs from running the function (https://github.com/tensorflow/tensorflow/blob/679988320d2f11211cd24ee5a2b18ff52bc04ad7/tensorflow/python/ops/custom_gradient.py#L330) and see if there are any variables we missed and add those to the list. ", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210528, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/3e0b70f9f97ab6ecb097c263851f8a59/43535.ipynb). Thanks!"]}, {"number": 43510, "title": "keyword benchmark broken for bluepill with TAGS=cmsis-nn", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): c8109b89a2bf7fce60680682794f6ff4ca25de1b\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): bluepill\r\n\r\nCommand:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=bluepill TAGS=cmsis-nn keyword_benchmark\r\n```\r\n\r\nError:\r\n```\r\ntensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c: In function 'arm_elementwise_mul_s8':\r\ntensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c:178:21: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]\r\n   while (loop_count > 0U)\r\n                     ^\r\ncc1: all warnings being treated as errors\r\n```\r\n\r\nThis was not caught by our CI system because we do not build the benchmarks. PR #43509 should allow us to catch these issues before they are merged.\r\n\r\n", "comments": ["Hi, I've just faced this same issue building microlite after cloning the master [f1cef23](https://github.com/tensorflow/tensorflow/commit/f1cef2358b2979c42b524b08fc61ab9922502c99). \r\n\r\nI used the command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\r\n    TARGET=cortex_m_generic \\\r\n    TARGET_ARCH=cortex-m4+fp \\\r\n    TARGET_TOOLCHAIN_ROOT=/opt/gcc-arm-none-eabi-9-2020-q2-update/bin/ \\\r\n    OPTIMIZED_KERNEL_DIR=cmsis_nn microlite\r\n```\r\n\r\nIt returned the error (shortened for simplicity):\r\n```\r\ntensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c: In function 'arm_elementwise_mul_s8':\r\ntensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c:114:21: error: comparison of integer expressions of different signedness: 'int32_t' {aka 'long int'} and 'unsigned int' [-Werror=sign-compare]\r\n  114 |   while (loop_count > 0U)\r\n      |                     ^\r\ntensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c:178:21: error: comparison of integer expressions of different signedness: 'int32_t' {aka 'long int'} and 'unsigned int' [-Werror=sign-compare]\r\n  178 |   while (loop_count > 0U)\r\n      |                     ^\r\ntensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c: At top level:\r\ncc1: error: unrecognized command line option '-Wno-unused-private-field' [-Werror]\r\ncc1: all warnings being treated as errors\r\ntensorflow/lite/micro/tools/make/Makefile:676: recipe for target 'tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m4+fp_default/obj/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.o' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m4+fp_default/obj/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.o] Error 1\r\n```\r\n\r\nWhich branch or tag would you suggest to use to avoid this issue?\r\n\r\nThank you!", "After a quick look, it seems like `tensorflow/lite/micro/tools/make/Makefile` pulls a \"broken\" version of CMSIS. I really did not investigate how and which version it is pulled, but I can see in my localhost that the pulled version specifies `int32_t loop_count;` while the [current master](https://github.com/ARM-software/CMSIS_5/blob/master/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c#L65) specifies [`uint32_t loop_count;`](https://github.com/ARM-software/CMSIS_5/blob/20285262657d1b482d132d20d755c8c330d55c1f/CMSIS/NN/Source/BasicMathFunctions/arm_elementwise_mul_s8.c#L65).\r\n\r\nI think I should be able to overcome this by building my own version of CMSIS from master and providing the `CMSIS_PATH` parameter to make. It would be nice if your CI could cross-check the compatibility though.\r\n\r\nThanks!", "I can confirm that changing `int32_t loop_count` to `uint32_t loop_count` solves the problem."]}]