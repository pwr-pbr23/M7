[{"number": 42470, "title": "Fix doc layout error for regularizer", "body": "Without the additional blank line, the doc shows as follows\r\n![image](https://user-images.githubusercontent.com/5104719/90545959-46301680-e13e-11ea-93e7-a39e8224af55.png)\r\n", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, if possible please include more such changes in a single PR.Thank you\r\nCC @mihaimaruseac @chanshah"]}, {"number": 42469, "title": "Ported TFLM Micro Speech example to CEVA-DSP SP500", "body": "This is a port of the Micro Speech example to CEVA-DSP SP500 processor.\r\n", "comments": ["@yair-ehrenwald Can you please resolve conflicts? Thanks!", "> @yair-ehrenwald Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned  Done, thanks :)\r\n", "@yair-ehrenwald Can you please resolve conflicts? Thanks!", "> @yair-ehrenwald Can you please resolve conflicts? Thanks!\r\n\r\nDone :)\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@yair-ehrenwald  Any update on this PR? Please. Thanks!", "@yair-ehrenwald Any update on this PR? Please. Thanks!", "It has been 29 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 42468, "title": "Compiler crash with clang 7.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): LLVM Clang 7.1.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nWhen building TF Micro I get a compiler crash on micro_allocator.cc (commit  d5ed5f9).\r\n[dump.txt](https://github.com/tensorflow/tensorflow/files/5091697/dump.txt)\r\n(See attached dump)\r\nThe issue apparently is unstable support in clang 7.1 for flexible members.\r\n\r\ndefining data[] as data[0] in TfLiteIntArray definition solves the issue.\r\nSee PR:\r\nhttps://github.com/tensorflow/tensorflow/pull/42464\r\n\r\n\r\n\r\n\r\n", "comments": ["Copying some communication over email into this github issue:\r\n\r\nHi Yair,\r\n\r\nWe don't have an easy way to reproduce your error, so could you try one of these two fixes at your end and send in a PR if either of them work:\r\n\r\n * Add an appropriate check for your compiler version / platform in tensorflow/lite/c/common.h similar to [this commit](https://github.com/tensorflow/tensorflow/commit/481aec85d669dad80f8d8e0c53e03c45f956959d).\r\n\r\n * If that doesn't work, try changing the initialization of the offending line to const TfLiteIntArray kZeroLengthIntArray = {0};\r\n   * This bit of initialization has changed over different commits ([1](https://github.com/tensorflow/tensorflow/commit/076bbc5edfe655299b006b3c1b2c6281d330d638#diff-6dace628e5372fb3c3fbb76e0fd9d37aR51), [2](https://github.com/tensorflow/tensorflow/commit/290487b03ed7a9fa78af6faa2d8c19f7e5fde30e#diff-6dace628e5372fb3c3fbb76e0fd9d37aR56), [3](https://github.com/tensorflow/tensorflow/commit/c29d6434bae6680039ca4c8b9aaf6dd30dae4c62#diff-6dace628e5372fb3c3fbb76e0fd9d37aR53)) so if option 1 does the trick for you, that would likely be more future-proof.\r\n\r\nThanks,\r\nAdvait\r\n"]}, {"number": 42467, "title": "no kernel image is available for execution on the device", "body": "I've installed TensorFlow 2.3.0 on windows 10 but cant run any python scrips contain TensorFlow codes!\r\nGPU card : NVIDIA 960m\r\nOS : Windows 10\r\nCuda : 10.1\r\ncudnn : 7.6.5.32\r\npython: 3.7.7\r\n\r\nthis is log output when I run script :\r\n ```\r\npy main.py\r\n2020-08-18 21:19:56.898389: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-18 21:19:59.658108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-18 21:20:00.194181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-18 21:20:00.207551: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-18 21:20:00.213582: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-18 21:20:00.218522: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-18 21:20:00.224225: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-18 21:20:00.236176: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-18 21:20:00.244819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-18 21:20:00.251416: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-18 21:20:00.257794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-18 21:20:00.263340: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-18 21:20:00.286389: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e0eb7cb660 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-18 21:20:00.297402: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-18 21:20:00.302999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-18 21:20:00.317067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-18 21:20:00.323311: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-18 21:20:00.329936: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-18 21:20:00.335930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-18 21:20:00.341258: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-18 21:20:00.348285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-18 21:20:00.354092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-18 21:20:00.360523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-18 21:20:00.439805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-18 21:20:00.447079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-08-18 21:20:00.452262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-18 21:20:00.456464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3121 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-18 21:20:00.480959: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e0eb7ca5e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-18 21:20:00.493404: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\n2020-08-18 21:20:00.797182: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\n\r\n```\r\nI've had this problem since I tried install TensorFlow version +2 , If I install TensorFlow 15.3.1 I wont have this problem !!!\r\nbut since TensorFlow 1+ wont have supported next year I want to install TensorFlow 2+.\r\n", "comments": ["@aligoglos \r\n\r\nPlease, provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "this is code I executed :\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nbase_model = keras.applications.ResNet50(include_top=False,\r\n        weights='D:/Deep/_Deep_Models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\n        )\r\n```\r\nI didn't do any other commands, just `py main.py`.", "I have different `cuda ` in my system it could make problem for `TF`?", "Just wanted to mention that we get the same \"_no kernel image is available for execution on the device_\" error with TensorFlow 2.3.0 on our NVidia 940MX cards (which are also compute capability 5.0, similar to author's 960M) in both precompiled python and c_api libraries. However, the same code is running fine on NVidia 1080Ti (compute capability 6.1) and 2080Ti (compute capability 7.5) cards.\r\n\r\nWe figured that it might be related to this line in the 2.3.0 release notes:\r\n>TF 2.3 includes PTX kernels only for compute capability 7.0 to reduce the TF pip binary size. Earlier releases included PTX for a variety of older compute capabilities.", "> TF 2.3 includes PTX kernels only for compute capability 7.0 to reduce the TF pip binary size. Earlier releases included PTX for a variety of older compute capabilities.\r\n\r\nYes.  See [here](https://www.tensorflow.org/install/gpu).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42467\">No</a>\n", "Any general instruction to get the old PTX kernels back?\r\n\r\nEDIT: [Found it here](https://github.com/tensorflow/tensorflow/issues/41990#issuecomment-670626077) from @sanjoy. I read around and it seems a full discussion about this PTX issue is more complete there.\r\n\r\nThe easy way: Install tf-nightly.\r\n\r\nThe hard way: Compile from source, while adjusting the Compute Capability flag.", "Yup, tf-nightly should work on cards with CC 5.0 (and so would TF 2.4).", "It looks like you're also not supporting CC8 (the A100s) yet?", "> It looks like you're also not supporting CC8 (the A100s) yet?\r\n\r\nA100 (CC8) should be supported on tf-nightly.  Please let us know if that's not the case.", "It's been working great using tf-nightly, thanks!"]}, {"number": 42466, "title": "_compute_causal_padding() missing 1 required positional argument: 'inputs'", "body": "This is from issue #42453.\r\n\r\n```\r\nTypeError: _compute_causal_padding() missing 1 required positional argument: 'inputs'\r\n```", "comments": ["Anybody here?  @gbaned ", "Ping, this is a one line fix for a crash. PTAL!", "ping @gbaned "]}, {"number": 42465, "title": "Using a model with LeakyRelu in Tensorflow lite micro not supported?", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nI created a CNN model for analyzing a sensor grid that uses LeakyReLU.  Now I've been tasked to make the trained model work on a microcontroller.  The model loads fine ( after converting it etc. ), but as soon as I have it instatiate the micro interpreter it tells me that LeakyRelu is not supported mainly by saying \"\r\nDidn't find op for builtin opcode 'LEAKY_RELU' version '1'\r\nFailed to get registration from op code LEAKY_RELU\r\n\"\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nThe following is the setup code I have so far\r\n\r\n    static tflite::MicroErrorReporter micro_error_reporter;\r\n    mErrorReporter = &micro_error_reporter;\r\n    \r\n    mModel = tflite::GetModel(model_kara_cnn_tflite);\r\n    if (mModel->version() != TFLITE_SCHEMA_VERSION) {\r\n      TF_LITE_REPORT_ERROR(mErrorReporter, \"Model provided is schema version %d not equal to supported version %d.\", mModel->version(), TFLITE_SCHEMA_VERSION);\r\n      return false;\r\n    }\r\n    \r\n    constexpr int tensor_arena_size = 16 * 1024;\r\n    uint8_t tensor_arena[tensor_arena_size];\r\n    tflite::MicroMutableOpResolver<5> resolver;\r\n    resolver.AddConv2D();\r\n    resolver.AddFullyConnected();\r\n    resolver.AddMaxPool2D();\r\n    resolver.AddSoftmax();\r\n    resolver.AddBuiltin(tflite::BuiltinOperator_LEAKY_RELU, *tflite::ops::builtin::Register_LEAKY_RELU());\r\n\r\n    tflite::MicroInterpreter interpreter(mModel, resolver, tensor_arena, tensor_arena_size, mErrorReporter);\r\n    TfLiteStatus allocate_status = interpreter.AllocateTensors();\r\n    if (allocate_status != kTfLiteOk) {\r\n      TF_LITE_REPORT_ERROR(mErrorReporter, \"AllocateTensors() failed\");\r\n      return false;\r\n    }\r\n    \r\n    std::cout << \"Setup complete\" << std::endl;\r\n    return true;\r\n", "comments": ["Changing the model to \r\nmodel = models.Sequential([\r\n    layers.experimental.preprocessing.Rescaling(1./100, input_shape=(height, width, 1)),\r\n    layers.Conv2D(32, kernel_size=(3, 3),activation='relu'),\r\n    layers.Conv2D(64, (3, 3), activation='relu',padding='same'),\r\n    layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\r\n    layers.Conv2D(128, (3, 3), activation='relu',padding='same'),\r\n    layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\r\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n    layers.MaxPooling2D(pool_size=(2, 2),padding='same'),\r\n    layers.Flatten(),\r\n    layers.Dense(128, activation='linear'),\r\n    layers.Dense(num_classes, activation='softmax')\r\n  ])\r\n\r\nGets me past the LeakyRelu problem, but now it says\r\n\"\r\nDidn't find op for builtin opcode 'RESHAPE' version '1'\r\nFailed to get registration from op code RESHAPE\r\n\"", "So I can assume that LeakyReLU and the keras experimental preprocessing operations are not supported by tf lite micro?", "LeakyRelu is not supported in TF Lite Micro. The supported operations can be seen in the file [all_ops_resolver.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/all_ops_resolver.cc).\r\nAlso see https://www.tensorflow.org/lite/microcontrollers#limitations to know more.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "It would be great if LeakyReLU support is added."]}, {"number": 42464, "title": "Fix flexible members crash in clang 7.1 (Fixes #42468)", "body": "Fix flexible members crash in clang 7.1\r\n\r\nFixes #42468 \r\n\r\nDefining data as data[] causes compiler crash in clang 7.1 when defining\r\nconst TfLiteIntArray kZeroLengthIntArray = {0, {}};\r\nin micro_allocator.cc\r\n", "comments": ["Note for @gbaned:\r\nPer [this comment](https://github.com/tensorflow/tensorflow/pull/42464#pullrequestreview-469611582), let's wait until @yair-ehrenwald has had a chance to make a github issue and link to it from this PR before we merge it.", "this is the relevant github issue: https://github.com/tensorflow/tensorflow/issues/42468", "@yair-ehrenwald -- thanks. I have updated the PR description comment to annotate that this PR should fix the issue that you created.\r\n\r\nI think we are ready to merge."]}, {"number": 42463, "title": "Error executing quickstart noteboook", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 10\r\n-   **TensorFlow installed from (source or binary)**: conda gpu version\r\n-   **TensorFlow version (use command below)**: 2.1.0\r\n-   **Python version**: 3.7.7\r\n-   **CUDA/cuDNN version**: cudatoolkit - 10.1.243 - h74a9793_0 - anaconda\r\n                                                cudnn - 7.6.5 - cuda10.1_0 - anaconda\r\n-   **GPU model and memory**: GTX 750 TI - 2GB\r\n\r\n### Describe the problem\r\nI wanted to test the tf quick start notebook: https://tensorflow.google.cn/tutorials/quickstart/advanced and got the following error message:\r\n```UnknownError                              Traceback (most recent call last)\r\n<ipython-input-10-cdcc13267505> in <module>\r\n      9 \r\n     10   for images, labels in train_ds:\r\n---> 11     train_step(images, labels)\r\n     12 \r\n     13   for test_images, test_labels in test_ds:\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\anaconda3\\envs\\tf\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node my_model/conv2d/Conv2D (defined at <ipython-input-5-1e051998210b>:10) ]] [Op:__inference_train_step_566]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node my_model/conv2d/Conv2D:\r\n images (defined at <ipython-input-10-cdcc13267505>:11)\r\n\r\nFunction call stack:\r\ntrain_step\r\n```\r\n\r\nI installed tf, cuda and cudnn using ```conda install -c anaconda tensorflow-gpu```", "comments": ["@Fetzii \r\nPlease try limiting GPU memory growth as shown in this [guide here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and let us know if you are facing the same issue. \r\n\r\nCan you please try setting allow_growth option at the top of your code:\r\n\r\n```\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(config=config)\r\n\r\n# your code \r\n```\r\n\r\nYou may refer to similar error issues below:\r\n#41886 #36025  #41196 [link](https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in) ", "Solved issue using #36025", "@Fetzii \r\nThank you for your update, glad to hear the issue is resolved."]}, {"number": 42462, "title": "Initialize external_command_encoder_ to nil", "body": "The uninitialized value can cause other functions (e.g., Invoke()) to\r\nbehave as if the user has set an external command encoder.  Since the\r\nvalue is an invalid pointer, this causes the process to crash.", "comments": ["Thanks for the catch! why don't you change line `635` to \r\n\r\n```objectivec\r\nid<MTLComputeCommandEncoder> external_command_encoder_ = nil;\r\n```\r\n\r\ninstead?", "@jfpoole  Can you please check @teijeong's comments and keep us posted ? Thanks!", "@gbaned Updated! Let me know if you or @teijeong have any more comments."]}, {"number": 42461, "title": "Update the right logic like previous", "body": "The Arm judgment branch is different with previous, and cause the build fail on linux_aarch64.\r\n\r\nSee: https://status.openlabtesting.org/builds/builds?project=tensorflow%2Ftensorflow&job_name=tensorflow-arm64-build-daily-master\r\n\r\nThis is bring by https://github.com/tensorflow/tensorflow/commit/c3bddbddaa19aa0d82d57b4e6e9ad99b4b3d9876", "comments": ["Hi team, need you kind review,please. @jaingaurav ", "@jdduke: Could you please take a look?", "Hi @jdduke , we are trying to build tensorflow master branch on linux aarch64. just try with --copt=-DARM_NON_MOBILE\r\n\r\nhttps://logs.openlabtesting.org/logs/09/09f5609f0fd282943defd4608ee90bb6883a394b/periodic-18/tensorflow-arm64-build-daily-master/cb0607e/job-output.txt.gz\r\nBut still get failed.", "Hmm, OK, if the previous one is wrong, why this affects linux aarch64 build so much. \r\nThe below is the test cmd in the build:\r\n`bazel build --config=opt --config=noaws --config=nogcp --config=nonccl --copt=-DARM_NON_MOBILE //tensorflow/tools/pip_package:build_pip_package --local_ram_resources=10240 --local_cpu_resources=6 --verbose_failures`", "Hi @jdduke , any kind advice to help? Thanks", "@AshokBhat Do you have any idea about this?", "> @AshokBhat Do you have any idea about this?\r\nAdding myself to this thread. \r\n", "Hmm, in the failed build command, it looks like that build flag isn't propagating, as build_pip_package is a shell script target that run its own bazel build command (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/build_pip_package.sh)\r\n\r\n```\r\n/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/core/kernels/batching_util/_objs/batch_resource_base/batch_resource_base.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/batching_util/_objs/batch_resource_base/batch_resource_base.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -g0 '-march=native' -g0 '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c \r\n```\r\n\r\nGiven that we already handle iOS/Android separately, and RPi is excluded from defining IS_MOBILE_PLATFORM, I'd be OK removing this section entirely:\r\n```\r\n// Require an outside macro to tell us if we're building for Raspberry Pi or\r\n// another ARM device that's not a mobile platform.\r\n#if !defined(RASPBERRY_PI) && !defined(ARM_NON_MOBILE) && \\\r\n    !defined(PLATFORM_GOOGLE)\r\n#define IS_MOBILE_PLATFORM\r\n#endif\r\n```\r\n\r\nIf you want to update your PR we can give that a try.", "@jdduke, Thanks very much, update the PR for only removing the metioned code. Will test on my aarch64 local env. And sync the result asap here.", "@jdduke , hi, just test on the current master branch with this PR change. Yeah, it gets pass.", "@jdduke, hi, in my local env, it can pass. Now I setup a openlab job with merging this PR to test on branch. Let's see the result. Thanks for your help", "> @jdduke , hi, just test on the current master branch with this PR change. Yeah, it gets pass.\r\n\r\nSame here, I have tested this new patch and it passes the build.", "Hi, verified from us, the latest PR can solve the aarch64 build issue. Thanks\r\nhttps://status.openlabtesting.org/builds/build/40e76679f7a940acb338c066027e2148"]}, {"number": 42460, "title": "Add va_end to TF_Log", "body": "@mihaimaruseac \r\nI fogot adding `va_end` when using `va_list`", "comments": []}, {"number": 42459, "title": "Accuracy is lost after save/load", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): See below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 1909\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: Not relevant.\r\n\r\n**Describe the current behavior**\r\nWhen saving a pre-trained model and loading it again, the accuracy drops to its default value. Minimal example:\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(units=2, activation='softmax', name='output'))\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=10),\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\ndummy_data_x = [[0, 0],\r\n                [1, 0],\r\n                [0, 1],\r\n                [1, 1]]\r\ndummy_data_y = [0, 1, 0, 1]\r\nprint(model.evaluate(x=dummy_data_x, y=dummy_data_y))\r\nmodel.fit(x=dummy_data_x, y=dummy_data_y, epochs=10)\r\nprint(model.evaluate(x=dummy_data_x, y=dummy_data_y))\r\nmodel.save('test_model')\r\nmodel = tf.keras.models.load_model('test_model')\r\nprint(model.evaluate(x=dummy_data_x, y=dummy_data_y))\r\n```\r\nThe model is extremely simple (read: bad/useless) for comparison's sake. Before training it, the evaluation results in:\r\n```\r\n1/1 [==============================] - 0s 0s/step - loss: 0.9013 - accuracy: 0.5000\r\n[0.9013183116912842, 0.5]\r\n```\r\nThe loss is obviously random at first, but crucially the accuracy is 50% because it guesses 0 every time. After training, it evaluates to:\r\n```\r\n1/1 [==============================] - 0s 0s/step - loss: 0.0000e+00 - accuracy: 1.0000\r\n[0.0, 1.0]\r\n```\r\nThe loss dropped to zero and the model can perfectly interpret the data. Now I save the model to the disk and immediately load it from the same location. When I now evaluate it, I get:\r\n```\r\n1/1 [==============================] - 0s 1000us/step - loss: 0.0000e+00 - accuracy: 0.5000\r\n[0.0, 0.5]\r\n```\r\nEven though the model has the same structure, weights, and loss, and all four example inputs are evaluated correctly, TensorFlow says the accuracy is 50%, which is not true.\r\n\r\n**Describe the expected behavior**\r\nThe accuracy should remain the same when loading a previously trained and saved model.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs**\r\nSee above.\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200818`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0f47d46269897a03a6c56d227b306a52/untitled263.ipynb). Thanks!", "@ThatDockerUser This is a known [issue](https://github.com/tensorflow/tensorflow/issues/42045). Team is working on correcting it. \r\n\r\nPlease check the response from @k-w-w \r\n\r\n> This is a bug with using the sparse categorical accuracy. For now, please compile the model with metrics='sparse_categorical_accuracy' instead of just 'accuracy'.\r\n\r\nWith the above change (workaround), the results are same before saving and after loading the model back. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/6ec731f53fcabf9444c1cd430e4d5dfb/untitled263.ipynb). \r\n\r\nWe will follow the progress [with that previous issue](https://github.com/tensorflow/tensorflow/issues/42045). Thanks!\r\n", "@jvishnuvardhan Thanks for the reply. The SC-accuracy does appear to work. I'm curious, though: How is it different from \"regular\" accuracy?", "@ThatDockerUser Based on loss function you defined in the `model.compile`, under the hood TF will select appropriate `accuracy` method. In your case, it should select 'sparse_categorical_accuracy` but currently there is a known bug. It will be corrected soon. Until then please specify explicitly as mentioned above. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been fixed in commit 9d8947. Please try the latest tf-nightly if you need the fix immediately, otherwise the next official TF release will have the fix. Marking this as closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42459\">No</a>\n", "As on 20th Oct 2020. \r\nif anyone is still looking around, using python 3.610.\r\nModel.save and load_model not working for keras 2.3.1 and tensor flow 2.2.0  \r\nusing model.to_json and models.model_from_json also doesn't work. \r\nThe loaded model has definitely something missing and gives way lesser accuracy as the saved model in a different session/spyder consoles.\r\nupgrade the packages to keras 2.4.3 and tensorflow 2.3.0. together. ", "@akbaramed As mentioned above, the bug was corrected and we can no longer face the issue when you use `tf-nightly`. If you have `Tensorflow 2.3`, then please follow the workaround  `metrics='sparse_categorical_accuracy' instead of just 'accuracy'.\r\n\r\nIn the near future, stable `TF2.4` will be released. Thanks!\r\n\r\nIf this is still an issue with `tf-nightly`, please provide a standalone code to reproduce the issue. Thanks!"]}, {"number": 42458, "title": "CosineSimilarity documentation range incorrect", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n\"Note that it is a negative quantity between -1 and 0\" should be changed to \"Note that it is a negative quantity between -1 and 1\"\r\n\r\n### Correct links\r\n\r\n### Parameters defined\r\n\r\n### Returns defined\r\n\r\n### Raises listed and defined\r\n\r\n### Usage example\r\n\r\n### Request visuals, if applicable\r\n\r\n### Submit a pull request?\r\n\r\nDon't plan to submit pull request\r\n", "comments": ["There is no negative quantity in (0,1).", "This is fixed now with commit [7e7641d](https://github.com/tensorflow/tensorflow/commit/7e7641d95c6c9b7e46b129c10ec7a965fb2f848d) . Thanks!"]}, {"number": 42457, "title": "AttributeError: module 'tensorflow.python.keras' has no attribute 'abs'", "body": "<em>I am working on a multi-class image classification problem by using a siamese model.</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Nvidia 920MX\r\n\r\n\r\n\r\nI am working on a image classification problem with multiple classes and i follow a siamese face recognition sample in here. I have saved processed data in **.npy** format and i have used **Lambda** in the siamese model. It shows an error in `lambda `:\r\n\r\n**`distance_euclid = Lambda( lambda tensors : K.abs( tensors[0] - tensors[1] ))( [output_x1 , output_x2] )`**\r\n", "comments": ["@lakwin-chandula \r\nPlease refer to [this link](https://stackoverflow.com/questions/57406870/importerror-cannot-import-name-abs-from-tensorflow-python-keras-impl-keras-b) with same error and let us know.\r\n\r\nFew more links with similar suggestions:\r\n[link](https://www.edureka.co/community/67412/tensorflow-importerror-cannot-import-name-abs-error) [link1](https://github.com/tensorflow/tensorflow/issues/19436#issuecomment-433685257) [link2](https://github.com/tensorflow/tensorflow/issues/20984#issuecomment-407882330)\r\n\r\n", "This attribute error seems to be triggered by `from tensorflow.python import keras as K`. I have changed it to `import tensorflow.python.keras.backend as K`. But then, it shows an `AttributeError: module 'tensorflow' has no attribute 'python'`. \r\n\r\n[AttributeError: module 'tensorflow' has no attribute 'python' #42486](https://github.com/tensorflow/tensorflow/issues/42486)", "@lakwin-chandula\r\nAs the new error faced is been tracked in #42486 we will have to move this issue to closed status please confirm, also as initial error abs is resolved as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42457\">No</a>\n"]}, {"number": 42456, "title": "Error Importing Tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:  (Windows 10 64 bits)\r\n- Mobile device: None\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:  2.3.0\r\n- Python version: 3.8.4\r\n- Installed using virtualenv? pip? conda? : pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Nvidia GeForce 920M 2GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen trying to import tensorflow, i got the following error:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n>     from tensorflow.python._pywrap_tensorflow_internal import *\r\n> ImportError: DLL load failed while importing _pywrap_tensorflow_internal: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"deep_versions.py\", line 2, in <module>\r\n>     import tensorflow\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n>     from tensorflow.python.tools import module_util as _module_util\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n>     from tensorflow.python.eager import context\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n>     from tensorflow.python import pywrap_tfe\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\Muril\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n>     from tensorflow.python._pywrap_tensorflow_internal import *\r\n> ImportError: DLL load failed while importing _pywrap_tensorflow_internal: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n# tensorflow\r\nimport tensorflow\r\nprint('tensorflow: %s' % tensorflow.__version__)\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI'm using Anaconda, and installed through pip in the command prompt. Using `python -VV` i got the following:\r\n\r\n`Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]`\r\n\r\nLater, i tried install using `conda install -c conda-forge tensorflow`, and got the following:\r\n\r\n```\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\r\nCollecting package metadata (repodata.json): done\r\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\r\nSolving environment: |\r\nFound conflicts! Looking for incompatible packages.\r\nThis can take several minutes.  Press CTRL-C to abort.\r\nfailed\r\n\r\nUnsatisfiableError: The following specifications were found\r\nto be incompatible with the existing python installation in your environment:\r\n\r\nSpecifications:\r\n\r\n  - tensorflow -> python[version='3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|3.7.*']\r\n\r\nYour python: python=3.8\r\n\r\nIf python is on the left-most side of the chain, that's the version you've asked for.\r\nWhen python appears to the right, that indicates that the thing on the left is somehow\r\nnot available for the python version you are constrained to. Note that conda will not\r\nchange your python version to a different minor version unless you explicitly specify\r\nthat.\r\n\r\nThe following specifications were found to be incompatible with your system:\r\n\r\n  - feature:/win-64::__cuda==10.1=0\r\n\r\nYour installed version is: 10.1\r\n```\r\n", "comments": ["@muriloasouza \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "@ravikyram \r\n\r\n1 - It's an Intel Core i7-5500U;\r\n2 - Not really sure what microsoft visual c++ i should download there, x64? x86? ARM64?\r\n3 - Do i really need to install those versions of Python (3.6 or 3.7)? I've seen it should work in Python 3.8;\r\n4 - Both my CPU and Python are on 64 bits (as you can see in the `python -VV` result i posted earlier).\r\n\r\nEDIT: All right, i got it now. It was the microsoft visual c++ redistributable (x64) that was missing. I'll close this for now. Thanks for your help @ravikyram .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42456\">No</a>\n"]}, {"number": 42455, "title": "I have facing below mention error in new version of python 3.8. Can any suggest how to solve it?", "body": "\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\Prognica Work\\Application Code\\unet_keras-master\\New_UNET.py\", line 20, in <module>\r\n    from keras.models import Model, load_model\r\n\r\n  File \"C:\\Anaconda\\lib\\site-packages\\keras\\__init__.py\", line 5, in <module>\r\n    raise ImportError(\r\n\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`", "comments": ["@rohitthanki9,\r\nPlease provide the complete code and the exact sequence of commands / steps that you executed before running into the error.\r\n\r\nAlso, as mentioned in the error log, please install/update TensorFlow to the latest version using the below command and check if you are still facing the same issue. \r\n\r\n```\r\npip install tensorflow --upgrade\r\n```\r\n\r\nThanks!", "Hi,\r\n\r\nI have already install upgrade version of tensorflow 2.3.0. I am simply design basic CNN model.  ", "Hi Abhilash,\r\nI have simple call keras and tensorflow in below code and get mentioned errors. \r\n\r\nimport os\r\nimport sys\r\nimport random\r\nimport warnings #\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom tqdm import tqdm\r\nfrom itertools import chain\r\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\r\nfrom skimage.transform import resize\r\nfrom skimage.morphology import label\r\nfrom keras.models import Model, load_model\r\nfrom keras.layers import Input\r\nfrom keras.layers.core import Dropout, Lambda\r\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\r\nfrom keras.layers.pooling import MaxPooling2D\r\nfrom keras.layers.merge import concatenate\r\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\r\nseed = 42\r\nnp.random.seed = seed\r\n\r\nIMG_WIDTH = 128\r\nIMG_HEIGHT = 128\r\nIMG_CHANNELS = 3\r\n\r\nTRAIN_PATH = 'data/train/'\r\nTEST_PATH = 'data/test/'\r\n\r\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\r\ntest_ids = next(os.walk(TEST_PATH))[1]\r\n\r\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\r\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\r\n\r\nprint('Resizing training images and masks')\r\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):   \r\n    path = TRAIN_PATH + id_\r\n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]  \r\n#    img = plt.imsave(path + '/images/' + id_ + '.jpg',img,cmap=plt.cm.gray)\r\n    img = imread(path + '/images/' + id_ + '.jpg')[:,:,:IMG_CHANNELS]  \r\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\r\n    X_train[n] = img  #Fill empty X_train with values from img\r\n    \r\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\r\n    \r\n    for mask_file in next(os.walk(path + '/masks/'))[2]:\r\n        mask_ = imread(path + '/masks/' + mask_file)\r\n        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \r\n                                      preserve_range=True), axis=-1)\r\n        mask = np.maximum(mask, mask_)  \r\n            \r\n    Y_train[n] = mask   \r\n\r\n# test images\r\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\r\nsizes_test = []\r\nprint('Resizing test images') \r\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\r\n    path = TEST_PATH + id_\r\n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]  \r\n#    img = plt.imsave(path + '/images/' + id_ + '.jpg',img,cmap=plt.cm.gray)\r\n    img = imread(path + '/images/' + id_ + '.jpg')[:,:,:IMG_CHANNELS]\r\n    sizes_test.append([img.shape[0], img.shape[1]])\r\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\r\n    X_test[n] = img\r\n\r\nprint('Done!')", "@rohitthanki9,\r\nIn order to expedite the trouble-shooting process, could you please share the dataset as well.\r\n\r\nRegarding the DLL error, you might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python or 32-bit OS\r\n- You have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) package\r\n- Your CPU does not support AVX instructions. \r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42455\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42455\">No</a>\n"]}, {"number": 42453, "title": "SeparableConv1D fails with padding=\"causal\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux/amd64 tensorflow/tensorflow:2.3.0-gpu-jupyter\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v1.12.1-39480-g36dabea898 2.4.0-dev20200817\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n`tf.keras.layers.SeparableConv1D` fails on build with `padding=causal`\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.Input((None,None,3)))\r\nmodel.add(tf.keras.layers.SeparableConv1D(\r\n                    dilation_rate=1,\r\n                    filters = 3,\r\n                    kernel_size=2,\r\n                    padding='causal',\r\n))\r\n```\r\n```\r\n2.4.0-dev20200817\r\nv1.12.1-39480-g36dabea898 2.4.0-dev20200817\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-f2de2533598a> in <module>\r\n      9                     filters = 3,\r\n     10                     kernel_size=2,\r\n---> 11                     padding='causal',\r\n     12 ))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    462     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    463     try:\r\n--> 464       result = method(self, *args, **kwargs)\r\n    465     finally:\r\n    466       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\r\n    220       # If the model is being built continuously on top of an input layer:\r\n    221       # refresh its output.\r\n--> 222       output_tensor = layer(self.outputs[0])\r\n    223       if len(nest.flatten(output_tensor)) != 1:\r\n    224         raise ValueError(SINGLE_LAYER_OUTPUT_ERROR_MSG)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    929     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    930       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 931                                                 input_list)\r\n    932 \r\n    933     # Maintains info about the `Layer.call` stack.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1067         # Check input assumptions set after layer building, e.g. input shape.\r\n   1068         outputs = self._keras_tensor_symbolic_call(\r\n-> 1069             inputs, input_masks, args, kwargs)\r\n   1070 \r\n   1071         if outputs is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    799       return nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    800     else:\r\n--> 801       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    802 \r\n    803   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    840           # TODO(kaftan): do we maybe_build here, or have we already done it?\r\n    841           self._maybe_build(inputs)\r\n--> 842           outputs = call_fn(inputs, *args, **kwargs)\r\n    843 \r\n    844         self._handle_activity_regularization(inputs, outputs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n   2030   def call(self, inputs):\r\n   2031     if self.padding == 'causal':\r\n-> 2032       inputs = array_ops.pad(inputs, self._compute_causal_padding())\r\n   2033     if self.data_format == 'channels_last':\r\n   2034       strides = (1,) + self.strides * 2 + (1,)\r\n\r\nTypeError: _compute_causal_padding() missing 1 required positional argument: 'inputs'\r\n```\r\n\r\nHere's where it [fails](https://github.com/tensorflow/tensorflow/blob/75801da4cd321aabbf79e78da1e5de1a10ba4c2a/tensorflow/python/keras/layers/convolutional.py#L249)\r\n\r\nSame thing happens in stable version 2.3.0.", "comments": ["I have tried in colab with Tensorflow version 2.3, nightly version (`2.4.0-dev20200818`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a00dcae3a05c920edc9f22ea7ba63ebb/untitled262.ipynb). Thanks!", "This is fixed with tf-nightly version. See [gist](https://colab.research.google.com/gist/ymodak/6a3b5b233fdea0015578bafde5e34ee1/github_issue39852.ipynb)\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42453\">No</a>\n"]}, {"number": 42452, "title": "Added SPLIT_V op and test to micro", "body": "This is a redo of PR #39279 (https://github.com/tensorflow/tensorflow/pull/39279)\r\n\r\nCLA issue should be fixed and split_v implementation is changed to use TfLiteEvalTensor and the tests uses KernerRunner.\r\n\r\nFIxes #41690 and fixes #42582 ", "comments": ["Could you file a github issue with more context on why you need this new reference kernel?\r\n\r\nThe more detail the better, for example:\r\n * what your application is\r\n * what model architecture looks like\r\n * any performance criteria that you have\r\n * what embedded targets are you planning to run your models on?\r\n * what your plans are for optimized implementations for the specific platforms that you care about?\r\n\r\nThis will help us better understand the needs of our users.", "@advaitjain Opened an issue: https://github.com/tensorflow/tensorflow/issues/42582\r\n\r\n", "@yair-ehrenwald  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "> @yair-ehrenwald Can you please check @advaitjain's comments and keep us posted ? Thanks!\r\n\r\nOn it :)", "@yair-ehrenwald can you please check this error \r\n\r\n`/tensorflow/lite/core/api/flatbuffer_conversions.cc:163:11: error: enumeration value 'BuiltinOperator_SPLIT_V' not handled in switch\r\n`", "One more thing before we can get this merged.\r\n\r\nThe PR is failing the CI checks because of unused variables in the release build. You can add some `#ifdef DEBUG` to fix those.\r\n\r\nTo reproduce the errors, run this command:\r\n```\r\ntensorflow/lite/micro/tools/make/tools/ci_build/test_x86.sh\r\n```", "> @yair-ehrenwald can you please check this error\r\n> \r\n> `/tensorflow/lite/core/api/flatbuffer_conversions.cc:163:11: error: enumeration value 'BuiltinOperator_SPLIT_V' not handled in switch `\r\n\r\nWe don't have an easy command to reproduce this error. I'll look into that tomorrow, but what is missing is a `case SPLIT_V`", "> @yair-ehrenwald can you please check this error\r\n> \r\n> `/tensorflow/lite/core/api/flatbuffer_conversions.cc:163:11: error: enumeration value 'BuiltinOperator_SPLIT_V' not handled in switch `\r\n\r\nShould be fixed. ", "> One more thing before we can get this merged.\r\n> \r\n> The PR is failing the CI checks because of unused variables in the release build. You can add some `#ifdef DEBUG` to fix those.\r\n> \r\n> To reproduce the errors, run this command:\r\n> \r\n> ```\r\n> tensorflow/lite/micro/tools/make/tools/ci_build/test_x86.sh\r\n> ```\r\n\r\nFixed.", "Also note that I merged tensorflow/master and pushed your branch to move things along so you might have to pull from your remote before proceeding.", "> Another set of CI errors that can be reproduced with:\r\n> \r\n> ```\r\n> make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4 third_party_downloads\r\n> make -j8 -f tensorflow/lite/micro/tools/make/Makefile BUILD_TYPE=release TAGS=cmsis-nn TARGET=stm32f4 build\r\n> ```\r\n\r\nFixed.\r\n"]}, {"number": 42451, "title": "ModuleNotFoundError: No module named 'tensorflow_examples'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Just trying to import and check**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Win10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **pip with GPU support**\r\n- TensorFlow version (use command below): **2.0.0**\r\n- Python version: **3.5**\r\n- Bazel version (if compiling from source): **No**\r\n- GCC/Compiler version (if compiling from source):**No**\r\n- CUDA/cuDNN version: **Cuda compilation tools, release 10.1, V10.1.243**(nvcc --version)\r\n- GPU model and memory: **GeForce 920M**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nversion - 2.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nIn my Notebook's first cell, I wrote :\r\n```\r\n!pip install git+https://github.com/tensorflow/examples.git\r\n!pip install -U tfds-nightly\r\n```\r\nOn the next cell :\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow_examples.models.pix2pix import pix2pix\r\n```\r\nBut it gives me : *ModuleNotFoundError: No module named 'tensorflow_examples'*\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should have exported the module.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nHere is the pip install log :\r\n```\r\nCollecting git+https://github.com/tensorflow/examples.git\r\n  Cloning https://github.com/tensorflow/examples.git to c:\\users\\mua\\appdata\\local\\temp\\pip-req-build-qtmqgj7m\r\nRequirement already satisfied (use --upgrade to upgrade): tensorflow-examples===b30a40f9416fc38cfa91ca03d835ba1fc432a824- from git+https://github.com/tensorflow/examples.git in e:\\software\\python 3.5\\lib\\site-packages\r\nRequirement already satisfied: absl-py in e:\\software\\python 3.5\\lib\\site-packages (from tensorflow-examples===b30a40f9416fc38cfa91ca03d835ba1fc432a824-) (0.8.1)\r\nRequirement already satisfied: six in e:\\software\\python 3.5\\lib\\site-packages (from tensorflow-examples===b30a40f9416fc38cfa91ca03d835ba1fc432a824-) (1.14.0)\r\nBuilding wheels for collected packages: tensorflow-examples\r\n  Building wheel for tensorflow-examples (setup.py): started\r\n  Building wheel for tensorflow-examples (setup.py): finished with status 'done'\r\n  Created wheel for tensorflow-examples: filename=tensorflow_examples-b30a40f9416fc38cfa91ca03d835ba1fc432a824_-py3-none-any.whl size=136427 sha256=40d5b23f277f4634313116bf6205588e8668a499798fe1c7fdad143fc6144b68\r\n  Stored in directory: C:\\Users\\MUA\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-5wvvxv8d\\wheels\\e2\\f1\\08\\a5d8eb62f62cc814d511a70115a5467b1135ec8270dd16d620\r\nSuccessfully built tensorflow-examples\r\n  Running command git clone -q https://github.com/tensorflow/examples.git 'C:\\Users\\MUA\\AppData\\Local\\Temp\\pip-req-build-qtmqgj7m'\r\nWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\nYou should consider upgrading via the 'e:\\software\\python 3.5\\python.exe -m pip install --upgrade pip' command.\r\nCollecting tfds-nightly\r\n  Downloading tfds_nightly-3.2.1.dev202007220105-py3-none-any.whl (3.4 MB)\r\nRequirement already satisfied, skipping upgrade: wrapt in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (1.11.2)\r\nRequirement already satisfied, skipping upgrade: numpy in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (1.17.4)\r\nRequirement already satisfied, skipping upgrade: tqdm in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (4.45.0)\r\nCollecting tensorflow-metadata\r\n  Downloading tensorflow_metadata-0.23.0-py3-none-any.whl (43 kB)\r\nCollecting promise\r\n  Downloading promise-2.3.tar.gz (19 kB)\r\nCollecting dill\r\n  Downloading dill-0.3.2.zip (177 kB)\r\nRequirement already satisfied, skipping upgrade: six in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (1.14.0)\r\nCollecting attrs>=18.1.0\r\n  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\r\nRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (3.10.0)\r\nRequirement already satisfied, skipping upgrade: absl-py in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (0.8.1)\r\nRequirement already satisfied, skipping upgrade: termcolor in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: requests>=2.19.0 in e:\\software\\python 3.5\\lib\\site-packages (from tfds-nightly) (2.23.0)\r\nCollecting future\r\n  Downloading future-0.18.2.tar.gz (829 kB)\r\nCollecting googleapis-common-protos\r\n  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\r\nRequirement already satisfied, skipping upgrade: setuptools in e:\\software\\python 3.5\\lib\\site-packages (from protobuf>=3.6.1->tfds-nightly) (41.6.0)\r\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in e:\\software\\python 3.5\\lib\\site-packages (from requests>=2.19.0->tfds-nightly) (1.25.8)\r\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in e:\\software\\python 3.5\\lib\\site-packages (from requests>=2.19.0->tfds-nightly) (2.9)\r\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in e:\\software\\python 3.5\\lib\\site-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\r\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in e:\\software\\python 3.5\\lib\\site-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\r\nBuilding wheels for collected packages: promise, dill, future\r\n  Building wheel for promise (setup.py): started\r\n  Building wheel for promise (setup.py): finished with status 'done'\r\n  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21498 sha256=82af1fb81258e76c2ddec82ec8870ec0901b560e2547ddff0f81e096cd65fdc2\r\n  Stored in directory: c:\\users\\mua\\appdata\\local\\pip\\cache\\wheels\\b6\\3e\\4e\\d80f74df03a8059f631b23ec49939d8fa0a2633522596b6ffd\r\n  Building wheel for dill (setup.py): started\r\n  Building wheel for dill (setup.py): finished with status 'done'\r\n  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78977 sha256=22a67eb861aca650bc9f1e039c15cb8ef9a87fbe88a139952868f352dd8f51aa\r\n  Stored in directory: c:\\users\\mua\\appdata\\local\\pip\\cache\\wheels\\5c\\4b\\fd\\db4143df7b4a4301b4068a2ed49f300b76b13d87b23bf375da\r\n  Building wheel for future (setup.py): started\r\n  Building wheel for future (setup.py): finished with status 'done'\r\n  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491061 sha256=f19a6fd742fd80f4a1995a132c1c10b6de14b112ef5110bcb31eff13a2379306\r\n  Stored in directory: c:\\users\\mua\\appdata\\local\\pip\\cache\\wheels\\c4\\f0\\ae\\d4689c4532d1f111462ed6a884a7767d502e511ee65f0d8e1b\r\nSuccessfully built promise dill future\r\nInstalling collected packages: googleapis-common-protos, tensorflow-metadata, promise, dill, attrs, future, tfds-nightly\r\nSuccessfully installed attrs-19.3.0 dill-0.3.2 future-0.18.2 googleapis-common-protos-1.52.0 promise-2.3 tensorflow-metadata-0.23.0 tfds-nightly-3.2.1.dev202007220105\r\nWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\nYou should consider upgrading via the 'e:\\software\\python 3.5\\python.exe -m pip install --upgrade pip' command.\r\n```\r\n", "comments": ["@maifeeulasad,\r\nI was able to run the given code snippet without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/139f8b05a55206beecc0d499eed54611/42451.ipynb).\r\n\r\nCould you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!", "!pip install -q git+https://github.com/tensorflow/examples.git\r\nit worked for me \r\nyou can see here\r\n[https://stackoverflow.com/questions/50313441/modulenotfounderror-no-module-named-tensorflow-examples](url)\r\ncheck if you are facing the same issue? Thanks!\r\n", "@amahendrakar It's worked fine in colab, the problem was in local system..\r\n@sum-code , really appreciate it, it works, it works perfectly, thanks...", "Hi, maybe you should just write it without the 's' in http:\r\n!pip install -q git+http://github.com/tensorflow/examples.git\r\nThat worked for me, I had the same problem!"]}, {"number": 42450, "title": "What Grappler optimizers turned on by default?", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/graph_optimization\r\n\r\n## Description of issue (what needs changing):\r\nIt should be cleared which optimizers Grappler applies by default. \r\nFor now it's not clear if i should turn on a lot of features by myself\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? - No.\r\n", "comments": ["Hi @shkarupa-alex! I'm currently working on the documentation of `set_optimizer_experimental_options` to specify the parameters more clearly. By the looks of it, it seems like by default non of the optimizers are enabled. I believe that the different functions of the graph optimizers are explained thoroughly so depending on your problem you should decide which features to turn on.", "Hi @Harsh188 \r\n\r\nI have been trying to leverage grappler for some time and I'm finding it hard to find the relevant documentation.\r\n\r\nI sub-classed tf.Module following **Saving a custom model here** [here](https://www.tensorflow.org/guide/saved_model#saving_a_custom_model).  There I write several graphs (with tf.function) and I would like to optimize them with grappler for fast execution and re-use.\r\n\r\nCould you exemplify how to run the MetaOptimizer procedure described [here](http://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf) on slide 13?.  Further, I'm wondering how to save the resulting optimized graph for later re-use.\r\n\r\nThanks!", "@andrescodas unfortunately I'm not the best person to tackle your questions so excuse me if I don't answer your question.\r\n\r\nBy the looks of it `MetaOptimizer` is the top-level which performs the graph optimizations. Take a look at the list of optimizers provided in https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_experimental_options and determine which ones suit you. Using `set_optimizer_experimental_options` you can then enable the optimizer of your choice.\r\n\r\nIf you have any specific documentation changes you have in mind please let me know! I'll do my best to clear up the confusion.", "> If you have any specific documentation changes you have in mind please let me know! I'll do my best to clear up the confusion.\r\n\r\nThank you!.  I have seen `set_optimizer_experimental_options` however I'm missing an example use case in the interface of tf.saved_model and grappler.  E.g.\r\n\r\nSuppose I have a complex graph that I want to optimize and save for later use:\r\n\r\n```python\r\nclass CustomModule(tf.Module):\r\n\r\n  @tf.function\r\n  def f(self, x, y):\r\n    z = x * y   # potentially complex graph where I want to run the different grappler optimizers\r\n    return z\r\n\r\nmodule = CustomModule()\r\n\r\n# \r\n#  here I apply graph optimization to `module`.  How can I implement run_grappler_optimizations_on_module?\r\n#\r\nmodule = run_grappler_optimizations_on_module(module)\r\n\r\n\r\n# here I save it\r\ntf.saved_model.save(module, '/my/model/directory')\r\n```\r\n\r\nLater in a different process I may load it and run,\r\n``` python \r\n\r\nmodule = tf.saved_model.load('/my/model/directory')\r\n\r\n# here I run the optimized function without needing to re-run run_grappler_optimizations_on_module\r\nz = module.f(...)\r\n\r\n```\r\n\r\n", "For anyone still have interest in whether a grappler optimizer is default on or off, please check `tensorflow/core/protobuf/rewriter_config.proto`. This is the protobuf file used to send grappler information from python side to backend C++ side and it includes rich information on what every grappler optimizer does.", "@shkarupa-alex,\r\nAs the [Documentation of Grappler Optimizers](https://www.tensorflow.org/guide/graph_optimization#available_graph_optimizers) specify which Optimizers are turned on by Default, can you please confirm if we can close this issue? Thanks!", "Now it's clear.\r\nThanks!", "Closing the issue as it has been resolved."]}, {"number": 42449, "title": "No module named 'tensorflow.python.platform'", "body": "Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow installed from (source or binary):binary\r\nTensorFlow version: Version 2.3\r\nPython version:3.8\r\nInstalled using virtualenv? pip? conda?: in a conda eviroment with pip\r\nI want to import tensorflow but everytime I get\r\ntensorflow.python.platform import self_check\r\nModuleNotFoundError: No module named 'tensorflow.python.platform'\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem\r\nI have already tried different tensorflow 2.x version and also completed uninstalled anconda and reinstall it. I also tried it in different enviorments.\r\nAlso I had tensorflow running, but I update and also deleted some of the Microsoft Visual C++ Redistributable due to another problem. I did update and reinstalled all of the version but tensorflow is still not working.\r\n\r\nAlso I looked in the file and there is no python.plattform. Is there anyway to copy it or does anyone have some other solution?", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42449\">No</a>\n"]}, {"number": 42448, "title": "[INTEL MKL] Enable NCHW to NHWC conversion for CPU - part1", "body": "This is an already reviewed PR #39760 with a fix to GPU failure.\r\n\r\nThis PR enables layout (data format) conversion from NCHW to NHWC on CPU. This is useful (1) when a model is trained on GPU and later doing inference/fine-tuning on CPU and (2) helps quantizing NCHW trained model for CPU.\r\n\r\nTo make the PR small, we have included a few unit tests in this part1. More unit tests will follow on future PRs.", "comments": ["@gbaned @ezhulenev @andyly I had some CLA issues in #39760 while I was fixing GPU failure. This PR is identical to old one except for the small fix to the GPU failure. Sorry for the inconvenience."]}, {"number": 42447, "title": "ValueError: `tape` is required when a `Tensor` loss is passed.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (macOS Catalina):\r\n- TensorFlow installed from pip:\r\n- TensorFlow version 2.3.0:\r\n- Python version 2.8:\r\n\r\n**Describe the current behavior**\r\n\r\nGetting the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 277, in <module>\r\n    k3\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 385, in minimize\r\n    loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 440, in _compute_gradients\r\n    raise ValueError(\"`tape` is required when a `Tensor` loss is passed.\")\r\nValueError: `tape` is required when a `Tensor` loss is passed.\r\n```\r\nwhereas `loss` is a number.\r\n\r\n**Describe the expected behavior**\r\nSince `loss` is a number, we expect tensorflow to not say that `Tensor` loss has been passed.\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\ndef loss(ground_truth, output):\r\n    return np.sum(np.positive(ground_truth - output))\r\n\r\ndef kernel(*args):\r\n  return tf.Variable(np.random.rand(*args), dtype=tf.float32) # kernel 1\r\ncolor_channels = 3\r\nfeature_size = 32\r\nk1 = kernel(5, 5, color_channels, feature_size)\r\nk2 = kernel(3, 3, feature_size, feature_size) # kernel 2\r\nk3 = kernel(3, 3, feature_size, feature_size) # kernel 3\r\nwidth = 100\r\nheight = 200\r\ny_train_left_nod = np.random.rand(width, height, 3)\r\n\r\ny_train_left_noc = tf.keras.preprocessing.image.load_img(\"/img/path\")\r\ny_train_left_noc = tf.keras.preprocessing.image.img_to_array(y_train_left_noc)\r\nsoft_argmin = np.random.rand(y_train_left_noc.shape[0], y_train_left_noc.shape[1])  \r\nloss = loss(y_train_left_noc[:,:,0], soft_argmin).tolist() # y\r\n\r\ntrain = tf.keras.optimizers.Adam().minimize(loss, [\r\n    k1,\r\n    k2,\r\n    k3])\r\n```\r\n", "comments": ["@zendevil,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'color_channels' is not defined`. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar I have updated the original code. The only change you need to make is have a valid image path.", "Hi @zendevil, the loss argument for `Optimizer.minimize` should be a callable, [as noted here in the docs](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#minimize).\r\n\r\nThe error message you're seeing [is here in the source code:](https://github.com/tensorflow/tensorflow/blob/43cfb92ac5e2348718155905245194723f674697/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L499)\r\n```\r\nif not callable(loss) and tape is None:\r\n    raise ValueError(\"`tape` is required when a `Tensor` loss is passed.\")\r\n```\r\nThis error message seems only to be in tf-nightly, and if you run the code in TF 2.3 you'll notice a slightly different error message `TypeError: 'float' object is not callable` which is perhaps a more clear message. Either way, since you're passing a scalar loss value, and not a callable, an error is expected. If you want to use a scalar loss value and not pass the callable, then you should use `GradientTape`, as noted in the error message. Happy to share some examples on how to do that is helpful. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42447\">No</a>\n"]}, {"number": 42445, "title": "Setting multithreading  for tensorflowlite does not work\uff01", "body": "Hi guys,\r\nI set up tensorflowlite for four threads,but compared with single thread, the time consumption has not decreased. by the way, i quantizate my models.\r\nHope to get your help,ths!", "comments": ["@hzq-zjm \r\n\r\nCan you please fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "The relevant codes are as follows\uff1a   \r\n   \r\n\r\n\r\n         private static Interpreter tflite_segm;\r\n             private static Interpreter tflite_ocr ;\r\n             /** Options for configuring the Interpreter. */\r\n             private static Interpreter.Options tfliteOptions_seg = new Interpreter.Options();;\r\n             private static Interpreter.Options tfliteOptions_ocr = new Interpreter.Options();\r\n\r\n             /** The loaded TensorFlow Lite model. */\r\n             private static MappedByteBuffer tfliteModel_segm;\r\n             private static MappedByteBuffer tfliteModel_ocr;\r\n\r\n             tfliteModel_segm=loadModelFile(\"frozen_qnt\",activity);\r\n            tfliteModel_ocr =loadModelFile(\"model_12y4r\",activity);\r\n\r\n            tfliteOptions_seg.setNumThreads(4);\r\n            tfliteOptions_ocr.setNumThreads(4);\r\n\r\n            tflite_segm = new Interpreter(tfliteModel_segm,tfliteOptions_seg);\r\n            tflite_ocr = new Interpreter(tfliteModel_ocr,tfliteOptions_ocr);", "Only the CPU is used to run the model.", "@hzq-zjm What model are you running?\r\nCurrently, only Conv2D and DepthwiseConv has multithreaded kernels.\r\nSo you will only get benefit of they are significant in your model.", "> @hzq-zjm What model are you running?\r\n> Currently, only Conv2D and DepthwiseConv has multithreaded kernels.\r\n> So you will only get benefit of they are significant in your model.\r\n\r\nThks for your reply. There are two models in my project, one of which is trained based on deeplab-v3 +, and the other is OCR model  based on densenet-18.  All models are quantized by post quantization, which makes multithreading not work?", "Could you run the performance benchmark(https://www.tensorflow.org/lite/performance/benchmarks) on both models with the --enable_op_profiling=true?\r\nIt will give much more details.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 42444, "title": "Can I specify the version of NAPI ?", "body": "Can I specify the version of nAPI? In my environment, Node 10.15.x only supports NAPi-v4, but tensorFlow is actually NAPi-V6 (/node_modules/_@tensorflow_tfjs-node@2.1.0@@tensorflow/tfjs-node/lib/ NAPi-v6) after My deployment.\r\n\r\n- tensorflowVersion\uff1a2.1.0\r\n\r\n![image](https://user-images.githubusercontent.com/12997948/90458256-7e305d00-e130-11ea-8dee-98dccb97bd3f.png)\r\n\r\n", "comments": ["@yangshengdong,\r\nIssues related to TensorFlow.js are handled in the tensorflow/tfjs repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/tfjs/issues/new) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 42443, "title": "Keras Model accepts unnamed ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nColab.\r\n\r\n- TensorFlow installed from (source or binary):\r\ncolab\r\n\r\n- TensorFlow version (use command below):\r\n\r\ntf 2.3.0\r\n\r\n- Python version:\r\ncolab default\r\n\r\n**Describe the current behavior**\r\n\r\nKeras model (in eager mode at least) accepts a dictionary for named input, but will accept without warning any dictionary of the correct length regardless of the key names.\r\n\r\n**Describe the expected behavior**\r\n\r\nA warning or error if the keys provided as input when running a keras model do not match the named input.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Define a simple model with named input\r\na = tf.keras.Input(shape=[1], name='a')\r\nb = tf.keras.Input(shape=[1], name='b')\r\nout = tf.concat([a, b], axis=0)\r\nout = tf.reduce_sum(out, axis=0)\r\nmodel = tf.keras.Model([a, b], [out])\r\n\r\n# As expected models runs with named input\r\nmodel({'a': tf.convert_to_tensor([1]), 'b': tf.convert_to_tensor([1])})\r\n\r\n# However, model will silently accept any dictionary with two tensors\r\n# (of appropriate shape) without warning.\r\nmodel({'c': tf.convert_to_tensor([1]), 'd': tf.convert_to_tensor([1])})\r\n\r\n# It will warn if the dictionary has too many entries\r\nmodel({'c': tf.convert_to_tensor([1]), 'd': tf.convert_to_tensor([1]), 'e': tf.convert_to_tensor([1])})\r\n\r\n# Or raise an Assertion if the dictionary has too few entries.\r\nmodel({'c': tf.convert_to_tensor([1])})\r\n```", "comments": ["@jjh42 \r\nI ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/eb5f441b670e9c8714d9adfeed572f46/untitled377.ipynb), please share a colab gist with the error and let us know.", "Yes that assertion error is as what I noted in the code on the last line\r\n\r\n```\r\n# Or raise an Assertion if the dictionary has too few entries.\r\nmodel({'c': tf.convert_to_tensor([1])})\r\n```\r\n\r\nIf you remove that last line it won't raise an assertion error and the key issue is this line:\r\n```\r\n# However, model will silently accept any dictionary with two tensors\r\n# (of appropriate shape) without warning.\r\nmodel({'c': tf.convert_to_tensor([1]), 'd': tf.convert_to_tensor([1])})\r\n```\r\n\r\nthe model will silently accept a dictionary with completely the wrong keys if it is of the correct length.", "```\r\n        # In the case that the graph is constructed with dict input tensors,\r\n        # We will use the original dict key to map with the keys in the input\r\n        # data. Note that the model.inputs is using nest.flatten to process the\r\n        # input tensors, which means the dict input tensors are ordered by their\r\n        # keys.\r\n```\r\nRefered from https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/functional.py#L532\r\n\r\nIt seems that in tf2.3.0 it only cares about the order of the dict input tensors, but doesn't check the the keys' name.", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/d1740f300d9779e3af0872dd1e91c233/untitled59.ipynb)..Thanks !", "@jjh42 This was resolved in recent TF versions. [Here](https://colab.research.google.com/gist/jvishnuvardhan/fa909317e2d6fdd2d48eaa1e30b6ea65/untitled377.ipynb) is a gist for a reference.\r\n\r\nIt throws a warning as expected. I am closing this issue as this was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42443\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42443\">No</a>\n"]}, {"number": 42442, "title": "Update tensorflow.keras.metrics.pbtxt", "body": "See https://github.com/tensorflow/tensorflow/pull/42097#issuecomment-675149062\r\n\r\nccing @pavithrasv : is this the right place to make this edit?", "comments": ["Thank you for the PR @adriangb. You need not update the pbtxt files afaik. It's enough to add those strings in losses.py file against the function. https://github.com/tensorflow/tensorflow/blob/442c7015fce8889fac11f415f20d43e0576520a3/tensorflow/python/keras/losses.py#L1455", "Ah makes sense, will do!", "Just pushed the change @pavithrasv, please take a look when you have a chance. Thanks.", "I assume the test failures due to API change are expected and just require manual approval/override?", "@adriangb  Can you please address Ubuntu Sanity errors? Thanks!", "I think I see what happened. Can you re-approve @gbaned ?", "@adriangb  Still, Ubuntu Sanity errors appearing, Can you fix those?. Thanks!", "I see that I left duplicate decorator, I *think* that was the problem this time. I rebased to the last commit in master and re-applied the decorator (a single time). For some reason `ci_sanity.sh` is building the container but not running for me so... do you mind approving again @gbaned ? Thanks", "Sanity checks are now passing. It looks like the failures are from API change, which is correct since this technically does change the API.", "@adriangb this test is failing https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/api_compatibility_test.py\r\n```\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test with\r\n\"--update_goldens\" flag set to \"True\" to update goldens when making changes to\r\nthe public TF python API.\r\n```", "Well this is adding an API, but only because that API was accidentally omitted. This API change should be approved.", "cc @pavithrasv ", "That's right, this is not changing any API, it is adding couple of new APIs.", "Thank you , @pavithrasv can you please help with internal failures ?", "Sure, I have replied on it already!", "@adriangb  Any update on this PR? Please. Thanks!", "> @adriangb Any update on this PR? Please. Thanks!\r\n\r\nWhat updates do you need from me? This is waiting on internal Google approval AFAIK.", "> > @adriangb Any update on this PR? Please. Thanks!\r\n> \r\n> What updates do you need from me? This is waiting on internal Google approval AFAIK.\r\n\r\n@adriangb  Sorry, I thought it is waiting for update from you.  Thanks! \r\n", "@pavithrasv  Any update on this PR? Please. Thanks!", "@adriangb this test is still failing , please generate goldens https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/api_compatibility_test.py\r\n```\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test with\r\n\"--update_goldens\" flag set to \"True\" to update goldens when making changes to\r\nthe public TF python API.\r\n```\r\n", "please use the command \r\n```\r\n\r\n blaze run third_party/tensorflow/tools/api/tests:api_compatibility_test \\\r\n          -- --update_goldens True\r\n```\r\n\r\ncc @pavithrasv ", "Isn't this something you can do internally? I have no idea what `blaze` is, I'm not a googler", "@adriangb we usually ask users to generate goldens when there is a change in Apis so that you can credit for changes , you can install bazel and run above command \r\ncheck for a similar [PR](https://github.com/tensorflow/tensorflow/pull/42539#issuecomment-686816117) where goldens were generated by users.", "cc @mihaimaruseac ", "Oh ok, you meant _bazel_. https://github.com/tensorflow/tensorflow/pull/42442#issuecomment-702374343 says `blaze run` which confused me a bit.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42442) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42442) for more info**.\n\n<!-- ok -->", "The change adds a symbol to the API and therefore needs to be reviewed by API owners. More details: https://github.com/tensorflow/community/blob/master/governance/api-reviews.md", "> The change adds a symbol to the API and therefore needs to be reviewed by API owners. More details: https://github.com/tensorflow/community/blob/master/governance/api-reviews.md\r\n\r\nCould you at least tag the API owners? The link in the doc you linked is currently broken (https://github.com/orgs/tensorflow/teams/api-owners). @pavithrasv already okayed this PR.", "> > The change adds a symbol to the API and therefore needs to be reviewed by API owners. More details: https://github.com/tensorflow/community/blob/master/governance/api-reviews.md\r\n> \r\n> Could you at least tag the API owners? The link in the doc you linked is currently broken (https://github.com/orgs/tensorflow/teams/api-owners). @pavithrasv already okayed this PR.\r\n\r\nForgot to mention that I did add \"API review\" tag.", "Btw, I'm running bazel to update the goldens but even with `--jobs=1` and inside a fresh `tensorflow/tensorflow:devel` container I'm getting unrelated compilation errors and it takes ages. I wish there was a way to make changes in TF that didn't require compiling a bunch of unrelated C++ APIs. If there is, please do share!", "Let me import this locally and try doing the goldens internally.", "> Let me import this locally and try doing the goldens internally.\r\n\r\nThank you! I gave you access to my fork in case you want to just push there.", "> > Let me import this locally and try doing the goldens internally.\r\n> \r\n> Thank you! I gave you access to my fork in case you want to just push there.\r\n\r\nThanks for giving access to the fork. I'll work on the imported change since that is slightly faster, but if the changed failed to import I would have used the fork"]}, {"number": 42441, "title": "Erroneously triggering tf.function retracing warnings when rapidly creating new TF models.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, Google Colab\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): occurs in v2.3.0 and tf-nightly v2.4.0a20200817; NOT in v2.2.0\r\n- Python version: 3.7.7\r\n\r\n\r\n**Related Issues**\r\n- https://github.com/tensorflow/tensorflow/issues/34025\r\n- https://github.com/tensorflow/tensorflow/issues/38561\r\n\r\n\r\n**Describe the current behavior**\r\nThe function retracing warning (see below) is triggered constantly when creating multiple independent models. Similar bugs occured recently in the above mentioned issues, though while those occur when doing rapid predictions on the same model does this bug occur solely when creating and predicting multiple new models rapidly (which is necessary e.g. for TF evolutionary frameworks). The tf.function retracing warning is triggered after the creation of the first 5 models. The bug occurs in v2.3.0 and todays tf-nightly, though not in v2.2.0. I attempted workarounds mentioned in previous issues (setting `experimental_relax_shapes` to True, disabling eager execution and setting `step` to 1), though none worked. I also attempted to provide fixed input shape and batch_size as mentioned in the warning message, though this was also unsuccessful. \r\nWhile I am no expert in tf.function does it seem to me that the counter to trigger the warning seems to be a global variable and not seperate for each model, therefore triggering the warning that excessive retracings for the current model occured even though it was only fast consecutive initial retracing for each one of multiple models. Just a hunch though.\r\n\r\nExact warning message:\r\n\r\n> WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3c67c45a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\r\ny = np.array([[0], [1], [1], [0]])\r\n\r\nfor i in range(50):\r\n    tf.print(f'Model {i}...')\r\n\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(units=2))\r\n    model.add(tf.keras.layers.Dense(units=1))\r\n\r\n    prediction = model.predict(x)\r\n```\r\n\r\nGoogle colab reproducing code: https://colab.research.google.com/drive/1QLFNCDWw37x6kP2AajB4g3Lidn1oLRx7?usp=sharing", "comments": ["@PaulPauls \r\n\r\nI have tried in colab with TF 2.3.0 and nightly version(`2.4.0-dev20200817`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e1a45329766517a33e5783fd40d3a750/untitled259.ipynb).Thanks!", "Hi @PaulPauls, to clarify, for your use case you _are_ expecting retracing to happen on each pass through the loop? This is because you're using a different model each time, correct? If so, it seems different to me compared the issues you've linked to, where they were trying to prevent extra retraces from happening on each pass through the loop. So I'm not sure the warnings are too unexpected since most of the time you would want to prevent excessive retracing.\r\n\r\nHave you tried using `model.predict_step()` instead of `model.predict`? predict is a high-level end point that manages its own tf.function, so I think you're seeing the warning because you're falling into case (1). predict_step is just the logic of an inference step and shouldn't trigger the warning.", "Hi @nikitamaia, thank you for your response. I am no expert in function retracing, though my understanding is that due to the functional way TF allows for model creation does TF also retrace the whole model graph the first time it is called to ensure that it is correctly generated. I therefore _do_ expect retracing to happen once for each model when it is predicting for the first time - that is correct.\r\n\r\nHowever, because I consider this normal behaviour am I confused from the warning. If functionally each model works fine and is not retraced multiple times and TF only displays this warning because it keeps track of model retracings globally (and not per model), then everything is resolved from my side. I do disagree with the decision to track model retracing globally and not per model (as it triggers the unnecessary warning when multiple models are created in a short time, even though everything is in order), though this then would be a deliberate developer decision and not a bug. I however would suggest to adjust the warning message accordingly, as the warning references a single model instance for which the warning is displayed and doesn't refer to global tracking of model retracings (therefore making me think something went wrong with my models).\r\n\r\nI did link the related issues as according to the warning message I assumed that somehow through the creation of multiple models in a short time multiple function retracings occured for a single model. This would make the problem similar to the ones discussed in the related issues. At the time it was only a possible assumption that TF tracks function retracings globally.\r\n\r\nAnd using `predict_step()` instead of `predict(steps=1)` did indeed work, thank you!", "Closing this issue now since a workaround was found.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42441\">No</a>\n", "tf2.1.0: predict --> predict_on_batch", "This \"workaround\" is not applicable to me, as I want to rely on the functionality of `predict()` (in particular its conversion to a graph-based model). The original author is completely right, that warnings should only trigger for the same model.", "Any progress on suppressing these warnings will be tracked in #43555. Please check in there for updates."]}, {"number": 42440, "title": "Problem running tensorflow in Pycharm - ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 10, 4GB Ram, 64-bit OS, x64 basedprocessor, Microsoft Visual C++ 2015-19 Re....installed):\r\n- Mobile device (N/A):\r\n- TensorFlow installed from (pip install tensorflow Pycharm Terminal):\r\n- TensorFlow version: latest\r\n- Python version: 3.7.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nInstallation seems to work fine (successfully installed absl.....). But when I run import tensorflow as tf\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nC:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\Scripts\\python.exe C:/Users/D'AVON/PycharmProjects/Test/start.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/D'AVON/PycharmProjects/Test/start.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\D'AVON\\PycharmProjects\\Test\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["@PODEE,\r\nYou might be facing this issue because of the following reasons\r\n\r\n- You are running 32-bit Python\r\n- Your CPU does not support AVX instructions. Please share the make and model of your CPU, so that we can verify this.\r\n\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204.\r\n\r\nThanks!\r\n", "![SystemModel](https://user-images.githubusercontent.com/17114177/90534326-9d86b480-e147-11ea-9b44-d52de1aec442.PNG)\r\nSystem Model attached.\r\nI installed 64-bit Python and I've gone through most of the duplicate issues but still not working", "@PODEE,\r\nThe [product specification](https://ark.intel.com/content/www/us/en/ark/products/82105/intel-pentium-processor-n3540-2m-cache-up-to-2-66-ghz.html) for your CPU doesn't mention AVX support.\r\n\r\nIn this case, you can either [build TensorFlow](https://www.tensorflow.org/install/source_windows) from source or use [Google Colab](https://colab.research.google.com/) as alternatives. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42440\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42440\">No</a>\n"]}, {"number": 42439, "title": "Modil.fit() issue", "body": "Can you provide me the full code?\r\n\r\nI have this error:\r\n\"ValueError: Calling Model.fit in graph mode is not supported when the Model instance was constructed with eager mode enabled. Please construct your Model instance in graph mode or call Model.fit with eager mode enabled.\"\r\nWhen I tried to execute this code:\r\n\r\nhist = model.fit(X_train, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test))\r\n\r\nHow can I exceed it?\r\n\r\ntensorflow  2.3.0\r\nKeras          2.4.3\r\n\r\nThe code of CNN modal:\r\n[CNN modal.txt](https://github.com/tensorflow/tensorflow/files/5086399/CNN.modal.txt)\r\n", "comments": ["@Al-Badri179 \r\nI ran the code shared and face different error, please find [gist here](https://colab.research.google.com/gist/Saduf2019/0d0e205eb27b030afa6d213bf348e740/untitled377.ipynb)\r\nPlease fill the issue template for us to help.", "@Saduf2019 \r\nyou got this problem because when you copy and paste my code you missed some indents which causes this error.\r\n\r\nNow, you can check the code in your gist https://colab.research.google.com/gist/Saduf2019/0d0e205eb27b030afa6d213bf348e740/untitled377.ipynb#scrollTo=EUmhI-JaDSHX&line=276&uniqifier=1 \r\n\r\nJust all you need to upload the dataset it is found here:\r\nhttps://github.com/anujshah1003/own_data_cnn_implementation_keras\r\n\r\n\r\nThis is the full code if you couldn't find it on the above link:\r\n#Created on Thu May  4 23:32:34 2017\r\n#@author: anuj shah\r\n\r\n# Import libraries\r\nimport os,cv2\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn.utils import shuffle\r\n#from sklearn.cross_validation import train_test_split   does not work anymore because it was updated to:\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom keras import backend as K\r\n#K.set_image_dim_ordering('th')     this parameter has been changed\r\nK.set_image_data_format('channels_first')\r\n\r\nfrom keras.utils import np_utils\r\nfrom keras.models import Sequential\r\n\r\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\r\nfrom keras.optimizers import SGD,RMSprop,Adam\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n#x = tf.reshape(tf.range(24), [1, 2, 3, 4])\r\n#out = tf.transpose(x, [0, 2, 3, 1])\r\n\r\n#%%\r\nPATH = os.getcwd()\r\n# Define data path\r\ndata_path = PATH + '/data'\r\ndata_dir_list = os.listdir(data_path)\r\n\r\nimg_rows=128\r\nimg_cols=128\r\nnum_channel=1\r\nnum_epoch=20\r\n\r\n# Define the number of classes\r\nnum_classes = 4\r\n\r\n# load the 4 classes \r\nimg_data_list=[]\r\n\r\nfor dataset in data_dir_list:\r\n\timg_list=os.listdir(data_path+'/'+ dataset)\r\n\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\r\n\tfor img in img_list:\r\n\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\r\n\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\r\n\t\tinput_img_resize=cv2.resize(input_img,(128,128))\r\n\t\timg_data_list.append(input_img_resize)\r\n\t\r\nimg_data = np.array(img_data_list)     # convert the image to array\r\nimg_data = img_data.astype('float32')  # convert the image float\r\nimg_data /= 255                        # normalize the image to 255\r\nprint (img_data.shape)\r\n\r\n# convert the image into a proper shape\r\nif num_channel==1:\r\n\t#if K.image_dim_ordering()=='th':               # this parameter does not work anymore because it is updated to be:\r\n\tif K.image_data_format() == 'channels_first':\r\n\t\t\timg_data= np.expand_dims(img_data, axis=1)\r\n\t\t\tprint (img_data.shape)\r\n\telse:\r\n\t\t\timg_data= np.expand_dims(img_data, axis=4) \r\n\t\t\tprint (img_data.shape)\r\n\t\t\r\nelse:\r\n\t\tif K.image_data_format() == 'channels_first':              # using dimenesion ordering of theano \r\n\t\t\t\timg_data=np.rollaxis(img_data,3,1)\r\n\t\t\t\tprint (img_data.shape)\r\n\r\n#%%  preprocessing\r\nUSE_SKLEARN_PREPROCESSING=False\r\n\r\nif USE_SKLEARN_PREPROCESSING:\r\n\t# using sklearn for preprocessing\r\n\tfrom sklearn import preprocessing\r\n\t\r\n\tdef image_to_feature_vector(image, size=(128, 128)):\r\n\t\t\t# resize the image to a fixed size, then flatten the image into\r\n\t\t\t# a list of raw pixel intensities\r\n\t\t\treturn cv2.resize(image, size).flatten()\r\n\t\r\n\timg_data_list=[]\r\n\tfor dataset in data_dir_list:\r\n\t\timg_list=os.listdir(data_path+'/'+ dataset)\r\n\t\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\r\n\t\tfor img in img_list:\r\n\t\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\r\n\t\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\r\n\t\t\tinput_img_flatten=image_to_feature_vector(input_img,(128,128))\r\n\t\t\timg_data_list.append(input_img_flatten)\r\n\t\r\n\timg_data = np.array(img_data_list)\r\n\timg_data = img_data.astype('float32')\r\n\tprint (img_data.shape)\r\n\timg_data_scaled = preprocessing.scale(img_data)\r\n\tprint (img_data_scaled.shape)\r\n\t\r\n\tprint (np.mean(img_data_scaled))\r\n\tprint (np.std(img_data_scaled))\r\n\t\r\n\tprint (img_data_scaled.mean(axis=0))\r\n\tprint (img_data_scaled.std(axis=0))\r\n\tif K.image_data_format() == 'th':\r\n\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\r\n\t\tprint (img_data_scaled.shape)\r\n\t\t\r\n\telse:\r\n\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\r\n\t\tprint (img_data_scaled.shape)\r\n\tif K.image_data_format() == 'th':     \r\n\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\r\n\t\tprint (img_data_scaled.shape)\r\n\t\t\r\n\telse:\r\n\t\timg_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\r\n\t\tprint (img_data_scaled.shape)\r\n\r\nif USE_SKLEARN_PREPROCESSING:\r\n\timg_data=img_data_scaled\r\n\r\n#%%\r\n# Assigning Labels\r\n# Define the number of classes\r\nnum_classes = 4\r\n\r\nnum_of_samples = img_data.shape[0]\r\nlabels = np.ones((num_of_samples,),dtype='int64')\r\n\r\nlabels[0:202]=0        # label for cat\r\nlabels[202:404]=1       # label for dog\r\nlabels[404:606]=2        # label for horse\r\nlabels[606:]=3            # label for human\r\n\t  \r\nnames = ['cats','dogs','horses','humans']\r\n\r\n# convert class labels to on-hot encoding\r\nY = np_utils.to_categorical(labels, num_classes)    # represent each class with 4 digits like (1,0,0,0) for cat and (0,1,0,0) for dog\r\n\r\n#Shuffle the dataset\r\nx,y = shuffle(img_data,Y, random_state=2)\r\n\r\n# Split the dataset\r\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\r\n\r\n# Defining the model\r\ninput_shape=img_data[0].shape\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.keras import backend as k\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Convolution2D(32, 3,3,padding='same',input_shape=input_shape))    # change from boreder_mode='same' to padding='same'\r\nmodel.add(Activation('relu'))\r\nmodel.add(Convolution2D(32, 3, 3))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Convolution2D(64, 3, 3))\r\nmodel.add(Activation('relu'))\r\n#model.add(Convolution2D(64, 3, 3))\r\n#model.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes))\r\nmodel.add(Activation('softmax'))\r\n\t\t\r\n#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\n#model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\r\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\r\n\r\n# Viewing model_configuration\r\nmodel.summary()\r\n\r\nmodel.get_config()\r\nmodel.layers[0].get_config()\r\nmodel.layers[0].input_shape\t\t\t\r\nmodel.layers[0].output_shape\t\t\t\r\nmodel.layers[0].get_weights()\r\nnp.shape(model.layers[0].get_weights()[0])\r\nmodel.layers[0].trainable\r\n\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior() \r\n\r\n#%%\r\n# Training\r\nhist = model.fit(X_train, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test))\r\n\r\n# Training with callbacks\r\nfrom keras import callbacks\r\n\r\nfilename='model_train_new.csv'\r\ncsv_log=callbacks.CSVLogger(filename, separator=',', append=False)\r\n\r\nearly_stopping=callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')\r\n\r\nfilepath=\"Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5\"\r\n\r\ncheckpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n\r\ncallbacks_list = [csv_log,early_stopping,checkpoint]\r\n\r\nhist = model.fit(X_train, y_train, batch_size=16, nb_epoch=num_epoch, verbose=1, validation_data=(X_test, y_test),callbacks=callbacks_list)\r\n\r\n# visualizing losses and accuracy\r\ntrain_loss=hist.history['loss']\r\nval_loss=hist.history['val_loss']\r\ntrain_acc=hist.history['acc']\r\nval_acc=hist.history['val_acc']\r\nxc=range(num_epoch)\r\n\r\nplt.figure(1,figsize=(7,5))\r\nplt.plot(xc,train_loss)\r\nplt.plot(xc,val_loss)\r\nplt.xlabel('num of Epochs')\r\nplt.ylabel('loss')\r\nplt.title('train_loss vs val_loss')\r\nplt.grid(True)\r\nplt.legend(['train','val'])\r\n#print plt.style.available # use bmh, classic,ggplot for big pictures\r\nplt.style.use(['classic'])\r\n\r\nplt.figure(2,figsize=(7,5))\r\nplt.plot(xc,train_acc)\r\nplt.plot(xc,val_acc)\r\nplt.xlabel('num of Epochs')\r\nplt.ylabel('accuracy')\r\nplt.title('train_acc vs val_acc')\r\nplt.grid(True)\r\nplt.legend(['train','val'],loc=4)\r\n#print plt.style.available # use bmh, classic,ggplot for big pictures\r\nplt.style.use(['classic'])\r\n\r\n#%%\r\n# Evaluating the model\r\nscore = model.evaluate(X_test, y_test, show_accuracy=True, verbose=0)\r\nprint('Test Loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n\r\ntest_image = X_test[0:1]\r\nprint (test_image.shape)\r\n\r\nprint(model.predict(test_image))\r\nprint(model.predict_classes(test_image))\r\nprint(y_test[0:1])\r\n\r\n# Testing a new image\r\ntest_image = cv2.imread('data/Humans/rider-8.jpg')\r\ntest_image=cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)\r\ntest_image=cv2.resize(test_image,(128,128))\r\ntest_image = np.array(test_image)\r\ntest_image = test_image.astype('float32')\r\ntest_image /= 255\r\nprint (test_image.shape)\r\n\r\nif num_channel==1:\r\n\t\t\t\tif K.image_data_format()\t==\t'channels_first':\r\n\t\t\t\t\t\t\t\ttest_image= np.expand_dims(test_image, axis=0)\r\n\t\t\t\t\t\t\t\ttest_image= np.expand_dims(test_image, axis=0)\r\n\t\t\t\t\t\t\t\tprint (test_image.shape)\r\n\t\t\t\telse:\r\n\t\t\t\t\t\t\t\ttest_image= np.expand_dims(test_image, axis=3) \r\n\t\t\t\t\t\t\t\ttest_image= np.expand_dims(test_image, axis=0)\r\n\t\t\t\t\t\t\t\tprint (test_image.shape)\r\nelse:\r\n\t\t\t\tif K.image_data_format() == 'channels_first':\r\n\t\t\t\t\t\t\t\ttest_image=np.rollaxis(test_image,2,0)\r\n\t\t\t\t\t\t\t\ttest_image= np.expand_dims(test_image, axis=0)\r\n\t\t\t\t\t\t\t\tprint (test_image.shape)\r\n\t\t\t\telse:\r\n\t\t\t\t\t\t\t\ttest_image= np.expand_dims(test_image, axis=0)\r\n\t\t\t\t\t\t\t\tprint (test_image.shape)\r\n\t\t\r\n# Predicting the test image\r\nprint((model.predict(test_image)))\r\nprint(model.predict_classes(test_image))\r\n\r\n#%%\r\n# Visualizing the intermediate layer\r\ndef get_featuremaps(model, layer_idx, X_batch):\r\n\tget_activations = K.function([model.layers[0].input, K.learning_phase()],[model.layers[layer_idx].output,])\r\n\tactivations = get_activations([X_batch,0])\r\n\treturn activations\r\n\r\nlayer_num=3\r\nfilter_num=0\r\n\r\nactivations = get_featuremaps(model, int(layer_num),test_image)\r\n\r\nprint (np.shape(activations))\r\nfeature_maps = activations[0][0]      \r\nprint (np.shape(feature_maps))\r\n\r\n#if K.image_dim_ordering()=='th':\r\nif K.image_data_format() == 'channels_first':\r\n\tfeature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\r\nprint (feature_maps.shape)\r\n\r\nfig=plt.figure(figsize=(16,16))\r\nplt.imshow(feature_maps[:,:,filter_num],cmap='gray')\r\nplt.savefig(\"featuremaps-layer-{}\".format(layer_num) + \"-filternum-{}\".format(filter_num)+'.jpg')\r\n\r\nnum_of_featuremaps=feature_maps.shape[2]\r\nfig=plt.figure(figsize=(16,16))\t\r\nplt.title(\"featuremaps-layer-{}\".format(layer_num))\r\nsubplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\r\nfor i in range(int(num_of_featuremaps)):\r\n\tax = fig.add_subplot(subplot_num, subplot_num, i+1)\r\n\t#ax.imshow(output_image[0,:,:,i],interpolation='nearest' ) #to see the first filter\r\n\tax.imshow(feature_maps[:,:,i],cmap='gray')\r\n\tplt.xticks([])\r\n\tplt.yticks([])\r\n\tplt.tight_layout()\r\nplt.show()\r\nfig.savefig(\"featuremaps-layer-{}\".format(layer_num) + '.jpg')\r\n\r\n#%%\r\n# Printing the confusion matrix\r\nfrom sklearn.metrics import classification_report,confusion_matrix\r\nimport itertools\r\n\r\nY_pred = model.predict(X_test)\r\nprint(Y_pred)\r\ny_pred = np.argmax(Y_pred, axis=1)\r\nprint(y_pred)\r\n#y_pred = model.predict_classes(X_test)\r\n#print(y_pred)\r\ntarget_names = ['class 0(cats)', 'class 1(Dogs)', 'class 2(Horses)','class 3(Humans)']\r\n\t\t\t\t\t\r\nprint(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\r\n\r\nprint(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\r\n\r\n\r\n# Plotting the confusion matrix\r\ndef plot_confusion_matrix(cm, classes,\r\n                          normalize=False,\r\n                          title='Confusion matrix',\r\n                          cmap=plt.cm.Blues):\r\n    \"\"\"\r\n    This function prints and plots the confusion matrix.\r\n    Normalization can be applied by setting `normalize=True`.\r\n    \"\"\"\r\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n    plt.title(title)\r\n    plt.colorbar()\r\n    tick_marks = np.arange(len(classes))\r\n    plt.xticks(tick_marks, classes, rotation=45)\r\n    plt.yticks(tick_marks, classes)\r\n\r\n    if normalize:\r\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n        print(\"Normalized confusion matrix\")\r\n    else:\r\n        print('Confusion matrix, without normalization')\r\n\r\n    print(cm)\r\n\r\n    thresh = cm.max() / 2.\r\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n        plt.text(j, i, cm[i, j],\r\n                 horizontalalignment=\"center\",\r\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n\r\n    plt.tight_layout()\r\n    plt.ylabel('True label')\r\n    plt.xlabel('Predicted label')\r\n\r\n# Compute confusion matrix\r\ncnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\r\n\r\nnp.set_printoptions(precision=2)\r\n\r\nplt.figure()\r\n\r\n# Plot non-normalized confusion matrix\r\nplot_confusion_matrix(cnf_matrix, classes=target_names,\r\n                      title='Confusion matrix')\r\n#plt.figure()\r\n# Plot normalized confusion matrix\r\n#plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True,\r\n#                      title='Normalized confusion matrix')\r\n#plt.figure()\r\nplt.show()\r\n\r\n#%%\r\n# Saving and loading model and weights\r\nfrom keras.models import model_from_json\r\nfrom keras.models import load_model\r\n\r\n# serialize model to JSON\r\nmodel_json = model.to_json()\r\nwith open(\"model.json\", \"w\") as json_file:\r\n    json_file.write(model_json)\r\n# serialize weights to HDF5\r\nmodel.save_weights(\"model.h5\")\r\nprint(\"Saved model to disk\")\r\n\r\n# load json and create model\r\njson_file = open('model.json', 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nloaded_model = model_from_json(loaded_model_json)\r\n# load weights into new model\r\nloaded_model.load_weights(\"model.h5\")\r\nprint(\"Loaded model from disk\")\r\n\r\nmodel.save('model.hdf5')\r\nloaded_model=load_model('model.hdf5')\r\n\r\n\r\n\r\n\r\n\r\n", "@Al-Badri179 \r\nThere are way too many indentation errors in the code please provide with a colab gist with the error for us to analyse.", "I don't know how to do that in colab, how you can assist me to do this?", "@Al-Badri179 \r\nPlease use [this link](https://colab.sandbox.google.com/notebooks/intro.ipynb#recent=true),please use this to update the code and error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "am facing this error too, have you found a solution ? \r\n", "@Yessmin \r\nCan you please create a new issue with all the details for us to analyse and help."]}]