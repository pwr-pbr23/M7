[{"number": 46609, "title": "Trilinear interpolation in UpSampling3D", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent UpSampling3D implementation only has the nearest-neighbor interpolation. UpSampling2D has both nearest-neighbor and bilinear interpolations. \r\n\r\n**Will this change the current api? How?**\r\nYes, with trilinear interpolation UpSampling3D will have an argument like interpolation='trilinear', similar to the one in UpSampling2D, interpolation='bilinear'. \r\n\r\n**Who will benefit with this feature?**\r\n3D vision tasks that use linear interpolation upsampling features, such as in 3D medical image segmentation. \r\n\r\n**Any Other info.**", "comments": ["This sounds like a good feature, would you be willing to contribute?", "> This sounds like a good feature, would you be willing to contribute?\r\n\r\nI am not a professional software engineer, but I am interested in this so did a tentative implementation on this topic, with hope that it can serve as a reference to the official developers of tf:\r\n-- I implemented (1) nearest neighbor, (2) linear, (3) cubic, and (4) PCHIP (piecewise cubic Hermite interpolating polynomial) interpolations for 1D, 2D and 3D tf upsampling layers (both channels_first and channels_last) in a uniform fashion. Therefore, the new UpSampling?D layers now can use these four interpolation methods. \r\n-- In the new UpSampling2D and UpSampling3D, I have set the default interpolation to be `nearest`, and have set that the legal options are `nearest`, `blinear`/`trilinear`, `linear`, `cubic`, and `pchip`. In these options, `linear` is equal to `bilinear` in the 2D case and is equal to `trilinear`in the 3D case. \r\n-- I suggest that the upsampling layers have an argument `align_corners`, as implemented in PyTorch. I implemented this option in the new upsampling layers for all 1D, 2D and 3D cases, and have set the default value to be `True` (PyTorch set a default value of `False`, which I think is perhaps suboptimal in terms of interpolation). \r\n-- I only did several simple tests on the newly implemented interpolation functions, but I don't know how to test the revised upsampling layers. Basically, I only modified the call function part in the UpSampling?D classes. The other modifications to these classes are trivial. The revised UpSampling?D parts should be correct and accurate, but obviously require more rigorous, more systematic tests. \r\n-- The driver functions for linear, cubic and PCHIP interpolations are obtained from others, where I have indicated the sources in the Link line. I only implemented the interfaces of these interpolations so that they can be used in tf. \r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom numpy import (zeros, where, diff, floor, minimum, maximum, array, concatenate, logical_or, logical_xor,\r\n                   sqrt)\r\n\r\n\r\ndef linear_interpolate(x_fix, y_fix, x_var):\r\n    '''\r\n        Functionality:\r\n            1D linear interpolation\r\n        Author:\r\n            Michael Osthege\r\n        Link:\r\n            https://gist.github.com/michaelosthege/e20d242bc62a434843b586c78ebce6cc\r\n    '''\r\n\r\n    x_repeat = np.tile(x_var[:, None], (len(x_fix), ))\r\n    distances = np.abs(x_repeat - x_fix)\r\n\r\n    x_indices = np.searchsorted(x_fix, x_var)\r\n\r\n    weights = np.zeros_like(distances)\r\n    idx = np.arange(len(x_indices))\r\n    weights[idx, x_indices] = distances[idx, x_indices - 1]\r\n    weights[idx, x_indices - 1] = distances[idx, x_indices]\r\n    weights /= np.sum(weights, axis=1)[:, None]\r\n\r\n    y_var = np.dot(weights, y_fix.T)\r\n\r\n    return y_var\r\n\r\n\r\ndef cubic_interpolate(x, y, x0):\r\n    '''\r\n        Functionliaty:\r\n            1D cubic spline interpolation\r\n        Author:\r\n            Raphael Valentin\r\n        Link:\r\n            https://stackoverflow.com/questions/31543775/how-to-perform-cubic-spline-interpolation-in-python\r\n    '''\r\n\r\n    x = np.asfarray(x)\r\n    y = np.asfarray(y)\r\n\r\n    # remove non finite values\r\n    # indexes = np.isfinite(x)\r\n    # x = x[indexes]\r\n    # y = y[indexes]\r\n\r\n    # check if sorted\r\n    if np.any(np.diff(x) < 0):\r\n        indexes = np.argsort(x)\r\n        x = x[indexes]\r\n        y = y[indexes]\r\n\r\n    size = len(x)\r\n\r\n    xdiff = np.diff(x)\r\n    ydiff = np.diff(y)\r\n\r\n    # allocate buffer matrices\r\n    Li = np.empty(size)\r\n    Li_1 = np.empty(size - 1)\r\n    z = np.empty(size)\r\n\r\n    # fill diagonals Li and Li-1 and solve [L][y] = [B]\r\n    Li[0] = sqrt(2 * xdiff[0])\r\n    Li_1[0] = 0.0\r\n    B0 = 0.0  # natural boundary\r\n    z[0] = B0 / Li[0]\r\n\r\n    for i in range(1, size - 1, 1):\r\n        Li_1[i] = xdiff[i - 1] / Li[i - 1]\r\n        Li[i] = sqrt(2 * (xdiff[i - 1] + xdiff[i]) - Li_1[i - 1] * Li_1[i - 1])\r\n        Bi = 6 * (ydiff[i] / xdiff[i] - ydiff[i - 1] / xdiff[i - 1])\r\n        z[i] = (Bi - Li_1[i - 1] * z[i - 1]) / Li[i]\r\n\r\n    i = size - 1\r\n    Li_1[i - 1] = xdiff[-1] / Li[i - 1]\r\n    Li[i] = sqrt(2 * xdiff[-1] - Li_1[i - 1] * Li_1[i - 1])\r\n    Bi = 0.0  # natural boundary\r\n    z[i] = (Bi - Li_1[i - 1] * z[i - 1]) / Li[i]\r\n\r\n    # solve [L.T][x] = [y]\r\n    i = size - 1\r\n    z[i] = z[i] / Li[i]\r\n    for i in range(size - 2, -1, -1):\r\n        z[i] = (z[i] - Li_1[i - 1] * z[i + 1]) / Li[i]\r\n\r\n    # find index\r\n    index = x.searchsorted(x0)\r\n    np.clip(index, 1, size - 1, index)\r\n\r\n    xi1, xi0 = x[index], x[index - 1]\r\n    yi1, yi0 = y[index], y[index - 1]\r\n    zi1, zi0 = z[index], z[index - 1]\r\n    hi1 = xi1 - xi0\r\n\r\n    # calculate cubic\r\n    f0 = zi0/(6*hi1)*(xi1-x0)**3 + \\\r\n         zi1/(6*hi1)*(x0-xi0)**3 + \\\r\n         (yi1/hi1 - zi1*hi1/6)*(x0-xi0) + \\\r\n         (yi0/hi1 - zi0*hi1/6)*(xi1-x0)\r\n\r\n    return f0\r\n\r\n\r\ndef pchip_interpolate(xi, yi, x, mode=\"mono\", verbose=False):\r\n    ''' \r\n        Functionality:\r\n            1D PCHP interpolation\r\n        Authors:\r\n            Michael Taylor <mtaylor@atlanticsciences.com>\r\n            Mathieu Virbel <mat@meltingrocks.com>\r\n        Link:\r\n            https://gist.github.com/tito/553f1135959921ce6699652bf656150d\r\n    '''\r\n\r\n    if mode not in (\"mono\", \"quad\"):\r\n        raise ValueError(\"Unrecognized mode string\")\r\n\r\n    # Search for [xi,xi+1] interval for each x\r\n    xi = xi.astype(\"double\")\r\n    yi = yi.astype(\"double\")\r\n\r\n    x_index = zeros(len(x), dtype=\"int\")\r\n    xi_steps = diff(xi)\r\n    if not all(xi_steps > 0):\r\n        raise ValueError(\"x-coordinates are not in increasing order.\")\r\n\r\n    x_steps = diff(x)\r\n    if xi_steps.max() / xi_steps.min() < 1.000001:\r\n        # uniform input grid\r\n        if verbose:\r\n            print(\"pchip: uniform input grid\")\r\n        xi_start = xi[0]\r\n        xi_step = (xi[-1] - xi[0]) / (len(xi) - 1)\r\n        x_index = minimum(maximum(floor((x - xi_start) / xi_step).astype(int), 0), len(xi) - 2)\r\n\r\n        # Calculate gradients d\r\n        h = (xi[-1] - xi[0]) / (len(xi) - 1)\r\n        d = zeros(len(xi), dtype=\"double\")\r\n        if mode == \"quad\":\r\n            # quadratic polynomial fit\r\n            d[[0]] = (yi[1] - yi[0]) / h\r\n            d[[-1]] = (yi[-1] - yi[-2]) / h\r\n            d[1:-1] = (yi[2:] - yi[0:-2]) / 2 / h\r\n        else:\r\n            # mode=='mono', Fritsch-Carlson algorithm from fortran numerical\r\n            # recipe\r\n            delta = diff(yi) / h\r\n            d = concatenate((delta[0:1], 2 / (1 / delta[0:-1] + 1 / delta[1:]), delta[-1:]))\r\n            d[concatenate((array([False]), logical_xor(delta[0:-1] > 0, delta[1:] > 0), array([False])))] = 0\r\n            d[logical_or(concatenate((array([False]), delta == 0)), concatenate(\r\n                (delta == 0, array([False]))))] = 0\r\n        # Calculate output values y\r\n        dxxi = x - xi[x_index]\r\n        dxxid = x - xi[1 + x_index]\r\n        dxxi2 = pow(dxxi, 2)\r\n        dxxid2 = pow(dxxid, 2)\r\n        y = (2 / pow(h, 3) * (yi[x_index] * dxxid2 * (dxxi + h / 2) - yi[1 + x_index] * dxxi2 *\r\n                              (dxxid - h / 2)) + 1 / pow(h, 2) *\r\n             (d[x_index] * dxxid2 * dxxi + d[1 + x_index] * dxxi2 * dxxid))\r\n    else:\r\n        # not uniform input grid\r\n        if (x_steps.max() / x_steps.min() < 1.000001 and x_steps.max() / x_steps.min() > 0.999999):\r\n            # non-uniform input grid, uniform output grid\r\n            if verbose:\r\n                print(\"pchip: non-uniform input grid, uniform output grid\")\r\n            x_decreasing = x[-1] < x[0]\r\n            if x_decreasing:\r\n                x = x[::-1]\r\n            x_start = x[0]\r\n            x_step = (x[-1] - x[0]) / (len(x) - 1)\r\n            x_indexprev = -1\r\n            for xi_loop in range(len(xi) - 2):\r\n                x_indexcur = max(int(floor((xi[1 + xi_loop] - x_start) / x_step)), -1)\r\n                x_index[1 + x_indexprev:1 + x_indexcur] = xi_loop\r\n                x_indexprev = x_indexcur\r\n            x_index[1 + x_indexprev:] = len(xi) - 2\r\n            if x_decreasing:\r\n                x = x[::-1]\r\n                x_index = x_index[::-1]\r\n        elif all(x_steps > 0) or all(x_steps < 0):\r\n            # non-uniform input/output grids, output grid monotonic\r\n            if verbose:\r\n                print(\"pchip: non-uniform in/out grid, output grid monotonic\")\r\n            x_decreasing = x[-1] < x[0]\r\n            if x_decreasing:\r\n                x = x[::-1]\r\n            x_len = len(x)\r\n            x_loop = 0\r\n            for xi_loop in range(len(xi) - 1):\r\n                while x_loop < x_len and x[x_loop] < xi[1 + xi_loop]:\r\n                    x_index[x_loop] = xi_loop\r\n                    x_loop += 1\r\n            x_index[x_loop:] = len(xi) - 2\r\n            if x_decreasing:\r\n                x = x[::-1]\r\n                x_index = x_index[::-1]\r\n        else:\r\n            # non-uniform input/output grids, output grid not monotonic\r\n            if verbose:\r\n                print(\"pchip: non-uniform in/out grids, \" \"output grid not monotonic\")\r\n            for index in range(len(x)):\r\n                loc = where(x[index] < xi)[0]\r\n                if loc.size == 0:\r\n                    x_index[index] = len(xi) - 2\r\n                elif loc[0] == 0:\r\n                    x_index[index] = 0\r\n                else:\r\n                    x_index[index] = loc[0] - 1\r\n        # Calculate gradients d\r\n        h = diff(xi)\r\n        d = zeros(len(xi), dtype=\"double\")\r\n        delta = diff(yi) / h\r\n        if mode == \"quad\":\r\n            # quadratic polynomial fit\r\n            d[[0, -1]] = delta[[0, -1]]\r\n            d[1:-1] = (delta[1:] * h[0:-1] + delta[0:-1] * h[1:]) / (h[0:-1] + h[1:])\r\n        else:\r\n            # mode=='mono', Fritsch-Carlson algorithm from fortran numerical\r\n            # recipe\r\n            d = concatenate(\r\n                (delta[0:1], 3 * (h[0:-1] + h[1:]) / ((h[0:-1] + 2 * h[1:]) / delta[0:-1] +\r\n                                                      (2 * h[0:-1] + h[1:]) / delta[1:]), delta[-1:]))\r\n            d[concatenate((array([False]), logical_xor(delta[0:-1] > 0, delta[1:] > 0), array([False])))] = 0\r\n            d[logical_or(concatenate((array([False]), delta == 0)), concatenate(\r\n                (delta == 0, array([False]))))] = 0\r\n        dxxi = x - xi[x_index]\r\n        dxxid = x - xi[1 + x_index]\r\n        dxxi2 = pow(dxxi, 2)\r\n        dxxid2 = pow(dxxid, 2)\r\n        y = (2 / pow(h[x_index], 3) *\r\n             (yi[x_index] * dxxid2 * (dxxi + h[x_index] / 2) - yi[1 + x_index] * dxxi2 *\r\n              (dxxid - h[x_index] / 2)) + 1 / pow(h[x_index], 2) *\r\n             (d[x_index] * dxxid2 * dxxi + d[1 + x_index] * dxxi2 * dxxid))\r\n    return y\r\n\r\n\r\ndef Interpolate1D(x, y, xx, method='nearest'):\r\n    '''\r\n        Functionality:\r\n            1D interpolation with various methods\r\n        Author:\r\n            Kai Gao <nebulaekg@gmail.com>\r\n    '''\r\n\r\n    n = len(x)\r\n    nn = len(xx)\r\n    yy = np.zeros(nn)\r\n\r\n    # Nearest neighbour interpolation\r\n    if method == 'nearest':\r\n        for i in range(0, nn):\r\n            xi = np.abs(xx[i] - x).argmin()\r\n            yy[i] = y[xi]\r\n\r\n    # Linear interpolation\r\n    elif method == 'linear':\r\n\r\n        # # slower version\r\n        # if n == 1:\r\n        #     yy[:-1] = y[0]\r\n\r\n        # else:\r\n        #     for i in range(0, nn):\r\n\r\n        #         if xx[i] < x[0]:\r\n        #             t = (xx[i] - x[0]) / (x[1] - x[0])\r\n        #             yy[i] = (1.0 - t) * y[0] + t * y[1]\r\n\r\n        #         elif x[n - 1] <= xx[i]:\r\n        #             t = (xx[i] - x[n - 2]) / (x[n - 1] - x[n - 2])\r\n        #             yy[i] = (1.0 - t) * y[n - 2] + t * y[n - 1]\r\n\r\n        #         else:\r\n        #             for k in range(1, n):\r\n        #                 if x[k - 1] <= xx[i] and xx[i] < x[k]:\r\n        #                     t = (xx[i] - x[k - 1]) / (x[k] - x[k - 1])\r\n        #                     yy[i] = (1.0 - t) * y[k - 1] + t * y[k]\r\n        #                     break\r\n\r\n        # # faster version\r\n        yy = linear_interpolate(x, y, xx)\r\n\r\n    # Cubic interpolation\r\n    elif method == 'cubic':\r\n        yy = cubic_interpolate(x, y, xx)\r\n\r\n    # Piecewise cubic Hermite interpolating polynomial (PCHIP)\r\n    elif method == 'pchip':\r\n        yy = pchip_interpolate(x, y, xx, mode='mono')\r\n\r\n    return yy\r\n\r\n\r\ndef Interpolate2D(x, y, f, xx, yy, method='nearest'):\r\n    '''\r\n        Functionality:\r\n            2D interpolation implemented in a separable fashion\r\n            There are methods that do real 2D non-separable interpolation, which are\r\n                more difficult to implement. \r\n        Author:\r\n            Kai Gao <nebulaekg@gmail.com>\r\n    '''\r\n\r\n    n1 = len(x)\r\n    n2 = len(y)\r\n    nn1 = len(xx)\r\n    nn2 = len(yy)\r\n\r\n    w = np.zeros((nn1, n2))\r\n    ff = np.zeros((nn1, nn2))\r\n\r\n    # Interpolate along the 1st dimension\r\n    for j in range(0, n2):\r\n        w[:, j] = Interpolate1D(x, f[:, j], xx, method)\r\n\r\n    # Interpolate along the 2nd dimension\r\n    for i in range(0, nn1):\r\n        ff[i, :] = Interpolate1D(y, w[i, :], yy, method)\r\n\r\n    return ff\r\n\r\n\r\ndef Interpolate3D(x, y, z, f, xx, yy, zz, method='nearest'):\r\n    '''\r\n        Functionality:\r\n            3D interpolation implemented in a separable fashion\r\n            There are methods that do real 3D non-separable interpolation, which are\r\n                more difficult to implement. \r\n        Author:\r\n            Kai Gao <nebulaekg@gmail.com>\r\n    '''\r\n\r\n    n1 = len(x)\r\n    n2 = len(y)\r\n    n3 = len(z)\r\n    nn1 = len(xx)\r\n    nn2 = len(yy)\r\n    nn3 = len(zz)\r\n\r\n    w1 = np.zeros((nn1, n2, n3))\r\n    w2 = np.zeros((nn1, nn2, n3))\r\n    ff = np.zeros((nn1, nn2, nn3))\r\n\r\n    # Interpolate along the 1st dimension\r\n    for k in range(0, n3):\r\n        for j in range(0, n2):\r\n            w1[:, j, k] = Interpolate1D(x, f[:, j, k], xx, method)\r\n\r\n    # Interpolate along the 2nd dimension\r\n    for k in range(0, n3):\r\n        for i in range(0, nn1):\r\n            w2[i, :, k] = Interpolate1D(y, w1[i, :, k], yy, method)\r\n\r\n    # Interpolate along the 3rd dimension\r\n    for j in range(0, nn2):\r\n        for i in range(0, nn1):\r\n            ff[i, j, :] = Interpolate1D(z, w2[i, j, :], zz, method)\r\n\r\n    return ff\r\n\r\n\r\ndef UpInterpolate1D(x, size=2, interpolation='nearest', data_format='channels_first', align_corners=True):\r\n    '''\r\n        Functionality:\r\n            1D upsampling interpolation for tf\r\n        Author:\r\n            Kai Gao <nebulaekg@gmail.com>\r\n    '''\r\n\r\n    x = x.numpy()\r\n\r\n    if data_format == 'channels_last':\r\n        nb, nr, nh = x.shape\r\n    elif data_format == 'channels_first':\r\n        nb, nh, nr = x.shape\r\n\r\n    r = size\r\n    ir = np.linspace(0.0, nr - 1.0, num=nr)\r\n\r\n    if align_corners:\r\n        # align_corners=True assumes that values are sampled at discrete points\r\n        iir = np.linspace(0.0, nr - 1.0, num=nr * r)\r\n    else:\r\n        # aling_corners=False assumes that values are sampled at centers of discrete blocks\r\n        iir = np.linspace(0.0 - 0.5 + 0.5 / r, nr - 1.0 + 0.5 - 0.5 / r, num=nr * r)\r\n        iir = np.clip(iir, 0.0, nr - 1.0)\r\n\r\n    if data_format == 'channels_last':\r\n        xx = np.zeros((nb, nr * r, nh))\r\n        for i in range(0, nb):\r\n            for j in range(0, nh):\r\n                t = np.reshape(x[i, :, j], (nr))\r\n                xx[i, :, j] = Interpolate1D(ir, t, iir, interpolation)\r\n\r\n    elif data_format == 'channels_first':\r\n        xx = np.zeros((nb, nh, nr * r))\r\n        for i in range(0, nb):\r\n            for j in range(0, nh):\r\n                t = np.reshape(x[i, j, :], (nr))\r\n                xx[i, j, :] = Interpolate1D(ir, t, iir, interpolation)\r\n\r\n    return tf.convert_to_tensor(xx, dtype=x.dtype)\r\n\r\n\r\ndef UpInterpolate2D(x,\r\n                    size=(2, 2),\r\n                    interpolation='nearest',\r\n                    data_format='channels_first',\r\n                    align_corners=True):\r\n    '''\r\n        Functionality:\r\n            2D upsampling interpolation for tf\r\n        Author:\r\n            Kai Gao <nebulaekg@gmail.com>\r\n    '''\r\n\r\n    x = x.numpy()\r\n\r\n    if data_format == 'channels_last':\r\n        nb, nr, nc, nh = x.shape\r\n    elif data_format == 'channels_first':\r\n        nb, nh, nr, nc = x.shape\r\n\r\n    r = size[0]\r\n    c = size[1]\r\n    ir = np.linspace(0.0, nr - 1.0, num=nr)\r\n    ic = np.linspace(0.0, nc - 1.0, num=nc)\r\n\r\n    if align_corners:\r\n        # align_corners=True assumes that values are sampled at discrete points\r\n        iir = np.linspace(0.0, nr - 1.0, num=nr * r)\r\n        iic = np.linspace(0.0, nc - 1.0, num=nc * c)\r\n    else:\r\n        # aling_corners=False assumes that values are sampled at centers of discrete blocks\r\n        iir = np.linspace(0.0 - 0.5 + 0.5 / r, nr - 1.0 + 0.5 - 0.5 / r, num=nr * r)\r\n        iic = np.linspace(0.0 - 0.5 + 0.5 / c, nc - 1.0 + 0.5 - 0.5 / c, num=nc * c)\r\n        iir = np.clip(iir, 0.0, nr - 1.0)\r\n        iic = np.clip(iic, 0.0, nc - 1.0)\r\n\r\n    if data_format == 'channels_last':\r\n        xx = np.zeros((nb, nr * r, nc * c, nh))\r\n        for i in range(0, nb):\r\n            for j in range(0, nh):\r\n                t = np.reshape(x[i, :, :, j], (nr, nc))\r\n                xx[i, :, :, j] = Interpolate2D(ir, ic, t, iir, iic, interpolation)\r\n\r\n    elif data_format == 'channels_first':\r\n        xx = np.zeros((nb, nh, nr * r, nc * c))\r\n        for i in range(0, nb):\r\n            for j in range(0, nh):\r\n                t = np.reshape(x[i, j, :, :], (nr, nc))\r\n                xx[i, j, :, :] = Interpolate2D(ir, ic, t, iir, iic, interpolation)\r\n\r\n    return tf.convert_to_tensor(xx, dtype=x.dtype)\r\n\r\n\r\ndef UpInterpolate3D(x,\r\n                    size=(2, 2, 2),\r\n                    interpolation='nearest',\r\n                    data_format='channels_first',\r\n                    align_corners=True):\r\n    '''\r\n        Functionality:\r\n            3D upsampling interpolation for tf\r\n        Author:\r\n            Kai Gao <nebulaekg@gmail.com>\r\n    '''\r\n\r\n    x = x.numpy()\r\n\r\n    if data_format == 'channels_last':\r\n        nb, nr, nc, nd, nh = x.shape\r\n    elif data_format == 'channels_first':\r\n        nb, nh, nr, nc, nd = x.shape\r\n\r\n    r = size[0]\r\n    c = size[1]\r\n    d = size[2]\r\n    ir = np.linspace(0.0, nr - 1.0, num=nr)\r\n    ic = np.linspace(0.0, nc - 1.0, num=nc)\r\n    id = np.linspace(0.0, nd - 1.0, num=nd)\r\n\r\n    if align_corners:\r\n        # align_corners=True assumes that values are sampled at discrete points\r\n        iir = np.linspace(0.0, nr - 1.0, num=nr * r)\r\n        iic = np.linspace(0.0, nc - 1.0, num=nc * c)\r\n        iid = np.linspace(0.0, nd - 1.0, num=nd * d)\r\n    else:\r\n        # aling_corners=False assumes that values are sampled at centers of discrete blocks\r\n        iir = np.linspace(0.0 - 0.5 + 0.5 / r, nr - 1.0 + 0.5 - 0.5 / r, num=nr * r)\r\n        iic = np.linspace(0.0 - 0.5 + 0.5 / c, nc - 1.0 + 0.5 - 0.5 / c, num=nc * c)\r\n        iid = np.linspace(0.0 - 0.5 + 0.5 / d, nd - 1.0 + 0.5 - 0.5 / d, num=nd * d)\r\n        iir = np.clip(iir, 0.0, nr - 1.0)\r\n        iic = np.clip(iic, 0.0, nc - 1.0)\r\n        iid = np.clip(iid, 0.0, nd - 1.0)\r\n\r\n    if data_format == 'channels_last':\r\n        xx = np.zeros((nb, nr * r, nc * c, nd * d, nh))\r\n        for i in range(0, nb):\r\n            for j in range(0, nh):\r\n                t = np.reshape(x[i, :, :, :, j], (nr, nc, nd))\r\n                xx[i, :, :, :, j] = Interpolate3D(ir, ic, id, t, iir, iic, iid, interpolation)\r\n\r\n    elif data_format == 'channels_first':\r\n        xx = np.zeros((nb, nh, nr * r, nc * c, nd * d))\r\n        for i in range(0, nb):\r\n            for j in range(0, nh):\r\n                t = np.reshape(x[i, j, :, :, :], (nr, nc, nd))\r\n                xx[i, j, :, :, :] = Interpolate3D(ir, ic, id, t, iir, iic, iid, interpolation)\r\n\r\n    return tf.convert_to_tensor(xx, dtype=x.dtype)\r\n\r\n\r\n# ################################################################################\r\n@keras_export('keras.layers.UpSampling1D')\r\nclass UpSampling1D(Layer):\r\n    \"\"\"Upsampling layer for 1D inputs.\r\n  Repeats each temporal step `size` times along the time axis.\r\n  Examples:\r\n  >>> input_shape = (2, 2, 3)\r\n  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\r\n  >>> print(x)\r\n  [[[ 0  1  2]\r\n    [ 3  4  5]]\r\n    [[ 6  7  8]\r\n    [ 9 10 11]]]\r\n  >>> y = tf.keras.layers.UpSampling1D(size=2)(x)\r\n  >>> print(y)\r\n  tf.Tensor(\r\n    [[[ 0  1  2]\r\n      [ 0  1  2]\r\n      [ 3  4  5]\r\n      [ 3  4  5]]\r\n      [[ 6  7  8]\r\n      [ 6  7  8]\r\n      [ 9 10 11]\r\n      [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)\r\n  Args:\r\n    size: Integer. Upsampling factor.\r\n  Input shape:\r\n    3D tensor with shape: `(batch_size, steps, features)`.\r\n  Output shape:\r\n    3D tensor with shape: `(batch_size, upsampled_steps, features)`.\r\n  \"\"\"\r\n    def __init__(self, size=2, data_format='None', interpolation='nearest', align_corners=True, **kwargs):\r\n        super(UpSampling1D, self).__init__(**kwargs)\r\n        self.data_format = conv_utils.normalize_data_format(data_format)\r\n        self.size = int(size)\r\n        self.input_spec = InputSpec(ndim=3)\r\n        self.interpolation = interpolation\r\n        if self.interpolation not in {'nearest', 'linear', 'cubic', 'pchip'}:\r\n            raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\r\n                             'or `\"linear\"` '\r\n                             'or `\"cubic\"` '\r\n                             'or `\"pchip\"`.')\r\n        self.align_corners = align_corners\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        input_shape = tf.TensorShape(input_shape).as_list()\r\n        size = self.size * input_shape[1] if input_shape[1] is not None else None\r\n        return tf.TensorShape([input_shape[0], size, input_shape[2]])\r\n\r\n    def call(self, inputs):\r\n        return UpInterpolate1D(inputs,\r\n                               self.size,\r\n                               data_format=self.data_format,\r\n                               interpolation=self.interpolation,\r\n                               align_corners=self.align_corners)\r\n\r\n    def get_config(self):\r\n        config = {'size': self.size}\r\n        base_config = super(UpSampling1D, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\n@keras_export('keras.layers.UpSampling2D')\r\nclass UpSampling2D(Layer):\r\n    \"\"\"Upsampling layer for 2D inputs.\r\n  Repeats the rows and columns of the data\r\n  by `size[0]` and `size[1]` respectively.\r\n  Examples:\r\n  >>> input_shape = (2, 2, 1, 3)\r\n  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\r\n  >>> print(x)\r\n  [[[[ 0  1  2]]\r\n    [[ 3  4  5]]]\r\n    [[[ 6  7  8]]\r\n    [[ 9 10 11]]]]\r\n  >>> y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)\r\n  >>> print(y)\r\n  tf.Tensor(\r\n    [[[[ 0  1  2]\r\n        [ 0  1  2]]\r\n      [[ 3  4  5]\r\n        [ 3  4  5]]]\r\n      [[[ 6  7  8]\r\n        [ 6  7  8]]\r\n      [[ 9 10 11]\r\n        [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)\r\n  Args:\r\n    size: Int, or tuple of 2 integers.\r\n      The upsampling factors for rows and columns.\r\n    data_format: A string,\r\n      one of `channels_last` (default) or `channels_first`.\r\n      The ordering of the dimensions in the inputs.\r\n      `channels_last` corresponds to inputs with shape\r\n      `(batch_size, height, width, channels)` while `channels_first`\r\n      corresponds to inputs with shape\r\n      `(batch_size, channels, height, width)`.\r\n      It defaults to the `image_data_format` value found in your\r\n      Keras config file at `~/.keras/keras.json`.\r\n      If you never set it, then it will be \"channels_last\".\r\n    interpolation: A string, one of `nearest` or `bilinear`.\r\n  Input shape:\r\n    4D tensor with shape:\r\n    - If `data_format` is `\"channels_last\"`:\r\n        `(batch_size, rows, cols, channels)`\r\n    - If `data_format` is `\"channels_first\"`:\r\n        `(batch_size, channels, rows, cols)`\r\n  Output shape:\r\n    4D tensor with shape:\r\n    - If `data_format` is `\"channels_last\"`:\r\n        `(batch_size, upsampled_rows, upsampled_cols, channels)`\r\n    - If `data_format` is `\"channels_first\"`:\r\n        `(batch_size, channels, upsampled_rows, upsampled_cols)`\r\n  \"\"\"\r\n    def __init__(self, size=(2, 2), data_format=None, interpolation='nearest', align_corners=True, **kwargs):\r\n        super(UpSampling2D, self).__init__(**kwargs)\r\n        self.data_format = conv_utils.normalize_data_format(data_format)\r\n        self.size = conv_utils.normalize_tuple(size, 2, 'size')\r\n        self.input_spec = InputSpec(ndim=4)\r\n        self.interpolation = interpolation\r\n        if self.interpolation not in {'nearest', 'bilinear', 'linear', 'cubic', 'pchip'}:\r\n            raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\r\n                             'or `\"bilinear\"` '\r\n                             'or `\"linear\"` '\r\n                             'or `\"cubic\"` '\r\n                             'or `\"pchip\"`.')\r\n        if self.interpolation == 'bilinear':\r\n            self.interpolation = 'linear'\r\n        self.align_corners = align_corners\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\r\n        if self.data_format == 'channels_first':\r\n            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\r\n            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\r\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], height, width])\r\n        else:\r\n            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\r\n            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\r\n            return tensor_shape.TensorShape([input_shape[0], height, width, input_shape[3]])\r\n\r\n    def call(self, inputs):\r\n        return UpInterpolate2D(inputs,\r\n                               self.size,\r\n                               data_format=self.data_format,\r\n                               interpolation=self.interpolation,\r\n                               align_corners=self.align_corners)\r\n\r\n    def get_config(self):\r\n        config = {'size': self.size, 'data_format': self.data_format, 'interpolation': self.interpolation}\r\n        base_config = super(UpSampling2D, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\n@keras_export('keras.layers.UpSampling3D')\r\nclass UpSampling3D(Layer):\r\n    \"\"\"Upsampling layer for 3D inputs.\r\n  Repeats the 1st, 2nd and 3rd dimensions\r\n  of the data by `size[0]`, `size[1]` and `size[2]` respectively.\r\n  Examples:\r\n  >>> input_shape = (2, 1, 2, 1, 3)\r\n  >>> x = tf.constant(1, shape=input_shape)\r\n  >>> y = tf.keras.layers.UpSampling3D(size=2)(x)\r\n  >>> print(y.shape)\r\n  (2, 2, 4, 2, 3)\r\n  Args:\r\n    size: Int, or tuple of 3 integers.\r\n      The upsampling factors for dim1, dim2 and dim3.\r\n    data_format: A string,\r\n      one of `channels_last` (default) or `channels_first`.\r\n      The ordering of the dimensions in the inputs.\r\n      `channels_last` corresponds to inputs with shape\r\n      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\r\n      while `channels_first` corresponds to inputs with shape\r\n      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\r\n      It defaults to the `image_data_format` value found in your\r\n      Keras config file at `~/.keras/keras.json`.\r\n      If you never set it, then it will be \"channels_last\".\r\n  Input shape:\r\n    5D tensor with shape:\r\n    - If `data_format` is `\"channels_last\"`:\r\n        `(batch_size, dim1, dim2, dim3, channels)`\r\n    - If `data_format` is `\"channels_first\"`:\r\n        `(batch_size, channels, dim1, dim2, dim3)`\r\n  Output shape:\r\n    5D tensor with shape:\r\n    - If `data_format` is `\"channels_last\"`:\r\n        `(batch_size, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)`\r\n    - If `data_format` is `\"channels_first\"`:\r\n        `(batch_size, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)`\r\n  \"\"\"\r\n    def __init__(self,\r\n                 size=(2, 2, 2),\r\n                 data_format=None,\r\n                 interpolation='nearest',\r\n                 align_corners=True,\r\n                 **kwargs):\r\n        super(UpSampling3D, self).__init__(**kwargs)\r\n        self.data_format = conv_utils.normalize_data_format(data_format)\r\n        self.size = conv_utils.normalize_tuple(size, 3, 'size')\r\n        self.input_spec = InputSpec(ndim=5)\r\n        self.interpolation = interpolation\r\n        if interpolation not in {'nearest', 'trilinear', 'linear', 'cubic', 'pchip'}:\r\n            raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\r\n                             'or `\"trilinear\"` '\r\n                             'or `\"linear\"` '\r\n                             'or `\"cubic\"` '\r\n                             'or `\"pchip\"`.')\r\n        if self.interpolation == 'trilinear':\r\n            self.interpolation = 'linear'\r\n        self.align_corners = align_corners\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        input_shape = tensor_shape.TensorShape(input_shape).as_list()\r\n        if self.data_format == 'channels_first':\r\n            dim1 = self.size[0] * input_shape[2] if input_shape[2] is not None else None\r\n            dim2 = self.size[1] * input_shape[3] if input_shape[3] is not None else None\r\n            dim3 = self.size[2] * input_shape[4] if input_shape[4] is not None else None\r\n            return tensor_shape.TensorShape([input_shape[0], input_shape[1], dim1, dim2, dim3])\r\n        else:\r\n            dim1 = self.size[0] * input_shape[1] if input_shape[1] is not None else None\r\n            dim2 = self.size[1] * input_shape[2] if input_shape[2] is not None else None\r\n            dim3 = self.size[2] * input_shape[3] if input_shape[3] is not None else None\r\n            return tensor_shape.TensorShape([input_shape[0], dim1, dim2, dim3, input_shape[4]])\r\n\r\n    def call(self, inputs):\r\n        return UpInterpolate3D(inputs,\r\n                               self.size,\r\n                               data_format=self.data_format,\r\n                               interpolation=self.interpolation,\r\n                               align_corners=self.align_corners)\r\n\r\n    def get_config(self):\r\n        config = {'size': self.size, 'data_format': self.data_format}\r\n        base_config = super(UpSampling3D, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n```", "```python\r\n# ###############################################################################\r\nprint('========================================================================')\r\n\r\n# Verify simple 2D matrix interpolation on align_corners with equal ratios\r\n# The result are consistent with PyTorch results\r\nc = np.array(range(1, 5)).reshape(1, 1, 2, 2)\r\nc = tf.convert_to_tensor(c, dtype=tf.float32)\r\nprint(c)\r\nd1 = UpInterpolate2D(c, (2, 2), interpolation='linear', align_corners=True)\r\nd2 = UpInterpolate2D(c, (2, 2), interpolation='linear', align_corners=False)\r\nd3 = UpInterpolate2D(c, (4, 4), interpolation='linear', align_corners=True)\r\nd4 = UpInterpolate2D(c, (4, 4), interpolation='linear', align_corners=False)\r\n\r\nimport torch\r\nc = np.array(range(1, 5)).reshape(1, 1, 2, 2)\r\nc = torch.from_numpy(c).type(torch.FloatTensor)\r\nm = torch.nn.Upsample(scale_factor=(2, 2), mode='bilinear', align_corners=True)\r\nw1 = m(c)\r\nm = torch.nn.Upsample(scale_factor=(2, 2), mode='bilinear', align_corners=False)\r\nw2 = m(c)\r\nm = torch.nn.Upsample(scale_factor=(4, 4), mode='bilinear', align_corners=True)\r\nw3 = m(c)\r\nm = torch.nn.Upsample(scale_factor=(4, 4), mode='bilinear', align_corners=False)\r\nw4 = m(c)\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up1 align=true')\r\nprint(d1.numpy())\r\nprint(w1.numpy())\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up1 align=false')\r\nprint(d2.numpy())\r\nprint(w2.numpy())\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up2 align=true')\r\nprint(d3.numpy())\r\nprint(w3.numpy())\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up2 align=false')\r\nprint(d4.numpy())\r\nprint(w4.numpy())\r\n\r\n# Verify simple 2D matrix interpolation on align_corners with unequal ratios\r\n# The result are consistent with PyTorch results\r\nc = np.array(range(1, 5)).reshape(1, 1, 2, 2)\r\nc = tf.convert_to_tensor(c, dtype=tf.float32)\r\nprint(c)\r\nd1 = UpInterpolate2D(c, (2, 3), interpolation='linear', align_corners=True)\r\nd2 = UpInterpolate2D(c, (2, 3), interpolation='linear', align_corners=False)\r\nd3 = UpInterpolate2D(c, (4, 3), interpolation='linear', align_corners=True)\r\nd4 = UpInterpolate2D(c, (4, 3), interpolation='linear', align_corners=False)\r\n\r\nimport torch\r\nc = np.array(range(1, 5)).reshape(1, 1, 2, 2)\r\nc = torch.from_numpy(c).type(torch.FloatTensor)\r\nm = torch.nn.Upsample(scale_factor=(2, 3), mode='bilinear', align_corners=True)\r\nw1 = m(c)\r\nm = torch.nn.Upsample(scale_factor=(2, 3), mode='bilinear', align_corners=False)\r\nw2 = m(c)\r\nm = torch.nn.Upsample(scale_factor=(4, 3), mode='bilinear', align_corners=True)\r\nw3 = m(c)\r\nm = torch.nn.Upsample(scale_factor=(4, 3), mode='bilinear', align_corners=False)\r\nw4 = m(c)\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up1 align=true')\r\nprint(d1.numpy())\r\nprint(w1.numpy())\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up1 align=false')\r\nprint(d2.numpy())\r\nprint(w2.numpy())\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up2 align=true')\r\nprint(d3.numpy())\r\nprint(w3.numpy())\r\n\r\nprint(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> up2 align=false')\r\nprint(d4.numpy())\r\nprint(w4.numpy())\r\n\r\nprint('========================================================================')\r\n\r\n# Verify 1D\r\nnb = 3\r\nnh = 2\r\nn1 = 40\r\nr = 8\r\nc = np.zeros(n1 * nb * nh)\r\nfor i in range(0, n1 * nb * nh):\r\n    c[i] = np.math.sin(i * 0.1 * i)\r\nc = np.reshape(c, (nb, nh, n1))\r\nc = tf.convert_to_tensor(c)\r\nd1 = UpInterpolate1D(c, r, interpolation='nearest', data_format='channels_first')\r\nd2 = UpInterpolate1D(c, r, interpolation='linear', data_format='channels_first')\r\nd3 = UpInterpolate1D(c, r, interpolation='cubic', data_format='channels_first')\r\nd4 = UpInterpolate1D(c, r, interpolation='pchip', data_format='channels_first')\r\n\r\nprint(c.shape)\r\nprint(d1.shape)\r\n\r\nx = np.linspace(0.0, 1.0, num=n1)\r\nxx = np.linspace(0.0, 1.0, num=n1 * r)\r\n\r\nimport matplotlib.pyplot as plt\r\nfig, axs = plt.subplots(5, 1)\r\naxs[0].plot(x, c[int(nb / 2.0), int(nh / 2.0), :].numpy().reshape(n1))\r\naxs[1].plot(xx, d1[int(nb / 2.0), int(nh / 2.0), :].numpy().reshape(n1 * r))\r\naxs[2].plot(xx, d2[int(nb / 2.0), int(nh / 2.0), :].numpy().reshape(n1 * r))\r\naxs[3].plot(xx, d3[int(nb / 2.0), int(nh / 2.0), :].numpy().reshape(n1 * r))\r\naxs[4].plot(xx, d4[int(nb / 2.0), int(nh / 2.0), :].numpy().reshape(n1 * r))\r\naxs[0].set_title('Data')\r\naxs[1].set_title('Nearest Neighbor Interpolation')\r\naxs[2].set_title('Linear Interpolation')\r\naxs[3].set_title('Cubic Interpolation')\r\naxs[4].set_title('PCHIP Interpolation')\r\nfor i in range(0, 4):\r\n    axs[i].set_xticks([])\r\nplt.show()\r\n\r\nprint('========================================================================')\r\n\r\n# Verify 2D\r\nn1 = 50\r\nn2 = 60\r\nr = 4\r\nc = np.zeros((n1, n2))\r\nfor i in range(0, n1):\r\n    for j in range(0, n2):\r\n        c[i, j] = np.math.sin(0.1 * i + 0.5 * j)\r\nc = np.reshape(c, (1, n1, n2, 1))\r\nc = tf.convert_to_tensor(c)\r\nd1 = UpInterpolate2D(c, (r, r), interpolation='nearest', data_format='channels_last')\r\nd2 = UpInterpolate2D(c, (r, r), interpolation='linear', data_format='channels_last')\r\nd3 = UpInterpolate2D(c, (r, r), interpolation='cubic', data_format='channels_last')\r\nd4 = UpInterpolate2D(c, (r, r), interpolation='pchip', data_format='channels_last')\r\n\r\nprint(c.shape)\r\nprint(d1.shape)\r\n\r\nimport matplotlib.pyplot as plt\r\nfig, axs = plt.subplots(3, 2)\r\naxs[0, 0].imshow(c.numpy().reshape(n1, n2), aspect='auto', interpolation='none')\r\naxs[1, 0].imshow(d1.numpy().reshape(n1 * r, n2 * r), aspect='auto', interpolation='none')\r\naxs[1, 1].imshow(d2.numpy().reshape(n1 * r, n2 * r), aspect='auto', interpolation='none')\r\naxs[2, 0].imshow(d3.numpy().reshape(n1 * r, n2 * r), aspect='auto', interpolation='none')\r\naxs[2, 1].imshow(d4.numpy().reshape(n1 * r, n2 * r), aspect='auto', interpolation='none')\r\naxs[0, 0].set_title('Data')\r\naxs[1, 0].set_title('Nearest Neighbor Interpolation')\r\naxs[1, 1].set_title('Linear Interpolation')\r\naxs[2, 0].set_title('Cubic Interpolation')\r\naxs[2, 1].set_title('PCHIP Interpolation')\r\nfor i in range(3):\r\n    for j in range(2):\r\n        axs[i, j].set_xticks([])\r\nplt.show()\r\n\r\nprint('========================================================================')\r\n\r\n# Verify 3D\r\nn1 = 40\r\nn2 = 30\r\nn3 = 20\r\nr = 2\r\nc = np.zeros((n1, n2, n3))\r\nfor i in range(0, n1):\r\n    for j in range(0, n2):\r\n        for k in range(0, n3):\r\n            c[i, j, k] = np.math.sin(0.5 * i + 0.5 * j**1.2 + 0.2 * k**1.5)\r\nc = np.reshape(c, (1, n1, n2, n3, 1))\r\nc = tf.convert_to_tensor(c)\r\n\r\nfor interp in ['nearest', 'linear', 'cubic', 'pchip']:\r\n\r\n    d = UpInterpolate3D(c, (r, r, r), interpolation=interp, data_format='channels_last')\r\n\r\n    print(c.shape)\r\n    print(d.shape)\r\n\r\n    import matplotlib.pyplot as plt\r\n    fig, axs = plt.subplots(3, 2)\r\n    axs[0, 0].imshow(c[0, :, :, int(n3 / 2.0), 0].numpy().reshape(n1, n2),\r\n                     aspect='auto',\r\n                     interpolation='none')\r\n    axs[0, 1].imshow(d[0, :, :, int(n3 / 2.0 * r), 0].numpy().reshape(n1 * r, n2 * r),\r\n                     aspect='auto',\r\n                     interpolation='none')\r\n    axs[1, 0].imshow(c[0, :, int(n2 / 2.0), :, 0].numpy().reshape(n1, n3),\r\n                     aspect='auto',\r\n                     interpolation='none')\r\n    axs[1, 1].imshow(d[0, :, int(n2 / 2.0 * r), :, 0].numpy().reshape(n1 * r, n3 * r),\r\n                     aspect='auto',\r\n                     interpolation='none')\r\n    axs[2, 0].imshow(c[0, int(n1 / 2.0), :, :, 0].numpy().reshape(n2, n3),\r\n                     aspect='auto',\r\n                     interpolation='none')\r\n    axs[2, 1].imshow(d[0, int(n1 / 2.0 * r), :, :, 0].numpy().reshape(n2 * r, n3 * r),\r\n                     aspect='auto',\r\n                     interpolation='none')\r\n    axs[0, 0].set_title('Data x1-x2')\r\n    axs[0, 1].set_title('Interpolation x1-x2')\r\n    axs[1, 0].set_title('Data x1-x3')\r\n    axs[1, 1].set_title('Interpolation x1-x3')\r\n    axs[2, 0].set_title('Data x2-x3')\r\n    axs[2, 1].set_title('Interpolation x2-x3')\r\n    for i in range(3):\r\n        for j in range(2):\r\n            axs[i, j].set_xticks([])\r\n    plt.suptitle(interp + ' Interpolation')\r\n    plt.show()\r\n```\r\n\r\n... and the results should be:\r\n![Figure_1](https://user-images.githubusercontent.com/10822134/107133948-c98b2300-68aa-11eb-8e78-0998ce297f13.png)\r\n![Figure_2](https://user-images.githubusercontent.com/10822134/107133952-cd1eaa00-68aa-11eb-90bc-ef5406f61ec8.png)\r\n![Figure_3a](https://user-images.githubusercontent.com/10822134/107133953-cee86d80-68aa-11eb-877e-4e0a4120aace.png)\r\n![Figure_3b](https://user-images.githubusercontent.com/10822134/107133955-d0199a80-68aa-11eb-899b-0642cf24bd55.png)\r\n![Figure_3c](https://user-images.githubusercontent.com/10822134/107133956-d14ac780-68aa-11eb-9340-dd7ee3b8007f.png)\r\n![Figure_3d](https://user-images.githubusercontent.com/10822134/107133959-d3ad2180-68aa-11eb-967d-f9d5ef4bcf47.png)\r\n\r\n", "It sounds like a good feature. I am trying to test the UpSampling3D in a keras model. \"AttributeError: 'Tensor' object has no attribute 'numpy'  \" is reported from UpInterpolate3D  funcition (x = x.numpy()). Do you have the same issue? The run_eagerly mode doesn't work.", "Is there any update on this issue? trilinear resize would be very useful in 3d medical image segmentation tasks", "Triage note: we'd like to open this for community contribution."]}, {"number": 46602, "title": "Race condition in port picker leads to test failures", "body": "**System information**\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 11.0\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the TensorFlow tests in parallel through bazel leads to various failures all ultimately with a failed start of the gRPC server due to \"Address already in use\".\r\nAs the reason I suspect a race condition in tensorflow/core/platform/default/net.cc function `PickUnusedPortOrDie`: When multiple processes try to pick a random, unused port, they may end up picking a port just picked but not yet used by another test process.\r\n\r\nThis does not always happen but often enough to become a problem on our 6-GPU system (i.e. 6 parallel processes). I also think the problem is amplified by the use of `rand()` without properly (i.e. randomly or even at all) seeding it first which means the process likely try the same ports in the same order.\r\n\r\nThis is supported by the output of the next test:\r\n```\r\nW tensorflow/core/platform/default/net.cc:65] bind(port=52649) failed: Address already in use\r\nW tensorflow/core/platform/default/net.cc:65] bind(port=54915) failed: Address already in use\r\nW tensorflow/core/platform/default/net.cc:65] bind(port=64017) failed: Address already in use\r\n```\r\n\r\nI.e. how high are the chances that 3 random ports in order are already used? Hence the ports chosen are not random.\r\n\r\n**Describe the expected behavior**\r\n\r\nTests succeed (by choosing real unused ports)\r\nI'd suggest to \"block\" a port through other means, currently a static `std::unordered_set` is used, but e.g. a temporary file/folder would be more appropriate.\r\n\r\n**Other info / logs** \r\n```\r\n\r\n[ RUN      ] CAPI.RemoteExecuteSilentCopiesAsyncFunc\r\n2021-01-21 21:18:32.222204: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-21 21:18:32.224170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-01-21 21:18:32.478962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0035:05:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.50GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2021-01-21 21:18:32.478977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-21 21:18:32.481822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-01-21 21:18:32.481866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-01-21 21:18:32.483470: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-21 21:18:32.484308: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-21 21:18:32.486499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-21 21:18:32.488202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-21 21:18:32.492019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-21 21:18:32.528329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-21 21:18:33.198519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-21 21:18:33.198528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-21 21:18:33.198533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-21 21:18:33.211048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:1/device:GPU:0 with 30150 MB memory) -> physical GPU (device: 0, n\r\name: Tesla V100-SXM2-32GB, pci bus id: 0035:05:00.0, compute capability: 7.0)\r\nE0121 21:18:33.212193952  159657 server_chttp2.cc:40]        {\"created\":\"@1611263913.212114250\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/tran\r\nsport/chttp2/server/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1611263913.212112418\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/com_github_grpc_grpc/src/cor\r\ne/lib/iomgr/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1611263913.212096081\",\"description\":\"Unable to configure socket\",\"fd\":33,\"file\":\"external/com_github_grpc_grpc/src/core/lib/i\r\nomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1611263913.212090911\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external/com_github_grpc_grpc/src/core/\r\nlib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1611263913.212112068\",\"description\":\"Unable to configure socket\",\"fd\":33,\"file\"\r\n:\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1611263913.212108551\",\"description\":\"Address already in use\",\"errno\":98,\"\r\nfile\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\n2021-01-21 21:18:33.212232: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:533] Unknown: Could not start gRPC server\r\ntensorflow/c/eager/c_api_remote_test_util.cc:81: Failure\r\nValue of: tensorflow::GrpcServer::Create( server_def, tensorflow::Env::Default(), &worker_server1) .ok()\r\n  Actual: false\r\nExpected: true\r\n[  FAILED  ] CAPI.RemoteExecuteSilentCopiesAsyncFunc (998 ms)\r\n```\r\n\r\nOther failing tests are //tensorflow/c/eager:c_api_cluster_test_gpu, //tensorflow/c/eager:c_api_remote_function_test_gpu, //tensorflow/c/eager:c_api_remote_test_gpu", "comments": ["@Flamefire \r\nPlease share a simple stand alone code/steps followed before you encounter this issue.", "Nothing special, just building the tests with bazel:\r\n\r\n`bazel test --compilation_mode=opt --config=opt --subcommands --verbose_failures --config=noaws --jobs=64 --copt=\"-fPIC\" --distinct_host_configuration=false --test_output=errors --build_tests_only --test_env=TF_GPU_COUNT=6 --test_env=TF_TESTS_PER_GPU=1 --local_test_jobs=6 --test_tag_filters='gpu,-no_gpu,-nogpu,-no_pip,-no_oss,-oss_serial,-benchmark-test,-v1only' --build_tag_filters='gpu,-no_gpu,-nogpu,-no_pip,-no_oss,-oss_serial,-benchmark-test,-v1only' --test_timeout='300,450,1200,3600' --test_size_filters=small --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute -- //tensorflow/c/eager:c_api_cluster_test_gpu //tensorflow/c/eager:c_api_remote_function_test_gpu //tensorflow/c/eager:c_api_remote_test_gpu`", "Does this reproduce with current head? There were a few changes in port picker recently", "Yes, the handling hasn't changed and `rand` is still being used without proper initialization via `srand` (or the C++11 <random> lib): https://github.com/tensorflow/tensorflow/blob/c0948d3b29345a0f0eed8dfb4ea1fa2e88e987bb/tensorflow/core/platform/default/net.cc#L119"]}, {"number": 46598, "title": "_pywrap_tensorflow_internal.pyd is to big after PyInstaller pack", "body": "Hi,\r\n\r\nI'm working on packing our Algo pipe Micro-services built from 4 services that are wrapping Algo API's using TensorFlow 2.3.1.\r\nI'm using PyInstaller with a unified spec file to pack all 4 services into one folder so all services executables will use shared resources.\r\n\r\nOne of the resources is \"_pywrap_tensorflow_internal.pyd\" file.\r\n\r\nThe issue is that after the latest version this file size has increased from about 140 MB to 694 MB.\r\nI did not found any related issue yet, I'm thinking that the PyInstaller has increased this file size due to our version progress.\r\n\r\nWhat could be the reason?\r\nIs there an option to decrease this file size?\r\n\r\n\r\n**System information**\r\n- OS Platform: Win:\r\n- TensorFlow installed: from source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.7.6 x64\r\n- Installed using virtualenv? pip? conda?: pip\r\n- PyInstaler version: 4.2\r\n\r\n", "comments": ["@roeyilan \r\nPlease share the error log for us to analyse.", "It's an error, I'm having issue with the file size.\nI'm trying to understand why it increased in about 500 MB", "I recently ran into the same thing. The `_pywrap_tensorflow_internal.pyd` adds 766MB. Anything that can be done to reduce this? Or split this file up and include only what is used?\r\n\r\nUsing `tensorflow = \"===2.4.0\"`", "In TensorFlow 2.5.0, it it bigger, 838MB."]}, {"number": 46595, "title": "tf.keras.preprocessing.image_dataset_from_directory includes hidden dirs as classes", "body": "I am using google colab, and filed this as an issue there but they told me to file it here instead. https://github.com/googlecolab/colabtools/issues/1824\r\n\r\n It seems that google colab uses . prefixed hidden directories `.ipynb_checkpoints`, which image_dataset_from_directory thinks is a class.\r\n\r\nDescribe the current behavior:\r\nWhen passing class names it produces the error:\r\nValueError: The class_names passed did not match the names of the subdirectories of the target directory. Expected: ['.ipynb_checkpoints', 'no_skip', 'skip'], but received: ['no_skip', 'skip']\r\nWhen not passing class names it picks up 3 classes, not two.\r\n\r\nDescribe the expected behavior:\r\ntf.keras.preprocessing.image_dataset_from_directory(...) should not include hidden .directories.\r\n\r\nThe web browser you are using (Chrome, Firefox, Safari, etc.):\r\nNot relevant\r\n\r\nLink (not screenshot!) to a minimal, public, self-contained notebook that\r\nreproduces this issue (click the Share button, then Get Shareable Link):\r\nhttps://colab.research.google.com/drive/1sI2Zk7-YjIOg3nRDyHtUtczwjevQfdPF?usp=sharing", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d5813effbc7af9ea285b1933d908b8a7/46595.ipynb). Thanks!", "@jhihn It looks for a directory structure (as shown [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory?version=nightly)) to infer data and labels. I think `.ipynb_checkpoints` should not be saved in the data directory. Thanks!", "I didn't save .ipynb_checkpoints there. The Google Colab did. I don't have control over that. And I think it should NOT include hidden (.) directories anyway. .ipynb_checkpoints would not appear in a normal unix directory listing unless you used the \"-a\" option. ", "@jhihn Colab save notebooks under `MyDrive/Colab Notebooks`. In this [gist](https://colab.research.google.com/gist/jvishnuvardhan/3f616896ffa3696c9bd1d834a05eec18/46595.ipynb), we cannot reproduce the issue. If we manually create `ipynb_checkpoints` under the `data` where you have training files, then it will throw an error. \r\n\r\nI am not sure about inner details about the `tf.keras.preprocessing.image_dataset_from_directory` and how to implement the same to exclude hidden files without affecting other modules. If you think you can implement it without breaking other modules, then please feel free to contribute through Pull Request. Thanks! ", "So it's using Python's os.walk() function which according to https://www.tutorialspoint.com/python/os_walk.htm\r\nincludes the `.` files (hidden)\r\n\r\nThis is often undesired and is covered in this stackoverflow post:\r\nhttps://stackoverflow.com/questions/13454164/os-walk-without-hidden-folders\r\n\r\nI, and many others think os.walk() should have a independent no-hidden-files option, but the python people have not added it \ud83e\udd37\u200d\u2642\ufe0f \r\nUPDATE: Your probably want to be cross-platform rather than solely reply on the unix convention, and for python 3.5, you can use stat():\r\n```\r\nimport os, stat\r\n\r\ndef has_hidden_attribute(filepath):\r\n    return bool(os.stat(filepath).st_file_attributes & stat.FILE_ATTRIBUTE_HIDDEN)\r\n```\r\n\r\nUpdate 2: I did some more digging and this looks like the place for the test to be added:\r\nhttps://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/python/keras/preprocessing/dataset_utils.py#L69\r\n\r\nI would imagine: \r\n```\r\ndef is_hidden(directory, subdir):\r\n    if os.name == 'nt':\r\n        return bool(os.stat(os.path.join(directory, subdir)).st_file_attributes & stat.FILE_ATTRIBUTE_HIDDEN)\r\n    else\r\n        return subdir.startswith('.')\r\n\r\nif os.path.isdir(os.path.join(directory, subdir)) and not is_hidden(directory, subdir):\r\n```\r\n", "Hi, I'm having the same problem on Jupiter lab. Is there any workaround for this?"]}, {"number": 46591, "title": "Misleading warning when loading weights from hdf5 format weights", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0, nightly\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.0, 8.0.4\r\n- GPU model and memory: 1060, 6GB\r\n\r\n\r\n**Describe the current behavior**\r\nWhen loading weights with `skip_mismatch=True`, and if the stored weights don't match with the current layer weights, \r\nthe warning seems to show misleading shapes for the stored weights.\r\n\r\n**Describe the expected behavior**\r\nI have attached a minimal code to produce the warning. The warning seems confusing. The expected shapes of weights shown in \r\nthe warning should be `((3, 4, 7, 32) vs (3, 4, 7, 16))` rather than `((3, 4, 7, 32) vs (16, 7, 3, 4))`. I suspect the transpose here is causing the change of shape. \r\nhttps://github.com/tensorflow/tensorflow/blob/3c84fc9a4def10a80dcf5901c57210d3562f13cd/tensorflow/python/keras/saving/hdf5_format.py#L407-L411\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential(\r\n    tf.keras.layers.Conv2D(16, [3, 4], use_bias=False, input_shape=[10, 10, 7]))\r\nmodel.save_weights('test.h5')\r\n\r\ntf.keras.backend.clear_session()\r\n\r\nnew_model = tf.keras.Sequential(\r\n    tf.keras.layers.Conv2D(32, [3, 4], use_bias=False, input_shape=[10, 10, 7]))\r\nnew_model.load_weights('test.h5', skip_mismatch=True, by_name=True)\r\n\r\nWARNING:tensorflow:Skipping loading of weights for layer conv2d due to mismatch in shape ((3, 4, 7, 32) vs (16, 7, 3, 4)).\r\n\r\n```\r\n", "comments": ["@srihari-humbarwadi \r\nI ran the stand alone code shared and do not see any warning, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ca32d1fa7556ee0e2eab62fb07325f5c/untitled505.ipynb)", "@Saduf2019 \r\nIt may require you to restart your kernel before you try to load the weights into the new model. I have modified your gist and we can now see the warning.\r\n![image](https://user-images.githubusercontent.com/24864163/105493807-df5fec00-5cdf-11eb-939b-baeef919adee.png)\r\nHere is the modified [gist](https://colab.research.google.com/gist/srihari-humbarwadi/ea0bb0e4f1473c2bb52cd7e82be9d85a/untitled505.ipynb)", "I am able to replicate the issue on [tf 2.4 ](https://colab.research.google.com/gist/Saduf2019/70e44138bd31227b37e9d4db4ecb5c12/untitled510.ipynb)and [nightly](https://colab.research.google.com/gist/Saduf2019/756a53cc6d2fd80a9d56d1bcc98b1c3f/untitled509.ipynb)", "Hello! This transpose occurs because, in the past (years ago), the weights of Conv layers could be saved with channel-first or channel-last format. This code tries both formats, and when it fails it raises a warning with the transposed shapes. I'll make the warning a bit clearer so that in the future users won't get tripped out about this. Thanks for the report!", "Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c70843fb8cc13024a7b6547eda000445/untitled91.ipynb).Thanks!"]}, {"number": 46580, "title": "tflite model can not inference when use tensorflow op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- source\r\n- TensorFlow version (use command below):\r\n- 2.4.0\r\n- Python version:\r\n- 3.6.0\r\n- Bazel version (if compiling from source):\r\n- 3.1.-\r\n\r\nwhen I convert a tf model to tflite and use tensorflow op with\r\n```python\r\n     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                          tf.lite.OpsSet.SELECT_TF_OPS]\r\n     converter.allow_custom_ops = True\r\n```\r\nand I managed to get the tflite file, now I want to use the TFlite model for inference.\r\nI add \"//tensorflow/lite/delegates/flex:delegate\" in tflite_cc_shared_object deps, and rebuild tenosrflowlite lib:\r\n```shell\r\nbazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so\r\n```\r\nand I copy libtensorflowlite.so to /usr/lib.\r\n\r\nthen, Inference program was compiled using g++, the compilation was successful, an executable program TFlite was generated,I use the ldd command to view the libraries used by tflite:\r\n```shell\r\n\tlinux-vdso.so.1 (0x00007ffe09fb1000)\r\n\tlibtensorflowlite.so => /usr/lib/libtensorflowlite.so (0x00007f4017d84000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f40179fb000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f40177e3000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f40173f2000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4017054000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f4016e4e000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f4016c46000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4016a27000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x000055b71a930000)\r\n```\r\nI executed the tflite executable\uff1a\r\n```shell\r\n./tflite\r\n```\r\nbut I get error:\r\n```shell\r\nERROR: Op builtin_code out of range: 129. Are you using old TFLite binary with newer model?\r\nERROR: Registration failed.\r\n\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nWhich step went wrong? ", "comments": ["@thaink @ravikyram @abattery  who can help me\uff1fAm I doing the right thing\uff1f", "You need to build your binary with TFLite build with the same or newer version of the TensorFlow that were used for the converter. I doubt that your TFLite model is converted by the tf-nightly but your inference program is based on the 2.4.0 verison."]}, {"number": 46554, "title": "Auto-generated cortex-m0 hello world make project failing to link", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux pop-os 5.8.0-7625-generic\r\n- TensorFlow installed from (source or binary): source \r\n- Tensorflow version (commit SHA if source): 2.3.2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): cortex-M0\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI've downloaded the master branch as zip, extracted the archive and navigated to the root of the directory structure and ran:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m0 microlite`\r\n\r\nfollowed by\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_generic TARGET_ARCH=cortex-m0 generate_hello_world_make_project`\r\n\r\nafterwards I `cd` to `tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m0_default/prj/hello_world/make/` and execute `make` which results in the following error: \r\n\r\n`tensorflow/lite/micro/cortex_m_generic/debug_log.cc:25:10: fatal error: tensorflow/lite/micro/cortex_m_generic/debug_log_callback.h: No such file or directory\r\n   25 | #include \"tensorflow/lite/micro/cortex_m_generic/debug_log_callback.h\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake: *** [Makefile:36: tensorflow/lite/micro/cortex_m_generic/debug_log.o] Error 1`\r\n\r\nwhich I'm able to resolve by copying \r\n\r\n`debug_log_callback.h`\r\n\r\nfrom \r\n\r\n`tensorflow/lite/micro/cortex_m_generic`\r\n\r\nto the\r\n\r\n`tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m0_default/prj/hello_world/make/tensorflow/lite/micro/cortex_m_generic`\r\n\r\nexecuting `make` from within `tensorflow/lite/micro/tools/make/gen/cortex_m_generic_cortex-m0_default/prj/hello_world/make/` which now runs smoothly and produces the individual out files until the first call to the linker: \r\n\r\n```\r\n'arm-none-eabi-g++' -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter   -mcpu=cortex-m0 -mfpu=auto -DTF_LITE_MCU_DEBUG_LOG -mthumb -mfloat-abi=soft -funsigned-char -mlittle-endian -Wno-type-limits -Wno-unused-private-field -fomit-frame-pointer -MD -DCPU_M0=1  -I. -I./third_party/gemmlowp -I./third_party/flatbuffers/include -I./third_party/ruy -o hello_world tensorflow/lite/micro/all_ops_resolver.o tensorflow/lite/micro/cortex_m_generic/debug_log.o tensorflow/lite/micro/memory_helpers.o tensorflow/lite/micro/micro_allocator.o tensorflow/lite/micro/micro_error_reporter.o tensorflow/lite/micro/micro_interpreter.o tensorflow/lite/micro/micro_profiler.o tensorflow/lite/micro/micro_string.o tensorflow/lite/micro/micro_time.o tensorflow/lite/micro/micro_utils.o tensorflow/lite/micro/recording_micro_allocator.o tensorflow/lite/micro/recording_simple_memory_allocator.o tensorflow/lite/micro/simple_memory_allocator.o tensorflow/lite/micro/test_helpers.o tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o tensorflow/lite/micro/memory_planner/greedy_memory_planner.o tensorflow/lite/micro/memory_planner/linear_memory_planner.o tensorflow/lite/micro/testing/test_conv_model.o tensorflow/lite/c/common.o tensorflow/lite/core/api/error_reporter.o tensorflow/lite/core/api/flatbuffer_conversions.o tensorflow/lite/core/api/op_resolver.o tensorflow/lite/core/api/tensor_utils.o tensorflow/lite/kernels/internal/quantization_util.o tensorflow/lite/kernels/kernel_util.o tensorflow/lite/schema/schema_utils.o tensorflow/lite/micro/kernels/activations.o tensorflow/lite/micro/kernels/add.o tensorflow/lite/micro/kernels/arg_min_max.o tensorflow/lite/micro/kernels/ceil.o tensorflow/lite/micro/kernels/circular_buffer.o tensorflow/lite/micro/kernels/comparisons.o tensorflow/lite/micro/kernels/concatenation.o tensorflow/lite/micro/kernels/conv.o tensorflow/lite/micro/kernels/conv_test_common.o tensorflow/lite/micro/kernels/depthwise_conv.o tensorflow/lite/micro/kernels/dequantize.o tensorflow/lite/micro/kernels/detection_postprocess.o tensorflow/lite/micro/kernels/elementwise.o tensorflow/lite/micro/kernels/ethosu.o tensorflow/lite/micro/kernels/flexbuffers_generated_data.o tensorflow/lite/micro/kernels/floor.o tensorflow/lite/micro/kernels/fully_connected.o tensorflow/lite/micro/kernels/fully_connected_common.o tensorflow/lite/micro/kernels/hard_swish.o tensorflow/lite/micro/kernels/kernel_runner.o tensorflow/lite/micro/kernels/kernel_util.o tensorflow/lite/micro/kernels/l2norm.o tensorflow/lite/micro/kernels/logical.o tensorflow/lite/micro/kernels/logistic.o tensorflow/lite/micro/kernels/maximum_minimum.o tensorflow/lite/micro/kernels/mul.o tensorflow/lite/micro/kernels/neg.o tensorflow/lite/micro/kernels/pack.o tensorflow/lite/micro/kernels/pad.o tensorflow/lite/micro/kernels/pooling.o tensorflow/lite/micro/kernels/prelu.o tensorflow/lite/micro/kernels/quantize.o tensorflow/lite/micro/kernels/quantize_common.o tensorflow/lite/micro/kernels/reduce.o tensorflow/lite/micro/kernels/reshape.o tensorflow/lite/micro/kernels/resize_nearest_neighbor.o tensorflow/lite/micro/kernels/round.o tensorflow/lite/micro/kernels/shape.o tensorflow/lite/micro/kernels/softmax.o tensorflow/lite/micro/kernels/split.o tensorflow/lite/micro/kernels/split_v.o tensorflow/lite/micro/kernels/strided_slice.o tensorflow/lite/micro/kernels/sub.o tensorflow/lite/micro/kernels/svdf.o tensorflow/lite/micro/kernels/svdf_common.o tensorflow/lite/micro/kernels/tanh.o tensorflow/lite/micro/kernels/transpose_conv.o tensorflow/lite/micro/kernels/unpack.o tensorflow/lite/micro/examples/hello_world/main.o tensorflow/lite/micro/examples/hello_world/main_functions.o tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/examples/hello_world/output_handler.o tensorflow/lite/micro/examples/hello_world/constants.o -Wl,--fatal-warnings -Wl,--gc-sections -lm\r\n```\r\n\r\nwhich errs with:\r\n\r\n```\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-abort.o): in function `abort':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/stdlib/../../../../../../../../newlib/libc/stdlib/abort.c:59: undefined reference to `_exit'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-exit.o): in function `exit':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/stdlib/../../../../../../../../newlib/libc/stdlib/exit.c:64: undefined reference to `_exit'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-sbrkr.o): in function `_sbrk_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/sbrkr.c:51: undefined reference to `_sbrk'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-signalr.o): in function `_kill_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/signalr.c:53: undefined reference to `_kill'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-signalr.o): in function `_getpid_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/signalr.c:83: undefined reference to `_getpid'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-writer.o): in function `_write_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/writer.c:49: undefined reference to `_write'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-closer.o): in function `_close_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/closer.c:47: undefined reference to `_close'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-fstatr.o): in function `_fstat_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/fstatr.c:55: undefined reference to `_fstat'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-isattyr.o): in function `_isatty_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/isattyr.c:52: undefined reference to `_isatty'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-lseekr.o): in function `_lseek_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/lseekr.c:49: undefined reference to `_lseek'\r\n/usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/bin/ld: /usr/lib/gcc/arm-none-eabi/9.2.1/../../../arm-none-eabi/lib/thumb/v6-m/nofp/libc.a(lib_a-readr.o): in function `_read_r':\r\n/build/newlib-CVVEyx/newlib-3.3.0/build/arm-none-eabi/thumb/v6-m/nofp/newlib/libc/reent/../../../../../../../../newlib/libc/reent/readr.c:49: undefined reference to `_read'\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [Makefile:42: hello_world] Error 1\r\n```\r\n", "comments": ["I managed to compile the example after manually editing linker flags in the project Makefile, by changing: \r\n\r\n`LDFLAGS += -Wl,--fatal-warnings -Wl,--gc-sections -lm`\r\n\r\nto \r\n\r\n`LDFLAGS += --specs=nosys.specs -Wl,--fatal-warnings -Wl,--gc-sections -lm`\r\n\r\nfollowing the suggestion found on [SO](https://stackoverflow.com/questions/19419782/exit-c-text0x18-undefined-reference-to-exit-when-using-arm-none-eabi-gcc).\r\n\r\nI have yet to verify that the executable actually runs.", "Reassigning to @petewarden since this is an issue with the project generation."]}, {"number": 46538, "title": "Add Windows build to nightly libtensorflow C packages", "body": "The [C API page](https://www.tensorflow.org/install/lang_c#nightly_libtensorflow_c_packages) says that \"libtensorflow packages are built nightly and uploaded to GCS for all supported platforms\" but the [libtensorflow-nightly GCS bucket](https://storage.googleapis.com/libtensorflow-nightly) does not have builds for Windows. The README's list of [offical builds](https://github.com/tensorflow/tensorflow#official-builds) also indicates that Windows nightlies should be available.\r\n\r\nPlease make Windows libtensorflow nightlies available for download.", "comments": ["We are working on it and this should be resolved soon. Thanks for the issue and your patience."]}, {"number": 46532, "title": "mkl_layout_pass_test fails due to optional BFLOAT16 support in MKL/oneDNN", "body": "Using TF 2.4 on a haswell system\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the test //tensorflow/core/common_runtime:mkl_layout_pass_test or //tensorflow/core/kernels:mkl fails at the BFloat16 types with e.g. \r\n```\r\ntensorflow/core/common_runtime/mkl_layout_pass_test.cc:270: Failure\r\nExpected equality of these values:\r\n  DoMklLayoutOptimizationPass()\r\n    Which is: \"A(BFloat16Input);B(BFloat16Input);C(Conv2D)|A->C;B->C:1\"\r\n  \"A(\" \"BFloat16Input\" \");B(\" \"BFloat16Input\" \");C(_MklConv2D);DMT/_0(Const);\" \"DMT/_1(Const)|A->C;A:control->DMT/_0:control;A:control->\" \"DMT/_1:control;B->C:1;DMT/_0->C:2;DMT/_1->C:3\"\r\n    Which is: \"A(BFloat16Input);B(BFloat16Input);C(_MklConv2D);DMT/_0(Const);DMT/_1(Const)|A->C;A:control->DMT/_0:control;A:control->DMT/_1:control;B->C:1;DMT/_0->C:2;DMT/_1->C:3\"\r\n[  FAILED  ] MklLayoutPassTest.NodeMerge_Conv2DWithBias_Negative_NoAddBias_DT_BFLOAT16 (1 ms)\r\n```\r\nThis is expected on non AVX512 systems as https://github.com/tensorflow/tensorflow/blob/e211aab91939c2666987cc7ccaee29ee0dd2ba41/tensorflow/core/graph/mkl_graph_util.h#L212 checks for runtime support and exits if not found causing the operation to not be changed and failing the test.\r\n\r\n**Describe the expected behavior**\r\n\r\nTests pass on any valid system.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCompile TF with Bazel and `//tensorflow/core/common_runtime:mkl_layout_pass_test` as the target or run that binary manually on a non-AVX-512 system.", "comments": ["@Flamefire \r\nWe are checking it and feedback as soon.\r\n\r\nThank you!"]}, {"number": 46518, "title": "Very slow tf.TensorArray and a possible bug when used with range()", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution: macOS Big Sur and Ubuntu 18 (Google colab)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8.7 on macOS and 3.6 on google colab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI created a few functions to benchmark `tf.TensorArray.write()` operations with python lists and it takes ages in comparison when writing 1000+ numbers. Here's a colab [notebook](https://colab.research.google.com/drive/1CySveLoSpbXB5uNWVcCB7ITlTN9dlYDA?usp=sharing) and the results are shown below:\r\n\r\n    from time import perf_counter\r\n    import tensorflow as tf\r\n    \r\n    \r\n    def list_appends(n):\r\n        l = []\r\n        for i in range(n):\r\n            l.append(i)\r\n    \r\n    def list_to_tensor(n):\r\n        l = []\r\n        for i in range(n):\r\n            l.append(i)\r\n        l = tf.convert_to_tensor(l)\r\n        return \r\n    \r\n    \r\n    def t_array(n):\r\n        t = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n        for i in range(n):\r\n            t = t.write(i, i)\r\n        t = t.stack()\r\n        return \r\n    \r\n    \r\n    @tf.function\r\n    def t_array_tff(n):\r\n        t = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n        for i in range(n):\r\n            t = t.write(i, i)\r\n        t = t.stack()\r\n        return \r\n    \r\n    \r\n    @tf.function\r\n    def t_array_tff_tfr(n):\r\n        t = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n        for i in tf.range(n):\r\n            t = t.write(i, i)\r\n        t = t.stack()\r\n        return \r\n\r\n**Tests:**\r\n\r\n    n = 100000\r\n\r\n    %timeit list_appends(n)\r\n\r\n100 loops, best of 3: 9.09 ms per loop\r\n\r\n    %timeit list_to_tensor(n)  # Same function + converts the resulting list to tf.Tensor\r\n\r\nThe slowest run took 4.30 times longer than the fastest. This could mean that an intermediate result is being cached.\r\n10 loops, best of 3: 36 ms per loop\r\n\r\n    %timeit t_array(n)  #  TensorArray\r\n\r\n1 loop, best of 3: 3.25 s per loop\r\n\r\n    %timeit t_array_tff_tfr(n)  # TensorArray + tf.function + tf.range()\r\n\r\n1 loop, best of 3: 1.53 s per loop\r\n\r\n\r\nThe following gets stuck until I kill the code, throws a lot of errors and I'm not sure whether this is a normal behavior.\r\n\r\n    %timeit t_array_tff(n)  # TensorArray + tf.function + range()\r\n\r\nError:\r\n\r\n    ERROR:root:Internal Python error in the inspect module.\r\n    Below is the traceback from this internal error.\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n        exec(code_obj, self.user_global_ns, self.user_ns)\r\n      File \"<ipython-input-8-2ea8237511a6>\", line 1, in <module>\r\n        get_ipython().magic('timeit t_array_tff(n)  # TensorArray + tf.function + range()')\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\r\n        return self.run_line_magic(magic_name, magic_arg_s)\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\r\n        result = fn(*args,**kwargs)\r\n      File \"<decorator-gen-59>\", line 2, in timeit\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\r\n        call = lambda f, *a, **k: f(*a, **k)\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\", line 1057, in timeit\r\n        time_number = timer.timeit(number)\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\", line 139, in timeit\r\n        timing = self.inner(it, self.timer)\r\n      File \"<magic-timeit>\", line 1, in inner\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n        result = self._call(*args, **kwds)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n        self._initialize(args, kwds, add_initializers_to=initializers)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\r\n        *args, **kwds))\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n        graph_function, _ = self._maybe_define_function(args, kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n        graph_function = self._create_graph_function(args, kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\r\n        capture_by_value=self._capture_by_value),\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n        out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\r\n        user_requested=True,\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 459, in converted_call\r\n        result = converted_f(*effective_args, **kwargs)\r\n      File \"/tmp/tmp1hatjldp.py\", line 24, in tf__t_array_tff\r\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(n),), None, fscope), None, loop_body, get_state, set_state, ('t',), {'iterate_names': 'i'})\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py\", line 444, in for_stmt\r\n        _py_for_stmt(iter_, extra_test, body, None, None)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py\", line 473, in _py_for_stmt\r\n        body(target)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/operators/control_flow.py\", line 459, in protected_body\r\n        original_body(protected_iter)\r\n      File \"/tmp/tmp1hatjldp.py\", line 22, in loop_body\r\n        t = ag__.converted_call(ag__.ld(t).write, (ag__.ld(i), ag__.ld(i)), None, fscope)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 350, in converted_call\r\n        return _call_unconverted(f, args, kwargs, options, False)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 479, in _call_unconverted\r\n        return f(*args)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\r\n        return _add_should_use_warning(fn(*args, **kwargs),\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1159, in write\r\n        return self._implementation.write(index, value, name=name)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 543, in write\r\n        name=name)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/list_ops.py\", line 202, in tensor_list_set_item\r\n        index >= input_list_size,\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4937, in less_equal\r\n        \"LessEqual\", x=x, y=y, name=name)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 522, in _apply_op_helper\r\n        preferred_dtype=default_dtype)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n        return func(*args, **kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n        return constant_op.constant(value, dtype, name=name)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 265, in constant\r\n        allow_broadcast=True)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 287, in _constant_impl\r\n        \"Const\", [], [dtype_value.type], attrs=attrs, name=name).outputs[0]\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 592, in _create_op_internal\r\n        compute_device)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3536, in _create_op_internal\r\n        op_def=op_def)\r\n      File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2027, in __init__\r\n        output_type = pywrap_tf_session.TF_OperationOutputType(tf_output)\r\n    KeyboardInterrupt\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\r\n        stb = value._render_traceback_()\r\n    AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\r\n        return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\r\n        return f(*args, **kwargs)\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\r\n        records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n      File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\r\n        frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n      File \"/usr/lib/python3.6/inspect.py\", line 1452, in getframeinfo\r\n        lines, lnum = findsource(frame)\r\n      File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\r\n        file = getsourcefile(object) or getfile(object)\r\n      File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\r\n        if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n      File \"/usr/lib/python3.6/inspect.py\", line 732, in getmodule\r\n        for modname, module in list(sys.modules.items()):\r\n    KeyboardInterrupt\r\n    ---------------------\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nis this normal behavior? I was expecting better performance specially when used with `tf.function` but it's either not a proper use case or an issue.\r\n\r\n**Standalone code to reproduce the issue**\r\nIncluded in the demonstration above.\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I ran the code shared and face memory related issues, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/75fdc0dad0344355c9b02a21b6263f07/untitled503.ipynb)", "`t_array_tff` is definitely not recommended. Within `tf.function`, you almost always want to use `tf.range`. The `range` version is there just for compatibility reasons. For more detail, `t_array_tff` is equivalent to a function where you write `t = t.write(i, i)` 100000 times, which takes a very long time to compile.\r\n\r\nAnother caveat: `tf.function` is compiled just-in-time, when it's first called. That means that the first call will be much slower. So when benchmarking, it's usualy a good idea to do a warm-up run to get rid of the one-time compilation costs. It's also a good idea to pass `tf.constant` arguments, otherwise the function might recompile. Something like this should do:\r\n\r\n```\r\nt_array_tff_tfr(tf.constant(1))  # Dry run, to compile the function\r\n%timeit t_array_tff_tfr(tf.constant(n))\r\n```\r\n\r\nIt also means that if you have a `tf.function` that you only ever call once, it will probably not give you good performance.\r\n\r\nThe lower performance of `t_array` and `t_array_tff_tfr` is expected (but there are some good news, read on). In general, TF ops (like `tf.TensorArray` and `for ... in tf.range`) have relatively large overhead making them inefficient at working with small Tensors. This is being actively looked at, and different runtimes are expected to have much improved performance. But in the meantime, you'll need to be mindful when writing code that performs a lot of steps with small tensors.\r\n\r\nIt's also expected that `t_array_tff_tfr` is about 3x faster than `t_array` - usually `tf.function` gives better performance over Eager.\r\n\r\nNow, there is a way to get a very efficient `tf.function` by setting the flag `experimental_compile=True` (in tf-nightly, that flag is `jit_compile=True`). That enables XLA compilation, which gives you better-than-Python performance (~10x in my tests). The caveat is that it doesn't always work with dynamic sizes, so for instance in our case it means you have to pre-allocate the TensorArray:\r\n\r\n```\r\n@tf.function(experimental_compile=True)\r\ndef t_array_tff_tfr_comp(n):\r\n    t = tf.TensorArray(dtype=tf.int32, size=n, dynamic_size=False)\r\n    for i in tf.range(n):\r\n        t = t.write(i, i)\r\n    return t.stack()\r\n```\r\n", "@mdanatg Thanks, that's very informative. I initially did this test because I have several methods that are called within another method decorated by `tf.function`. There are 3 loops that use python `range()` as well as 5 lists that are being populated inside the loop and I'm considering whether the switch to `tf.TensorArray`s is likely to improve performance vs introduce more bottlenecks. You'll find the methods in question below, if you want to look at / suggest improvements.\r\n\r\n\r\n    def calculate_returns(self, masks, rewards, values, log_probs, entropies):\r\n        states = tf.numpy_function(func=self.get_states, inp=[], Tout=tf.float32)\r\n        for step in range(self.transition_steps):\r\n            actions, step_log_probs, step_entropies, step_values = self.model(states)\r\n            states, step_rewards, step_dones = tf.numpy_function(\r\n                self.step_envs,\r\n                [actions],\r\n                (tf.float32, tf.float32, tf.float32),\r\n            )\r\n            step_masks = 1 - step_dones\r\n            log_probs.append(step_log_probs)\r\n            values.append(step_values)\r\n            rewards.append(step_rewards)\r\n            masks.append(step_masks)\r\n            entropies.append(step_entropies)\r\n        next_values = self.model(states)[-1]\r\n        returns = [next_values]\r\n        for step in reversed(range(self.transition_steps)):\r\n            returns.insert(0, rewards[step] + masks[step] * self.gamma * returns[0])\r\n        return returns\r\n\r\n    def calculate_loss(self, returns, values, log_probs, entropies):\r\n        value_loss = 0.0\r\n        action_loss = 0.0\r\n        entropy_loss = 0.0\r\n        for step in range(self.transition_steps):\r\n            advantages = tf.stop_gradient(returns[step]) - values[step]\r\n            value_loss += tf.reduce_mean(tf.square(advantages))\r\n            action_loss += -tf.reduce_mean(\r\n                tf.stop_gradient(advantages) * log_probs[step]\r\n            )\r\n            entropy_loss += tf.reduce_mean(entropies[step])\r\n        value_loss /= self.transition_steps\r\n        action_loss /= self.transition_steps\r\n        entropy_loss /= self.transition_steps\r\n        return (\r\n            self.value_loss_coef * value_loss\r\n            + action_loss\r\n            - entropy_loss * self.entropy_coef\r\n        )\r\n\r\n    @tf.function\r\n    def train_step(self, clip_norm=0.5):\r\n        \"\"\"\r\n        Do 1 training step.\r\n        Args:\r\n            clip_norm: Gradient clipping value passed to tf.clip_by_global_norm()\r\n\r\n        Returns:\r\n            None\r\n        \"\"\"\r\n        masks = []\r\n        rewards = []\r\n        values = []\r\n        log_probs = []\r\n        entropies = []\r\n        with tf.GradientTape() as tape:\r\n            returns = self.calculate_returns(\r\n                masks, rewards, values, log_probs, entropies\r\n            )\r\n            loss = self.calculate_loss(returns, values, log_probs, entropies)\r\n        gradients = tape.gradient(loss, self.model.trainable_variables)\r\n        gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\r\n        self.model.optimizer.apply_gradients(\r\n            zip(gradients, self.model.trainable_variables)\r\n        )", "I see, that makes sense. In this case, because you're already inside a tf.function (it applies recursively), switching these lists and loops to their TF counterparts should add more overhead, although I'd expect it to be small compared to the bulk of the function (that is `self.model` and gradient calculations). So the difference is more like:\r\n\r\n```\r\n@tf.function\r\ndef list_appends_tff(n):\r\n    l = []\r\n    for i in range(n):\r\n        l.append(i)\r\n    return tf.stack(l)\r\n\r\n@tf.function\r\ndef t_array_tff_tfr(n):\r\n    t = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n    for i in tf.range(n):\r\n        t = t.write(i, i)\r\n    return t.stack()\r\n```\r\n\r\nThe former is really equivalent to writing (because all Python code runs at compilation, and is used to generate the graph):\r\n\r\n```\r\n@tf.function\r\ndef list_appends_tff(n):\r\n    t1 = 0  # from l.append(0)\r\n    t2 = 1\r\n    ...\r\n    tn = n\r\n    return tf.stack([t1, t2, ..., tn])\r\n```\r\n\r\nSo you see it has lots more code, but no loops or lists so it should be fastest to run (but will compile slower).\r\n\r\nThe most important factor you need to consider is whether the lists and loops are static, that is, whether they have the same length and number of steps every time. At a glance, that amounts to the question: \"is self.transition_steps constant?\". If the answer is yes, then I'd move to the second question. If the answer is no, then you will need TensorArray and tf.range for things to work correctly (and there are a few other caveats to watch out for in that case).\r\n\r\nIf `self.transition_steps` is constant, then the next question is how large is it. If it's small, say ~10, then list/range should be fine, your graph will be larger, but not by a whole lot. If `transition_steps` is larger, then you might end up with a graph that's much larger than it could be, and switching to `tf.range` ought to give you improved startup times (although each step is potentially slower).\r\n\r\nNote that if you switch to TensorArray, you'll also need to refactor `calculate_returns` a bit, because you'll have to write e.g. `log_probs = log_probs.write(step, step_log_probs)` instead of just `log_probs.append(step_log_probs)`. I think I'd try gradual changes: start with changing just one of the lists to see if that changes performance in any way, and take it from there."]}, {"number": 46515, "title": "Tensorflow 2.4 about 10% slower than 2.3 for training RNN", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): difference between 2.3 and 2.4\r\n- Python version: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nIn TensorFlow 2.4, a basic RNN model runs about 10% slower than the same model in TensorFlow 2.3. This appears on both the CPU and GPU. I've created Colab notebooks to demonstrate the issue.\r\n\r\nThe step time reported by the progress bar also appears longer (and when I did some of my own timing, I also saw the step as longer), so I don't think it has to do with callbacks or any sort of overhead outside the step function.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect them to be the same, if not better in 2.4 (since it has newer CUDA, etc.).\r\n\r\nI know that TensorFlow 2.4 no longer has XLA enabled by default. I tried enabling it on my local machine, and found that it didn't help (in fact it made things slower in TF 2.4). Furthermore, if I enable it on Colab with `os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=2\"` I see slower speeds on GPU for both 2.3 and 2.4. So I don't think that's the problem. (Also, the fact that there's a slowdown on CPU suggests it's not an XLA thing.)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nTF 2.3: https://colab.research.google.com/drive/1xnfP17Bzan7-ICI_JtMfiEdTvgwrLoEz?usp=sharing\r\nTF 2.4: https://colab.research.google.com/drive/17rccQnDvLr4mAiGmAoJyK7t0hlrEpBoV?usp=sharing", "comments": ["I also did some similar experiments on a CNN, and found that there is also a slowdown there, though it seems less significant (only about 5% instead of 10%):\r\n\r\nTF 2.3: https://colab.research.google.com/drive/10WcsVCkyxRrrclpmPWSaWAI8UslZAR_Q?usp=sharing\r\nTF 2.4: https://colab.research.google.com/drive/1dolfWxeq_PV3UGvTFMo6E9sC1doeZENv?usp=sharing", "Doing some profiling on a related model (one which also showed a slowdown in TF 2.3), I noticed that while the \"Device compute time\" has stayed about the same from 2.2 to 2.3 to 2.4 (if not decreased slightly), the \"Kernel launch time\" has increased (for that model, from 0.6 to 1.7 to 2.8 ms per step). Is there some reason the kernel launch time would be so much higher in 2.3 vs 2.2, and then higher again in 2.4?\r\n\r\nI should also note that with the original SimpleRNN model I posted above, if I scale things up (using `input_d=256` and `units=1024`), then I see similar performance between 2.3 and 2.4. So it seems to be related to the amount of work each kernel is performing (could be a longer kernel launch time, could be something else).\r\n\r\nOn the one hand, if there was a conscious decision made to trade off faster computation for longer kernel launch times (and thus improve performance on models that have fewer kernels doing more work, but degrade performance on models with many smaller kernels), I understand why this trade-off would make sense. On the other hand, there are definitely still many cases (even large models) that for one reason or another use smaller kernels.", "Was able to reproduce the issue. Code runs a bit slower on TF v2.4 when compared with TF v2.3. \r\n\r\nWhereas with TF-nightly, Colab doesn't detect the GPU. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9e761711f1354daca7af3647f5a89e7b/46515.ipynb). Thanks!", "Adding @yhliang2018 and @tomerk since this is more a like overall performance issue rather than just RNN issue.", "Hi @hunse do you have a link to your profiling? Especially where it notes the change in kernel launch time)", "(And ideally traces of before & after as well)", "Here's [the profiling where I saw an increase in kernel launch time](https://drive.google.com/file/d/1XJ_TOnqUXNhbqHmWVC3Qy7cdFKFReQ6x/view?usp=sharing) (one folder is TF 2.3, the other is TF 2.4). The profiling overview reports that TF 2.4 has more ops on the host, though I wouldn't think that that would increase kernel launch time, unless it's having to wait for other ops on the host to finish before it launches the next one.\r\n\r\nUnfortunately, that profiling is from a network in our NengoDL package, so it's not an isolated TensorFlow example (I could pass along code for it, but it wouldn't be pure TensorFlow code, so I'm not sure if that's useful). I had been trying to isolate the slowdown in the Colab notebooks above, but it appears that that's not perfect: When I run that code on my local machine, I don't see a slowdown. So I don't think that that's fully captured the problem.", "Hi @hunse, I've finally had a chance to dig in to your profiles. Checking the traces, t looks like your tf 2.4 runs are using 8 compute threads on the host (+ 3 tf data threads). On the other hand, your tf 2.3 runs are only using 4 or 5 compute threads on the host + 3 tf.data threads. The GPU compute itself looks like it has basically the same time breakdown across both so it *seems* like placement of ops hasn't changed at least at a quick glance, but both the host in 2.4 spends much more of its time on send/receives.\r\n\r\nPerhaps context switching among the extra threads is slowing down your tensor data transfers? (e.g. via reducing cache coherency)\r\nCould you try reducing the thread count in 2.4 to match what 2.3 used by default? https://www.tensorflow.org/api_docs/python/tf/config/threading\r\n\r\n(Though off the top of my head I'm not sure what if any changes we made to the default threadpool size selection)", "Thanks for the suggestion, @tomerk. I standardized the number of threads, which seems to have reduced the discrepancy between TF 2.3 and 2.4 in that case.\r\n\r\nHowever, for that same network, when I compare between 2.2 and 2.4, I still see a significant difference. The difference is still there whether I use 4, 2, or 1 thread.\r\n\r\nI've [attached some profiling for 1 thread](https://drive.google.com/file/d/1woH_2s8ElLuoT1qEPQ4ebPVC1PIL4HJv/view?usp=sharing) showing results from 2.2, 2.3, and 2.4. The big difference between 2.2 and 2.3 is that there's a lot more host-chip communication at the end of each step, which seems to increase the amount of time between steps.\r\n\r\nIn another model, I see more of a difference between 2.3 and 2.4 than 2.2 and 2.3 ([profiling here](https://drive.google.com/file/d/1c1R9cbBLF50w7oh2UZ3UBNoC-zX7dHf0/view?usp=sharing)). Even when accounting for the fact that TF 2.3 and 2.4 seem to take a bit longer to start up, when I measure just the time to run the core loop on the device it takes about 650 ms in 2.2, 665 ms in 2.3, and 697 ms in 2.4. Both 2.3 and 2.4 appear to have more host-device communication, but in 2.3 it's more spread-out and doesn't result in as much time between steps, whereas in 2.4 it all happens between steps. I'm not sure if this is the entire reason (or any of the reason) for the slowdown, but it is the most salient difference to me.", "Hmm I don't seem to be able to view these profiles. Not sure if it's an issue on my side or not. Let me try getting someone else since I'm not sure I'll have the bandwidth to dig in to this.", "I have observed the same phenomenon, with tensorflow2.0 GPU, I can run the same RNN program 300 seconds, but with tensorflow 2.4 GPU, the same code takes 30 minutes to run, such a huge difference. Therefore unfortunately I have to downgrade 2.4 to 2.0 again ", "another example: on Quadro RTX 6000 .. TF 2.4 ETA 7 hours for single iteration, TF 2.3 40 minutes. Same everything except for downgrading to 2.3 between the two runs.", "I have also seen the performance degradation on normal multi-layer network with CPU only too. To me the performance decreases about 15%. There is another issue with multi-thread predicating. With 2.4 and 2.5 release, I got error like this occasionally(I do pre-warm predication for the model before launching threads):\r\n```\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 1915, in predict_on_batch\r\n    self.predict_function = self.make_predict_function()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 1545, in make_predict_function\r\n    if self.predict_function is not None:\r\nAttributeError: 'Sequential' object has no attribute 'predict_function'\r\n```\r\nThis doesn't happen at all with same code on release 2.3. "]}, {"number": 46504, "title": "micro: port op BATCH_MATMUL from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator BATCH_MATMUL from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro making minimal changes and not including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test\r\n", "comments": []}, {"number": 46502, "title": "Docker image tensorflow:latest-gpu-jupyter - Setting a password doesn't work", "body": "Using image from here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu-jupyter.Dockerfile\r\n\r\nDownloaded from dockerhub with `docker pull tensorflow/tensorflow:latest-gpu-jupyter`\r\n\r\nWhen running the jupyter server, we can normally set up a password in the login page.\r\n\r\nHowever, with this image I get an error 500 after configuring the password.\r\n\r\nThe password is effectively written to `/root/.jupyter/jupyter_notebook_config.json` but it isn't taken into account by the jupyter server. I.e. you can't connect just as if you had put a wrong password.\r\n\r\nOn the other hand, this works perfectly when using images from [jupyter/docker-stacks](https://github.com/jupyter/docker-stacks/).", "comments": ["@Atralb \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019 \r\n\r\nI'm talking about running the docker container provided by yourselves. I have linked to the dockerfile and every single package version is in there, I couldn't give you a more exhaustive list than that.\r\n\r\nThe single additional step I can give you is: `docker run -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter`, and then it's the steps performed in the GUI as explained in my original post.", "Hello,\r\n  Any updates about this issue? Please don't close it, -p PASSWORD:password does not help. Thank you.", "Apologies for the delay. Is this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is still an issue. I'm trying to set a password by using:\r\n```\r\ndocker run -e PASSWORD=password tensorflow/tensorflow:1.15.5-gpu-py3-jupyter\r\n```\r\n and it is not working.  See: https://github.com/tensorflow/tensorflow/issues/6351#issuecomment-637476227"]}, {"number": 46498, "title": "Issues when Cross-building TFLite GPU Delegate and loading the built so file in Python", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS x86_64 PC\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 582c8d2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RK3399 Ubuntu 18.04 LTS aarch64\r\n\r\n**Describe the problem**\r\n\r\nHost PC environment:\r\nOS: Ubuntu 18.04 x86_64 AMD64\r\nnative gcc(g++): 7.5.0\r\ncross-compiler aarch64-linux-gnu-gcc(aarch64-linux-gnu-g++): 8.3.0, which is download automatically by bazel when building\r\nbazel: 3.7.2\r\npython: virtual env python 3.7\r\n\r\nTarget Device environment:\r\nDevice: RK3399\r\nCPU: armv8\r\nGPU: Mali T860\r\nOS: Ubuntu 18.04 aarch64\r\nnative gcc(g++): 7.5.0\r\nOpenCL: 1.2\r\n\r\nI have two issues: the first is one **when cross-building OpenCL backend TFLite GPU delegate on the host PC**; the second is **when loading the TFLite GPU delegate so file** in Python on the target device platform.\r\n\r\nFirst, I could NOT build **OpenCL(v1.2) backend TFLite GPU delegate** for OpenCL version 1.2.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104850767-fa231100-592b-11eb-94e1-82fba7d297bb.png)\r\n\r\nSo I built **OpenCL(v2.2) backend TFLite GPU delegate**.\r\nI tried to use the built OpenCL(v2.2) TFLite GPU delegate in Python on RK3399 aarch64 but I met the following issue.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104851345-69e6cb00-592f-11eb-984c-d4bc18def731.png)\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI followed the following steps on my host PC.\r\n\r\nsudo apt update\r\nsudo apt-get install software-properties-common\r\nsudo apt update\r\nsudo apt install git curl\r\nsudo apt install python3.7 python3.7-dev python3.7-venv python3.7-distutils\r\nsudo apt install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\r\n\r\ncd ~\r\npython3.7 -m venv py37\r\nsource ~/py37/bin/activate\r\npip install cython\r\npip install wheel\r\npip install numpy\r\n\r\ngit clone -b r2.4 https://github.com/tensorflow/tensorflow.git tensorflow_r2.4\r\ncd tensorflow_r2.4\r\n./configure\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104850283-2be6a880-5929-11eb-9b09-7572ddb59263.png)\r\n\r\nFor OpenCL 1.2:\r\nbazel build -s -c opt --config=elinux_aarch64 --copt=\"-DMESA_EGL_NO_X11_HEADERS\" --copt=\"-DEGL_NO_X11\" --copt=\"-DCL_DELEGATE_NO_GL\" --copt=\"-DCL_TARGET_OPENCL_VERSION=120\" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n\r\nBut this failed as the above figure.\r\nSo I built again for OpenCL 2.2\r\nbazel build -s -c opt --config=elinux_aarch64 --copt=\"-DMESA_EGL_NO_X11_HEADERS\" --copt=\"-DEGL_NO_X11\" --copt=\"-DCL_DELEGATE_NO_GL\" --copt=\"-DCL_TARGET_OPENCL_VERSION=220\" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\nThis building was successfull and I got a tflite gpu delegate so file.\r\n\r\nI tried to load the built GPU delegate so file in Python as the following.\r\n\r\n![image](https://user-images.githubusercontent.com/47862419/104851016-520e4780-592d-11eb-9457-b49bb752efca.png)\r\n\r\nBut I met an issue as the above figure.\r\n\r\nWhat I want to know are:\r\n1) how to build OpenCL(v1.2 rather than v2.2) backend TFLite GPU delegate so file on the host PC or the target platform\r\n2) how to fix the issue when loading the built TFLite GPU delegate so file in Python.\r\n", "comments": ["For the second issue when loading TFLite GPU delegate, I added two functions **_tflite_plugin_create_delegate_** and  **_tflite_plugin_destroy_delegate_** in the delegate module so the issue disappeared and my Python inference using OpenCL backend TFLite GPU delegate works.\r\n", "Verified the issue. I'll investigate.", "@rose-jinyang the OpenCL delegate code is written in 2.2 API. So you can't build it with \"-DCL_TARGET_OPENCL_VERSION=120\".\r\nBut the generated binary can run on OpenCL 1.2 devices. Have you tried it?", "Yes, I checked that it can run on OpenCL 1.2 devices.\r\nDid u add tflite_plugin_create_delegate and tflite_plugin_destroy_delegate in the delegate module?", "No I didn't. You can send a PR for that. I can review the change.", "\r\n\r\n\r\n> Yes, I checked that it can run on OpenCL 1.2 devices.\r\n> Did u add tflite_plugin_create_delegate and tflite_plugin_destroy_delegate in the delegate module?\r\n\r\nHi, Even I am trying to do the same thing, but I get the error, 'undefined symbol: tflite_plugin_create_delegate'. How did you add it in the delegate module that it disappeared for you?", "@suyash-narain You can refer https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/external\r\n\r\n@rose-jinyang I'm wondering if you're going to share your change with a PR.", "@terryheo Thanks. I was trying to build the GPU delegate with opencl backend, using \r\nbazel build -s -c opt --config=elinux_aarch64 --copt=\"-DMESA_EGL_NO_X11_HEADERS\" --copt=\"-DEGL_NO_X11\" --copt=\"-DCL_DELEGATE_NO_GL\" --copt=\"-DCL_TARGET_OPENCL_VERSION=220\"\r\nto which, I get a build error: 'BUILD FAILED, 'DCL_TARGET_OPENCL_VERSION=220' no such file or directory'\r\nI have installed the 'mesa-common-dev, libegl1-mesa-dev, libgles2-mesa-dev, ocl-icd-opencl-dev' too on my host linux machine running ubuntu 18.04.\r\n"]}, {"number": 46497, "title": "Add option to write tensor to raw byte string", "body": "`tf.io.serialize_tensor` appears to serialize to protobuf. It would be nice to have the option to serialize to a raw byte string as this allows for a lot more flexibility when dealing with data in unsupported formats (e.g. > 16 int bit depth audio data).\r\n\r\nI know that this is possible with the `.numpy()` option. However, `.numpy()` is not available in graph mode, meaning there is no way to serialize to byte string in `tensorflow-serving`.\r\n\r\nIdeally, `tensorflow-io` would solve this problem. However, as yet there does [not appear](https://github.com/tensorflow/io/issues/414) to be an easy way to get `tensorflow-serving` to support `tensorflow-io` ops.", "comments": []}, {"number": 46493, "title": "Efficient and simple encoding for large tensors (such as `encode_raw`) for the use with tensorflow serving needed", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes - tough I have no experience with developing tf ops\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe stock gRPC/protobuf encoding for requests used in tensorflow serving carries large per-value processing overhead,\r\neven though that encoding is the recommended encoding for optimal performance.\r\nThis might not matter much on the server side, where the encoding time is usually dwarved by the model processing time.\r\nHowever, on the client side, decoding of the repeated values within the tensor proto may cause significant processing load.\r\n(In my particular reinforcement learning case, the majority of time is spent decoding protobuf).\r\n\r\nOn the request side, this can be avoided by dumping binary data into a string and then using `tf.io.decode_raw` in the model.\r\nHowever, for the response, a corresponding `tf.io.encode_raw` is missing.\r\nFurthermore, there seems to be no graph mode compatible way to \"view\"/\"bitcast\" data into a `tf.string`\r\n(even the workaround presented in https://stackoverflow.com/questions/43403147/how-to-create-a-encode-raw-tensorflow-function ported to TF 2.4.0 is not usable, because apparently `tf.strings.join` is not supported in graph mode). \r\n\r\nOf course, one could work around by implementing a custom API using the C++ API of tensorflow serving,\r\nbut that is a large effort with a much more limited application than a general `tf.io.encode_raw`.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of tensorflow serving requiring large prediction outputs. As a basic IO operation, many more use cases may be around.\r\n\r\n", "comments": ["I have interest in this feature and will start investigaate this soon :).", "@burnpanck Hi. I have implemented the `encode_raw` op in the above PR. I wonder if you could provide a realistic use case of it in terms of code that we could add to the doc? Thank you."]}, {"number": 46486, "title": "[TFLite] OpenCL strange performance compared to OpenGL", "body": "I've run some tests and I keep seeing the behavior below, where CPU execution and OpenGL execution match all expectations (histogram shape, speed), but OpenCL performs unpredictably. The graph refers to tests run on Pixel 4 with an extremely simple model (400x400x1 input => 2D mean => 1x1 output), collecting about 3k samples for each plot.\r\n\r\n![cl_plot](https://user-images.githubusercontent.com/15526561/104822300-c370cc00-5841-11eb-90f1-d4bdc14a8802.png)\r\n\r\n- The first plot `cpu` is for CPU execution (no delegate). Execution takes 10ms on average. \r\n- The second `ssbo_mono_gl` is for GPU execution with OpenGL. Input is passed as SSBO with the correct dimensions (400x400x1). Shape is nice and it's faster than CPU, as one would expect.\r\n- The third `ssbo_mono_cl` is for GPU execution with OpenCL. Input is passed as SSBO with the correct dimensions (400x400x1). I can see several issues:\r\n  - On average, 2.5 times slower than OpenGL.\r\n  - The results cluster around two speed levels. \r\n  \r\n  I'm not concerned with OpenCL slower than OpenGL in theory (might be due to SSBO input) **but** histogram shows that it's averagely slower just because of the right cluster. The left cluster performs just fine, actually better than GL. This looks suspicious to me.\r\n\r\n- Things get even weirder in the fourth plot `ssbo_dhwc4_cl` where I pass a dhwc4 (400x400x4) SSBO to the delegate. What I see:\r\n  - OpenCL performance gets about 4 times slower, which seems to suggest that most time is spent on conversions? The model is always the same (single channel).\r\n  - The majority of samples now belongs to the fast cluster! Why would this shift happen?\r\n  \r\nI'd like to ask two questions,\r\n1. Is there a known explanation for this (especially the two clusters, and the DHWC4 shift)? Maybe the SSBO to OpenCL conversion should be investigated.\r\n2. If it's not tensorflow's fault but rather weird behavior from the GPU driver, are we really sure that OpenCL should be the preferred implementation in the V2 delegate? With this data in hand I would choose OpenGL hands down, yet TF prefers OpenCL when available. I know I can use `TFLITE_GPU_EXPERIMENTAL_FLAGS_GL_ONLY`, I'm just questioning the default.", "comments": ["Hi @impjdi , could you give some insights? Thanks!", "Without looking at the model, it's hard to tell why.  It's possible that it's taking all the time in conversions + gpu waits.  If you want to dig deeper, you can use `//tensorflow/lite/delegates/gpu/cl/testing/run_performance_profiling.sh` to help diagnose the issue.  It's not going to solve anything, but you could pinpoint what is taking so long / where.", "I described the model above, it has a single node which is a 2D mean. [simple_model_400x400x1.tflite.zip](https://github.com/tensorflow/tensorflow/files/5995424/simple_model_400x400x1.tflite.zip)\r\n\r\nI used this model so that the profiling above would benchmark mostly i/o conversions. I run the CL profiler but it's not telling me much. My issue is not poor CL performance per se but rather\r\n- weird CL behavior, clustering around two performance points\r\n- performance being sometimes much worse than GL, despite CL being the default\r\n- DHWC4 worsening CL performance instead of making it better\r\n\r\nAlso my tests were focused on SSBO input which I don't think that `run_performance_profiling.sh` is using. \r\nHere is the output on Pixel 4 @impjdi \r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nPrecision: CalculationsPrecision::F16\r\nStorage type: TensorStorageType::TEXTURE_ARRAY\r\nPer kernel timing(1 kernels):\r\n  mean 0 - 0.219904ms\r\n--------------------\r\nAccumulated time per operation type:\r\n  mean - 0.219904ms\r\n--------------------\r\nIdeal total time: 0.219904\r\n--------------------\r\n\r\nMemory for intermediate tensors - 0.305178 MB\r\nTotal time - 0.228136ms\r\nTotal time - 0.228282ms\r\nTotal time - 0.227741ms\r\nTotal time - 0.228017ms\r\nTotal time - 0.226982ms\r\nTotal time - 0.228176ms\r\nTotal time - 0.226889ms\r\nTotal time - 0.227861ms\r\nTotal time - 0.228688ms\r\nTotal time - 0.228273ms\r\n\r\n```\r\n\r\n"]}, {"number": 46482, "title": "Addition of Pull request template", "body": "I noticed that this repository has a issue related template, but is missing PR related template. Adding this will have high value to the community as it will help for first time contributers to navigate through their issue effectively.", "comments": ["@vat0599 thanks for the suggestion. We will discuss with team internally.\r\n\r\ncc @mihaimaruseac "]}, {"number": 46475, "title": "Memory leak in Conv2D/Activation on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary, the standard docker distribution\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GeForce RTX 2070, 8GB\r\n\r\n**Describe the current behavior**\r\nI upgraded to TF 2.4.0 from TF 2.1.2, and training a very simple convolutional network, which worked fine in 2.1.2, started running out of memory during training. I distilled a simple reproducible example that demonstrates the issue. Each training epoch consumes about 50MB of additional memory and, given enough epochs, it grows to infinity (or 32 GB in my case). It only occurs on GPU, the same thing runs fine on CPU.\r\n\r\n**Describe the expected behavior**\r\nMemory not growing, or growing only very little\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport gc\r\nimport os\r\nimport psutil\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Activation\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\n\r\ninput_tensor = tf.keras.layers.Input(shape=(512,64,1))\r\n\r\nx = Conv2D(filters=32, kernel_size=(5,5), strides=(2,2), padding='same')(input_tensor)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Conv2D(filters=64, kernel_size=(4,4), strides=(2,2), padding='same')(x)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Flatten()(x)\r\n\r\nx = Dense(5, activation='sigmoid')(x)\r\n\r\nmodel = tf.keras.Model(inputs=input_tensor, outputs=x)\r\n\r\n\r\ntrain_x = np.random.random((2048, 512, 64, 1))\r\ntrain_y = np.random.random((2048, 5))\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam())\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\nfor i in range(50):\r\n    model.fit(train_x, train_y, epochs=1, batch_size=32, verbose=0)\r\n    gc.collect()\r\n    print(i, process.memory_info().rss // 1000000)\r\n```\r\n\r\n**Note 1**\r\nNow, if you uncomment the BatchNormalization() layers creation, the memory problem disappears. So, it is somehow caused by the Activation layer following immediately the Conv2D\r\n\r\n**Note 2**\r\nThe memory problem also occurs if I train multiple epochs in a single fit() call, such as \r\n```\r\nmodel.fit(train_x, train_y, epochs=50, batch_size=32)\r\n```\r\nI used the for loop only to be able to call garbage collection and print the memory.\r\n\r\n**Note 3**\r\nA Conv2D layer with activation embedded in it, such as\r\n```\r\nConv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same', activation='relu')\r\n```\r\nalso causes the memory issue\r\n\r\n\r\n\r\n", "comments": ["I think I have the same issue. I've narrowed it a bit further. Memory usage continually grows until no more memory is available. Here is my minimal working example:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import sys\r\n    physical_devices = tf.config.list_physical_devices('GPU')\r\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n    \r\n    def create_model():\r\n        inputs = tf.keras.Input(shape=(80, 80, 4))\r\n        x = tf.keras.layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(inputs)\r\n        x = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(x)\r\n        x = tf.keras.layers.Flatten()(x)\r\n        outputs = tf.keras.layers.Dense(5, activation='linear')(x)\r\n    \r\n        model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n        return model\r\n    \r\n    model = create_model()\r\n    \r\n    @tf.function\r\n    def get_action(state):\r\n        action_values = model(state)\r\n        return tf.math.argmax(action_values, axis=1)\r\n    \r\n    for i in range(150000):\r\n        states = np.random.randint(0, 255, (32, 80, 80, 4))\r\n        s = get_action(states)\r\n        if i % 50 == 0:\r\n            sys.stdout.write(f'\\rIter: {i}')\r\n\r\nA few things I've noticed:\r\n\r\n1. It functions normally without the convolution layers (both of them). Similar to the original post's Note 1.\r\n2. It functions normally without `tf.function` or on CPU\r\n\r\n**Update:**\r\n\r\n1. The memory leak only occurs with ReLu activation function. LeakyRelu does not cause the memory leak unless setting `alpha=0`.\r\n2. Tanh activation causes a crash with exit code: -1073741571 (0xC00000FD)\r\n", "@jan-x-marek \r\ni ran the code shared and do not see any memory leak on colab, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bce73135c0560aebd6c54caf41695e04/untitled502.ipynb).", "@Saduf2019 I am not getting the issue in the colab. @jan-x-marek and I are both using Cuda 11.0, but the colab is on 10.1. From what I could tell [here](https://www.tensorflow.org/install/source#gpu), TF 2.4 uses Cuda 11.0, correct? ", "@Saduf2019 \r\n\r\nAs @zlee406 says, your Colab notebook is running on CUDA V10.1.243. Me and @zlee406 are running on CUDA 11 (V11.0.221 in my case), as required by the install guide: https://www.tensorflow.org/install/gpu\r\n\r\nThis is how my memory grows when I run the sample code:\r\n0 4194\r\n1 4280\r\n2 4376\r\n3 4439\r\n4 4530\r\n5 4583\r\n6 4614\r\n7 4670\r\n8 4731\r\n9 4773\r\n10 4802\r\n11 4871\r\n12 4920\r\n13 4960\r\n14 5016\r\n15 5100\r\n16 5159\r\n17 5228\r\n18 5289\r\n19 5367\r\n20 5422\r\n21 5469\r\n22 5526\r\n23 5561\r\n24 5598\r\n25 5659\r\n26 5726\r\n27 5786\r\n28 5846\r\n29 5873\r\n30 5959\r\n31 6008\r\n32 6065\r\n33 6155\r\n34 6198\r\n35 6252\r\n36 6287\r\n37 6338\r\n38 6412\r\n39 6456\r\n40 6546\r\n41 6598\r\n42 6653\r\n43 6730\r\n44 6783\r\n45 6800\r\n46 6874\r\n47 6926\r\n48 6967\r\n49 7064", "I have the same issue. It seems that the problem only occurs when using Conv2D with stride != 1.", "A potential substitute until the issue is resolved is just to replace the ReLu activation with `tf.where(x <= 0., 0., x)`", "I have same issue in windows. cuda11, cudnn8 and libtensorflow 2.3.1 which compiled by me.\r\nI replaced ReLU to LeakyReLU(0.001) after that, no more leaks.\r\nSame issue in tf 2.3.1, so It could be a bug of CUDA11 or cuDNN8.", "I have encountered the same problem and was able to create a slightly smaller example to reproduce it. If left running for long enough the RAM usage increases until swapping causes the GPU utilization to drop.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ngpu = tf.config.experimental.list_physical_devices('GPU')[0]\r\ntf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(128, 3, 1, 'same', activation='relu')\r\n])\r\n\r\ndataset = tf.random.normal((1024, 32, 32, 128))\r\n\r\nmodel.compile(loss='mse')\r\n\r\nmodel.fit(dataset, dataset, 8, epochs=1000)\r\n```", "I temporarily fixed the issue by replacing `ReLU()` with `lambda x : tf.math.maximum(x, 0.0)`.", "I have the same issue with cuda 11 and tensorflow >= 2.4.0.\r\nReplaced relu with elu and memory leak was gone. \r\nWhat it also fixed for me: Swapping my Google cloud VM from a Nvidia Tesla V100 to a Nvidia P100 WITHOUT replacing the ReLU. So ReLU + Nvidia P100 works fine, ReLU + Nvidia Tesla V100 results in memory leak.", "Can confirm a similar reproduction with the following setup:\r\n\r\nBase container: cuda:11.0-cudnn8-devel-ubuntu18.04\r\nTensorflow version: 2.4.1\r\nDriver version: 450.51.06\r\nGPU: T4 \r\n\r\nApplying the ReLU fix from @garrettbingham fixes the repro script, though I'm currently validating whether this fixes our real workload too.", "@jan-x-marek, Thanks for saving my time.", "I experience the same on 2.4.1 with RTX 3090. Replaced `relu` with `elu` as a temporary fix.", "A bit more update from my testing:\r\n\r\nI applied the relu fix globally to our codebase using a `get_custom_objects().update({'relu': Lambda(lambda x: tf.maximum(0., x))})` call. This *didn't seem to solve it anymore*, as opposed to replacing it inline. I'm wondering whether something related to relu *as a name* is problematic, leading to some optimization here or there on certain hardware? It might also be that whatever is bad around relu happens before my hook ran, though it did run in global file scope (import time) so it'd have to be another import-time side-effect. ", "Using LeakyReLU instead of relu saved my day, continuous host memory increase is gone. \r\n\r\nEDIT: It also worked for me to give my relu layers a `None `activation and use a `tf.keras.layers.ReLU()` layer after each of them. ", "I was seeing this issue using with following config:\r\n\r\n```\r\nTF1.14, cuda 11.0, cudnn 8.0.2, conv2d with stride != 1 and a relu right after it.\r\n```\r\n\r\n**I tried upgrading cudnn version from 8.0.2 to 8.2.0 and now the issue is gone.**\r\n\r\nNote that there is an item in cudnn 8.1 [release note](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-810) that could be related: \r\n\r\n```\r\nCalling cudnnConvolutionBiasActivationForward() or executing a cuDNN backend plan for fused convolution-bias-activation operation graphs, can lead to a memory leak. This issue is fixed in the current release.\r\n```", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5f24e4448776eb46a19972f35d6986d6/untitled194.ipynb)..Thanks !", "@sushreebarsa Have you tried the fix I posted above? Just want to see if it works for TF2.x.", "has anyone find the issue\r\nI am getting a spike after 3rd epochs and the memory seem to increase for every epoch with 500mb increment \r\nused leaky RELU as activation and tf.keras.clear_session() at epoch end \r\nI haven't been able to train my model more than 4 epochs\r\nI've been training my model in colab\r\n\r\n", "Sorry for the off-topic but\r\n> memory seem to increase for every epoch with 500mb increment\r\n> used leaky RELU\r\n\r\nLeaky ReLU - now 20% more leaky!", "> Sorry for the off-topic but\r\n> \r\n> > memory seem to increase for every epoch with 500mb increment\r\n> > used leaky RELU\r\n> \r\n> Leaky ReLU - now 20% more leaky!\r\n\r\nwell for me there is no leak in ReLU as well as Leaky ReLU, the problem lies with model fit when evaluating the validation dataset \r\nNo leaks if the validation dataset is not included in the model fit so I had to independently run fit and evaluate\r\nNo leaks for now and runs fine for now.", "This thread is a life saver. Replaced `relu = tf.keras.layers.Activation(tf.nn.relu)` with `relu = lambda x : tf.math.maximum(x, 0.0)` like @garrettbingham said and no more memory leaks.\r\nThis issue needs to be fixed asap, it's a real struggle to find out where the leak even comes from. Only way I found out was by removing tf.function from everything and realising the leak disapeared (don't know why), which after a while I realised only happened on functions where the model itself was called.", "I faced the issue with Cuda 11.3 and CuDNN 8.2.0.\r\nThe problem persists in the latest Cuda 11.5 and CuDNN 8.3.1.\r\nInterestingly, it only happens with enabled AUTOTUNE.\r\nIn a  screenshot below you can see a ~2Gb memory leak coming from cudnn_frontend::ExecutionPlanBuilder_v8::build(), but the inner stack is referencing cudnn_cnn_infer64_8.dll\r\n\r\n![image](https://user-images.githubusercontent.com/46108258/144567553-b1ab31b9-4f0a-4c9c-baf5-84088c99bdae.png)\r\n\r\n"]}, {"number": 46474, "title": "tensorflow.io.gfile.GFile: utf-8 can't decode byte 0xe9 (with both 'r' and 'rb' arguments), while normal open can read the file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary via pip\r\n- TensorFlow version: 2.4.0\r\n- Tensorflow IO version: 0.17.0\r\n- Tensorflow datasets: 4.2.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 11.2.67/8.0.5.39\r\n- GPU model and memory: GTX 965M/8Gb\r\n\r\n**Describe the current behavior**\r\nI am trying to generate a tensorflow dataset using tfds, by following this tutorial: https://www.tensorflow.org/datasets/add_dataset. I do so using the dataset from https://achrafothman.net/site/asl-smt/ (Corpus sample data) which consists of 2 text files, one in English, and the other in American SIgn Language glossed (still text). The files are utf-8-sig encoded, but the same behaviour happens when I save the files as utf-8. Before doing the tfds dataset, I simply opened the file with open, and that worked just fine. However, I am now trying to open them while creating my own dataset, so I import tfds. However, this library raises an error when one uses os.open, saying that we should use its own function, GFile. I then replaced my open with gfile.GFile(filename, 'r'), but it produces the following error when I am running the test (`python -m asgl_dataset.asgl_dataset_test`) (It does pass some tests before raising an error, which I guess are not important to paste here):\r\n```\r\n======================================================================\r\nERROR: test_download_and_prepare_as_dataset (__main__.AslgDatasetTest)\r\nAslgDatasetTest.test_download_and_prepare_as_dataset\r\nRun the decorated test method.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\testing\\test_utils.py\", line 276, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\testing\\dataset_builder_testing.py\", line 307, in test_download_and_prepare_as_dataset\r\n    self._download_and_prepare_as_dataset(self.builder)\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\testing\\dataset_builder_testing.py\", line 372, in _download_and_prepare_as_dataset\r\n    builder.download_and_prepare(download_config=download_config)\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 446, in download_and_prepare\r\n    download_config=download_config,\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1181, in _download_and_prepare\r\n    leave=False,\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 1176, in <listcomp>\r\n    for split_name, generator\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\", line 295, in submit_split_generation\r\n    return self._build_from_generator(**build_kwargs)\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py\", line 359, in _build_from_generator\r\n    leave=False,\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tqdm\\std.py\", line 1165, in __iter__\r\n    for obj in iterable:\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow_datasets\\testing\\dataset_builder_testing.py\", line 402, in _iter_examples\r\n    for key, ex in generator:\r\n  File \"D:\\Documents\\Epitech\\Gesture\\gesturesoftware\\src\\backend\\aslg pc12 dataset\\aslg_dataset\\aslg_dataset.py\", line 79, in _generate_examples\r\n    english = gfile.GFile('english_processed_utf8.txt', 'rb').read()\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 117, in read\r\n    self._preread_check()\r\n  File \"D:\\Anaconda\\envs\\deeplearning_37\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 80, in _preread_check\r\n    compat.path_to_str(self.__name), 1024 * 512)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 85: invalid continuation byte\r\n```\r\nFrom my understanding, I think that GFile simply does not handle special characters, such as '\u00e9'. After searching the web, I found that a common solution was to use GFile(filename, 'rb') (instead of 'r', so that the file is read as bytes), but this results in the exact same behavior. \r\n\r\n**Describe the expected behavior**\r\nThe files should open properly using GFile, or AT LEAST let the user use open, and let me move on to parse the text they contain so that I can produce my dataset. \r\n\r\n**Standalone code to reproduce the issue**\r\nThe files were downloaded from the previously mentioned link (https://achrafothman.net/site/asl-smt/), and then first processed using a small script. The files with '_utf8' in the name were opened with Windows' notepad, and then I used 'saved as', and selected the encoding to be 'utf-8' instead of 'utf-8 without bom'.\r\n\r\nSmall script:\r\n```python\r\ndef delete_end_dots(input_file_name, output_file):\r\n    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_f, open(output_file, \"w+\", encoding=\"utf-8\") as out_f:\r\n        for line in input_f:\r\n            line = line.strip()\r\n            splitted = line.split(\" \")\r\n            if splitted[-1] == '.':\r\n                splitted = splitted[:-1]\r\n            line = \" \".join(splitted)\r\n            print(line, file=out_f)\r\n\r\n\r\ndelete_end_dots(\"sample-corpus-asl-en.asl.txt\", \"asl_processed.txt\")\r\ndelete_end_dots(\"sample-corpus-asl-en.en.txt\", \"english_processed.txt\")\r\n```\r\n\r\naslg_dataset.py:\r\n```python\r\n\r\n\"\"\"aslg_dataset dataset.\"\"\"\r\n\r\nimport sys\r\nprint(\"started on kernel {}\".format(print(sys.executable)))\r\n\r\nimport tensorflow_datasets as tfds\r\nimport re\r\nimport tensorflow.io.gfile as gfile\r\n\r\n# TODO(aslg_dataset): Markdown description  that will appear on the catalog page.\r\n_DESCRIPTION = \"\"\"\r\n## ASLG-SMT Dataset \r\n## by Achraf Othman and Mohamed Jemni\r\nThis dataset has 87706 sentences, in both english and American Sign Language, glossed.\r\nWe added some processing compared to the original dataset: punctuation is split from the words for english (some sentences did not apply this rule).\r\nwe also assert that each word is separated from anything else, for both (e.g. 123word becomes 123 word).\r\nWe finally delete any duplicate spaces. \r\n\"\"\"\r\n\r\n# TODO(aslg_dataset): BibTeX citation\r\n_CITATION = \"\"\"@INPROCEEDINGS{8336054,\r\nauthor={A. {Othman} and M. {Jemni}},\r\nbooktitle={2017 6th International Conference on Information and Communication Technology and Accessibility (ICTA)},\r\ntitle={An XML-gloss annotation system for sign language processing},\r\nyear={2017},\r\nvolume={},\r\nnumber={},\r\npages={1-7},\r\ndoi={10.1109/ICTA.2017.8336054}}\r\n\"\"\"\r\n\r\n\r\nclass AslgDataset(tfds.core.GeneratorBasedBuilder):\r\n  \"\"\"DatasetBuilder for aslg_dataset dataset.\"\"\"\r\n\r\n  VERSION = tfds.core.Version('1.0.0')\r\n  RELEASE_NOTES = {\r\n      '1.0.0': 'Initial release.',\r\n  }\r\n\r\n  def _info(self) -> tfds.core.DatasetInfo:\r\n    \"\"\"Returns the dataset metadata.\"\"\"\r\n    # TODO(aslg_dataset): Specifies the tfds.core.DatasetInfo object\r\n    return tfds.core.DatasetInfo(\r\n        builder=self,\r\n        description=_DESCRIPTION,\r\n        features=tfds.features.FeaturesDict({\r\n            # These are the features of your dataset like images, labels ...\r\n            'input_text': tfds.features.Text(),\r\n            'output_text': tfds.features.Text(),\r\n        }),\r\n        # If there's a common (input, target) tuple from the\r\n        # features, specify them here. They'll be used if\r\n        # `as_supervised=True` in `builder.as_dataset`.\r\n        supervised_keys=None,  # Set to `None` to disable\r\n        citation=_CITATION,\r\n    )\r\n\r\n  def _split_generators(self, _):\r\n    \"\"\"Returns SplitGenerators.\"\"\"\r\n    # TODO(aslg_dataset): Downloads the data and defines the splits\r\n    # path = dl_manager.download_and_extract('https://todo-data-url')\r\n\r\n    # TODO(aslg_dataset): Returns the Dict[split names, Iterator[Key, Example]]\r\n    return {\r\n        # 'train': self._generate_examples(path / 'train_imgs'),\r\n        'train': self._generate_examples()\r\n    }\r\n\r\n  def _generate_examples(self):\r\n    \"\"\"Yields examples.\r\n    File encoding is utf-8-sig\"\"\"\r\n    english = gfile.GFile('english_processed_utf8.txt', 'rb').read() ## I tried to open the file, and then use readlines, but it produces the same error\r\n    asl = gfile.GFile('asl_processed_utf8.txt', 'rb').read() ## '_utf8' in the file name describes the same file as without, except it was saved with an utf-8 encoding instead of utf-8-sig. The exact same behaviour occurs with both files (with and without '_utf8')\r\n    # with open('english_processed.txt', encoding='utf-8-sig') as english_f, open('asl_processed.txt', encoding='utf-8-sig') as asl_f:\r\n    english = [eng for eng in english.split('\\n') if eng]\r\n    asl = [a for a in asl.split('\\n') if a]\r\n    for i, sentences in enumerate(zip(english, asl)):\r\n      eng_sen, asl_sen = self._clean_sentences(*sentences)\r\n      yield i, {\r\n        'input_text': eng_sen.strip(),\r\n        'output_text': asl_sen.strip()\r\n      }\r\n\r\n  @staticmethod\r\n  def _clean_sentences(eng_sen, asl_sen):\r\n    eng = eng_sen.replace(\"!\", \" ! \").replace(\".\", \" . \").replace(\",\", \" , \")\r\n    eng = re.sub(r'([\\-\\'a-zA-Z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff]+)', r' \\1 ', eng)\r\n    eng = re.sub(r' +', ' ', eng)\r\n\r\n    asl = re.sub(r'([0-9]+(?:[.][0-9]*)?)', r' \\1 ', asl_sen)\r\n    asl = re.sub(r' +', ' ', asl)\r\n\r\n    return eng, asl\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe directory in which everything happens has been generated using `tfds new`. The only file I modified was the one pasted before, which is the main python file. We then use `tfds build` before running the test.", "comments": ["@fabrien \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Hey, @ravikyram! It's not online, so I don't have a link for you, unfortunately. However, I do give everything necessary to reproduce:\r\n\r\n`tfds new aslg_dataset` in a directory\r\n\r\ncopy paste the given code in the file aslg_dataset.py\r\nYou can download the corpus files at the given link as indicated, process them with the given small python script given (or not, to be honnest it will not change anything if you don't use the script), and name them accordingly. You can then run\r\n\r\n`tfds build`\r\n\r\nand finally run the python file (from the superior directory) with\r\n\r\n`python -m aslg_dataset.aslg_dataset_test.py`\r\n\r\nIf you want to, I can upload my full directory here, but I guess you are not interested in a 87000x2 sentences corpus on that issue ^^, and the only modified file from what `tfds new` gives you, is aslg_dataset.py, for which you already have the code. In brief: I already gave the standalone code.", "Wondering if this is related to #33590"]}, {"number": 46462, "title": "Tensorflow Lite OpenCL delegate outputs NaN's when CPU works fine", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S20 w/ Qualcomm chipset\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): Nightly\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.4\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Adreno 650\r\n\r\n**Describe the current behavior**\r\nFor my model, the CPU (with or without XNNPACK delegate) produces the correct outputs on Snapdragon 865. However, some of the outputs from the OpenCL GPU delegate are completely wrong. I see NaN's and numbers like 5.83379e+29 in some of the output tensors in the network. The model has more than 1 output, other output tensors of the same model , however, show entirely correct results.\r\n\r\nAs far as I am aware, I have turned off FP16 optimizations so that model runs at full 32 bit floating point precision:\r\n\r\n        TfLiteGpuDelegateOptionsV2 options;\r\n        options.is_precision_loss_allowed = 0;\r\n        options.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER;\r\n        options.inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;\r\n        options.inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n        options.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n \r\n**Describe the expected behavior**\r\nCPU outputs and GPU delegate outputs should match.\r\n\r\n**Standalone code to reproduce the issue**\r\nYou can find the C++ sample code (along with the .tflite model, the original Keras model, Keras->TFLite conversion script, complete sample output from CPU and OpenCL GPU) that reproduces this error at https://github.com/DwayneDuane/tensorflow_lite_opencl_bug\r\n", "comments": ["Hi\r\nCould u provide the commands to build OpenCL backend TFLite GPU delegate?\r\nThanks\r\n", "@rose-jinyang \r\nHere is my build command:\r\n\r\n`bazel build -c opt --config=android_arm64 --cxxopt=-std=c++14 --config=mkl --config=mkl_aarch64 --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 //tensorflow/lite:libtensorflowlite.so //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so`\r\n", "Hi @DwayneDuane \r\nCould u check the following issue?\r\nhttps://github.com/tensorflow/tensorflow/issues/46498\r\nThanks"]}, {"number": 46447, "title": "Make tf.image.resize compatible with XLA compilation", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, XLA compilation does not work for `tf.image.resize` when method is `tf.image.ResizeMethod.BILINEAR` or `tf.image.ResizeMethod.NEAREST_NEIGHBOR.` Because `half_pixel_centers` is forced to true [here](https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/image_ops_impl.py#L1623-L1627), but the tf2xla kernel forces `half_pixel_centers` to false [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/image_resize_ops.cc#L592).\r\n\r\nThis makes users unable to do upsampling/downsampling for images on TPU.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, just change underlying behavior.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who needs to use down/up sampling method for XLA.\r\n\r\n**Any Other info.**\r\n\r\nColab to reproduce errors.\r\n\r\nhttps://colab.research.google.com/drive/1RKW1U3pqoc7w58ifq0qjb6WqBJQ3cD9V?usp=sharing\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ab3cafb61d1f52249596f76f3b53e762/46447-2-5.ipynb). Thanks!"]}, {"number": 46444, "title": "Spurious results from a full-integer TensorFlow Lite model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.5.0-dev20210114 (`tf-nightly`)\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have been able to convert the [Boundless model](https://tfhub.dev/s?q=google%2Fboundless) with full integer quantization. However, the results of the model are spurious. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe result should be consistent with the results from other TFLite variants of the same model. \r\n\r\nHere's the expected result - \r\n\r\n![image](https://user-images.githubusercontent.com/22957388/104683094-3bdd6d00-571c-11eb-87fa-f927b3c5a713.png)\r\n\r\nThe masked and the generated images are from the model. \r\n\r\nHere's the result from the integer model - \r\n\r\n![image](https://user-images.githubusercontent.com/22957388/104683131-4f88d380-571c-11eb-9655-5ce8f2f49fc2.png)\r\n\r\nNotice how the masked and the generated images are spurious. Is this because I am constructing the representative dataset to only have 50 examples? Is the converter not able to appropriately approximate the activation ranges? \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab Notebook https://colab.research.google.com/gist/sayakpaul/5f52b65c849e751e81e4073d6cf737bf/boundless_tflite.ipynb\r\n\r\nCc: @abattery \r\n", "comments": ["@ymodak \r\nI am able to replicate the issue reported, please fin the [gist here](https://colab.research.google.com/gist/Saduf2019/ef14c11086b41416877508ab7bc7df55/untitled496.ipynb)", "CCing @terryheo", "@abattery you probably used the email alias of Terry. ", "Hi @sayakpaul,\r\n\r\nI tried your colab, and got this result.\r\n![image](https://user-images.githubusercontent.com/4837376/107916096-07720080-6fa9-11eb-8b23-9915c28b55d7.png)\r\n\r\nAlthough image is not as good as dynamic_range quantized one, the error seems to affect only masked region. Can you try again with latest TF nightly?\r\n\r\nAlso, as I commented on the other issue https://github.com/tensorflow/tensorflow/issues/45932, you need to provide `tf.lite.OpsSet.TFLITE_BUILTINS` to `supportedd_ops` for dynamic range quantization.", "> Although image is not as good as dynamic_range quantized one, the error seems to affect only masked region. Can you try again with latest TF nightly?\r\n\r\nYes, will try and let you know. \r\n\r\n> Although image is not as good as dynamic_range quantized one, the error seems to affect only masked region. Can you try again with latest TF nightly?\r\n\r\nYes, noted. ", "@teijeong I followed your suggestion on trying out with latest `tf-nightly`. No luck.\r\n\r\nHere's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/c645035b5d8fc529a7396711c9013463/boundless_tflite.ipynb). ", "Was able to replicate the issue in TF 2.6.0-dev20210602,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/9f796a7fdb02ae030d236f36a9ac8623/untitled496.ipynb#scrollTo=HMiJeeuIpCN9)..Thanks !"]}, {"number": 46428, "title": "loss function input \"y_pred\" should be full list of model-outputs", "body": "\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes, but it seems that multiple other things might depend on it\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn the current version of Keras, the loss function is applied for each output of the model elementwise (if there are multiple), or one can define for each individual output one loss function. This seems to be not really consistent with other fundamentals in Keras / TF, and it would be better to have at least the possibility to get the full list of model-outputs as a y_pred input of the loss function. It's especially confusing since in the train_step\r\nhttps://github.com/tensorflow/tensorflow/blob/a338a52543e06c26772c2f5007b6a5aad6768fa3/tensorflow/python/keras/engine/training.py#L790-L791\r\none passes the full list of model-outputs as y_pred, but the loss function is run multiple times for each individual model-output and hence gets every element of the list separately as y_pred\r\n\r\n**Will this change the current api? How?**\r\nYes, there wouldn't be multiple losses for a model but only one. One could still get the same results by building a \"wrapper\" around the individual losses and adding it as a loss. \r\n\r\n**Who will benefit from this feature?**\r\nEveryone who wants to build more complex models, especially if you have different output dimensions in your model-output, where the proposed solution is in https://github.com/keras-team/keras/issues/14140 would not work anymore (e.g., one output an image and one output a label).  \r\n\r\n**Any Other info.**\r\n", "comments": ["this will also help while passing Input image (x_train) to loss function for image reconstruction loss.  also in multi-task network and VAE ", "A temporary solution is to name each layer before output,.     \r\n\r\nthen use \r\n`def loss_function(y_true, y_pred):\r\nif y_pred.name = \"output1 \":\r\nXYZ\r\nif y_pred.name =  \"output2\":\r\nZYX\r\n\r\nreturn  loss `\r\n\r\nhowever to combine these losses,.  we might need to call multiple times in compile method\r\n\r\n`\r\nmodel.compile(optimizer= optimizer, loss={ 'output1' : loss_function ,  'output2' : loss_function } , loss_weights = accordingly )\r\n`\r\n\r\n\r\n"]}, {"number": 46425, "title": "Remove no_cuda_on_cpu_tap tag and fix tests", "body": "This is using TF 2.4.0\r\n\r\n**Describe the problem**\r\n\r\nWhen building and running tests I stumbled over the seemingly required test tag `no_cuda_on_cpu_tap` which is only documented in one place:\r\n```\r\n    # Running cuda on cpu will trigger tests guarded by GOOGLE_CUDA but NCHW\r\n    # won't be available, which result in test failures. So disable that.\r\n```\r\n\r\nThis sounds like a workaround for a problem when writing the tests. The most obvious incarnation for this is e.g. `//tensorflow/core/common_runtime:ring_reducer_test`. That is added with `tf_cuda_cc_test` which adds 2 tests: One with `_gpu` suffix gpu tags and one without. However as the test code uses `GOOGLE_CUDA` to decide whether to test on CPU or GPU the CPU test version cannot be run when TF is built with CUDA, which is (AFAIK) the regular case.\r\n\r\nHence my suggestion would be to \"fix\" the tests to run only on GPU if it is tagged with `gpu` and pass that to the test code as a define (e.g. `TF_TEST_USE_GPU`)\r\n\r\n", "comments": []}, {"number": 46416, "title": "Timeseries example does not work with Preprocessing layers", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n\r\n## Description of issue (what needs changing):\r\nThe examples use methods such as \r\n```\r\ndata = np.array(data, dtype=np.float32)\r\nds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n      data=data,\r\n      targets=None,\r\n      sequence_length=self.total_window_size,\r\n      sequence_stride=1,\r\n      shuffle=True,\r\n      batch_size=32,)\r\n```\r\nBut, this pretty much disables the usage of preprocessing layers to be built. And also, such an elaborate example does not use sequence features or input layer methods. The method ```timeseries_dataset_from_array``` pretty much did away any usage of preprocessing layers in the model. We can not effectively use feature layer engineering because we have to convert all the feature columns into float. \r\n\r\n", "comments": ["@summa-code \r\n\r\nThank you for raising the issue.\r\nDo you have any use case that supports the issue. Please feel free to submit a PR if you have use case supporting this.Thanks!", "It is example code. That is what i described. How do i use Categorical column in the example they have shown ? "]}, {"number": 46402, "title": "New feature request:implementation of _get_control_flow_context function", "body": "For interpretability of siamese-like networks with two legs (blocks of BiLSTM), I need to get the gradients of output (similarity value between 0-1) with respect to two inputs (two sequence) for which I use the gradient tape. But then I run to the following error: \r\n\r\n_tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new_\r\n\r\ncoming from: tensor flow/python/eager/backprop.py - def _get_control_flow_context\r\n\r\nCould you please explain how this can be solved? Can use of this function be avoided for now or can you please explain what I need to implement there?\r\n \r\nTraining the network was fine and prediction results look good. The issue is only with interpretability of the method. Here is where my code faces this error:\r\n\r\n\r\n```\r\n# convert to tensor and reshape\r\nsample_seq_1 = tf.convert_to_tensor(train_seq1[0], dtype=tf.float32)\r\nsample_seq_2 = tf.convert_to_tensor(train_seq2[0], dtype=tf.float32)\r\nsample_seq_1 = tf.expand_dims(sample_seq_1, 0)\r\nsample_seq_2 = tf.expand_dims(sample_seq_2, 0)\r\n\r\n# inference in tape\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch([sample_seq_1, sample_seq_2])\r\n    similarity_value_pred = siamese_net_model([sample_seq_1, sample_seq_2])[0,0]\r\n\r\n# getting gradients\r\ntape.gradient(similarity_value_pred, [sample_seq_1, sample_seq_1])\r\n```\r\n\r\n", "comments": ["@sepidehsaran \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram @amahendrakar \r\nI have created [this colab notebook](https://colab.research.google.com/drive/1k-FCSkw6CfGyMkwR5eTS4U8-jGVzd24I?usp=sharing) to reproduce the issue on dummy data. \r\nI couldn't reproduce it with TF 2.4 but I get none for the gradients when using this TF version."]}, {"number": 46390, "title": "Support for LSTM and GRU ", "body": "Hello,\r\n\r\nI wonder if there is support for LSTM and GRU within TensorFlow Lite for Microcontrollers (audio NN models) or any plan to have these operations? \r\n\r\nThank you.\r\n\r\nBest regards,\r\nPeter", "comments": ["@peter197321,\r\nCan you please refer [this link](https://www.tensorflow.org/lite/convert/rnn#:~:text=Known%20issues%2Flimitations-,Overview,TensorFlow%20Lite's%20fused%20LSTM%20operations.&text=Provide%20native%20support%20for%20standard,This%20is%20the%20recommended%20option.) which explains LSTM support for TF Lite? Thanks! ", "We don't have a good answer at this time. https://github.com/tensorflow/tensorflow/issues/43380 is a related issue and I am going to close the current issue in favor of the earlier one so that any progress can be discussed in a consolidated location.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46390\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46390\">No</a>\n", "Hi apols to ask again but wondering if 'GRU within TensorFlow Lite for Microcontrollers (audio NN models)' has had any development?\r\n\r\n", "This is not currently on the list of planned work.\r\n\r\nHaving said that,the person who created a different github issue on this topic (https://github.com/tensorflow/tensorflow/issues/43380) is an active contributor to TFLM, so if your use-case is aligned with what @yair-ehrenwald is doing, that may be the path forward.\r\n\r\nAny additional details on your use case would be useful.\r\n", "It was just a thought to the upcoming esp32-s3 that has vector support and maybe be capable and trying to think of an edge KWS that would work for both microcontroller & embedded linux with tensorflow-lite and reuse of converted models.\r\nThe tensorflow4micro has options for kws and if you where to try and keep a common model then higher frameworks will need to use what works on tf4m which just made me wonder with rnn / gru being common rather than the tail wagging the dog is it possible to have tf4m support?\r\n\r\nThe low cost microcontrollers are of interest for distributed array kws where voice = near / noise = fair by distribution and position and a kws would just be a simple low cost networked sensor that may co-exist with standard standard TensorFlow Lite devices and models.\r\n\r\n", "@peter197321 @StuartIanNaylor \r\n\r\nHi, sorry for reopening this so late, been out of the office for a while.\r\n\r\nI've successfully used and deployed models using Keras.GRUCell with TFLM. \r\nUsing Keras.GRU directly results in the TFLite convertor creating a WHILE op so is problematic, but GRUCell works just fine and is implemented with fully connected layers. \r\nYou will probably need to do some rewriting to your model and make it work \"frame by frame\" but you probably need to do that anyway to get it to run on an actual device.\r\n\r\nIf you have any specific issues feel free to tag me in an issue with more details and I'll have a look.\r\n\r\n", "I never really got to specifics would the examples @\r\n\r\nhttps://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn\r\nhttps://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn_state\r\n\r\nMake a good base as relatively low ops and have non-stream & stream examples.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/testdata/while_op_with_forwarding_input.bin\r\n\r\nPS does that mean Keras.GRU is also in the pipeline?\r\n", "Commenting to know if there's been any update to adding support for LSTM or GRU layers in TFLM, and if so, could somebody point out some resources to understand how to do it?"]}, {"number": 46380, "title": "Add support for converting mixed_float16 models to TF_lite", "body": "**System information**\r\n- Windows 10\r\n- tensorflow==2.4.0 installed via pip\r\n\r\n**Project details:**\r\n* [notebook](https://captaindario.github.io/DaKanjiRecognizer-ML/DaKanjiRecognizer.html)\r\n* [github repository](https://github.com/CaptainDario/DaKanjiRecognizer-ML)\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tf_models_dir) # path to the SavedModel directory\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\nE:\\projects\\DaKanjiRecognizer\\.venv_dev\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    209     try:\r\n--> 210       model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n    211                                                  toco_flags_str, input_data_str,\r\n\r\nE:\\projects\\DaKanjiRecognizer\\.venv_dev\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     31   \"\"\"Wraps TocoConvert with lazy loader.\"\"\"\r\n---> 32   return _pywrap_toco_api.TocoConvert(\r\n     33       model_flags_str,\r\n\r\nException: <unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_1_input/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x64x64x1xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_2/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x31x31x32xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_3/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x14x14x32xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_4/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x6x6x64xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-15-758d3ba4bcca> in <module>\r\n      7 # Convert the model\r\n      8 converter = tf.lite.TFLiteConverter.from_saved_model(tf_models_dir)\r\n----> 9 tflite_model = converter.convert()\r\n     10 \r\n     11 # Save the model.\r\n\r\nE:\\projects\\DaKanjiRecognizer\\.venv_dev\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    737     converter_kwargs.update(quant_mode.converter_flags())\r\n    738 \r\n--> 739     result = _convert_saved_model(**converter_kwargs)\r\n    740     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n    741     if calibrate_and_quantize:\r\n\r\nE:\\projects\\DaKanjiRecognizer\\.venv_dev\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in convert_saved_model(saved_model_dir, saved_model_version, saved_model_tags, saved_model_exported_names, **kwargs)\r\n    630     model_flags.saved_model_exported_names.extend(saved_model_exported_names)\r\n    631   toco_flags = build_toco_flags(**kwargs)\r\n--> 632   data = toco_convert_protos(\r\n    633       model_flags.SerializeToString(),\r\n    634       toco_flags.SerializeToString(),\r\n\r\nE:\\projects\\DaKanjiRecognizer\\.venv_dev\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    214       return model_str\r\n    215     except Exception as e:\r\n--> 216       raise ConverterError(str(e))\r\n    217 \r\n    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_1_input/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x64x64x1xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_2/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x31x31x32xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_3/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x14x14x32xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"DaKanjiRecognizer/conv2D_4/Conv2D@__inference__wrapped_model_1048891\" at \"StatefulPartitionedCall@__inference_signature_wrapper_1049439\") at \"StatefulPartitionedCall\")): 'tfl.conv_2d' op operand #0 must be tensor of 32-bit float or QI8 type or QUI8 type or QI16 type values, but got 'tensor<?x6x6x64xf16>'\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n\r\n\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\nThere is no TF lite mixed precision model but [here is a model trained with default precision](https://github.com/CaptainDario/DaKanjiRecognizer/tree/main/data).\r\n\r\n\r\n**Any other info / logs**\r\n\r\nWhen I train the model using ```mixed_float16``` set with:\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\npolicy = mixed_precision.Policy('float32')\r\n#policy = mixed_precision.Policy('mixed_float16')      # <-- uncomment this\r\nmixed_precision.set_policy(policy)\r\n```\r\nthe converter crashes. As stated [here](https://www.tensorflow.org/lite/guide/ops_compatibility) (if I understand correctly) there is currently no support for this scenario. <br/>\r\nIf this is not planned to be implemented, are there any workarounds to still use float16 on supported GPUs. So one can still speed up training and is able to train larger models.", "comments": ["Sorry for encountering this. Currently, TFLite do not support mixed float16 models in TF yet.", "Is there a way to convert tha savedModel to float32 and than convert it to a TF_lite model?\r\nAnd is it planned to support ```mixed_float16``` for TF_lite models?", "We are running into this issue as well since we've switched from the graph rewrite to the Keras mixed precision API.\r\n\r\n@reedwm What is the recommended way to convert a model with `dtype_policy=\"mixed_float16` back to `float32` in order to  support conversion to TFLite? I cannot seem to find any official docs on what the recommended workflow is for this case.", "Checkout the following minimal example which results in a segfault:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\r\n\r\nimg = tf.keras.layers.Input(shape=(224, 224, 3))\r\nx = tf.keras.layers.Conv2D(16, 3)(img)\r\nx = tf.keras.layers.BatchNormalization(momentum=0.7)(x)\r\nmodel = tf.keras.Model(img, x)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```", "I never found a solution and I am still interested in it.", "> I never found a solution and I am still interested in it.\r\n\r\nSo one workaround that seems to work for us is to clone the model and overwrite the `dtype`:\r\n```python\r\nmodel = tf.keras.models.clone_model(\r\n    model,\r\n    clone_function=lambda layer: layer.__class__.from_config(\r\n        {**layer.get_config(), \"dtype\": \"float32\"}\r\n    ),\r\n)\r\n```\r\nHowever this doesn't play nicely with custom objects and seems like a quite brittle solution.", "Thank you!\r\nI will give this a try because using float16 on RTX GPU's is such a big improvement for me.\r\n\r\nHowever I really hope that somebody from the TF team will look at this again.", "> Is there a way to convert tha savedModel to float32 and than convert it to a TF_lite model?\r\n\r\n@lgeiger solution works, but as he said, it is brittle (doesn't work with subclassed models, there is no guarantee a layer's config has a `dtype` attribute, etc). It will work in many cases though.\r\n\r\nAlternatively, the model can be rebuilt as a float32 model. This will work in any case as long as the model can be rebuilt and supports float32. In either case, the weights of the mixed precision model must then be copied to the weights of the float32 model. For example\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Build a toy model\r\ndef get_model():\r\n  img = tf.keras.layers.Input(shape=(224, 224, 3))\r\n  x = tf.keras.layers.Conv2D(16, 3)(img)\r\n  x = tf.keras.layers.BatchNormalization(momentum=0.7)(x)\r\n  model = tf.keras.Model(img, x)\r\n  return model\r\n\r\n# Create a mixed_float16 model\r\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\r\nmodel = get_model()\r\n\r\n# Compile and train model\r\nmodel.compile('sgd', 'mse')\r\nx = np.random.normal(size=(64, 224, 224, 3))\r\ny = np.random.normal(size=(64, 222,222, 16))\r\nmodel.fit(x, y)\r\n\r\n# Create a float32 model with the same weights as the mixed_float16 model, so\r\n# that it loads into TF Lite\r\ntf.keras.mixed_precision.set_global_policy(\"float32\")\r\nf32_model = get_model()\r\nf32_model.set_weights(model.get_weights())\r\n\r\n# Load model into TF lite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(f32_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\nThis strategy works for any case where a model is trained in mixed precision but a float32 model is needed for inference.\r\n\r\n> And is it planned to support mixed_float16 for TF_lite models?\r\n\r\n@abattery can you answer this? Not supporting mixed_float16 for TF lite is a significant usability issue. I think this should be done in TF lite (and any other inference backend), but an alternative would be to have a tool to convert a Keras model or SavedModel to float32.\r\n\r\n> Checkout the following minimal example which results in a segfault:\r\n\r\nThis raises an exception for me (probably was fixed recently). Better than a segfault, but we should still support this use case.\r\n", "@reedwm Thanks for reminding us of the feature request for mixed_float16 for TFLite. \r\n\r\nCCing @talumbau", "@reedwm thank you very much for the code snippet!!\r\nNow I am able to train with float 16 precision but still save a tflite model.", "@talumbau @abattery Has there been any progress on this issue?\r\n\r\nSince the deprecation of the grappler mixed precision training workflow people using the new recommended mixed precision training with Keras are not able to convert models to TFLite anymore without additional code. This is very problematic if the workflow relies on storing saved models which are expected to be reloadable without any code but unfortunately cannot be easily converted to float32 to workaround this issue.", "@abattery After a training a model with mixed precision why is not possible to save it with `float16` quantization flags enabled?\r\n\r\n```\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(eff)\r\n\r\n# set quantization, additional ops\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\n```\r\n\r\nWhat is the difference between a float16 quantized model and with mixed_precision trained model?"]}, {"number": 46365, "title": "micro_speech example: FeatureProvider tries to read audio data that is in the future", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): -\r\n- TensorFlow installed from (source or binary): source / Arduino library\r\n- Tensorflow version (commit SHA if source): -\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33\r\n\r\n**Describe the problem**\r\n\r\nThe audio capture subsystem keeps track of the latest audio capture time. This is the `time_in_ms` passed into `PopulateFeatureData()` from micro_speech/feature_provider.cc. `PopulateFeatureData` will then work backwards from this timestamp to fetch audio data for any slices that it hasn't cached yet.\r\n\r\nI believe the code should be as follows: https://github.com/hollance/tensorflow/commit/9fe6428a70ebb78fb471c5ddf8ff3d6fb7f8b14d\r\n\r\nReason: `current_step` is the index of the last slice that will be read. Since each slice is 30ms in length, we must subtract that from the timestamp, otherwise the last slice will go (partially) out of bounds and we will read stale data from the audio capture ring buffer.\r\n\r\nFor example, if the `time_in_ms` is 2010 (this the point in time *up to* which audio has been captured), the current code will set `current_step` to 2010 / 20 = 100. In the loop that computes new slices, this step value is multiplied again by 20 to create `slice_start_ms`, which becomes 2000. Then we read 30ms worth of audio starting from `slice_start_ms`. But we don't have 30ms worth of audio left in the capture buffer, only 10. The current implementation will try to read 20ms into the future, which is really old data from roughly 0.5 seconds ago.\r\n\r\nApparently making this change will cause `feature_provider_mock_test` to fail. I haven't looked into this in detail yet but I'm a little suspicious at the provided duration of 970ms. The correct duration for a 49x40 spectrogram is 990ms (or really anything between 990 and 1009ms, but those extra ms aren't used because they don't fill up a full window). I suspect the reason 970 was used here is that the sample is actually 1000ms and 30ms was (mistakenly) subtracted to work around the bug in feature_provider.cc.\r\n", "comments": []}]