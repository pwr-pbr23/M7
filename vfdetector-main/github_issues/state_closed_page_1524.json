[{"number": 7206, "title": "Add libcupti.so path to Docker.gpu", "body": "More info in #7207", "comments": ["Can one of the admins verify this patch?", "If you want to leave the comment in, I'm fine with that, but please revise the sentence to be more specific about what \"we\" refers to (i.e., TensorFlow).", "How about, \"For CUDA profiling, TensorFlow requires CUPTI.\"", "@thesuperzapper Sounds good to me.", "@tensorflow-jenkins test this please", "@caisq can you approve this?", "@rmlarsen,  @thesuperzapper said he would update the comment line. Let's wait for him on that?", "@caisq SGTM", "Done, sorry about the delay, time-zones and such. "]}, {"number": 7205, "title": "Branch 146316196", "body": "", "comments": ["Three tests failed in Python 3\r\n//tensorflow/tools/docs:doc_generator_visitor_test                       FAILED in 1.5s\r\n//tensorflow/tools/docs:generate_test                                    FAILED in 12.2s\r\n//tensorflow/tools/docs:parser_test\r\n\r\nThe root causes seem to be the same: \r\ncache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/parser_test.runfiles/org_tensorflow/tensorflow/tools/docs/parser.py\", line 619, in generate_global_index\r\n    for full_name, py_object in index.iteritems():\r\nAttributeError: 'dict' object has no attribute 'iteritems'", "Use `dict.items()` instead. `dict.iteritems()` was removed in python3", "Will submit fixes internally before pushing again.", "I should have fixed this already, let me just push again.", "@caisq  It seems that in the commit 47f4b3c, the markdown images of your links disappear! I'm reading your tfdbg docs. PLS help to fix this, Thanks", "@burness, the images have always been missing in this GitHub repo. To view the doc with working image links, please visit. https://www.tensorflow.org/versions/master/how_tos/debugger/"]}, {"number": 7204, "title": "Should use math_ops.reduce_prod() in mixture.py instead of array_ops.reduce_prod()", "body": "Hi,\r\n\r\nI tried to use mixture density distribution with batch_size **None**, and triggered this error\r\n```bash\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/mixture.pyc in _sample_n(self, n, seed)\r\n    260       else:\r\n    261         batch_shape = self.batch_shape()\r\n--> 262         batch_size = array_ops.reduce_prod(batch_shape)\r\n    263       static_event_shape = self.get_event_shape()\r\n    264       if static_event_shape.is_fully_defined():\r\n\r\nAttributeError: 'module' object has no attribute 'reduce_prod'\r\n```\r\nPlease see https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/distributions/python/ops/mixture.py#L262\r\n\r\nI believe this is a bug since `reduce_prod` is in `math_ops.py` instead of `array_ops.py`.\r\n\r\nI'm using version v0.12.1, but I checked it's the same for r1.0\r\n\r\nThanks", "comments": ["I hit this too.\r\n\r\nFor others waiting for a fix, monkey patch is a temporary workaround:\r\n\r\n```python\r\n# Workaround for https://github.com/tensorflow/tensorflow/issues/7204\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\narray_ops.reduce_prod = math_ops.reduce_prod\r\n```\r\n", "Working on the fix now.  Will push either tonight or by Tuesday.\n\nOn Wed, Feb 8, 2017 at 11:08 AM, Matt Wytock <notifications@github.com>\nwrote:\n\n> I hit this too.\n>\n> For others waiting for a fix, monkey patch is a temporary workaround:\n>\n> # Workaround for https://github.com/tensorflow/tensorflow/issues/7204from tensorflow.python.ops import array_opsfrom tensorflow.python.ops import math_ops\n> array_ops.reduce_prod = math_ops.reduce_prod\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7204#issuecomment-278429322>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzkp7cq5OSEraqxFVXuqBCRvH7fXks5rahKegaJpZM4L0py1>\n> .\n>\n", "Any update on this? @ebrevdo was this fixed?", "Yes, someone sent in a PR.  We don't have any unit tests checking this case\nsadly.\n\nOn Jun 16, 2017 11:30 AM, \"Skye Wanderman-Milne\" <notifications@github.com>\nwrote:\n\n> Any update on this? @ebrevdo <https://github.com/ebrevdo> was this fixed?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7204#issuecomment-309100630>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim17xPx9sDox4gEbbygvy2GR4tRpxks5sEsmxgaJpZM4L0py1>\n> .\n>\n"]}, {"number": 7203, "title": "Noticable lag with different Inception model", "body": "Running the demo app on Android works absolutelly great. Then I tried retraining the inception5h model (as used by the app) with no success. I retrained Inception v3 model but then image recognition would lag. I tried quantizating the model but it lagged even more. So what I'm asking is for a direction or help on the issue or a retrainable Inception5h model with some instructions.", "comments": ["For help retraining inception5h you can try asking on StackOverflow, but I imagine mainly it's the same as Inception v3 just with different layer names (found in ClassifierActivity.java).\r\n\r\nRegarding the latency, Inception v3 is a larger model so it's expected that it will run more slowly. What sort of numbers are you seeing, and on what devices?", "I get about 1 or 2 predictions per second on Huawei Mate 9.\r\n\r\nStackOverlow: http://stackoverflow.com/questions/42003846/retraining-inception5h-model-from-tensorflow-android-camera-demo", "This seems within normal ranges (hard to say without exact numbers). Closing as there is no addressable TensorFlow issue here."]}, {"number": 7202, "title": "increase size of nccl_manager_test", "body": "This test timed out when testing https://github.com/tensorflow/tensorflow/pull/7197 , increasing size small->medium", "comments": ["Can one of the admins verify this patch?"]}, {"number": 7201, "title": "Feeding to boolean placeholder -- works in TF 0.11 but not in 0.12.1", "body": "@yaroslavvb The info is attached below. I've targeted the problem to the conflict of tf.cond on boolean placeholder with the ExponentialMovingAverage's apply function.\r\n\r\nThe same code works just fine in TF 0.11\r\n\r\nEnvironment Info\r\n============================\r\nOS: Ubuntu 14.04\r\nCUDA: 8.0 with cuDNN 5.1\r\nTEnsorflow 0.12.1\r\n\r\nMinimal Example:\r\n============================\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith tf.Graph().as_default():\r\n    with tf.device('/gpu:0'):\r\n        isTrain = tf.placeholder(tf.bool, shape=())\r\n        user_input = tf.placeholder(tf.float32, shape=(2,4))\r\n\r\n        with tf.variable_scope('batchnorm') as sc:\r\n           batch_mean, batch_var = tf.nn.moments(user_input, [0,])\r\n           ema = tf.train.ExponentialMovingAverage(decay=0.9)\r\n\r\n           ema_apply_op = tf.cond(isTrain,\r\n                           lambda: ema.apply([batch_mean, batch_var]),\r\n                           lambda: tf.no_op())\r\n\r\n    init = tf.initialize_all_variables()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n    sess.run([batch_mean], feed_dict={user_input:[[1,2,3,4],[2,3,4,5]], isTrain: True})\r\n```\r\n\r\nError Message:\r\n============================\r\n```\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype bool\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```", "comments": ["Could you fill out the template?", "A related post about the problem in StackOverflow:\r\nhttp://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\r\n\r\nIt seems that the boolean placeholder is not correctly initialized. \r\nAfter using\r\n```python\r\nsess.run(init, {isTrain: True})\r\n```\r\n\r\nThe minimal example works fine.", "Can you try in a later version (ie, nightly?)\r\n\r\nI tried your example in a version I built from head last week, and it worked without error\r\n", "Closing pending more information.  If it's fixed in HEAD, we're good.", "charlesq34 - you said it works when you specify {isTrain: True}, but what if you need the condition to be dependent on the inputs?\r\nI can't feed in advance the feed_dict on the  \"initialize_all_variables\" stage [sess.run(init)], because I later on have multiple calls for sess.run().\r\n\r\nany ideas?", "@EfratSo Wouldn't feeding in True/False as per your condition in your subsequent calls make it work?"]}, {"number": 7200, "title": "Allow for data stored in an arbitrary location", "body": "If you want to store data somewhere you have more space or be able to ignore a folder in sync, change the data_root.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 7199, "title": "Incorrect computation results on /gpu:1 on dual K80 servers", "body": "We have a bunch of servers with 2 K80 cards each (2 GPUs per card, for a total of 4 GPUs per machine). We're having problems with even simple computations on `/gpu:1` returning wrong results. This happens across multiple machines. Interestingly, if we re-map the GPUs, using `CUDA_VISIBLE_DEVICES` (e.g. `CUDA_VISIBLE_DEVICES=\"3,2,1,0\"`), it's still always `/gpu:1` that has problems.\r\n\r\nTo reproduce, I wrote a short script that multiplies random matrices together on each GPU, comparing the results with Numpy:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef test(sess, device):\r\n    x = tf.placeholder(tf.float32, (500, 500))\r\n    w_np = np.random.random((500, 20)).astype(np.float32)\r\n    w = tf.Variable(w_np, dtype=tf.float32)\r\n\r\n    with tf.device(\"/\" + device):\r\n        y = tf.matmul(x, w)\r\n\r\n    x_np = np.random.random((500, 500)).astype(np.float32)\r\n    y_np = np.dot(x_np, w_np)\r\n\r\n    with sess.as_default():\r\n        sess.run([tf.initialize_all_variables()])\r\n        y_tf = sess.run([y], feed_dict={x: x_np})\r\n\r\n    assert(np.all(np.abs(y_tf - y_np) < 1e-3))\r\n\r\n\r\ndef test_all():\r\n    sess = tf.Session()\r\n    for i in range(4):\r\n        device = \"/gpu:{}\".format(i)\r\n        try:\r\n            for _ in range(10):\r\n                test(sess, device)\r\n        except AssertionError:\r\n            print \"GPU {} FAILED!\".format(i)\r\n            continue\r\n        print \"GPU {} passed\".format(i)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test_all()\r\n```\r\n\r\nAfter some initialization logging, this produces the output:\r\n\r\n```\r\nGPU 0 passed\r\nGPU 1 FAILED!\r\nGPU 2 passed\r\nGPU 3 passed\r\n```\r\n\r\nEnvironment Information (we've tried several different permutations):\r\nOS: CentOS 7.2\r\nCUDA: 8.0.44 and 7.5.17\r\nCUDNN: 5.1 and 5.0\r\nTensorflow: 0.11.0rc0, 0.11.0, 0.12.1, 1.0.0rc0\r\nNvidia drivers: 352.39, 367.48\r\n\r\nWe have another machine running Ubuntu 16.04 with 4x Titan Xs. All 4 GPUs pass this test script there.\r\n", "comments": ["What's the diff when it's wrong?", "It's way off. Just did a test. Mean absolute difference over the whole result matrix was 125. ", "On our cluster, we have a bunch of K80's as well. I just ran your example, and nothing went wrong.\r\n\r\nFor reference, I am running the docker implementation, and I had to change one line of your code, because it was throwing depreciation warnings. \r\nI changed, `tf.initialize_all_variables()` to `tf.global_variables_initializer()`\r\n\r\n![k80-test](https://cloud.githubusercontent.com/assets/5735406/22568553/144ef162-e9f9-11e6-906f-54a96cc6ac9a.PNG)\r\n", "What OS are you running? That's really the only thing we weren't able to try multiple versions of.\r\n\r\nOtherwise, can you think of any other environment information I could give that might be relevant?\r\n\r\nRe: the deprecation warning, I think that was changed from 0.11 to 0.12. The script was originally written for 0.11.", "Here is my environment: \r\n\r\n**Host:**\r\nOS: Ubuntu 16.04.1\r\nDriver: 375.20\r\nDocker: 1.13\r\nNvidia-Docker: 1.0.0\r\n\r\n**Docker:**\r\nImage: gcr.io/tensorflow/tensorflow:latest-gpu\r\nTensorflow: 0.12.1\r\nCUDA: 8.0.44\r\nCUDNN: 5.1.10\r\n\r\n\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7198, "title": "\"Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\" on Windows 10", "body": "Tensorflow was working on my new Windows 10 for about 24 hours then broke. \r\nRunning a Keras lstm_text-generator with a small test txt file, all was well.\r\nAfter running the same script with a larger 8mb txt file as input data, python encountered an error, system froze, rebooted. \r\n\r\n### Environment info\r\n\r\nWindows 10\r\nTensorflow 0.12.1\r\nPython 3.5.2\r\n\r\nPath includes both:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64\r\n(as recommended here: https://github.com/tensorflow/tensorflow/issues/6235 and https://github.com/tensorflow/tensorflow/issues/5968 )\r\n\r\n\r\n### cmd\r\n\r\n`testing>python saveModel_best_only-pima.py\r\nUsing TensorFlow backend.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:01:00.0\r\nTotal memory: 12.00GiB\r\nFree memory: 10.00GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)\r\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #0: CUDA driver version is insufficient for CUDA runtime version\r\nFailed to initialize CUDA device #1: CUDA driver version is insufficient for CUDA runtime version\r\n...`\r\n\"\r\n\r\n\r\n###  Directory of C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\r\n\r\n2017-01-31  03:37 PM    <DIR>          .\r\n2017-01-31  03:37 PM    <DIR>          ..\r\n2017-01-31  03:37 PM    <DIR>          Win32\r\n2017-01-31  05:15 PM    <DIR>          x64\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? \r\nhttp://stackoverflow.com/questions/3253257/cuda-driver-version-is-insufficient-for-cuda-runtime-version\r\n\r\n\r\n\r\n", "comments": ["Issue resolved.\r\nReinstalled all CUDA CUDNN drivers.\r\n"]}, {"number": 7197, "title": "Add microsecond timestamps to LOG messages", "body": "Fixes #2076", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thanks for the quick review! We are using this to debug some performance problems, so getting it into the nightly is helpful", "Test failures:\r\n\r\n- //tensorflow/tools/docs:generate_test\r\n- //tensorflow/tools/docs:doc_generator_visitor_test\r\n\r\n```\r\n File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/generate_test.runfiles/org_tensorflow/tensorflow/tools/docs/generate.py\", line 68, in write_docs\r\n    for full_name, py_object in index.iteritems():\r\nAttributeError: 'dict' object has no attribute 'iteritems'\r\n```\r\n\r\nThis is a Python2 -> Python3 compatibility error, the line in both of them needs to read `six.iteritems(index)`\r\n\r\n- //tensorflow/contrib/learn:estimator_test\r\n`TIMEOUT in 305.0s`\r\n\r\nThe test needs to be `size=large`\r\n\r\n- //tensorflow/tools/docs:parser_test\r\n```\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/parser_test.runfiles/org_tensorflow/tensorflow/tools/docs/parser_test.py\", line 120, in test_generate_markdown_for_class\r\n    self.assertTrue('a_method(arg=\\'default\\')' in docs)\r\nAssertionError: False is not true\r\n```\r\n\r\n...\r\n` \\_(\u30c4)_/\u00af`", "Test failures re unrelated. Merging."]}, {"number": 7196, "title": "./configure modifies build_config.bzl which is version-controlled", "body": "Configure modifies `build_config.bzl` that is tracked by version control. This causes following annoyances:\r\n\r\n1. When you do `git pull`, it'll overwrite `build_config.bzl` after which you need to do a ./configure and rebuild. I sometimes do testing of the same change on Mac and Unix host, and use `git pull` to sync them, which will overwrite my machine-specific config with default value.\r\n2. You can't use `git commit -a` because that'll include `build_config.bzl`\r\n\r\nCurrent work-around for 1. is to do following when pulling:\r\n```\r\ngit stash\r\ngit pull\r\ngit stash pop\r\n```\r\nIf you merge failed message, do\r\n```\r\ngit checkout --theirs -- tensorflow/core/platform/default/build_config.bzl\r\n```\r\nFor 2. it's possible to use `skip-worktree`\r\n\r\n@aselle what do you thinking about adding `build_config.bzl` to `.gitignore`?", "comments": ["Such trivial change may be better discussed in a PR, so closing for now"]}, {"number": 7195, "title": "Running tensorflow-gpu on windows 10", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI am trying to run to train a convolutional network on lenet dataset but whenever I open a session and start `sess.run(training_operation,` feed_dict={x: batch_x, y: batch_y})` I get the following error:\r\n\r\n> name: GeForce GTX 980\r\n> major: 5 minor: 2 memoryClockRate (GHz) 1.178\r\n> pciBusID 0000:01:00.0\r\n> Total memory: 8.00GiB\r\n> Free memory: 6.70GiB\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\n> F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:198] Unexpected Event status: 1\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n### Environment info\r\nOperating System: Windows 10 Home\r\n\r\nInstalled version of CUDA and cuDNN:  \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCUDA 8.0\r\ncuDNN 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n\r\n> pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\n> 0.12.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Can you try upgrading to the 1.0.0rc version of `tensorflow-gpu`? There were some buggy GPU kernels in the 0.12 release that caused similar failures.", "I have a similar setup, but with a gtx-1080, and was having the same problem. I did indeed get it working by using the 1.0.0rc version of tensorflow-gpu. It did require slight tweaking of syntax though, so that I had to explicitly add \"labels=\" and \"logits=\" for softmax_cross_entropy_with_logits.", "Looks like the problem is resolved in a more recent version.\r\nClosing the issue.", "@mrry \r\nCan you try upgrading to the 1.0.0rc version of tensorflow-gpu? There were some buggy GPU kernels in the 0.12 release that caused similar failures.\r\n\r\nfrom where may i install 1.0.0rc version of tensorflow-gpu. \r\n\r\n", "I assume you are asking us to update our binaries uploaded to pypi.\r\nWe will update those once we verify that rc(release candidate) versions are sufficiently stable.\r\nUntil then, please Use the URLs here to download our pre-built binaries for different operating systems:\r\nhttps://www.tensorflow.org/versions/r1.0/get_started/os_setup#pip_installation"]}, {"number": 7194, "title": "Branch 146269198", "body": "Push internal changes.", "comments": ["Jenkins, test this please.", "@gunan I talked to @martinwicke and @jhseu. I'll force push this and their respective fixes for py3 docgen and win saver_test will go in separately."]}, {"number": 7193, "title": "ImportError: No module named 'tensorflow.python' Mac GPU", "body": "Running 2 Jupyter kernels reflecting conda envs:\r\n- tensorflow CPU\r\n- tensorflow GPU\r\nBoth are configured with 0.12.1 for Mac.\r\n\r\nCPU VERSION (tensorflow 0.12.1) works well as expected: \r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"OUT: tensorflow imported\")\r\n```\r\nOUT: tensorflow imported\r\n\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\ndef get_available_CPU_GPU():\r\n    devices = device_lib.list_local_devices()\r\n    #return [x.name for x in devices if x.device_type == 'CPU']\r\n    return [x.name for x in devices ]\r\nprint(get_available_CPU_GPU())\r\n```\r\n\r\n['/cpu:0']  - as expected only CPU shows\r\n\r\nSAME CODE GPU VERSION (tensorflow-gpu 0.12.1 ):\r\n\r\nOUT: tensorflow imported\r\n\r\nSo **it sees the TensorFlow correctly, but then only a part of it**.\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n in ()\r\n----> 1 **from tensorflow.python.client import device_lib**\r\n      2 \r\n      3 def get_available_CPU_GPU():\r\n      4     devices = device_lib.list_local_devices()\r\n      5     #return [x.name for x in devices if x.device_type == 'CPU']\r\n\r\nImportError: No module named 'tensorflow.python'\r\n```\r\n```\r\nfeatures = tf.placeholder(tf.float32, (None, 32, 32, 3))\r\nAttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n```\r\n\r\n\r\n\r\n", "comments": ["I cannot reproduce the exact problem on my mac-gpu machine.\r\nCould you try reinstalling, maybe it is a problem with some files being copied during installation?\r\n", "@UkiDLucas is it possible uu installed using `conda install`?\r\nThat is not an officially supported way to install. I recommend you retry installing with `pip install` even after using `conda` to create the environment.\r\n\r\nAs I also cannot reproduce the problem, can did not receive a response for 10 days now, I am closing this issue."]}, {"number": 7192, "title": "Java: Remind readers to run \"configure\" to build", "body": "Fixes #7169", "comments": []}, {"number": 7191, "title": "[WIP] Add standlone Poplar plugin scaffolding", "body": "This pull request replaces https://github.com/tensorflow/tensorflow/pull/7163\r\n\r\nSummary:\r\n1) Add poplar and popnn third_party library support\r\n2) Add a poplar XLA device, plugin python wrapper, and appropriate test\r\n3) Adding copyright messages, and 'poplar configured' function\r\n4) Adding platform, compiler and executable stubs for IPU/Poplar\r\n5) Add XLA symbols to the external symbols in tensorflow main dynamic library\r\n6) Add a transfer manager for Poplar", "comments": ["Can one of the admins verify this patch?", "Replying to the comment by @vrv at the end of the closed pull request:\r\n\r\ni think it would be great if the poplar plugin could be entirely separate from the main repo.  I think that what is required is some exporting of XLA headers, much like what is done for plugin user ops.\r\n\r\nIt would certainly please my lords and masters if we could place the whole thing in a different repo.  As it stands, I think that the poplar stuff needs to be in the bazel build system, because there isn't really a pretty way of getting at all of the headers for a compilation otherwise.\r\n\r\nIt would be excellent, though, if I were able to download a 'TF compiled with XLA' binary from the web, and it came with exported headers for all necessary XLA interfaces.  Or even if I were able to build one locally, and then compile my driver against XLA headers installed in the python site lib.\r\n\r\n", "@DavidNorman Take a look at this rule: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L22   -- it is used by the pip packager to copy over all relevant headers needed to build user ops into the pip-installed directories (Ideally, we'd be able to provide a development header library that doesn't involve python, but pip works in the short term).\r\n\r\nIt is entirely probable that the transitive headers required for XLA are not packaged properly -- one thing that might be worth doing is looking at https://www.tensorflow.org/how_tos/adding_an_op/#with_tensorflow_binary_installation to figure out how you would compile user ops, but doing the same for the XLA device -- for every header that is missing, we would add it to that 'included_headers' rule somehow, so you can simply use gcc or clang with the appropriate includes to build a custom device shared object.  If that solves your bazel dependency problem, let us know!\r\n\r\n@hawkinsp @eliben  I don't know know too much about what xla headers need to be public -- is it possible for you to work out what should be exported for people who want to develop a custom device in XLA, so we can add it to the appropriate header libraries?", "i think that if the pip installation contained the appropriate XLA external interface headers then would be great.  in the longer term, like you say, a non-pip based export would be good.  You don't provide a binary XLA w/o GPU download at the moment.  Is there an intention to provide this in the longer term?\r\n\r\nmy only concern then would be that I really like your XLA unit tests.  It would be a shame to be unable to use them to test the plugin.\r\n", "Yeah, putting all XLA dev headers in pip or non-pip is a great goal.  Not sure about the binary XLA, maybe the others can speak to that.\r\n\r\nAlso, it indeed would be nice to have an \"XLA test validation suite\" that was built in a way that you didn't need to have a custom device specified in our code, so feature request noted :)", "The binary thing isn't too much trouble anyway. We can always build it locally. \r\n\r\nToday I realised that my cc_library target is generating a .so file without any dependent .so files in it (as expected now I think about it). \r\n\r\nThis is quite inconvenient, and is another reason why having an XLA external headers target is important. \r\n\r\nI can work around it with another genrule that calls a utility to insert the relevant dependencies. \r\n\r\nI'll start an OOB conversation about a couple of other queries I have. \r\n\r\n", "I believe progress is being made out-of-band, so marking as 'stalled' for now.", "Indeed - i don't believe that this merge request should ever actually be merged.  Apart from cherry picking any little bits in the core code base.\r\n\r\nIncidentally, I discovered that `*protobuf*` should probably be added to the exports file, along with `*xla*`\r\n\r\n", "Okay thanks @DavidNorman for sending this -- I think we just need to work a bit more on XLA packaging of headers and we should be good -- feel free to send us PRs for what you think needs to be exposed if we're a bit overwhelmed to get it to it immediately."]}, {"number": 7190, "title": "Branch 146241206", "body": "Push internal changes.", "comments": ["@martinwicke looks like the tests you added are broken.\r\nIs there a fix ready?", "@gunan Martin submitted a fix. I'll simply do  a new push."]}, {"number": 7189, "title": "Typo.", "body": "", "comments": ["Thanks for fixing."]}, {"number": 7188, "title": "Unexpected behavior after updating a placeholder", "body": "The output of the following snippet\r\n```\r\nph = tf.placeholder(tf.int32)\r\nph += 1000\r\na = ph + 0\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\nprint(sess.run([a], {ph: 5}))\r\n```\r\nis `[5]`, and not `[1005]` as could be expected.\r\nPerhaps it's because placeholders can't be evaluated, and therefore TF ignores the update op of `ph`?", "comments": ["you overwrote your original placeholder in `ph+=1000` line so you are not feeding what you think you are feeding. PS we are trying to keep questions like this on stackoverflow and this list for bugs in TensorFlow itself"]}, {"number": 7187, "title": "hclhkbu dlbench shows Tensorflow is slower than other frameworks", "body": "Based on https://github.com/tensorflow/tensorflow/issues/7065#issuecomment-276648478\r\n\r\nRecent update of [Benchmarking State-of-the-Art Deep Learning Software Tools](https://arxiv.org/abs/1608.07249) (by @shyhuai @FreemanX @xiaowec , if I got it right) shows some performance issues. For example, (see table 7) `AlexNet-R` is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences. \r\n\r\nIn addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.\r\n\r\nVersion used was TensorFlow 0.11 (commit [47dd089](https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25)) with CUDA 8.0 and cuDNN 5.1\r\n\r\n cc @yaroslavvb @annarev \r\n\r\n", "comments": ["PS, the code is at https://github.com/hclhkbu/dlbench/, and the authors are quite prompt at updating it. I found an issue with CIFAR multi-GPU benchmark and it was merged within a day -- https://github.com/hclhkbu/dlbench/pull/4", "assigning to @annarev for further triage", "Hi Randl,\r\n\r\nThank you for the information.  We are working through the benchmark and days away from publishing a performance guide.  There are a couple things that I noticed at a glance with the code they are using.  Before I mention them I want to stress that this is code that was published in the TensorFlow repo and I am not shifting blame.  Now some things to look for:\r\n\r\n- Loading data with feed_dict as such:  `sess.run([train_op, average_op], feed_dict=feed_dict).` \r\nThis is almost the slowest possible approach and is often used in examples.\r\n- Allowing the preprocessing (of say images) to end up on the GPU.  This happens if it is not placed on the CPU.  This can result in 6x+ increased performance.\r\n\r\nThere are other little tweaks but with just the two \"tricks\" above I suspect a few of the benchmarks you listed would improve dramatically.  There are other tweaks but using TF 1.0+ and the above techniques would help a lot.  \r\n\r\nI will leave this open until I can post some numbers and possibly get someone to post a PR to the benchmark project.  \r\n\r\nThank you again for following up and opening a new issue.\r\n\r\n\r\n", "@tfboyd great to know you guys are looking at it! I think their multi-GPU resnet example is especially intriguing. It's basically fork of official cifar multi-GPU example which I think was originally made by @shlens and doesn't have obvious problems like feed_dict, yet there's a large performance gap with competitors.", "How much performance hit is there in that tf stores its  filter weights differently from what gets passed to Cudnn, requiring a dimension reordering on calls to fprop and bprop? ", "It should be a \"pretty big\" hit, GPU things should should use NCHW because of CuDNN -- https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/performance/performance_guide.md", "@yaroslavvb To be clear I am referring to how the filter weights are stored, not how the input or output data is represented. I believe that for weights cuDNN supports only one format and that TF must do a live conversion.", "@cancan101 I'm not aware of any transposes done on filter weights between TF and CuDNN. Because filters are learned, it's fine to have a layout mismatch between TF and CuDNN, the training will adapt to the wrong layout.", "What about: https://github.com/tensorflow/tensorflow/blob/0be81439c91e297b078152dd0c266471b24bde7f/tensorflow/core/kernels/conv_ops.cc#L610-L612\r\n\r\nSee also https://github.com/tensorflow/tensorflow/blob/33e88f6d7b69f796d4fad452394237448f62b976/tensorflow/core/kernels/conv_ops_gpu_3.cu.cc#L335-L338:\r\n>// A GPU helper function that converts TensorFlow filter format to Cudnn filter\r\n// format.\r\n", "Nice find .... cc @zheng-xq our GPU perf expert", "@yaroslavvb Also even if you can just treat the dimensions of the filter weight matrix as opaque during training, inference may happen on another device, etc that isn't GPU based so the weights must be converted somewhere. \r\n\r\nThe [docs for TF](https://www.tensorflow.org/extend/tool_developers/#weight_formats) state: \r\n>The ordering of convolution weight values is often tricky to deal with when converting between different frameworks. In TensorFlow, the filter weights for the Conv2D operation are stored on the second input, and are expected to be in the order [filter_height, filter_width, input_depth, output_depth], where filter_count increasing by one means moving to an adjacent value in memory.\r\n\r\nthat being said, pending answers to the perf question, I am thinking of opening a feature request to add a `weight_format` along the lines of `data_format` to Conv2D. This will allow various kernel implementers to store weights more efficiently (and mark them as such for down the road conversions).\r\n\r\nIn addition to cuDNN, BNNS (https://github.com/tensorflow/tensorflow/issues/3001) also uses that same format.", "Jumping onto this thread because @zheng-xq and I tuned the perf in the past... As long as you use NCHW order, cudnn speed should basically be good. Another thing you might want to check is the workspace size: usually if you pass in a workspace size that is above 128MB, cudnn can choose a faster path. If you pass in something under 16, speed may be affected because winograd (usually the fastest one) usually need 64 or so.\r\n\r\nThe benchmark is a bit unfair to TensorFlow (and Caffe, fwiw), mainly because one will need to choose exhaustive search with a big enough workspace in cudnn, and that is sometimes overlooked by benchmarkers. For example, TensorFlow uses cudnnGet* instead of cudnnFind*:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1844\r\n\r\nwhich is a fine approach in most cases with maybe 5-10% perf difference in corner cases.", "@Yangqing what are your thoughts on https://github.com/tensorflow/tensorflow/issues/8287, namely that regardless of the input dimension ordering, TF much convert the _weights_ on each forward / backward pass.", "Nowadays TensorFlow does autotuning by itself. For example:\n\nhttps://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/core/kernels/conv_ops.cc#L657\n\nFor most conv operations, filters transformation is a small overhead\ncompared to the actual computation. However, this might not be true in\nextreme cases. So yes, eventually filter_format support might be needed\nsomewhere down the road.\n\n\nOn Wed, Mar 29, 2017 at 1:10 PM, Alex Rothberg <notifications@github.com>\nwrote:\n\n> @Yangqing <https://github.com/Yangqing> what are your thoughts on #8287\n> <https://github.com/tensorflow/tensorflow/issues/8287>, namely that\n> regardless of the input dimension ordering, TF much convert the *weights*\n> on each forward / backward pass.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-290210453>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APAgThpyAewgYIZy60TcGL2Brs6qLRUcks5rqrrTgaJpZM4Lz7Z1>\n> .\n>\n", "@cancan101 I haven't benchmarked but I believe that the transform overhead is minimal. cudnnTransformTensor should be having a very high throughput. I like #8287 though, especially in inference time when you have a small batch (and computation becomes much quicker). For training the overhead is negligible like @zheng-xq said.", "Obvious to me that they choose to force all through tensors and still didn't understand that each \"class\" of will have a different assert needed. The example given was only one of many possibilities. If training tensor example, or if input buffer there should be a different method more appropriate for the \"class\" being used", "@Yangqing What exactly is the \"workspace size\" that you are referring to, and what is it defaulted to? I see some stuff [here](https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1832-L1833), but don't really see it documented. Further how often / when is the optimal algorithm chosen?  Is `TF_CUDNN_WORKSPACE_LIMIT_IN_MB` what you are talking about? The default is 4GB which seems >> size you listed.\r\n\r\nMight be cool to offer some sort of verbose logging about the tuning options along the lines of [torch7](https://github.com/szagoruyko/cudnn.torch/blob/master/README.md#modes).\r\n\r\nAlso why does TensorFlow use cudnnGet* instead of cudnnFind* (I would think specifically the `cudnnFind...Ex` variety)?", "I have not had a chance to test the latest nightly yet. So previous benchmark performance can't be relative. ", "Relavent", "So back to the initial issue, any updates on the performance differences observed on (single) core in TF vs (py)Torch? ", "Benchmark results (TensorFlow not comparison to other platforms) and code should be posted late next week, which will help get us all on the same set of scripts.  I have done some PyTorch testing with real data on image-classification models and did not see a significant difference in my very informal testing.  I found TensorFlow to be slightly faster with real data, but am not a PyTorch expert.  I did notice that the VGG16 model loss goes to NaN after a few steps, which then results in a speedup but I was not sure if I did something wrong or if it was the model.  Beyond that PyTorch had some issues with their input pipeline with models that process images very quickly, e.g. something like Alexnet with batch size of 512, but that did not seem like a huge deal.  If there is something specific let me know, I did not see pytorch referenced in this thread and only recall the VGG16 thread.  If it it looks like it would lead to a TensorFlow speedup then it is a worth while investigation for the community.  \r\n", "I was playing around earlier today with my implementation of Wide ResNets in TF (using NCHW and fused batch norm, with `feed_dict` but verifying that `nvidia-smi` showed near 100% GPU usage throughout) and comparing to a PyTorch implementation (https://github.com/xternalz/WideResNet-pytorch).\r\n\r\nI saw batches taking something like 55% as long in the PyTorch version as in my TF version \u2013 ~155 ms for the PyTorch impl v ~285 ms in my TF impl on a p2.xlarge on AWS, using WRN-16-4. I'll post my code.", "@taion That would be great.  If all goes well I would like to start focusing on other types of models in about a week.  ", "I've described the situation in https://github.com/tensorflow/tensorflow/issues/9322.", "I am marking this closed.  We may add CIFAR-10 to the performance script but for training ImageNet the numbers between TensorFlow and other platforms are very similar.  The benchmarks cover K80s on Amazon and Google as well as P100s via the DGX-1. ", "@tfboyd I'm still a bit worried about this strange AlexNet-R (AlexNet on CIFAR-10) behavior. The fact that GTX 980 is faster GTX 1080 (and overall low speed) supposes some bug in either TF or implementation of network. Will appreciate if you take a look one more time. ", "@Randl   I have not tested it but if they wrapped this in a with device CPU I suspect it would make a huge difference based on helping a few people recently.  \r\n\r\nimages, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)\r\n\r\nIt would make a big difference in all of the tests that used CIFAR that did not place the input pipeline on the CPU.  The AlexNet numbers on the benchmarks we published looked good.  Wrapping the input pipeline in the with device CPU has resulted in huge (as in 4x+) performance increases for people.  And on the positive side if you use Estimators it does this for you by default.  You give it the input function and it makes sure it is placed on the CPU.  Since they just modified the CIFAR example, which did not wrap the input pipeline in the CPU (our bad for sure), I am pretty confident of the performance boost.  On another issue someone doubted the boost and was shocked they got I think 4x or more increase in images per second.  \r\n", "Still sounds unlikely to have lower speed on 1080 compared to 980", "@Randl where are the AlexNet 980/1080 numbers? Note that AlexNet has become \"the ugly stepchild\" of model benchmarking since it has low compute to data-transfer ratio so it's hard to get high utilization on newer hardware", "It is a one line change.  It just takes time to figure out how to run their test so I can verify it works as expected.\r\n\r\nUntil that is fixed, guessing why one GPU is slower than another is guessing.  Hopefully I sort it out in the next 48 hours or so.  There may be other minor issues but I am not concerned if their model is correct or everything is optimal.  Just like everyone else in the community, I want to make sure they have the basics so they can maximize whatever they are doing.", "Tensorflow:\r\n980 - 0.227\r\n1080 - 0.317\r\nK80 - 0.385\r\n\r\nAt the same time for Caffe:\r\n980 - 0.041\r\n1080 - 0.027\r\nK80 - 0.137\r\n\r\n", "@Randl \r\none line change to move input processing to CPU.  This would also impact the resnet test if I was reading it correctly.  I have no idea if the AlexNet implementations are the same between the platforms or if anything else is different.  My point is make sure to put your input processing on CPU it makes a difference.  Using your number I reduced the time by 83%.  On my testing I was getting 0.144 per step (I love the GTX 1080 but there are a lot of variations and the last dlbench was on TF .11 which was a long time ago), which is a reduction of 63%.  Put another way I went from 7K images/sec to 19K images per second in my apples-to-apples test (my numbers before and after) for a speedup of 2.7x.    \r\n\r\nIt is possible your numbers are from a different batch size than the default 1024.\r\n\r\n```python\r\n      with tf.device('/cpu:0'):\r\n        images, labels = cifar10_input.inputs(False, FLAGS.data_dir, FLAGS.batch_size)\r\n```\r\n\r\n```bash\r\npython alexnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin\r\n2017-05-11 06:51:40.045511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-11 06:51:40.045815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.91GiB\r\nFree memory: 7.34GiB\r\n2017-05-11 06:51:40.045828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 \r\n2017-05-11 06:51:40.045832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y \r\n2017-05-11 06:51:40.045843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nmin_queue_examples:  20000\r\n('Images: ', <tf.Tensor 'shuffle_batch:0' shape=(1024, 32, 32, 3) dtype=float32>)\r\n('padded_input: ', <tf.Tensor 'Pad_1:0' shape=(1024, 36, 36, 3) dtype=float32>)\r\n('padded_input: ', <tf.Tensor 'Pad_2:0' shape=(1024, 19, 19, 32) dtype=float32>)\r\n('padded_input: ', <tf.Tensor 'Pad_3:0' shape=(1024, 11, 11, 32) dtype=float32>)\r\n('pool3: ', <tf.Tensor 'pool3:0' shape=(1024, 3, 3, 64) dtype=float32>)\r\n2017-05-11 06:51:41.810110: step 0, loss = 2.30 (736.7 examples/sec; 1.390 sec/batch)\r\nepoch: 1, loss: 2.35\r\n2017-05-11 06:51:44.403091: step 50, loss = 2.30 (19262.2 examples/sec; 0.053 sec/batch)\r\nepoch: 2, loss: 2.29\r\n2017-05-11 06:51:47.085227: step 100, loss = 2.28 (19319.7 examples/sec; 0.053 sec/batch)\r\nepoch: 3, loss: 2.27\r\n2017-05-11 06:51:49.763903: step 150, loss = 2.25 (19135.9 examples/sec; 0.054 sec/batch)\r\nepoch: 4, loss: 2.27\r\n2017-05-11 06:51:52.439839: step 200, loss = 2.30 (19603.0 examples/sec; 0.052 sec/batch)\r\nepoch: 5, loss: 2.30\r\n2017-05-11 06:51:55.103668: step 250, loss = 2.30 (19260.7 examples/sec; 0.053 sec/batch)\r\n\r\n```", "I am also CPU bound.  I am using an old skool cool 2500K clocked to 4.2Ghz or so and it was maxed.  No idea if that will make a real world difference but just FYI.", "ResNet (I assume 50) was not a dramatic as I saw ~170ms and then after adding the with CPU and the WINOGRAD NONFUSED flag I was getting 150ms or so.  Again I have no idea if everyone is using the same ResNet.  When NVIDIA does these tests they check the FLOPS used to make sure they are similar between the platforms.  Making sure the models are exact seems painful.  Again my goal is that people doing research and work are following best practices and getting decent performance.  I bet there are a couple (not much more than that) more tweaks I can make to the code and I am pretty novice.  \r\n\r\n```bash\r\ntoby@tfbelwar:~/hongkong_bench/dlbench/tools/tensorflow/cnn/resnet_tf_10$ TF_ENABLE_WINOGRAD_NONFUSED=1 python resnet_cifar10.py --data_dir=/home/toby/CIFAR-10/cifar-10-batches-bin\r\n2017-05-11 07:26:26.859794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-11 07:26:26.860125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:907] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.91GiB\r\nFree memory: 7.26GiB\r\n2017-05-11 07:26:26.860140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:928] DMA: 0 \r\n2017-05-11 07:26:26.860144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] 0:   Y \r\n2017-05-11 07:26:26.860155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:997] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\r\nmin_queue_examples:  20000\r\n('Images: ', <tf.Tensor 'shuffle_batch:0' shape=(128, 32, 32, 3) dtype=float32>)\r\n('shortcut: ', <tf.Tensor 'scale1/block9/Relu:0' shape=(128, 32, 32, 16) dtype=float32>)\r\n('x: ', <tf.Tensor 'scale2/block1/B/batchnorm/add_1:0' shape=(128, 16, 16, 32) dtype=float32>)\r\n('stride: ', 2)\r\n('shortcut: ', <tf.Tensor 'scale2/block1/shortcut/avg_pool:0' shape=(128, 16, 16, 16) dtype=float32>)\r\n('shortcut: ', <tf.Tensor 'scale2/block1/shortcut/Pad:0' shape=(128, 16, 16, 32) dtype=float32>)\r\n('shortcut: ', <tf.Tensor 'scale2/block9/Relu:0' shape=(128, 16, 16, 32) dtype=float32>)\r\n('x: ', <tf.Tensor 'scale3/block1/B/batchnorm/add_1:0' shape=(128, 8, 8, 64) dtype=float32>)\r\n('stride: ', 2)\r\n('shortcut: ', <tf.Tensor 'scale3/block1/shortcut/avg_pool:0' shape=(128, 8, 8, 32) dtype=float32>)\r\n('shortcut: ', <tf.Tensor 'scale3/block1/shortcut/Pad:0' shape=(128, 8, 8, 64) dtype=float32>)\r\n2017-05-11 07:26:42.052644: step 0, loss = 4.66 (51.4 examples/sec; 2.490 sec/batch)\r\n2017-05-11 07:26:56.862701: step 100, loss = 1.65 (855.0 examples/sec; 0.150 sec/batch)\r\n2017-05-11 07:27:11.718538: step 200, loss = 1.40 (853.8 examples/sec; 0.150 sec/batch)\r\n2017-05-11 07:27:26.637125: step 300, loss = 1.51 (883.9 examples/sec; 0.145 sec/batch)\r\n2017-05-11 07:27:41.486478: step 400, loss = 1.34 (879.2 examples/sec; 0.146 sec/batch)\r\n\r\n```\r\nI was not CPU maxed but there might be a small gain with a better CPU.  My GTX 1080 might also be clocked higher, I did not read their settings.  I also use my GTX as my main display but I doubt that matters in this case.  \r\n\r\n\r\n", "My GTX 1080 is clocked slightly higher.  I happened to check their logs, which is nice that they include.  The gains are still solid for AlexNet.  My ResNet number is similar to their number for v7 testing.  To be honest I am kind of surprised because I get a much lower number on my faster clocked GTX before I fix the input pipeline problem.  Oh well, interesting but not interesting enough to install TF 0.11.0.  :-)", "I changed their ResNet to use Fused and it is now down to ~105ms which is aligned with the other platforms.  I also checked the accuracy at 42 epochs (that seems to be what they are doing) and it is ~79% which seems to line up with the previous results.  I am pretty sure they are using NHWC so there might be a small gain moving to NCHW but given the step times are close to the other platforms I am not sure I would mess with it for CIFAR-10.  I think this is extra closed at this point.  I am trying to find time to send a PR to the HK team.  "]}, {"number": 7186, "title": "missing fclose", "body": "\\contrib\\pi_examples\\label_image\\label_image.cc line 105\r\n\r\nmissing fclose(infile);\r\n\r\n", "comments": ["would you like to send a PR to fix this?", "BTW, PR for such small change is quite easy and you can do it mostly from browser:\r\n\r\nFirst, click \"Fork\" on github tensorflow page \r\n\r\nThen in Terminal:\r\n```\r\ngit remote add mine https://github.com/<username>/tensorflow\r\ngit remote add tfmain https://github.com/tensorflow/tensorflow.git\r\ngit fetch tfmain\r\ngit checkout tfmain/master -b bugfix\r\ngit push --set_upstream mine\r\n\r\n```\r\nThen in bugfix branch on <username>/tensorflow on Github, find file, click \"Edit\" button, edit file, then click commit, then click \"Pull Requests\" and select `tensorflow/master` on left and `<username>/tensorflow:bugfix` on right", "@yaroslavvb Looks like this issue has been resolved."]}, {"number": 7185, "title": "Float support for scatter_nd", "body": "Hi,\r\nIt seems the updates input to scatter_nd is limited to int32 and int64 input: [https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#scatter_nd](https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#scatter_nd)\r\n\r\nIt'd be great to have float32 and float64 support. Is this possible?\r\n\r\nThanks!", "comments": ["This also works\r\n\r\n```\r\nindices = tf.constant([[4], [3], [1], [7]])\r\nupdates = tf.constant([9., 10., 11., 12.], dtype=tf.float32)\r\nshape = tf.constant([8])\r\nscatter = tf.scatter_nd(indices, updates, shape)\r\nsess = tf.Session()\r\nprint(sess.run(scatter))\r\n```", "Thanks @yaroslavvb. I guess I misread the API. It works for me too. "]}, {"number": 7184, "title": "1", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": []}, {"number": 7183, "title": "If using multiple parameter servers, how to divide model parameters on these PSs? ", "body": "If using multiple parameter servers, how to divide model parameters on these PSs?  each PS stores an equal number of parameters and updates them?", "comments": ["Hi could you post this on stackoverflow with tag tensorflow please? The issues are meant for bugs/features in TensorFlow itself\r\n"]}, {"number": 7182, "title": "Fix possible flake sources in saver_test and supervisor_test.", "body": "", "comments": ["Test failures are unrelated. Merging."]}, {"number": 7181, "title": "can't open tensorboard while training ", "body": "(You can navigate to http://127.0.1.1:6006)\r\n * Running on http://0.0.0.0:6006/ (Press CTRL+C to quit)\r\n127.0.0.1 - - [31/Jan/2017 23:25:27] \"GET /neon-animation/neon-animation-behavior.html HTTP/1.1\" 200 -\r\n127.0.0.1 - - [31/Jan/2017 23:25:27] \"GET /neon-animation/web-animations.html HTTP/1.1\" 200 -\r\n127.0.0.1 - - [31/Jan/2017 23:25:27] \"GET /iron-overlay-behavior/iron-overlay-backdrop.html HTTP/1.1\" 200 -\r\n127.0.0.1 - - [31/Jan/2017 23:25:27] \"GET /web-animations-js/web-animations-next-lite.min.js HTTP/1.1\" 200 -\r\n127.0.0.1 - - [31/Jan/2017 23:25:27] \"GET /paper-input/paper-input.html HTTP/1.1\" 200 -\r\n127.0.0.1 - - [31/Jan/2017 23:25:27] \"GET /paper-slider/paper-slider.html HTTP/1.1\" 200 -\r\n*** Error in `/home/menglin/anaconda2/envs/tensorflow/bin/python': double free or corruption (!prev): 0x00000000009f6a80 ***\r\nAborted (core dumped)\r\n", "comments": ["version is '0.12.head'", "@dsmilkov could you please take a look at this issue?\r\nI guess the reason is that I used the wrong version of bazel\r\nBuild label: 0.4.2\r\n", "If you already have a suspect of the reason, can you try to fix it and come back please? Thanks.", "Thanks for your advise. I may do that later. I don't know anything about bazel and it takes time to compile Tensorflow, so I'm not sure I should use a newer version or older version.  I expect someone who knows the library to help me. I'm pretty busy utils this Wednesday , I may try to fix it afterward.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7180, "title": "Bump up size for estimator_test", "body": "Avoid estimator_test timeout as seen in other PR builds, such as https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3437/console", "comments": ["@martinwicke This change will disable this test in some of our CI builds.\r\nI think the change is warranted right now. But we will eventually need to resolve it.", "@gunan @martinwicke @terrytangyuan estimator_test is currently not sharded. Is it possible to shard it, like `shard_count = 4`?", "@caisq Good idea. I've changed the commit to add shard count instead, assuming all tests are independent.", "@gunan Just for my curiosity. Large tests are disabled internally? Under what environments/devices?", "@tensorflow-jenkins test this please", "@terrytangyuan You are right. The test failures may be due to a BUILD file format requirement. Can you move the `shard_count` line after `srcs` and before `srcs_version`.", "No, please do not use sharding, they can cause even worse complications\nwith testinfra. Id rather we just disable this test.\n\nLarge tests have 15 minute timeouts, and build phase added on top of that\ntakes them out of our goals for presubmit time limits. Therefore we\nconciously disable large tests in our testinfra.\n\nOn Feb 1, 2017 10:44 AM, \"Shanqing Cai\" <notifications@github.com> wrote:\n\n> @terrytangyuan <https://github.com/terrytangyuan> You are right. The test\n> failures may be due to a BUILD file format requirement. Can you move the\n> shard_count line after srcs and before srcs_version.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7180#issuecomment-276743212>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOcVdKp55mFcvCyUKpIn6doJhFc49ks5rYNJ5gaJpZM4Lzd3N>\n> .\n>\n", "Okay...should I change this to manual? Let me know what's the correct way\nto do it. I think it's better to document the best practices somewhere to\navoid this back and forth conversation.\n\nOn Wed, Feb 1, 2017 at 12:50 PM, gunan <notifications@github.com> wrote:\n\n> No, please do not use sharding, they can cause even worse complications\n> with testinfra. Id rather we just disable this test.\n>\n> Large tests have 15 minute timeouts, and build phase added on top of that\n> takes them out of our goals for presubmit time limits. Therefore we\n> conciously disable large tests in our testinfra.\n>\n> On Feb 1, 2017 10:44 AM, \"Shanqing Cai\" <notifications@github.com> wrote:\n>\n> > @terrytangyuan <https://github.com/terrytangyuan> You are right. The\n> test\n> > failures may be due to a BUILD file format requirement. Can you move the\n> > shard_count line after srcs and before srcs_version.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/pull/\n> 7180#issuecomment-276743212>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AHlCOcVdKp55mFcvCyUKpIn6doJhFc49ks5rYNJ5gaJpZM4Lzd3N>\n>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7180#issuecomment-276745075>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEEnSor6B6cNjoL8NAyV1XJMpU-lncNuks5rYNQRgaJpZM4Lzd3N>\n> .\n>\n", "The best thing to do would be to split this test into more manageable\npieces, or even better, make it faster (less iterations, etc., if such a\nthing is still possible).\n\nOn Wed, Feb 1, 2017 at 10:56 AM, Yuan (Terry) Tang <notifications@github.com\n> wrote:\n\n> Okay...should I change this to manual? Let me know what's the correct way\n> to do it. I think it's better to document the best practices somewhere to\n> avoid this back and forth conversation.\n>\n> On Wed, Feb 1, 2017 at 12:50 PM, gunan <notifications@github.com> wrote:\n>\n> > No, please do not use sharding, they can cause even worse complications\n> > with testinfra. Id rather we just disable this test.\n> >\n> > Large tests have 15 minute timeouts, and build phase added on top of that\n> > takes them out of our goals for presubmit time limits. Therefore we\n> > conciously disable large tests in our testinfra.\n> >\n> > On Feb 1, 2017 10:44 AM, \"Shanqing Cai\" <notifications@github.com>\n> wrote:\n> >\n> > > @terrytangyuan <https://github.com/terrytangyuan> You are right. The\n> > test\n> > > failures may be due to a BUILD file format requirement. Can you move\n> the\n> > > shard_count line after srcs and before srcs_version.\n> > >\n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly, view it on GitHub\n> > > <https://github.com/tensorflow/tensorflow/pull/\n> > 7180#issuecomment-276743212>,\n> > > or mute the thread\n> > > <https://github.com/notifications/unsubscribe-auth/\n> > AHlCOcVdKp55mFcvCyUKpIn6doJhFc49ks5rYNJ5gaJpZM4Lzd3N>\n> >\n> > > .\n> > >\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/pull/\n> 7180#issuecomment-276745075>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AEEnSor6B6cNjoL8NAyV1XJMpU-lncNuks5rYNQRgaJpZM4Lzd3N>\n>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7180#issuecomment-276745954>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_TYMocotuTUixZB3Ejb4x9Xpv4l0ks5rYNVfgaJpZM4Lzd3N>\n> .\n>\n", "Id prefer either Martin's suggestion, or disabling the test. I am OK with\neither right now, as this is becoming a blocker for many contributions.\n\nOn Wed, Feb 1, 2017 at 11:00 AM, Martin Wicke <notifications@github.com>\nwrote:\n\n> The best thing to do would be to split this test into more manageable\n> pieces, or even better, make it faster (less iterations, etc., if such a\n> thing is still possible).\n>\n>\n> On Wed, Feb 1, 2017 at 10:56 AM, Yuan (Terry) Tang <\n> notifications@github.com\n> > wrote:\n>\n> > Okay...should I change this to manual? Let me know what's the correct way\n> > to do it. I think it's better to document the best practices somewhere to\n> > avoid this back and forth conversation.\n> >\n> > On Wed, Feb 1, 2017 at 12:50 PM, gunan <notifications@github.com> wrote:\n> >\n> > > No, please do not use sharding, they can cause even worse complications\n> > > with testinfra. Id rather we just disable this test.\n> > >\n> > > Large tests have 15 minute timeouts, and build phase added on top of\n> that\n> > > takes them out of our goals for presubmit time limits. Therefore we\n> > > conciously disable large tests in our testinfra.\n> > >\n> > > On Feb 1, 2017 10:44 AM, \"Shanqing Cai\" <notifications@github.com>\n> > wrote:\n> > >\n> > > > @terrytangyuan <https://github.com/terrytangyuan> You are right. The\n> > > test\n> > > > failures may be due to a BUILD file format requirement. Can you move\n> > the\n> > > > shard_count line after srcs and before srcs_version.\n> > > >\n> > > > \u2014\n> > > > You are receiving this because you were mentioned.\n> > > > Reply to this email directly, view it on GitHub\n> > > > <https://github.com/tensorflow/tensorflow/pull/\n> > > 7180#issuecomment-276743212>,\n> > > > or mute the thread\n> > > > <https://github.com/notifications/unsubscribe-auth/\n> > > AHlCOcVdKp55mFcvCyUKpIn6doJhFc49ks5rYNJ5gaJpZM4Lzd3N>\n> > >\n> > > > .\n> > > >\n> > >\n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly, view it on GitHub\n> > > <https://github.com/tensorflow/tensorflow/pull/\n> > 7180#issuecomment-276745075>,\n> > > or mute the thread\n> > > <https://github.com/notifications/unsubscribe-auth/\n> > AEEnSor6B6cNjoL8NAyV1XJMpU-lncNuks5rYNQRgaJpZM4Lzd3N>\n> >\n> > > .\n> > >\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/pull/\n> 7180#issuecomment-276745954>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AAjO_\n> TYMocotuTUixZB3Ejb4x9Xpv4l0ks5rYNVfgaJpZM4Lzd3N>\n>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7180#issuecomment-276747381>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOVxOS16DMa-cdG9C1XybTXfZ3SU4ks5rYNZCgaJpZM4Lzd3N>\n> .\n>\n", "@tensorflow-jenkins test this please", "The failing tests are unrelated. Merging."]}, {"number": 7179, "title": "Remove local_run in Experiment", "body": "", "comments": ["@martinwicke this changes the API by removing \"local_eval_frequency\", can you review, please?", "This has been deprecated for months so I think it would be safe to remove it. It would be better if there's internal deprecation tracking for removal dates and the person who's responsible for removal on time. ", "Yes, the dates were a mistake.\n", "@martinwicke does this mean you approve of this change or not :-)", "Taking the train. Will \u200bupdate when I've arrived.\n", "@tensorflow-jenkins Test this please", "Ping! Could this be merged? ", "This will break a bunch of things internally. We're planning some reorganization of this anyway, so let's leave it as is. Removing things is fiendishly hard. Better not to try without access to the internal tests. :( \r\n\r\n "]}, {"number": 7178, "title": "Update tf_upgrade.py to handle more upgrade cases", "body": "- {ones,zeros}_initializer\r\n- tf.sparse_split", "comments": []}, {"number": 7177, "title": "Move default for dnn_activation_fn to _dnn_linear_combined_model_fn for consistency", "body": "This is for making it more consistent with assigning default values for other params", "comments": ["Test failure due to timeout. Will be fixed in https://github.com/tensorflow/tensorflow/pull/7180", "@tensorflow-jenkins Test this please", "This is a very trivial change and shouldnt break anything.", "@tensorflow-jenkins test this please", "ping for @martinwicke ", "Mustafa, enlist Zakaria or George for reviews -- I could not find github accounts for them."]}]