[{"number": 30486, "title": "Keras TimeDistributed on a Model creates duplicate layers, and is inconsistent with TimeDistributed on a Layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu 1.14.0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: GTX1060M\r\n\r\n**Describe the current behavior**\r\nWrapping a model in a TimeDistributed layer creates duplicate nodes in the graph. If we follow the docs [(link)](https://keras.io/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers) and create a simple Dense Model wrapped in a TD Layer:\r\n```\r\n    inner_input = keras.layers.Input((2,))\r\n    dense = keras.layers.Dense(2, activation='relu')(inner_input)\r\n    inner_model = keras.Model(inputs=inner_input, outputs=dense)\r\n\r\n    full_input = keras.layers.Input((2,2))\r\n    td_2 = keras.layers.TimeDistributed(inner_model)(full_input)\r\n    model = keras.models.Model(full_input, td_2)\r\n    model.compile('SGD', 'mse')\r\n```\r\nYou end up with this:\r\n![bad_td](https://user-images.githubusercontent.com/24449147/60799985-d4e60c00-a1a6-11e9-84e8-e0b6ddd2a789.png)\r\nFirstly, if you follow the documentation approach you end up with an additional Dense Layer (Bottom Left). This eats up memory, and happens because you build the inner model then rebuild it again when you build the TimeDistributed model. This can be avoided by parameterizing your model [(link)](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#building_models), but it can be a very painful workaround if your model is complex. But for demonstrations sake, here's the model as an object:\r\n```\r\n    class InnerModel(keras.Model):\r\n        def __init__(self):\r\n            super(InnerModel, self).__init__()\r\n\r\n            self.dense = keras.layers.Dense(2, activation='relu')\r\n\r\n        def call(self, inputs, training=None, mask=None):\r\n            out = self.dense(inputs)\r\n            return out\r\n\r\n        def compute_output_shape(self, input_shape):\r\n            return (input_shape[0], 2)\r\n\r\n    input = keras.layers.Input((2,2))\r\n    td_model = InnerModel()\r\n    time_dist = keras.layers.TimeDistributed(td_model)(input)\r\n    model = keras.models.Model(input, time_dist)\r\n    model.compile('SGD', 'mse')\r\n```\r\nHere's the improved graph:\r\n![better_td](https://user-images.githubusercontent.com/24449147/60801864-58552c80-a1aa-11e9-9550-24068fffc43c.png)\r\n\r\nSo the additional Dense layer is gone, but there are still two dense layers inside. They have different contents too.\r\n![better_td_internal](https://user-images.githubusercontent.com/24449147/60801763-25ab3400-a1aa-11e9-9721-00482828ed58.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nIf you compare to what you get if you just wrap the Dense layer itself:\r\n```\r\n    full_input = keras.layers.Input((2,2,2))\r\n    td_2 = keras.layers.TimeDistributed(keras.layers.Dense(2, activation='relu'))(full_input)\r\n    model = keras.models.Model(full_input, td_2)\r\n    model.compile('SGD', 'mse')\r\n```\r\n![good_td](https://user-images.githubusercontent.com/24449147/60800184-5c337f80-a1a7-11e9-8dd3-901054a537d9.png)\r\n\r\nI'd expect wrapping a model should result in a very similar looking graph to wrapping a layer\r\n\r\n**Code to reproduce the issue**\r\nFull code for creating the graphs:\r\nhttps://gist.github.com/LukeBolly/0efeca3db275dee97c5f0fbf1f400b5a\r\n", "comments": ["If you define your custom model as a Layer instead of a Model, it produces the expected graph without the additional nodes", "@LukeBolly this seems to be fixed in the latest nightly, could you please try updating? Otherwise it may be an issue with TensorBoard, but the layers in the outer Model look good to me as far as training in Keras", "This is fixed with `tf-nightly` '2.1.0-dev20200109' version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30486\">No</a>\n"]}, {"number": 30485, "title": "(Deeplab)(ios)(tflite)using deeplab on ios application ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone 6\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):0.27\r\n- GCC/Compiler version (if compiling from source):Nan\r\n- CUDA/cuDNN version:Nan\r\n- GPU model and memory:Nan\r\n**Describe the current behavior**\r\ndeeplab doesn't segment on ios \r\n**Describe the expected behavior**\r\ndeeplab segmentation on ios application\r\n**Code to reproduce the issue**\r\n`tflite_convert ----output_format=TFLITE --inference_type=FLOAT --inference_input_type=FLOAT --input_arrays=sub_2 --input_shapes=1,224,224,3 --output_arrays=ResizeBilinear_2 --output_file=/Users/Karizma/Downloads/deeplabv3_mnv2_pascal_trainvall/frozen-224.tflite --graph_def=/Users/Karizma/Downloads/deeplabv3_mnv2_pascal_trainvall/frozen-224.pb --mean_values=128 --std_dev_values=127 --allow_custom_ops --post_training_quantize`\r\n**Other info / logs**\r\nHi, I have trained deep lab on my custom dataset(200*150) with 224 as crop size and during the test, it detects for crop with crop size 224 .\r\nnow what I need is to integrate my model on ios application, i was able to successfully convert the model to tflite .but i does not detecte anything i don't get it whats the problem\r\nbecause when i tried to convert a deeplab pretrained mobilenet mode [llink](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz) it works for me on mobile and for my model no ,however i have tested my model (.pb model)with python code and it detects\r\nthis is my model architecture i hope it will be helpful to indrestoud what's going on\r\n![s](https://user-images.githubusercontent.com/34586193/60801386-c8de5800-a16e-11e9-9a5f-ca319de7b0ee.png)\r\n![ss](https://user-images.githubusercontent.com/34586193/60801396-ced43900-a16e-11e9-8a6d-e9374beea91a.png)\r\n", "comments": ["We've recently added an Image Segmentation example app for iOS, using DeepLabV3 model.\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios\r\n\r\nCould you try running it?", "Closing due to lack of activity, but please feel free to reopen if the above example does not work for you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30485\">No</a>\n"]}, {"number": 30484, "title": "Deprecated random_uniform & random_normal ops removed from test and example files", "body": "\r\n`From /media/siju/DATA/ubuntu_cache/bazel/_bazel_siju/800a48f78ba10e98e4a18f338aa2c1e2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/debug/examples_test.runfiles/org_tensorflow/tensorflow/python/debug/examples/debug_tflearn_iris.py:38: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\r\n`\r\n`From /media/siju/DATA/ubuntu_cache/bazel/_bazel_siju/800a48f78ba10e98e4a18f338aa2c1e2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/examples/tutorials/word2vec_test.runfiles/org_tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py:175: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n`", "comments": ["Can one of the admins verify this patch?", "@alextp @caisq there are few changes in contrib folder ? do we need this ?", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac"]}, {"number": 30483, "title": "Hwloc mirror - download issue", "body": "I am trying to cross-compile TF for Raspberry Pi via Docker container - as described in documentation. Unfortunately. it breaks with the following message:\r\n\r\nERROR: /workspace/tensorflow/core/BUILD:2432:1: no such package '@hwloc//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /home/dmitry/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_dmitry/eab0d61a99b6696edb3d2aff87b585e8/external/hwloc/hwloc-2.0.3.tar.gz: Tried to reconnect at offset 6,391,630 but server didn't support it and referenced by '//tensorflow/core:lib_internal_impl'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@hwloc//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /home/dmitry/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_dmitry/eab0d61a99b6696edb3d2aff87b585e8/external/hwloc/hwloc-2.0.3.tar.gz: Tried to reconnect at offset 6,391,630 but server didn't support it\r\n\r\nFixes tried with no luck:\r\n- remove a semi-downloaded package from Bazel cache\r\n- download it from the original site and put to a cache\r\n- download from a mirror with Chrome (breaks in two parts)\r\n\r\n**System information**\r\nTF_BUILD_INFO = {\r\ncontainer_type: \"pi-python3\", \r\ncommand: \"tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE\", \r\nsource_HEAD: \"c407b045b8802f9eded430ef48be18cd85e4788c\", \r\nsource_remote_origin: \"https://github.com/tensorflow/tensorflow.git\", \r\nOS: \"Linux\", \r\nkernel: \"4.15.0-54-generic\", \r\narchitecture: \"x86_64\", \r\nprocessor: \"Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz\", \r\nprocessor_count: \"8\", \r\nmemory_total: \"16305540 kB\", \r\nswap_total: \"16657404 kB\", \r\nBazel_version: \"Build label: 0.24.1\", \r\nJava_version: \"1.8.0_222-ea\", \r\nPython_version: \"2.7.6\", \r\ngpp_version: \"g++ (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\", \r\nswig_version: \"\", \r\nNVIDIA_driver_version: \"418.56\", \r\nCUDA_device_count: \"0\", \r\nCUDA_device_names: \"\", \r\nCUDA_toolkit_version: \"\"\r\n}\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\"     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE\r\n\r\n\r\n", "comments": ["@DimaK-tracxpoint Please provide us the Tensorflow version. Thanks!", "Checked out r1.14\r\nCommit c407b045b8802f9eded430ef48be18cd85e4788c\r\n", "I am having the same kind of problem on a RHEL 7.6 box with Anaconda 3, devtoolset-8, and rh-git29.\r\nBazel is 0.26.1\r\n\r\n```\r\n - /data/p36078/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'hwloc':\r\n   java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /root/.cache/bazel/_bazel_root/7e95f03115517fb37af5a5010ad847b8/external/hwloc/hwloc-2.0.3.tar.gz: Read timed out\r\nERROR: /data/p36078/tensorflow/tensorflow/tools/pip_package/BUILD:155:1: no such package '@hwloc//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /root/.cache/bazel/_bazel_root/7e95f03115517fb37af5a5010ad847b8/external/hwloc/hwloc-2.0.3.tar.gz: Read timed out and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@hwloc//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz, https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz] to /root/.cache/bazel/_bazel_root/7e95f03115517fb37af5a5010ad847b8/external/hwloc/hwloc-2.0.3.tar.gz: Read timed out\r\nINFO: Elapsed time: 40.800s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```\r\n\r\nRELEASE.md --> Release 1.12.2 (with a Release 1.13.0 a few lines _below_)\r\n", "Temporary workaround is to edit the first URI in third_party/hwloc/workspace.bzl somewhere between domain name and file extension - so it will point to a non-existing location. For example:\r\n...\r\nurls = [\r\n            \"http://mirror.tensorflow.org/download.open-mpi.org/release/hwloc/**v2.100**/hwloc-2.0.3.tar.gz\",\r\n            \"https://download.open-mpi.org/release/hwloc/v2.0/hwloc-2.0.3.tar.gz\",\r\n        ],\r\n...\r\nBazel will fall back to the second URL and proceed.", "I just hit this issue while trying to compile tensorflow 1.14 from source.\r\nI tried to download the file manually from the http://mirror.tensorflow.org site and I got a checkpoint warning of a malicious file and the download was blocked. It's likely a false positive as the checksum of the file from the mirror and the same file from the open-mpi.org site are the same and checkpoint AV doesn't alert on that site.\r\nI also checked other files from the mirror and they download fine.", "@DimaK-tracxpoint  We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please check [**`link1`**](https://stackoverflow.com/questions/64647466/how-to-cross-compile-tensorflow-lite-for-raspberry-pi) , [**`link2`**](https://github.com/tensorflow/tensorflow/issues/38037).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30483\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30483\">No</a>\n"]}, {"number": 30482, "title": "Can't compile TensorFlow 1.3 on Jetson TX2", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04 on Jetson TX2\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.3.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.4.5 (not working, see below)\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: CUDA 8.0, cuDNN 6.0.\r\n- GPU model and memory: [NVIDIA Pascal GPU, 8GB 128-bit LPDDR4 Memory](https://developer.nvidia.com/embedded/jetson-tx2)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI need to build TensorFlow 1.3 on a Jetson TX2 machine (aarch64). I can't get the build to work, since I don't have Bazel for aarch64 and can't seem to build it. Since TensorFlow can be used on Jetson TX2, I guess that I'm not building it the right way.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFollow [all the steps in the Bazel installation guide](https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu), then\r\n\r\n```bash\r\nwget https://github.com/tensorflow/tensorflow/archive/v1.3.1.tar.gz\r\ntar zxf v1.3.1.tar.gz\r\ncd tensorflow-1.3.1\r\n./configure\r\n```\r\n\r\nThe output:\r\n```\r\n/usr/local/bin/bazel: line 88: /usr/local/lib/bazel/bin/bazel-real: cannot execute binary file: Exec format error\r\n/usr/local/bin/bazel: line 88: /usr/local/lib/bazel/bin/bazel-real: Success\r\n```\r\n\r\nI realize that this is a Bazel problem, but since I can't figure out how to build Bazel for that machine, I may have gotten the building method wrong.", "comments": ["1.3 is well outside our support window. Please try again with a more recent release branch."]}, {"number": 30481, "title": "get the planning path from zed camera", "body": "hello\r\nmy question is about how can benefit from planning path topic get from Zed and trace the trajectory of zed motion\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "thank you sire"]}, {"number": 30480, "title": "No module named '_pywrap_tensorflow' error even after installing Visual C++ and MSVCP140.dll.", "body": "I'm trying to run the simple speech recognition network example of Tensorflow:\r\n\r\npython tensorflow/examples/speech_commands/train.py\r\n\r\nBut I keep getting:\r\n\r\n> Traceback (most recent call last):\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n>During handling of the above exception, another exception occurred:\r\n\r\n>Traceback (most recent call last):\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n>During handling of the above exception, another exception occurred:\r\n\r\n>Traceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 79, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n>During handling of the above exception, another exception occurred:\r\n\r\n>Traceback (most recent call last):\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\DELL 7000\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\n>Failed to load the native TensorFlow runtime.\r\n\r\n>See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\n>for some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nMost of the solutions I found told me to reinstall visual c++ or to manually add MSVCP140.dll to the path. But neither of these solve my problem.\r\nThe github page it tells me to refer returns a 404.\r\nI'm on python 3.7.2 and tensorflow 1.0.0.\r\nKindly help if possible.", "comments": ["@SharawatChirag Does your CPU support AVX instruction set? Thanks!  "]}, {"number": 30479, "title": "Removed duplicated registration of Less with bfloat16", "body": "This fix tries to address the issue raised in #30476 where\r\nOp Less was registered twice which triggered `Multiple OpKernel registrations` error\r\nThis fix removes the duplication.\r\n\r\nThis fix fies #30476.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 30478, "title": "[TF2.0] Dataset iteration, dynamic TensorArray and reduce operations", "body": "**System information**\r\n- Have I written custom code: `yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OSX`\r\n- TensorFlow installed from (source or binary): `binary - 2.0.0-beta1`\r\n- TensorFlow version (use command below): `v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1`\r\n- Python version: `3.6`\r\n\r\n**Describe the current behaviour**\r\nI'm trying to apply a reduce operation over the result of TensorArray concatenation.\r\nThe concatenation happens in a for loop generated by iteration over a dataset.\r\nThe resulting value of the reduce operation is malformed tensor:\r\n- the shape is `()`\r\n- the actual value is `[float32]` of shape `(1,)`\r\n\r\nThis makes the resulting tensor effectively unusable because TF will then fail either because of shape information or because of the actual value of the tensor\r\n\r\nRemark:\r\n- If the for loop is generated from the `tf.range` operation, everything works as expected.\r\n\r\n**Describe the expected behaviour**\r\nGetting a valid result from the different reduce operations when applied to the result of a TensorArray concatenation operation when this one is filled in a for loop generated by iteration over a dataset.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nmean = tf.keras.metrics.Mean()\r\n\r\na = tf.random.uniform([10, 2])\r\nd = tf.data.Dataset.from_tensor_slices(a).batch(2)\r\n\r\n\r\n@tf.function\r\ndef compute(mean, dataset):\r\n    # I don't use the dataset at all\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    for i in tf.range(10):  # Simple for loop\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n    all_real_logits = arr.concat()\r\n\r\n    score = tf.reduce_mean(all_real_logits)\r\n    tf.print(tf.shape(score), score)  # -> [], 0.0653904751\r\n    mean.update_state(score)\r\n    return mean.result()\r\n\r\n\r\n@tf.function\r\ndef compute_error(mean, dataset):\r\n    # I use the dataset only to get the index\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    for i, _ in dataset.enumerate():  # Dataset for loop with enumerate\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n    all_real_logits = arr.concat()\r\n\r\n    score = tf.reduce_mean(all_real_logits)\r\n    tf.print(tf.shape(score), score)  # -> [], [-0.256373167] brackets!\r\n    mean.update_state(score)\r\n    return mean.result()\r\n\r\n\r\n@tf.function\r\ndef compute_error2(mean, dataset):\r\n    # I only use the dataset to simulate a for loop\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    i = tf.constant(0, tf.int32)\r\n    for _ in dataset:  # Dataset for loop\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n        i = i + 1\r\n    all_real_logits = arr.concat()\r\n\r\n    # score = tf.reduce_mean(all_real_logits)\r\n    score = tf.reduce_sum(all_real_logits)\r\n    tf.print(tf.shape(score), score)  # -> [], [-0.256373167] brackets!\r\n    mean.update_state(score)\r\n    return mean.result()\r\n\r\n\r\n# Works well\r\nprint(compute(mean, d))\r\n\r\n# Breaks because the shape is wrong for the score var\r\n# We have shape=() and the actual value is [float] of shape (1,)\r\n# Yet we can't do score[0] because the shape is ()\r\n# In the end the score var becomes unusable\r\n\r\n# Error: Cannot update variable with shape [] using a Tensor with shape [1], shapes must be equal.\r\nprint(compute_error(mean, d))\r\n\r\n# it seems that the error still occurs as long as the call to\r\n# arr.write is inside a for loop generated by iteration on a\r\n# dataset\r\n# Switching the reduce operation does not change the behaviour\r\nprint(compute_error2(mean, d))\r\n\r\n```\r\n", "comments": ["Was able to reproduce the issue on Colab with Tensorflow version 2.0.0.beta1. ", "@morgangiraud thank you for the detail instructions for how to reproduce the issue.\r\n\r\nWhen `tf.function` is used to decorate a function that uses Python-style iteration over a dataset, Autograph is used to rewrite the loop to a graph equivalent. It seems that there is a bug in how TensorArray objects used inside of the loop are handled.\r\n\r\nI am cc-ing @mdanatg, an expert on Autograph to take a closer look. ", "Thanks for taking the time to look into it. \ud83d\udc4d\ud83c\udffb\r\n\r\nAlso, it might be unrelated but I've been playing with autograph, `tf.function`,  datasets and TensorArray quite a lot and I found a bunch of strange behaviours:\r\n- https://github.com/tensorflow/tensorflow/issues/30409\r\n- https://github.com/tensorflow/tensorflow/issues/29996\r\n\r\nJust wanted to gather everything here as it might be useful.", "Indeed, we seem to be dealing with an inconsistent Tensor object here, one whose static shape does not match its actual value.\r\nThis seems to be caused by a combination of issues, but ultimately the smoking gun seems to be `tf.reduce_mean`.\r\n\r\nTo clarify how AutoGraph transforms these pieces of code:\r\n\r\n * in the first instance (`compute`), a simple `tf.while_loop` is used\r\n * in both of the second instances (`compute_error`, `compute_error2`), `Datset.reduce` is used instead\r\n\r\nThe two should be entirely consistent, but in reality they are not:\r\n\r\n * in the case of `tf.while_loop`, the `TensorArray` retains a static shape on its elements, so that ultimately, the result of `concat()` is a shape `[None, 1]`\r\n * in the case of `Dataset.reduce` however, the `TensorArray` seems to lose its inferred shape (this shape is memorized after the first call to `write`; this in turn causes `concat()` to generate an entirely unknown (that is, dynamic) shape and rank\r\n\r\nThe consequence of this inconsistency highlights a bug in `tf.reduce_mean`, which in the former case returns a scalar (correctly), but in the latter case it returns a size-1 vector (incorrectly). What's more, in both cases it reports a static shape of `()`.\r\n\r\nThis code highlights both the inconsistency and the bug. It's based on the original code, but contains the equivalent code that autograph generates, to highlight the differences:\r\n\r\n```\r\n@tf.function(autograph=False)\r\ndef compute():\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    def body(i, arr):\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n        i += 1\r\n        return i, arr\r\n    def cond(i, arr):\r\n      return i < 10\r\n    _, arr = tf.while_loop(cond, body, (0, arr))\r\n\r\n    c = arr.concat()\r\n    m = tf.reduce_mean(c)\r\n    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)\r\n    tf.print('Reported shape of tf.reduce_mean(TensortArray.concat()):', m.shape)\r\n    tf.print('Actual value of tf.reduce_mean(TensortArray.concat()):', m)\r\n    return m\r\n\r\n@tf.function(autograph=False)\r\ndef compute_ds():\r\n    arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)\r\n    def body(state, _):\r\n        i, arr = state\r\n        real_logits = tf.random.normal([5, 1])\r\n        arr = arr.write(tf.cast(i, tf.int32), real_logits)\r\n        i += 1\r\n        return i, arr\r\n    en_ds = tf.data.Dataset.range(10).enumerate()\r\n    _, arr = en_ds.reduce((0, arr), body)\r\n\r\n    c = arr.concat()\r\n    # Making the shape known makes them inconsistency once more:\r\n    # c.set_shape([None, 1])\r\n    m = tf.reduce_mean(c)\r\n    tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)\r\n    tf.print('Reported shape of tf.reduce_mean(TensortArray.concat()):', m.shape)\r\n    tf.print('Actual value of tf.reduce_mean(TensortArray.concat()):', m)\r\n    return m\r\n\r\nprint('*** With tf.while_loop')\r\n_ = compute()\r\nprint()\r\nprint('*** With tf.Dataset.reduce')\r\n_ = compute_ds()\r\n```\r\n\r\n```\r\n*** With tf.while_loop\r\nTensortArray.concat() shape: TensorShape([None, 1]) rank: 2\r\nReported shape of tf.reduce_mean(TensortArray.concat()): TensorShape([])\r\nActual value of tf.reduce_mean(TensortArray.concat()): 0.266083568\r\n\r\n*** With tf.Dataset.reduce\r\nTensortArray.concat() shape: TensorShape(None) rank: None\r\nReported shape of tf.reduce_mean(TensortArray.concat()): TensorShape([])\r\nActual value of tf.reduce_mean(TensortArray.concat()): [0.0591446124]\r\n```\r\n\r\nNote, in the above output, the differences in shape (`[None, 1]` vs. `None`) and rank (`2` vs. `None`). Also note the mismatch between static shape and actual value (`TensorShape([])` vs. `[0.0591446124]`).\r\n\r\nFiled #30685 and #30686 to track each of these issues separately. Please re-open this issue if I missed anything not captured in there!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30478\">No</a>\n", "@mdanatg  Cool, thanks for taking the time to do this work.\r\n\r\nOne thing that might have been forgotten: it does not only happens with `tf.reduce_mean` but also with `tf.reduce_sum` and possibly with every `tf.reduce_` operations. (I didn't check that)", "Thanks, that's useful to know - added note to the issue thread."]}, {"number": 30477, "title": "Run Tflite model on server", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: all android phones\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version:python3\r\n- Bazel version (if compiling from source):4\r\n- GCC/Compiler version (if compiling from source):na\r\n- CUDA/cuDNN version:na\r\n- GPU model and memory:na\r\n\r\n\r\n\r\nI have a custom trained model and exported it on Tensorflow Lite. It run well on android phones. I would like to run the same model on a Ubuntu server. How can I do that? I am choosing tflite for its small file size but I could not find any way to take inference from *.tflite model directly on a Ubuntu server.\r\n", "comments": ["@ajinkya933 Did you get a chance look official [Tensorflow](https://www.tensorflow.org/lite/guide/inference) website for tensorflow tite Inference. Thanks!", "@gadagashwini yes, but there they are more focused on exploring the internal structure of tflite graph which can easily be done through softwares like Netron. What I am interested is in getting bounding box output on ubuntu server provided I have *.tflite file as an input", "?", "This is possible to do by installing the standard tensorflow pip and using the Python tflite interpreter\r\nhttps://www.tensorflow.org/api_docs/python/tf/lite/Interpreter\r\nThat's the easiest way, it is also possible to use the C++ API, but you'll need to compile tflite yourself."]}, {"number": 30476, "title": "Op Less<bfloat16> registered twice", "body": "In the 1.14 branch, the CPU kernel op \"Less\" is registered twice for the type bfloat16 (see https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/kernels/cwise_op_less.cc#L19 : first on line 19 and then on line 21).\r\n\r\nThis has the effect of throwing an error \"InvalidArgumentError: Multiple OpKernel registrations match NodeDef\" whenever a bfloat16 comparison on a CPU is attempted.\r\n", "comments": ["Added a PR #30479 for the fix.", "The fix looks good.\r\nNote that the same problem is present in master: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_less.cc#L19", "Please fix the same issue with bfloat16 registration for less_equal in 1.14:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_less_equal.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/kernels/cwise_op_less_equal.cc#L19\r\n", "Automatically closing this out since I understand it to be resolved by the PR #30479 (merged already), but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30476\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30476\">No</a>\n", "30479 resolves \"less op\" issue. I do not see that it addresses \"less_equal op\" issue.", "@mangushev Sorry I didn't see the comment about less_equal previously. Have added a PR 31459 to fix `LessEqual`."]}, {"number": 30475, "title": "Tensorflow v1.14.0 with MKL and eigen on osx", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX Mojave 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: on master branch, commit 3e5418c3d6\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): clang-1001.0.46.4\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried to install tensorflow along with MKL and eigen\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```shell\r\n# conda 4.7.5\r\nconda create -n idp-6 -c defaults python=3 pip six numpy wheel setuptools mock 'future>=0.17.1' 'keras-preprocessing=1.0.5' 'keras-applications=1.0.6' mkl mklml\r\nconda activate idp-6\r\ncd tensorflow\r\nexport TF_MKL_ROOT=$HOME/miniconda3/envs/idp-6\r\n# This is needed since license.txt does not exist in $TF_MKL_ROOT and the build system expects it\r\ntouch $TF_MKL_ROOT/license.txt\r\nyes \"\" | ./configure\r\nbazel build -c opt --verbose_failures --config=mkl --copt=\"-DEIGEN_USE_MKL_VML\" --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nWhen I ran it, I got an error at some point in through the build that mkl.h was not found.\r\n\r\nAs a quick fix, I made the following update and it worked (below). While this is not particularly elegant, but it works for me.\r\n\r\nIf there is interest, I can provide a pull request that does the right thing for win/osx/linux using third_party/mkl/BUILD as a template.\r\n\r\n```diff\r\ndiff --git i/third_party/eigen3/BUILD w/third_party/eigen3/BUILD\r\nindex bd6cb868a9..fb6970ee36 100644\r\n--- i/third_party/eigen3/BUILD\r\n+++ w/third_party/eigen3/BUILD\r\n@@ -35,6 +35,8 @@ cc_library(\r\n     deps = [\r\n         \"@eigen_archive//:eigen\",\r\n         \"@local_config_sycl//sycl\",\r\n+        \"@mkl_darwin//:mkl_headers\",\r\n+        \"@mkl_darwin//:mkl_libs_darwin\",\r\n     ],\r\n )\r\n```", "comments": ["Hi\r\nThanks for the quick fix.\r\nI'll share your fix to dev team.\r\nThank you.", "@skrisna Is it possible to close this issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30475\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30475\">No</a>\n"]}, {"number": 30474, "title": "[TF2.0] Bug allowing misuse of the batch dimension of a convolution layer", "body": "`tensorflow-1.14.0` rightfully complains about the following minimal example with `ValueError: could not broadcast input array from shape (20,6,6,32) into shape (10,6,6,32)`.\r\n\r\n`tensorflow==2.0.0-beta1` however happily runs it and prints `(20, 6, 6, 32)`.\r\n\r\n```python3\r\nimport numpy as np\r\nimport tensorflow.keras.backend as k\r\nfrom tensorflow.keras.layers import Input, Conv2D, Lambda\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef custom_reshape(inputs):\r\n    return k.reshape(inputs, (-1, 8, 8, 3))\r\n\r\ninputs = Input(shape=(8, 8, 6))\r\nx = Lambda(custom_reshape)(inputs)\r\nx = Conv2D(32, (3, 3))(x)\r\nmodel = Model(inputs=inputs, outputs=x)\r\nmodel.compile(loss='mean_squared_error', optimizer='nadam')\r\nprint(model.summary())\r\nbatch_size = 10\r\nresult = model.predict(np.ones((batch_size, 8, 8, 6)), batch_size=batch_size)\r\nprint(result.shape)\r\n```\r\n\r\nAs [per discussion](https://groups.google.com/a/tensorflow.org/forum/#!topic/testing/txsgcR3cubQ) this seems to be a bug in TF 2.0.", "comments": ["I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1.", "@gadagashwini\r\n\r\n> I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1.\r\n\r\nYou mean the other way around, right? I.e., the `ValueError` we get with `1.14.0` should be the correct behavior, while not raising this exception in `2.0.0-beta1` seems to be the problem.", "> @gadagashwini\r\n> \r\n> > I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1.\r\n> \r\n> You mean the other way around, right? I.e., the `ValueError` we get with `1.14.0` should be the correct behavior, while not raising this exception in `2.0.0-beta1` seems to be the problem.\r\n\r\nYes @Dobiasd I could able to get the `ValueError` with Tensorflow `1.14.0` and no exception with Tensorflow `2.0.0.beta1`.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30474\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30474\">No</a>\n", "@Dobiasd With tf-nightly-2.0-preview the code seems to work correctly:\r\n```python\r\n...\r\n...\r\nTraceback (most recent call last):\r\n  File \"p.py\", line 16, in <module>\r\n    result = model.predict(np.ones((batch_size, 8, 8, 6)), batch_size=batch_size)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 872, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 717, in predict\r\n    callbacks=callbacks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 395, in model_iteration\r\n    aggregator.aggregate(batch_outs, batch_start, batch_end)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 308, in aggregate\r\n    result.aggregate(batch_element, batch_start, batch_end)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 233, in aggregate\r\n    batch_element.shape, self.results.shape))\r\nValueError: Mismatch between expected batch size and model output batch size. Output shape = (20, 6, 6, 32), expected output shape = shape (10, 6, 6, 32)\r\nubuntu@ubuntu:/v# python\r\nPython 2.7.15+ (default, Nov 27 2018, 23:36:35) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.version.VERSION\r\n'2.0.0-dev20190713'\r\n>>> tf.version.GIT_VERSION\r\n'v1.12.1-6246-g5d4a6cee73'\r\n>>> \r\n\r\n```", "@yongtang\r\n\r\n> With tf-nightly-2.0-preview the code seems to work correctly\r\n\r\nThanks for the confirmation. I guess this was to be expected since [the fix](https://github.com/tensorflow/tensorflow/commit/37fcf0a0e04b2014864936397c25e6c398135772) includes an explicit test for this. :+1:"]}, {"number": 30473, "title": "[LITE] Deprecated tf.random_normal changed to tf.random.normal", "body": "` From /media/siju/DATA/ubuntu_cache/bazel/_bazel_siju/800a48f78ba10e98e4a18f338aa2c1e2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/experimental/examples/lstm_test.runfiles/org_tensorflow/tensorflow/lite/experimental/examples/lstm/bidirectional_sequence_lstm_test.py:86: The name tf.random_normal is deprecated. Please use tf.random.normal instead.`", "comments": ["Can one of the admins verify this patch?"]}, {"number": 30472, "title": "The flag 'log_dir' is defined twice.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nDuplicateFlagError: The flag 'log_dir' is defined twice.\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom absl import flags\r\nflags.DEFINE_string('log_dir', './log', 'Log directory')\r\n", "comments": ["I am able to reproduce the issue with Tensorflow 1.14.0 on Colab.", "I don't think this is a TensorFlow issue as you are just redefining a flag that tensorflow knows.\r\n\r\nIt can be argued that TensorFlow should not need flags as it is a library, though.", "@mihaimaruseac do you know where do we define the log_dir flag? I can't find it in a cursory search of the TF codebase...", "We don't, it's abseil defining it as soon as one of abseil modules is imported.", "Encountered similar issue when using [PocketFlow](https://github.com/Tencent/PocketFlow) which is running on top of TensorFlow. The message\r\n```\r\nwzh@docker2[\u2713]PocketFlow (r1.14*) $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py\r\nPython script: nets/resnet_at_cifar10_run.py\r\n# of GPUs: 1\r\nextra arguments:  --model_http_url https://api.ai.tencent.com/pocketflow --data_dir_local /home/wzh/datasets/cifar/cifar-10-batches-bin\r\nTraceback (most recent call last):\r\n  File \"utils/get_idle_gpus.py\", line 38, in <module>\r\n    gpu_smi_output = subprocess.check_output(cmd, shell=True)\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\r\n    **kwargs).stdout\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\r\n    output=stdout, stderr=stderr)\r\nsubprocess.CalledProcessError: Command 'nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits' returned non-zero exit status 9.\r\n'nets/resnet_at_cifar10_run.py' -> 'main.py'\r\nmulti-GPU training disabled\r\n[WARNING] TF-Plus & Horovod cannot be imported; multi-GPU training is unsupported\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 27, in <module>\r\n    tf.app.flags.DEFINE_string('log_dir', './logs', 'logging directory')\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\", line 58, in wrapper\r\n    return original_function(*args, **kwargs)\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/absl/flags/_defines.py\", line 241, in DEFINE_string\r\n    DEFINE(parser, name, default, help, flag_values, serializer, **args)\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/absl/flags/_defines.py\", line 82, in DEFINE\r\n    flag_values, module_name)\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/absl/flags/_defines.py\", line 104, in DEFINE_flag\r\n    fv[flag.name] = flag\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/absl/flags/_flagvalues.py\", line 430, in __setitem__\r\n    raise _exceptions.DuplicateFlagError.from_flag(name, self)\r\nabsl.flags._exceptions.DuplicateFlagError: The flag 'log_dir' is defined twice. First from absl.logging, Second from main.py.  Description from first occurrence: directory t\r\no write logfiles into\r\n```\r\n\r\nWhen with TF 1.12, there is no such issue. With TF 1.14, the `absl.flags._exceptions.DuplicateFlagError` is raised.", "@alextp , as @mihaimaruseac pointed out, the `log_dir` is not defined by TF itself, but defined by `absl` library which TF uses. See stack (I inserted the `assert` right after where the `log_dir` is defined)\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 25, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 54, in <module>\r\n    from tensorflow.python.platform import app\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 23, in <module>\r\n    from absl.app import run as _run\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/absl/app.py\", line 41, in <module>\r\n    from absl import logging\r\n  File \"/home/wzh/toolchain/venv/pocket/lib/python3.6/site-packages/absl/logging/__init__.py\", line 238, in <module>\r\n    assert False\r\n```\r\n\r\nThe `absl` dependency was introduced by commit [`2435a1875b574f5d299b1dee431ab5ceccd6132f`](https://github.com/tensorflow/tensorflow/commit/2435a1875b574f5d299b1dee431ab5ceccd6132f). And, the interesting part is, the patch declares itself to *resolve flag conflict*.\r\n\r\nAlso ping @revan , the patch owner :)\r\n", "Hi there, as Mihai said this flag is defined in absl: https://github.com/abseil/abseil-py/blob/master/absl/logging/__init__.py#L234\r\n\r\nImporting tensorflow as of 1.14 causes an import of absl logging, which itself defines certain flags like `log_dir`. The good news is the workaround is very easy -- just don't define the `log_dir` flag, and rely on the one already defined.\r\n\r\nIf for some reason you need to run on both TF 1.14 and pre-1.14, you can wrap your flag definition in a try/except.\r\n\r\nClosing this issue, please let us know if the workaround is insufficient.", "> The good news is the workaround is very easy -- just don't define the log_dir flag, and rely on the one already defined.\r\n\r\nThis breaks some apps built on top of TF, and all related tools should be rewritten, because these apps are likely to have their own default value of `log_dir` which the tools are not likely to overwrite. Now, the `log_dir` needs to be assigned.\r\n\r\nAnyway, this won't be bother me as I don't own such apps or tools.", "Though this issue is easy to fix, I still believe this is a compatibility issue that should be avoided. ", "> Hi there, as Mihai said this flag is defined in absl: https://github.com/abseil/abseil-py/blob/master/absl/logging/__init__.py#L234\r\n> \r\n> Importing tensorflow as of 1.14 causes an import of absl logging, which itself defines certain flags like `log_dir`. The good news is the workaround is very easy -- just don't define the `log_dir` flag, and rely on the one already defined.\r\n> \r\n> If for some reason you need to run on both TF 1.14 and pre-1.14, you can wrap your flag definition in a try/except.\r\n> \r\n> Closing this issue, please let us know if the workaround is insufficient.\r\n\r\nYes, this solved my issue with this problem. "]}, {"number": 30471, "title": "[TF2.0] Bug when export BatchNormalization layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n  binary\r\n- TensorFlow version (use command below):\r\n  2.0.0-beta0\r\n- Python version:\r\n  python 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n  cuda 10, \r\n- GPU model and memory:\r\n  GTX 1080ti\r\n\r\n**Describe the current behavior**\r\n```\r\nW0708 11:01:14.168962 140059689457472 saved_model.py:721] Skipping full serialization of \r\nobject <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at \r\n0x7f61c004a978>, because an error occurred while tracing layer functions. Error message: \r\nExpected Operation, Variable, or Tensor, got None\r\n```\r\n**Describe the expected behavior**\r\nThere should be no error message.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\n\r\nclass BNModel(tf.Module):\r\n\r\n    def __init__(self):\r\n        super(BNModel, self).__init__()\r\n        self.bn = BatchNormalization()\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([None, 2], tf.float32)])\r\n    def __call__(self, x):\r\n        return self.bn(x)\r\n\r\nif __name__ == '__main__':\r\n    module = BNModel()\r\n    import numpy as np\r\n    arr = np.random.randn(2,2)\r\n    module(arr)                                                                                                                                           \r\n    signatures = {\"serving_default\": module.__call__}                                                                         \r\n    tf.saved_model.save(module, \"saved_models\", signatures=signatures)                                                      \r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the reported issue with Tensorflow 2.0.0.beta0 and It works as expected with Tenosrflow 2.0.0.beta1. Thanks!", "My problem is solved. Thank you very much.  @gadagashwini ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30471\">No</a>\n"]}, {"number": 30470, "title": "Fix eager session testing with GC", "body": "This PR re-enable Java eager sessions tests related to garbage collection, which were temporarily disabled for flakiness.\r\n\r\nSince we can't be 100% sure to invoke the GC in the test, we will simulate it using a custom reference queue.\r\n\r\nCC: @sjamesr ", "comments": []}, {"number": 30469, "title": "TensorflowLite model for On-Device Speech Recognizer", "body": "# Description of issue\r\n\r\nThere a mention of a tensorflowlite model in the Google AI team [blog](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html)\r\n\r\n> made publicly available through the model optimization toolkit in the TensorFlow Lite library \r\n\r\nWhere can I find this model? Is this the right place: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/g3doc/models", "comments": ["@dattachandan Please have a look at this [link](https://www.tensorflow.org/lite/guide/get_started#1_choose_a_model). Thanks!", "@dattachandan what make publicly available in that blog is\r\n> the parameter quantization and hybrid kernel techniques we developed in 2016\r\n\r\nnot model(s) :-)\r\n\r\n@gadagashwini nope, ASR models are not there :)", "Thanks for the update!\r\n\r\n* You mean that the quantization scheme developed by the Google Speech group(to reduce the resolution of the parameters of a neural network from 32-bit floating point values to 8-bit integer values) that was proposed in the paper \"On the efficient representation and execution of deep acoustic models\" was publicly released in tensorflow lite library [here](https://www.tensorflow.org/model_optimization/guide/quantization/index)\r\n\r\n* Does the tested Gboard app mentioned in the [blog](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html) load the ASR TFLite model as applinks to JNI binary(.tflite using TocoConverter?) as the smartreply app [demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/models/smartreply/) \r\n\r\n* Is the blog mentioned 80mb model downloaded separately from the Gboard android app while downloading a language file, because the app size is 43.55 mb for release 8.3.6.250752527-release-armeabi-v7a? Or is it in com.google.android.googlequicksearchbox/app_g3_models on the android filesystem? what's the location on Android?\r\n\r\n![image](https://user-images.githubusercontent.com/2154248/60935718-f1a54f80-a31e-11e9-8bca-3f34bbbbaa07.png)\r\n\r\n\r\n* Can this model be used in Google Home in offline mode as well?\r\n\r\n* Can this ASR model also run in the [CORAL TPU](https://coral.withgoogle.com/models/) which uses TF Lite?", "@dattachandan can't speak for Google guys. Just what I know\r\n\r\n- that's what the blog article claimed.\r\n- not sure if tflite and tflite model are used in this case, probably not\r\n- none of them I guess, should be the feature called \"Faster voice typing\"\r\n- no idea\r\n- even if it's a tflite model, it should be a weight only quantization one. Not one trained with quantization-aware training. Unlikely to work on Coral Edge TPU.\r\n", "@dattachandan I'm working on getting this model imported in tensorflow as well, and documented my progress on [hackaday.io](https://hackaday.io/project/164399-android-offline-speech-recognition-natively-on-pc).\r\n\r\nYou'll find there how to get the models (there are 4 tflite models chained together), and how they feed each other. Last thing to get everything up and running is to feed the first in the chain with the audio stream, which must be some log-mel features or filterbanks or so.\r\n\r\nI did not get to that yet, so there is some work left for you. :smile: \r\nPlease report your progress here or preferably in my log on HaD.io (I doubt this git issue is the place for it).", "@dattachandan\r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 30467, "title": "InvalidArgumentError: Retval[0] does not have value when combining tf.case and l2_regularization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n- Python version:\r\n 2.7.15\r\n- CUDA/cuDNN version:\r\n7\r\n- GPU model and memory:\r\nGTX 1070, 8G\r\n\r\n**Describe the current behavior**\r\nGet the following error when using tf.case and slim.l2_regularization.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/yfeng23/test/tf/case_test.py\", line 20, in <module>\r\n    print(sess.run(loss))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\nx = tf.zeros((1, 8))\r\n\r\nfn = lambda: slim.fully_connected(x, 4,\r\n                                  weights_regularizer=slim.l2_regularizer(0.1))\r\n\r\npred_fn_pairs = [\r\n  (tf.equal(0, 0), fn),\r\n  (tf.equal(1, 0), fn)\r\n]\r\n\r\ny = tf.case(pred_fn_pairs, exclusive=True)\r\n\r\nloss = tf.losses.get_regularization_loss()\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  print(sess.run(loss))\r\n```\r\n\r\n", "comments": ["I have tried on colab with TF version 1.14 and was able to reproduce the issue.Thanks!", "Apologies for the delay in response. In your code snippet ```loss``` is not linked to the fully connected layer. Therefore we can reproduce the error with this minimal code snippet;\r\n```python\r\nlosses_1 = tf.losses.get_regularization_loss()\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  print(sess.run(losses_1))\r\n```\r\nOutput:\r\n```python\r\nInvalidArgumentError: Retval[0] does not have value\r\n```\r\nThus its working as expected. Thanks!", "Thanks for your reply.\r\n\r\nloss is the norm of the weights in the fully-connected layer. They are linked.\r\n\r\nThe error is caused by tf.case\r\n\r\nI didn't get any error using the following code. The output I get is 0.28207764. \r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\nx = tf.zeros((1, 8))\r\n\r\ny = slim.fully_connected(x, 4, weights_regularizer=slim.l2_regularizer(0.1))\r\n\r\nlosses_1 = tf.losses.get_regularization_loss()\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  print(sess.run(losses_1))\r\n```", "tf.case and slim's regularization are not compatible.\r\n\r\nYou need to create your regularizers outside of conditional branches (like tf.cond / tf.case) or explicitly return the regularization loss from the case / cond and accumulate it / add it to collections outside.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30467\">No</a>\n"]}, {"number": 30466, "title": "Batched tf.linalg.eigh is much slower on GPU than on CPU for many small matrices", "body": "**Describe the current behavior**\r\nSee title.\r\n\r\n**Describe the expected behavior**\r\nThere shouldn't be such a big discrepancy between the two (see below).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nsym = lambda a: 0.5 * (a + tf.matrix_transpose(a))\r\nwith tf.device('cpu:0'):\r\n    _ = tf.linalg.eigh(sym(tf.random.uniform((100000, 2, 2))))  # fast (~0.02s)\r\nwith tf.device('gpu:0'):\r\n    _ = tf.linalg.eigh(sym(tf.random.uniform((100000, 2, 2))))  # slow (~7.3s)\r\n```\r\n\r\n**System information**\r\n- os platform: Linux-4.4.0-154-generic-x86_64-with-debian-stretch-sid\r\n- gpu: GeForce GTX TITAN X\r\n- python version: 3.7.3\r\n- tf.version.VERSION = 1.13.1\r\n- tf.version.COMPILER_VERSION = 5.4.0\r\n- CUDA 10, cuDNN 7", "comments": ["@calincru I tried executing the code on Colab with Tensorflow-gpu 1.13.1 but it executed with same duration (~0.01s) for GPU and CPU. Can you try once let us know is this still an issue. Thanks! ", "@gadagashwini The code that I included just builds the graph.  You should either enable eager execution or run it in a session.  Here's a more complete reproducing code that you can run on Colab: https://gist.github.com/calincru/890fbf8ad4b547381adcd2cc632c35e3", "I could see the GPU performance is much slower than the CPU. I ran the code on Colab with tensorflow-gpu 1.13.1.  ", "@reedwm would you help to take a look? Thanks.", "Can we expedite this a little bit, please?\r\n\r\nFWIW, there's a similar issue in PyTorch (see https://github.com/pytorch/pytorch/issues/22573). But AFAIU cuSolver should be fast for small matrices.  Is it expected to be slower than CPU, though?", "I'm experiencing the same issue running in tensorflow-gpu 1.12.0. Profiling trace attached if this is at all helpful. There are many, many small copies back and forth between CPU/GPU.\r\n[timeline_eigh_gpu.json.tar.gz](https://github.com/tensorflow/tensorflow/files/3468312/timeline_eigh_gpu.json.tar.gz)\r\n\r\n", "Hi, \r\nI have experienced the problem already at the beginning of 2019 (https://stackoverflow.com/questions/55655235/tf-linalg-eigh-extremely-slow-on-gpu-normal). I believe the problem is simply how the GPU tries to solve this problem. Without any prove I believe, that the GPU solves one matrix at a time and hence does this with 100.000 matrices of size 2x2 in your case. The problem is, that you try to solve the eigenvalue problem of each matrix of size 2x2 on hundreds if not thousands of cores, which is basically making things worse rather than better. Hence a few (or even one!) fast CPU core is much faster than trying to split a very easy calculation on many slow cores. You should also experience a similar thing if you have a CPU with many cores (lets say 16 cores and 32 threads) - at least that was what I experienced when I calculated small matrices with scipy.linalg.eighs on 32 threads vs 1 thread.\r\n\r\nOverall I have not found any way yet to calculate efficiently many small matrices on the GPU and I am not sure, whether this is actually possible. It would of course be nice, if one could specify that one wants to parallelise things in the batch dimension instead of the matrix dimensions ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30466\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30466\">No</a>\n"]}, {"number": 30465, "title": "problem about creating my own model", "body": "when i run the code which is write in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model\r\nbazel run -c opt --copt=-mavx2 --copt=-mfma \\\r\ntensorflow/examples/speech_commands:train -- \\\r\n--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \\\r\n--wanted_words=\"yes,no\" --silence_percentage=25 --unknown_percentage=25 --quantize=1\r\nit also said\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 344\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nERROR: /home/wxy/tensorflow-master/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 344\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/home/wxy/tensorflow-master/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\n and referenced by '//third_party/python_runtime:headers'\r\nERROR: Analysis of target '//tensorflow/examples/speech_commands:train' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 8.612s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (33 packages loaded, 707 targets c\\\r\nFAILED: Build did NOT complete successfully (33 packages loaded, 707 targets c\\\r\nonfigured)\r\n\r\ni run it in Ubuntu 16.04 and i install the numpy whatever in python 2.x or python 3.n\r\nthank you", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "I use the Linux Ubuntu 16.04 in VMware with Tensorflow 1.14 with CPU. I install Tensorflow with pip.\r\nBuild label: 0.27.1\r\nPython 3.5.2\r\n[GCC 5.4.0 20160609] on linux\r\ntf.__version__\r\n'1.14.0'\r\n\r\nthank you", "Hi,\r\n\r\nI have been able to compile the model for micro_speech successfully. I would recommend setting up a environment using [Docker.](https://www.tensorflow.org/install/docker). Its much easier to work with and it takes care of all TensorFlow related dependencies. \r\n\r\nAlso if possible try not to train the model using  VMWare as the training will be too slow. Instead try getting hold of a Linux computer with a good GPU and CPU along with plenty of RAM (16GB or more). \r\n\r\nOnce your Docker environment is setup login to your docker container, install Bazel program, and start training as per [the step here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech). It should work.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30465\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30465\">No</a>\n"]}, {"number": 30464, "title": "Tensorboard does not display more then 100 bounding boxes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. (I've prepared pipeline.config, classes.bptxt and one tfrecord\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.01 (Linux smok 4.15.0-45-generic #48-Ubuntu SMP Tue Jan 29 16:28:13 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux) \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):\r\nv1.12.0-0-ga6d8ffae09 1.12.0\r\n(I've also tried b'v1.12.0-6120-gdaab2673f2' 1.13.0-dev20190116 and results are the same)\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory: GTX 1080Ti, RTX 2080\r\n\r\n**Describe the current behavior**\r\nI run training \r\n```\r\npython object_detection/model_main.py --pipeline_config_path=/mnt/data/environments/bag/100bboxes/pipeline.config --model_dir=/mnt/data/environments/bag/100bboxes/exp\r\n```\r\nThis is only one step training and its goal is to visualize bounding boxes in tensorboard images tam. \r\nOn the \"IMAGES\" tab in tensorboard I don't see all bounding boxes which are in the tfrecord file. \r\nOptions provided in the pipeline.config\r\n```\r\nnum_visualizations: 200                                                                                                                   \r\nmax_num_boxes_to_visualize: 200\r\n```\r\nhave no effect on this. \r\n\r\n**Describe the expected behavior**\r\nI see all bounding boxes (ground thruth) in the tensorboard>images (pictures on the right)\r\n\r\n**Code to reproduce the issue**\r\nI've created [repo](https://github.com/wkoziej/100bboxes) where you can find\r\n * pipeline.config - one step training,  the same tfrecord for training and evaluation\r\n * classes.pbtxt - two sample classes\r\n * t.tfrecord - containg two white images: one with 100 bounding boxes and second with 200 bounding boxes\r\n * notebook-images-view.png - view from notebook when I've tried visualise bboxes \r\n * tensorboard-images-view.png - tensorboard images view \r\n\r\n\r\n", "comments": ["For faster resolution please post the issue on [TF-tensorboard](https://github.com/tensorflow/tensorboard/issues)\r\nrepository.Thanks!\r\n", "@ravikyram I believe this is an issue with the tensorflow/models/research/object_detection. I tried to debug the issue but I do lack a lot of context here. Can you please reassign it to the person from the owners listed [here](https://github.com/tensorflow/models/tree/master/research/object_detection#maintainers)? Thanks! ", "Today I made also simple experiments. \r\n1. In first experiment I prepared 100 images with *less* then 100 objects on every and labeled it automaticaly. It was training set. Similarly I prepared tfrecord with 30 images for evaluation \r\n2.  In second experiment I prepared 100 images with *more* then 150 objects on every and labeled it automaticaly. It was training set. Similarly I prepared tfrecord with 30 images for evaluation\r\n\r\nI trained faster rcnn (resnet101) (10000 steps) on each set. \r\nOn this image ![Here](https://user-images.githubusercontent.com/138819/60996427-de14db80-a354-11e9-8b27-5c6d6b282120.png) you can see comparison of mAP for first (red one) and second (rose one) experiment. I'm almost sure that there is also problem with training/evaluation.\r\n\r\nOn second image ![Here](https://user-images.githubusercontent.com/138819/60996429-de14db80-a354-11e9-8d10-2e14423a87ca.png) you can see sample images from tensorboard images tab for set with more then 150 objects and on third ![here](https://user-images.githubusercontent.com/138819/60996430-dead7200-a354-11e9-853b-a7eced1dd38d.png) for set with less then 100.\r\n\r\nWill it helps if I upload somewhere this tfrecords and pipelines (its about 50M) ?", "Hi, \r\n\r\nI've made a config and tfrecord's for this issue [on my drive](https://drive.google.com/open?id=1ZFB1pTfASNU7tUeHDAO06tO0mYm67XiO) You can download it, change paths in pipeline.config and run training (run.sh - set CUDA_VISIBLE_DEVICES and paths).\r\n\r\nCan I help you in some way to solve this issue?\r\n", "I've tried [tensorpack lib](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) and succesfully got   [results for image with more then 100 bboxes](https://user-images.githubusercontent.com/138819/62245092-0d14ef00-b3e1-11e9-90e4-7e9634bb6603.png)\r\n", "I\u2019ve struggle the same issue and done some investigation around this topis. It looks like the issue is **only in the evaluation step of a training process**.\r\n\r\n## Experiment 1 - training on pictures with less than 100 objects\r\nI\u2019ve done an experiment\u00a0where\u00a0training set had only images with **less than 100 objects**. I\u2019ve done the training, which went fine - **mAP around 0.82**. \r\n\r\nThen, when I was exporting the model for inference, I\u2019ve changed in pipeline.config, in section\u00a0`second_stage_post_processing {\u00a0batch_non_max_suppression { }}`value of parameter `max_detections_per_class` to 300. \r\n\r\nI\u2019ve checked the predictions for a photo where there **was over 100 objects in the picture**. The number of predictions was **correct - it marked all objects**.\r\n\r\n## Experiment 2 - training on pictures with more than 100 objects\r\nI\u2019ve then done the same experiment, but with a training set having some of the\u00a0images with **over 100 objects**, and parameter `max_detections_per_class` set to 300 from the beginning. The **final mAP was around a 0.3** - much less than in the first experiment. But, the training loss curves looked similar. This means for me, that the **training process is going fine, only the evaluation step has some issues.**\u00a0\r\n\r\n## Metrics\r\nLegend:\r\nOrange - evaluation, images with less than 100 objects\r\nRed - evaluation, images with more than 100 objects\r\n\r\nGray - training, images with less than 100 objects\r\nBlue - training, images with more than 100 objects\r\n\r\nEvaluation mAP \r\n![Screenshot 2019-08-08 at 08 48 08](https://user-images.githubusercontent.com/6491400/62681185-62409a00-b9b9-11e9-8021-d33e580fa1f6.png)\r\n\r\nEvaluation loss\r\n![Screenshot 2019-08-08 at 08 48 33](https://user-images.githubusercontent.com/6491400/62681183-62409a00-b9b9-11e9-8969-9fe3db9c4fc8.png)\r\n\r\nTraining loss\r\n![Screenshot 2019-08-08 at 08 48 28](https://user-images.githubusercontent.com/6491400/62681184-62409a00-b9b9-11e9-9029-b72602c33f84.png)\r\n\r\n## Evaluator Issue\r\nI\u2019ve checked evaluator sorce code. The COCO evaluator has a fixed value of 100 maximum detections ([link to source code](https://github.com/cocodataset/cocoapi/blob/636becdc73d54283b3aac6d4ec363cffbb6f9b20/PythonAPI/pycocotools/cocoeval.py#L28)). This causes, that **mAP and other metrics calculations are\u00a0incorrect**, because in the evaluation dataset there are examples, where in Ground Truth photos there is over 100 objects, and during evaluation only 100 detections are done. \r\n\r\nAny ideas how to tackle the problem with maxDets limit in COCO evaluator? \r\n\r\nI've already tried setting the maxDets parameter to [1, 10, 300] for box_evaluator ([after this line](https://github.com/k-lyda/models/blob/6b586a910d74a44f57da4d2335c79a20dc2803ab/research/object_detection/metrics/coco_evaluation.py#L214)), but this caused, that mAP was calculated as -1.000, so something were not working fine.", "https://github.com/tensorflow/models/issues/5465", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30464\">No</a>\n"]}, {"number": 30463, "title": "Update loss docstrings to match behavior.", "body": "https://github.com/tensorflow/tensorflow/pull/27784\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/27190\r\n\r\n", "comments": []}, {"number": 30462, "title": "Update ops_version.md", "body": "fix wrong `\\n` issue while reviewing Chinese translation and a bug in sample code.", "comments": ["Can one of the admins verify this patch?", "@miaout17 Can you please take a look on this PR? Thanks!", "@kuri-leo Can you please resolve conflicts? Thanks!", "Thanks for still remember me,\r\n\r\n1. Conflict resolved where https://github.com/tensorflow/tensorflow/commit/5b68afa3abfa6fe985068e52be1ff1eb9e57efc3 brings in.\r\n2. Reset original 3 commits https://github.com/tensorflow/tensorflow/commit/3f0b5e9048dca5acef1589efbeb11c1d56f4796e as almost a year, too many commits to merge and rebase with the latest master.\r\n3. Create https://github.com/tensorflow/tensorflow/commit/8348484f45d506471da29b2b08192ec22c3d69f5 to fix the left issue.\r\n\r\nBesides, could you please tell what happened? I think this is not an issue that should take so long time to review."]}, {"number": 30461, "title": "correctly describe sample_weight and Reduction.NONE behavior", "body": "https://github.com/tensorflow/tensorflow/pull/27784\r\nhttps://github.com/tensorflow/tensorflow/issues/27190\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30461) for more info**.\n\n<!-- need_author_cla -->", "Fix user e-mail."]}, {"number": 30460, "title": "Update loss docstrings to match behavior.", "body": "https://github.com/tensorflow/tensorflow/issues/27190\r\nhttps://github.com/tensorflow/tensorflow/pull/27784", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30460) for more info**.\n\n<!-- need_author_cla -->", "Fixing user name."]}, {"number": 30459, "title": "Memory leak when using `tf.linalg.expm` in `tf.keras.layers.Layer`", "body": "**System information**\r\n- Have I written custom code: *Yes*\r\n- OS Platform and Distribution: *Arch Linux*\r\n- TensorFlow installed from (source or binary): *Binary*\r\n- TensorFlow version (use command below): *2.0.0 Beta 1*\r\n- Python version: *3.7.3*\r\n- CUDA/cuDNN version: *10.0.130* / *7.6.0*\r\n- GPU model and memory: *Nvidia GTX 1060, 6GB*\r\n\r\n\r\n**Describe the current behavior**\r\nI'm currently working on a costum Keras Layer and need to use `tf.linalg.expm` several times. \r\nWhile training my model I noticed that my system would be out of memory after a few minutes. \r\nReducing the Layer definition step by step I noticed that the error should be related to `tf.linalg.expm`. \r\nEvery time the layers `call` method is invoked more and more memory get's allocated.\r\nReplacing `return tf.linalg.expm(X)` in the following code example with e.g. `return tf.matmal(X,X)` does not yield this extreme leak of memory.\r\n\r\n**Describe the expected behavior**\r\nThe memory allocated by an invocation of a`call` method using `tf.linalg.expm` should be freed.\r\n![Number of `call` invocations plotted against memory allocated in Gigabytes](https://user-images.githubusercontent.com/2351787/60768608-18b51480-a0c6-11e9-9d0f-baad36136cc8.png)\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nimport psutil\r\n\r\n\r\ndef memory():\r\n    pid = os.getpid()\r\n    py = psutil.Process(pid)\r\n    memory_use = py.memory_info()[0] / 2. ** 30\r\n    return memory_use\r\n\r\nclass TestLayer(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, **kwargs):\r\n        \r\n        super(TestLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.s = input_shape\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None):\r\n        \r\n        X = tf.matmul(tf.transpose(inputs),inputs)\r\n        \r\n        return tf.linalg.expm(X)\r\n    \r\n\r\n\r\nS = tf.keras.Input(shape=(1,), name=\"sequence\", dtype=tf.float64)\r\nT = TestLayer()(S)\r\nmodel = tf.keras.Model(inputs=S,outputs=T)\r\ndataset = tf.data.Dataset.from_tensors(tf.constant(1)).repeat().map(lambda x: tf.ones([tf.random.uniform([1], minval=100, maxval=3000, dtype=tf.int32)[0],1],dtype=tf.float64))\r\n\r\nmemory_usage = []\r\n\r\ni = 0\r\nfor data in dataset:\r\n    s = model(data)\r\n    \r\n    memory_usage.append(memory())\r\n\r\n    if i == 1000:\r\n        break\r\n        \r\n    i = i+1\r\n    \r\nplt.figure()\r\nplt.plot(memory_usage)\r\nplt.xlabel(\"iterations\")\r\nplt.ylabel(\"memory usage\")\r\nplt.show()\r\n```\r\n\r\n**Other info / logs**\r\nWarning produced when using `tf.linalg.expm`: \r\n```\r\n    WARNING: Logging before flag parsing goes to stderr.\r\n    W0707 13:40:07.108526 139932625143616 deprecation.py:323] From /home/darvin/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/linalg/linalg_impl.py:280: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\n    Instructions for updating:\r\n    Use tf.where in 2.0, which has the same broadcast rule as np.where\r\n```", "comments": ["Could reproduce the issue with TF Version 2.0 beta.", "@liemonade,\r\nCould you please upgrade to TensorFlow v2.3 and check if you are still facing the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30459\">No</a>\n"]}, {"number": 30458, "title": "can not convert tf model to tflite", "body": "Hi.i tried convert my tensorflow model to tensorflow lite with saved model of pretrained model such as:\r\n\r\n\r\nimport tensorflow as tf\r\nconverter=tf.lite.TFLiteConverter.from_saved_model(\"ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/saved_model\" ,input_arrays=None , input_shapes = (1,300,300,3),output_arrays=None, tag_set=None, signature_key=None)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\n\r\n\r\n\r\nbut i got this error:\r\n\r\n2019-07-07 16:56:02.926259: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-07 16:56:02.948528: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192640000 Hz\r\n2019-07-07 16:56:02.948956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ab6990 executing computations on platform Host. Devices:\r\n2019-07-07 16:56:02.948982: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0707 16:56:02.949664 140418756732736 deprecation.py:323] From /home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nTraceback (most recent call last):\r\n  File \"TFlite_converter.py\", line 30, in <module>\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(\"ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/saved_model\" ,input_arrays=None , input_shapes = (1,300,300,3),output_arrays=None, tag_set=None, signature_key=None)\r\n  File \"/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 757, in from_saved_model\r\n    output_arrays, tag_set, signature_key)\r\n  File \"/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py\", line 204, in freeze_saved_model\r\n    util.set_tensor_shapes(in_tensors, input_shapes)\r\n  File \"/home/davari/virtualenvironments/detection_on_mobile/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py\", line 145, in set_tensor_shapes\r\n    for name, shape in shapes.items():\r\nAttributeError: 'tuple' object has no attribute 'items'\r\n\r\ncan any one helps me?", "comments": ["The tflite works well for me in TF1.13. What's your tf version ?\r\nNote: i work with hdf5 and not saved_model.", "> The tflite works well for me in TF1.13. What's your tf version ?\r\n> Note: i work with hdf5 and not saved_model.\r\n\r\nThanks for your response.\r\nmy tf version is:1.15\r\n\r\nwhich library and dependencies should i have?please tell me all of them", "@Davari393 I'm afraid the stable version is only till 1.14 from tensorflow. Kindly check the version and only use the latest stable version for best performance. Also before loading the model,check for custom objects for your model if any.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30458\">No</a>\n"]}, {"number": 30457, "title": "Dataset: Feedable Iterator does not support tf.VarLenFeature (SparseTensor).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): **1.13.1**\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI want to do validation after some training steps, so I choose `Feedable Iterator API`. However Feedable Iterator API does not support tf.VarLenFeature(namely `SparseTensor`), VarLenFeature is useful for Recommend System.\r\n\r\nAny suggestions to work around? Below is a code snippet.\r\n\r\n**Code to reproduce the issue**\r\n1. Generate the test data\r\n```\r\nimport tensorflow as tf                                                         \r\nimport numpy as np                                                              \r\n                                                                                \r\ndef save_tfrecords(data, label, desfile):                                       \r\n    with tf.python_io.TFRecordWriter(desfile) as writer:                        \r\n        for i in range(len(data)):                                              \r\n            features = tf.train.Features(                                       \r\n                feature = {                                                     \r\n                    \"data\":tf.train.Feature(int64_list = tf.train.Int64List(value=data[i])),\r\n                    \"label\":tf.train.Feature(int64_list = tf.train.Int64List(value=[label[i]]))\r\n                }                                                               \r\n            )                                                                   \r\n            example = tf.train.Example(features = features)                     \r\n            serialized = example.SerializeToString()                            \r\n            writer.write(serialized)                                            \r\n                                                                                \r\ndata_size = 32                                                                  \r\ndata = []                                                                       \r\nfor i in range(data_size):                                                      \r\n    tmp = np.array(range(i + 1)).astype(np.int64)                               \r\n    data.append(tmp)                                                            \r\nlabel = np.array(range(data_size)).astype(np.int64)                             \r\nsave_tfrecords(data, label, 'train.tfrecord')                                   \r\n                                                                                \r\ndata_size = 8                                                                   \r\ndata = []                                                                       \r\nfor i in range(data_size):                                                      \r\n    tmp = np.array(range(i + 1)).astype(np.int64)                               \r\n    data.append(tmp)                                                            \r\nlabel = np.array(range(data_size)).astype(np.int64)                             \r\nsave_tfrecords(data, label, 'test.tfrecord')   \r\n```\r\n2. Run the test code.\r\n```\r\nimport tensorflow as tf                                                         \r\n                                                                                \r\ndef get_dataset(files):                                                         \r\n    def parser(record):                                                         \r\n      keys_to_features = {                                                      \r\n          \"data\": tf.VarLenFeature(tf.int64),                                   \r\n          \"label\": tf.FixedLenFeature((), tf.int64),                            \r\n      }                                                                         \r\n      parsed = tf.parse_single_example(record, keys_to_features)                \r\n                                                                                \r\n      # parsed['data'] = parsed['data'].values                                  \r\n                                                                                \r\n      return parsed                                                             \r\n                                                                                \r\n    dataset = tf.data.TFRecordDataset(files)                                    \r\n    dataset = dataset.map(parser)                                               \r\n    dataset = dataset.batch(4)                                                  \r\n    dataset = dataset.repeat(1)                                                 \r\n    return dataset                                                              \r\n                                                                                \r\n                                                                                \r\ngraph = tf.Graph()                                                              \r\nwith graph.as_default():                                                        \r\n    training_ds = get_dataset('train.tfrecord')                                 \r\n    validation_ds = get_dataset('test.tfrecord')                                \r\n                                                                                \r\n    handle = tf.placeholder(tf.string, shape=[])                                \r\n    iterator = tf.data.Iterator.from_string_handle(                             \r\n        handle, training_ds.output_types, training_ds.output_shapes)            \r\n    print(training_ds.output_types, training_ds.output_shapes)                  \r\n    next_element = iterator.get_next()                                          \r\n                                                                                \r\n    training_iterator = training_ds.make_initializable_iterator()               \r\n    validation_iterator = validation_ds.make_initializable_iterator()           \r\n                                                                                \r\n                                                                                \r\nwith graph.as_default():                                                        \r\n                                                                                \r\n    with tf.train.MonitoredTrainingSession() as sess:                           \r\n        training_handle = sess.run(training_iterator.string_handle())           \r\n        validation_handle = sess.run(validation_iterator.string_handle())       \r\n        sess.run(training_iterator.initializer)                                 \r\n        count_training = 0                                                      \r\n        while not sess.should_stop():                                           \r\n            x = sess.run(next_element, feed_dict={handle: training_handle})     \r\n            count_training += 1                                                 \r\n            print('{} [training] {}'.format(count_training, x))                 \r\n                                                                                \r\n            if count_training % 4 == 0:                                         \r\n                sess.run(validation_iterator.initializer)                       \r\n                count_validation = 0                                            \r\n                while True:                                                     \r\n                    try:                                                        \r\n                        y = sess.run(next_element, feed_dict={handle: validation_handle})\r\n                        count_validation += 1                                   \r\n                        print('  {} [validation] {}'.format(count_validation, y))\r\n                    except tf.errors.OutOfRangeError:                           \r\n                        break    \r\n```\r\n**Other info / logs**\r\n```\r\n{'data': tf.int64, 'label': tf.int64} {'data': TensorShape([Dimension(None), Dimension(None)]), 'label': TensorShape([Dimension(None)])}\r\n\r\nW tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at iterator_ops.cc:1225 : Invalid argument: Data type mismatch at component 0: expected int64 but got variant.\r\nTraceback (most recent call last):\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Data type mismatch at component 0: expected int64 but got variant.\r\n\t [[{{node IteratorFromStringHandleV2}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 46, in <module>\r\n    x = sess.run(next_element, feed_dict={handle: training_handle})\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 676, in run\r\n    run_metadata=run_metadata)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1171, in run\r\n    run_metadata=run_metadata)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1270, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1327, in run\r\n    run_metadata=run_metadata)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 1091, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Data type mismatch at component 0: expected int64 but got variant.\r\n\t [[node IteratorFromStringHandleV2 (defined at test.py:30) ]]\r\n\r\nCaused by op 'IteratorFromStringHandleV2', defined at:\r\n  File \"test.py\", line 30, in <module>\r\n    handle, training_ds.output_types, training_ds.output_shapes)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 288, in from_string_handle\r\n    output_shapes=output_structure._flat_shapes)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1596, in iterator_from_string_handle_v2\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/env/tf-1.13-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Data type mismatch at component 0: expected int64 but got variant.\r\n\t [[node IteratorFromStringHandleV2 (defined at test.py:30) ]]\r\n```\r\n", "comments": ["Related issue #29347", "I am able to reproduce the issue with tensorflow version 1.13.1 on Google Colab. Thanks!", "@gadagashwini Thanks. Looking forward to solutions.", "Setting the `output_classes` argument of `from_string_handle` to `training_ds.output_classes` will fix your issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30457\">No</a>\n", "@jsimsa It works, really appreciate your help.", "@guangyuyan please take a look at the documentation of [from_string_handle](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#from_string_handle). `output_classes` is a separate argument from `output_shapes`.", "it works, appreciate!"]}, {"number": 30456, "title": "Unable to use canned RNN Estimator", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not really (very close to stock example [here](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/RNNEstimator))\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Version 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip (from binary?)\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n`TypeError: Input must be a SparseTensor.` when using [RNNEstimator](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/RNNEstimator).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo errors when used correctly (maybe that is the issue).\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsequence_feature_colums = [tf.contrib.feature_column.sequence_numeric_column(\"test\")]\r\n\r\nestimator = tf.contrib.estimator.RNNEstimator(\r\n    head=tf.contrib.estimator.regression_head(),\r\n    sequence_feature_columns=sequence_feature_colums)\r\n\r\ndef input_fn_train():\r\n  dataset = tf.data.Dataset.from_tensor_slices(({\"test\": [0]}, [0]))\r\n  dataset = dataset.batch(1)\r\n  return dataset\r\n\r\nestimator.train(input_fn=input_fn_train, steps=1)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could reproduce the issue with TF Version 1.14.", "@arthurarg who worked on RNN esitmator", "Also, just FYI, contrib is going away in tf 2.0, and we are not actively fixing anything in contrib.", "The input tensor needs to be a sparse tensor, and you need to specify the\nnumber of cells. The following code should work:\n```\n\nsequence_feature_colums =\n[tf.contrib.feature_column.sequence_numeric_column(\"test\")]\n\nestimator = tf.contrib.estimator.RNNEstimator(\nnum_units=[3],\nhead=tf.contrib.estimator.regression_head(),\nsequence_feature_columns=sequence_feature_colums)\n\ndef input_fn_train():\ndataset = tf.data.Dataset.from_tensor_slices((\n{\"test\": tf.SparseTensorValue([[0, 0]], [0.], (1, 1))},\n[0]))\ndataset = dataset.batch(1)\nreturn dataset\n\nestimator.train(input_fn=input_fn_train, steps=1)\n```\n\nOn Wed, Jul 24, 2019 at 12:34 AM Qianli Scott Zhu <notifications@github.com>\nwrote:\n\n> @arthurarg <https://github.com/arthurarg> who worked on RNN esitmator\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30456?email_source=notifications&email_token=ACC37MAJZRMMLHZY576SVFTQA7LUVA5CNFSM4H6VHQYKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2VED7A#issuecomment-514474492>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACC37MEYBISXBZLEEQG3UCTQA7LUVANCNFSM4H6VHQYA>\n> .\n>\n", "@dippynark Did @arthurarg's solution helped you to resolve this issue? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30456\">No</a>\n"]}]