[{"number": 44118, "title": "Optimize GPU memory consumption: Decrease heap usage at the beginning of the training and allow GPU to use 100% fragmentation.", "body": "**System information**\r\n- Are you willing to contribute it (Yes/No): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu\r\n- TensorFlow installed from (source or binary): (in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI just use Tensorflow 2.3 to implement one paper, which has an official Pytorch version. Everything is fine except for the Batch_size of the training data, where the official pytorch could use `batch_size` of `32` with 8GPU, while I could only deploy `16` with the same GPUs and same settings of neural network. I tried to use tensorboard Profiler to check and optimize my training loop.\r\n\r\nHere is the Profiler output of 10 steps of my model training.\r\n![image](https://user-images.githubusercontent.com/28552583/96349804-8aab4480-10e4-11eb-8fdb-808798b5b999.png)\r\n![image](https://user-images.githubusercontent.com/28552583/96349883-0ad1aa00-10e5-11eb-894d-bb49763c15e8.png)\r\n\r\n\r\nYou can see that the image points out the peak heap usage occurs when the GradientTape of the last layer of the network asked for it. However, after this allocation of the GPU memory, the peak memory usage goes down from `7.41 GiBs` to around only `6 GiBs`, so I am wondering why TF2 will allocate so much heap usage at the beginning of each training step and will not use this part during the loop, and the difference between the heap and memory usage. Is there any way that I could optimize the heap allocation of the heap so that I could fit my `32 batch_size` to the graphic memory?\r\n\r\nI also noticed that the memory capacity of what TF Profiler shows is `10.96 GiBs`, which is only `90%` of Fragmentation. My GPU memory has got `12196 MiB` memory to use, which means that there are still some available space for the training. Some blogs' workarounds don't work, such as `tf.config.experimental.set_memory_growth(gpu, True)`. Thus, I am looking for a way to permit TF to use this part of the graphic memory, say `100%` fragmentation.\r\n\r\nI tried to use the following code as the documentation says, but I still failed:\r\n``` python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\nI am sure that if I solve the above two problems, I could increase my `batch_size` from `16` to `32`, since a lot of GPU memory was wasted before. Appreciate your help sincerely.\r\n\r\nI am using multi-gpu. So error callback is provided:\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 29, in <module>\r\n    main()\r\n  File \"main.py\", line 25, in main\r\n    trainer.train()\r\n  File \"/home/lz/potter/EDVR/trainers/train.py\", line 238, in train\r\n    loss, acc = self.train_epoch(epoch)\r\n  File \"/home/lz/potter/EDVR/trainers/train.py\", line 191, in train_epoch\r\n    loss, psnr = self.multi_train_step(batch_x, batch_y)\r\n  File \"/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1843, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1923, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 545, in call\r\n    outputs = execute.execute(\r\n  File \"/home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 3 root error(s) found.\r\n  (0) Resource exhausted:  OOM when allocating tensor with shape[4,256,360,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node StatefulPartitionedCall/conv2d_61/Conv2D}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[Identity_2/_190]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted:  OOM when allocating tensor with shape[4,256,360,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node StatefulPartitionedCall/conv2d_61/Conv2D}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (2) Resource exhausted:  OOM when allocating tensor with shape[4,256,360,640] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node StatefulPartitionedCall/conv2d_61/Conv2D}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[GroupCrossDeviceControlEdges_0/StatefulPartitionedCall/Adam/Adam/update_1_1/Const/_155]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_multi_train_step_36442]\r\n\r\nFunction call stack:\r\nmulti_train_step -> multi_train_step -> multi_train_step\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to optimize the consumption of GPU memory.", "comments": ["> I also noticed that the memory capacity of what TF Profiler shows is `10.96 GiBs`, which is only `90%` of Fragmentation.\r\n\r\nWe leave 6% of the GPU memory free for libraries like cuDNN and cuBLAS, that could explain part of it, but not all of it.  @imintz any idea?", "@sanjoy Thanks for your reply! Here is a solution to use the rest part of GPU memory I found on the Internet:\r\n``` python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\n    if gpus:\r\n        # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\r\n        try:\r\n            for gpu in gpus:\r\n                tf.config.experimental.set_virtual_device_configuration(\r\n                    gpu,\r\n                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12195)])\r\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n        except RuntimeError as e:\r\n            # Virtual devices must be set before GPUs have been initialized\r\n            print(e)\r\n```\r\nI just set the memory_limit to 12195 MB directly and the TF will use the full portion of my GPU.\r\n"]}, {"number": 44099, "title": "Feature request: ResNet34", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\ncreate ResNet34 similar to https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50\r\n\r\n**Will this change the current api? How?**\r\n\r\nI don't think so. Hopefully it's an easy adaption of ResNet50 code.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople like me who are new to DL and Transfer Learning. With ResNet34 being smaller that ResNet50 I can iterate and learn quicker.\r\n\r\n**Any Other info.**\r\n\r\nI believe there are a few implementations outside of tf:\r\nhttps://github.com/calmisential/TensorFlow2.0_ResNet\r\nhttps://github.com/taki0112/ResNet-Tensorflow\r\nhttps://github.com/qubvel/classification_models", "comments": ["This issue will be closed once [this](https://github.com/keras-team/keras/pull/16363) PR merge. Thanks!"]}, {"number": 44090, "title": "How to remove unused operators from tensorflowlite_c.so to reduce it's size?", "body": "As in the title. How can one remove unused operators (automatically or manually) in the build of C API library? There's nice process regarding Select Ops, where one has to pass models during the build, but there isn't anything similar present for .so. There is section about .aar (https://www.tensorflow.org/lite/guide/reduce_binary_size), but it's not my use case.", "comments": ["@MrSherish If you're comfortable writing your own `BUILD` files, you could use the `tflite_custom_cc_library` rule to create a library with only the op kernels used for your provided model file.\r\n\r\nSee: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/build_def.bzl#L772\r\n\r\n@thaink Would there be a more convenient way?", "If you want to use the C++ API then extracting the aar file you will have .so file inside it.\r\n\r\nFor C API, I am afraid the rule is not working for the C api yet.\r\nFor C api, currently you need to create a new custom TfLiteInterpreter using TfLiteInterpreterCreateWithSelectedOps in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api_experimental.h", "Right now my model is not using Select Ops at all, so I do not need C++ API at all, or experimental API from C. What I am trying to achieve is reducing size of library in C, removing unused built-in ops.", "I am working on a script to do so.\r\nAt the moment you can try removing unused op in tensorflow/lite/kernels/register.cc\r\nand rebuild the tensorflow/lite/experimental/ios:TensorFlowLiteC_framework", "This should be possible now, @thaink can you provide a link to the docs/guidance?", "You can follow the guide in https://www.tensorflow.org/lite/guide/reduce_binary_size#advanced_usages_build_custom_cc_shared_libraries\r\nto reduce the binary size."]}, {"number": 44073, "title": "`size` argument of TensorArray works only when specified by pythonic int but tf.Tensor doesn't", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu`\r\n- TensorFlow installed from (source or binary):  `(in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple`\r\n- TensorFlow version (use command below): `v2.3.0-rc2-23-gb36436b087 2.3.0`\r\n- Python version: `3.8`\r\n- CUDA/cuDNN version: `conda install cudatoolkit=10.1 cudnn=7.6.5`\r\n- GPU model and memory:  ` 4 x Titan xp 12GB`\r\n\r\n**Describe the current behavior**\r\n\r\nReference issue: #43698 \r\nThanks for @saxenasaurabh 's answer. I could initialized the `TensorArray` with a fixed size. However, when I tried to specify the size of the `TensorArray` with tf.Tensor from `tf.shape`, the length of the tensor after stacked together will be None:\r\n```\r\nemb.shape: TensorShape([2, 3, 180, 320, 32])\r\ncor_l.shape: TensorShape([None, 2, 180, 320])\r\ncor_prob1.shape: TensorShape([2, None, 180, 320])\r\ncor_prob2.shape: TensorShape([2, None, 180, 320, 1])\r\ncor_prob3.shape: TensorShape([2, None, 180, 320, 32])\r\ncor_prob4.shape: TensorShape([2, 180, 320, None, 32])\r\ncor_prob5.shape: TensorShape([2, 180, 320, None])\r\naligned_fea.shape: TensorShape([2, 180, 320, 3, 32])\r\naligned_fea.shape: TensorShape([2, 180, 320, 96])\r\n```\r\nParts of the code are as following shows:\r\n``` python\r\naligned_fea_shape = tf.shape(aligned_fea) # B, N, H, W, C\r\nB = aligned_fea_shape[0]\r\nN = aligned_fea_shape[1]\r\nH = aligned_fea_shape[2]\r\nW = aligned_fea_shape[3]\r\nC = aligned_fea_shape[4]\r\n\r\n####\r\nsome other codes omit here\r\n####\r\n\r\nemb = tf.reshape(emb, [B, N, H, W, -1])\r\ntf.print(\"emb.shape:\", emb.shape)\r\n\r\ncor_l = tf.TensorArray(dtype=tf.float32, size=N) # TENSORFLOW BUG HERE !\r\ndef cond(i, N, input, arr):\r\n    return tf.less(i, N)\r\ndef body(i, N, input, arr):\r\n    emb_nbr = input[:, i, :, :, :]\r\n    cor_tmp = tf.reduce_sum(emb_nbr * emb_ref, axis=3) # B, H, W\r\n    arr = arr.write(i, cor_tmp)\r\n    i = tf.add(i, 1)\r\n    return i, N, input, arr\r\n_, _, _, cor_l = tf.while_loop(cond, body, [0, N, emb, cor_l]) # N * (B, H, W)\r\ncor_l = cor_l.stack() # N, B, H, W\r\ntf.print(\"cor_l.shape:\", cor_l.shape)\r\n\r\ncor_l = tf.transpose(cor_l, [1, 0, 2, 3]) # B, N, H, W\r\n\r\ncor_prob = tf.sigmoid(cor_l)  # B, N, H, W\r\ntf.print(\"cor_prob1.shape:\", cor_prob.shape)\r\n\r\ncor_prob = tf.expand_dims(cor_prob, axis=4) # B, N, H, W, 1\r\ntf.print(\"cor_prob2.shape:\", cor_prob.shape)\r\n\r\ncor_prob = tf.tile(cor_prob, [1, 1, 1, 1, C]) # B, N, H, W, C\r\n\r\ncor_prob = tf.transpose(cor_prob, [0, 2, 3, 1, 4]) # B, H, W, N, C\r\ncor_prob = tf.reshape(cor_prob, [B, H, W, -1]) # B, H, W, NC\r\n\r\naligned_fea = tf.transpose(aligned_fea, [0, 2, 3, 1, 4])  # [B, H, W, N, C]\r\ntf.print(\"aligned_fea.shape:\", aligned_fea.shape)\r\n\r\naligned_fea = tf.reshape(aligned_fea, [B, H, W, -1]) * cor_prob\r\ntf.print(\"aligned_fea.shape:\", aligned_fea.shape)\r\n```\r\nLook into the comment where `TENSORFLOW BUG` points out:\r\nIf `size` is specified by N, which is tf.Tensor, the output will be None. Otherwise, the size is designated by python int, eg. `cor_l = tf.TensorArray(dtype=tf.float32, size=self.nframe)` where self.nframe is pythonic int, everything is fine and the first size of the tensor will be fixed. The output is as follows:\r\n```\r\nemb.shape: TensorShape([2, 3, 180, 320, 32])\r\ncor_l.shape: TensorShape([3, 2, 180, 320])\r\ncor_prob1.shape: TensorShape([2, 3, 180, 320])\r\ncor_prob2.shape: TensorShape([2, 3, 180, 320, 1])\r\ncor_prob3.shape: TensorShape([2, 3, 180, 320, 32])\r\ncor_prob4.shape: TensorShape([2, 180, 320, 3, 32])\r\ncor_prob5.shape: TensorShape([2, 180, 320, 96])\r\naligned_fea.shape: TensorShape([2, 180, 320, 3, 32])\r\naligned_fea.shape: TensorShape([2, 180, 320, 96])\r\n```\r\n\r\nThus I am wondering if this is a bug of TensorFlow since the documentation says:\r\n```\r\nsize: (optional) int32 scalar `Tensor`: the size of the TensorArray. Required if handle is not provided.\r\n```\r\n\r\nPlease feel free to contact me if more testing code is required.", "comments": ["@Harrypotterrrr \r\nPlease provide with stand alone indented code to replicate the issue or if possible share a colab gist with the error reported.", "@Saduf2019 Here is a sample code to reproduce the bug I mentioned, and I found the bug occurs only when deploying the model to multi GPUs: \r\n``` python\r\nimport os\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\r\nimport tensorflow as tf\r\n\r\nclass TestModel(tf.keras.Model):\r\n\r\n    def __init__(self, N):\r\n        super(TestModel, self).__init__()\r\n        self.tAtt_1 = tf.keras.layers.Conv2D(4, (3, 3), (1, 1), \"same\")\r\n        self.tAtt_2 = tf.keras.layers.Conv2D(4, (3, 3), (1, 1), \"same\")\r\n        self.nframes = N\r\n        self.center = self.nframes // 2\r\n\r\n    def __call__(self, aligned_fea):\r\n\r\n        aligned_fea_shape = tf.shape(aligned_fea) # B, N, H, W, C\r\n        B = aligned_fea_shape[0]\r\n        N = aligned_fea_shape[1]\r\n        H = aligned_fea_shape[2]\r\n        W = aligned_fea_shape[3]\r\n        C = aligned_fea_shape[4]\r\n\r\n        # for i in range(self.nframe):\r\n        #     aligned_fea = aligned_fea.write(i, x)\r\n        #     tf.print(\"alienged_fea:\", aligned_fea.size())\r\n\r\n        emb_ref = self.tAtt_1(aligned_fea[:, self.center, :, :, :])  # B, H, W, C\r\n        emb = tf.reshape(aligned_fea, [-1, H, W, C])  # BN, H, W, C\r\n        emb = self.tAtt_2(emb)\r\n        emb = tf.reshape(emb, [B, N, H, W, -1])\r\n\r\n        cor_l = tf.TensorArray(dtype=tf.float32, size=N) # TENSORFLOW BUG HERE. REPLACE N with self.nframes, everything will be OK\r\n\r\n        def cond(i, N, input, arr):\r\n            return tf.less(i, N)\r\n\r\n        def body(i, N, input, arr):\r\n            emb_nbr = input[:, i, :, :, :]\r\n            cor_tmp = tf.reduce_sum(emb_nbr * emb_ref, axis=3)  # B, H, W\r\n            arr = arr.write(i, cor_tmp)\r\n            i = tf.add(i, 1)\r\n            return i, N, input, arr\r\n\r\n        _, _, _, cor_l = tf.while_loop(cond, body, [0, N, emb, cor_l])  # N * (B, H, W)\r\n\r\n        tf.print(\"aliged_fea shape:\", cor_l.size())\r\n\r\n        t = cor_l.stack()\r\n        tf.print(\"Stack tensor shape:\", t.shape)\r\n\r\n        return t\r\n\r\n\r\ngpu_list = [f\"/gpu:{i}\" for i in range(2)]\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=gpu_list, cross_device_ops=tf.distribute.NcclAllReduce())\r\nnframes = 5\r\n\r\ndef train_step(model, x):\r\n    output = model(x)\r\n\r\n@tf.function\r\ndef multigpu_train_step(model, x):\r\n    mirrored_strategy.run(train_step, args=(model, x))\r\n\r\nvalues = tf.random.normal([16,5,32,32,3])\r\nsample_dataset = tf.data.Dataset.from_tensor_slices(values)\r\nsample_dataset = sample_dataset.batch(4)\r\nwith mirrored_strategy.scope():\r\n    model = TestModel(nframes)\r\n    sample_dataset = mirrored_strategy.experimental_distribute_dataset(sample_dataset)\r\n    for x in sample_dataset:\r\n        multigpu_train_step(model, x)\r\n```\r\nThis is part of the attention implementation, so just ignore the meaning of the code.", "Hi @Harrypotterrrr, you mentioned that you were only able to repro this with multiple GPUs? I'm seeing the the following output even with a CPU runtime in colab. Do you see the same?\r\n\r\n```\r\naliged_fea shape: 5\r\nStack tensor shape: TensorShape([None, 4, 32, 32])\r\naliged_fea shape: 5\r\nStack tensor shape: TensorShape([None, 4, 32, 32])\r\naliged_fea shape: 5\r\nStack tensor shape: TensorShape([None, 4, 32, 32])\r\naliged_fea shape: 5\r\nStack tensor shape: TensorShape([None, 4, 32, 32])\r\n```\r\n\r\nSomething else to note here is that you do get the first dimension evaluated as 5 when you comment out @tf.function. In that case N becomes an EagerTensor `tf.Tensor(5, shape=(), dtype=int32)` with a value, instead of a graph tensor `Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)`. So you can pass an eager tensor to the size argument.", "@nikitamaia Hi, Appreciate your reply! I use the same code as above mentioned and just set a environment variable as`os.environ['CUDA_VISIBLE_DEVICES'] = '-1'`, which I think tensorflow will only use CPU. The output could identify the first dimension of the tensor:\r\n```\r\naliged_fea shape: 5\r\nStack tensor shape: TensorShape([5, 2, 32, 32])\r\naliged_fea shape: 5\r\nStack tensor shape: TensorShape([5, 2, 32, 32])\r\nWARNING:tensorflow:From /home/lz/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\nInstructions for updating:\r\nuse `tf.profiler.experimental.stop` instead.\r\n```\r\nMy version of TensorFlow 2 and other settings of the system is shown in the first block of this issue.\r\nCould you please refer to another issue #43698 I put forward one month ago. It is still a problem related to `TensorArray`", "Moreover, I am always looking for a good way to dynamically stack or concatenate some tensors together in an efficient way. (The intuition of this kind of operation is to combine several temporal features together to fid to the next layer of the network). I use Tensorboard Profiler to analyze the runtime of each step during the network forwarding, and it points out that the step such as `tf.stack` consumes extremely large amount of time compared to convolution. So could you give me some suggestions on this kind of operations so that I could do it more efficiently?", "Just trying to make sure we isolate where the issue is first. Please [see this gist](https://colab.research.google.com/gist/nikitamaia/544c15416abf576acce59d95fd8abd14/untitled14.ipynb), I am using the default CPU runtime and also the default distribution strategy. Given these setting I'm still seeing the None first dimension. Please let me know if I'm missing something.", "There is no missing. The gist is complete", "This is expected, partly due to a known and admittedly surprising behavior of `tf.shape`. `tf.shape` always returns a `Tensor`, even when the shape is statically known.\r\n\r\nNote that passing a Tensor as `size` argument to `TensorArray` indicates that the size is dynamic and not statically known. Since the shape of the stacked result represents the statically known value, it is unknown when the original `size` was a Tensor.\r\n\r\nThe simplest workaround is to use `N = aligned_fea.shape[1].value`. That will only work if the shape of `aligned_fea` is known, but I believe that's the case? At any rate, you could also handle dynamic shapes with something like this:\r\n\r\n```\r\nN = aligned_fea.shape[1]\r\nif N is None:\r\n  # Dynamc shape, revert to tf.shape; N will be a Tensor\r\n  N = tf.shape(aligned_fea)[1]\r\n# Static shape, N is a Python int\r\n```", "@mdanatg Thanks for your suggestion. I knew it could be solved by recording the shape of the tensor before the `TensorArray` loop, and after performing the `stack` operation, `reshape` could deal with the `None` problem well. However, it is very confusing for the user to get such kind of Tensor with `None` at the first dimension.\r\nMoreover, what I want to do is more `pythonic` using TF2. Look at the following code:\r\n```\r\ncor_l = []\r\nfor i in range(self.nframes):\r\n    emb_nbr = emb[:, i, :, :, :]\r\n    cor_tmp = tf.reduce_sum(emb_nbr * emb_ref, axis=3)  # B, H, W\r\n    cor_l.append(cor_tmp)\r\ncor_l = tf.stack(cor_l)  # N, B, H, W\r\n```\r\nIn Autograph mode, which means the training loop is in the decorator of `@tf.function`, the user can not iteratively operates the tensor in the above way, which is much more intuitive and pythonic and Pytorch supports.\r\nI am also using TF Profiler to test if `TensorArray` is the best way to deal with such loop operations in an efficient way, while the result shows that the running time is exactly right limited by this `TensorArray stack` step. The reference issue has been pushed to #44118\r\nThis problem has troubled me for a long time. I could make my total repository public to you if you are interested in the bug/tricky problem I pointed out. The coding work is with another Googler working in Mountain View.", "@Harrypotterrrr I agree, and your input helps push toward resolving these issues sooner.", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/6ddec09e2911381c8e8f04c7659bf56b/44073.ipynb). Thanks!"]}, {"number": 44061, "title": "TensorFlow C API vs. TensorRT: \"Op type not registered 'TRTEngineOp' in binary running on ...\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): docker image (binary)\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GTX 1660 Ti, mobile, 6GB\r\n\r\n**Describe the current behavior**\r\n- I have a SavedModel that I convert to a TensorRT-optimized model using trt.TrtGraphConverterV2\r\n- I try to load the resulting model via the TensorFlow C API\r\n- My C++ program displays the following error and exits:\r\n  Op type not registered 'TRTEngineOp' in binary running on [docker container]. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) 'tf.contrib.resampler' should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.'\r\n\r\n**Describe the expected behavior**\r\nI expect the model to be loaded and to be able to use it for inference, as I can with a non-TensorRT-optimized model.\r\n\r\n**Standalone code to reproduce the issue**\r\nI'm happy to post code but it seems all you have to do is load a model with TF_LoadSessionFromSavedModel (which doesn't exit) and then, in my case, when I call TF_GraphOperationByName, it prints the error and exits.\r\n\r\n**Other info / logs**\r\nIf you google this error message, a lot of people have encountered this problem over the last 2 years.\r\nSome people were able to solve it with much earlier versions of TensorFlow by calling:\r\n  TF_LoadLibrary(\"_trt_engine_op.so\", status);\r\nBut this does not work for me, the docker image doesn't even seem to have this .so file anywhere, and other people online claim the call is unnecessary.\r\nIt seems that TensorRT has been folded into TensorFlow version 2, but maybe somebody forgot to put it into the C API libraries?\r\n", "comments": ["I have the same exact problem with tf 2.4, cuda 11.2, trt 7 with C++ api... did someone solve it?", "@jappoz \r\nFor cuda 11.2 please use tf 2.5 or nightly and let us know.", "> @jappoz\r\n> For cuda 11.2 please use tf 2.5 or nightly and let us know.\r\n\r\nDone. Switching to tf 2.5.0-rc1 did not solve the problem."]}, {"number": 44058, "title": " go get github.com/tensorflow/tensorflow/tensorflow/go throwing an error", "body": "Getting below error while installing tensorflow go module\r\n\r\n```\r\n-bash-4.2$ export LIBRARY_PATH=$LIBRARY_PATH:/home/gouser/lib/\r\n-bash-4.2$ go get github.com/tensorflow/tensorflow/tensorflow/go\r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::length_error::length_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::__throw_out_of_range_fmt(char const*, ...)@GLIBCXX_3.4.20'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::domain_error::domain_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `typeinfo for std::_V2::error_category@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_notify_all(unsigned int*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `lgammaf@GLIBC_2.23'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::default_error_condition(int) const@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::equivalent(int, std::error_condition const&) const@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::~error_category()@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::generic_category()@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `__cxa_throw_bad_array_new_length@CXXABI_1.3.8'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::invalid_argument::invalid_argument(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::equivalent(std::error_code const&, int) const@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::system_category()@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::out_of_range::out_of_range(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_wait_until(unsigned int*, unsigned int, bool, std::chrono::duration<long, std::ratio<1l, 1l> >, std::chrono::duration<long, std::ratio<1l, 1000000000l> >)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::underflow_error::underflow_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)())@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::range_error::range_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::overflow_error::overflow_error(char const*)@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `operator delete(void*, unsigned long)@CXXABI_1.3.9'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::__future_base::_State_baseV2::_Make_ready::_M_set()@GLIBCXX_3.4.21'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `lgamma@GLIBC_2.23'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `operator delete[](void*, unsigned long)@CXXABI_1.3.9'\r\n/home/gouser/lib/libtensorflow.so: undefined reference to `std::_V2::error_category::_M_message(int) const@GLIBCXX_3.4.21'\r\n```\r\n\r\n**System information**\r\n- CentOS Linux release 7.6.1810 (Core):\r\n- Python version:\r\n-bash-4.2$ python --version\r\nPython 2.7.6\r\n-bash-4.2$ python3 --version\r\nPython 3.6.8\r\n\r\n\r\n\r\n", "comments": ["@shrikantkeni \r\nplease refer to this issue with same error reported #30111 [link](https://github.com/google/tink/issues/186), [link1](https://www.gitmemory.com/issue/tensorflow/tensorflow/30111/505720326)", "I have executed the steps mentioned in 30111 by cloning source from git. but compilation of tensorflow using bazel failed with beow exception \r\n\r\n```\r\nbazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\n\r\n\r\n\r\nuild_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nINFO: Found applicable config definition build:opt in file /root/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/tools/lib_package:libtensorflow_test (215 packages loaded, 20060 targets configured).\r\nINFO: Found 1 test target...\r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/sandbox\r\nERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/com_google_protobuf/BUILD:412:1: C++ compilation of rule '@com_google_protobuf//:protoc' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-std=c++14'\r\nTarget //tensorflow/tools/lib_package:libtensorflow_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 504.836s, Critical Path: 17.26s\r\nINFO: 20 processes: 20 local.\r\nFAILED: Build did NOT complete successfully\r\n//tensorflow/tools/lib_package:libtensorflow_test               FAILED TO BUILD\r\n```", "@Saduf2019  \r\nhttps://www.tensorflow.org/install/lang_go\r\nI have also followed the steps mentioned in this link to install go packages, but that also throwing an error\r\nI just want to understand whether the steps mentioned in this link is up to date?", "@shrikantkeni \r\nPlease let us know the tf version you are referring to.", "I just cloned latest version \r\n\r\ngo get github.com/tensorflow/tensorflow/tensorflow/go\r\n\r\nand \r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n\r\n", "https://www.tensorflow.org/install/lang_c\r\n\r\nI have also downloaded the library from above link and  tried compiling sample C code \r\n\r\n\r\n[root@localhost ~]# gcc hello_tf.c -I /usr/local/include/tensorflow/c/ -L /usr/local/lib/ -ltensorflow -o hello_tf\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::length_error::length_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::__throw_out_of_range_fmt(char const*, ...)@GLIBCXX_3.4.20'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::logic_error::logic_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::domain_error::domain_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `typeinfo for std::_V2::error_category@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_notify_all(unsigned int*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `lgammaf@GLIBC_2.23'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::error_category::default_error_condition(int) const@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::error_category::equivalent(int, std::error_condition const&) const@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::error_category::~error_category()@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::generic_category()@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `__cxa_throw_bad_array_new_length@CXXABI_1.3.8'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::invalid_argument::invalid_argument(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::error_category::equivalent(std::error_code const&, int) const@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::system_category()@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::out_of_range::out_of_range(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_wait_until(unsigned int*, unsigned int, bool, std::chrono::duration<long, std::ratio<1l, 1l> >, std::chrono::duration<long, std::ratio<1l, 1000000000l> >)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::underflow_error::underflow_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)())@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::range_error::range_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::overflow_error::overflow_error(char const*)@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `operator delete(void*, unsigned long)@CXXABI_1.3.9'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::__future_base::_State_baseV2::_Make_ready::_M_set()@GLIBCXX_3.4.21'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `lgamma@GLIBC_2.23'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `operator delete[](void*, unsigned long)@CXXABI_1.3.9'\r\n/usr/local/lib//libtensorflow.so: undefined reference to `std::_V2::error_category::_M_message(int) const@GLIBCXX_3.4.21'\r\ncollect2: error: ld returned 1 exit status\r\n\r\n\r\n", "Please let me know if any specific linux version which I have to use to make this work.", "@jhseu Do you know who should be notified about golang issues like this?", "My guess is this is caused by an old libstdc++, probably from an older Linux distribution. For Ubuntu, I think 16.04 or later should have the right version."]}, {"number": 44046, "title": "dataset windows don't work in batches", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes (see below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 64 bit\r\n- TensorFlow installed from (source or binary): pip install tensorflow, pip install tensorflow-nightly\r\n- TensorFlow version (use command below):Currently on nightly - v1.12.1-43485-g0af213f96c 2.4.0-dev20201012 , but also happens on 2.3 release.\r\n- Python version: Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: Tried 10.1 and 11.0\r\n- GPU model and memory:  GeForce RTX 2080 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.23GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 357.69GiB/s\r\n\r\n**Describe the current behavior**\r\n\r\nIf I create a dataset of windowed data, then batch it up using `tf.data.Dataset.window(5,1,1,True).batch(10,True)` the output of iterating the resulting dataset doesn't evaluate as a tensor and I can't run `model.fit()` on it. It throws an exception:\r\n\r\n`TypeError: Inputs to a layer should be tensors. Got: <tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x000001BC82925250>\r\n`\r\n\r\n**Describe the expected behavior**\r\n\r\nThis not working basically breaks the tf.data loading mechanism for windowed data for me. I want to be able to create a preprocessing pipeline that does the following:\r\n1) load some data\r\n2) apply a sliding window to the data\r\n3) batch the data\r\n4) prefetch the data\r\n5) run model.fit\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# make a source dataset\r\nsource_data=tf.data.Dataset.range(100)\r\n# take two windows of different size onto the source dataset as\r\n# our inputs and outputs\r\n# and batch them up into 20 data points\r\ninputs=source_data.window(10,1,1,True).batch(20,True)\r\ntargets=source_data.window(5,1,1,True).batch(20,True)\r\n# zip this into a single training data set\r\ntraining_set=tf.data.Dataset.zip((inputs,targets))\r\n# a very simple network that takes input of [batch_size,10] and outputs [batch_size,5]\r\ninLayer=tf.keras.layers.Input(shape=(10,),batch_size=20)\r\noutLayer=tf.keras.layers.Dense(5)(inLayer)\r\nmodel=tf.keras.Model(inputs=inLayer,outputs=outLayer)\r\nmodel.compile(optimizer='adam',loss='mse')\r\n# this fit call fails\r\nmodel.fit(training_set)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"d:/machinelearning/predictiveaudio/hellogoodbye/testtf.py\", line 20, in <module>\r\n    model.fit(training_set)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1081, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2976, in _get_concrete_function_internal_garbage_collected  \r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3371, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3206, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:790 train_function  *\r\n        return step_function(self, iterator)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:780 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1268 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2734 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3355 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:773 run_step  **\r\n        outputs = model.train_step(data)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:739 train_step\r\n        y_pred = self(x, training=True)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:989 __call__\r\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n    C:\\Users\\micro\\miniconda3\\envs\\tf23\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:201 assert_input_compatibility\r\n        raise TypeError('Inputs to a layer should be tensors. Got: %s' % (x,))\r\n\r\n    TypeError: Inputs to a layer should be tensors. Got: <tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x000001BC82925250>\r\n```\r\n", "comments": ["I have tried in colab with TF nightly version(`2.4.0-dev20201016`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/fed173fce51f810a423a538a563351e9/untitled460.ipynb).Thanks!", "I think you need something like: `source_data.window(10,1,1,True).map(lambda ds: ds.batch(20)))))`.\r\n`window` maps to datasets, not to tensors, so you can't immediatly batch after `window`.\r\n(I'm not with the tensorflow team)", "@amitport that would batch each window rather than batching the windowed data points. Batch is a dataset operation, so it should work on all datasets including those returned by window.\r\n\r\nThere is a nasty hack workaround using flat_map, but it copies all the data and on my machine it made the preprocessing way slower than with the old keras.sequence thing.", "so your batches are groups of datasets (each window is a dataset), I'm not sure what you're trying to do, but model fit needs to get something that produces actual tensor batches", "The batches should contain a series of values which are a sliding window over the underlying dataset. i.e. if I have a dataset [1,2,3,4,5,6,7,8,9,10], window size 3, batch size 2, I want tensors to come out like:\r\n[ [ 1,2,3],[2,3,4] ]\r\n[ [3,4,5],[4,5,6] ]\r\n\r\nThis is surely what window is for? It is to transform a dataset into another dataset, just like batch is? \r\nI can't see what window is useful for if it doesn't work to window time series data.\r\n\r\nEssentially, it seems like when you are producing datasets using dataset transformations, a basic minimum expectation is that when you transform a dataset, the resulting transformed dataset should itself be able to generate input to model.fit?\r\n\r\nMaybe it needs a different window method, but realistically, there's no reason why it shouldn't be possible to have a transformation which is used by model.fit which goes from a collection of N datasets of size AxB to tensors in the form NxAxB, without having to do flat_map on the structure.\r\n\r\nThe horrible workaround is to do a flat_map then batch twice. Which at least on my dataset seems slow.\r\n ```\r\nd=tf.data.Dataset.range(10)\r\nw=d.window(3,1,1)\r\nw = w.flat_map(lambda x:x).batch(3).batch(2) #flat map but do nothing to values, then batch twice to get what model.fit expects from windowed datasets\r\n```", "same problem faced as @joemarshall , isn't there any provision to directly make batches of windows?", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2c16d831ea65ea739a9f75e868268071/44046.ipynb). Thanks!", "Facing this exact issue, are there any updates on this? Timeseries windowing is a quite important workflow.", "@ymodak any updates on this? Unfortunately, flat_map hack does not work with dict.", "Same here. At least what is the temporary solution? ", "> Same here. At least what is the temporary solution?\r\n\r\nMy workaround solution:\r\n\r\n```\r\nkeys=('a', 'b', 'c')\r\nwindow_size = 2\r\nds = tf.data.Dataset.from_tensor_slices(\r\n    {'a': [1, 2, 3],\r\n     'b': [4, 5, 6],\r\n     'c': [7, 8, 9]})\r\n\r\n# map dict to Dataset\r\nds = ds.flat_map(lambda features_dict: tf.data.Dataset.zip(tuple([tf.data.Dataset.from_tensors(v) for v in features_dict.values()])))\r\n# get properly windowed Dataset\r\nds = ds.window(window_size, shift=1, drop_remainder=True)\r\nds = ds.flat_map(lambda *features_ds: tf.data.Dataset.zip(tuple([f.batch(window_size) for f in features_ds])))\r\n# map back to dict\r\nds = ds.map(lambda *features_ds_tuple: {key: f for key, f in zip(keys, features_ds_tuple)})\r\n\r\nfor v in ds:\r\n    print(f\"iteration: \",  v)\r\n```"]}, {"number": 44032, "title": "Densenet (Floating point models) reported accuracy is much lower than the original paper", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/guide/hosted_models#floating_point_models\r\n\r\n## Description of issue (what needs changing):\r\nThe accuracy of Densenet reported in the URL is much lower than the one reported in the original paper.\r\n\r\n### Clear description\r\nIn the original paper (https://arxiv.org/abs/1608.06993), the performance is as follows:\r\nTop-1: 74.98%; Top-5: 92.29% (Densenet-121)\r\nHowever, the performance reported in the URL is as follows:\r\nTop-1: 64.2%; Top-5: 85.6% (Densenet-?)\r\nAnd there is no information about the depth of the provided densenet model.\r\n\r\nIs there any modification for this model?\r\nOr they are typos?\r\n", "comments": ["@cmhungsteve \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or a colab gist with the issue.", "It's not about the code. I guessed it's about the typos on the website.\r\n\r\nI found that the accuracy numbers of Densenet on the website (https://www.tensorflow.org/lite/guide/hosted_models#floating_point_models) are very different from the Densenet paper (https://arxiv.org/abs/1608.06993).\r\nI wonder the reason for the inconsistency.\r\nThank you.", "Apologies for the delayed response, this completely missed my radar.\r\n\r\nI actually don't know the origin of the TFLite-reported DenseNet numbers, @miaout17 are you familiar with those numbers? Are we still tracking it? It seems to be fairly dated, we might consider removing it completely."]}, {"number": 44016, "title": "TFLite not respecting ByteBuffer's limit", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android (Fire OS 5)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Kindle Fire HD 8\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): newest tflite 0.0.0-nightly as of 14.10.20\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have a lot of data in ByteBuffer that I want to run in chunks (for streaming the model). However, when passing a slice of ByteBuffer with limit set to less than the underlying array, TFLite errors out here:\r\nhttps://github.com/tensorflow/tensorflow/blob/76c685252469e800aa4486c50f6a390f26b806a7/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L443\r\n\r\n**Describe the expected behavior**\r\nI expect TFLite to take the limit() of the buffer as its size, not just the capacity():\r\nhttps://github.com/tensorflow/tensorflow/blob/76c685252469e800aa4486c50f6a390f26b806a7/tensorflow/lite/java/src/main/java/org/tensorflow/lite/Tensor.java#L441\r\n\r\n**Standalone code to reproduce the issue**\r\n```java\r\nByteBuffer bb = getBuffer();\r\nBuffer sliced = bb.slice().limit(8);\r\ntflite.run(sliced, output);\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.xxx.xxx/com.xxx.xxx.XXX}: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (xxx) with 16000 bytes from a Java Buffer with 650880 bytes.\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2473)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2541)\r\n        at android.app.ActivityThread.access$800(ActivityThread.java:160)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1321)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:135)\r\n        at android.app.ActivityThread.main(ActivityThread.java:5597)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at java.lang.reflect.Method.invoke(Method.java:372)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:984)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:779)\r\n     Caused by: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (xxx) with 16000 bytes from a Java Buffer with 650880 bytes.\r\n        at org.tensorflow.lite.Tensor.throwIfSrcShapeIsIncompatible(Tensor.java:444)\r\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:189)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:159)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:319)\r\n        at com.xxx.xxx.XXX.xxx(XXX.java:75)\r\n        at com.xxx.xxx.XXX.onCreate(XXX.java:30)\r\n        at android.app.Activity.performCreate(Activity.java:6010)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1122)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2426)\r\n```", "comments": ["@Acrobot \r\nCan you please provide us with a colab gist with the error reported.", "> @Acrobot\r\n> Can you please provide us with a colab gist with the error reported.\r\n\r\nHi Saduf, I'm unfortunately not really sure what you are asking about. The issue manifests itself in runtime in an Android application. I'm not sure that colab supports that?\r\n\r\nYou can reproduce this issue by taking a model that has a fixed input size and trying to run the inference with a slice of (a bigger) ByteBuffer, with limit set to the model input size.\r\n\r\nAs an example:\r\n* model has a 1D input with width=40\r\n* you have a ByteBuffer that has capacity=80\r\n* you create a slice of that ByteBuffer and set the limit=40 (in other words, only the 0-39 range should be considered)\r\n* you pass that limited slice to TFLite's interpreter run() option\r\n* you can observe the error that you tried to copy to a TensorFlowLite tensor with 40 bytes from a Java buffer with 80 bytes (the limit on the slice is not considered)"]}, {"number": 44005, "title": "ValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table", "body": "tensorflow 2.3.1\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run ```! pip install -U tensorflow```  at the begin and run ```! saved_model_cli show --dir 'my_pet_classifier' --all``` at the end of this [document](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)'s colab , it raises error :\r\n\r\n```\r\n2020-10-14 08:20:39.291886: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['Age'] tensor_info:\r\n        dtype: DT_INT64\r\n        shape: (-1, 1)\r\n        name: serving_default_Age:0\r\n    inputs['Breed1'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Breed1:0\r\n    inputs['Color1'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Color1:0\r\n    inputs['Color2'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Color2:0\r\n    inputs['Fee'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_Fee:0\r\n    inputs['FurLength'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_FurLength:0\r\n    inputs['Gender'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Gender:0\r\n    inputs['Health'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Health:0\r\n    inputs['MaturitySize'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_MaturitySize:0\r\n    inputs['PhotoAmt'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: serving_default_PhotoAmt:0\r\n    inputs['Sterilized'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Sterilized:0\r\n    inputs['Type'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Type:0\r\n    inputs['Vaccinated'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1, 1)\r\n        name: serving_default_Vaccinated:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['dense_1'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\n2020-10-14 08:20:41.728805: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-14 08:20:41.738107: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-10-14 08:20:41.738155: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (1a8174b158ac): /proc/driver/nvidia/version does not exist\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/saved_model_cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 1185, in main\r\n    args.func(args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 715, in show\r\n    _show_all(args.dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 307, in _show_all\r\n    _show_defined_functions(saved_model_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/saved_model_cli.py\", line 187, in _show_defined_functions\r\n    trackable_object = load.load(saved_model_dir)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 603, in load\r\n    return load_internal(export_dir, tags, options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 633, in load_internal\r\n    ckpt_options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 131, in __init__\r\n    self._restore_checkpoint()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\", line 330, in _restore_checkpoint\r\n    load_status = saver.restore(variables_path, self._checkpoint_options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 1320, in restore\r\n    checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\", line 209, in restore\r\n    restore_ops = trackable._restore_from_checkpoint_position(self)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\", line 914, in _restore_from_checkpoint_position\r\n    tensor_saveables, python_saveables))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\", line 290, in restore_saveables\r\n    tensor_saveables)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 361, in validate_and_slice_inputs\r\n    _add_saveable(saveables, seen_ops, converted_saveable_object)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 331, in _add_saveable\r\n    saveable.name)\r\nValueError: The same saveable will be restored with two names: layer_with_weights-1/_table/.ATTRIBUTES/table\r\n```", "comments": ["Maybe [https://github.com/tensorflow/tfjs/issues/3977](https://github.com/tensorflow/tfjs/issues/3977)  is the same problem .", "@jvishnuvardhan Sorry. I am not familiar with this part.", "Hi , @jvishnuvardhan . Is there anyone who knows how to solve it ?", "I run into the same problem. And the issue seems reproducible on 2.4.1 (which is the current default version on Colab).\r\nThe error messages from saved_model_cli on 2.4.1 is slightly changed from saved_model_cli on 2.3.1.\r\n\r\nThe cell executed in [the tutorial Colab notebook](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers):\r\n\r\n```\r\n! saved_model_cli show --all --dir my_pet_classifier\r\n```\r\n\r\nThe output from the cell:\r\n```\r\n(snip)\r\n  Method name is: tensorflow/serving/predict\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/saved_model_cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py\", line 990, in main\r\n    args.func(args)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py\", line 691, in show\r\n    _show_all(args.dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py\", line 283, in _show_all\r\n    _show_defined_functions(saved_model_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/tools/saved_model_cli.py\", line 176, in _show_defined_functions\r\n    trackable_object = load.load(saved_model_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/load.py\", line 528, in load\r\n    return load_internal(export_dir, tags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/load.py\", line 114, in __init__\r\n    meta_graph.graph_def.library))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 312, in load_function_def_library\r\n    func_graph = function_def_lib.function_def_to_graph(copy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/function_def_to_graph.py\", line 59, in function_def_to_graph\r\n    fdef, input_shapes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/function_def_to_graph.py\", line 218, in function_def_to_graph_def\r\n    op_def = default_graph._get_op_def(node_def.op)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 3712, in _get_op_def\r\n    c_api.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type), buf)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'DenseBincount' in binary running on 0796e2c3d036. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nThe notebook I used to reproduced the issue was saved in my gist.\r\n\r\nhttps://colab.research.google.com/gist/nagachika/335371ac06432a19ca7027b64d61f740/preprocessing_layers.ipynb\r\n\r\nI hope this might help.", "@nagachika Looks like this is an issue when we have `--all`. In the mean time, Removing that `--all` may be workaround.\r\n```\r\n! saved_model_cli show --dir my_pet_classifier #this doesn't throw error\r\n# ! saved_model_cli show --all --dir my_pet_classifier  # this throws the error\r\n\r\n```\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/ca3f4e19a2de5947aae4d3b67917467b/preprocessing_layers.ipynb) is a gist for our reference. Thanks!", "@jvishnuvardhan Thank you for your information!\r\n\r\nI have confirmed that the following command line works for me to retrieve inputs/outputs signatures.\r\nThanks again!\r\n\r\n```\r\nsaved_model_cli show --tag_set serve --signature_def serving_default --dir my_pet_classifier\r\n```", "Hi @allenlavoie, @k-w-w,\r\n\r\nI just got assigned this.  I have no idea what's going wrong, do you have any advice?\r\n\r\nThat model saves and loads cleanly in the colab tutorial. \r\n\r\nIs there something wrong with `saved_model_cli`? Is that tool still maintained?", "@MarkDaoust \r\n\r\nThis is closely linked to https://github.com/tensorflow/serving/issues/1720 and https://github.com/tensorflow/serving/issues/1812\r\n\r\nAll these are somehow affecting saved_model_cli and also breaking inference via TFMS\r\n\r\nSo for starters, try loading the saved model in TFMS or saved_model_cli and you'll notice that both of them are failing. It could be an issue that the ops in binaries of saved_model_cli and TFMS are not in sync with the TF experimental.preprocessing ops.", "cc @ymodak @jvishnuvardhan ", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist here. Thanks!", "Team, you have been able to reproduce this issue all the way from version 2.3.1 but when can we expect a solution ? "]}, {"number": 44000, "title": "Comparison of gpu acceleration between gtx 1660ti(6g video memory) and quadro gv100(32g video memory)", "body": "I use the same mnist data set code to classify on gtx 1660ti graphics card and Quadro gv100 graphics card, and test that GPU acceleration is available.\r\nWhy is the speed on Quadro gv100 slower than gtx 1660ti, and what conditions should be met to turn on gpu acceleration on Quadro gv100? But I have verified that gpu acceleration is available.\r\n![image](https://user-images.githubusercontent.com/64268860/95950710-a635f700-0e27-11eb-9116-5585afaf5f65.png)\r\nFig. 1 is the experimental result (37s) on gtx 1660ti\r\n![image](https://user-images.githubusercontent.com/64268860/95955723-ded9ce80-0e2f-11eb-9bd5-2f145a1214c2.png)\r\n![image](https://user-images.githubusercontent.com/64268860/95955752-ec8f5400-0e2f-11eb-8073-0f92795295c6.png)\r\nFig. 2 and 3 is the result of the experiment on Quadro gv100\r\n![image](https://user-images.githubusercontent.com/64268860/95955860-12b4f400-0e30-11eb-84fe-0972181df3a9.png)\r\n\r\nFig. 4 shows the memory usage of Quadro gv100\r\nBecause of this problem, I trained very slowly when I used a lot of data to do semantic segmentation experiments. I don't think this Quadro gv100 graphics card should be trained so slowly.\r\nThank you very much for helping me!", "comments": ["@lihualilee \r\n\r\nPlease, help us with code snippet instead of screenshot to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", " @ravikyram \r\nPlease, help us with code snippet instead of screenshot to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\nWe conducted experiments on two different computers. One computer's GPU is Quadro GV100 and the other is gtx 1660ti.\r\nThe operating system of Quadro GV100 is windows server 2016, and the operating system of gtx 1660ti is windows10\r\nWe run the same code under the same computer. The code is as follows:\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nprint(tf.test.is_gpu_available())\r\n\"\"\" WARNING:tensorflow:From :4: is_gpu_available (from\r\ntensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for\r\nupdating: Use tf.config.list_physical_devices('GPU')` instead.\r\nTrue\r\n\"\"\"\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\nx_train = x_train.reshape(60000,28,28,1).astype(\"float32\") / 255\r\nx_test = x_test.reshape(10000,28,28,1).astype(\"float32\") / 255\r\n\r\ny_train = y_train.astype(\"float32\")\r\ny_test = y_test.astype(\"float32\")\r\ninputs = keras.Input(shape=(28,28,1), name=\"digits\")\r\n\r\nx= layers.Conv2D(32,kernel_size=3,strides=1,padding=\"same\",activation='relu')(inputs)\r\nx= layers.Conv2D(64,kernel_size=3,strides=1,padding=\"same\",activation='relu')(x)\r\n\r\nx = layers.MaxPooling2D(pool_size=2,strides=2,padding='same')(x)\r\n\r\nx = layers.Conv2D(128,kernel_size=3,strides=1,padding=\"same\",activation='relu')(x)\r\nx = layers.Conv2D(256,kernel_size=3,strides=1,padding=\"same\",activation='relu')(x)\r\nx = layers.MaxPooling2D(pool_size=2,strides=2,padding='same')(x)\r\n\r\nx = layers.Conv2D(512,kernel_size=5,strides=1,padding=\"same\",activation='relu')(x)\r\nx = layers.Conv2D(1024,kernel_size=5,strides=1,padding=\"same\",activation='relu')(x)\r\nx = layers.GlobalAveragePooling2D()(x)\r\n\r\nx = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(x)\r\nx = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\r\noutput= layers.Dense(10)(x)\r\noutputs = layers.Activation('softmax', dtype='float32')(output)\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.summary()\r\nmodel.compile( optimizer=\"adam\", loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['acc'])\r\n\r\nhistory = model.fit(x_train,y_train,batch_size=256,epochs=20,validation_data=(x_test, y_test))\r\n\u201c\u201d\u201c\r\nEpoch 1/20\r\n235/235 [==============================] - 139s 593ms/step - loss: 0.6885 - acc: 0.7536 -\r\nval_loss: 0.0754 - val_acc: 0.9765\r\nEpoch 2/20\r\n235/235 [==============================] - 133s 568ms/step - loss: 0.0652 - acc: 0.9805 -\r\nval_loss: 0.0290 - val_acc: 0.9902\r\nEpoch 3/20\r\n\u201d\u201c\u201d\r\n\r\nI use the same mnist data set code to classify on gtx 1660ti graphics card and quadro gv100 graphics card, and test that GPU acceleration is available.\r\nWhy is the speed on quadro gv100 slower than gtx 1660ti? It takes 240 seconds to train an epoch when conducting experiments on Quadro gv100.\r\nIt takes 37 seconds to train an epoch in the experiment on gtx 1660. The performance of Quqdro gv100 should be better than that of gtx 1660ti.\r\nWhy did I get a completely different point of view when I went to the lab? quadro gv100 is so much slower than gtx 1660ti.\r\nWhat conditions should be met to turn on gpu acceleration on quadro gv100? But I have verified that gpu acceleration is available\r\n\r\nThe operating system of Quadro GV100 is windows server 2016, and the operating system of gtx 1660ti is windows10", "@lihualilee \r\n\r\nWhich TF version you are using?\r\nThanks!", "@ Quadro GV100:TF2.2\r\ngtx 1660:TF2.0\r\n", "@lihualilee Can you try re-running with the same TF version on GV100 and GTX?  Right now it isn't clear if this is a regression from 2.0 to 2.2 or something else.  Also, can you try using tf-nightly?  That will tell us if the problem is fixed at HEAD.\r\n\r\nFinally, have you looked into the issue with the [TensorFlow profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras)?", "@sanjoy \r\nI don't think this is a problem between tf versions 2.2 and 2.0. However, when I was training the code, the performance of the graphics card remained unchanged. Like the idle state, it was always P2 level, and the power remained unchanged. At 30w/250w, the temperature of the graphics card did not rise. In fact, if the gpu is working, its temperature will rise rapidly, which is very strange.", "Wait I wanted to ask\nDoes 1660ti even have cuda cores for tensorflow"]}, {"number": 43995, "title": "Regularization Loss is getting added twice if model is encapsulated in another model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pre-installed TF on Colab\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n```\r\nimport tensorflow as tf\r\ntfk = tf.keras\r\ntfkl = tf.keras.layers\r\n\r\ndef test_model(input_shape):\r\n  input_x = tfkl.Input(shape=input_shape)\r\n  h = tfkl.Flatten()(input_x)\r\n  h = tfkl.Dense(10, activation='relu', activity_regularizer=tf.keras.regularizers.l1())(h)\r\n\r\n  return tf.keras.models.Model(inputs=input_x, outputs=h)\r\n\r\ndef another_model(input_shape, tm):\r\n  input_x = tfkl.Input(shape=input_shape)\r\n  m = tm(input_x)\r\n  return tf.keras.models.Model(inputs=input_x, outputs=m)\r\n\r\ntm = test_model(input_shape)\r\nprint(another_model(input_shape, tm).losses)\r\n\r\n```\r\nAbove prints\r\n\r\n```\r\n[<tf.Tensor 'dense_27/ActivityRegularizer/truediv:0' shape=() dtype=float32>,\r\n <tf.Tensor 'functional_20/dense_27/ActivityRegularizer/truediv:0' shape=() dtype=float32>]\r\n```\r\n\r\nThis pattern of creating simple models (modules) and then assembling them as part of the big model is widely used and as shown in the above code snippets and its outcome this ends up adding the regularization loss twice.\r\n\r\nI noticed it while adding KLDivergenceRegularization from tensorflow_probability and then reproduced it using a simple example as shown above.\r\n\r\nPlease suggest if there is a workaround as I said earlier I have many models that I assemble.\r\n\r\nRegards & thanks\r\nKapil\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@ksachdeva \r\nI ran the code shared and face a different issue, please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/165c745200ca1aa88b0d5545d07d40c7/untitled433.ipynb) and share all dependencies or share a colab gist with error reported.", "Hi @Saduf2019 \r\n\r\nYou could use any input_shape e.g.  input_shape = [28, 28, 1]\r\n\r\nNevertheless, please find the link to the colab notebook \r\n\r\nhttps://colab.research.google.com/drive/1oa5fdFEuM2d_k-OrL4Rif8EcV0EJjavR?usp=sharing\r\n\r\nRegards\r\nKapil", "i am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/39b6b45eac74c96b51ebee77ebcd45e5/untitled437.ipynb).", "Hi,\r\n\r\nIs there any action on this?\r\n\r\nIMHO, this is a very dangerous bug as the practice of using Loss layers, activity regularization, and model wrapping is quite common. \r\n\r\nRegards\r\nKapil", "pinging again! \r\n\r\n@Saduf2019 since you were able to reproduce this is there a way to get a resolution on this.", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/2a386bd801d28ce02473b187e8cb7ba3/43995.ipynb). Thanks!", "@sachinprasadhs seems like it is not a priority for the TensorFlow team. It is a very serious bug in my opinion. I have been a very big advocate of TensorFlow & Keras in my organization but this experience has left me questioning if it is indeed time to switch to PyTorch. Very disappointed!", "@deeb02 Can you please take a look at this issue? Thanks!", "Was able to reproduce the issue in TF v2.6 please find the gist **[here](https://colab.research.google.com/gist/kumariko/664c72e5a70286c728757d2eec419b95/regularizer-issue.ipynb#scrollTo=XvcdejTdTtAj)**.Thanks!"]}, {"number": 43977, "title": "Method to get k smallest elements from a tensor", "body": "[`tf.math.top_k`](https://www.tensorflow.org/api_docs/python/tf/math/top_k) only returns `k` largest elements from the input `tensor` along a specified `axis`. However, there is no direct way to obtain `k` smallest elements from a `tensor`. The equivalent method [`topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) in `pytorch` has an `bool` flag `largest` to toggle the output from `k` largest elements to `k` smallest elements and vice versa. It would be useful to have similar option  (or an alternative method like `bottom_k`) for this purpose so that hacky solution like the following can be avoided.\r\n\r\n```python\r\ntf.negative(tf.math.top_k(tf.negative(A)))\r\n```\r\nFinding `k` minimum values can be potentially very useful in many cases especially for writing custom `ops`. This does not seem to be too hard to implement or drastically change the API.\r\n\r\n**System information**\r\n- TensorFlow version: 2.3\r\n\r\nRelated discussion in [Stackoverflow](https://stackoverflow.com/questions/44548227/minimum-k-values-of-a-tensor).", "comments": []}, {"number": 43969, "title": "Memory leak with tf.data.Dataset.map", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18-04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nigthtly\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nTest is failing if line with dataset.map is not commented.\r\nIt shows the garbage collection is not working when using even a simple function with tf.data.Dataset.map\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.test_util import assert_no_garbage_created\r\n\r\nclass TestStandardTrainer(tf.test.TestCase):\r\n  @assert_no_garbage_created\r\n  def test_dataset_map(self):\r\n      data_tensor = {'x': tf.constant([1, 2, 3], dtype=tf.float32),\r\n                     'y': tf.constant([4, 5, 6], dtype=tf.float32)}\r\n      dataset = tf.data.Dataset.from_tensor_slices(data_tensor)\r\n\r\n      # We split the data in 2\r\n      dataset = dataset.map(lambda x: (x, x))\r\n\r\nimport unittest\r\nunittest.main(argv=['first-arg-is-ignored'], exit=False)\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTest should pass\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs**\r\nNone\r\n", "comments": ["/cc @jsimsa ", "I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f50501329b6b512e47b881216493fb4d/untitled436.ipynb)", "Was able to replicate your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4fa68a3a37efccd1235514576d9f45b7/43969.ipynb). Thanks!"]}, {"number": 43958, "title": "Enable profiling in release builds", "body": "@tensorflow/micro\r\n\r\nProfiler is turned on/off depending on if BUILD_TYPE is debug or any of the release builds.\r\nIt would be useful to be able to decouple profiling from the NDEBUG-define, so that it's possible to enable profiling for a release build. The reason for this is that the reference kernels runs slower in debug mode in release mode, so the measurements are not really representative \r\n\r\n\r\nOne idea is to add a build type \"release_with_profiling\", which works in a similar way as \"release_with_logs\".", "comments": []}, {"number": 43939, "title": "support Quantization Aware Training for Conv2dTranspose->BathcNorm->Activation", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n\r\n**Describe the feature and the current behavior/state.**\r\nthe sequence \"Conv2dTranspose->BathcNorm->Activation\" is not defined in\r\ntensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_layout_transform.py\r\n\r\n**Who will benefit with this feature?**\r\nanyone with this sequence in the network\r\n\r\n", "comments": ["Is there any update on this feature?\r\nQAT Conv2dTranspose support would really help my Unet training. ", "@limbtio , my temporary solution is to train float with batch norm, and before retraining fold the the batch norm params into the conv2dtranspose (through editing the h5 file before load_weights", "@eladc4 thanks, but as far as I understand, the conv2dtranspose itself is not supported for quantization aware training. When I use tfmot.quantization.keras.quantize_model, I get the following error:\r\nRuntimeError: Layer decoder_stage0a_transpose:<class 'tensorflow.python.keras.layers.convolutional.Conv2DTranspose'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.\r\n", "@limbtio you're correct of course, i quantize the network by cloning it and quantizing supported layers. it works because the ReLU following the conv2dtranspose is quantized, and quantization params can be inferred back to the conv2dtranspose, which works for my framework"]}, {"number": 43937, "title": "please expose uniform_full_int on tf.random", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis request is about adding `tf.random.uniform_full_int` similarly to how `tf.random.uniform` exists\r\n\r\nit's currently not trivial to get the full range of ints using `tf.random.uniform` (specifically because maxval is expected to be of the int type but is exluded from the range)\r\n\r\n**Will this change the current api? How?**\r\nYes, adding `tf.random.uniform_full_int` similarly to how `tf.random.uniform` exists\r\n\r\n(note that there is `tf.random.Generator.uniform_full_int`, but this request applies to `tf.random`)", "comments": ["I'll add this.", "Any news on this? @wangpengmit \r\nI think it is reasonable to just say Generators are the only way to go in tf, but please also deprecate the old `tf.random.uniform` and the like, otherwise the differences in these APIs make things harder to reason about and test.", "I got preempted and haven't got to it. Fancy contributing a PR?"]}, {"number": 43929, "title": "Missing motivation for TF hacking in Pix2Pix Generative tutorial", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nThe note from\r\nhttps://www.tensorflow.org/tutorials/generative/pix2pix#generate_images\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUnder the section pointed to above there is a note addressing a hack in the code where training mode is used for a piece of code serving as \"visual validation\". To me, as a person learning from this tutorial, it is insufficient. Although it mitigates the uncertainty around unexpected measures by saying that this is done for the sake of certain shapes of statistics, it also cuts abruptly by saying \"and this is what we [don't] want\". This leaves me puzzled with why do we want such thing, the more so that further on we take just a single sample which means that the batch norm degenerates to instance norm. \r\n\r\n### Clear description\r\n\r\nCan someone please add motivation to why training statistics are preferred in this case and perhaps relate to the issue of BN in the case of a single example?\r\n", "comments": ["See section 3.3 in the paper: https://arxiv.org/pdf/1611.07004.pdf", "@Antymon,\r\nCan you please respond to the above comment? Thanks! ", "> @Antymon,\r\n> Can you please respond to the above comment? Thanks!\r\n\r\nSure! Thank you for the reference @yashk2810, that does shed some light indeed!\r\n\r\n@rmothukuru As for the issue - I haven't noticed any change in the tutorial contents, or have I missed something? If you want to help just me that's fine, but if you want to enlighten more people perhaps you would like to revisit the contents I have referred to above. \r\n\r\nThank you!"]}, {"number": 43905, "title": "Prefetch to GPU: prefetch_to_device does not do anything (from_generator), tf.data.Dataset API unclear", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Official 2.4-nightly docker (version 2.4.0-dev20201005)\r\n- TensorFlow version (use command below): Official 2.4-nightly docker (version 2.4.0-dev20201005)\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA: 11.1, cuDNN: 8.0.2\r\n- GPU model and memory: Nvidia GTX 1080 Ti, 11170MiB\r\n\r\nThe API concerning moving CPU to GPU with prefetching extremely unclear using tf.data.Dataset. The function 'prefetch_to_device' simply does not work, even though it was stated that it should be fixed by TF 2.3 or TF 2.4 in the following issue:\r\n[issue 35563](https://github.com/tensorflow/tensorflow/issues/35563)\r\n\r\nIn order to show the behavior, I have written a standalone test that goes over four options:\r\n - 0: only use 'prefetch_to_device'\r\n - 1: Use 'copy_to_device'\r\n - 2: Use 'copy_to_device' and prefetch\r\n - 3: Use 'copy_to_device' and 'prefetch_to_device'\r\n\r\nI would expect options 0, 2, and 3 to be almost identical. However, only option 2 does actually the indented behavior. (Option 1 does not do prefetching, so in that sense it also does what it is intended for).\r\n\r\nI am also not even 100% sure if option 2 is optimal. It does seem that the other GPU ops are blocked during the MemCpy. It could be that it only looks as if it is covered. But I do not have the expertise to judge this case. In terms of timings I did notice that option 2 was the fastest: \r\n\r\n~~~\r\nOption prefetch_gpu\r\ndata/logs/pa_20201009-082628_prefetch_gpu\r\nTime lapsed= 0:00:01.177912\r\n\r\nOption copy\r\ndata/logs/pa_20201009-082628_copy\r\nTime lapsed= 0:00:00.916617\r\n\r\nOption copy_prefetch\r\ndata/logs/pa_20201009-082628_copy_prefetch\r\nTime lapsed= 0:00:00.831833\r\n\r\nOption copy_prefetch_gpu\r\ndata/logs/pa_20201009-082628_copy_prefetch_gpu\r\nTime lapsed= 0:00:00.926764\r\n~~~\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport os\r\nimport datetime\r\nfrom tqdm import tqdm\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nprint('TF version', tf.__version__)\r\n\r\n\r\n@tf.function\r\ndef do_stuff(wmat, tf_var):\r\n\r\n    A = tf.matmul(wmat + 1, tf.transpose(wmat))\r\n    error = tf.reduce_mean(tf_var)\r\n    return error, A \r\n\r\nexp_uuid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\nn_batches = 20\r\n\r\nweights = [None] * n_batches\r\nfor i in range(n_batches):\r\n    weights[i] = tf.constant(np.random.rand(2000,10000), dtype=tf.float32)\r\n\r\n\r\ndef gen():\r\n    for i in weights:\r\n        yield i\r\n\r\noption_names = ['prefetch_gpu', 'copy', 'copy_prefetch', 'copy_prefetch_gpu']\r\nfor option in range(4):\r\n\r\n    dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.float32))\r\n\r\n    if option == 0:\r\n        ## Option 1: prefetch_gpu\r\n        #\r\n        ## output:  \r\n        ##          weights device /job:localhost/replica:0/task:0/device:CPU:0\r\n        ##          weights device after identity /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n        gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0')\r\n        dataset.apply(gpu_transform)\r\n\r\n    elif option == 1:\r\n        ## Option 1: only copy\r\n        #\r\n        ## output:\r\n        ##          weights device /job:localhost/replica:0/task:0/device:GPU:0\r\n        dataset = dataset.apply(tf.data.experimental.copy_to_device(\"/gpu:0\"))\r\n\r\n    elif option == 2:\r\n        ## Option 2: copy + prefetch\r\n        ## as suggested in https://github.com/tensorflow/tensorflow/issues/35563#issuecomment-602160568\r\n        #\r\n        ## output:\r\n        ##          weights device /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n        dataset = dataset.apply(tf.data.experimental.copy_to_device(\"/gpu:0\"))\r\n        with tf.device(\"/gpu:0\"):\r\n            dataset = dataset.prefetch(1)\r\n\r\n    elif option == 3:\r\n        ## Option 3: copy + prefetch_gpu\r\n        #\r\n        ## output:\r\n        ##          weights device /job:localhost/replica:0/task:0/device:GPU:0\r\n        dataset = dataset.apply(tf.data.experimental.copy_to_device(\"/gpu:0\"))\r\n        gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0')\r\n        dataset.apply(gpu_transform)\r\n\r\n\r\n    tf_var = tf.Variable(np.zeros(3))\r\n    adam = tf.keras.optimizers.Adam(1e-4) \r\n    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])\r\n\r\n    tf.profiler.experimental.start(logpath)\r\n    start = datetime.datetime.now()\r\n    for b, wmat in tqdm(enumerate(dataset)):\r\n        with tf.GradientTape() as tape:\r\n\r\n            if b == 0:\r\n                print('\\n weights device', wmat.device)\r\n                print('')\r\n\r\n            if option == 0:\r\n                wmat = tf.identity(wmat, 'move_to_gpu')\r\n                if b == 0:\r\n                    print('weights device after identity', wmat.device)\r\n                    print('')\r\n\r\n            # Do some calculations\r\n            result = do_stuff(wmat, tf_var)\r\n        \r\n        grads = tape.gradient(result[0], [tf_var])\r\n        adam.apply_gradients(zip(grads, [tf_var]))\r\n    stop = datetime.datetime.now()\r\n    tf.profiler.experimental.stop()\r\n\r\n    print(f'\\nOption {option_names[option]}')\r\n    print(logpath)\r\n    print('Time lapsed=', stop - start)\r\n```\r\n\r\nOption 0: prefetch_to_device \r\n![prefetch_gpu_overview](https://user-images.githubusercontent.com/28840027/95562130-0984eb00-0a1c-11eb-89c0-0949ed49eeb8.png)\r\n![prefetch_gpu_trace](https://user-images.githubusercontent.com/28840027/95562131-0a1d8180-0a1c-11eb-8840-b49acebe77ad.png)\r\n\r\nOption 1: copy_to_device\r\n![copy_overview](https://user-images.githubusercontent.com/28840027/95562121-07229100-0a1c-11eb-847e-ce5f0b0dd9e1.png)\r\n![copy_trace](https://user-images.githubusercontent.com/28840027/95562129-08ec5480-0a1c-11eb-8eb9-fc4646f2ee3d.png)\r\n\r\nOption 2: copy_to_device + prefetch\r\n![copy_prefetch_overview](https://user-images.githubusercontent.com/28840027/95562125-0853be00-0a1c-11eb-9402-90584ba2360c.png)\r\n![copy_prefetch_trace](https://user-images.githubusercontent.com/28840027/95562127-0853be00-0a1c-11eb-963e-4ef90664a366.png)\r\n\r\nOption 3: copy_to_device + prefetch_to_device\r\n![copy_prefetch_gpu_overview](https://user-images.githubusercontent.com/28840027/95562123-07bb2780-0a1c-11eb-947f-27aa82348e1f.png)\r\n![copy_prefetch_gpu_trace](https://user-images.githubusercontent.com/28840027/95562124-07bb2780-0a1c-11eb-9254-47fdc7ab2e81.png)\r\n\r\n\r\nFiles:\r\n[pa_20201009-082628_prefetch_gpu.zip](https://github.com/tensorflow/tensorflow/files/5353593/pa_20201009-082628_prefetch_gpu.zip)\r\n[pa_20201009-082628_copy.zip](https://github.com/tensorflow/tensorflow/files/5353594/pa_20201009-082628_copy.zip)\r\n[pa_20201009-082628_copy_prefetch.zip](https://github.com/tensorflow/tensorflow/files/5353596/pa_20201009-082628_copy_prefetch.zip)\r\n[pa_20201009-082628_copy_prefetch_gpu.zip](https://github.com/tensorflow/tensorflow/files/5353597/pa_20201009-082628_copy_prefetch_gpu.zip)\r\n\r\n", "comments": ["Note that this originated from an initial stack overflow question:\r\nhttps://stackoverflow.com/questions/64142435/tensorflow-how-to-prefetch-data-on-the-gpu-from-cpu-tf-data-dataset-from-gener/64239650#64239650", "I ran the code on nightly please find the [gist here](https://colab.research.google.com/gist/Saduf2019/c3ca83194b6d1d8bb17e8843377465bd/untitled428.ipynb).", "@rubenverhack Sorry for the slow response. This looks like a typo, instead of `dataset.apply(gpu_transform)` it should be `dataset = dataset.apply(gpu_transform)`. Dataset transformations are functional, creating new datasets instead of modifying their source dataset.", "@aaudiber Thank you for taking the time to review this. It's true that that was a mistake in this example. I can confirm that option 3 (prefetch_to_device) is as fast as option 2 (copy to device + prefetch). \r\n\r\n![Selection_237](https://user-images.githubusercontent.com/28840027/97869983-987edd80-1d12-11eb-9cde-7a69478ec5f9.png)\r\n\r\nHowever, I do still see a noticeable time that the GPU is performing nothing else than MemcpyH2D. I am not sure if I'm reading this correctly, but why would that be the case? \r\n\r\n", "> However, I do still see a noticeable time that the GPU is performing nothing else than MemcpyH2D. I am not sure if I'm reading this correctly, but why would that be the case?\r\n\r\nSame to me. After using `dataset = dataset.apply(tf.data.experimental.prefetch_to_device(device='/gpu:0', buffer_size=16))`, it is faster than `dataset = dataset.prefetch(buffer_size=16)`. \r\n\r\nBut there is still noticeable time spent on `cudaMemcpyH2D`. Please see [here](https://github.com/tensorflow/tensorflow/issues/44836#issuecomment-726866188) for more details.\r\n\r\nMaybe `prefetch_to_device` does not really `prefetch`?", "Same problem to me, i notice that gpu stop training the model when transferring data, and it would transfer data when computing.", "TLDR: `prefetch_to_device` works as expected, when the data produced by the input pipeline is allocated in [pinned host memory](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/).\r\n\r\nI have recently investigated this issue. \r\n\r\nFirst, to compare `copy_to_device` and `prefetch_to_device` performance, I have modified the original repro as follows:\r\n\r\n```\r\ndef main(argv):\r\n  del argv\r\n\r\n  @tf.function\r\n  def do_stuff(wmat, tf_var):\r\n    result = tf.matmul(wmat + 1, tf.transpose(wmat))\r\n    error = tf.reduce_mean(tf_var)\r\n    return error, result\r\n\r\n  n_batches = 100\r\n  weights = [None] * n_batches\r\n  for i in range(n_batches):\r\n    weights[i] = tf.constant(np.random.rand(2000, 10000), dtype=tf.float32)\r\n\r\n  def gen():\r\n    for weight in weights:\r\n      yield weight\r\n\r\n  option_names = [\r\n      'prefetch_to_device', 'copy_to_device', 'copy_to_device_and_prefetch'\r\n  ]\r\n  dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.float32))\r\n\r\n  if FLAGS.option == 0:\r\n    dataset = dataset.apply(tf.data.experimental.copy_to_device('/gpu:0'))\r\n\r\n  if FLAGS.option == 1:\r\n    dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/gpu:0'))\r\n\r\n  tf_var = tf.Variable(np.zeros(3))\r\n  adam = tf.keras.optimizers.Adam(1e-4)\r\n\r\n  for _, wmat in tqdm(enumerate(dataset)):\r\n    with tf.GradientTape() as tape:\r\n      result = do_stuff(wmat, tf_var)\r\n      grads = tape.gradient(result[0], [tf_var])\r\n      adam.apply_gradients(zip(grads, [tf_var]))\r\n```\r\n\r\nUsing this program I have collected the following traces:\r\n\r\ncopy_to_device\r\n![Screen Shot 2021-04-16 at 4 17 35 PM](https://user-images.githubusercontent.com/1072079/115477095-0737f080-a1f8-11eb-9763-578fd18354ca.png)\r\n\r\nprefetch_to_device\r\n![Screen Shot 2021-04-16 at 4 17 46 PM](https://user-images.githubusercontent.com/1072079/115477132-1ae35700-a1f8-11eb-8de2-70e06d014dd4.png)\r\n\r\nWhile a single step of the `prefetch_to_device` version of the program is slightly faster (based on the trace view measurement as opposed to TF profiler overview, which might not be correctly detecting \"steps\" for this program) -- 54.9ms vs 62.6ms -- the trace also shows that the data copy seems to be delayed by something.\r\n\r\nNext, I investigated whether making sure that the input pipeline produces data that is allocated in [pinned host memory](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/), resolves the issue.\r\n\r\nCurrently, tf.data only allocates pinned host memory for input pipelines that use the `map().batch()` pattern. Therefore, I needed to modify the program as follows:\r\n\r\n```\r\ndef main(argv):\r\n  del argv\r\n\r\n  @tf.function\r\n  def do_stuff(wmat, tf_var):\r\n    result = tf.matmul(wmat + 1, tf.transpose(wmat))\r\n    error = tf.reduce_mean(tf_var)\r\n    return error, result\r\n\r\n  option_names = [\r\n      'copy_to_device', 'prefetch_to_device', 'copy_to_device_and_prefetch'\r\n  ]\r\n\r\n  dataset = tf.data.Dataset.range(1024 * 100)\r\n  dataset = dataset.map(\r\n      lambda _: tf.random.uniform([10000]),\r\n      num_parallel_calls=tf.data.AUTOTUNE)\r\n  dataset = dataset.batch(batch_size=1024)\r\n\r\n  if FLAGS.option == 0:\r\n    dataset = dataset.apply(tf.data.experimental.copy_to_device('/gpu:0'))\r\n\r\n  if FLAGS.option == 1:\r\n    dataset = dataset.apply(tf.data.experimental.prefetch_to_device('/gpu:0'))\r\n\r\n  tf_var = tf.Variable(np.zeros(3))\r\n  adam = tf.keras.optimizers.Adam(1e-4)\r\n\r\n  for _, wmat in tqdm(enumerate(dataset)):\r\n    with tf.GradientTape() as tape:\r\n      result = do_stuff(wmat, tf_var)\r\n      grads = tape.gradient(result[0], [tf_var])\r\n      adam.apply_gradients(zip(grads, [tf_var]))\r\n```\r\n\r\nThese changes end up resolving the issue:\r\n\r\ncopy_to_device\r\n![Screen Shot 2021-04-20 at 4 37 45 PM](https://user-images.githubusercontent.com/1072079/115477493-df955800-a1f8-11eb-82a6-68469efae6ff.png)\r\n\r\nprefetch_to_device\r\n![Screen Shot 2021-04-20 at 4 38 45 PM](https://user-images.githubusercontent.com/1072079/115477505-e6bc6600-a1f8-11eb-820f-fabdeec1a14e.png)", "@jsimsa \r\n> TLDR: `prefetch_to_device` works as expected, when the data produced by the input pipeline is allocated in [pinned host memory](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/).\r\n\r\nSo the conclusion is:\r\n`prefetch_to_device` actually works, but it needs be to used with `map().batch()` to get better performance? <br>\r\n\r\n\r\n> Currently, tf.data only allocates pinned host memory for input pipelines that use the map().batch() pattern. \r\n\r\nAnd where can I get such information? From TensorFlow source code?", "`prefetch_to_device` works when the data produced by the input pipeline is allocated in pinned host memory. `map().batch()` is currently the option to achieve this through tf.data. This is not documented anywhere and is a subtle implementation detail: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L517\r\n\r\nI plan to investigate whether there is any downside to making all tf.data allocations use pinned host memory and if not, make this available more broadly. Note that this might not address the issue for input pipelines that do not allocate memory through tf.data. For instance, the example in the original description of this issue performs the memory allocation outside of tf.data.", "@jsimsa i see that you were able to get the tracer to show prefetching (memcpy overlap with gpu compute)\r\n\r\nbut did you see an actual improvement in training speed?\r\n\r\n```\r\nds = tf.data.Dataset.range(2**13)\r\nds = ds.map(\r\n  lambda _: np.random.randint(0, 10, (4096)).astype(np.float32),\r\n  num_parallel_calls=tf.data.AUTOTUNE)\r\n\r\nds = ds.batch(batch_size=4096).cache().repeat(-1)\r\npf_ds = ds.prefetch(16)\r\ngpu_pf_ds = ds.apply(tf.data.experimental.prefetch_to_device('/gpu:0', buffer_size=16))\r\n\r\n\r\ndef time_ds(ds, ramp_steps=100, steps=200):\r\n  @tf.function\r\n  def _step(wmat, tf_var):\r\n    with tf.device('/gpu:0'):\r\n      result = tf.matmul(wmat + 1, tf.transpose(wmat))\r\n      error = tf.reduce_mean(tf_var)\r\n      return error, result\r\n\r\n  tf_var = tf.Variable(np.zeros(3))\r\n  adam = tf.keras.optimizers.Adam(1e-4)\r\n\r\n  assert steps > ramp_steps\r\n  for i, elem in enumerate(ds):\r\n    if i == ramp_steps:\r\n      t = time.time()\r\n    if i == steps:\r\n      print(time.time() - t)\r\n      break\r\n    with tf.GradientTape() as tape:\r\n      result = _step(elem, tf_var)\r\n      grads = tape.gradient(result[0], [tf_var])\r\n      adam.apply_gradients(zip(grads, [tf_var]))\r\n```\r\n\r\n`time_ds(ds)`\r\n`time_ds(pf_ds)`\r\n`time_ds(gpu_pf_ds)`\r\n\r\nall 3 show roughly 8 seconds per 100 steps on colab GPU", "> `prefetch_to_device` works when the data produced by the input pipeline is allocated in pinned host memory. `map().batch()` is currently the option to achieve this through tf.data. This is not documented anywhere and is a subtle implementation detail: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L517\r\n> \r\n> I plan to investigate whether there is any downside to making all tf.data allocations use pinned host memory and if not, make this available more broadly. Note that this might not address the issue for input pipelines that do not allocate memory through tf.data. For instance, the example in the original description of this issue performs the memory allocation outside of tf.data.\r\n\r\nAny better solutions?", "I'm running a siamese network on Kaggle, using a tf Dataset (prefetching in the final step of dataset).   \r\nI'm noticing that GPU is not being used at all.   \r\nI also tried to run the same notebook on Google Colab, and seems that GPU is not being used.   \r\nAnyone running in the same issue?   ", "> I'm running a siamese network on Kaggle, using a tf Dataset (prefetching in the final step of dataset). I'm noticing that GPU is not being used at all. I also tried to run the same notebook on Google Colab, and seems that GPU is not being used. Anyone running in the same issue?\r\n\r\nHi, What do you mean by \"GPU is not being used\"? \r\n", "> > I'm running a siamese network on Kaggle, using a tf Dataset (prefetching in the final step of dataset). I'm noticing that GPU is not being used at all. I also tried to run the same notebook on Google Colab, and seems that GPU is not being used. Anyone running in the same issue?\r\n> \r\n> Hi, What do you mean by \"GPU is not being used\"?\r\n\r\nI mean that the GPU memory isn't being modified (always 0%). \r\nI don't know what's happening."]}, {"number": 43904, "title": "Per Example Gradients fail with LSTM", "body": "**System information**\r\nColab with `tf-nightly-gpu==2.4.0-dev20201007`\r\n\r\n**Issue**\r\nPer example gradients via `tf.vectorized_map` with an LSTM model fails. It works if you set `unroll=True` for the LSTM.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Here](https://colab.research.google.com/gist/n2cholas/4bf3e70aa2ec310f69fe90bb21cb1747/lstm-per-eg-grads.ipynb) is a colab gist with the error.\r\n", "comments": ["I tried in colab with TF nightly version(`2.4.0-dev20201007`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e19a7904bbdd99b4260df14e6fa39ab8/untitled437.ipynb).Thanks!", "Seems that the error is caused by following:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/pfor.py:4044 f  *\r\n        [converter._convert_helper(x).t for x in func._func_graph_outputs])\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/pfor.py:1555 _convert_helper  **\r\n        raise ValueError(message)\r\n\r\n    ValueError: No pfor vectorization defined for StatelessCase\r\n```\r\n\r\nAssigning to @sun51 for StatelessCase", "\r\nI am able to replicate this issue on tf 2.5 , please find the [gist here](\r\nhttps://colab.research.google.com/gist/Saduf2019/ff2ac41c5a428e86d6e834c44767599c/untitled589.ipynb)"]}, {"number": 43891, "title": "No way to initialize Interpreter with Model reference in TensorFlowLiteSwift", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n\r\nTensorFlowLiteSwift 2.3.0\r\n\r\n- Are you willing to contribute it (Yes/No):\r\n\r\nMaybe\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFor inference you need to use Interpreter class. But all initializers uses model file parameters. This prevents sharing of the model in different threads.  \r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, Model class needs to be public and Interpreter class will need new initializer functions.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nWho wants to use inference in separate threads.\r\n\r\n", "comments": []}, {"number": 43878, "title": "Using SYSTEM cURL (build against OpenSSL) conflicts with BoringSSL", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.1\r\n\r\n**Describe the problem**\r\n\r\nDue to a hard-coded configure value of cURL (see https://github.com/tensorflow/tensorflow/issues/40065#issuecomment-667129086) we build TensorFlow with a system-installed cURL via `TF_SYSTEM_LIBS=curl`. This curl uses the systems OpenSSL library (for security reasons this has to be managed by the system)\r\n\r\nSome parts of TF use BoringSSL, e.g. grpcio (build as a dependency) seems to contain/build a BoringSSL itself.\r\nNow loading the TF lib (via an import in Python) loads cURL, which loads OpenSSL which causes conflicts with the BoringSSL used and ultimately leads to invalid free calls:\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- build TensorFlow with TF_SYSTEM_LIBS=curl\r\n- Run any code using cURL through TF in Python. Example below:\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\n\r\nbuilder = tfds.builder('stanford_dogs')\r\nbuilder.download_and_prepare()\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\n*** Error in `python': free(): invalid pointer: 0x00000000046bfd08 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x81299)[0x7f2b1dba9299]\r\n<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_STRING_free+0x35)[0x7f2aac3d0205]\r\n<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_primitive_free+0xbd)[0x7f2aac3d371d]\r\n<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_template_free+0x8f)[0x7f2aac3d3b1f]\r\n<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(asn1_item_combine_free+0x2a0)[0x7f2aac3d39f0]\r\n<prefix>/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2(ASN1_item_free+0x17)[0x7f2aac3d3a77]\r\n<prefix>/lib/libcurl.so.4(+0x57a20)[0x7f2aaae7ea20]\r\n<prefix>/lib/libcurl.so.4(+0x593fb)[0x7f2aaae803fb]\r\n<prefix>/lib/libcurl.so.4(+0x59fd7)[0x7f2aaae80fd7]\r\n<prefix>/lib/libcurl.so.4(+0x110e2)[0x7f2aaae380e2]\r\n<prefix>/lib/libcurl.so.4(+0x30215)[0x7f2aaae57215]\r\n<prefix>/lib/libcurl.so.4(curl_multi_perform+0x83)[0x7f2aaae58473]\r\n<prefix>/lib/libcurl.so.4(curl_easy_perform+0x11b)[0x7f2aaae50a6b]\r\n<prefix>/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow15CurlHttpRequest4SendEv+0x2b9)[0x7f2ac88c34f9]\r\n```\r\n\r\nIs there any solution from the side of TF to avoid this? From reading the docs of BoringSSL it should be possible to define `BORINGSSL_PREFIX` to something, so the symbols from OpenSSL and BoringSSL do not clash. But I'm not sure if I understood that correctly as BoringSSL would need to use that when being built already", "comments": ["@Flamefire Could you please let us know if we can move this issue to closed status as  the [PR](https://github.com/easybuilders/easybuild-easyblocks/pull/2197) is merged ? Thanks!", "No it is not resolved as the issue still persists and there is no official documentation about how to resolve this. The PR you linked is a workaround in a totally different repo which we found to work for us. It obviously doesn't fix the underlying issue."]}, {"number": 43849, "title": "TFLite ConverterError: Node has inputs from different frames", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): with pip\r\n- TensorFlow version (or github SHA if from source):  2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nexport_dir = os.path.join('export_dir', 'out')\r\n\r\nif not os.path.exists('export_dir'):\r\n    os.mkdir('export_dir')\r\n\r\ntf.compat.v1.enable_control_flow_v2()\r\ntf.compat.v1.enable_v2_tensorshape()\r\n\r\ndef wrap_frozen_graph(graph_def, inputs, outputs):\r\n    def _imports_graph_def():\r\n        tf.compat.v1.import_graph_def(graph_def, name=\"\")\r\n    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\r\n    import_graph = wrapped_import.graph\r\n    return wrapped_import.prune(\r\n             tf.nest.map_structure(import_graph.as_graph_element, inputs),\r\n             tf.nest.map_structure(import_graph.as_graph_element, outputs))\r\n\r\ngraph_def = tf.compat.v1.GraphDef()\r\nloaded = graph_def.ParseFromString(open(os.path.join(export_dir, 'saved_model.pb'),'rb').read())\r\n\r\nconcrete_func = wrap_frozen_graph(\r\n        graph_def, inputs=['extern_data/placeholders/data/data:0', 'extern_data/placeholders/data/data_dim0_size:0'],\r\n        outputs=['output/output_batch_major:0'])\r\nconcrete_func.inputs[0].set_shape([1, 50])\r\nconcrete_func.inputs[1].set_shape([1])\r\nconcrete_func.outputs[0].set_shape([1, 100])\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\nconverter.post_training_quantize=True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                               tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\n\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nif not os.path.exists('tflite'):\r\n    os.mkdir('tflite')\r\noutput_model = os.path.join('tflite', 'model.tflite')\r\nwith open(output_model, 'wb') as f:\r\n     f.write(tflite_model)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: {{node output/rec/target0_embed/linear/embedding_lookup}} has inputs from different frames. The input {{node output/rec/target0_embed/linear/embedding_lookup/Enter}} is in frame 'output/rec/while/while_context'. The input {{node output/rec/target0_embed/linear/embedding_lookup/axis}} is in frame ''.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/home/mdigangi/bin/tflite-toolkit/tools/ckpt2pb.py\", line 52, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 900, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 633, in convert\r\n    **converter_kwargs)\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 574, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/mdigangi/environments/venv-tflite2/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: {{node output/rec/target0_embed/linear/embedding_lookup}} has inputs from different frames. The input {{node output/rec/target0_embed/linear/embedding_lookup/Enter}} is in frame 'output/rec/while/while_context'. The input {{node output/rec/target0_embed/linear/embedding_lookup/axis}} is in frame ''.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://drive.google.com/file/d/1DUyxmwA4eIl5SqZ_R5jMJ1ACH3_0jOFK/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI trained this Tranformer for NMT using [Returnn](https://github.com/rwth-i6/returnn/), which creates a TF model. It uses tf.compat.v1 for retrocompatibility. After creating the model, I freezed it using \r\n```\r\nfreeze_graph.py --output_graph export_dir/out/checkpoint_FROZEN.pb --clear_devices --output_node_names output/output_batch_major --input_meta_graph export_dir/0/params.003.compiled.meta --input_checkpoint export_dir/0/params.003.compiled --input_binary True\r\n```\r\nthis script succeeds but adds the node output/rec/target0_embed/linear/embedding_lookup/axis, which creates the error. The node is added [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py#L404-L420).  The node creating the error is the embedding layer of the decoder, which must be called recursively. I tried some quick hacks like remove the \"frame_name\" from one node, adding it to the other, or setting the first to '', but nothing of this works. \r\n\r\nAdded 9 October 2020: if I use the concrete function for inference, it also doesn't work. Maybe it is a problem of TF2 in interpreting frozen graph produced with style of TF1\r\n\r\nAdded 12 October 2020: I tried to write a minimal code that can reproduce the issue and I've found out that maybe the problem is not due to the last node not being in the frame but the second one. This is the interesting part of the GraphDef of the graph that generates the error:\r\n```\r\n[name: \"source_embed_raw/linear/embedding_lookup/axis\"\r\nop: \"Const\"\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@source_embed_raw/W\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"value\"\r\n  value {\r\n    tensor {\r\n      dtype: DT_INT32\r\n      tensor_shape {\r\n      }\r\n      int_val: 0\r\n    }\r\n  }\r\n}\r\n, name: \"source_embed_raw/linear/embedding_lookup\"\r\nop: \"GatherV2\"\r\ninput: \"source_embed_raw/W/read\"\r\ninput: \"extern_data/placeholders/data/data\"\r\ninput: \"source_embed_raw/linear/embedding_lookup/axis\"\r\nattr {\r\n  key: \"Taxis\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tindices\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tparams\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@source_embed_raw/W\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 512\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"batch_dims\"\r\n  value {\r\n    i: 0\r\n  }\r\n}\r\n, name: \"source_embed_raw/linear/embedding_lookup/Identity\"\r\nop: \"Identity\"\r\ninput: \"source_embed_raw/linear/embedding_lookup\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 512\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n, name: \"output/rec/target_embed_raw/linear/embedding_lookup/axis\"\r\nop: \"Const\"\r\ninput: \"^output/rec/while/Identity\"\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@output/rec/target_embed_raw/W\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"value\"\r\n  value {\r\n    tensor {\r\n      dtype: DT_INT32\r\n      tensor_shape {\r\n      }\r\n      int_val: 0\r\n    }\r\n  }\r\n}\r\n, name: \"output/rec/target_embed_raw/linear/embedding_lookup\"\r\nop: \"GatherV2\"\r\ninput: \"output/rec/target_embed_raw/linear/embedding_lookup/Enter\"\r\ninput: \"output/rec/output/Reshape_2\"\r\ninput: \"output/rec/target_embed_raw/linear/embedding_lookup/axis\"\r\nattr {\r\n  key: \"Taxis\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tindices\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tparams\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@output/rec/target_embed_raw/W\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 512\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"batch_dims\"\r\n  value {\r\n    i: 0\r\n  }\r\n}\r\n, name: \"output/rec/target_embed_raw/linear/embedding_lookup/Enter\"\r\nop: \"Enter\"\r\ninput: \"output/rec/target_embed_raw/W/read\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@output/rec/target_embed_raw/W\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 22526\r\n        }\r\n        dim {\r\n          size: 512\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"output/rec/while/while_context\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 10\r\n  }\r\n}\r\n, name: \"output/rec/target_embed_raw/linear/embedding_lookup/Identity\"\r\nop: \"Identity\"\r\ninput: \"output/rec/target_embed_raw/linear/embedding_lookup\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 512\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n]\r\n```\r\nThe error is about the /Axis node not being in the frame but also the Reshape child is not in the frame. \r\nIn this small example instead:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\n\r\ndef loop(m):\r\n    x = tf.compat.v1.get_variable(\"x\", dtype=tf.float32, shape=[5, 5], initializer=tf.random_uniform_initializer())\r\n    l = tf.constant(0.005)\r\n    m = tf.math.abs(tf.nn.embedding_lookup(x, y)) + tf.math.abs(m) + l\r\n    return m\r\n\r\ndef cond(m):\r\n    return tf.less(tf.math.reduce_sum(m), tf.Variable(1.0))\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.compat.v1.variable_scope(\"init\", reuse=tf.compat.v1.AUTO_REUSE) as scope:\r\n        y = tf.compat.v1.placeholder(name=\"y\", shape=[5], dtype=tf.int32)\r\n        m = tf.Variable([[0.0]*5]*5, shape=[5, 5], dtype=tf.float32, name=\"m\")\r\n        z = tf.while_loop(body=loop, cond=cond, loop_vars=[m], name=\"z\")[0]\r\n    output = tf.math.reduce_sum(z, name=\"output\")\r\n\r\nnodes = [n for n in graph.as_graph_def().node if \"embedding_lookup\" in n.name]\r\nprint(nodes)\r\nwith tf.compat.v1.Session(graph=graph) as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    print(sess.run(output, feed_dict={y: [0, 1, 2, 3, 4]}))\r\n    tf.compat.v1.saved_model.simple_save(sess, os.path.join(\"export_dir\", \"0\"), inputs={\"y\": y}, outputs={\"output\": output})\r\n```\r\n```\r\n[name: \"init/z/embedding_lookup/axis\"\r\nop: \"Const\"\r\ninput: \"^init/z/Identity\"\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@init/x\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"value\"\r\n  value {\r\n    tensor {\r\n      dtype: DT_INT32\r\n      tensor_shape {\r\n      }\r\n      int_val: 0\r\n    }\r\n  }\r\n}\r\n, name: \"init/z/embedding_lookup\"\r\nop: \"GatherV2\"\r\ninput: \"init/z/embedding_lookup/Enter\"\r\ninput: \"init/z/embedding_lookup/Enter_1\"\r\ninput: \"init/z/embedding_lookup/axis\"\r\nattr {\r\n  key: \"Taxis\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tindices\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tparams\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@init/x\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"batch_dims\"\r\n  value {\r\n    i: 0\r\n  }\r\n}\r\n, name: \"init/z/embedding_lookup/Enter\"\r\nop: \"Enter\"\r\ninput: \"init/x/read\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@init/x\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"init/z/while_context\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 10\r\n  }\r\n}\r\n, name: \"init/z/embedding_lookup/Enter_1\"\r\nop: \"Enter\"\r\ninput: \"init/y\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@init/x\"\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"init/z/while_context\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 10\r\n  }\r\n}\r\n, name: \"init/z/embedding_lookup/Identity\"\r\nop: \"Identity\"\r\ninput: \"init/z/embedding_lookup\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n]\r\n```\r\nThe GatherV2 Op has an Enter and a RefEnter input, both having the same frame, besides the Axis input. In this case it converts successfully to TFLite and the axis not being in the frame does not cause any errors. This works despite the lack of output shapes in this second GraphDef. Any ideas?\r\n\r\nUpdate 13 October 2020: If I don't set tf.enable_control_flow_v2() the conversion succeeds but then the interpreter is not able to use the model.", "comments": ["I converted the existing code to TF2, using the compat.v1 APIs, but with the TF2 behavior enabled (it wasn't before) and I got into the same error as #42410. The error is probably due to a bug in the TFLite implementation of tf.nn.embedding_lookup.\r\nI need to clean up the code a bit and ensure that this is the case, because now I'm getting more errors...\r\n\r\nEdit: I can confirm that I bypassed the problem by replacing embedding_lookup with gather_nd. Apparently there is a bug in Gather for TFLite."]}, {"number": 43839, "title": "Unexpected output shape when trying to convert to Frozen Graph using convert_variables_to_constants_v2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):  Using Deep Learning VM Instance on GCP. Not sure.\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.0\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Tesla T4 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am using this [answer](https://stackoverflow.com/a/61004639/5052482) by TF_Support on StackOverflow to convert a model to to a frozen graph. However, the output shape that I am getting for my custom model is is very different (weird) from the one that's there in my model. More information in the code snippet below.\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n\r\ndef blah():\r\n    jumps = tf.keras.layers.Input(shape=(269, 8))\r\n    loc_a = tf.keras.layers.Input(shape=(32))\r\n    loc_b = tf.keras.layers.Input(shape=(256))\r\n    loc_c = tf.keras.layers.Input(shape=(2))\r\n    def conv_block(x):\r\n        x = tf.keras.layers.Conv1D(filters=32, kernel_size=1, padding='same')(x)\r\n        x = tf.keras.layers.BatchNormalization()(x)\r\n        x = tf.keras.layers.Activation('linear')(x)\r\n        return x\r\n    \r\n    x = jumps\r\n    for i in range(1):\r\n        x = conv_block(x)\r\n    _loc_a = tf.keras.layers.Dense(269)(loc_a)\r\n    _loc_b = tf.keras.layers.Dense(269)(loc_b)\r\n    _loc_c = tf.keras.layers.Dense(269)(loc_c)\r\n    _loc = tf.keras.layers.Concatenate()([_loc_a, _loc_b, _loc_c])\r\n    _loc = tf.keras.layers.Dense(269)(_loc)\r\n    _loc = tf.keras.layers.Reshape((269, 1))(_loc)\r\n    \r\n    x = tf.keras.layers.Concatenate()([_loc, x])\r\n    x= tf.keras.layers.Dense(9)(x)\r\n#     x= tf.keras.layers.Dense(9)(x)\r\n#     x = tf.keras.layers.Flatten()(x)\r\n    \r\n    return tf.keras.Model(inputs=[jumps, loc_a, loc_b, loc_c], outputs=[x])\r\n\r\nmodel = blah()\r\n\r\nmodel.summary()\r\n\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_2 (InputLayer)            [(None, 32)]         0                                            \r\n__________________________________________________________________________________________________\r\ninput_3 (InputLayer)            [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\ninput_4 (InputLayer)            [(None, 2)]          0                                            \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 269)          8877        input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 269)          69133       input_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 269)          807         input_4[0][0]                    \r\n__________________________________________________________________________________________________\r\ninput_1 (InputLayer)            [(None, 269, 8)]     0                                            \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 807)          0           dense[0][0]                      \r\n                                                                 dense_1[0][0]                    \r\n                                                                 dense_2[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv1d (Conv1D)                 (None, 269, 32)      288         input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 269)          217352      concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\nbatch_normalization (BatchNorma (None, 269, 32)      128         conv1d[0][0]                     \r\n__________________________________________________________________________________________________\r\nreshape (Reshape)               (None, 269, 1)       0           dense_3[0][0]                    \r\n__________________________________________________________________________________________________\r\nactivation (Activation)         (None, 269, 32)      0           batch_normalization[0][0]        \r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 269, 33)      0           reshape[0][0]                    \r\n                                                                 activation[0][0]                 \r\n__________________________________________________________________________________________________\r\ndense_4 (Dense)                 (None, 269, 9)       306         concatenate_1[0][0]              \r\n==================================================================================================\r\nTotal params: 296,891\r\nTrainable params: 296,827\r\nNon-trainable params: 64\r\n\r\n# convert model to a frozen graph\r\n\r\nfull_model = tf.function(lambda x: model(x))\r\n\r\nfull_model = full_model.get_concrete_function([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=\"input_0\"),\r\n                                              tf.TensorSpec(model.inputs[1].shape, model.inputs[0].dtype, name=\"input_1\"),\r\n                                              tf.TensorSpec(model.inputs[2].shape, model.inputs[0].dtype, name=\"input_2\"),\r\n                                              tf.TensorSpec(model.inputs[3].shape, model.inputs[0].dtype, name=\"input_3\")])\r\n\r\nfrozen_func = convert_variables_to_constants_v2(full_model)\r\nfrozen_func.graph.as_graph_def(add_shapes=True)\r\nlayers = [op.name for op in frozen_func.graph.get_operations()]\r\nprint(\"-\" * 50)\r\n# print(\"Frozen model layers: \")\r\n# for layer in layers:\r\n#     print(layer)\r\nprint(\"-\" * 50)\r\nprint(\"Frozen model inputs: \")\r\nprint(frozen_func.inputs)\r\nprint(\"Frozen model outputs: \")\r\nprint(frozen_func.outputs)\r\n# Save frozen graph from frozen ConcreteFunction to hard drive\r\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph,\r\n                  logdir=\"./frozen_models\",\r\n                  name=\"frozen_graph.pb\",\r\n                  as_text=False)\r\n\r\nFrozen model inputs: \r\n[<tf.Tensor 'input_0:0' shape=(None, 269, 8) dtype=float32>, <tf.Tensor 'input_1:0' shape=(None, 32) dtype=float32>, <tf.Tensor 'input_2:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>]\r\nFrozen model outputs: \r\n[<tf.Tensor 'Identity:0' shape=(None, None, 9) dtype=float32>]\r\n'./frozen_models/frozen_graph.pb'\r\n\r\n```\r\n\r\nThe output shape of the Frozen Model should match the output shape of the model created. However it does not. \r\n\r\nThe output shape of the model should be `(None, 269, 9)` whereas the output shape of the Frozen Graph is `(None, None, 9)`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["i am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/amahendrakar/513437d86989d697ec6997e66fca335a/43839.ipynb)", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210528, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/d09b5c8b91dd5d1b2877e48a5146e602/43839.ipynb). Thanks!", "Was able to reproduce issue in Tf 2.6, please find the gist **[here](https://colab.research.google.com/gist/kumariko/9b5aff4ed8cd48caf53838cc1893a4ee/43839.ipynb#scrollTo=zv9ho8Ra3nA3)**. Thanks!", "@mdanatg In the meantime, if this is valid, do you think that we could extend https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants_test.py with an expected failing test?", "We don't know if it's valid, it needs more investigation.", "With the @kumariko reproduction with TF 2.6 on the same original model I see this ops output:\r\n\r\n```\r\n[<tf.Tensor 'input_0:0' shape=(None, 269, 8) dtype=float32>]\r\n[<tf.Tensor 'input_2:0' shape=(None, 256) dtype=float32>]\r\n[<tf.Tensor 'input_1:0' shape=(None, 32) dtype=float32>]\r\n[<tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_5/MatMul/ReadVariableOp/resource:0' shape=(32, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_5/MatMul/ReadVariableOp:0' shape=(32, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_5/MatMul:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_5/BiasAdd/ReadVariableOp/resource:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_5/BiasAdd/ReadVariableOp:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_5/BiasAdd:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_6/MatMul/ReadVariableOp/resource:0' shape=(256, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_6/MatMul/ReadVariableOp:0' shape=(256, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_6/MatMul:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_6/BiasAdd/ReadVariableOp/resource:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_6/BiasAdd/ReadVariableOp:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_6/BiasAdd:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_7/MatMul/ReadVariableOp/resource:0' shape=(2, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_7/MatMul/ReadVariableOp:0' shape=(2, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_7/MatMul:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_7/BiasAdd/ReadVariableOp/resource:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_7/BiasAdd/ReadVariableOp:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_7/BiasAdd:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/concatenate_2/concat/axis:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/concatenate_2/concat:0' shape=(None, 807) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_8/MatMul/ReadVariableOp/resource:0' shape=(807, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_8/MatMul/ReadVariableOp:0' shape=(807, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_8/MatMul:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_8/BiasAdd/ReadVariableOp/resource:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_8/BiasAdd/ReadVariableOp:0' shape=(269,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_8/BiasAdd:0' shape=(None, 269) dtype=float32>]\r\n[<tf.Tensor 'model_1/reshape_1/Shape:0' shape=(2,) dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/strided_slice/stack:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/strided_slice/stack_1:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/strided_slice/stack_2:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/strided_slice:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/Reshape/shape/1:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/Reshape/shape/2:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/Reshape/shape:0' shape=(3,) dtype=int32>]\r\n[<tf.Tensor 'model_1/reshape_1/Reshape:0' shape=(None, 269, 1) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/ExpandDims/dim:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/ExpandDims:0' shape=(None, 1, 269, 8) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/ExpandDims_1/ReadVariableOp/resource:0' shape=(1, 8, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/ExpandDims_1/ReadVariableOp:0' shape=(1, 8, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/ExpandDims_1/dim:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/ExpandDims_1:0' shape=(1, 1, 8, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D:0' shape=(None, 1, 269, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/Conv1D/Squeeze:0' shape=(None, 269, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/BiasAdd/ReadVariableOp/resource:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/BiasAdd/ReadVariableOp:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/conv1d_1/BiasAdd:0' shape=(None, 269, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/ReadVariableOp/resource:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/ReadVariableOp:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/add/y:0' shape=() dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/add:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/Rsqrt:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/mul/ReadVariableOp/resource:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/mul/ReadVariableOp:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/mul:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/mul_1:0' shape=(None, 269, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/ReadVariableOp_2/resource:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/ReadVariableOp_2:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/ReadVariableOp_1/resource:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/ReadVariableOp_1:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/mul_2:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/sub:0' shape=(32,) dtype=float32>]\r\n[<tf.Tensor 'model_1/batch_normalization_1/batchnorm/add_1:0' shape=(None, 269, 32) dtype=float32>]\r\n[<tf.Tensor 'model_1/concatenate_3/concat/axis:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/concatenate_3/concat:0' shape=(None, 269, 33) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/free:0' shape=(2,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/axes:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/concat/axis:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/concat:0' shape=(3,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/transpose:0' shape=(None, 269, 33) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Shape:0' shape=(3,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/GatherV2/axis:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/GatherV2:0' shape=(2,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Const:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Prod:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/GatherV2_1/axis:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/GatherV2_1:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Const_1:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Prod_1:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/stack:0' shape=(2,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Reshape:0' shape=(None, None) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/ReadVariableOp/resource:0' shape=(33, 9) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/ReadVariableOp:0' shape=(33, 9) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/MatMul:0' shape=(None, 9) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/Const_2:0' shape=(1,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/concat_1/axis:0' shape=() dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot/concat_1:0' shape=(3,) dtype=int32>]\r\n[<tf.Tensor 'model_1/dense_9/Tensordot:0' shape=(None, None, 9) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/BiasAdd/ReadVariableOp/resource:0' shape=(9,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/BiasAdd/ReadVariableOp:0' shape=(9,) dtype=float32>]\r\n[<tf.Tensor 'model_1/dense_9/BiasAdd:0' shape=(None, None, 9) dtype=float32>]\r\n[]\r\n[<tf.Tensor 'Identity:0' shape=(None, None, 9) dtype=float32>]\r\n```"]}, {"number": 43838, "title": "RPC retries do not work on Istio", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, but it fits on one screen\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): stock `tensorflow/tensorflow:2.2.0` docker image on k8s\r\n- TensorFlow installed from (source or binary): binary (docker image)\r\n- TensorFlow version (use command below): `v2.2.0-rc4-8-g2b96f3662b 2.2.0`\r\n- Python version: `Python 3.6.9`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running a two-worker job *in an istio-enabled namespace* on kubernetes, tensorflow will crash in `tf.keras.layers.Dense()` with `tensorflow.python.framework.errors_impl.UnavailableError` with a very high probability *if worker `0` is not available yet at the moment where worker `1` sends an RPC to it*. Note that in a distributed environment perfect synchronization of tasks is impossible to achieve due to scheduling contraints, network issues, etc. In my experiments the crashed happend when worker `0` started executing more than a second later than worker `1`. The example code injects an artificial 10 second delay when running on task `1`.\r\n\r\n**Describe the expected behavior**\r\n\r\nWorker 1 should retry for a reasonable amount of time (a minute or so?) until worker 0 comes up. This is indeed what happens when *not* running on Istio (e.g. when istio sidecar injection is disabled for the pods by changing the `sidecar.istio.io/inject` annotation in attached manifest to `\"false\"`) then this example job succeeds every time.\r\n\r\nMy suspicion is that the retry code (either in gRPC layer or in tensorflow itself) gets confused in case there is an envoy transparent proxy in between the workers:\r\n- when there is no istio, the process gets a `-ECONNREFUSED`, and knows how to retry this,\r\n- where there is istio, the process establishes a connection (to the local transparent proxy) just fine, but then the proxy reports back a connection error in a higher layer, e.g.:\r\n```\r\n':status', '200'\r\n'content-type', 'application/grpc'\r\n'grpc-status', '14'\r\n'grpc-message', 'upstream connect error or disconnect/reset before headers. reset reason: connection failure'\r\n'server', 'envoy'\r\n'x-envoy-upstream-service-time', '36'\r\n```\r\nand the resulting exception looks different enough for the retry code to not kick in, causing an immediate crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere is a kubernetes manifest sufficient to reproduce the issue. This includes the code, which just creates a model and attempts to compile it.\r\n\r\n**Note:** this needs to be applied against a namespace in which istio sidecar injection is enabled (with `kubectl apply -n YOUR_NS -f` or similar).\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: script\r\ndata:\r\n  mnist-tensorflow.py: |\r\n    from __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n    import logging\r\n    import time\r\n    import sys\r\n\r\n    import tensorflow as tf\r\n\r\n\r\n    def main(pod_name):\r\n        logging.info(\"Started!\")\r\n        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)\r\n\r\n        logging.info(\"pod name: [%s]\", pod_name)\r\n        if pod_name.endswith('0'):\r\n            logging.warning(\"sleeping 10 seconds\")\r\n            time.sleep(10)\r\n\r\n        logging.warning(\"proceeding\")\r\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n        logging.warning(\"strategy created\")\r\n\r\n        with strategy.scope():\r\n            model = tf.keras.Sequential(\r\n                [\r\n                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(28, 28, 1)),\r\n                    tf.keras.layers.MaxPooling2D(),\r\n                    tf.keras.layers.Flatten(),\r\n                    tf.keras.layers.Dense(64, activation=\"relu\"),\r\n                    tf.keras.layers.Dense(10),\r\n                ]\r\n            )\r\n            model.compile(\r\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.SGD(learning_rate=0.5, momentum=0.1),\r\n                metrics=[\"accuracy\"])\r\n\r\n        logging.info(\"Success!\")\r\n\r\n\r\n    if __name__ == \"__main__\":\r\n        logging.basicConfig(format='%(asctime)s.%(msecs)03d %(levelname)-8s %(message)s', level=logging.INFO,\r\n                            datefmt='%Y-%m-%d %H:%M:%S')\r\n        main(sys.argv[1])\r\n---\r\napiVersion: kubeflow.org/v1\r\nkind: TFJob\r\nmetadata:\r\n  name: tf-job-mnist\r\nspec:\r\n  cleanPodPolicy: None  # For easy debugging\r\n  tfReplicaSpecs:\r\n    Worker:\r\n      replicas: 2\r\n      restartPolicy: Never\r\n      template:\r\n        metadata:\r\n          annotations:\r\n            sidecar.istio.io/logLevel: \"debug\"\r\n            sidecar.istio.io/inject: \"true\"\r\n        spec:\r\n          containers:\r\n          - name: tensorflow\r\n            image: tensorflow/tensorflow:2.2.0\r\n            command: [\"python\", \"-u\", \"/tmp/script/mnist-tensorflow.py\", \"$(POD_NAME)\"]\r\n            env:\r\n            - name: TF_CPP_MIN_LOG_LEVEL\r\n              value: \"0\"\r\n            - name: POD_NAME\r\n              valueFrom:\r\n                fieldRef:\r\n                  apiVersion: v1\r\n                  fieldPath: metadata.name\r\n            volumeMounts:\r\n            - name: script\r\n              mountPath: /tmp/script\r\n          volumes:\r\n          - name: script\r\n            configMap:\r\n              name: script\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nHere is the python stack trace that I encounter. Sometimes the error message varies slightly.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/tests/jobs/mnist-tensorflow.py\", line 44, in <module>\r\n    main(sys.argv[1])\r\n  File \"/tmp/tests/jobs/mnist-tensorflow.py\", line 30, in main\r\n    tf.keras.layers.Dense(10),\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 129, in __init__\r\n    self.add(layer)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 456, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 198, in add\r\n    layer(x)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 897, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2416, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 163, in build\r\n    dtype=self.dtype)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 577, in add_weight\r\n    caching_device=caching_device)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 743, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 141, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 66, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1767, in creator_with_resource_vars\r\n    return self._create_variable(next_creator, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 610, in _create_variable\r\n    values.SyncOnReadVariable, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/values.py\", line 694, in create_mirrored_variable\r\n    value_list = real_mirrored_creator(**kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 602, in _real_mirrored_creator\r\n    v = next_creator(**kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2598, in default_variable_creator\r\n    shape=shape)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1434, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1567, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\", line 386, in initial_value_fn\r\n    collective_instance_key)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/collective_ops.py\", line 176, in broadcast_recv\r\n    communication_hint=communication_hint.lower())\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_collective_ops.py\", line 57, in collective_bcast_recv\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: upstream connect error or disconnect/reset before headers. reset reason: connection failure\r\nAdditional GRPC error information:\r\n{\"created\":\"@1601905369.391893191\",\"description\":\"Error received from peer ipv4:192.168.197.248:2222\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"upstream connect error or disconnect/reset before headers. reset reason: connection failure\",\"grpc_status\":14} [Op:CollectiveBcastRecv]\r\n```\r\n\r\nThe interesting pieces of envoy logs are between `2020-10-05T13:42:49.336` (when TF makes the call) and `2020-10-05T13:42:49.391` (where it crashes):\r\n- [fail-log-pod.tf-job-mnist-worker-1.istio-proxy.txt](https://github.com/tensorflow/tensorflow/files/5338766/fail-log-pod.tf-job-mnist-worker-1.istio-proxy.txt) lines 30923-31015\r\n- [fail-log-pod.tf-job-mnist-worker-0.istio-proxy.txt](https://github.com/tensorflow/tensorflow/files/5338769/fail-log-pod.tf-job-mnist-worker-0.istio-proxy.txt) lines 31080-31423\r\n\r\nTF logs are short:\r\n- [fail-log-pod.tf-job-mnist-worker-1.tensorflow.txt](https://github.com/tensorflow/tensorflow/files/5338764/fail-log-pod.tf-job-mnist-worker-1.tensorflow.txt)\r\n- [fail-log-pod.tf-job-mnist-worker-0.tensorflow.txt](https://github.com/tensorflow/tensorflow/files/5338767/fail-log-pod.tf-job-mnist-worker-0.tensorflow.txt)\r\n\r\nFor completeness, mostly irrelevant:\r\n- [fail-log-pod.tf-job-mnist-worker-1.istio-validation.txt](https://github.com/tensorflow/tensorflow/files/5338765/fail-log-pod.tf-job-mnist-worker-1.istio-validation.txt)\r\n- [fail-log-pod.tf-job-mnist-worker-0.istio-validation.txt](https://github.com/tensorflow/tensorflow/files/5338768/fail-log-pod.tf-job-mnist-worker-0.istio-validation.txt)\r\n\r\n", "comments": ["Hi @porridge this might be related to #42498. Can you take a look at the [last comment](https://github.com/tensorflow/tensorflow/issues/42498#issuecomment-702416224) in that thread and let me know if it's helpful? ", "It's related, but the comment it not very useful. I don't think doing the retries in high-level user code is a proper fix.\r\nAlso please note that TF does attempt to retry (as proven by my repro case not failing in non-istio environment), it's just that the retry code is not kicking in when an envoy proxy is involved.\r\n\r\nI already have a dirty workaround of using a `retryPolicy: OnFailure`, but the point of this ticket is to fix this properly."]}, {"number": 43832, "title": "Enable multiple input pipelines with a single machine, MultiWorkerMirroredStrategy's experimental_distribute_datasets_from_function", "body": "**System information**\r\n- TensorFlow version (you are using): TF 2.3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` provides an easy way to perform multi-GPU training on a single instance.  `tf.distribute.experimental.MultiWorkerMirroredStrategy.experimental_distribute_datasets_from_function()` allows for manually sharding the input dataset.  This function gets passed a `tf.distribute.InputContext`.\r\n\r\nOn a single-machine, 8 GPU setup, the function gets called exactly once with the following `input_context`:\r\n```python3\r\ntf.distribute.InputContext(\r\n    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=8\r\n)\r\n```\r\nThe result is that on a single machine, we cannot shard the dataset manually.\r\n\r\nInstead, it would be great if TensorFlow saw a single machine as having 8 differnet input pipelines --- one for each GPU.  The result would enable sharding differently for each GPU.\r\n\r\n**Will this change the current api? How?**\r\nEither the default behavior would change or a flag might need to be added to `MultiWorkerMirroredStrategy` or `experimental_distribute_datasets_from_function` to specify this new behavior.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who requires multiple GPUs to train their models, but doesn't want the extra sophistication of running on multiple machines.\r\n\r\n**Any Other info.**\r\nIt would be great if each input pipeline ran as a seperate python application.  That would enable multi-CPU out of the box if the dataset was designed for single CPU initially.", "comments": ["Hi @jeisinge, `MultiWorkerMirroredStrategy` actually is designed for training on multiple machines, and not a single instance.\r\n`MirroredStrategy` is the strategy for the scenario of a single machine with multiple GPUs.\r\n\r\nCan you provide a little more information about this feature? Specifically, why are you interested in sharding the data differently for each GPU? `MirroredStrategy` is a synchronous training strategy, so I am not clear on why you would want to do a different sharding of the dataset for each GPU. More context here on your use case would be helpful.\r\n\r\nAdditionally, I'm confused about the benefit here, as `MirroredStrategy` already allows for training on multiple GPUs without the extra sophistication of running on multiple machines. Additionally, TF runs on multiple CPU cores out of the box, and `MultiWorkerMirroedStrategy` can be used to distribute training across multiple machines that are CPU only. ", "Howdy @nikitamaia, great questions.  I should have provided additional context.  First, I am working through Python/TF 2 as quickly as I possibly can --- if I say something incorrectly, please correct my understanding!\r\n\r\n### Why `MultiWorkerMirroredStrategy`?\r\nWe first started with `MirroredStrategy`, but ran into the warning ` WARNING:tensorflow:Efficient allreduce is not supported for 23 IndexedSlices`.  After reviewing https://github.com/tensorflow/tensorflow/issues/41898 and additional documentation/YouTubes, it appears that `MultiWorkerMirroredStrategy` has additional optimizations.  When we run, `MultiWorkerMirroredStrategy` outputs information about being optimized for one machine.   And, these optimizations appear to work better for us than `MirroredStrategy`.  Of course, if things change in TF 2.4, than we will have to change with them!\r\n\r\n### Why control the sharding?\r\nOur dataset is comes from parquet files that are partioned by day.  In each file, we might have a couple million examples.  And, the number of files are just a couple per GPU.  Due to this, we cannot shard by file --- or else some GPUs might get one file and other might get four files.  Instead, we shard by batches within the parquet file's row groups.  This enables each GPU to get an equal amount of examples from each day of data --- with no repeats.\r\n\r\nThe additional issue we run into with just having one Input Pipeline is that our Input Pipeline is too slow to feed all of the GPUs.  We have CPU operations like WordPiece that, to my knowledge, cannot be parallelized on the GPU easily.  The result is that on our trains, one CPU is pegged at 100% while the other 30 are doing nothing.  Again, this is fine when we run with one GPU --- the CPU operations can feed one GPU just fine with one CPU.", "Thanks for the extra context. Just to clarify my understanding a bit more, is the central issue here that with your current set up, the input pipeline is a bottleneck? The CPU takes too much time to prepare a batch and send it to the GPUs for training and hence the GPUs sit idle at the end of each training step while the CPU is busy prepping the next batch?", "The situation is that we would like to be able to train faster on a relatively simple setup of a single machine, multi-GPU.  There are two complications.  First, `MirroredStrategy` has issues with our preprocessing that utilizes `SparseTensor.`  The second issue is the one you described - ensuring the GPUs are fully utilized.\r\n\r\nThe first issue is solved by utilizing `MultiWorkerMirroredStrategy`.\r\n\r\nHowever, `MultiWorkerMirroredStrategy` does not appear to support `experimental_distribute_datasets_from_function`.  The result is that we are required to have _one_ data input pipeline that has enough throughput to support multiple GPUs.  This second, additional requirement is complicated by Python's GIL.\r\n\r\nMake sense?  And, of course, if I am looking at the solution space wrong, please let me know --- I am all ears!", "`MultiWorkerMirroredStrategy` does support [experimental_distribute_datasets_from_function](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy#experimental_distribute_datasets_from_function). Did you run into problems with this?\r\n\r\nAs for GPU utilization, have you had a chance to look at any of the performance guides? There is one for [optimizing input pipeline performance](https://www.tensorflow.org/guide/data_performance) and [analyzing input pipeline performance](https://www.tensorflow.org/guide/data_performance_analysis). There is also a guide for [debugging GPU performance in general](https://www.tensorflow.org/guide/gpu_performance_analysis). The guides for input pipelines also show how to parallelize data extraction and data transformations, and how to use the TF Profiler to get a more precise sense of what part of the pipeline is the bottleneck. Let me know if these are helpful.", "I agree that `MultiWorkerMirroredStrategy` works with `experimental_distribute_datasets_from_function` on a multi-GPU system.  It just doesn't work as I expected.  In particular, it doesn't appear to do anything different than processing a regular dataset.\r\n\r\nDuring processing of a regular Dataset via `experimental_distribute_dataset`, one input pipeline is created.  The output of this dataset is distributed to multiple GPUs.\r\n\r\n`experimental_distribute_datasets_from_function` appears to have the exact same behavior.\r\n\r\nIt is my understanding that `experimental_distribute_datasets_from_function` was designed to allow manually sharding of the dataset.  My expectation was that I would have this behavior on a multi-GPU system.  Specifically, I expected `experimental_distribute_datasets_from_function` to call the `dataset_fn` once per GPU.  This would allow me to manually shard my dataset.  However, my `dataset_fn` is only called once.\r\n\r\nI do not believe this is a defect because the documentation states,\r\n\r\n> `dataset_fn` will be called once for each worker in the strategy.\r\n\r\nMy understanding is that on a Multi-GPU system, TF views it as a one worker with 8 GPUs, not eight workers with one GPU.  So, I believe this to be working as designed.\r\n\r\nMy feature request is to enable being able to manually shard per GPU on a multi-GPU system.\r\n\r\nMake sense?\r\n\r\nAnd, thank you for the links.  I completely get the reason why my data pipeline is slow --- Python's GIL doesn't allow for my Python/CPU-based transform to parallelize over multiple CPUs.  There are a number of solutions to this.", "Hello @jeisinge , the API doc says `Each replica on that worker will dequeue one batch of inputs from the local Dataset (i.e. if a worker has two replicas, two batches will be dequeued from the Dataset every step).` So `tf.distribute` will distribute your dataset across different GPUs. Our `tf.data` pipeline has been optimized for multi-GPUs and it will try to utilize your machine resources to not bottleneck the training. So if you see performance issues, it would be useful if you can show us a profile. As for the GIL overhead, I am wondering where the GIL overhead comes from?", "Correct - I believe TensorFlow is working as documented.  However, this design does not work for our needs.  The profiler won't be useful because the operations that limit our throughput are performed before the TF data pipeline.\r\n\r\nSpecifically, we have operations that cannot _easily_ be optimized for TensorFlow.  These operations run in Python where the GIL is used and, thus, run on a single CPU.  The result is that our data pipeline works well for the situation of 1 GPU:1 CPU, but, we still only use one CPU when we scale up to 8 GPUs on a single system --- effectively 8 GPU:1 CPU.  Ideally, as we scale up GPUs, we also scale up python's CPU usage.\r\n\r\nDoes that GIL restriction make sense? Is there another work around?\r\n\r\nOne example of transformations that are not ideal for TensorFlow text processing --- https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py , BERT tokenization, etc.\r\n\r\nTo work around these, we have attempted a couple of workarounds: rewriting preprocessed data, utilizing newer TF Text functionality, and only running on one GPU.  None of these are ideal because they limit batch size, iteration speed, increase storage management requirements, do not fully utilize underlying hardware, or are slower.\r\n\r\nThis feature request is to be able to take a data pipeline (pre-`tf.data` and `tf.data`) that runs well for 1 GPU:1 CPU and ensure it also runs well on an 8-GPU by enabling the pre-`tf.data` pipeline to utilize at least 8 CPUs.  This can be accomplished by running one data pipeline per GPU in separate processes."]}, {"number": 43800, "title": "Possibility to compile python code when @tf.function is used in TensorFlow 2.x ", "body": "**System information**\r\n- TensorFlow version (you are using): **2.3.0**\r\n- Are you willing to contribute it (Yes/No): **No**\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWith TensorFlow 2.x, eager mode is enabled by default, so it is essential to use @tf.function to speed up the performance.\r\ntf.function decorator needs access to the python source code, therefore it makes it impossible to compile our python code and create an executable python build (binary) because the decorator (autograph) always needs access to the python source code. This issue makes it impossible to obfuscate the code and can create a security concern at some situations.  \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nProbably yes, but if there is any workaround with the current api, please let me know!\r\n\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who deploys a deep learning solution in the customer site or cloud or willing to create code obfuscation\r\n", "comments": ["The recommended way to deploy learning solutions is to use [tf.saved_model](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). That generates a GraphDef protobuff which you can then load outside a Python runtime, for example from a C++ binary.\r\nThe graph is a lowering of the original Python code, and generally not possible to reverse.\r\n\r\nThat said, tf.function doesn't directly support ahead of time compilation into an executable binary, and that's a feature to consider.", "Thanks for the comment. tf.saved_model can definitely help when we have done the training stage but in case we have to do on-premise training we can not obfuscate the code and when we have already exposed the code, tf.saved_model can not  help that much!\r\nI am not sure if there is a workaround at the moment but if a new feature can be added (dealing with @tf.function) for the purpose of code obfuscation (or compiling) it will really useful especially for security concerns! ", "@soroosh-rz could you clarify the bit about in-premise training? Did you mean that you cannot use saved_model before training is complete?", "@mdanatg Sorry I did not explain it well but the answer is yes! If I want to elaborate on it, we can consider the case when we have spent time and we have developed a general good model for a specific purpose or platform! Now we have to go to the customer (or several customers) site that has stored the data on-perm in order to perform training to fit our model first to their data. In this case you may not want to expose your code but only adjust (or train) your model to the new data.\r\nIn tech startups and consulting companies this can be common case that you agree to deploy your models (and the downstream application) but not giving all your codes and the mechanics behind it. \r\nWith Tensorflow 1 and the _session_ based approach it is possible to compile the python code and create binary. But if we use tf.function decorator in Tensorflow 2 it is impossible to compile or obfuscate the code. and if we don't use tf.function in eager mode, it becomes slow. ", "@ccrusius to confirm whether resuming training is supported after loading from a saved model", "In the general case I think the answer is \"no,\" as we don't support saving optimizers and their state, for example.", "> In the general case I think the answer is \"no,\" as we don't support saving optimizers and their state, for example.\n\nYes we are still waiting to add a warning with https://github.com/tensorflow/tensorflow/pull/42846", "Has it been solved by any method? I am exactly in the same situation as @soroosh-rz  and looking for a way to bypass.", "I've been able to successfully retrain a saved Keras model, so I think that is a valid alternative at the moment. @tomerk for more info; I think there may be some tutorials around.\r\n\r\nI think this works as a low level with tf.function as well, so long as you only save the model itself, without any of the training scaffolding, and re-create any optimizers/gradients after loading.", "> I've been able to successfully retrain a saved Keras model, so I think that is a valid alternative at the moment. @tomerk for more info; I think there may be some tutorials around.\r\n> \r\n> I think this works as a low level with tf.function as well, so long as you only save the model itself, without any of the training scaffolding, and re-create any optimizers/gradients after loading.\r\n\r\nI don't understand. How would you train further from the saved Keras model without having tf.function in the training step? \r\n\r\nAlso, this issue occurs in the same way when you have to map with parse function in creating TFDataset. So it really needs a way to work around with tf.function", "Ah, you're right, your use case is different.\r\n\r\nWe might be able to resolve the part about autograph: at runtime, autograph saves a temporary file with the transformed Python, and that could be used by the compiler instead.\r\n\r\nAre you using open-source tools for the compilation? If so we could look into adding support for it.", "I am using pyarmor to obfuscate my python script. \r\nSo my brief version of code is like below\r\n```python\r\nclass Trainer():\r\n    def __init__(self, x):\r\n        \"\"\" skip this part for brief explanation \"\"\"\r\n\r\n    def train_step(self):\r\n        @tf.function\r\n        def default_grad(x, y, optimizer, train_metric):\r\n            with tf.GradientTape() as tape:\r\n                logits = self.model(x, training=True)\r\n                loss_value = self.loss_fn(y, logits)\r\n            grads = tape.gradient(loss_value, self.model.trainable_weights)\r\n            optimizer.apply_gradients((grad, var) for (grad, var) in zip(grads, self.model.trainable_weights) if grad is not None)\r\n            train_metric.update_state(y, logits)\r\n            return -loss_value\r\n\r\n        return default_grad\r\n\r\n    def test_step(self):\r\n        @tf.function\r\n        def default_test(x, y, val_metric, val_acc):    \r\n            val_logits = self.model(x, training=False)\r\n            loss_value = self.loss_fn(y, val_logits)\r\n            val_metric.update_state(y, val_logits)\r\n            val_acc.update_state(y, val_logits)\r\n            return -loss_value\r\n\r\n        return default_test\r\n\r\n    def do_train(self):\r\n        train_fn = self.train_step()\r\n        test_fn = self.test_step()\r\n        for i, (x_batch_train, y_batch_train, _) in enumerate(self.train_dataset):\r\n            if i == num_steps:\r\n                break\r\n            train_loss = train_fn(x_batch_train, y_batch_train, optimizer, train_metric)\r\n            if checkpoint:\r\n                for j, (x_batch_val, y_batch_val, _) in enumerate(val_dataset):\r\n                    val_loss = test_fn(x_batch_val, y_batch_val, val_metric, val_acc)\r\n```\r\n\r\nand there are other scripts that makes a component together with the script above. These scripts are obfuscated together through Pyarmor. \r\nAt runtime, there is a mother process .py file that holds every deep learning computation, and this mother process calls the `Trainer` above as its subprocess. So this mother process file is what the pyarmor package decodes at the runtime. \r\n\r\nI would really appreciate your help and advice", "Thanks. Basically this would require autograph to work in offline mode. It doesn't support that at the moment, so it would need some improvements to work. I don't think it can easily work with the current API.\r\n\r\nThe only workaround would be to turn autograph off (i.e. `@tf.function(autograph=False)`), but that may require rewriting your control flow by hand (i.e. you'd need to write `tf.while_loop` instead of `for i in tf_range`. It's worth a try.\r\n\r\nFor adding the necessary autograph support --\r\n\r\nCurrently, autograph reads the python function, and generates a temporary Python file with the transformed code, which it then loads. What we'd need is to be able to tell autograph to run in two modes: (1) generate those files in your sources directory, (2) run in a mode which just imports those sources instead of transforming dynamically again.\r\n\r\nBasically, in mode (1), [this call](https://github.com/tensorflow/tensorflow/blob/fc4e20907f5c828b0b8d26324715184737a20d48/tensorflow/python/autograph/pyct/transpiler.py#L486) would need to save the Python code in your working directory. In mode (2), the same code would need to `import` and then call the function saved in mode (1). That should work with the pyarmor limitations.", "Thank you for your explanation.\r\nIt sounds this job is going to be tough to me but let me try to make it clear. \r\nSo, what I understood is that there is one option that is turning off the autograph while rewriting the control flow, and there is another option that is keeping the autograph by doing mode 1 and 2. \r\nIn mode 1, would the Python code, to be transformed through the call, be the entire script with trainer or just the tf.function part? ", "Yes, sorry you don't have any good options right now.\r\n\r\nThe second option would likely need us to implement some improvements, otherwise it would take quite a bit of work for you to complete.\r\n\r\nFor mode 1, it would be just the tf.function parts, as before. The way I think it could work is that you'd run the python script once (without obfuscation) to trigger the transformations (e.g. a fake training run or something in these lines); after that run, a bunch of new Python source files would appear next to your code, so that pyarmor can pick them up and compile them into the binary. For example if you had a `foo/bar.py` with a `@tf.function` around a `def fn()` function, you'd see a new `foo/bar_fn.py` file or something in those lines.", "Hi @mdanatg \r\nBack again to ask for your help. I have been away from this issue for a while due to other issues, and then now I started to work on this again recently.\r\nSo, the mode 1 and 2 explained above, I managed to get the code converted by the transformation call which looks like below. To help you to understand better, it is about generating anchor boxes and here I am trying to map images with their anchors into TFDataset. \r\n\r\n```Python\r\n# coding=utf-8\r\ndef outer_factory():\r\n\r\n    def inner_factory(ag__):\r\n\r\n        def tf__encode_batch(self, batch_images, gt_boxes, cls_ids, idx_info):\r\n            with ag__.FunctionScope('encode_batch', 'fscope', ag__.STD) as fscope:\r\n                do_return = False\r\n                retval_ = ag__.UndefinedReturnValue()\r\n                images_shape = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(batch_images),), None, fscope)\r\n                batch_size = ag__.ld(images_shape)[0]\r\n                labels = ag__.converted_call(ag__.ld(tf).TensorArray, (), dict(dtype=ag__.ld(tf).float32, size=ag__.ld(batch_size), dynamic_size=True), fscope)\r\n\r\n                def get_state():\r\n                    return (labels,)\r\n\r\n                def set_state(vars_):\r\n                    nonlocal labels\r\n                    (labels,) = vars_\r\n\r\n                def loop_body(itr):\r\n                    \"\"\" something long inside \"\"\"\r\n                try:\r\n                    do_return = True\r\n                    retval_ = (ag__.ld(batch_images), ag__.converted_call(ag__.ld(labels).stack, (), None, fscope), ag__.ld(idx_info))\r\n                except:\r\n                    do_return = False\r\n                    raise\r\n                return fscope.ret(retval_, do_return)\r\n        return tf__encode_batch\r\n    return inner_factory\r\n```\r\nI found the `ag__` is a reference to the Autograph module, but I am not sure how I should do to replace the original py_function with this code above. \r\n\r\nAlso I tried the first method that is converting the function into TF-specific control flow and I am struggling with it too. But I am not sure it would be okay to ask it here because it will be more specifically about using TensorArray in the graph execution rather than the autograph. ", "If it's a one-off, I might be able to help translate the code. If you're looking for a more general solution, read on -\r\n\r\nTo replace the original py_function function, the most common method is to use a decorator. That's what tf.function does, for example.\r\n\r\nSo technically you could write a wrapper function like:\r\n\r\n```\r\ndef cached_autograph(f):\r\n  if <mode 1>:\r\n    ret_fn = tf.autograph.to_graph(f)\r\n    <save ret_fn to persistent file>\r\n  else:\r\n    ret_fn = <import persistent file>\r\n    (re-attach ag__, closure, globals to ret_fn)\r\n  return ret_fn\r\n\r\n@tf.function(autograph=False)\r\n@cached_autograph\r\ndef encode_batch(...)\r\n```\r\n\r\nSide note: if you'd be willing to try rebuilding TF from source, it might be easier to do everything inside the existing infra, because it already does things like attaching the closure and globals to the new function. The integration point would be somewhere around here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/52e8fcf5b5db8a830e4dcde1322ae01ca53dd42c/tensorflow/python/autograph/pyct/transpiler.py#L464\r\n\r\n```\r\ndef transform_function(self, fn, user_context, precompilation=None):\r\n\r\n# ... existing code ...\r\nlogging.log(1, '%s is not cached for subkey %s', fn, cache_subkey)\r\n\r\n# >>> begin new code\r\nif precompilation == 'use':\r\n  nodes, ctx = <import from persistent file>\r\nelse:\r\n# <<< end new code\r\n\r\n  # TODO(mdan): Confusing overloading pattern. Fix.\r\n  nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n\r\n# >>> begin new code\r\n  if precompilation == 'generate':\r\n    <save nodes and ctx to cache file>\r\n# <<< end new code\r\n``` \r\n\r\n`parser.unparse` might be of help transforming nodes into source code.", "@mdanatg Thanks for the explanations, when you suggest to rebuild tf, how one would go to pass `precompilation` to the `transform_function`, is the `tf.function` decorator somehow calling `transform_function` under the hood or is  this another decorator to call additionally, similarly to `cached_autograph` that you suggest above?\r\n\r\nA clarification: AFAIK when (for example) a different numpy array or float value are passed to a tf.function then the function is retraced (a different graph is generated potentially), does this means that only the traces explicitly done in the precompilation='generate' phase from source code will be available later on in the compiled library in precompilation='use' mode? Or is tracing not influenced by this and can be done later by the compiled code, could you clarify this point?\r\n\r\nIs there any plan to increase support for the compilation of tf.functions in the future?\r\n\r\nThanks", "> is the tf.function decorator somehow calling transform_function under the hood\r\n\r\nYes, tf.function calls autograph, from [here](https://github.com/tensorflow/tensorflow/blob/369f2a0ac43cae2f6a486555d870600a5ad8004f/tensorflow/python/framework/func_graph.py#L1136), and autograph eventually calls `transform_function`.\r\n\r\n>  does this means that only the traces explicitly done in the precompilation='generate' phase from source code will be available later on in the compiled library in precompilation='use' mode\r\n\r\nIt's true, autograph is called once for each retrace. However, it only really needs to run once, because the transformed code is reused for all future traces. There is a strong assumption that the precompiled code is invariant to the arguments of the function. For that reason, `transform_function` can afford to employ a [cache](https://github.com/tensorflow/tensorflow/blob/369f2a0ac43cae2f6a486555d870600a5ad8004f/tensorflow/python/autograph/pyct/transpiler.py#L426), and that means that as long as you trace once, the source code should be available for all future retraces.\r\n\r\n> Is there any plan to increase support for the compilation of tf.functions in the future?\r\n\r\nYes, we hope to offer a more powerful interface for compilation in the future, though at the moment we're focused more on resolving some of the semantic inconsistencies of tf.function.", "Thanks a lot for the very clear answers,\r\nI would like to attempt what you suggest, how would you proceed to implement the following steps:\r\n   ` <save nodes and ctx to cache file>` and `nodes, ctx = <import from persistent file>` \r\nare there already some utils to save those that I could use ?\r\n\r\n\r\nI was also trying an alternative route, of using `tf.autograph.to_code` to output source code interpretable and then compile that with `tf.function(autograph=False)`, but I have a problem that I do not know how to define `ag__`\r\ne.g.\r\n```\r\nag__ = tf.autograph\r\n\r\n@tf.function(autograph=False)\r\ndef tf__tanh_loop(x):\r\n    with ag__.FunctionScope('tanh_loop', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\r\n        do_return = False\r\n        retval_ = ag__.UndefinedReturnValue()\r\n\r\n        def get_state():\r\n            return (x,)\r\n\r\n        def set_state(vars_):\r\n            nonlocal x\r\n            (x,) = vars_\r\n\r\n        def loop_body():\r\n            nonlocal x\r\n            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)\r\n            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)\r\n\r\n        def loop_test():\r\n            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)\r\n        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})\r\n        try:\r\n            do_return = True\r\n            retval_ = ag__.ld(x)\r\n        except:\r\n            do_return = False\r\n            raise\r\n        return fscope.ret(retval_, do_return)\r\n\r\n```\r\n`AttributeError: module 'tensorflow._api.v2.autograph' has no attribute 'FunctionScope' `                                                                        \r\n\r\nI tried also with\r\n```\r\nag__ = tf.autograph.operators\r\n```\r\nbut got an error as well\r\n```\r\nAttributeError: module 'tensorflow.python.autograph.operators' has no attribute 'FunctionScope'                                               \r\n```\r\n\r\nI'd be happy to contribute with a pull request if it could be useful", "@mdanatg some further precisations regarding my previous message. I noticed that:\r\n\r\n1. `nodes` is a `gast.gast.FunctionDef` object, as you mentioned above it can be transformed to source code with `parser.unparse(nodes)`. In the specific case of the tanh_loop function aboveit leads to the same code that `tf.autograph.to_code` produced, with the only difference being in the fuction name (`tanh_loop` instead of `tf__tanh_loop`). Output of `parser.unparse(nodes)` below:\r\n```\r\n# coding=utf-8                                                                                                           \r\ndef tanh_loop(x):                                                                                                        \r\n    with ag__.FunctionScope('tanh_loop', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_f\r\neatures=(), internal_convert_user_code=True)) as fscope:                                                                 \r\n        do_return = False\r\n        retval_ = ag__.UndefinedReturnValue()\r\n\r\n        def get_state():\r\n            return (x,)\r\n\r\n        def set_state(vars_):\r\n            nonlocal x\r\n            (x,) = vars_\r\n\r\n        def loop_body():\r\n            nonlocal x\r\n            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)\r\n            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)\r\n\r\n        def loop_test():\r\n            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)\r\n        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})\r\n        try:\r\n            do_return = True\r\n            retval_ = ag__.ld(x)\r\n        except:\r\n            do_return = False\r\n            raise\r\n        return fscope.ret(retval_, do_return)\r\n```\r\nThe string can also be converted back with `parser.parse`\r\nthis means that nodes could be saved in a text file and then converted back. This seems ok.\r\n\r\n2. `ctx` is instead a `tensorflow.python.autograph.pyct.transformer.Context` object, and I saw internally it has other objects references inside like: `EntityInfo`, `naming.Namer` and `converter.ProgramContext`, so in absence of an already made utils, I guess I should go at the end of the chain of objects and save all the parameters needed to create a ctx object and then load them with a function creating the objects pointed by ctx from these parameters and then create the ctx object\r\n\r\n3. Could you give some insights on how to pass `precompilation` to the `transform_function` (passing through `tf.function`, mybe just as an extra kwarg?)\r\n\r\n4. What do you think about the other way which I mentioned above of spelling out the tf.functions with autograph before the compilation and then compile it without autograph, with @tf.function(autograph=False)?\r\nRelated to this, would you happen to know what `ag__` should be for the python interpreter to parse the code?\r\n\r\nPlease let me know your thoughts about this. Thank you", "> `<save nodes and ctx to cache file>` and `nodes, ctx = <import from persistent file>` are there already some utils to save those that I could use ?\r\n\r\nThere are some functions in [parser.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/pyct/parser.py) that might be of help.  \r\n\r\n> how to define `ag__`\r\n\r\nThe module is dynamic at the moment, which has been a source of headackes. It's a bit runaround (lacks a dedicated API), but you could obtain it by calling `api.PyToTF().get_extra_locals()['ag__']` (defined [here](https://github.com/tensorflow/tensorflow/blob/c2ebc9cdbd4332e91d60869c82b473bdfbc97eda/tensorflow/python/autograph/impl/api.py#L186))\r\n\r\n> (tanh_loop instead of tf__tanh_loop)\r\n\r\nI think the function is renamed later, so that should be ok. Not 100% though, you'd have to verify.\r\n\r\n> save all the parameters needed to create a ctx object and then load them with a function creating the objects pointed by ctx from these parameters and then create the ctx object\r\n\r\nThat's about right. The context object was not designed for serialization, but I think it should be straightforward to add a few utilities for doing it, it's a PODO. You may want to add a couple of \"encode/decode\" methods to avoid leaking its contents. The user field might be a bit tricky, we'd probably need to define an interface and require users to implement that interface for serialization.\r\n\r\n> how to pass precompilation to the transform_function (passing through tf.function, mybe just as an extra kwarg?)\r\n\r\nYea, threading an extra kwarg sounds like the best option. It's not immediately clear which functions should receive it, so we'd have to prototype and iterate a bit.\r\n\r\n> What do you think about the other way which I mentioned above of spelling out the tf.functions with autograph before the compilation and then compile it without autograph\r\n\r\nI think that's worth a try as well. If we resolve the `ag__` issue, then it would be fairly clean.\r\n\r\n\r\n"]}, {"number": 43790, "title": "Google Collab error for TPU - UnavailableError: {{function_node __inference_train_function_99378}} failed to connect to all addresses ", "body": "No longer able to train model using google cloud TPU on my [gist](https://colab.research.google.com/gist/ScruffySilky/f681aca8130f40555b41f98323887c15/42038.ipynb?authuser=1&pli=1), it was training fine 2 months ago and now I get the following error:\r\n\r\n```\r\nUnavailableError: {{function_node __inference_train_function_99378}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1601903304.230958587\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1601903304.089639211\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node IteratorGetNext}}]]\r\n```\r\n\r\nSeems related to this issue: https://github.com/tensorflow/tensorflow/issues/43037\r\n", "comments": ["@ScruffySilky \r\nPlease refer to these issues and let us know if the issue exist: [link](https://stackoverflow.com/questions/63523682/error-training-keras-model-on-google-colab-using-tpu-runtime) #39099 #43125 #42038", "> @ScruffySilky\r\n> Please refer to these issues and let us know if the issue exist: [link](https://stackoverflow.com/questions/63523682/error-training-keras-model-on-google-colab-using-tpu-runtime) #39099 #43125 #42038\r\n\r\nOf the solutions provided, this one https://github.com/tensorflow/tensorflow/issues/39099 is the only one that removes the error: using !pip install tf-nightly==2.2.0-dev20200505, however it outputs a different error and I still cannot proceed, [Gist](https://colab.research.google.com/gist/ScruffySilky/f681aca8130f40555b41f98323887c15/42038.ipynb#scrollTo=BCcpB-9_Pni4): \r\n\r\n```\r\nEpoch 1/10\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-7-6ac9763b188d> in <module>()\r\n    129 \r\n    130 generator = data_generator(texts, train_features, 1, max_sequence)\r\n--> 131 model.fit_generator(generator, steps_per_epoch=steps, epochs=10, callbacks=callbacks_list, verbose=1)\r\n    132 model.save(mydrive + '/output/weights.hdf5')\r\n\r\n7 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nNotFoundError: 'OptimizeDatasetV2' is neither a type of a primitive operation nor a name of a function registered in binary running on n-dde0e0b9-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:__inference_train_function_40349]\r\n```", "@ScruffySilky \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/40622#issuecomment-654488935), try with tf 2.3 and let us know.", "Have just tried, using tf 2.3.0 reverts to the same error: \r\n\r\n```\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-8-6ac9763b188d> in <module>()\r\n    129 \r\n    130 generator = data_generator(texts, train_features, 1, max_sequence)\r\n--> 131 model.fit_generator(generator, steps_per_epoch=steps, epochs=10, callbacks=callbacks_list, verbose=1)\r\n    132 model.save(mydrive + '/output/weights.hdf5')\r\n\r\n16 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: {{function_node __inference_train_function_56800}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1602080688.607997979\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1602080688.315542576\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node IteratorGetNext}}]]\r\n```\r\n", "In the past this error message has indicated a version mismatch. Can you first try and import from tensorflow.keras instead of native keras?", "> In the past this error message has indicated a version mismatch. Can you first try and import from tensorflow.keras instead of native keras?\r\n\r\nHave done so, receive the same error, [GIST](https://colab.research.google.com/gist/ScruffySilky/1db7bf789b92f9d6523b72cc8ccedb75/42038.ipynb?authuser=1)", "Facing the same issue with tf 2.3.0", "Did anyone find solution to this? Facing the same issue on colab\r\n", "This is still a persistent issue", "same here... just encounter similar error\r\n\r\nTensorflow 2.4.0", "Was able to reproduce the error in Tensorflow 2.5 using colab TPU, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/5bf64a99404d6d04ccb73ec2b3f5469d/43790.ipynb). Thanks!", "Anybody able to solve this error ? My code is stuck at this.", "Still no solution? Should we consider this to not using generators on TPU?", "Has anybody been able to solve this? \r\n\r\nI'm trying to run BERT on Google Colab TPU, however I'm getting a similar error. \r\nTensorflow version 2.8.0\r\nCode I'm using for loading the TPU is vastly based on the original code for pre-training T5 by Google taken from [here:](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb#scrollTo=Set_Up) \r\n```\r\nprint(\"Installing dependencies...\")\r\n%tensorflow_version 2.x\r\n\r\nimport functools\r\nimport os\r\nimport time\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\r\n\r\nimport tensorflow.compat.v1 as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nBASE_DIR = \"gs://bucket-xx\" #@param { type: \"string\" }\r\nif not BASE_DIR or BASE_DIR == \"gs://\":\r\n  raise ValueError(\"You must enter a BASE_DIR.\")\r\nDATA_DIR = os.path.join(BASE_DIR, \"data/text.csv\")\r\nMODELS_DIR = os.path.join(BASE_DIR, \"models/bert\")\r\nON_CLOUD = True\r\n\r\n\r\nif ON_CLOUD:\r\n  print(\"Setting up GCS access...\")\r\n  import tensorflow_gcs_config\r\n  from google.colab import auth\r\n  # Set credentials for GCS reading/writing from Colab and TPU.\r\n  TPU_TOPOLOGY = \"v2-8\"\r\n  try:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n    TPU_ADDRESS = tpu.get_master()\r\n    print('Running on TPU:', TPU_ADDRESS)\r\n  except ValueError:\r\n    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n  auth.authenticate_user()\r\n  tf.enable_eager_execution()\r\n  tf.config.experimental_connect_to_host(TPU_ADDRESS)\r\n  tensorflow_gcs_config.configure_gcs_from_colab_auth()\r\n\r\ntf.disable_v2_behavior()\r\n\r\n# Improve logging.\r\nfrom contextlib import contextmanager\r\nimport logging as py_logging\r\n\r\nif ON_CLOUD:\r\n  tf.get_logger().propagate = False\r\n  py_logging.root.setLevel('INFO')\r\n\r\n@contextmanager\r\ndef tf_verbosity_level(level):\r\n  og_level = tf.logging.get_verbosity()\r\n  tf.logging.set_verbosity(level)\r\n  yield\r\n  tf.logging.set_verbosity(og_level)\r\n```\r\nCode I'm using to run BERT is this: \r\n```\r\n!python /content/scripts/run_mlm.py \\\r\n--model_name_or_path bert-base-cased \\\r\n--tpu_num_cores 8 \\\r\n--validation_split_percentage 20 \\\r\n--line_by_line \\\r\n--learning_rate 2e-5 \\\r\n--per_device_train_batch_size 128 \\\r\n--per_device_eval_batch_size 256 \\\r\n--num_train_epochs 4 \\\r\n--output_dir MODELS_DIR \\\r\n--train_file /content/text.csv\r\n```\r\n\r\nthe `run_mlm.py` script can be seen [here](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/language-modeling/run_mlm.py).\r\n\r\nFull error message can be seen [here](https://pastebin.com/93EW1GuV).\r\n\r\nAny help is much appreciated thanks.", "@[Dimiftb](https://github.com/Dimiftb)  I have the same issue, did you achieve to resolve it? ", "@JessicaLopezEspejel Unfortunately not. I had to resort to using GPU. Please let me know if you find a solution.", "@Dimiftb in this [link](https://github.com/google-research/text-to-text-transfer-transformer/issues/1003) I explain the solution it worked for me. \r\n\r\nI hope it helps you. ", "@JessicaLopezEspejel It seems that you've got an authentication issue with your bucket rather than what is described here. My error message is:\r\n```\r\n(0) INTERNAL: {{function_node __inference_train_function_60782}} failed to connect to all addresses\r\n```\r\n\r\nCommenting out the lines below doesn't really solve the issue.\r\n```\r\ntf.config.experimental_connect_to_host(TPU_ADDRESS)\r\ntensorflow_gcs_config.configure_gcs_from_colab_auth()\r\n```\r\n\r\nAppreciate your help anyway."]}, {"number": 43787, "title": "Interpreter takes too much time to  return results while similar iOS app makes it 10 times faster", "body": "I use following code to create Interpreter, prepare image and execute task to get proper results:\r\n\r\n    private var imageData: ByteBuffer\r\n    private var outputBuffer = Array(1) { Array(256) { Array(256) { FloatArray(NUM_CLASSES) } } }\r\n    private var gpuDelegate: GpuDelegate? = null\r\n    private val interpreter: Interpreter\r\n    private var intValues: IntArray\r\n\r\n    init {\r\n        interpreter = getInterpreter(context, tfLiteModel)\r\n        interpreter.allocateTensors()\r\n        imageData = ByteBuffer.allocateDirect(1 * inputWidth * inputHeight * 3 * 4)\r\n        imageData.order(ByteOrder.nativeOrder())\r\n        intValues = IntArray(inputWidth * inputHeight)\r\n\r\n    }\r\n\r\n    private fun getInterpreter(\r\n        context: Context,\r\n        tfLiteModel: String\r\n    ): Interpreter {\r\n\r\n        val compatList = CompatibilityList()\r\n        val tfLiteOptions = Interpreter.Options().apply {\r\n            if (compatList.isDelegateSupportedOnThisDevice) {\r\n                val delegateOptions = compatList.bestOptionsForThisDevice\r\n                delegateOptions.setPrecisionLossAllowed(true)\r\n                this.addDelegate(GpuDelegate(delegateOptions))\r\n            } else {\r\n                setNumThreads(NUM_THREADS)\r\n            }\r\n        }\r\n        return Interpreter(loadModelFile(context, tfLiteModel), tfLiteOptions)\r\n    }\r\n\r\n    @Throws(IOException::class)\r\n    private fun loadModelFile(context: Context, modelFile: String): MappedByteBuffer {\r\n        val fileDescriptor = context.assets.openFd(modelFile)\r\n        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)\r\n        val fileChannel = inputStream.channel\r\n        val startOffset = fileDescriptor.startOffset\r\n        val declaredLength = fileDescriptor.declaredLength\r\n        val retFile = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\r\n        fileDescriptor.close()\r\n        return retFile\r\n    }\r\n\r\n\r\n    fun close() {\r\n        interpreter?.close()\r\n        gpuDelegate?.close()\r\n    }\r\n\r\n    fun prepareImage() {\r\n        imageData.rewind()\r\n        for (i in 0 until intValues.size) {\r\n            val pixelValue = intValues[i]\r\n            imageData.putFloat((((pixelValue shr 16) and 0xFF) - IMAGE_MEAN) / IMAGE_STD)\r\n            imageData.putFloat((((pixelValue shr 8) and 0xFF) - IMAGE_MEAN) / IMAGE_STD)\r\n            imageData.putFloat(((pixelValue and 0xFF) - IMAGE_MEAN) / IMAGE_STD)\r\n        }\r\n        imageData.rewind()\r\n    }\r\n\r\n    fun execute(bitmap: Bitmap): Array<Array<Array<FloatArray>>> {\r\n\r\n        bitmap.getPixels(intValues, 0, bitmap.width, 0, 0, bitmap.width, bitmap.height)\r\n        prepareImage()\r\n        val timing = System.currentTimeMillis()\r\n        interpreter.run(imageData, outputBuffer)\r\n        Log.d(\"executetiming\", \"run() - ${System.currentTimeMillis() - timing}\")\r\n        return outputBuffer\r\n\r\n    }\r\n\r\n    companion object {\r\n        private const val NUM_CLASSES: Int = 2\r\n        private const val IMAGE_MEAN = 127.5f\r\n        private const val IMAGE_STD = 127.5f\r\n        private const val NUM_THREADS = 4\r\n    }\r\n}\r\nThe app is something between object recognition and image segmentation. I recognize certain objects with my camera and put colors on them pixel by pixel using Canvas.\r\nThe problem is that I am getting following timings:\r\n\r\n> 2020-10-05 10:02:34.134 27093-27278/com.example.sampleappkt D/executetiming: run() - 550\r\n2020-10-05 10:02:34.841 27093-27278/com.example.sampleappkt D/executetiming: run() - 625\r\n2020-10-05 10:02:35.603 27093-27278/com.example.sampleappkt D/executetiming: run() - 645\r\n2020-10-05 10:02:36.345 27093-27278/com.example.sampleappkt D/executetiming: run() - 608\r\n2020-10-05 10:02:37.068 27093-27278/com.example.sampleappkt D/executetiming: run() - 628\r\n2020-10-05 10:02:37.804 27093-27278/com.example.sampleappkt D/executetiming: run() - 640\r\n2020-10-05 10:02:38.462 27093-27278/com.example.sampleappkt D/executetiming: run() - 591\r\n2020-10-05 10:02:39.187 27093-27278/com.example.sampleappkt D/executetiming: run() - 632\r\n2020-10-05 10:02:39.921 27093-27278/com.example.sampleappkt D/executetiming: run() - 650\r\n2020-10-05 10:02:40.669 27093-27278/com.example.sampleappkt D/executetiming: run() - 627\r\n2020-10-05 10:02:41.387 27093-27278/com.example.sampleappkt D/executetiming: run() - 621\r\n2020-10-05 10:02:42.126 27093-27278/com.example.sampleappkt D/executetiming: run() - 648\r\n2020-10-05 10:02:42.825 27093-27278/com.example.sampleappkt D/executetiming: run() - 635\r\n2020-10-05 10:02:43.527 27093-27278/com.example.sampleappkt D/executetiming: run() - 636\r\n2020-10-05 10:02:44.203 27093-27278/com.example.sampleappkt D/executetiming: run() - 576\r\n2020-10-05 10:02:44.952 27093-27278/com.example.sampleappkt D/executetiming: run() - 654\r\n2020-10-05 10:02:45.670 27093-27278/com.example.sampleappkt D/executetiming: run() - 649\r\n2020-10-05 10:02:46.399 27093-27278/com.example.sampleappkt D/executetiming: run() - 667\r\n2020-10-05 10:02:47.001 27093-27278/com.example.sampleappkt D/executetiming: run() - 509\r\n2020-10-05 10:02:47.579 27093-27278/com.example.sampleappkt D/executetiming: run() - 495\r\n2020-10-05 10:02:48.321 27093-27278/com.example.sampleappkt D/executetiming: run() - 660\r\n2020-10-05 10:02:49.053 27093-27278/com.example.sampleappkt D/executetiming: run() - 624\r\n2020-10-05 10:02:49.764 27093-27278/com.example.sampleappkt D/executetiming: run() - 617\r\n2020-10-05 10:02:50.520 27093-27278/com.example.sampleappkt D/executetiming: run() - 659\r\n2020-10-05 10:02:51.317 27093-27278/com.example.sampleappkt D/executetiming: run() - 694\r\n2020-10-05 10:02:52.099 27093-27278/com.example.sampleappkt D/executetiming: run() - 658\r\n\r\nI've checked performance using app from here: https://www.tensorflow.org/lite/performance/measurement\r\nAnd I got this:\r\n![deeplab_float32_converted](https://user-images.githubusercontent.com/55881948/95068369-81de6a00-0705-11eb-9933-34d46d9188e9.png)\r\n\r\nI use Samsung Galaxy S7 Edge for my tests, but I think even if it isn't newest phone those timings shouldn't be that high.\r\nModel used is Mobilenet.\r\nSame model needs 80ms overal when used on iPhone 8 with 2 threads.\r\nHow can I get rid of \"Warmup\" process launched everytime?", "comments": ["@wiggy\r\nWhat version of tf are you using, i see that you have not filled in the issue template.", "I'm sorry, I forgot about that.\r\nI tested this with 2 versions:\r\n0.0.0-nightly - those results you can see on my first post\r\n2.3.0 - there was even worse because i got timings between 750-850ms", "this is strange, can you try use the bench tool to just benchmark the model?\r\n\r\nhttps://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary", "I don't really know how to work with bazel :/ that's why I've made steps included in https://www.tensorflow.org/lite/performance/measurement only", "Hi Chao, can you help take a look?\r\n\r\nthanks!", "It may help, but that doesn't completely resolve my problem, but I've tested the app on **Xiaomi POCO F2 Pro** and times had shortened to ~130ms but it is still 2 times longer than iPhone8.  POCO F2 Pro is slightly better that iPhone 11 Max in benchmarks, so it has to be much much better than iPhone 8 but still slower. Probably it is about platforms. Apple makes libraries for their and only their hardware and ofc software (iOS) but could it be that much faster? ", "it's possible the benchmark is running on the small core in Xiaomi but big core in the iPhone", "Actually I am talking about logs from my app because Xiaomi blocked benchmark somehow (actually logcat) even if I disabled all security options on the device. My interpreter initialization looks like this:\r\n ```\r\nval compatList = CompatibilityList()\r\n        val tfLiteOptions = Interpreter.Options().apply {\r\n            if (compatList.isDelegateSupportedOnThisDevice) {\r\n                val delegateOptions = compatList.bestOptionsForThisDevice\r\n                delegateOptions.setPrecisionLossAllowed(true)\r\n                this.addDelegate(GpuDelegate(delegateOptions))\r\n            } else {\r\n                setNumThreads(NUM_THREADS) // 4 threads\r\n            }\r\n        }\r\n        return Interpreter(loadModelFile(context, tfLiteModel), tfLiteOptions)\r\n```\r\nLooks like Galaxy S7 is working on 4 threads with CPU because warmup + inference gives >500ms so same result as I get from my app. I've checked that Xiaomi works using GPU acceleration."]}, {"number": 43783, "title": "bitwise op with 1 msb throws in tf.function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\ntf 2.2, windows 10 (can't install tf 2.3 because various open issues)\r\ncuda 10.1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.2.0-57-g25fba035f3 2.2.1\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.int32)])\r\ndef f(t):\r\n    return tf.bitwise.bitwise_and(t, 0b10000000000000000000000000000000)\r\n\r\nf(tf.constant(2, dtype=tf.int32))\r\n```\r\nthrows `OverflowError: Python int too large to convert to C long`\r\n\r\n(Works fine without `tf.function`)\r\n\r\n**Describe the expected behavior**\r\nshould not throw (return 0 in this case)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<root>/t.py\", line 8, in <module>\r\n    f(tf.constant(2, dtype=tf.int32))\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"<site-packages>\\tensorflow\\python\\framework\\func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"<site-packages>\\tensorflow\\python\\eager\\def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"<site-packages>\\tensorflow\\python\\framework\\func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.autograph.impl.api.StagingError: in user code:\r\n\r\n    <root>/t.py:6 f  *\r\n        return tf.bitwise.bitwise_and(t, 0b10000000000000000000000000000000)\r\n    <site-packages>\\tensorflow\\python\\ops\\gen_bitwise_ops.py:81 bitwise_and  **\r\n        \"BitwiseAnd\", x=x, y=y, name=name)\r\n    <site-packages>\\tensorflow\\python\\framework\\op_def_library.py:470 _apply_op_helper\r\n        preferred_dtype=default_dtype)\r\n    <site-packages>\\tensorflow\\python\\framework\\ops.py:1341 convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    <site-packages>\\tensorflow\\python\\framework\\tensor_conversion_registry.py:52 _default_conversion_function\r\n        return constant_op.constant(value, dtype, name=name)\r\n    <site-packages>\\tensorflow\\python\\framework\\constant_op.py:262 constant\r\n        allow_broadcast=True)\r\n    <site-packages>\\tensorflow\\python\\framework\\constant_op.py:300 _constant_impl\r\n        allow_broadcast=allow_broadcast))\r\n    <site-packages>\\tensorflow\\python\\framework\\tensor_util.py:452 make_tensor_proto\r\n        nparray = np.array(values, dtype=np_dt)\r\n\r\n    OverflowError: Python int too large to convert to C long\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@amitport \r\nI ran the code shared on tf nightly and do not face any issues, please try on nightly and update.\r\nPlease find the gist for the [same here](https://colab.research.google.com/gist/Saduf2019/512b0732a1f3fd88eea53391ec3ca52a/untitled418.ipynb).\r\n\r\nWith respect to the error faced, please refer to [this link](https://stackoverflow.com/questions/38314118/overflowerror-python-int-too-large-to-convert-to-c-long-on-windows-but-not-ma) and let us know, as i can see the error does not occur on [tf 2.2](https://colab.research.google.com/gist/Saduf2019/f43012a89326ebf68b5cf83178aa114a/untitled424.ipynb)", "@Saduf2019 the problem is on windows and tf@2.2 and the number is not bigger than `sys.maxsize`.\r\n\r\n(I think it is important to note again tf@2.3 and nightly have many issues on windows, I, and many others can't upgrade and verify this on those versions) in any case if you manage to run this with windows in any version it will be interesting to know.\r\n\r\nAlso note that it runs fine without the `tf.function` annotation"]}]