[{"number": 10346, "title": "[OpenCL] Cleans reverse_op.cc", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n"]}, {"number": 10345, "title": "[OpenCL] Cleans sendrecv_ops.cc", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10344, "title": "[OpenCL] Cleans sequence_ops.cc", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10343, "title": "[OpenCL] Cleans relu ops", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10342, "title": "Compilation error when install with CUDA 7.0: image_ops_gpu.cu.pic.o was not created", "body": "Similar to \r\n[https://github.com/tensorflow/tensorflow/issues/10258](url) \r\nand \r\n[https://stackoverflow.com/questions/44116381/error-when-install-tensorflow-from-source](url), \r\nno working solutions yet.\r\nWhen I'm trying to install tensorflow r1.1 from source with CUDA 7.0, this error comes up:\r\n```\r\n1 error detected in the compilation of \"/tmp/tmpxft_00005478_00000000-10_image_ops_gpu.cu.compute_52.cpp1.ii\".\r\nERROR: /root/workspace/tensorflow/tensorflow/contrib/image/BUILD:20:1: output 'tensorflow/contrib/image/_objs/python/ops/_image_ops_gpu/tensorflow/contrib/image/kernels/image_ops_gpu.cu.pic.o' was not created.\r\nERROR: /root/workspace/tensorflow/tensorflow/contrib/image/BUILD:20:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nEnvironment is Ubuntu 14.04, CUDA 7.0, CUDNN 4, gcc 4.8.4, bazel 0.4.5, tensorflow r1.1.\r\nBecause I don't have the privilege to upgrade the cuda driver version 346.46, which does not support CUDA 7.5 or 8.0, I have to stay on CUDA 7.0.\r\nCould anyone help me? Thanks! ", "comments": ["We currently support building with CUDA 8.0.\r\nBuilding with earlier CUDA versions is supported only as best effort. However these days there are a few higher priority issues we need to focus on.\r\nI will mark this issue as community support, maybe someone from the community can help."]}, {"number": 10341, "title": "[OpenCL] Cleans Slice op", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.\r\n", "stage_op_test timeout known issue, merging."]}, {"number": 10340, "title": "[OpenCL] Cleans reduction ops", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 10339, "title": "[OpenCL] Cleans Pad op", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "stage_op_test flake is a known issue. Merging."]}, {"number": 10338, "title": "tensorflow.Session() crashes if executed after importing scipy.optimize and pytorch", "body": "### Configuration\r\n\r\nPython version: 3.5.2 (same for Python 3.6)\r\nSciPy version: 0.19.0\r\nPyTorch version: 0.1.12_2\r\nTensorFlow version: 1.1.0\r\nHost system: Ubuntu 16.04\r\n\r\n### Dockerfile to reproduce my setup:\r\n\r\n```\r\nFROM nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04\r\n\r\nRUN apt-get update -qq \\\r\n && apt-get install -yq -qq --no-install-recommends \\\r\n    python3 \\\r\n    python3-dev \\\r\n    curl \\\r\n    ca-certificates\r\nRUN curl -O https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py && rm get-pip.py\r\nRUN pip3 install numpy scipy\r\nRUN pip3 install --no-cache-dir tensorflow-gpu\r\nRUN pip3 install --no-cache-dir http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp35-cp35m-linux_x86_64.whl\r\n```\r\n\r\n### Describe the problem\r\n\r\n```\r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import scipy.optimize\r\n>>> import torch\r\n>>> import tensorflow\r\n>>> tensorflow.Session()\r\n*** Error in `python3': free(): invalid pointer: 0x00007f28329efac0 ***\r\n```\r\n\r\n### Output\r\n\r\nThe above error message is followed by a huge backtrace that starts like this ...\r\n\r\n```\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f28796ed7e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x7fe0a)[0x7f28796f5e0a]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f28796f998c]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_stringbufIcSt11char_traitsIcESaIcEE8overflowEi+0x181)[0x7f28377aefa1]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_streambufIcSt11char_traitsIcEE6xsputnEPKcl+0x89)[0x7f2837805e79]\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libshm.so(_ZSt16__ostream_insertIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_PKS3_l+0x1c5)[0x7f2832764235]\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libshm.so(_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc+0x27)[0x7f28327644f7]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20BaseGPUDeviceFactory17GetValidDeviceIdsERKSsPSt6vectorIiSaIiEE+0x7c7)[0x7f27fb0a8127]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x15a)[0x7f27fb0a9dca]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorIPNS_6DeviceESaIS8_EE+0x17d)[0x7f27fb0d2bad]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow20DirectSessionFactory10NewSessionERKNS_14SessionOptionsE+0x98)[0x7f27fb0904c8]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow10NewSessionERKNS_14SessionOptionsEPPNS_7SessionE+0x127)[0x7f27fb103c07]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x114c081)[0x7f27f968e081]\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0xe8670b)[0x7f27f93c870b]\r\npython3(PyCFunction_Call+0x4f)[0x4e9b9f]\r\npython3(PyEval_EvalFrameEx+0x614)[0x524414]\r\npython3[0x52d2e3]\r\npython3(PyEval_EvalFrameEx+0x50ee)[0x528eee]\r\npython3(PyEval_EvalCodeEx+0x88a)[0x52e87a]\r\npython3[0x4ebd38]\r\npython3(PyObject_Call+0x47)[0x5b7167]\r\npython3[0x4f413e]\r\n\r\n...\r\n```\r\n\r\nand ends like this\r\n\r\n```\r\n7f2879cf3000-7f2879cf4000 rw-s cfdf2000 00:2e 14                         /dev/nvidia0\r\n7f2879cf4000-7f2879cf5000 rw-s 18da776000 00:2e 12                       /dev/nvidiactl\r\n7f2879cf5000-7f2879e7a000 rw-p 00000000 00:00 0 \r\n7f2879e7a000-7f2879e7b000 rw-s cfdf2000 00:2e 14                         /dev/nvidia0\r\n7f2879e7b000-7f2879e7c000 rw-s 1ec635d000 00:2e 12                       /dev/nvidiactl\r\n7f2879e7c000-7f2879e7d000 rw-s cfdf2000 00:2e 14                         /dev/nvidia0\r\n7f2879e7d000-7f2879e7e000 rw-s 1ffafd3000 00:2e 12                       /dev/nvidiactl\r\n7f2879e7e000-7f2879e7f000 rwxp 00000000 00:00 0 \r\n7f2879e7f000-7f2879e81000 rw-p 00000000 00:00 0 \r\n7f2879e81000-7f2879e82000 r--p 00025000 08:01 3973057                    /lib/x86_64-linux-gnu/ld-2.23.so\r\n7f2879e82000-7f2879e83000 rw-p 00026000 08:01 3973057                    /lib/x86_64-linux-gnu/ld-2.23.so\r\n7f2879e83000-7f2879e84000 rw-p 00000000 00:00 0 \r\n7ffe1932c000-7ffe1934d000 rw-p 00000000 00:00 0                          [stack]\r\n7ffe193bf000-7ffe193c1000 r--p 00000000 00:00 0                          [vvar]\r\n7ffe193c1000-7ffe193c3000 r-xp 00000000 00:00 0                          [vdso]\r\nffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\r\nAborted (core dumped)\r\n```", "comments": ["I should mention that the crash does NOT happen if the imports are in a different order or any of the imports is left out. Very obscure!", "Looks like some sort of GPU resource conflict? @mrry @zheng-xq ", "TensorFlow is the only one using the GPU. PyTorch does not access it just because its imported. In particular, nvidia-smi does not report any GPU processes after the imports and before the session initialization.", "Last time I checked pytorch is exposing a lot of symbols it shouldn't have exposed. And this very often caused segfaults when used with other libraries.", "So should I report this bug to PyTorch?\r\n\r\nAny idea for a workaround? My tests have to import both \u2026", "Nagging Assignee @rohan100jain: It has been 440 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still a problem? Its been a while.", "Using the latest versions of TensorFlow, PyTorch and SciPy, this does not seem to be a problem anymore:\r\n\r\n```\r\nFROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\r\n\r\nRUN apt-get update -qq \\\r\n && apt-get install -yq -qq --no-install-recommends \\\r\n    python3 \\\r\n    python3-dev \\\r\n    curl \\\r\n    ca-certificates\r\nRUN curl -O https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py && rm get-pip.py\r\nRUN pip3 install numpy scipy\r\nRUN pip3 install --no-cache-dir \"tensorflow-gpu==1.10.1\"\r\nRUN pip3 install --no-cache-dir \"torch==0.4.1\"\r\n```\r\n\r\nworks as expected:\r\n\r\n```\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import scipy.optimize\r\n>>> import torch\r\n>>> import tensorflow\r\n>>> \r\n>>> tensorflow.Session()\r\n2018-08-29 06:06:15.578937: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-08-29 06:06:16.351733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 10.91GiB freeMemory: 1.00GiB\r\n2018-08-29 06:06:16.351830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-29 06:06:16.950514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-29 06:06:16.950610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-29 06:06:16.950633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-29 06:06:16.950953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 729 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n<tensorflow.python.client.session.Session object at 0x7f1113676dd8>\r\n>>> \r\n```"]}, {"number": 10337, "title": "Merge rc1 back into master.", "body": "", "comments": ["http://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/5055/"]}, {"number": 10336, "title": "[OpenCL] Cleans pack and unpack ops", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please."]}, {"number": 10335, "title": "[OpenCL] Cleans dense_update_ops", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Possible infra issue in the failing test.\r\nJenkins, test this please.", "known flake, merging."]}, {"number": 10334, "title": "[OpenCL] Cleans debug ops", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@caisq One of the Linux GPU tests TIMEOUT'ed .. I can't see how that's related to this changeset.", "Yeah, that timeout was unrelated. Thanks for the PR.", "No problem! I have plenty more :D"]}, {"number": 10333, "title": "[OpenCL] Cleans variable op", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10332, "title": "[Feature request] dynamically catch exceptions in TensorFlow as part of the graph execution", "body": "Via [this StackOverflow question](https://stackoverflow.com/questions/44137542/dynamically-catch-exceptions-in-tensorflow-as-part-of-the-graph-execution):\r\n\r\nE.g. the `QueueBase.dequeue` function can raise an `OutOfRangeError` exception which I will receive in Python from the `Session.run` call. It would be nice to catch the exception inside the graph, similar as `tf.cond`. E.g. something like:\r\n\r\n    result = tf.on_exception(queue.dequeue(), lambda: 42)\r\n\r\nMaybe also the first argument would need to be a `lambda` such that it can properly set the context.\r\nTo make this work, like in `tf.cond`, the result from both arguments would need to be of the same type.\r\n", "comments": ["This would be a very radical change to TensorFlow. What is the use case?", "I was thinking about having some special logic inside my graph which does something different depending on whether there was such an exception or not.\r\nAlso, as I have multiple queues in my graph, I could do some more fine-grained handling.\r\nBut I see that this requires quite radical changes, and I can also solve my tasks in other ways, so maybe just let's close this.\r\n", "Sounds good. Glad to hear you're able to solve your problem in other ways.", "Can this be reopened? I have a use case. In my case, I am trying to use cholesky decomposition to solve a system, however sometimes it is not pos semi definite in which case it throws an error. I would like in this case to use SVD.\r\nI need to do this check on each sub batch, i.e. using tf.unstack and doing the solve on each sub matrix.\r\nIt would be nice to be able to do \r\n```\r\nresult = tf.stack([tf.on_exception(_with_cholesky, _without)(...) for ... in tf.unstack(...)],axis=0)\r\n```", "@Joshuaalbert Would it be possible to solve this problem with `tf.cond`?", "This is possible if there are a few conds and if the number of conds is static. But what I need to do is unstack along batch and perform the solve for each batch with a possible exception for each.", "Yes but even if an exception catching API was provided, it would most likely be designed in a way where you would need to say which specific error type and/or message regex you would want to catch, rather than catch-all. If you know what the error could be in advance, then it would most likely be possible to perform an expression on the input to see if it would result in an error, and if so, branch.\r\n\r\nIn your case, since you're working with batches, is it possible to maybe break the problem up into multiple sess.run calls?", "Yes in my case (and in all cases that I can imagine using this API) I know what the exception would be.\r\nIndeed, one could sequentially do multiple sess.run calls with a try/catch around the sess.run pointing to different implementations, but this would, firstly, unparallelize the procedure completely, and also obfuscate the program structure. Ideally, it would maintain the python principles of readability so that collaborators can easily understand the code.", "There is another use case. The team who is creating GPflow then TF version of GPy (Gaussian processes for python) has an issue when cholesky fails/doesn't give numpy equivalent (https://github.com/GPflow/GPflow/issues/78).", "While I'm still not sure if in-graph exception handling is something we can do, your recent comments suggest you might be interested in [Eager execution mode](https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html). It's also possible @josh11b, @asimshankar, or @clattner might want to weigh in on this issue.", "@jart Eager execution mode sounds like https://github.com/yaroslavvb/imperative but more full featured. I was actually hoping something like this would come along. Indeed this gives access to pythonic control flow. But is there not a pretty large efficiency penalty as there can be no cse optimization?", "I'm not sure how graph optimizations work in Eager. It's possible something like `@tf.defun` would be able to apply those sorts of optimizations to your critical sections of code. [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) would be a better place to ask (I'm just a triager.)\r\n\r\nTensorFlow graphs are turing complete ([example](https://github.com/akimach/EsotericTensorFlow/blob/master/brain_fuck.py)) and have [data flow ops](https://www.tensorflow.org/api_docs/cc/group/data-flow-ops) like queues. So it should be *possible* to do what you want to do, today, using a high performance pure graph without exceptions. While obtaining this kind of performance generally requires making tradeoffs on elegance, there's actually a lot of people on the team working on much more radical solutions that will let us eat our cake and have it too. They will take time.", "@Joshuaalbert : At this point, depending on the precise computation there may be efficiency penalties with eager execution (it really depends on the model. For example, see the [eager example benchmarks](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/resnet50), some of which are are competitive with graph execution, some of which are not), and we're working towards addressing those.\r\n\r\nAs @jart pointed out, we're unlikely to have the bandwidth to design or pursue in-graph exception handling in the near future.", "Here's another use-case. I'd like to loop, reading from a dataset until that dataset is exhausted. I need to do this in the sub-graph initializing a variable in my model. AFAIK, dataset iterator signals the end by throwing an exception. \r\nPerhaps, this can be more easily solved, if the iterator API would allow to test end-of-range?\r\n"]}, {"number": 10331, "title": "[OpenCL] Removes half concat op registration", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please.", "Test failure unrelated. Merging."]}, {"number": 10330, "title": "[OpenCL] Cleans cast operation", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10329, "title": "Problem exporting the weight matrix for each layer along with all the hyper parameters of each layer and their connectivity.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, I have modified Tensorflow before but not w.r.t. this problem. (I used the binaries provided by conda and pip)\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nmacOS 10.12 Sierra for iMac\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\n1.1.+\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\npip and pip version\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nn/a\r\n\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\n\r\nn/a\r\n\r\n### Describe the problem\r\n\r\nHow do you extract the weights of each of the intermediate layers in tensorflow?  I use the /tmp/cifar-train but I couldn't find the exact location of the weight matrix along with the hyper parameters?  \r\n\r\n![screen shot 2017-05-31 at 10 18 13 am](https://cloud.githubusercontent.com/assets/9545735/26639453/7ca8beb2-45ea-11e7-8ae6-d1fba7c190d5.png)\r\n\r\nI see the following files above but there is no documentation that tells how to export the weight matrix along with the hyper parameters for the entire network and each layer for the exact configuration.\r\n\r\n### Source code / logs\r\n\r\nn/a", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(That said, in this case the files you're looking at are produced by [`tf.train.Saver`](https://www.tensorflow.org/api_docs/python/tf/train/Saver) - whose documentation will describe the process of exporting and importing tensor contents a bit).\r\n\r\nHope that helps.\r\n"]}, {"number": 10328, "title": "[OpenCL] Extends tile ops to int16 and int32", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10327, "title": "[OpenCL] Cleans DynamicStitch Op", "body": " Re-order code to follow general rule:\r\n\r\n  - CPU Operation registration\r\n  - GPU Operation registration\r\n  - SYCL Operation registration", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Tensorboard failures unrelated.\r\nMerging."]}, {"number": 10326, "title": "[OpenCL] Removes ReductionFunctor for SYCLDevice", "body": "  We are using Eigen implementation", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Linux GPU seems to be failing for CUDA related code.", "Jenkins, test this please"]}, {"number": 10325, "title": "How to add cwise_op_floor_mod to build libtensorflow-core.a for ANDROID", "body": "OS: Ubuntu 16.04 64bits\r\nAndroid Version: 7.1 (Nougat)\r\nNDK Version: android-ndk-r12b\r\nHEXAGON SDK: 3.1\r\n\r\nWhen i am trying to build benchmark app, using make build, floor mod operation is not getting added by default, \r\n\r\n### By default:- \r\nwhen I issue below command, cwise_op_floor_mod.cc is not getting compiled against most of the files getting compiled in core/kernel/ folder\r\n`make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID NDK_ROOT=~/android-ndk-r12b`\r\n\r\n### Changes tried to add floor mod but failed:-\r\ntensorflow/core/kernels/cwise_op_floor_mode.cc - changes in bold \r\n\r\n--- snip ---\r\n\r\n```\r\n#include \"tensorflow/core/kernels/cwise_ops_common.h\"\r\n \r\n namespace tensorflow {\r\n REGISTER2(BinaryOp, CPU, \"FloorMod\", functor::safe_floor_mod, int32, int64);\r\n REGISTER2(BinaryOp, CPU, \"FloorMod\", functor::floor_fmod, float, double);\r\n \r\n **REGISTER_KERNEL_BUILDER(Name(\"FloorMod\")\r\n                             .Device(DEVICE_CPU)\r\n                             .HostMemory(\"x\")\r\n                             .HostMemory(\"y\")\r\n                             .HostMemory(\"z\")\r\n                             .TypeConstraint<int32>(\"T\"),\r\n                         BinaryOp<CPUDevice, functor::safe_floor_mod<int32>>);**\r\n #if GOOGLE_CUDA\r\n // A special GPU kernel for int32.\r\n // TODO(b/25387198): Also enable int32 in device memory. This kernel\r\n // registration requires all int32 inputs and outputs to be in host memory.\r\n REGISTER_KERNEL_BUILDER(Name(\"FloorMod\")\r\n                             .Device(DEVICE_GPU)\r\n                             .HostMemory(\"x\")\r\n                             .HostMemory(\"y\")\r\n                             .HostMemory(\"z\")\r\n                             .TypeConstraint<int32>(\"T\"),\r\n                         BinaryOp<CPUDevice, functor::safe_floor_mod<int32>>);\r\n #endif\r\n\r\n```\r\n-- eo snip --\r\n\r\nsomebody help out adding floor mod in the build.\r\nthanks\r\n", "comments": ["Not all operations are included for Android by default in order to reduce binary size.\r\n\r\nTo include `cwise_op_floor_mode.cc` in your build, you'll need to add that to [`tf_op_files.txt`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt) (more details in the [README](https://github.com/tensorflow/tensorflow/blob/4be052a5fc9e6569fcf07da3e9615b463e008266/tensorflow/contrib/makefile/README.md#fixing-makefile-issues))\r\n\r\nOr if you use bazel, then to the [`//tensorflow/core/kernels: android_extended_ops_group1`](https://github.com/tensorflow/tensorflow/blob/fe589d9/tensorflow/core/kernels/BUILD#L4126) target.\r\n\r\nLet us know if that works.\r\n\r\nFYI @andrewharp ", "it does works, thanks\r\n"]}, {"number": 10324, "title": "[OpenCL] Registers stack op", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10323, "title": "[OpenCL] Extends softmax op to cover double", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10322, "title": "[OpenCL] Fixes Split op", "body": "Split should alway go through SYCL device", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10321, "title": "[OpenCL] Transpose to go through Eigen", "body": "Transpose for SYCL should use Eigen implementation", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10320, "title": "[OpenCL] Cleans cwise ops", "body": "This PR cleans OpenCL related code for cwise operations.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10319, "title": "[XLA] Add a 'custom' fusion op type", "body": "This allows backends that may have fusions that are not included in the main enumeration.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@eliben are you reviewing this?", "Jenkins, test this please.", "cool - thanks\r\n", "@martinwicke Hi there.  Could this be merged now?  Cheers\r\n"]}, {"number": 10318, "title": "Remove \"bazel clean\"", "body": "Reverting #8880 @gunan (see #10236)\r\nunnecessary since bazelbuild/bazel#2759 was merged", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "During upgrade to cudnn 6, we discovered another issue.\r\nhttps://github.com/bazelbuild/bazel/issues/3062\r\nDue to this, we may need to hold on to bazel clean a little while longer.\r\nAnd even expand it by running `bazel clean --expunge`", "Bazel clean --expunge at each reconfigure is really extreme I would\nrecommend against it. Especially since there is an easy\u200b workaround if you\nreally want it to force a refetch of cuda configure: add an environment\nvariable to the list of variable to invalidate on, say\ncuda_configure_version, that you increase when you want a refetch of that\nrepository. Cuda configure is relatively cheap so even if you make it that\nall the time, it shouldn't be a performance issue unless the output of cuda\nconfigure change.\n\nOn Fri, Jun 2, 2017, 9:27 PM gunan <notifications@github.com> wrote:\n\n> During upgrade to cudnn 6, we discovered another issue.\n> bazelbuild/bazel#3062 <https://github.com/bazelbuild/bazel/issues/3062>\n> Due to this, we may need to hold on to bazel clean a little while longer.\n> And even expand it by running bazel clean --expunge\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10318#issuecomment-305887465>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf05C5dcL3ACN89astDuMjG-jP-B-ks5sAGIqgaJpZM4NrmfX>\n> .\n>\n", "@damienmg I understand the concern around the cost of clean --expunge.\r\nhowever, in #10307 we are trying to use the existing environment variables to invalidate the cuda repo and get it to be rebuilt, with no results.\r\nWe tried to edit configure script to Provide defaults for `TF_CUDNN_VERSION`, where it was \"\" before, now we provided \"6\", but local_config_cuda still did not get rebuilt.\r\n\r\nCould you chime in on that other pull request to direct us towards a working solution?"]}, {"number": 10317, "title": "mnist_with_summaries does not produce node statistics", "body": "### System information\r\n- Windows 7 Enterprise 64\r\n- TF 1.1.0 installed from pip\r\n- CUDA 8.0/cuDNN 5.1.5\r\n- GeForce GTX TITAN X\r\n- command: python mnist_with_summaries.py\r\n\r\n### Describe the problem\r\nIn https://www.tensorflow.org/get_started/graph_viz, mnist_with_summaries is given as an illustration of how to use tensorboard to show node statistics. However, it does not produce node statistics on my computer. Everything else is working correctly (the graph is there, summaries as well, the training goes smoothly).\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n", "comments": ["@chihuahua Mind taking a look at this? Could be some type of regression in tf_graph.", "I am trying to repro the issue. This is what I see in the graphs tab. I clicked the Memory radio button on the left side and then selected a node. What do you see?\r\n\r\n<img width=\"778\" alt=\"screen shot 2017-06-04 at 3 23 26 am\" src=\"https://cloud.githubusercontent.com/assets/4221553/26760745/701b65a8-48d5-11e7-935f-477fb63c0b35.png\">\r\n", "One small nit: Maybe you've tried this, but users have to select a Session run (to see node-specific stats for that run).\r\n<img width=\"206\" alt=\"screen shot 2017-06-04 at 3 26 34 am\" src=\"https://cloud.githubusercontent.com/assets/4221553/26760761/d5b447ea-48d5-11e7-9b5c-70c181306f06.png\">\r\n\r\n\r\n", "Thanks, problem solved."]}]