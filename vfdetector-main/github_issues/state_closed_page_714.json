[{"number": 32160, "title": "[tf.data] Increase default buffer size to 256 MB", "body": "A larger buffer size brings better throughput at the cost of buffer bloat.", "comments": ["Ping @frankchn @mrry to review", "@jsimsa Any rationale behind the introduction and removal of this particular TODO? We'd like to fix it the first time as we observed small buffer size might not be optimal for certain storage options, e.g. HDFS.", "The reason why the TODO is removed is that there is no one right buffer size that makes sense for all possible storage systems and changing this value to be 1000x bigger would affect memory usage of existing uses that would not necessarily benefit from it.\r\n\r\nThis TODO was introduced when integration between TensorFlow and GCS was being worked on but for the same reason as stated above, it was not a good idea and instead, the storage system specific buffering was performance in the storage system client."]}, {"number": 32159, "title": "TF 2.0 regression: cloudpickle cannot serialize tf.keras.Sequential.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (code included below in the issue)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0\r\n- Python version: Python 3.6.7 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nUsing cloudpickle to serialize a Python function that uses `tf.keras.Sequential` fails with a recursion error.\r\n\r\n**Note** that this works with `tensorflow==1.14.0`.\r\n\r\nI imagine it also fails with other things, not just `tf.keras.Sequential`.\r\n\r\n```python\r\nimport cloudpickle  # cloudpickle.__version__ == '1.2.1'\r\nimport tensorflow as tf  # tf.__version__ == '2.0.0-rc0'\r\n\r\ndef f():\r\n    tf.keras.Sequential\r\n\r\ncloudpickle.loads(cloudpickle.dumps(f))  # This fails.\r\n```\r\n\r\nThe last line fails with\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-23-25cc307e6227> in <module>\r\n----> 1 cloudpickle.loads(cloudpickle.dumps(f))\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in __getattr__(self, item)\r\n     48 \r\n     49   def __getattr__(self, item):\r\n---> 50     module = self._load()\r\n     51     return getattr(module, item)\r\n     52 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in _load(self)\r\n     42   def _load(self):\r\n     43     \"\"\"Import the target module and insert it into the parent's namespace.\"\"\"\r\n---> 44     module = _importlib.import_module(self.__name__)\r\n     45     self._parent_module_globals[self._local_name] = module\r\n     46     self.__dict__.update(module.__dict__)\r\n\r\n... last 2 frames repeated, from the frame below ...\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in __getattr__(self, item)\r\n     48 \r\n     49   def __getattr__(self, item):\r\n---> 50     module = self._load()\r\n     51     return getattr(module, item)\r\n     52 \r\n\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n\r\nSee https://stackoverflow.com/questions/57750920/ray-tensorflow-gpu-2-0-recursionerror/57761034#57761034", "comments": ["I have tried on colab with TF 1.14 and able to execute the code.However i am able to reproduce the issue with TF 2.0.0-rc0 and 2.0 nightly versions.Please, find the [gist ](https://colab.sandbox.google.com/gist/ravikyram/af3b48253cc090e9c2048db71024d890/untitled143.ipynb)here.Thanks!", "What is the goal of serialization here? We have several saving methods that allow you to save and revive a keras model that might be more appropriate here.", "@karmel Note that this issue is not actually about serializing a keras model, but rather about serializing a function that creates a keras model. The APIs you refer to don't help in this case, I think.\r\n\r\nCloudpickle is the standard when it comes to general purpose serialization of arbitrary Python objects (including functions and classes). This is used by many distributed computing frameworks (like Ray, PySpark, Dask, IPython Parallel) that serialize arbitrary user-defined functions and ship them to remote worker processes to be executed.\r\n\r\nAs long as TensorFlow plays nicely with cloudpickle, then cloudpickle will be able to serialize arbitrary functions/classes that use TensorFlow. Serializing arbitrary functions/classes is most likely out of scope for TensorFlow, and so it makes sense to have cloudpickle handle that.", "@yifeif Can you take a look at the cloud pickle issue? It looks like it's getting caught up in an infinite recursion loop in LazyLoader", "Looks like we might need to handle __getstate__, __setstate__ for the LazyLoader at virtual pip level? cc @mihaimaruseac", "Seems that that is the case, `__getstate__`, `__setstate__`, `__getinitargs__` and `__getnewargs__`.\r\n\r\nI will send a fix later today/tomorrow.", "Update: the issue comes from the unpickling part, as shown from the script below:\r\n\r\n```python\r\n_p = print\r\nimport cloudpickle  # cloudpickle.__version__ == '1.2.1'\r\nimport tensorflow as tf  # tf.__version__ == '2.0.0-rc0'\r\n\r\ndef f():\r\n  _p(\"f() called\")\r\n  tf.keras.Sequential\r\n  _p(\"f() ending\")\r\n\r\n_p(\"Dumping...\")\r\ns = cloudpickle.dumps(f)\r\n_p(\"dumped, loading...\")\r\ncloudpickle.loads(s)\r\n_p(\"done\")\r\n```\r\n\r\nThis outputs:\r\n\r\n```console\r\nDumping...\r\ndumped, loading...\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    cloudpickle.loads(s)\r\n  File \"/tmp/gh/1/lib/python3.6/site-packages/tensorflow/__init__.py\", line 51, in __getattr__\r\n    _p(\"{}.__getattr__({})\".format(self._local_name, item))\r\n  File \"/tmp/gh/1/lib/python3.6/site-packages/tensorflow/__init__.py\", line 51, in __getattr__\r\n    _p(\"{}.__getattr__({})\".format(self._local_name, item))\r\n  File \"/tmp/gh/1/lib/python3.6/site-packages/tensorflow/__init__.py\", line 51, in __getattr__\r\n    _p(\"{}.__getattr__({})\".format(self._local_name, item))\r\n  [Previous line repeated 330 more times]\r\nRecursionError: maximum recursion depth exceeded while calling a Python object\r\n```\r\n\r\nFurther investigation reveals that during unpickling `__setattr__` needs to be called (equivalently, `__setstate__` could be called but it needs at least one `__setattr__` to store the new state on the module's `__dict__`). However, the lazy loading approach we're using assumes read only modules, we cannot add new attributes. Even defining an emtpy `__setattr__` results in infinite recursion at the `import tensorflow` line.", "Another update:\r\n\r\n`tf.keras.Sequential`, `tf.keras` and `tf.estimator` all result in the infinite recursion errors.\r\n\r\n`tf.math.sin` doesn't.", "We cannot fix this in time for TF 2.0 final release. In fact, we cannot really fix this unless we give up Python 2 support, so we're looking at a fix that should come up by start of next year or so.\r\n\r\nSorry for the delay, but as we didn't support serialization via pickling we never tested if this functionality would get broken by our changes. We'll fix this in the future", "Thanks for the update @mihaimaruseac. Fixing this in the future would be great. Out of curiosity, why does fixing it mean giving up support for Python 2?", "We are using a custom lazy loader object to mimic functionality that is present only in Python3.5 and later to create some modules on the fly.", "@mihaimaruseac I saw https://github.com/tensorflow/tensorflow/commit/4675891bd3c9e9ee7a57552486ec5bdc40379787 . Is it relevant to this issue?", "I'll have to check this, as it is on a different path.", "Is there any type of workaround for this? Running Ray and TF 2.0 and now facing this issue. Would be great to see a fix for this any time soon, rather than next year.", "If you are just blocked by some framework that ships the serialized function you could bypass the tensorflow serialization by using `importlib.import_module` and then during de-serialization make sure the module you use is shipped/available in the PYTHONPATH.\r\n\r\nSomething like:\r\n```\r\nmymodule.py\r\ndef tf_fn():\r\n   tf.keras.Sequential\r\n\r\ndef f():\r\n    module = importlib.import_module(\"mymodule\")\r\n    return module.tf_fn()\r\n```\r\n\r\nIn our use case to run distributed TensorFlow on Hadoop we provide a [safe_experiment function ](https://github.com/criteo/tf-yarn/blob/master/tf_yarn/__init__.py#L566) function and then we upload the TensorFlow functions inside a module to the cluster. This works as a workaround for the moment with tf2.\r\n\r\n", "@jharaldson the easiest workaround might be the one described in https://github.com/ray-project/ray/issues/5614#issuecomment-527292289.\r\n\r\nAnother workaround is described in https://stackoverflow.com/a/57761034/7858504", "Coming back to the example at https://github.com/tensorflow/tensorflow/issues/32159#issuecomment-528016376\r\n\r\nIn python2 all works\r\n\r\n```console\r\n(py2) mihaimaruseac@ankh:/tmp/pickle/py2$ python test.py\r\nDumping...\r\ndumped, loading...\r\ndone\r\n```\r\n\r\nIn python3.5 the error is from `PyCapsule` objects:\r\n\r\n```console\r\n(py35) mihaimaruseac@ankh:/tmp/pickle/py35$ python test.py\r\nDumping...\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 11, in <module>\r\n    s = cloudpickle.dumps(f)\r\n  File \"/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py\", line 1125, in dumps\r\n    cp.dump(obj)\r\n  File \"/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py\", line 482, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\r\n    self.save(obj)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py\", line 556, in save_function\r\n    return self.save_function_tuple(obj)\r\n  File \"/tmp/pickle/py35/lib/python3.5/site-packages/cloudpickle/cloudpickle.py\", line 758, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.5/pickle.py\", line 814, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.5/pickle.py\", line 840, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.5/pickle.py\", line 774, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 801, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 627, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.5/pickle.py\", line 814, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.5/pickle.py\", line 840, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.5/pickle.py\", line 814, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.5/pickle.py\", line 840, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle PyCapsule objects\r\n```\r\n\r\nIn Python3.7 the error is from `_LazyLoader`\r\n\r\n```console\r\n(py37) mihaimaruseac@ankh:/tmp/pickle/py37$ python test.py\r\nDumping...\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 11, in <module>\r\n    s = cloudpickle.dumps(f)\r\n  File \"/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py\", line 1125, in dumps\r\n    cp.dump(obj)\r\n  File \"/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py\", line 482, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py\", line 556, in save_function\r\n    return self.save_function_tuple(obj)\r\n  File \"/tmp/pickle/py37/lib/python3.7/site-packages/cloudpickle/cloudpickle.py\", line 758, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 856, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 882, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 816, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.7/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _LazyLoader objects\r\n```\r\n\r\nCan't test python3.6 anymore due to an issue in my system.\r\n\r\nManually applying the fix from 4675891bd3c9e9ee7a57552486ec5bdc40379787 to the other codepath makes that test pass in all 3 instances.\r\n\r\nI'm running the change through more tests and will submit a fix.", "Let's wait for a new tf-nightly and test this again. Or, you can build from source, with 353b8a1", "Seems the fix works with Ray. However if we use custom layers with functions decorated with @tf.function there are still pickling issues. As a workaround for that I figured one could save the model as a \"savedmodel\" on a distributed storage and then have the ray worker load the model from the distributed storage, but this throws an error. \r\n\r\nNote: Removing the LSTM layer does not result in an error, which would suggest that this error is related to the while operation (as the error suggests).\r\n\r\n```\r\nLookupError: No gradient defined for operation 'while' (op type: While)\r\n```\r\n\r\nCode to reproduce\r\n```\r\nimport tensorflow as tf\r\nimport ray \r\nimport numpy as np\r\n\r\nray.init()\r\n\r\ndef build_save_model():\r\n    lstm_in = tf.keras.Input(shape=(24,1))\r\n    lstm_out = tf.keras.layers.LSTM(6)(lstm_in)\r\n    dense_out = tf.keras.layers.Dense(24)(lstm_out)\r\n    model = tf.keras.Model([lstm_in], dense_out)\r\n    model.save('/path/in/common/storage/lstm_model')\r\n\r\n@ray.remote\r\nclass Worker():\r\n    def __init__(self):\r\n        self.model = tf.keras.models.load_model('/path/in/common/storage/lstm_model')\r\n        self.model.compile(optimizer=tf.keras.optimizers.Adam(1e-1), loss=tf.keras.losses.mse)\r\n        self.data = np.arange(24).reshape(1,24,1)\r\n        self.label = np.arange(24).reshape(1,24)\r\n        \r\n    def train(self):\r\n        history = self.model.fit(self.data, self.label, epochs=10)\r\n        return history.history\r\n        \r\nbuild_save_model()\r\nlstm_worker = Worker.remote()\r\nw = ray.get(lstm_worker.train.remote())\r\n```\r\n\r\nError\r\n```\r\n---------------------------------------------------------------------------\r\nRayTaskError                              Traceback (most recent call last)\r\n<ipython-input-3-a18941ca631a> in <module>\r\n     22 build_save_model()\r\n     23 lstm_worker = Worker.remote()\r\n---> 24 w = ray.get(lstm_worker.train.remote())\r\n\r\n/opt/conda/lib/python3.6/site-packages/ray/worker.py in get(object_ids)\r\n   2245             if isinstance(value, RayError):\r\n   2246                 last_task_error_raise_time = time.time()\r\n-> 2247                 raise value\r\n   2248 \r\n   2249         # Run post processors.\r\n\r\nRayTaskError: ray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2326, in get_attr\r\n    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 331, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2330, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2326, in get_attr\r\n    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 331, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2330, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 607, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2495, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/registry.py\", line 97, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: While\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray_worker (pid=1397, host=thesis-clustering-7dfb7867df-pk5fc)\r\n  File \"<ipython-input-3-a18941ca631a>\", line 19, in train\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 785, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 337, in fit\r\n    total_epochs=epochs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 127, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2366, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2675, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2565, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 974, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(x, y, sample_weights))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2164, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 269, in _process_single_batch\r\n    grads = tape.gradient(scaled_total_loss, trainable_weights)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1029, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 766, in _backward_function\r\n    return self._rewrite_forward_and_call_backward(call_op, *args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 685, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 594, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 642, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 974, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 632, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 669, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 336, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 669, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 685, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 594, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 642, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 974, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 632, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 669, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 336, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 669, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 685, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 594, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 642, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 974, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 632, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 623, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'while' (op type: While)\r\n```\r\n\r\n", "Can you run a model with `while` but without pickling/unpickling? Afaik, `while` doesn't have gradients (maybe with gradient tape but then I don't know if those get pickled anyway)", "Two examples that runs without errors:\r\n\r\n1. the above example commenting out the @ray.remote decorator and call the train function without the remote call.\r\n\r\n2. the above example adding a return statement to the build_save_model() to return the built model. We swap out tf.keras.models.load_model() in the Worker to self.model = build_save_model() and call train()\r\n\r\nOne example that runs with error:\r\n1. we build and save the model in the Worker (as part of remote call) and tries to load the saved model in the main python session (not remote)\r\n\r\n", "Any updates on this?", "Are there any updates regarding this issue? Has there been a fix (such as https://github.com/tensorflow/tensorflow/commit/353b8a1adcb471a48ef9b1c5cbfc6097d036473e) applied to tf 1.15?", "No, but if you want to make a cherry-pick we can merge it if and when we do a new patch release on 1.15", "@mihaimaruseac I opened https://github.com/tensorflow/tensorflow/pull/39034 for this.", "I think this can be closed now as it has been solved and backported to 1.15 too.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32159\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32159\">No</a>\n"]}, {"number": 32158, "title": "tf.gather should support batch_dims == rank(indices)", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes, if it's just overzealous checks.\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, `tf.gather` requires that `batch_dim` is less than `rank(indices)`, but it seems like `batch_dim == rank(indices)` should logically be supported as well. This would be equivalent to a scalar lookup in the last axis of `params`.\r\n\r\nSupporting this should come pretty natural and I think that, for the CPU code at least, it is already supported; the checks just need to be changed from `<` to `<=`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.", "comments": ["+1. I just hit this issue as well. Note that it can be worked around by artificially adding an axis before the gather then removing it after the gather.", "@mpdn @MattShannon This issue has been fixed in https://github.com/tensorflow/tensorflow/commit/93ef77d971716636f88c39bff6a82cfad6a24000#diff-0a1b05e2252bef657373e11f17bfa5d3\r\n\r\nI will close this issue for now, but feel free to reopen if the issue persists."]}, {"number": 32157, "title": "[TF 2.0] non-tensor argument to target of tf.function logging slows down training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Modified the stock DCGAN TensorFlow example**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google Colab and Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.0.0-rc0**\r\n- Python version: **3.7**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **Colab: unknown, Windows 10: CUDA=10.0, cuDNN=7.4.2**\r\n- GPU model and memory: **Colab: unknown, Windows 10: 1080 Ti 11 GB**\r\n\r\n**Describe the current behavior**\r\nLogging summary scalars (for review in TensorBoard) leads to significantly increased training times. \r\n\r\n**Code to reproduce the issue**\r\nUsing the DCGAN tutorial (https://www.tensorflow.org/beta/tutorials/generative/dcgan), and modifying it to include a summary_writer :\r\n\r\n```\r\n# Load the TensorBoard notebook extension\r\n%load_ext tensorboard\r\nlogs_base_dir = \"./logs\"\r\nos.makedirs(logs_base_dir, exist_ok=True)\r\n\r\nsummary_writer = tf.summary.create_file_writer(logs_base_dir)\r\n\r\n%tensorboard --logdir {logs_base_dir}\r\n```\r\n\r\nand adding scalar logging to the training step function:\r\n\r\n```\r\n@tf.function\r\ndef train_step(images, step):\r\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\r\n\r\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n        generated_images = generator(noise, training=True)\r\n\r\n        real_output = discriminator(images, training=True)\r\n        fake_output = discriminator(generated_images, training=True)\r\n\r\n        gen_loss = generator_loss(fake_output)\r\n        disc_real_loss, disc_fake_loss = discriminator_loss(real_output, fake_output)\r\n        disc_loss = disc_real_loss + disc_fake_loss\r\n    \r\n    # new code here only:\r\n    tf.summary.scalar('gen_loss', gen_loss, step=step)\r\n    tf.summary.scalar('disc_loss', disc_loss, step=step) \r\n    tf.summary.scalar('total_loss', gen_loss+disc_loss, step=step)\r\n    summary_writer.flush()\r\n\r\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\r\n\r\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\r\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\r\n```\r\n\r\nThe average time per epoch [running on Google Colab, and on a Windows 10 PC with 1080 Ti] is a lot longer when logging:\r\n\r\nOriginal configuration:  **24.6 secs** (Colab),  **8.5 secs** (Windows)\r\nWith logging summary scalars:  **220 secs** (Colab),  **44.7 secs** (Windows)\r\n\r\n**Describe the expected behavior**\r\nI would have expected logging to place a minimal overhead cost on training performance. Perhaps related to the `tf.function` compilation?\r\n", "comments": ["@4OH4 ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks", "@4OH4 - thanks for raising the issue.\r\n\r\nI'm having trouble reproducing the slowness when trying the DCGAN tutorial on Colab with TF 2.0.0-rc0, as you described.  I've made the following insertion to the body of `train_step()`:\r\n\r\n```python\r\n    # new code here only:\r\n    with summary_writer.as_default():\r\n      tf.summary.scalar('gen_loss', gen_loss, step=0)\r\n      tf.summary.scalar('disc_loss', disc_loss, step=0) \r\n      tf.summary.scalar('total_loss', gen_loss+disc_loss, step=0)\r\n    summary_writer.flush()\r\n```\r\n\r\nNote that this differs from your modification in that:\r\n* I'm using `summary_writer.as_default()` to activate summary writing; without this the `tf.summary.scalar` lines should be no-ops\r\n* I'm passing `step=0` to avoid modification to the signature of `train_step()` as in your example, which I assume involved other modifications to the callsite\r\n\r\nWith this change, running `train(train_dataset, 3)` (aka 3 epochs) takes the same amount of time as without the change for me, about 11 seconds per epoch.\r\n\r\nIf you're still seeing this issue, can you provide the precise code that you ran (ideally as a full `.ipynb`), including any changes to set the summary writer as the default and to propagate the step value, and what exactly you ran to check the average time per epoch?", "I think that, actually, i was mistaken and it is not `tf.summary` that is causing the slow down, but that `step` is not a tensor.\r\n\r\nI created a variable `step=0` in `train()`, and then incremented it each image batch using `step+=1`. This caused the training slow-down, from 11 secs/epoch to ~168 secs/epoch, as printed by `train()`.\r\n\r\nChanging this to `step=tf.Variable(0)` and `step.assign_add(1)` fixes the problem.\r\n\r\nSimilarly, removing the line `step+=1` in `train()` also fixes the problem - it seems to occur only if `step` is non-zero?\r\n\r\nHere is a gist that demonstrates the effect:\r\nhttps://gist.github.com/4OH4/036062cd76ecd0a9dc83b787ee75e2dd#file-dcgan-ipynb\r\n\r\nPerhaps its obvious to many that all the arguments going into the `tf.function` target should be tensors - I overlooked that at the time. It is confusing though that the code still runs, just much more slowly. ", "I'm sorry for the confusion, here is the relevant doc https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args\r\n\r\nMany users are hitting this and we definitely want to address this.  I will be adding a \"too-many-retrace\" warning very soon, likely this week.  And we are discussing what we should we do more on the top of that.\r\n\r\nClosing this issue as it's not a bug (although it's confusing).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32157\">No</a>\n"]}, {"number": 32156, "title": "Distribute strategy extended broadcast_to broadcasts different tensors when using tf.random", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):1.14\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nstrategy.extended.broadcast_to does not broadcast the same value when using tf.random.uniform.According to the description of the method, it is supposed to mirror a tensor on one device to all worker devices\r\n\r\n**Describe the expected behavior**\r\nShould broadcast the same value to all devices?\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32156\">No</a>\n"]}, {"number": 32155, "title": "Use where_v2 to silence deprecation warning", "body": "Running this function currently prints the following deprecation warning:\r\n\r\n> W0902 14:30:32.759225 140344378930944 deprecation.py:323] From /lib/python3.6/site-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n\r\nThis PR fixes that.", "comments": []}, {"number": 32154, "title": "Missing apostrophe in documentation", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32154) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32154) for more info**.\n\n<!-- ok -->", "Please make the PR against master and then cherry-pick. Thank you."]}, {"number": 32153, "title": "How to run the XLA GPU UTs? it seems filtered.", "body": "bazel test --config=opt --config=cuda --test_verbose_timeout_warnings --verbose_failures --color=yes //tensorflow/compiler/xla/tests:multioutput_fusion_test\r\n\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: All specified test targets were excluded by filters\r\nINFO: Analyzed target //tensorflow/compiler/xla/tests:multioutput_fusion_test_gpu (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target and 0 test targets...\r\nTarget //tensorflow/compiler/xla/tests:multioutput_fusion_test_gpu up-to-date:\r\n  bazel-bin/tensorflow/compiler/xla/tests/multioutput_fusion_test_gpu\r\nINFO: Elapsed time: 6.680s, Critical Path: 6.21s\r\nINFO: 2 processes: 2 local.\r\nINFO: Build completed successfully, 7 total actions\r\nINFO: Build completed successfully, 7 total actions", "comments": ["Actually, the binary(_bazel-bin/tensorflow/compiler/xla/tests/multioutput_fusion_test_gpu_) could been run manually, but it could not run automatically.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32153\">No</a>\n"]}, {"number": 32152, "title": "Failed to build tensorflow pip package (Python 2.7.15+) inside of the docker image tensorflow/tensorflow:devel", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): docker image tensorflow/tensorflow:devel\r\n- TensorFlow version: 1.14\r\n- Python version: 2.7.15+\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nUnable to build tensorflow pip package inside of the provided docker image **tensorflow/tensorflow:devel** for the tensorflow development using Python 2.7+.\r\n\r\nThe errors are:\r\n```\r\nTraceback (most recent call last):\r\nFile \"/usr/lib/python2.7/site.py\", line 554, in <module>\r\nmain()\r\nFile \"/usr/lib/python2.7/site.py\", line 536, in main\r\nknown_paths = addusersitepackages(known_paths)\r\nFile \"/usr/lib/python2.7/site.py\", line 272, in addusersitepackages\r\nuser_site = getusersitepackages()\r\nFile \"/usr/lib/python2.7/site.py\", line 247, in getusersitepackages\r\nuser_base = getuserbase() # this will also set USER_BASE\r\nFile \"/usr/lib/python2.7/site.py\", line 237, in getuserbase\r\nUSER_BASE = get_config_var('userbase')\r\nFile \"/usr/lib/python2.7/sysconfig.py\", line 587, in get_config_var\r\nreturn get_config_vars().get(name)\r\nFile \"/usr/lib/python2.7/sysconfig.py\", line 538, in get_config_vars\r\n_CONFIG_VARS['userbase'] = _getuserbase()\r\nFile \"/usr/lib/python2.7/sysconfig.py\", line 215, in _getuserbase\r\nreturn env_base if env_base else joinuser(\"~\", \".local\")\r\nFile \"/usr/lib/python2.7/sysconfig.py\", line 201, in joinuser\r\nreturn os.path.expanduser(os.path.join(*args))\r\nFile \"/usr/lib/python2.7/posixpath.py\", line 262, in expanduser\r\nuserhome = pwd.getpwuid(os.getuid()).pw_dir\r\nKeyError: 'getpwuid(): uid not found: 2308'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ndocker run  -u $(id -u):$(id -g) -v $PWD:$PWD -i --env USER=$USER -w $PWD/repo --env HOME=$PWD  tensorflow/tensorflow:devel bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nWhen I run it without   `-u $(id -u):$(id -g)`, it works fine, but this is not the recommended way.\r\n\r\n**Any other info / logs**\r\nIf I run the image tensor/tensorflow:devel like this:\r\n\r\n```\r\ndocker run -it -u $(id -u):$(id -g) tensorflow/tensorflow:devel bash\r\n```\r\n\r\nand then I call this command inside of the docker image:\r\n```\r\nwhoami\r\n```\r\nThe result is \"cannot find name for user ID\"\r\n\r\nThe possible solution is put this line inside of the docker file: devel-cpu.Dockerfiles\r\nRUN chmod g+w /etc/passwd /etc/group\r\n\r\nThen this file /etc/passwd will be writable inside of the docker and it will be possible to \r\n- add passwd file entry for $(id -u)\r\n- add passwd file entry for $(id -g)\r\n\r\nThis is an issue only for some packages of Python 2.7, but not for Python 3.0\r\n", "comments": ["Thanks for reporting this. I've verified it and would welcome a PR to fix the issue.\r\n\r\nUnfortunately, since this only affects Python 2.7, it's not as high of a priority right now (AFAIK, TensorFlow will stop supporting Python 2.7 on January 1, when Python 2.7 is EOL), so I don't have the time to fix it myself this week.", "@angersson I opened a PR for this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32152\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32152\">No</a>\n"]}, {"number": 32151, "title": "[INTEL MKL] Add primitive cache for mkl softmax", "body": "Enable primitive reuse in MklSoftmax, please review.", "comments": ["@xiejw  Could you take a look at this PR and give some suggestions? Thank you!", "@penpornk Changes applied."]}, {"number": 32150, "title": "Add is_png, is_gif, is_bmp operators", "body": "Align with TensorFlow Core [code](https://github.com/tensorflow/tensorflow/blob/4213d5c1bd921f8d5b7b2dc4bbf1eea78d0b5258/tensorflow/core/kernels/decode_image_op.cc#L44)\r\n", "comments": ["I think this PR fits better with http://github.com/tensorflow/io"]}, {"number": 32149, "title": "Updated doc for all symbols in tf.keras.activations", "body": "Updated Descriptions, Examples, Arguments, Returns and Raises lists.", "comments": ["Would a reviewer be assigned for this? @gbaned ", "We don't update release branches after final release except in case of a security vulnerability. In that case, we only update the branches only as needed to fix the vulnerability and release the patch.\r\n\r\nAs such, since this PR is against one such branch (`r1.13`), I'm closing it. Please open the PR against master if needed there."]}, {"number": 32148, "title": "[Intel MKL] Cache bias tensor for INT8 inference", "body": "Bias tensor reorder overhead in int8 inference is significant, especially for low latency models (i.e. mobilenet). The reorder primitive is used to quantize fp32 bias tensor into int8. By caching the reordered bias tensor, int8 models' latency got significant improvement.", "comments": ["@penpornk Changes applied, hope I didn't miss any of your opinions.", "@penpornk Changes applied."]}, {"number": 32147, "title": "ModuleNotFoundError: No module named 'tensorflow' ", "body": "After \r\nconda install tensorflow \r\nand\r\nconda activate tensorflow_env\r\n\r\nwhile running code\r\nimport tensorflow as tf\r\n**Error**\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-7-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\nModuleNotFoundError: No module named 'tensorflow'", "comments": ["Hi, I think there might be mainly two reasons for this. The most probable reasons and their solutions are described below:\r\n\r\n1. If you are using Python 2.7, then Tensroflow might not work. Upgrade it to Python 3.5 and above. \r\n    For this open the conda prompt and type:\r\n       conda install python = 3.5\r\n    Then type the commands to get tensorflow \r\n2. You might be already having Python in your Program Data. To check if the paths are not the same:\r\n     i) Open command prompt and :-\r\n       >>>python\r\n       >>>import sys\r\n       >>>print(sys.executable)\r\n       Note down the path.\r\n      ii) Open conda prompt and type the same commands. If the paths are not the same then, \r\n      uninstall both (or all) the python packages you have. Install anaconda package and make sure to \r\n      check the \"add to path\" option.\r\n       ", "@georgevettathu \r\nWhich version of tensorflow you are using?\r\nCan you please go through this [link](https://www.tensorflow.org/install/pip) ?Thanks!", "Thanks. I have installed through the command\nconda install tensorflow\nIalso activitate invirinment also ok\nbut on call import tensorflow as tf\nerror message: as given below.\n\nModuleNotFoundError                       Traceback (most recent call\nlast)<ipython-input-1-64156d691fe5> in <module>----> 1 import\ntensorflow as tf\nModuleNotFoundError: No module named 'tensorflow'\n\n\nCan you please help me to install tensorflow properely\nmy system is windows 7 and pythen 3.7.2\nDr. George Mathew\n\n\n\nOn Tue, Sep 3, 2019 at 11:45 AM ravikyram <notifications@github.com> wrote:\n\n> @georgevettathu <https://github.com/georgevettathu>\n> Which version of tensorflow you are using?\n> Can you please go through this link\n> <https://www.tensorflow.org/install/pip> ?Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32147?email_source=notifications&email_token=ALSVGJTILAB3DPRDWNERRTTQHX6IVA5CNFSM4ISZNKY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5XDW3Y#issuecomment-527317871>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALSVGJUKTVZEWBJMK2VAYRTQHX6IVANCNFSM4ISZNKYQ>\n> .\n>\n", "again installed through the command\npip install tensorflow==2.0.0-rc0\nno onstallation errors but on call:\n\n# Import `tensorflow`\nimport tensorflow as tf\n\n# Initialize two constants\nx1 = tf.constant([1,2,3,4])\nx2 = tf.constant([5,6,7,8])\n\n# Multiply\nresult = tf.multiply(x1, x2)\n\n# Print the result\nprint(result)\n\nError as given below:\n\nERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\",\nline 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-23cedb5749e4>\", line 5, in <module>\n    x1 = tf.constant([1,2,3,4])\nAttributeError: module 'tensorflow' has no attribute 'constant'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\",\nline 2039, in showtraceback\n    stb = value._render_traceback_()\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\",\nline 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\",\nline 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\",\nline 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp,\npathname, description)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed with error code -1073741795\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\ultratb.py\",\nline 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\ultratb.py\",\nline 319, in wrapped\n    return f(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\ultratb.py\",\nline 353, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n  File \"C:\\ProgramData\\Anaconda37\\lib\\inspect.py\", line 1502, in getinnerframes\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\inspect.py\", line 1460, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\inspect.py\", line 696, in getsourcefile\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n  File \"C:\\ProgramData\\Anaconda37\\lib\\inspect.py\", line 733, in getmodule\n    if ismodule(module) and hasattr(module, '__file__'):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow\\__init__.py\",\nline 50, in __getattr__\n    module = self._load()\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow\\__init__.py\",\nline 44, in _load\n    module = _importlib.import_module(self.__name__)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\importlib\\__init__.py\", line\n127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\__init__.py\",\nline 42, in <module>\n    from tensorflow._api.v2 import audio\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\",\nline 10, in <module>\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\",\nline 9, in <module>\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow\\__init__.py\",\nline 50, in __getattr__\n    module = self._load()\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow\\__init__.py\",\nline 44, in _load\n    module = _importlib.import_module(self.__name__)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\importlib\\__init__.py\", line\n127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\",\nline 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\",\nline 74, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\",\nline 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-23cedb5749e4>\", line 5, in <module>\n    x1 = tf.constant([1,2,3,4])\nAttributeError: module 'tensorflow' has no attribute 'constant'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\",\nline 2039, in showtraceback\n    stb = value._render_traceback_()\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\",\nline 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\",\nline 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\ProgramData\\Anaconda37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\",\nline 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp,\npathname, description)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\ProgramData\\Anaconda37\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed with error code -1073741795\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.\n\ncan you please suggest any solution to this problem\n\n\nThanking you\n\n\nDr. George Mathew\n\n\n\nOn Tue, Sep 3, 2019 at 12:56 PM George Mathew <georgevettathu@gmail.com>\nwrote:\n\n> Thanks. I have installed through the command\n> conda install tensorflow\n> Ialso activitate invirinment also ok\n> but on call import tensorflow as tf\n> error message: as given below.\n>\n> ModuleNotFoundError                       Traceback (most recent call last)<ipython-input-1-64156d691fe5> in <module>----> 1 import tensorflow as tf\n> ModuleNotFoundError: No module named 'tensorflow'\n>\n>\n> Can you please help me to install tensorflow properely\n> my system is windows 7 and pythen 3.7.2\n> Dr. George Mathew\n>\n>\n>\n> On Tue, Sep 3, 2019 at 11:45 AM ravikyram <notifications@github.com>\n> wrote:\n>\n>> @georgevettathu <https://github.com/georgevettathu>\n>> Which version of tensorflow you are using?\n>> Can you please go through this link\n>> <https://www.tensorflow.org/install/pip> ?Thanks!\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/32147?email_source=notifications&email_token=ALSVGJTILAB3DPRDWNERRTTQHX6IVA5CNFSM4ISZNKY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5XDW3Y#issuecomment-527317871>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ALSVGJUKTVZEWBJMK2VAYRTQHX6IVANCNFSM4ISZNKYQ>\n>> .\n>>\n>\n", "@georgevettathu \r\nRequest you to uninstall python and tensorflow .Please, install it again and see if the problem still persists.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32147\">No</a>\n", "Dear ravikyram,\nThank you for your suggestion. Even after doing this no difference. Same\npath is showing in both methods.C:\\ProgramData\\Snsconda37\\python.exe\nDr. George Mathew\n\nOn Thu, Sep 12, 2019 at 5:46 PM ravikyram <notifications@github.com> wrote:\n\n> Closed #32147 <https://github.com/tensorflow/tensorflow/issues/32147>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32147?email_source=notifications&email_token=ALSVGJXBAJ45LLILI4BOLB3QJIXKHA5CNFSM4ISZNKY2YY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOTSZDNLY#event-2628925103>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALSVGJR7KBSFBABID5OSFZLQJIXKHANCNFSM4ISZNKYQ>\n> .\n>\n", "Apologies for the delay in response. What is your cpu make/model?\r\nI suspect that your cpu does not support AVX instructtion sets.\r\nSee [Hardware Requirements](https://www.tensorflow.org/install/pip#hardware-requirements)\r\nCan you please confirm?", "See also #19584", "Is this still an issue?", "Most likely its because of your cpu does not support AVX instruction sets. Feel free to reopen this issue if otherwise. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32147\">No</a>\n", "Dear Yasir,\nThank you so much for your suggestions. My CPU is:\n Intel (R) Core(TM)2 Duo CPU E7200 @ 2.53GHz\nWhether it support AVX instruction sets. What is the solution?\nCan you help me. I could install tensorflow but not working while calling.\nGeorge Mathew\n\nOn Thu, Oct 3, 2019 at 11:33 PM Yasir Modak <notifications@github.com>\nwrote:\n\n> Most likely its because of your cpu does not support AVX instruction sets.\n> Feel free to reopen this issue if otherwise. Thanks!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32147?email_source=notifications&email_token=ALSVGJUVQ327GAKYJLYAJDLQMYXYTA5CNFSM4ISZNKY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAJBKZQ#issuecomment-538056038>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALSVGJRWYMQMRJZS3SYV2J3QMYXYTANCNFSM4ISZNKYQ>\n> .\n>\n", "My python is Python 3.7.2\r\nI checked the path as you suggested.\r\n) Open command prompt and :-\r\n>>>python\r\n>>>import sys\r\n>>>print(sys.executable)\r\nNote down the path.\r\nii) Open conda prompt and type the same commands.\r\nI got the path as \r\nC:\\ProgramData\\Anaconda37\\python.exe\r\nin both cases\r\nMy CPU is:\r\n Intel (R) Core(TM)2 Duo CPU E7200 @ 2.53GHz\r\nWhether it support AVX instruction sets. What is the solution?\r\nThanking you\r\nGeorge Mathew", "Plese check if AVX is supported.\r\n\r\nIf it is not, the only options are either compiling from source or #19584", "Unfortunately your model does not support AVX instruction sets.\r\nSee https://ark.intel.com/content/www/us/en/ark/products/35348/intel-core-2-duo-processor-e7200-3m-cache-2-53-ghz-1066-mhz-fsb.html", "Thank you so much Mr. Yasir Modak\nDr. George Mathew\n\nOn Fri, Oct 4, 2019 at 10:50 PM Yasir Modak <notifications@github.com>\nwrote:\n\n> Unfortunately your model does not support AVX instruction sets.\n> See\n> https://ark.intel.com/content/www/us/en/ark/products/35348/intel-core-2-duo-processor-e7200-3m-cache-2-53-ghz-1066-mhz-fsb.html\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32147?email_source=notifications&email_token=ALSVGJVHRCFSKVPK3AFUFYDQM53PRA5CNFSM4ISZNKY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAMJ44Q#issuecomment-538484338>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALSVGJSOWMLAMLYVCN3SRK3QM53PRANCNFSM4ISZNKYQ>\n> .\n>\n", "when i start training using this command\r\n\r\n**python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101_voc07.config**\r\n\r\nand get respond something like this\r\n\r\n**2020-03-01 23:00:07.833275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nTraceback (most recent call last):\r\nFile \"train.py\", line 48, in\r\nfrom tensorflow.contrib import framework as contrib_framework\r\nFile \"C:\\Users\\Lab Visual I\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib_init_.py\", line 27, in\r\nfrom tensorflow.contrib import checkpoint\r\nFile \"C:\\Users\\Lab Visual I\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\checkpoint_init_.py\", line 37, in\r\nfrom tensorflow.contrib.checkpoint.python.containers import UniqueNameTracker\r\nFile \"C:\\Users\\Lab Visual I\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\contrib\\checkpoint\\python\\containers.py\", line 20, in\r\nfrom tensorflow.python.training.checkpointable import base as checkpointable_lib\r\nModuleNotFoundError: No module named 'tensorflow.python.training.checkpointable'**\r\n\r\nDescription of the issue (what needs to be changed):\r\ni really dont know how to fix it, i had searched on github and got nothing.", "let {PythonShell} = require('python-shell')\r\n\r\nvar myPythonScriptPath = 'tacotron-master/eval.py';\r\n\r\nvar options = {\r\nmode: 'text',\r\nargs: [' --checkpoint ~/tacotron/logs-tacotron/model.ckpt', '-185000']\r\n};\r\n\r\nPythonShell.run('tacotron-master/eval.py', options, function (err, results) {\r\nif (err) throw err;\r\n// results is an array consisting of messages collected during execution\r\nconsole.log('results: %j', results);\r\n});\r\ni wrote this code to run this command \"python3 eval.py --checkpoint ~/tacotron/logs-tacotron/model.ckpt-185000\"\r\nand gives me error\r\nNo module named 'tensorflow'\r\nwich is in another script.py that eval call import tensor flow\r\nwhat should i do to run this pre-trained model", "> Thanks. I have installed through the command conda install tensorflow Ialso activitate invirinment also ok but on call import tensorflow as tf error message: as given below. ModuleNotFoundError Traceback (most recent call last)<ipython-input-1-64156d691fe5> in <module>----> 1 import tensorflow as tf ModuleNotFoundError: No module named 'tensorflow' Can you please help me to install tensorflow properely my system is windows 7 and pythen 3.7.2 Dr. George Mathew\r\n> [\u2026](#)\r\n> On Tue, Sep 3, 2019 at 11:45 AM ravikyram ***@***.***> wrote: @georgevettathu <https://github.com/georgevettathu> Which version of tensorflow you are using? Can you please go through this link <https://www.tensorflow.org/install/pip> ?Thanks! \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#32147?email_source=notifications&email_token=ALSVGJTILAB3DPRDWNERRTTQHX6IVA5CNFSM4ISZNKY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5XDW3Y#issuecomment-527317871>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ALSVGJUKTVZEWBJMK2VAYRTQHX6IVANCNFSM4ISZNKYQ> .\r\n\r\npip install tensorflow==2.0.0a0", "Python 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> exit()\r\n\r\n(MachineLEarning) C:\\Users\\rosha\\Downloads\\Documents\\Python\\DMProject>pip install tensorflow==2.0.0a0\r\nCollecting tensorflow==2.0.0a0\r\n  Downloading https://files.pythonhosted.org/packages/c7/90/cf5e7fbf4a3c14b314e1ed6571eae1f9ad3b5e32a4e24b2b817466435a21/tensorflow-2.0.0a0-cp37-cp37m-win_amd64.whl (49.4MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49.5MB 139kB/s\r\nRequirement already satisfied: termcolor>=1.1.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.1.0)\r\nRequirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.18.2)\r\nRequirement already satisfied: google-pasta>=0.1.2 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.2.0)\r\nRequirement already satisfied: gast>=0.2.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.2.2)\r\nCollecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301\r\n  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0MB 156kB/s\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.34.2)\r\nRequirement already satisfied: protobuf>=3.6.1 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (3.11.3)\r\nRequirement already satisfied: absl-py>=0.7.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.9.0)\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.14.0)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.1.0)\r\nRequirement already satisfied: astor>=0.6.0 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (0.8.1)\r\nRequirement already satisfied: keras-applications>=1.0.6 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.0.8)\r\nRequirement already satisfied: grpcio>=1.8.6 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tensorflow==2.0.0a0) (1.28.1)\r\nCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115\r\n  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 419kB 182kB/s\r\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (1.0.1)\r\nRequirement already satisfied: markdown>=2.6.8 in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (3.2.1)\r\nRequirement already satisfied: setuptools in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==2.0.0a0) (46.1.3)\r\nRequirement already satisfied: h5py in c:\\users\\rosha\\.conda\\envs\\machinelearning\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0a0) (2.10.0)\r\nInstalling collected packages: tb-nightly, tf-estimator-nightly, tensorflow\r\n  Found existing installation: tensorflow 2.1.0\r\n    Uninstalling tensorflow-2.1.0:\r\n      Successfully uninstalled tensorflow-2.1.0\r\nSuccessfully installed tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\r\n\r\n(MachineLEarning) C:\\Users\\rosha\\Downloads\\Documents\\Python\\DMProject>python\r\nPython 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\rosha\\.conda\\envs\\MachineLEarning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n>>> import tensorflow as tf\r\n>>> import tensorflow as tf\r\n>>> import tensorflow\r\n>>> tensorflow --version\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'version' is not defined\r\n>>> tf.VERSION\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'VERSION'\r\n>>> x1 = tf.constant([1,2,3,4])\r\n2020-04-17 07:27:33.083319: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n>>> x1 = tf.constant([1])\r\n>>> x2 = tf.constant([5])\r\n>>> result = tf.multiply(x1,x2)\r\n>>> print(result)\r\ntf.Tensor([5], shape=(1,), dtype=int32)\r\n>>> a = tf.constant([3,4])\r\n>>> b = tf.constant([45,8])\r\n>>> print(tf.mu;tiply(a,b))\r\n  File \"<stdin>\", line 1\r\n    print(tf.mu;tiply(a,b))\r\n               ^\r\nSyntaxError: invalid syntax\r\n>>> print(tf.multiply(a,b))\r\ntf.Tensor([135  32], shape=(2,), dtype=int32)\r\n>>> x1 = tf.constant([1,2,3,4])\r\n>>> x2 = tf.constant([5,6,7,8])\r\n>>> result = tf.multiply(x1, x2)\r\n>>> print(result)\r\ntf.Tensor([ 5 12 21 32], shape=(4,), dtype=int32)", "Locking to prevent further notifications with similar errors that already have solution included in the thread"]}, {"number": 32146, "title": "vs2015 build tensorflow failed on windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7 64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: compile\r\n- Bazel version (if compiling from source): cmake 3.15.0\r\n- GCC/Compiler version (if compiling from source):  vs2015 update3\r\n- CUDA/cuDNN version:  no\r\n- GPU model and memory: no\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen using vs 2015 update3 to build tensorflow, got following error(see detail in log). \r\noperator system\uff1awindows7 x64\r\ntensorflow versiono: 1.12.0 - cpu\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. cmake \r\n2. vs 2015 update3 (build icu and abseil-cpp by myself, and linked to tensorflow project)\r\n3. build\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n85>tf_session_helper.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::SessionRef::SessionRef(class tensorflow::Session *)\" (??0SessionRef@tensorflow@@QEAA@PEAVSession@1@@Z) referenced in function \"struct TF_Session * __cdecl tensorflow::TF_NewSessionRef(struct TF_Graph *,struct TF_SessionOptions const *,struct TF_Status *)\" (?TF_NewSessionRef@tensorflow@@YAPEAUTF_Session@@PEAUTF_Graph@@PEBUTF_SessionOptions@@PEAUTF_Status@@@Z)\r\n85>c_api.cc.obj : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::NewRemoteDevices(class tensorflow::Env *,class tensorflow::WorkerCacheInterface *,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::function<void __cdecl(class tensorflow::Status const &,class std::vector<class tensorflow::Device *,class std::allocator<class tensorflow::Device *> > *)>)\" (?NewRemoteDevices@tensorflow@@YAXPEAVEnv@1@PEAVWorkerCacheInterface@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$function@$$A6AXAEBVStatus@tensorflow@@PEAV?$vector@PEAVDevice@tensorflow@@V?$allocator@PEAVDevice@tensorflow@@@std@@@std@@@Z@5@@Z) referenced in function \"class tensorflow::Status __cdecl `anonymous namespace'::GetAllRemoteDevices(class std::vector<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,class tensorflow::WorkerCacheInterface *,class std::unique_ptr<class tensorflow::DeviceMgr,struct std::default_delete<class tensorflow::DeviceMgr> > *)\" (?GetAllRemoteDevices@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@AEBV?$vector@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@std@@PEAVWorkerCacheInterface@3@PEAV?$unique_ptr@VDeviceMgr@tensorflow@@U?$default_delete@VDeviceMgr@tensorflow@@@std@@@5@@Z)\r\n85>c_api.cc.obj : error LNK2019: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::SessionMgr::CreateSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class tensorflow::ServerDef const &,bool)\" (?CreateSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVServerDef@2@_N@Z) referenced in function \"class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)\" (?UpdateTFE_ContextWithServerDef@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z)\r\n85>c_api.cc.obj : error LNK2019: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::SessionMgr::WorkerSessionForSession(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::shared_ptr<struct tensorflow::WorkerSession> *)\" (?WorkerSessionForSession@SessionMgr@tensorflow@@QEAA?AVStatus@2@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$shared_ptr@UWorkerSession@tensorflow@@@5@@Z) referenced in function \"class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)\" (?UpdateTFE_ContextWithServerDef@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z)\r\n85>c_api.cc.obj : error LNK2019: unresolved external symbol \"class tensorflow::eager::EagerClientCache * __cdecl tensorflow::eager::NewGrpcEagerClientCache(class std::shared_ptr<class tensorflow::GrpcChannelCache>)\" (?NewGrpcEagerClientCache@eager@tensorflow@@YAPEAVEagerClientCache@12@V?$shared_ptr@VGrpcChannelCache@tensorflow@@@std@@@Z) referenced in function \"class tensorflow::Status __cdecl `anonymous namespace'::UpdateTFE_ContextWithServerDef(int,class tensorflow::ServerDef const &,struct TFE_Context *)\" (?UpdateTFE_ContextWithServerDef@?A0xdd98b8cf@@YA?AVStatus@tensorflow@@HAEBVServerDef@3@PEAUTFE_Context@@@Z)\r\n", "comments": ["i have noticed that there is an issue just as the same as this one, but it seems like there is no solution for it. especially build under cmake.", "@YoungSharp, \r\nThe TensorFlow team does not officially support cmake, sorry. Please try out building from source with [Bazel](https://www.tensorflow.org/install/source_windows). Thanks!", "i had solved it by adding some file(find those unrecognized function in tensorflow folder) into project tensorflow and comment \"WorkerSessionForSession\" function then build succeeded.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32146\">No</a>\n", "> i had solved it by adding some file(find those unrecognized function in tensorflow folder) into project tensorflow and comment \"WorkerSessionForSession\" function then build succeeded.\r\n\r\nHi\uff0cI get the same error,and I haven't found the solutions.Could you please tell me how to solve it ?Thanks"]}, {"number": 32145, "title": "The function tf.graph_util.remove_training_nodes doesn't work as expected", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): NO\r\n- GCC/Compiler version (if compiling from source): NO\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nWhen I trying to remove training node with the function tf.graph_util.remove_training_nodes, a Identity node named \"FeatureExtractor/MobilenetV2/expanded_conv_13/expansion_output\r\n\" is not removed.\r\n\r\n![image](https://user-images.githubusercontent.com/4157827/64088048-358e6100-cd72-11e9-93c3-8dfbdab3fc14.png)\r\n\r\n**Describe the expected behavior**\r\nI tried this in Python2 and there is no such issue. Could you look into it?\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nYou can get the model from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\r\nAfter untar, you'll see ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb.\r\n```python\r\nimport tensorflow as tf\r\ninput_graph_def = tf.GraphDef()\r\nwith open(\"ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\", \"rb\") as f:\r\n    input_graph_def.ParseFromString(f.read())\r\n    out_graph_def = tf..graph_util.remove_training_nodes(input_graph_def)\r\n    tf.io.write_graph(out_graph_def, \"./\", \"fused.pb\", as_text=False)\r\n```\r\n Then you can review it in https://lutzroeder.github.io/netron/\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNA", "comments": ["I was able to reproduce the behavior using TF 1.15.2\r\nHowever unfortunately [`tf.graph_util.remove_training_nodes`](https://www.tensorflow.org/api_docs/python/tf/graph_util/import_graph_def) is deprecated and no longer part of TF 2.\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32145\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32145\">No</a>\n"]}, {"number": 32144, "title": "TF 2.0: metrics get mixed up when adding metrics in multiple layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6, Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1 and v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nCreating a Layer and Model, calling `add_metric` in both results in the metric values getting mixed up.\r\n\r\n**Describe the expected behavior**\r\nMetric values should not get mixed up\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import Model\r\nfrom tensorflow.python.keras.layers import Layer\r\n\r\n\r\nclass MyLayer(Layer):\r\n    def __init__(self, **kwargs):\r\n        super(MyLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, input, training=None, mask=None):\r\n        self.add_metric(tf.ones([32]) * 2.0, name='two', aggregation='mean')\r\n        return input\r\n\r\n\r\nclass MyModel(Model):\r\n    def __init__(self, **kwargs):\r\n        super(MyModel, self).__init__(**kwargs)\r\n        self._sampler = MyLayer(name='sampler')\r\n\r\n    def call(self, input, training=None, mask=None):\r\n        z = self._sampler(input)\r\n        self.add_metric(tf.ones([32]) * 1.0, name='one', aggregation='mean')\r\n        self.add_metric(tf.ones([32]) * 3.0, name='three', aggregation='mean')\r\n        return z\r\n\r\n\r\ndef train(dataset_train, epochs):\r\n    tf.config.experimental_run_functions_eagerly(True)\r\n\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\r\n\r\n    loss = tf.losses.mean_squared_error\r\n\r\n    model = MyModel()\r\n    model.compile(optimizer=optimizer, loss=loss, run_eagerly=True)\r\n\r\n    print('Training...')\r\n    history = model.fit(dataset_train, epochs=epochs, verbose=1, callbacks=[])\r\n\r\n\r\ndef main():\r\n    print('Preparing data...')\r\n    batch_size = 32\r\n    num_examples = 32\r\n    xdata = np.random.uniform(size=[num_examples, 16]).astype(np.float32)\r\n    dataset_train = tf.data.Dataset.from_tensor_slices((xdata, xdata))\r\n    dataset_train = dataset_train.batch(batch_size, drop_remainder=True)\r\n\r\n    train(dataset_train, epochs=3)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nOutput:\r\n```\r\n1/1 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - two: 1.0000 - one: 3.0000 - three: 2.0000\r\n```\r\nNote how the values got mixed up. 'two' should assume a value of 2.0, 'one' a value of 1.0, 'three' a value of 3.0.\r\n\r\n**Other info / logs**\r\nThe problem appeared in a more complex example (training on GPU, graph mode), so although the minimal example above is using eager mode, it also happens when running non-eagerly.\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1,2.0.0-rc0 and 2.0.0.dev20190902 and was able to reproduce the issue.Please, find the [gist ](https://colab.sandbox.google.com/gist/ravikyram/980587c5ffa5a6a8c2a049cc6dba2dce/untitled145.ipynb)here. Thanks!", "@peterkfm I am closing this issue as it was resolved by https://github.com/tensorflow/tensorflow/pull/32220. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/68142e9b75eef414bf38014498fc6c85/tf_32144.ipynb). Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32144\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32144\">No</a>\n"]}, {"number": 32143, "title": "Fix README grammatically", "body": "Found grammatical errors in README", "comments": []}, {"number": 32142, "title": "Custom loss with extra argument in TF 2.0", "body": "The traditional method of creating a custom loss function with an additional input for tf.keras no longer functions in tensorflow 2.0. It seems like the only way to do it now is with a custom training loop, which means you lose a lot of the convenience of keras (callbacks etc).\r\n\r\nIn the following case, the extra argument is the input data into the model, which is contained in a `Dataset`. In 1.14 case, I'd run `.make_one_shot_iterator().get_next()` on the dataset and then pass the tensor I get into the loss function. This isn't possible with eager execution, so instead I've tried to just pass the dataset.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass WeightedSDRLoss(tf.keras.losses.Loss):\r\n\r\n    def __init__(self, noisy_signal, reduction=tf.keras.losses.Reduction.AUTO, name='WeightedSDRLoss'):\r\n        super().__init__(reduction=reduction, name=name)\r\n        self.noisy_signal = noisy_signal\r\n\r\n    def sdr_loss(self, sig_true, sig_pred):\r\n        return (-tf.reduce_mean(sig_true * sig_pred) /\r\n                (tf.norm(tensor=sig_pred) * tf.norm(tensor=sig_true)))\r\n\r\n    def call(self, y_true, y_pred):\r\n        noise_true = self.noisy_signal - y_true\r\n        noise_pred = self.noisy_signal - y_pred\r\n        alpha = (tf.reduce_mean(tf.square(y_true)) /\r\n                 tf.reduce_mean(tf.square(y_true) + tf.square(self.noisy_signal - y_pred)))\r\n        return alpha * self.sdr_loss(y_true, y_pred) + (1 - alpha) * self.sdr_loss(noise_true, noise_pre$\r\n\r\ndata_x = np.random.rand(5, 4, 1)\r\ndata_y = np.random.rand(5, 4, 1)\r\n\r\nx = tf.keras.layers.Input(shape=[4, 1])\r\ny = tf.keras.layers.Activation('tanh')(x)\r\nmodel = tf.keras.models.Model(inputs=x, outputs=y)\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((data_x, data_y)).batch(1)\r\n\r\nmodel.compile(loss=WeightedSDRLoss(x))\r\nmodel.fit(train_dataset)\r\n```\r\n\r\n I get the error:\r\n\r\n````python\r\nop_name = '__inference_distributed_function_169', num_outputs = 2\r\ninputs = [<tf.Tensor: id=82, shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: id=83, shape=(), dtype=variant, numpy=<unprintable>>, <tf.Tensor 'input_1:0' shape=(None, 4, 1) dtype=float32>]\r\nattrs = ('executor_type', '', 'config_proto', b'\\n\\x07\\n\\x03GPU\\x10\\x00\\n\\x07\\n\\x03CPU\\x10\\x012\\x02J\\x008\\x01')\r\nctx = <tensorflow.python.eager.context.Context object at 0x11785f4e0>\r\nname = None\r\n\r\n    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):\r\n      \"\"\"Execute a TensorFlow operation.\r\n    \r\n      Args:\r\n        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to\r\n          execute.\r\n        num_outputs: The number of outputs of the operation to fetch.\r\n                     (Explicitly provided instead of being inferred for performance\r\n                     reasons).\r\n        inputs: A list of inputs to the operation. Each entry should be a Tensor, or\r\n          a value which can be passed to the Tensor constructor to create one.\r\n        attrs: A tuple with alternating string attr names and attr values for this\r\n          operation.\r\n        ctx: The value of context.context().\r\n        name: Customized name for the operation.\r\n    \r\n      Returns:\r\n        List of output Tensor objects. The list is empty if there are no outputs\r\n    \r\n      Raises:\r\n        An exception on error.\r\n      \"\"\"\r\n      device_name = ctx.device_name\r\n      # pylint: disable=protected-access\r\n      try:\r\n        ctx.ensure_initialized()\r\n        tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n                                                   op_name, inputs, attrs,\r\n>                                                  num_outputs)\r\nE                                                  TypeError: An op outside of the function building code is being passed\r\nE                                                  a \"Graph\" tensor. It is possible to have Graph tensors\r\nE                                                  leak out of the function building context by including a\r\nE                                                  tf.init_scope in your function building code.\r\nE                                                  For example, the following function will fail:\r\nE                                                    @tf.function\r\nE                                                    def has_init_scope():\r\nE                                                      my_constant = tf.constant(1.)\r\nE                                                      with tf.init_scope():\r\nE                                                        added = my_constant * 2\r\nE                                                  The graph tensor has name: input_1:0\r\n\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py:61: TypeError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test_loss():\r\n    \r\n        data_x = np.random.rand(5, 4, 1)\r\n        data_y = np.random.rand(5, 4, 1)\r\n    \r\n        x = keras.layers.Input(shape=[4, 1])\r\n        y = keras.layers.Activation('tanh')(x)\r\n        model = keras.models.Model(inputs=x, outputs=y)\r\n    \r\n        train_dataset = tf.data.Dataset.from_tensor_slices((data_x, data_y)).batch(1)\r\n        print(train_dataset)\r\n    \r\n        model.compile(loss=WeightedSDRLoss(x))\r\n>       model.fit(train_dataset)\r\n\r\ntest_preprocess.py:162: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:734: in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py:324: in fit\r\n    total_epochs=epochs)\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py:123: in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py:86: in execution_function\r\n    distributed_function(input_fn))\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:445: in __call__\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1141: in _filtered_call\r\n    self.captured_inputs)\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1224: in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n../../../anaconda3/envs/separate2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:511: in call\r\n    ctx=ctx)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nop_name = '__inference_distributed_function_169', num_outputs = 2\r\ninputs = [<tf.Tensor: id=82, shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: id=83, shape=(), dtype=variant, numpy=<unprintable>>, <tf.Tensor 'input_1:0' shape=(None, 4, 1) dtype=float32>]\r\nattrs = ('executor_type', '', 'config_proto', b'\\n\\x07\\n\\x03GPU\\x10\\x00\\n\\x07\\n\\x03CPU\\x10\\x012\\x02J\\x008\\x01')\r\nctx = <tensorflow.python.eager.context.Context object at 0x11785f4e0>\r\nname = None\r\n\r\n    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):\r\n      \"\"\"Execute a TensorFlow operation.\r\n    \r\n      Args:\r\n        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to\r\n          execute.\r\n        num_outputs: The number of outputs of the operation to fetch.\r\n                     (Explicitly provided instead of being inferred for performance\r\n                     reasons).\r\n        inputs: A list of inputs to the operation. Each entry should be a Tensor, or\r\n          a value which can be passed to the Tensor constructor to create one.\r\n        attrs: A tuple with alternating string attr names and attr values for this\r\n          operation.\r\n        ctx: The value of context.context().\r\n        name: Customized name for the operation.\r\n    \r\n      Returns:\r\n        List of output Tensor objects. The list is empty if there are no outputs\r\n    \r\n      Raises:\r\n        An exception on error.\r\n      \"\"\"\r\n      device_name = ctx.device_name\r\n      # pylint: disable=protected-access\r\n      try:\r\n        ctx.ensure_initialized()\r\n        tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n                                                   op_name, inputs, attrs,\r\n                                                   num_outputs)\r\n      except core._NotOkStatusException as e:\r\n        if name is not None:\r\n          message = e.message + \" name: \" + name\r\n        else:\r\n          message = e.message\r\n        six.raise_from(core._status_to_exception(e.code, message), None)\r\n      except TypeError as e:\r\n        keras_symbolic_tensors = [\r\n            x for x in inputs if ops._is_keras_symbolic_tensor(x)\r\n        ]\r\n        if keras_symbolic_tensors:\r\n          raise core._SymbolicException(\r\n              \"Inputs to eager execution function cannot be Keras symbolic \"\r\n>             \"tensors, but found {}\".format(keras_symbolic_tensors))\r\nE         tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_1:0' shape=(None, 4, 1) dtype=float32>]\r\n````\r\n\r\nBasically seems like the previous way of passing extra arguments into a loss function in keras is broken.\r\n\r\nI'm running tensorflow 2.0-rc0 and this happens regardless of the platform.\r\n", "comments": ["@lminer ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "@oanush I updated it with running code.", "I tested this and it seems like it works in the beta version.", "@lminer may I ask which of the two codes above you tested, and in which \"beta\" version? Is that rc0, rc1, or nightly?", "@bersbersbers sorry for the lack of clarity. This problem did not exist in beta0, but it does exist in rc0 and rc1.", "The provided code snippet looks incomplete. Can you please update it? Also can you please test with tf 2.0 nightly version and check if issue still persists?\r\n```pip install tf-nightly-2.0-preview```\r\nThanks!", "@ymodak I can confirm that the error is still present on the nightly preview. I updated the code so it should run no problem now.", "@pavithrasv, @ymodak As an update, this still does not work in the stable release of Tensorflow 2.0. However, if I set the flag `experimental_run_tf_function=False` in `model.compile` it runs without a problem.\r\n\r\nAt the end of the day, this is a bit of a hack in order to pass the model input into the cost function. It might be worthwhile to simply include a hook for model input in `keras.losses.Loss`. There are plenty of custom loss functions that require this input.", "@lminer Thanks for the `experimental_run_tf_function=False` tip, what exactly does it do? It's not documented anywhere... ", "@lminer Is this a good solution for you?:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass WeightedSDRLoss(tf.keras.losses.Loss):\r\n\r\n    def __init__(self, noisy_signal, reduction=tf.keras.losses.Reduction.AUTO, name='WeightedSDRLoss'):\r\n        super().__init__(reduction=reduction, name=name)\r\n        self.noisy_signal = noisy_signal\r\n\r\n    def sdr_loss(self, sig_true, sig_pred):\r\n        return (-tf.reduce_mean(sig_true * sig_pred) /\r\n                (tf.norm(tensor=sig_pred) * tf.norm(tensor=sig_true)))\r\n\r\n    def call(self, y_true, y_pred):\r\n        noise_true = self.noisy_signal - y_true\r\n        noise_pred = self.noisy_signal - y_pred\r\n        alpha = (tf.reduce_mean(tf.square(y_true)) /\r\n                 tf.reduce_mean(tf.square(y_true) + tf.square(self.noisy_signal - y_pred)))\r\n        return alpha * self.sdr_loss(y_true, y_pred) + (1 - alpha) * self.sdr_loss(noise_true, noise_pred)\r\n\r\ndata_x = np.random.rand(5, 4, 1)\r\ndata_y = np.random.rand(5, 4, 1)\r\n\r\ny_true = tf.keras.layers.Input(shape=[4, 1])\r\nx = tf.keras.layers.Input(shape=[4, 1])\r\ny_pred = tf.keras.layers.Activation('tanh')(x)\r\nmodel = tf.keras.models.Model(inputs=[x, y_true], outputs=y_pred)\r\nmodel.add_loss(WeightedSDRLoss(x)(y_true, y_pred))\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(((data_x, data_y),)).batch(1)\r\n\r\nmodel.compile()\r\nmodel.fit(train_dataset)\r\n```", "@feature-engineer, it's definitely better, but the other problem is loading a serialized version of the model. I have to do it in a seriously hacky way right now, where I recompile the model all over again with the same loss function. I use the original optimizer so I can preserve that state.", "I encountered a problem with the same solution.\r\nI can't say that it was the *same* problem, but setting `experimental_run_tf_function = False` on the compile method solved it.\r\n\r\nI'll update with the error message as soon as I am able to reproduce it, but it was essentially.\r\n```\r\nTensor cannot be converted to numpy array.\r\n```", "@lminer What should I do if the argument I want to pass to the loss function is not the input to the model but a different tensor in my Dataset? ", "@lminer @ymodak \r\nI'd like to report a follow-up issue. I tried to run Lminer's code snippet on TF2.0.0--I use Spyder 3.3.6, Python 3.7 I got an error\r\n`\r\n\tTypeError: Cannot convert 'auto' to EagerTensor of dtype float\r\n`\r\nI also tried to build up my own custom loss with the same structure as lminer's, the same issue was raised.\r\nIs that another bug of tf?\r\n\r\nPlus, the docs \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction?version=stable\r\nis a bit confusing. If my minibatch size is larger than 1, and if I use 'AUTO' option, shall I average my final loss over the minibatch? Do both 'AUTO' and 'SUM_OVER_BATCH_SIZE' mean that I will NOT need to do so if my code has nothing to do with distributed strategy?", "This workaround using `experimental_run_tf_function = False` in the compile line does not appear to work as of the newest Tensorflow version, 2.2.0, with Python 3.8 but does work fine with Tensorflow version 2.1.0 with Python 3.7. Both with Keras 2.3.1.", "@lminer Is this still an issue for you. I tried `TF2.4.1` and was able to save and load and didn't face any issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/e709aa673d747c9adba48b9efdadf9d3/untitled.ipynb) is a gist for reference. \r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n\r\nIf anyone has any similar issue, please share a standalone code to reproduce the issue. If you have any new issue, please file a new issue. thanks!", "Looks good", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32142\">No</a>\n"]}, {"number": 32141, "title": "Linspace for tensors", "body": "Issue [32093](https://github.com/tensorflow/tensorflow/issues/32093)\r\n\r\nAdds the possibility to run linspace on tensors, matching `numpy`'s behavior since `1.16.0`.\r\n\r\n- Implemented a version that does linspace on tensors along a given axis. The only change in the signature of the function is adding an optional parameter axis. Let me know if this is a problem and it should be done in a separate, new operation.\r\n- All unit tests from before are still passing (without changing them). Added unit tests for nd arrays. Added unit tests for corner cases (num=1). Compared the results to the outputs of the corresponding calls to `np.linspace`\r\n-  If `num > 1`, `start` and `stop` are guaranteed to be the first and the last elements of the result. The result is built by concatenating start, evenly spaced numbers and stop along the specified axis.\r\n- Added test for the case when num is a tensor and not a Python int\r\n- Negative axes are handled correctly\r\n- Changed the doc string of the parameter `err` of the method `assertNDArrayNear`, which is (I believe) incorrect: it says that it is `The maximum absolute difference allowed.`, while the function uses [`np.linalg.norm`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html) without specifying the order of the norm, hence it defaults to order 2 for vectors and matrices, which is different, e.g:\r\n```python\r\na = np.array([1., 2.])\r\nb = np.array([3., 5.])\r\nprint(np.linalg.norm(a - b)) # returns 3.6055\r\nprint(np.max(np.abs(a - b))) # returns 3.\r\n```\r\n\r\nFew things that I am not sure about and probably the more experienced reviewers can help with:\r\n- Is adding a new parameter with default value a problem for backward compatibility?\r\n- Because the implementation is different from the previous one (a raw kernel) and consisting of other operations (like `repeat`, `tile`, `range`, etc.), would this be a problem for example when deserializing a graph from previous versions?\r\n- Currently, if `num==0` a tensor with a shape with an added axis whose value is zero is returned (which matches Numpy's behavior), but this is different from the old 0D implementation.", "comments": ["Thanks for the PR! I suspect I am not the right reviewer. @nfelt has worked on linspace in Jan this year, perhaps they will have more context.", "So - I actually don't have very much context on linspace since my work in January was just a small bug fix.  I can take a look but since core TF isn't my primary area it may take a little while for me to do due diligence in reviewing this (especially since the new code is pretty different from the old approach).\r\n\r\n@yongtang I see you might have a little context from the originating feature request issue #32093 - would you be up for reviewing this?", "@nfelt Sure. I can help review the PR.", "In TensorFlow, APIs change from time to time though backward compatibility is always preserved. \r\n\r\nIn the past the \"backward compatibility\" is typically preserved within the same language binding. For example, in C++ when APIs are updated they are in the form of `Gather` vs `GatherV2`. For python APIs, backward-compatibility are preserved by extending additional args.\r\n\r\nI haven't see the change from one language binding to another in yet.\r\n\r\nIn this PR the binding is from C++ to python. This could potentially be desirable as it reduces the need to maintain C++ (harder to maintain than python).\r\n\r\nIn case switch from C++ to python is accepted, the C++ kernel of `Linspace` may need to continue to exist for backward-compatibility. The performance of python implementation on top of primitive ops vs C++ Linspace may also need to be profiled, so that there is not a big drop in performance. \r\n\r\nAdding API review team to see if switching from C++ implementation to python (for `linspace`) is accepted. /cc @tensorflow/api-owners ", "Thanks a lot for the comments so far! I just wanted to add that:\r\n\r\n* Moving this to a separate python operation (e.g `linspace_nd` or something is similar) is always possible\r\n* I have used the Numpy implementation as a reference - the idea is to build the range from `start` and `stop`, then repeat it so that it gets the same shape as `start`, and then multiply that by the step nd array (`delta`), and shift it so that it starts from `start`.", "@hristo-vrigazov Could you please check reviewer comments and keep us posted. Thanks!", "Yes, absolutely, I will check this soon. I was very busy these days.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32141) for more info**.\n\n<!-- need_author_consent -->", "Sorry for the mess, I did a mistake rebasing... Closing this and will reopen a new PR"]}, {"number": 32140, "title": "[XLA] [MLIR] Don't include CUDA when it isn't configured", "body": "The git transaction f78a3d92b281e4904773c4a26e740d8995ed252e introduced a dependency on CUDA for all XLA backends.  Not everyone has CUDA compiled into their backend.\r\n\r\nThis change eliminates that dependency when CUDA is not configured.", "comments": ["one for @jpienaar ?\r\n", "This looks good, but I'm curious how this adds CUDA dependency on all XLA backends, there might a configuration problem somewhere.\r\n\r\n@sherhut this might be of interest.", "I'm curious too, but I can't imagine why.  Perhaps the test which depends on the mlir_gpu_plugin is always sucked into the test suite due to some blanket 'all tests' link somewhere, and that drags in mlir_gpu_plugin.", "i do not believe that any of the 3 CI failures are down to this change.  2 are gRPC timeouts checking that a gRPC failure warning can actually be serialized, and some is something to do with bootstrapping the android test suite.\r\n\r\nplease try running the CI again.", "@gbaned  - can we re-run the test suite?  these failures are clearly nothing to do with the changes in this PR"]}, {"number": 32139, "title": "TF 2.0.0-rc0 + TFP 0.7 broken combo: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key", "body": "Error occurs:\r\ntf-gpu 2.0.0-rc0 with tfp 0.7\r\n\r\nCode to reproduce:\r\n\r\n```\r\nimport tensorflow_probability as tfp\r\ntfp.distributions.MultivariateNormalDiag([0.], [1.]).sample()\r\n```\r\n\r\nError returned:\r\n\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/home/pycharm_project/VAE/save_issue_reproduction.py\", line 3, in <module>\r\n>     tfp.distributions.MultivariateNormalDiag([0.], [1.]).sample()\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/distribution.py\", line 840, in sample\r\n>     return self._call_sample_n(sample_shape, seed, name, **kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/transformed_distribution.py\", line 391, in _call_sample_n\r\n>     y = self.bijector.forward(x, **bijector_kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py\", line 933, in forward\r\n>     return self._call_forward(x, name, **kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py\", line 904, in _call_forward\r\n>     mapping = self._lookup(x=x, kwargs=kwargs)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py\", line 1343, in _lookup\r\n>     mapping = self._from_x[x].get(subkey, mapping).merge(x=x)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py\", line 151, in __getitem__\r\n>     return super(WeakKeyDefaultDict, self).__getitem__(weak_key)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py\", line 181, in __hash__\r\n>     return hash(x)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\", line 713, in __hash__\r\n>     raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n> TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n\r\n", "comments": ["I'm getting the same error using the same versions of tensorflow and tensorflow probability. Here is my code trying to run MCMC which I adapted from \"Bayesian Methods For Hackcers\"\r\n\r\n```\r\nyards_data = array([ 7.,  7.,  4., 12.,  5., 17.], dtype=float32)\r\n\r\ndef joint_log_prob(yards_data, sigma):\r\n    rv_mu = tfd.Normal(loc=M, scale=S)\r\n    rv_sigma = tfd.Uniform(low=0.0, high=20.0)\r\n    \r\n    mu_sample = mu.sample()\r\n    yards = tfd.Normal(loc=mu_sample,scale=sigma)\r\n    #compute log prob\r\n    return (\r\n        yards.log_prob(yards_data)\r\n        + rv_sigma.log_prob(sigma)\r\n        + rv_mu.log_prob(mu_sample)\r\n    )\r\n\r\ndef unnormalized_log_posterior(sigma):\r\n    return joint_log_prob(yards_data, sigma)\r\n\r\ninitial_chain_state = [tf.cast(s, tf.float32) * tf.ones([], dtype=tf.float32, name=\"init_sigma\")]\r\nunconstraining_bijectors = [tfp.bijectors.Exp()]\r\nstep_size = tf.Variable(0.05, name='step_size', trainable=False)\r\n\r\n# Sample from the chain.\r\n[\r\n    sigma_samples,\r\n], kernel_results = tfp.mcmc.sample_chain(\r\n    num_results=100000,\r\n    num_burnin_steps=10000,\r\n    current_state=initial_chain_state,\r\n    kernel=tfp.mcmc.TransformedTransitionKernel(\r\n        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\r\n            target_log_prob_fn=unnormalized_log_posterior,\r\n            num_leapfrog_steps=2,\r\n            step_size=step_size,\r\n            step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(1),\r\n            state_gradients_are_stopped=True),\r\n        bijector=unconstraining_bijectors))\r\n```\r\n\r\nThis creates error:\r\n```\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\mcmc\\internal\\util.py:494: UserWarning: `step_size` is not a `tf.Tensor`, Python number, or Numpy array. If this parameter is mutable (e.g., a `tf.Variable`), then the behavior implied by `store_parameters_in_results` will silently change on 2019-08-01. Please consult the docstring for `store_parameters_in_results` details and use `store_parameters_in_results=True` to silence this warning.\r\n  param_name))\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-79-b8bfc92e8c79> in <module>\r\n     13             step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(1),\r\n     14             state_gradients_are_stopped=True),\r\n---> 15         bijector=unconstraining_bijectors))\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\mcmc\\sample.py in sample_chain(num_results, current_state, previous_kernel_results, kernel, num_burnin_steps, num_steps_between_results, trace_fn, return_final_kernel_results, parallel_iterations, name)\r\n    324         current_state)\r\n    325     if previous_kernel_results is None:\r\n--> 326       previous_kernel_results = kernel.bootstrap_results(current_state)\r\n    327 \r\n    328     if trace_fn is None:\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\mcmc\\transformed_kernel.py in bootstrap_results(self, init_state, transformed_init_state)\r\n    328         init_state_parts = (init_state if mcmc_util.is_list_like(init_state)\r\n    329                             else [init_state])\r\n--> 330         transformed_init_state_parts = self._inverse_transform(init_state_parts)\r\n    331         transformed_init_state = (\r\n    332             transformed_init_state_parts if mcmc_util.is_list_like(init_state)\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\mcmc\\transformed_kernel.py in fn(state_parts)\r\n     71   def fn(state_parts):\r\n     72     return [b.inverse(sp)\r\n---> 73             for b, sp in zip(bijector, state_parts)]\r\n     74   return fn\r\n     75 \r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\mcmc\\transformed_kernel.py in <listcomp>(.0)\r\n     71   def fn(state_parts):\r\n     72     return [b.inverse(sp)\r\n---> 73             for b, sp in zip(bijector, state_parts)]\r\n     74   return fn\r\n     75 \r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py in inverse(self, y, name, **kwargs)\r\n    975       NotImplementedError: if `_inverse` is not implemented.\r\n    976     \"\"\"\r\n--> 977     return self._call_inverse(y, name, **kwargs)\r\n    978 \r\n    979   def _compute_inverse_log_det_jacobian_with_caching(\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py in _call_inverse(self, y, name, **kwargs)\r\n    944       if not self._is_injective:  # No caching for non-injective\r\n    945         return self._inverse(y, **kwargs)\r\n--> 946       mapping = self._lookup(y=y, kwargs=kwargs)\r\n    947       if mapping.x is not None:\r\n    948         return mapping.x\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py in _lookup(self, x, y, kwargs)\r\n   1344     if y is not None:\r\n   1345       # We removed y at caching time. Add it back if we lookup successfully.\r\n-> 1346       mapping = self._from_y[y].get(subkey, mapping).merge(y=y)\r\n   1347     return mapping\r\n   1348 \r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py in __getitem__(self, key)\r\n    149   def __getitem__(self, key):\r\n    150     weak_key = HashableWeakRef(key, lambda w: self.pop(w, None))\r\n--> 151     return super(WeakKeyDefaultDict, self).__getitem__(weak_key)\r\n    152 \r\n    153   # This is the \"DefaultDict\" part.\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py in __hash__(self)\r\n    179     x = self()\r\n    180     if not isinstance(x, np.ndarray):\r\n--> 181       return hash(x)\r\n    182     # Note: The following logic can never be reached by the public API because\r\n    183     # the bijector base class always calls `convert_to_tensor` before accessing\r\n\r\nc:\\users\\ablan\\miniconda3\\envs\\tf2.0rc\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py in __hash__(self)\r\n    711     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and\r\n    712         (g is None or g._building_function)):  # pylint: disable=protected-access\r\n--> 713       raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n    714                       \"Instead, use tensor.experimental_ref() as the key.\")\r\n    715     else:\r\n\r\nTypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n\r\n\r\n```", "Thanks for the report! This is due to a change in TF's Tensor equality semantics that occurred between the TFP 0.7 release and the TF 2.0.0-rc0 prerelease. You can fix it by installing TFP 0.8-rc0 (`pip install tensorflow_probability==0.8.0rc0 --user --upgrade`), which should include the necessary updates.", "Closing for now; feel free to reopen if the problem still occurs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32139\">No</a>\n", "**After upgrade** \r\n```\r\npip install tensorflow_probability==0.8.0rc0 --user --upgrade\r\n```\r\n\r\n**Error still happens**\r\n```\r\nFile \"/Users/techmaster/LearnAI/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 713, in __hash__\r\n    raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\nTypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n```\r\n\r\n**My environment:**\r\n- MacOSX High Sierra\r\n- TensorFlow version 2.0.0-rc1\r\n- tf-estimator-nightly  1.14.0.dev2019080601\r\n- Keras                 2.2.5\r\n- Keras-Applications    1.0.8\r\n- Keras-Preprocessing   1.1.0\r\n\r\n**Piece of code causes error**\r\n```python\r\nintermediate_layer_model = Model(inputs=self.model.input, outputs=self.model.get_layer(\"embedding\").output)\r\nintermediate_output = intermediate_layer_model.predict(encoded_texts)\r\nreturn intermediate_output\r\n```", "still doesn't work after installing rc0", " tensorflow_probability==0.8.0rc0  did not fix the error. Any other solution?", "Same issue and the suggested solution doesn't work", "Same issue here. Doesn't work even after installing \"ensorflow_probability==0.8.0rc0\"", "Hi.\r\n\r\nSame issue. As @NabinAdhikari674 says, `tensorflow_probability` doesn't work. In my case the problem comes from trying to migrate from TF 1.14 to TF 2.0 with Keras 2.3.1 (if it helps).\r\n\r\n", "pip install tf-hub-nightly\r\nit works", "> pip install tf-hub-nightly\r\n> it works\r\n\r\nThat didn't work for me", "Me neither. :-(", "Me neither", "I am getting this error with tf2.0 migration on tf-nightly build. ", "I am also getting the same error when I am  trying to use the variable init_wt:\r\n```\r\n\r\ninit_wt = tf.keras.initializers.TruncatedNormal(stddev=weight_init_std, mean=weight_init_mean, seed=100)\r\n\r\ninit_wt \r\n\r\n<tensorflow.python.ops.init_ops_v2.TruncatedNormal at 0x1b7d6519f08>\r\n\r\n```\r\n\r\nI think the problem is caused by a mismatch between Keras and Tensorflow libraries as mentioned in [this Stackoverflow question](https://stackoverflow.com/questions/58778872/typeerror-tensor-is-unhashable-if-tensor-equality-is-enabled-instead-use-tens). The solution proposed works for me. But, it means downgrading TF version and not being able to use GPU.", "I'm having this problem as well", "ditto! :(", "(Not sure how I got assigned this; in fact I didnt even notice. Reassigned to alextp for further triage.)", "If the problem still occurs for you please open a new issue with the steps you need to reproduce the problem. As far as we can tell the TFP issue here has been fixed."]}, {"number": 32138, "title": "TF 2.0: tf.function+dataset do not use GPU by default", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0rc0 \r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: CUDA 10.0/ cuDNN 7.6\r\n- GPU model and memory: NVIDIA GeForce GT 630, 2048Mb\r\n\r\n**Describe the current behavior**\r\nWrapping iteration through the dataset with tf.function move computations from GPU to CPU.\r\n\r\n**Describe the expected behavior**\r\nI read in [tutorial](https://www.tensorflow.org/beta/guide/effective_tf2) that wrapping iteration through the dataset with tf.function should increase performance, but instead of it computations move from GPU to CPU and as result slow down. Is it correct behavior? \r\n\r\nI can fix this behavior by manual device placement, by I think that computations should be done on GPU by default.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\ndata = tf.Variable(initial_value=tf.zeros((1, 1000, 1000)))\r\ndataset = tf.data.Dataset.from_tensor_slices(data).repeat(1000)\r\n\r\nweights = tf.Variable(initial_value=tf.zeros((1000, 1000)))\r\n\r\nresult = tf.Variable(initial_value=tf.zeros((1000, 1000)))\r\n\r\ndef py_process():\r\n    for sample in dataset:\r\n        result.assign_add(tf.matmul(sample, weights))\r\n    return result\r\n\r\n@tf.function\r\ndef tf_process():\r\n    for sample in dataset:\r\n        result.assign_add(tf.matmul(sample, weights))\r\n    return result\r\n\r\n@tf.function\r\ndef tf_gpu_process():\r\n    for sample in dataset:\r\n        with tf.device('/device:GPU:0'):\r\n            result.assign_add(tf.matmul(sample, weights))\r\n    return result\r\n\r\nt = time.time()\r\npy_process()\r\nprint(\"py time\", time.time() - t)\r\n\r\nt = time.time()\r\ntf_process()\r\nprint(\"tf time\", time.time() - t)\r\n\r\nt = time.time()\r\ntf_gpu_process()\r\nprint(\"tf gpu time\", time.time() - t)\r\n```\r\n\r\nOutput:\r\n```\r\npy time 14.078931093215942\r\ntf time 47.290977478027344\r\ntf gpu time 13.702632665634155\r\n```\r\n\r\n**Other info / logs**\r\nI am tracking GPU usage with GPU-Z program, here is screenshot:\r\n![gpu-z](https://user-images.githubusercontent.com/39711437/64075084-56947a80-ccbc-11e9-9853-66555d2c5d06.gif)\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0.0-rc0, 2.0.0-dev20190902 and was able to reproduce the issue.Please, find the [gist](https://colab.sandbox.google.com/gist/ravikyram/4eb507f8f61984e08f4b5a02bb955307/untitled146.ipynb) here. Thanks!", "I'm looking at this. @jsimsa please let me know if you are aware of a similar issue.", "Could you please re-run your program logging device placement `https://www.tensorflow.org/beta/guide/using_gpu#logging_device_placement?\r\n\r\nThis will provide information about the placement of each op. My expectation is that the `reduce` dataset op introduced by Autograph will be placed on CPU while the `assign_add` and `matmul` ops will be placed on GPU.", "> Could you please re-run your program logging device placement `https://www.tensorflow.org/beta/guide/using_gpu#logging_device_placement?\r\n> \r\n> This will provide information about the placement of each op. My expectation is that the `reduce` dataset op introduced by Autograph will be placed on CPU while the `assign_add` and `matmul` ops will be placed on GPU.\r\n\r\nHere is full program log including device placement.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/3575806/log.txt)\r\n", "It seems like the log is only showing the matmul op executed eagerly, not the matmul op executed inside the reducedataset op. That matmul seems to be running on the CPU.", "@dimitree54 we came up with a two-part fix (tf.data + tf.autograph). The tf.data  part has been completed (https://github.com/tensorflow/tensorflow/commit/cc2f631baaa9d0d5e189d3c0b5fbfc4df110bf91#diff-b9d6ae3057dedddc9760f15603a3b041) and the tf.autograph part is worked on by @mdanatg .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32138\">No</a>\n", "@jsimsa, @mdanatg Ok, thank you. Is the second part (with autograph) fixed already? In which version of tensorflow 2 this fix will be available?", "Yes, the autograph workaround just landed, you should be able to try it out in the next nightly. It will be available in 2.1.", "In the mean time, the following workaround might work as well: `for sample in iter(dataset)`. Writing it in that way uses different placement rules, for subtle reasons."]}, {"number": 32137, "title": "tensorflow import issue: DLL load failed with error code -1073741795", "body": "ImportError: Traceback (most recent call last):\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: ImportError: Traceback (most recent call last):\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Users\\user\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "I have installed using the above method but not working\nDr. George Mathew\n\nOn Sun, Sep 1, 2019 at 3:32 PM tensorflow-bot[bot] <notifications@github.com>\nwrote:\n\n> From the template it looks like you are installing *TensorFlow* (TF)\n> prebuilt binaries:\n>\n>    - For TF-GPU - See point 1\n>    - For TF-CPU - See point 2\n>\n> ------------------------------\n>\n> *1. Installing TensorFlow-GPU (TF) prebuilt binaries*\n>\n> *TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5)\n> requires CUDA 9.0.*\n>\n>    - If you have above configuration and using *Windows* platform -\n>       - Try adding the CUDA, CUPTI, and cuDNN installation directories to\n>       the %PATH% environment variable.\n>       - Refer windows setup guide\n>       <https://www.tensorflow.org/install/gpu#windows_setup>.\n>    - If you have above configuration and using *Ubuntu/Linux* platform -\n>       - Try adding the CUDA, CUPTI, and cuDNN installation directories to\n>       the $LD_LIBRARY_PATH environment variable.\n>       - Refer linux setup guide\n>       <https://www.tensorflow.org/install/gpu#linux_setup>.\n>    - If error still persists then, apparently your CPU model does not\n>    support AVX instruction sets.\n>       - Refer hardware requirements\n>       <https://www.tensorflow.org/install/pip#hardware-requirements>.\n>\n> ------------------------------\n>\n> *2. Installing TensorFlow (TF) CPU prebuilt binaries*\n>\n> *TensorFlow release binaries version 1.6 and higher are prebuilt with AVX\n> instruction sets.*\n>\n> Therefore on any CPU that does not have these instruction sets, either CPU\n> or GPU version of TF will fail to load.\n> Apparently, your CPU model does not support AVX instruction sets. You can\n> still use TensorFlow with the alternatives given below:\n>\n>    - Try Google Colab to use TensorFlow.\n>       - The easiest way to use TF will be to switch to google colab\n>       <https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true>.You\n>       get pre-installed latest stable TF version. Also you can usepip\n>       install to install any other preferred TF version.\n>       - It has an added advantage since you can you easily switch to\n>       different hardware accelerators (cpu, gpu, tpu) as per the task.\n>       - All you need is a good internet connection and you are all set.\n>    - Try to build TF from sources by changing CPU optimization flags.\n>\n> *Please let us know if this helps.*\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32137?email_source=notifications&email_token=ALSVGJUPTJE4FX5AWYBTTWDQHOHLDA5CNFSM4ISWIEX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5T6XKY#issuecomment-526904235>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALSVGJR7GFYAUTGKS6M3GRTQHOHLDANCNFSM4ISWIEXQ>\n> .\n>\n", "@georgevettathu,Please provide the tensorflow version details. \r\n Does your CPU model support AVX instruction sets? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32137\">No</a>\n"]}, {"number": 32136, "title": "g3doc", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@JunhuaYe ,\r\nCan please fill the information requested in the standard template?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32136\">No</a>\n"]}, {"number": 32135, "title": "Bug in tf.math.lbeta?", "body": "Hi there,\r\n\r\nI noticed that there's a discrepancy between the implementations of the log-beta function in tensorflow and scipy.\r\n\r\nHere's a simple test (pytest) to reproduce this:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef test_lbeta():\r\n    import scipy.special\r\n\r\n    ab = [10, 20]\r\n    x = scipy.special.betaln(*ab).astype('float32')\r\n\r\n    # compare\r\n    ab = tf.constant(ab, dtype='float32')\r\n    with tf.Session() as sess:\r\n        y = sess.run(tf.math.lbeta(ab))\r\n\r\n    np.testing.assert_almost_equal(x, y)  # error: -19.115328 != -19.115334\r\n\r\n```\r\nThe relative error is about 3e-7, which is not negligible. It gets worse with larger `a` and `b`.\r\n\r\nThis can be problematic in situations that rely on cancellation of large numbers, e.g. in computations involving the log-pdf of a Beta distribution.\r\n\r\nIs this expected or is it a bug?\r\n", "comments": ["Please, let us know the TensorFlow version you are using. \r\nLooks like the code is incomplete.Can you please provide full code snippet to reproduce it on our environment.Thanks!", "```\r\n$ python3 -c 'import tensorflow as tf; print(tf.__version__)'\r\n```\r\n> 1.14.0\r\n\r\nIn order to reproduce the error just run `pytest` on the code above or call the `test_lbeta()` function yourself, e.g.\r\n\r\n```\r\n$ python3 -c \"\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef test_lbeta():\r\n    import scipy.special\r\n\r\n    ab = [10, 20]\r\n    x = scipy.special.betaln(*ab).astype('float32')\r\n\r\n    # compare\r\n    ab = tf.constant(ab, dtype='float32')\r\n    with tf.Session() as sess:\r\n        y = sess.run(tf.math.lbeta(ab))\r\n\r\n    np.testing.assert_almost_equal(x, y)\r\n\r\n\r\ntest_lbeta()\r\n\"\r\n```\r\n> AssertionError: \r\n> Arrays are not almost equal to 7 decimals\r\n>  ACTUAL: -19.115328\r\n>  DESIRED: -19.115334\r\n", "Hi @KristianHolsheimer, I think it it due to precision. If you take a deeper look at the scipy's implementation, you can find out the two arguments passed into `betaln` (python API) or `lbeta` (c API) are `double` a.k.a. `float64`.\r\n\r\nSo the computation for `lbeta` in scipy is all done with `double`, meaning the numbers are more precise. While in TensorFlow, if you pass `float` into it, the computation will be done in `float`, which results in less precision for intermediate/output values. So to do a fair test against scipy, we should make sure the numbers passed into TF is `float64`.\r\n\r\nhttps://colab.research.google.com/drive/1WcQV9wSKZkkqVc0zKiBlk-nC9Pmv9QzS\r\nhttps://github.com/scipy/scipy/blob/master/scipy/special/functions.json#L216-L220\r\nhttps://github.com/scipy/scipy/blob/master/scipy/special/_cephes.pxd#L7\r\nhttps://github.com/scipy/scipy/blob/master/scipy/special/cephes/beta.c#L138", "@KristianHolsheimer \r\n\r\nHave a look on @WindQAQ's suggestion and let us know if that helps to resolve the issue. Thanks!", "@WindQAQ You're absolutely right. That was it. Phew.. cheers!!"]}, {"number": 32134, "title": "Issue in the example on .../keras/basic_classification page", "body": "The code below and explanation  pick up from your page https://www.tensorflow.org/tutorials/keras/basic_classification its no actually working. In your case is working for the image in the position 0, but no for the rest due to always is picking up/checking test_labels[i] where i in this case is 0, so always is comparing with test_labels[0] which is '9' (anckle boot).\r\nIf you try with test_image[1] , or test_image[2] , ..... you can check how the chart is no show the correct color (predicted/true label)\r\n\r\n```\r\nimg = test_images[0]\r\n\r\npredictions_single = model.predict(img)\r\n\r\nplot_value_array(0, predictions_single, test_labels)\r\nplt.xticks(range(10), class_names, rotation=45)\r\nplt.show()\r\n```\r\n\r\nI only register this issue in order you can improve your documentation. \r\nThanks for you software.\r\n\r\n", "comments": ["@jjimenezcapi, In the above mentioned tutorial, test_labels[i], here i is the position of image in the dataset. Where, i=0,  ankle boot image and i=1 T-shirt/top image. Is it allowed to choose any of the image from dataset. Thanks!", "Yes I know, but the issue is about the function plot_value_array\r\nIts no possible to call it with another value than 0 for single predictions. It raised and unbound exception . Its only FYI", "@jjimenezcapi Thanks for pointing this issue. Actually it was corrected recently, but the website is not updated. You could check the [source code](https://github.com/tensorflow/docs/blob/r2.0rc/site/en/tutorials/keras/basic_classification.ipynb) and [google colab](https://colab.sandbox.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/tutorials/keras/basic_classification.ipynb#scrollTo=YFc2HbEVCaXd), both were updated and waiting for website to get update. I ran the source code for couple of other examples (other than `Ankle boot` example shown in the TF website) and it works as expected.\r\n\r\nAs you mentioned, In the website it shows\r\n```\r\nplot_value_array(0, predictions_single, test_labels)\r\n```\r\nIn the source codes, it was updated and shows as\r\n\r\n```\r\nplot_value_array(1, predictions_single[0], test_labels)\r\n```\r\n\r\nI am closing this issue as it was resolved. Thanks for your contribution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32134\">No</a>\n"]}, {"number": 32133, "title": "TF 2.0 - Gradient of 'tf.keras.layers.Dense with bias' produces non-deterministic result", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190826\r\n- TensorFlow version (use command below): v1.12.1-9705-g0fbc138 2.0.0-dev20190826\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.0.0/7.3.1\r\n- GPU model and memory: Titan Xp 11Gb\r\n\r\n**Describe the current behavior**\r\n (1) The following code produces the same 'numpy_data0.pkl', 'initial_params0.pkl', 'loss0.pkl' all the times (which means same data, same parameter, same loss), but 'grad0.pkl' changes. I checked it with 'diff' command between generated files. \r\n (2) It seems only with tensorflow 2.0 GPU version, this happens. I checked the code with tf-nightly-2.0-preview==2.0.0.dev20190830 (CPU version), it was ok. (= shows deterministic result)\r\n (3) Using custom dense layer + tf.keras.layers.ReLU() was ok also. (= shows deterministic result) Custom dense layer was\r\n```\r\nclass MyDenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, num_outputs):\r\n        super(MyDenseLayer, self).__init__()\r\n        self.num_outputs = num_outputs\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_variable(\"kernel\", initializer=tf.keras.initializers.GlorotUniform(),\r\n                                        shape=[int(input_shape[-1]),\r\n                                               self.num_outputs])\r\n        self.bias = self.add_variable(\"bias\", initializer=tf.zeros_initializer,\r\n                                        shape=[self.num_outputs])\r\n    def call(self, input):\r\n        return tf.matmul(input, self.kernel) + self.bias\r\n```\r\nAnd net with\r\n```\r\nnet = tf.keras.Sequential()\r\nnet.add(MyDenseLayer(100))\r\nnet.add(tf.keras.layers.ReLU())\r\nnet.add(MyDenseLayer(100))\r\nnet.add(tf.keras.layers.ReLU())\r\nnet.add(MyDenseLayer(1))\r\nnet.build((None, input_dim))\r\n```\r\n (+) When 'use_bias=False' option applied on hidden layers, is was ok. (= shows deterministic result)\r\n\r\n\r\n**Describe the expected behavior**\r\nSince CUDNN force to behave determinisically (os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'), and all the data/parameter/loss are the same, grad is expected to be same.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport pickle\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\r\n\r\nseed = 1234\r\nnp.random.seed(seed)\r\ntf.random.set_seed(seed)\r\nrandom.seed(seed)\r\n\r\n# NN Model\r\ninput_dim = 5\r\nnet = tf.keras.Sequential()\r\nnet.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer=None))\r\nnet.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer=None))\r\nnet.add(tf.keras.layers.Dense(1, activation=None, kernel_initializer=None))\r\nnet.build((None, input_dim))\r\n\r\n# Initial v_params\r\ninitial_v_params = net.variables\r\n\r\n# Update NN Model one-step\r\nx = np.random.normal(loc=0, scale=1., size=[1000, input_dim])\r\ny = np.random.normal(loc=0, scale=1., size=[1000])\r\n\r\nwith tf.GradientTape() as tape:\r\n    loss = tf.reduce_mean(tf.square(y - net(x)))\r\ngrad = tape.gradient(loss, net.trainable_variables)\r\n\r\n# Tag for comparing files\r\ntag = 1\r\n\r\nwith open('./numpy_data{}.pkl'.format(tag), 'wb') as f:\r\n    pickle.dump([x, y], f)\r\n\r\nwith open('./initial_params{}.pkl'.format(tag), 'wb') as f:\r\n    pickle.dump(initial_v_params, f)\r\n\r\nwith open('./loss{}.pkl'.format(tag), 'wb') as f:\r\n    pickle.dump(loss, f)\r\n\r\nwith open('./grad{}.pkl'.format(tag), 'wb') as f:\r\n    pickle.dump(grad, f)\r\n```\r\n", "comments": ["Please find the [gist](https://colab.research.google.com/gist/oanush/be3e950c7fbaa49429a8bbc7addd3d48/32133.ipynb) of colab when tried executing the given code.Thanks!", "@allenlavoie Any idea on how this would happen (for gradients)?", "I don't think this has anything to do with gradient infrastructure, which conceptually is just queuing up some ops. Sounds like some op used in a gradient does not give the same result every time. We don't generally guarantee exact results; if you're using deterministic CuDNN, possibly we're not using CuDNN in some case?\r\n\r\n@iganichev (who works on GPUs) could you decide whether this is a problem, or if epsilon differences are expected here?", "There can be many reasons for non-determinism. As Allen pointed out TF uses many libraries and hand-written kernels besides cuDNN on GPU including Eigen and cuBLAS. For example, in certain convolutions, it is faster to execute them using a GEMM function in cuBLAS. In general, getting TF to behave deterministically is pretty hard. This is a known issue.\r\n\r\nDoes this non-determinism cause a serious issue?", "Closing this based on above comments. Thanks all!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32133\">No</a>\n"]}, {"number": 32132, "title": "[Go]:Create tensor with golang insteading of cgo", "body": "Optimize the speed of encoding features that has string type.\r\n(1) Moving the encoding logic from CGO to golang to reduce the call of CGO\r\n(2) Batch writing offsets when encoding, not one by one.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32132) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32132) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 32131, "title": "Tensorflow tutorial code error", "body": "For tensorflow MNIST Fashion tutorial:\r\n\r\n`def` plot_image(i, predictions_array, true_label, img):\r\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\r\n  plt.grid(False)\r\n  plt.xticks([])\r\n  plt.yticks([])\r\n  \r\n  plt.imshow(img, cmap=plt.cm.binary)\r\n  \r\n  predicted_label = np.argmax(predictions_array)\r\n  if predicted_label == true_label:\r\n    color = 'blue'\r\n  else:\r\n    color = 'red'\r\n  \r\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\r\n                                100*np.max(predictions_array),\r\n                                class_names[true_label]),\r\n                                color=color)\r\n\r\n\r\nthe above code does not show the predicted labels and true labels for the predicted images.\r\nThe issue seems to be in plt.xlabel", "comments": ["Hi, there is no issue with plt.xlabel. \r\n       When I used the defined the same function and ran the code:\r\n\r\n_i = 0\r\nplt.figure(figsize=(6,3))\r\nplt.subplot(1,2,1)\r\nplot_image(i, predictions, test_labels, test_images)\r\nplt.show()_\r\n\r\nIt worked completely fine with me. I got this image with all the labels.\r\n![sneaker_pred](https://user-images.githubusercontent.com/46784336/64132174-79717c80-cdeb-11e9-99f1-6076a8be15c6.png)\r\n\r\n\r\n Please just check it.", "@AdityaShirodkar01, I tried the same tutorial. It worked fine. \r\nPTAL screenshot. Please do check once. Thanks\r\n![Screenshot from 2019-09-03 15-12-18](https://user-images.githubusercontent.com/48476109/64162671-65fef980-ce5d-11e9-814a-c363d007dbd3.png)\r\n", "@gadagashwini \r\nI was referring to def plot_image, and not def plot_value_array\r\n\r\nplt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\r\n                                100*np.max(predictions_array),\r\n                                class_names[true_label]),\r\n                                color=color)\r\n\r\nThe above code did not produce any prediction labels for me\r\nThe predicted label did show up when I replaced it with \r\nplt.xlabel(class_names[predicted_label])", "@AdityaShirodkar01, def plot_image plot the image and def plot_value_array plot the predicted values.\r\nPlease see the screenshot below.\r\n![Screenshot from 2019-09-09 14-20-02](https://user-images.githubusercontent.com/48476109/64517002-17de7000-d30d-11e9-98be-dfb093445084.png)\r\nThanks!", "@AdityaShirodkar01, \r\nClosing this issue, since no recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32131\">No</a>\n", "having issue with plot_value_array. each time i run it i get an error message stating \"plot_value_array is not define\" any help with this?\r\n\r\n![image](https://user-images.githubusercontent.com/50128025/72168844-3e510c80-33ce-11ea-96c3-8aff6b3f0f34.png)\r\n", "**TypeError: only integer scalar arrays can be converted to a scalar index.**\r\nI'm getting this error when I run the same code on different dataset i.e., CIFAR10. Please, anyone help me with this. Thank you.", "<ipython-input-139-521413acf4d5> in <module>\r\n     38 plot_images(i, predictions, test_labels, test_images)\r\n     39 plt.subplot(1,2,2)\r\n---> 40 plot_value_array(i, predictions,test_labels)\r\n     41 plt.show()\r\n\r\n<ipython-input-139-521413acf4d5> in plot_value_array(i, predictions_array, true_label)\r\n     30 \r\n     31     thisplot[predicted_label].set_color('red')\r\n---> 32     thisplot[true_label].set_color('green')\r\n     33 \r\n     34 \r\n\r\nTypeError: only integer scalar arrays can be converted to a scalar index\r\n\r\n\r\n\r\nI am getting this error from my plot i dint know what to do"]}]