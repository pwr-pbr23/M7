[{"number": 50672, "title": "Update errors_impl.py", "body": "Added the Code example for `tf.errors.ResourceExhaustedError`.\r\n\r\nFixes #29847.", "comments": ["To be a doctest, it should have a way to test the output, the example should show some output", "@mihaimaruseac,\r\nAdded the output. "]}, {"number": 50671, "title": "[MLIR][DISC] consider result types of func when checking legality", "body": "Result types of the function should also be taken into consideration\r\nwhen setting the legality.", "comments": []}, {"number": 50670, "title": "Internal: libdevice not found at ./libdevice.10.bc", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS 7.4 \r\n\r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version: 1.15\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip \r\n\r\n- CUDA/cuDNN version:\r\nCudatoolkit 10.0.130, cuDNN 7.6.5.32\r\n\r\n- GPU model and memory:\r\n```\r\n| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:15:00.0 Off |                    0 |\r\n| N/A   37C    P0    53W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n\r\n```\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to run my tensorflow based project on a cluster, I've installed all relevant dependencies within my anaconda environment the exact same way I did on my local machine where the project runs but I'm getting this error message:\r\n\r\n    tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n      (0) Internal: libdevice not found at ./libdevice.10.bc\r\n             [[{{node cluster_2_1/xla_compile}}]]\r\n             [[cluster_1_1/merge_oidx_20/_1]]\r\n      (1) Internal: libdevice not found at ./libdevice.10.bc\r\n             [[{{node cluster_2_1/xla_compile}}]]\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n2021-06-30 08:27:50.484735: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:69] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\r\n    2021-06-30 08:27:50.484775: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Searched for CUDA in the following directories:\r\n    2021-06-30 08:27:50.484781: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   ./cuda_sdk_lib\r\n    2021-06-30 08:27:50.484784: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   /usr/local/cuda\r\n    2021-06-30 08:27:50.484787: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   .\r\n    2021-06-30 08:27:50.484791: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:75] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work. \r\n```\r\n\r\nThis section of the traceback makes me think that tensorflow is searching for cuda locally instead of within the conda environment, to fix this do I need to set the XLA_FLAGS to `/u/usr/anaconda3/envs/Project_BM/lib/libdevice.10.bc`, if not where can I find the `/cuda/` directory within the `Project_BM` environment? \r\n\r\nIts also worth knowing that I'm running this on a cluster so I don't have root permissions.\r\n\r\nFull traceback - https://pastebin.com/njqNFWvC\r\n", "comments": ["@Nason-S \r\n\r\nKindly upgrade to 2.x stable version as there is no active support for 1.x, and let us know if the issue persist.\r\nPlease refer the similar issues [#4452](https://github.com/google/jax/issues/4452) and [#17801](https://github.com/tensorflow/tensorflow/issues/17801).\r\nFor latest test build configurations please refer [this](https://www.tensorflow.org/install/source#gpu)\r\nand as you are building in conda env please refer [link](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/) and let us know if it helps.\r\nThanks\r\n", "Thanks for the links, @martiningram answer found at [#4452](https://github.com/google/jax/issues/4452) was the solution to this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50670\">No</a>\n"]}, {"number": 50669, "title": "Internal Error with TF_GPU_ALLOCATOR=cuda_malloc_async", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop-OS 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.9.5\r\n- CUDA/cuDNN version: CUDA 11.4 / cuDNN 8.2.2\r\n- GPU model and memory: RTX 3080\r\n\r\n**Describe the current behavior**\r\nWhen using the TF_GPU_ALLOCATOR=cuda_malloc_async, TF throws an internal error after allocation of GPU: \r\n```\r\n2021-07-08` 12:44:26.553800: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-08 12:44:27.009583: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-07-08 12:44:27.034925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.035193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:2d:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.76GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-07-08 12:44:27.035207: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-08 12:44:27.036831: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-07-08 12:44:27.036855: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-07-08 12:44:27.037745: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-07-08 12:44:27.037863: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-07-08 12:44:27.038095: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n2021-07-08 12:44:27.038451: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-07-08 12:44:27.038515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-07-08 12:44:27.038573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.038841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.039405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-07-08 12:44:27.039873: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-08 12:44:27.040284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.040520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:2d:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.76GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-07-08 12:44:27.040556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.040800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.041145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-07-08 12:44:27.041164: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-08 12:44:27.296173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-07-08 12:44:27.296199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-07-08 12:44:27.296206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-07-08 12:44:27.296339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.296598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.296835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-08 12:44:27.297045: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.\r\n2021-07-08 12:44:27.297081: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\nTraceback (most recent call last):\r\n  File \"/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/./training.py\", line 424, in <module>\r\n    log_writer = tf.summary.create_file_writer(logdir) if log else None\r\n  File \"/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 479, in create_file_writer_v2\r\n    with ops.name_scope(name, \"create_file_writer\") as scope, ops.device(\"cpu:0\"):\r\n  File \"/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 5255, in device\r\n    return context.device(device_name_or_function)\r\n  File \"/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 2072, in device\r\n    ensure_initialized()\r\n  File \"/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 1867, in ensure_initialized\r\n    context().ensure_initialized()\r\n  File \"/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 525, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: No allocator statistics\r\n```", "comments": ["@sebltm ,\r\n\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).In this case, can you please try installing TensorFlow v2.5 with CUDA 11.2 and cuDNN 8.1 and check if you are facing the same error. Thanks!", "Works with TF 2.5 / CUDA 11.2 / cuDNN 8.1, however this only half solves my problem, since my reason for trying another version of CUDA / cuDNN is that I was getting frequent segmentation faults with this combination, which disappeared after migrating to CUDA 11.4 / cuDNN 8.2.2", "CC @nouiz \r\n\r\n> since my reason for trying another version of CUDA / cuDNN is that I was getting frequent segmentation faults with this combination\r\n\r\nIs there a GH issue about these segfaults?", "The \"No allocator statistics\" was fixed about a mount ago (#49173), but it got reverted and i just saw this today.\r\nI'll take a look at this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50669\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50669\">No</a>\n", "The fix was merged a few hours ago.\r\nCan you wait 24h and try TF nightly build to be sure that it also works for you?\r\nIf you have any comments on that new features, please share with us."]}, {"number": 50668, "title": "Can't use preprocess_input with mixed precision and Lambda layer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac and Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI recently upgraded tensorflow from 2.4 to 2.5 and I have a break in my code that didn't happen before. It is related to the use of mixed precision and preprocessing functions in tf.keras.applications. When using both with a Lambda layer:\r\n```python\r\ninput = tf.keras.Input((224, 224, 3), name=\"image\")\r\ntf.keras.layers.Lambda(tf.keras.applications.densenet.preprocess_input, name=\"x_preprocess\")(input)\r\n```\r\n, I have the following error:\r\n```bash\r\nTypeError: x and y must have the same dtype, got tf.float16 != tf.float32\r\n```\r\nYou can find a minimal example [here](https://colab.research.google.com/drive/1BrYCu4Bo7Y0D7_MaLbDlFW1_uHzaRuGE?usp=sharing)\r\n\r\nThe issue isn't raised if I don't use Lambda:\r\n```python\r\ninput = tf.keras.Input((224, 224, 3), name=\"image\")\r\ntf.keras.applications.densenet.preprocess_input(input)\r\n```\r\nI can use this solution, but the Lambda layer was useful to group all the preprocessing operations into a single layer.\r\n\r\n**Describe the expected behavior**\r\n\r\nBefore TF 2.5, this worked, so I assume this should still work\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1BrYCu4Bo7Y0D7_MaLbDlFW1_uHzaRuGE?usp=sharing\r\n", "comments": ["@anth2o It looks like your Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. As ([previously announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/)), all future development of Keras is expected to happen in the [github.com/keras-team/keras](github.com/keras-team/keras) repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!", "Thanks, since this bug seems easy to fix, I directly opened [a PR in the keras repo](https://github.com/keras-team/keras/pull/14917)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50668\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50668\">No</a>\n"]}, {"number": 50667, "title": "Converting Boosted tree model to tflite", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os Big Sur\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n\r\n### 2. Code\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model('edr_cust_model_new/1625736127/') # path to the SavedModel directory\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\ntflite_model = converter.convert()\r\n\r\n\r\n// Save the model.\r\nwith open('edr_cust_model_temp/model.tflite', 'wb') as f:\r\n  f.write(tflite_model)`\r\n\r\n### 3. Short Error log\r\nI am getting following error while conversion:\r\n\r\nConverterError: <unknown>:0: error: loc(\"boosted_trees\"): 'tf.BoostedTreesEnsembleResourceHandleOp' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"boosted_trees/BoostedTreesPredict\"): 'tf.BoostedTreesPredict' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"boosted_trees/head/predictions/str_classes\"): 'tf.AsString' op is neither a custom op nor a flex op\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.AsString {device = \"\", fill = \"\", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}\r\n\ttf.BoostedTreesEnsembleResourceHandleOp {container = \"\", device = \"\", shared_name = \"boosted_trees/\"}\r\n\ttf.BoostedTreesPredict {device = \"\", logits_dimension = 7 : i64, num_bucketized_features = 18 : i64}\r\n\r\n\r\n### 4. For detailed error, please refer to this file \r\n[tf_boosted_tree_convert_error_log.txt](https://github.com/tensorflow/tensorflow/files/6783426/tf_boosted_tree_convert_error_log.txt)\r\n", "comments": ["It looks like the ops are not supported in TensorFlow as well. You need to follow the steps as given [here](https://www.tensorflow.org/lite/convert#other_features) in `2. Unsupported in TensorFlow`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50667\">No</a>\n", "Hi Meghna, Thanks for the reply \r\n\r\ni followed the link, and tried to use register_custom_opdefs : But i am getting this \r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-81-1ca3274a817a> in <module>\r\n      4 \r\n      5   # Register custom opdefs before the invocation of converter API.\r\n----> 6 tf.lite.python.convert.register_custom_opdefs([custom_opdef])\r\n\r\nAttributeError: module 'tensorflow._api.v2.lite' has no attribute 'python'\r\n\r\n\r\nCan you please let me know  if there is anything i can do?\r\n\r\n\r\nThanks,\r\nKoushik", "@Koushik667 can you try running the following code?\r\n(This usually wouldn't work as your ops are not defined in TF, but let's try)\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('edr_cust_model_new/1625736127/') # path to the SavedModel directory\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\n```\r\n@abattery (tagging you in case you have more context) If there is an op that is unsupported in TF, how can we convert the model successfully. The API used [here](https://www.tensorflow.org/lite/convert#other_features) (` tf.lite.python.convert.register_custom_opdefs([custom_opdef])`) is not available to external users. It also looks like we have deprecated the older `converter.custom_opdefs=...` usage.", "Adding them into the allowlist in the Select TF op support will be the easiest way to support. I think the above Boost tree ops are not included in the allowlist but we have AsString op as Select TF ops. I think we can consider this as a feature request for adding the additions of Boost tree ops in the allowlist of the Select TF ops.\r\n\r\nPlease consider using https://www.tensorflow.org/lite/guide/ops_select ", "Thanks a lot @abattery! \r\n\r\n@Koushik667 In TFLite, we maintain a list of TF operators (called the [allowlist](https://www.tensorflow.org/lite/guide/op_select_allowlist#tensorflow_core_operators)) that don't have a corresponding TFLite operator implementation (eg: such as the BoostedTreesEnsembleResourceHandleOp). If a TF operator is in this allowlist, then we can convert the model by pulling in the TF op into a TFLite model by adding the constant `tf.lite.OpsSet.SELECT_TF_OPS`. (Note: this will lead to an increased model size) as follows:\r\n```\r\nconverter.target_spec.supported_ops = [\r\ntf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\ntf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n```\r\n\r\nIn your model, the operators used (eg: such as the BoostedTreesEnsembleResourceHandleOp) are not in this allowlist. Hence, though you set the above flag, it didn't work. To make this work, you need to follow the instructions in [Update TFLite Ops Allowlist for TF Ops](https://www.tensorflow.org/lite/guide/op_select_allowlist#add_tensorflow_core_operators_to_the_allowed_list) to add these operators to the allowlist. Once you complete this, your initial code should work as expected:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('edr_cust_model_new/1625736127/') # path to the SavedModel directory\r\nconverter.target_spec.supported_ops = [\r\ntf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\ntf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\ntflite_model = converter.convert()\r\n\r\n// Save the model.\r\nwith open('edr_cust_model_temp/model.tflite', 'wb') as f:\r\nf.write(tflite_model)\r\n```\r\n\r\n*Note: once the update is made, you still need to follow instructions [here](https://www.tensorflow.org/lite/guide/ops_select#run_inference) when running inference to use these TF operators*", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50667\">No</a>\n"]}, {"number": 50666, "title": "Quadratic runtime behavior in model.fit/predict with tf.keras.layers.experimental.preprocessing.TextVectorization", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nO(n^2) runtime behavior when calling model.fit or model.predict with a single TextVectorization layer. E.g. calling predict 2 times with n/2 vectors is twice as fast compared to calling predict one time with n vectors. For model.fit the batch_size seems to reduce the runtime linearly. Runtime seems to be O(n * number_of_iterations).\r\n\r\n**Describe the expected behavior**\r\nThe runtime should be linear with n (O(n))\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\n\r\n# generate some dummy data\r\nn = 1000000\r\nx = np.random.randint(1, 100, (n, 1)).astype(str)\r\ny = np.random.uniform(0, 1, (n, 1))\r\n\r\ninput = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\r\nvectorization = tf.keras.layers.experimental.preprocessing.TextVectorization()\r\nvectorization.adapt(np.unique(x))\r\noutput = vectorization(input)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\r\n\r\n# compare runtime of model.fit with different batch_size\r\n# -> for large n, doubling the batch_size halves the execution time!!!\r\nmodel.fit(x, y, epochs=1,batch_size=32)\r\nmodel.fit(x, y, epochs=1,batch_size=64)\r\nmodel.fit(x, y, epochs=1,batch_size=8192)\r\n\r\n# compare runtime of model.predict with different input size\r\n# -> for large n, calling predict 2 times with n/2 is twice as fast!!!\r\npred = model.predict(x,verbose=1)\r\npred = model.predict(x[:int(n/2)],verbose=1)\r\npred = model.predict(x[int(n/2+1):],verbose=1)\r\n```\r\n\r\n**Other info / logs** \r\nObserved runtimes. Absolute values may depend on your hardware. Ratios between runtimes should be reproducible.\r\n```\r\n31250/31250 [==============================] - 207s 7ms/step - loss: 3416.8721 (model.fit with batch_size = 32)\r\n15625/15625 [==============================] - 121s 8ms/step - loss: 3416.8367 (model.fit with batch_size = 64)\r\n123/123 [==============================] - 1s 10ms/step - loss: 3416.8489 (model.fit with batch_size = 8192)\r\n31250/31250 [==============================] - 221s 7ms/step (model.predict on n vectors)\r\n15625/15625 [==============================] - 56s 4ms/step (model.predict on n/2 vectors)\r\n15625/15625 [==============================] - 59s 4ms/step (model.predict on n/2 vectors)\r\n\r\n```", "comments": ["@aucth ,\r\n\r\nI ran the code shared and face a different error, please find the [gist](https://colab.research.google.com/gist/tilakrayal/78dd094a8f723caec480dadce6e21e8e/untitled50666.ipynb) here and share all dependencies to replicate the issue or share a colab gist with the reported error.Thanks!", "@tilakrayal \r\nSorry, there was a typo in my code example. I updated my code example above. You can also find the gist [here](https://colab.research.google.com/gist/aucth/9a020572f3427ad3e0f0974be2ae4b2e/untitled50666.ipynb).", "@aucth ,\r\n\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)", "[Link](https://github.com/keras-team/keras/issues/14915) to the issue on keras.", "@aucth ,\r\n\r\nCan you please feel free to close this issue, since it is already being tracked there? Thanks!"]}, {"number": 50665, "title": "Can tf.function fully support control flow", "body": "Hi\uff0c In eager mode, control flow is fully supported;\r\nhowever, in graph mode, Does tf2.x fully support control flow. if so,  we write whether the control flow can be expressed in python statements, or tf2.x specific API \u3002\r\n\r\n", "comments": ["@liangzelang \r\nPlease check this [documentation](https://www.tensorflow.org/guide/function) it answers all your questions, kindly open a new issue in tensorflow discussion in case of any further queries as this is not a bug or a feat request.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50664, "title": "r2.6 cherry-pick request: tflite: Update CMake 3rd party libraries hash", "body": "PiperOrigin-RevId: 383560588\r\nChange-Id: I6bb5121d844a3e18b6f404947b4b77e02846bd1d", "comments": []}, {"number": 50663, "title": "[Performance] Sparse Operation is tf.keras is super slow", "body": "I am training a network using sparse operation.  The input matrix is large (4656 columns), the output (following) matrix is a dense one with 2328 outputs. Average density of the input was just 0.5%\r\n\r\nDespite any methods works on GPU (Quadro P1000), either the input was scipy.COO, scipy.CSR or tf.SparseTensor, the performance was dropped by half compared to dense input , even I have activated the sparse=True in the input layer.\r\n\r\nTiming: \r\nInput(sparse=False) -> ~ 20s\r\nInput(sparse=True) -> ~ 44s\r\n\r\nInput Matrix: [50000, 4656] -> Output: [50000, 2328]\r\n\r\nTF Version: 2.5\r\nCudaToolkit: 11.2\r\ncuDNN: 8.2", "comments": ["@IchiruTake \r\n\r\nWe see that issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Aperformance&template=80-performance-issue.md) has not been filled. In order to expedite the trouble-shooting process, please provide a code snippet/colab gist to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50661, "title": "Calculation of gradient of 2D convolution operation through GradientTape returns None", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20.1\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.9.4\r\n- CUDA/cuDNN version: 10.1.243, 7.6.5 (both through conda)\r\n- GPU model and memory: GeForce RTX 2060 Rev. A, 6GB\r\n\r\n**Describe the current behavior**\r\nCurrently, I am trying to use `GradientTape` to capture the gradient of a 2D convolution operation `tf.nn.conv2d` on a test matrix. However, when I go to actually fetch the gradients, `tape.gradient()` returns `None` instead of the expected gradient. I tested the exact same code, except I disabled eager execution and didn't use `GradientTape`, and it returned the gradients just fine.\r\n\r\n**Describe the expected behavior**\r\nWhen using `tf.nn.conv2d` inside of `GradientTape`, TensorFlow should successfully calculate the gradients.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis code works:\r\n```\r\nimport os\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# sizes, fixed strides, in_channel, out_channel be 1 for now\r\nx_size = 4\r\nw_size = 3  # use an odd number here\r\nx_shape = (1, x_size, x_size, 1)\r\nw_shape = (w_size, w_size, 1, 1)\r\nout_shape = (1, x_size - w_size + 1, x_size - w_size + 1, 1)\r\nstrides = (1, 1, 1, 1)\r\n\r\n# numpy value\r\nx_np = np.random.randint(10, size=x_shape)\r\nw_np = np.random.randint(10, size=w_shape)\r\nout_scale_np = np.random.randint(10, size=out_shape)\r\n\r\n# tf forward\r\nx = tf.constant(x_np, dtype=tf.float32)\r\nw = tf.constant(w_np, dtype=tf.float32)\r\nout = tf.nn.conv2d(input=x, filters=w, strides=strides, padding='VALID')\r\nout_scale = tf.constant(out_scale_np, dtype=tf.float32)\r\nf = tf.reduce_sum(tf.multiply(out, out_scale))\r\n\r\n# tf backward\r\nd_out = tf.gradients(f, out)[0]\r\nd_x = tf.gradients(f, x)[0]\r\nd_w = tf.gradients(f, w)[0]\r\n\r\nprint(d_out, d_x, d_w)\r\n```\r\n\r\nThis code does not:\r\n```\r\nimport os\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# sizes, fixed strides, in_channel, out_channel be 1 for now\r\nwith tf.GradientTape() as g:\r\n    x_size = 4\r\n    w_size = 3  # use an odd number here\r\n    x_shape = (1, x_size, x_size, 1)\r\n    w_shape = (w_size, w_size, 1, 1)\r\n    out_shape = (1, x_size - w_size + 1, x_size - w_size + 1, 1)\r\n    strides = (1, 1, 1, 1)\r\n\r\n    # numpy value\r\n    x_np = np.random.randint(10, size=x_shape)\r\n    w_np = np.random.randint(10, size=w_shape)\r\n    out_scale_np = np.random.randint(10, size=out_shape)\r\n\r\n    # tf forward\r\n    x = tf.constant(x_np, dtype=tf.float32)\r\n    w = tf.constant(w_np, dtype=tf.float32)\r\n    out = tf.nn.conv2d(input=x, filters=w, strides=strides, padding='VALID')\r\n    out_scale = tf.constant(out_scale_np, dtype=tf.float32)\r\n    f = tf.reduce_sum(tf.multiply(out, out_scale))\r\n\r\n# tf backward\r\nd_out = g.gradient(f, out)[0]\r\nd_x = g.gradient(f, x)[0]\r\nd_w = g.gradient(f, w)[0]\r\n\r\nprint(d_out, d_x, d_w)\r\n```", "comments": ["Was  able to reproduce the issue on colab with TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/b5134f90248252be82c92792ba891a2a/untitled.ipynb). Thanks!", "'GradientTape' is different from `tf.gradients`.\r\nThe problem in the second code snippet is that your tensors are not being \"watched\" by `GradientTape`.\r\n`tf.Variable`s are automatically watched by `GradientTape`, but tensors are not. You must \"watch\" them, or use `tf.Variable`s instead.\r\nInformation can be found in this [guide](https://www.tensorflow.org/guide/eager#advanced_automatic_differentiation_topics).\r\n\r\nAnd a note: use `persistent=True` to be able to call `g.gradient` more than one time, otherwise it will throw an error. See the code below.\r\n\r\nSo you have two options to edit your code with `GradientTape`:\r\noption 1: watch your tensors.\r\n\r\n\r\n    import os\r\n    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n    import numpy as np\r\n    import tensorflow as tf\r\n\r\n    # sizes, fixed strides, in_channel, out_channel be 1 for now\r\n    with tf.GradientTape(persistent=True) as g: # use persistent=True to be able to call g.gradient more than once\r\n        x_size = 4\r\n        w_size = 3  # use an odd number here\r\n        x_shape = (1, x_size, x_size, 1)\r\n        w_shape = (w_size, w_size, 1, 1)\r\n        out_shape = (1, x_size - w_size + 1, x_size - w_size + 1, 1)\r\n        strides = (1, 1, 1, 1)\r\n\r\n        # numpy value\r\n        x_np = np.random.randint(10, size=x_shape)\r\n        w_np = np.random.randint(10, size=w_shape)\r\n        out_scale_np = np.random.randint(10, size=out_shape)\r\n\r\n        # tf forward\r\n        x = tf.constant(x_np, dtype=tf.float32)\r\n        w = tf.constant(w_np, dtype=tf.float32)\r\n\r\n        #########################################################\r\n        #########################################################\r\n        # watch your tensors of interest\r\n        g.watch(x)\r\n        g.watch(w)\r\n        #########################################################\r\n        #########################################################\r\n\r\n        out = tf.nn.conv2d(input=x, filters=w, strides=strides, padding='VALID')\r\n        out_scale = tf.constant(out_scale_np, dtype=tf.float32)\r\n        f = tf.reduce_sum(tf.multiply(out, out_scale))\r\n\r\n    # tf backward\r\n    d_out = g.gradient(f, out)[0]\r\n    d_x = g.gradient(f, x)[0]\r\n    d_w = g.gradient(f, w)[0]\r\n\r\n    print(d_out, d_x, d_w)\r\n\r\n\r\noption 2: use `tf.Variable`s.\r\n\r\n\r\n    import os\r\n    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\r\n    import numpy as np\r\n    import tensorflow as tf\r\n\r\n    # sizes, fixed strides, in_channel, out_channel be 1 for now\r\n    with tf.GradientTape(persistent=True) as g: # use persistent=True to be able to call g.gradient more than once\r\n        x_size = 4\r\n        w_size = 3  # use an odd number here\r\n        x_shape = (1, x_size, x_size, 1)\r\n        w_shape = (w_size, w_size, 1, 1)\r\n        out_shape = (1, x_size - w_size + 1, x_size - w_size + 1, 1)\r\n        strides = (1, 1, 1, 1)\r\n\r\n        # numpy value\r\n        x_np = np.random.randint(10, size=x_shape)\r\n        w_np = np.random.randint(10, size=w_shape)\r\n        out_scale_np = np.random.randint(10, size=out_shape)\r\n\r\n        ################### EDIT ###################\r\n        # tf forward\r\n        x = tf.Variable(x_np, dtype=tf.float32)\r\n        w = tf.Variable(w_np, dtype=tf.float32)\r\n        ################### EDIT ###################\r\n\r\n        out = tf.nn.conv2d(input=x, filters=w, strides=strides, padding='VALID')\r\n        out_scale = tf.constant(out_scale_np, dtype=tf.float32)\r\n        f = tf.reduce_sum(tf.multiply(out, out_scale))\r\n\r\n    # tf backward\r\n    d_out = g.gradient(f, out)[0]\r\n    d_x = g.gradient(f, x)[0]\r\n    d_w = g.gradient(f, w)[0]\r\n\r\n    print(d_out, d_x, d_w)\r\n\r\n\r\n`Warning`: when computing gradients using `tf.GradientTape().gradient` with respect to one variable only, the gradient is returned as a tensor and not as a list of a single tensor; so in this code you should write the final three lines without indexing, like this:\r\n\r\n    # tf backward\r\n    d_out = g.gradient(f, out)\r\n    d_x = g.gradient(f, x)\r\n    d_w = g.gradient(f, w)\r\n\r\n", "@kindalime \r\nYou may also [refer](https://stackoverflow.com/questions/53953099/what-is-the-purpose-of-the-tensorflow-gradient-tape) to these links for better understanding,[this guide](https://www.tensorflow.org/guide/autodiff) would clear all doubts.\r\nFor any further queries please open this issue in tensorlfow discussion forum and move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50661\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50661\">No</a>\n"]}, {"number": 50660, "title": "Fail to converted Tensorflow models to TensorFlow Lite", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (Linux Ubuntu 20.04):\r\n- TensorFlow installation (pip package):\r\n- TensorFlow library (tensorflow2.5):\r\n\r\n### 2. Code\r\nModel just use DNNLinearCombinedClassifier, when I converter .pb to tflite, it failed.\r\n\r\nModel: https://drive.google.com/file/d/18FTOAx_HJlbnX0HQYqOwzvXdAm8dzyCL/view?usp=sharing\r\n#### Way1\r\n```\r\nsaved_model_obj = tf.saved_model.load(export_dir=model_path)\r\n\r\n# Load the specific concrete function from the SavedModel.\r\nconcrete_func = saved_model_obj.signatures['serving_default']\r\n# Set the shape of the input in the concrete function.\r\nconcrete_func.inputs[0].set_shape((None,))\r\n\r\n# Convert the model to a TFLite model.\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nwith open('model.tflite','wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n**Error:** \r\nE        ValueError: Failed to import metagraph, check error log for more info.\r\n\r\n#### Way2\r\n```\r\nmodel = tf.saved_model.load(model_path)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=model_dir,signature_keys=['serving_default','predict','classification'])\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite','wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n**Error:**      \r\n>       raise ValueError(\"Only support a single signature key.\")\r\nE       ValueError: Only support a single signature key.\r\n", "comments": ["For the multiple signatures, the feature is not completed yet. Please consider providing the 'serving_default' only in the signature_keys argument.", "> For the multiple signatures, the feature is not completed yet. Please consider providing the 'serving_default' only in the signature_keys argument.\r\n\r\nWhen I set the signature_keys to \u2018serving_default\u2019 , get this error:\r\n```\r\nloc(\"map/TensorArrayV2_1\"): error: 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass\r\nloc(\"map/TensorArrayV2_1\"): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n\r\n0706.py:79 (test05)\r\nmodel_flags_str = b'\\xa2\\x013/home/zhangyj/github/guesslang/guesslang/data/model\\xa8\\x01\\x01\\xb2\\x01\\x05serve\\xba\\x01\\x0fserving_default'\r\ntoco_flags_str = b'\\x08\\x01\\x10\\x02 \\x01@\\x00P\\x00X\\x01`\\x01\\xc8\\x01\\x00\\xd0\\x01\\x00\\xe8\\x01\\x00\\x90\\x02\\x00'\r\ninput_data_str = None, debug_info_str = None, enable_mlir_converter = True\r\n\r\n    def toco_convert_protos(model_flags_str,\r\n                            toco_flags_str,\r\n                            input_data_str,\r\n                            debug_info_str=None,\r\n                            enable_mlir_converter=False):\r\n      \"\"\"Convert `input_data_str` according to model and toco parameters.\r\n    \r\n      Unless you know what you are doing consider using\r\n      the more friendly `tf.compat.v1.lite.toco_convert`.\r\n    \r\n      Args:\r\n        model_flags_str: Serialized proto describing model properties, see\r\n          `toco/model_flags.proto`.\r\n        toco_flags_str: Serialized proto describing conversion properties, see\r\n          `toco/toco_flags.proto`.\r\n        input_data_str: Input data in serialized form (e.g. a graphdef is common)\r\n        debug_info_str: Serialized `GraphDebugInfo` proto describing logging\r\n          information. (default None)\r\n        enable_mlir_converter: Enables MLIR-based conversion instead of the default\r\n          TOCO conversion. (default False)\r\n    \r\n      Returns:\r\n        Converted model in serialized form (e.g. a TFLITE model is common).\r\n      Raises:\r\n        ConverterError: When conversion fails in TFLiteConverter, usually due to\r\n          ops not being supported.\r\n        RuntimeError: When conversion fails, an exception is raised with the error\r\n          message embedded.\r\n      \"\"\"\r\n      # Historically, TOCO conversion failures would trigger a crash, so we would\r\n      # attempt to run the converter out-of-process. The MLIR conversion pipeline\r\n      # surfaces errors instead, and can be safely run in-process.\r\n      if enable_mlir_converter or not _toco_from_proto_bin:\r\n        try:\r\n>         model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n                                                     toco_flags_str, input_data_str,\r\n                                                     debug_info_str,\r\n                                                     enable_mlir_converter)\r\n\r\n../../soft/miniconda3/envs/tf25/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:291: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nmodel_flags_str = b'\\xa2\\x013/home/zhangyj/github/guesslang/guesslang/data/model\\xa8\\x01\\x01\\xb2\\x01\\x05serve\\xba\\x01\\x0fserving_default'\r\ntoco_flags_str = b'\\x08\\x01\\x10\\x02 \\x01@\\x00P\\x00X\\x01`\\x01\\xc8\\x01\\x00\\xd0\\x01\\x00\\xe8\\x01\\x00\\x90\\x02\\x00'\r\ninput_data_str = None, debug_info_str = None, enable_mlir_converter = True\r\n\r\n    def wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str,\r\n                             debug_info_str, enable_mlir_converter):\r\n      \"\"\"Wraps TocoConvert with lazy loader.\"\"\"\r\n>     return _pywrap_toco_api.TocoConvert(\r\n          model_flags_str,\r\n          toco_flags_str,\r\n          input_data_str,\r\n          False,  # extended_return\r\n          debug_info_str,\r\n          enable_mlir_converter)\r\nE     Exception: <unknown>:0: error: loc(\"map/TensorArrayV2_1\"): 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass\r\nE     <unknown>:0: error: loc(\"map/TensorArrayV2_1\"): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n\r\n../../soft/miniconda3/envs/tf25/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py:32: Exception\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    def test05():\r\n        model_dir = '/home/zhangyj/github/guesslang/guesslang/data/model'\r\n        #\r\n        # imported = tf.saved_model.load(model_dir)\r\n        # print(imported.signatures)\r\n    \r\n        model = tf.saved_model.load(model_dir)\r\n        converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=model_dir,signature_keys=['serving_default'])\r\n        # print(converter.target_spec.supported_ops)\r\n        # converter.target_spec.supported_ops = [\r\n        #     tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n        #     tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n        # ]\r\n>       tflite_model = converter.convert()\r\n\r\n0706.py:93: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n../../soft/miniconda3/envs/tf25/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:913: in convert\r\n    result = _convert_saved_model(**converter_kwargs)\r\n../../soft/miniconda3/envs/tf25/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:722: in convert_saved_model\r\n    data = toco_convert_protos(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nmodel_flags_str = b'\\xa2\\x013/home/zhangyj/github/guesslang/guesslang/data/model\\xa8\\x01\\x01\\xb2\\x01\\x05serve\\xba\\x01\\x0fserving_default'\r\ntoco_flags_str = b'\\x08\\x01\\x10\\x02 \\x01@\\x00P\\x00X\\x01`\\x01\\xc8\\x01\\x00\\xd0\\x01\\x00\\xe8\\x01\\x00\\x90\\x02\\x00'\r\ninput_data_str = None, debug_info_str = None, enable_mlir_converter = True\r\n\r\n    def toco_convert_protos(model_flags_str,\r\n                            toco_flags_str,\r\n                            input_data_str,\r\n                            debug_info_str=None,\r\n                            enable_mlir_converter=False):\r\n      \"\"\"Convert `input_data_str` according to model and toco parameters.\r\n    \r\n      Unless you know what you are doing consider using\r\n      the more friendly `tf.compat.v1.lite.toco_convert`.\r\n    \r\n      Args:\r\n        model_flags_str: Serialized proto describing model properties, see\r\n          `toco/model_flags.proto`.\r\n        toco_flags_str: Serialized proto describing conversion properties, see\r\n          `toco/toco_flags.proto`.\r\n        input_data_str: Input data in serialized form (e.g. a graphdef is common)\r\n        debug_info_str: Serialized `GraphDebugInfo` proto describing logging\r\n          information. (default None)\r\n        enable_mlir_converter: Enables MLIR-based conversion instead of the default\r\n          TOCO conversion. (default False)\r\n    \r\n      Returns:\r\n        Converted model in serialized form (e.g. a TFLITE model is common).\r\n      Raises:\r\n        ConverterError: When conversion fails in TFLiteConverter, usually due to\r\n          ops not being supported.\r\n        RuntimeError: When conversion fails, an exception is raised with the error\r\n          message embedded.\r\n      \"\"\"\r\n      # Historically, TOCO conversion failures would trigger a crash, so we would\r\n      # attempt to run the converter out-of-process. The MLIR conversion pipeline\r\n      # surfaces errors instead, and can be safely run in-process.\r\n      if enable_mlir_converter or not _toco_from_proto_bin:\r\n        try:\r\n          model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n                                                     toco_flags_str, input_data_str,\r\n                                                     debug_info_str,\r\n                                                     enable_mlir_converter)\r\n          return model_str\r\n        except Exception as e:\r\n>         raise ConverterError(str(e))\r\nE         tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"map/TensorArrayV2_1\"): 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass\r\nE         <unknown>:0: error: loc(\"map/TensorArrayV2_1\"): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n\r\n../../soft/miniconda3/envs/tf25/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:297: ConverterError\r\n```", "Please consider using the Select TF option. https://www.tensorflow.org/lite/guide/ops_select\r\n\r\nDynamic dimensional TensorArray is currently supported through the Select TF option only.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50659, "title": "AttributeError: '_UserObject' object has no attribute 'add_slot'", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.4\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): built on tf 2.5.0 converted on tf_nightly 2.7.0-dev20210707\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n```\r\n(You can paste links or attach files by dragging & dropping them below)\r\n- Include code to invoke the TFLite Converter Python API and the errors.\r\n- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.float16]\r\n    tflite_quant_model = converter.convert()\r\n\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/quantization_convert_test.py\", line 12, in <module>\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 1353, in from_saved_model\r\n    saved_model = _load(saved_model_dir, tags)\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 863, in load\r\n    result = load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 902, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 162, in __init__\r\n    self._load_all()\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 259, in _load_all\r\n    self._load_nodes()\r\n  File \"/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 448, in _load_nodes\r\n    slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n", "comments": ["The above error is coming from the TF saved model library while loading the saved model with the user objects.", "Thanks for pointing that out, it was part of the converting process so I thought I would just post it here, but I'll do it differently in the future.", "Can you please share a reproducible code to validate the issue? Thanks!", "Sure\r\n\r\nHere is the link:\r\n[collab](https://colab.research.google.com/drive/1KwygD93TKndmIgZTQQVQzhzwN2nbAOzB?usp=sharing)\r\n\r\nHere is the saved model directory:\r\n[model_full_real.zip](https://github.com/tensorflow/tensorflow/files/6793257/model_full_real.zip)", "I successfully converted your model into tflite by setting `custom_ops = True`. I have used TF 2.5 for conversion.\r\nhttps://www.tensorflow.org/lite/guide/ops_custom#convert_to_a_tensorflow_lite_model\r\nSee [gist](https://colab.research.google.com/gist/ymodak/fa1aec30c936a85ed357f0c612676e42/add_slot_error_reproduction.ipynb)", "@ymodak , it would be better to suggest using TF select op instead of custom ops since the custom ops require user implemented TFLite ops.", "Thanks for the pointer. I see that the conversion fails without `custom_ops` and using TF select ops.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/9e26b08a1ab1bd80712aadc88626519d/add_slot_error_reproduction.ipynb)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50659\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50659\">No</a>\n", "@RedJyve \r\nI had the same issue. I loaded the model in the same notebook that I was converting the samed_model to tflite and it solved the issue. I assume TensorFlow recognizes the used objects in RAM and it prevents further errors."]}, {"number": 50657, "title": "[oneDNN] Upgrade onednn to official version v2.3", "body": "MR to change the OneDNN Version to the Official v2.3 version: https://github.com/oneapi-src/oneDNN/releases/tag/v2.3", "comments": []}, {"number": 50655, "title": "[CHerryPick 2.6]Internal change", "body": "PiperOrigin-RevId: 382139944\nChange-Id: I21d4aeda4f40d900083da37fe57831ff7e549bad", "comments": []}, {"number": 50653, "title": "exporter_main_v2.py on official TF2 OD checkpoints produces saved_model.pb different than official saved_model.pb", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CUDA 11.4\r\n- GPU model and memory: GeForce RTX 3090\r\n\r\nI am trying to convert a re-trained TF2 object detection SSD MobilenetV2 model to a proprietary framework. I have successfully re-trained the network and it runs properly. However, I am having trouble with converting the saved_model.pb to the other framework. The conversion script from the SDK I am working with performs optimization on the saved_model.pb, using 'meta_optimizer.cc', which returns an empty graph after running through my re-trained model. I used '_exporter_main_v2.py_' to export my re-trained checkpoint to the saved_model.pb which I am having trouble with. \r\n\r\nThe issue is not with my training or checkpoints, but with the exporting process from checkpoint to a saved_model.pb using '_exporter_main_v2.py_'. I know this because I downloaded the SSD MobilenetV2 model from the TF2 Zoo to test with it. I have no issue converting the official saved_model.pb file found in the official repo, but when I try to convert the official checkpoints found in the repo to a saved_model.pb using '_exporter_main_v2.py_', I face the same issue trying to convert the newly produced saved_model.pb file to the proprietary framework. This means that something wrong is happening when executing the 'exporter_main_v2.py' script.\r\n\r\n**Describe the expected behavior**\r\nThe exported saved_model.pb file should not be different than the official saved_model.pb file found in the official repo. \r\n\r\nThe following is what I get, showing 0 nodes and 0 edges\r\n![grappler_empty_graph](https://user-images.githubusercontent.com/84783887/124790681-5c42f300-df19-11eb-9201-7c5258e8d59b.png)\r\n\r\n**Standalone code to reproduce the issue**\r\nThe model I downloaded is: http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz \r\n\r\nThe command I used to export the official checkpoint to a saved_model.pb is: **python ~/models/research/object_detection/exporter_main_v2.py --input_type image_tensor --pipeline_config_path pipeline.config --trained_checkpoint_dir checkpoint/ --output_directory exported_model/**\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50653\">No</a>\n"]}, {"number": 50652, "title": "Quantized Version of tf-lite model returning \"ERROR: Didn't find op for builtin opcode 'CONV_2D' version '5'\" when measuring performance", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy A50\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n\r\n**Describe the current behavior**\r\n\r\nCurrently following along this [guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/android) to measure performance of tf-lite models. When doing this: \r\n\r\n`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/non-quantized.tflite --enable_op_profiling=true `\r\n\r\nI am able to successfully return results like this: \r\n\r\n`Average inference timings in us: Warmup: 432930, Init: 35157, no stats: 387831`\r\n\r\nHowever, when I'm using a quantized version of the model:\r\n\r\n`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/quantized.tflite --enable_op_profiling=true` \r\n\r\nI am getting this error\r\n\r\n```\r\nLoaded model /data/local/tmp/quantized.tflite\r\nresolved reporter\r\nERROR: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n\r\nERROR: Registration failed.\r\n\r\nFailed to construct interpreter\r\nAborted\r\n```\r\n\r\n", "comments": ["Please make sure that the TensorFlow version of the benchmark tool is the same or higher version than the TensorFlow version used for the TensorFlow Lite conversion.", "> Please make sure that the TensorFlow version of the benchmark tool is the same or higher version than the TensorFlow version used for the TensorFlow Lite conversion.\r\n\r\nThanks. I was able to resolve it by using TF v2.5.0 both for the benchmark tool and the TF lite conversion. \r\nJust one quick question before I close the issue:\r\nIn terms of inference time, what is expected for quantized versions of model? Higher or Lower?\r\n\r\nThe reason I'm asking is because I'm getting slightly higher inference time with the quantized version which I find weird", "What inference timings do you observe in quantized version vs non-quantized version?\r\nPerformance improvement is observed for some well know models listed here https://www.tensorflow.org/lite/performance/measurement#android_performance_benchmarks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50652\">No</a>\n"]}, {"number": 50651, "title": "cudaMallocAsync: better VLOG and error message.", "body": "This combine a few trivial changes related to memory allocator: \r\nWAR some OOM, and better VLOG and error message.\r\n\r\nIn detail:\r\n\r\n* cudaMallocAsync:\r\n    - When OOM happens, sync the stream and try to allocate again. This allow extra coalescing/remapping and WAR some OOM.\r\n    - VLOG the peak memory usage.\r\n    - Extra VLOG when OOM including histogram of allocation, the list of allocated ptr and size and cudaMallocAsync stats.\r\n    - Better Error message.\r\n    - Check some return status that wasn't checked.\r\n* cudaMalloc allocator: Add extra VLOG\r\n* BFC Allocator VLOG.\r\n    - Allow to print a trace of allocation/free and to print the peak memory usage.\r\n    - Use increasing VLOG number for basic log, the peak then the trace.\r\n\r\n@sanjoy ", "comments": ["Check the return value: https://github.com/tensorflow/tensorflow/pull/50698\r\nSync when there is an OOM: https://github.com/tensorflow/tensorflow/pull/50699\r\n\r\nAnd I kept all the cosmetic/  VLOG changes in this PR.", "It didn't got merged. The github Ci status tell me this:\r\n```\r\nfeedback/copybara \u2014 Google internal checks FAILED for runs with create time 2021-07-16T23:25:25.581986539Z.\r\n```"]}, {"number": 50650, "title": "OpKernelConstruction allocate tensor that can persistent for Compute ", "body": "How can we allocate a tensor or buffer that could exist among each Compute invocation?  If so, how can we free them after usage? \r\n\r\nBased on the description in op_kernel.h, `allocate_temp` seems to only allow temporary tensor allocation during the construction. Is this correct?\r\n\r\nThanks a lot. ", "comments": ["@gyin-ai ,\r\n\r\nCan you please refer this link which provide information on [OpKernelConstruction](https://www.tensorflow.org/guide/create_op).It helps.", "Also this is not bug or feature request.Please feel free to close here and post in [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) where larger community reads questions there.Thanks!", "@tilakrayal I have checked the source code but didn't see any potential function to do so. https://github.com/tensorflow/tensorflow/blob/ae38275fbbb602ef00930a8969ee4a352ae107e3/tensorflow/core/framework/op_kernel.h#L254", "@gyin-ai ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 50648, "title": "Input shapes incompatible when using `tensorrt.Converter`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Using Colab (with GPU) and TensorFlow 2.5\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI have an image classification model that takes 224x224x3 inputs. Now when I am trying to convert it to TensorRT with the pre-built engine options enabled it's running into:\r\n\r\n```python\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py in build(self, input_fn)\r\n   1187       if not first_input:\r\n   1188         first_input = inp\r\n-> 1189       func(*map(ops.convert_to_tensor, inp))\r\n   1190 \r\n   1191     if self._need_trt_profiles():\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1709       TypeError: If the arguments do not match the function's signature.\r\n   1710     \"\"\"\r\n-> 1711     return self._call_impl(args, kwargs)\r\n   1712 \r\n   1713   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n    245     else:\r\n    246       return super(WrappedFunction, self)._call_impl(\r\n--> 247           args, kwargs, cancellation_manager)\r\n    248 \r\n    249   def prune(self, feeds, fetches, name=None, input_signature=None):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1727             raise structured_err\r\n   1728 \r\n-> 1729       return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n   1730 \r\n   1731   def _call_with_flat_signature(self, args, kwargs, cancellation_manager):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_with_flat_signature(self, args, kwargs, cancellation_manager)\r\n   1749           \"{} takes {} positional arguments but {} were given\".format(\r\n   1750               self._flat_signature_summary(), self._num_positional_args,\r\n-> 1751               len(args)))\r\n   1752     args = list(args)\r\n   1753     kwargs = dict(kwargs)\r\n\r\nTypeError: pruned(input_1) takes 1 positional arguments but 224 were given\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt's not clear as of now. [The documentation](https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter) does mention about the specifications but following those didn't help. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nparams = tf.experimental.tensorrt.ConversionParams(\r\n    precision_mode=\"FP16\",\r\n    maximum_cached_engines=16)\r\nconverter = tf.experimental.tensorrt.Converter(\r\n    input_saved_model_dir=\"classification_model\", conversion_params=params)\r\nconverter.convert()\r\n\r\ndef data_gen():\r\n    for image in images[:100]:\r\n        yield image\r\n\r\nconverter.build(input_fn=data_gen)  \r\nconverter.save(\"tensorrt_embedding_model\")  \r\n```\r\n\r\nwhere, `classification_model` is the location of the SavedModel (generated from `tf.keras.applications.DenseNet121`). \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sayakpaul \r\n\r\nWhile running the code I am facing an error stating `OSError: SavedModel file does not exist at: classification_model/{saved_model.pbtxt|saved_model.pb}`\r\n\r\nCould you please share the saved model file you are using in the code. Zip the contents of the folder, then drag and drop it in the input text box to share it. Thanks!\r\n\r\n", "You likely didn't read the description. \r\n\r\n> where, classification_model is the location of the SavedModel (generated from tf.keras.applications.DenseNet121).\r\n\r\nThis means you need first load the `DenseNet121` model, save it as a `SavedModel` and then proceed. ", "`build` method expects you to return inputs in a tuple. I have changed your `data_gen` function to match the correct signature. \r\nHere is  the full script.\r\n\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tqdm import tqdm\r\n\r\nprint('TensorFlow version:', tf.__version__)\r\n\r\nmodel = tf.keras.applications.DenseNet121()\r\ntf.saved_model.save(model, 'classification_model')\r\nprint('classification_model:', os.listdir('classification_model'))\r\n\r\nparams = tf.experimental.tensorrt.ConversionParams(\r\n    precision_mode=\"FP16\",\r\n    maximum_cached_engines=16)\r\nconverter = tf.experimental.tensorrt.Converter(\r\n    input_saved_model_dir=\"classification_model\",\r\n    conversion_params=params)\r\nconverter.convert()\r\n\r\ndef data_gen(steps=1):\r\n    for i in range(steps):\r\n        # converter.build expects you to send it a tuple containing the inputs\r\n        yield (tf.random.uniform([1, 224, 224, 3]),)\r\n\r\nconverter.build(input_fn=data_gen)  \r\nconverter.save(\"tensorrt_embedding_model\")\r\nprint('tensorrt_embedding_model:', os.listdir('tensorrt_embedding_model'))\r\n\r\ntrt_model = tf.saved_model.load('tensorrt_embedding_model')\r\nserving_fn = trt_model.signatures['serving_default']\r\nimage = tf.random.normal([1, 224, 224, 3])\r\n\r\nfor _ in tqdm(range(1000)):\r\n    predictions = serving_fn(image)\r\n```\r\n---\r\n```\r\nTensorFlow version: 2.5.0\r\nINFO:tensorflow:Assets written to: classification_model/assets\r\nclassification_model: ['variables', 'saved_model.pb', 'assets']\r\nINFO:tensorflow:Linked TensorRT version: (0, 0, 0)\r\nINFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\r\nINFO:tensorflow:Assets written to: tensorrt_embedding_model/assets\r\ntensorrt_embedding_model: ['variables', 'saved_model.pb', 'assets']\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:12<00:00, 79.92it/s]\r\n```", "I see. This is nowhere mentioned in the docs. Thanks, @srihari-humbarwadi :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50648\">No</a>\n"]}, {"number": 50647, "title": "Custommask warning when saving to h5 using built-in keras layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pypi\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\nWhen saving a model to h5 format, and the model contains one of the following layers:\r\n```\r\ntf.keras.layers.experimental.preprocessing.Resizing()\r\ntf.keras.layers.experimental.preprocessing.CenterCrop()\r\n```\r\n\r\nthere is a:\r\n\r\n`CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.`\r\n\r\nWhen trying to load the keras model with tf.keras.models.load_model, it'll fail with:\r\n\r\n`TypeError: ('Keyword argument not understood:', 'crop_to_aspect_ratio')`\r\n\r\n**Describe the expected behavior**\r\nIt should both correctly save and load\r\n\r\n**Standalone code to reproduce the issue**\r\n", "comments": ["@ghylander ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!\r\n\r\n", "unfortunately i can't, provide the dataset, but it's not relevant, it happens when saving an untrained model (only default weights from imagenet)\r\nby doing further tests i found out that the issue just started happening on the AWS machine we use for training models\r\n\r\nit just began happening even with model architectures that have been working for months.\r\nThere is specially something that confuses me, the code i am using to save the model is:\r\n\r\n\r\n```\r\nmodel.save('FULLPATHTODIRECTORY/tf_model')\r\nmodel.save('FULLPATHTODIRECTORY/keras_model.h5')\r\n```\r\n\r\nand the output is:\r\n```\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n/home/ubuntu/miniconda3/envs/VENVNAME/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  warnings.warn('Custom mask layers require a config and must override '\r\nINFO:tensorflow:Assets written to: FULLPATHTODIRECTORY/tf_model/assets\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\n```\r\n\r\nwhy is it rising a warning about compiling the model?\r\n\r\nhere gist with simplified code:\r\nhttps://colab.research.google.com/drive/1GOYa3nBBmw27h7aayCfskz-p0SxElQ0S?usp=sharing\r\n\r\ni used this dataset for the gist: https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip", "Hi, i cloned my venv and installed tf 2.4.2 on it and i don't have this issue\r\nis there something that changed in how models are saved?\r\nprevious to the stable 2.5.0 i had used both 2.5.0 nightly november release and march release, and i didnt have this issue before\r\ni changed nothing in the code, just uninstalled 2.5.0 and installed 2.4.2\r\nthe issue is also present in the gist i linked, which i assume to be a \"clean\" install", "@ghylander ,\r\n\r\nThe warning was added with TF 2.5 in commit [076b5be](https://github.com/tensorflow/tensorflow/commit/076b5be77d3933deb726de1fc9d954d284985b1e#diff-08518b3b0b22c5c9dc137c911fa716fe820878cb9bbf95806ffca0e0296ec7fa).Please refer this [comment](https://github.com/tensorflow/tensorflow/issues/50324#issuecomment-864687940) from the issue.Thanks!", "i usppose i'll close it as it seems to be a duplicate then", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50647\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50647\">No</a>\n"]}, {"number": 50646, "title": "Problem resuming training with checkpoints in TF > 2.4.1", "body": "**System information**\r\n- Windows 10 Pro\r\n- TF 2.4.1 and TF 2.5.0\r\n\r\n**Describe the current behavior**\r\nIt is no longer possible to resume training with checkpoints in TF > 2.4.1. This is not the case in previous versions of TF. The following error is displayed: tensorflow ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: <tf.Tensor: shape=(), dtype=int32, numpy=0>. It appears to be caused by OptimizerV2, where all the hyperparameters are assumed to be float when loading the adam optimizer, yet the optimizer saves some of the hyperparameters as integers. \r\n\r\n", "comments": ["I tried executing the [training checkpoint guide](https://www.tensorflow.org/guide/checkpoint) and it works fine with TF 2.5.0 as well.\r\nPerhaps you want to create a minimal code example to reproduce the issue observed. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50646\">No</a>\n"]}, {"number": 50644, "title": "TensorFlow Lite android compiling failed on Docker", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nI compile tensorflow lite in android mode according to the online guide: https://www.tensorflow.org/lite/guide/build_android\r\n\r\n- the docker file is provided by tflite \r\n    - https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile\r\n\r\n-  I entered into docker container but tflite compiling failed.\r\n   - jdk version: 1.8\r\n   - command : `bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite`\r\n\r\n![image](https://user-images.githubusercontent.com/45189361/124571996-dbcdb500-de7a-11eb-8190-1d498580b2fb.png)\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n- related issue [#50644 ]\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@thaink could you check this?", "It is fixed in commit 27551046f5c49939c508ed7a7c33cb6724aa747c.\r\nCould you sync to the master HEAD and try again?", "after [2755104](https://github.com/tensorflow/tensorflow/commit/27551046f5c49939c508ed7a7c33cb6724aa747c) is merged , compiling still failed\r\n![image](https://user-images.githubusercontent.com/45189361/124853053-d3d75780-dfd7-11eb-866c-10a92889bd62.png)\r\n", "Did you build the docker again?\r\nCan you upload the docker build log?", "The docker image is rebuilt successfully.\r\nI mean that I can't compile Tensorflow-Lite successfully in this docker image.\r\n\r\n- Compiling is according to this [guide](https://www.tensorflow.org/lite/guide/build_android#build_tensorflow_lite_locally)\r\n   - Compiling command: `bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite`\r\n   - error message:\r\n![image](https://user-images.githubusercontent.com/45189361/125035541-84bb2080-e0c4-11eb-984f-9408495dda67.png)\r\n\r\nI rebuilt this docker but the tflite compiling process afterwards still failed, could you help test this issue in your docker container?", "You need to install the sdk using the command `android update sdk` and run ./configure.\r\nIt is described in the guide. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50644\">No</a>\n"]}, {"number": 50641, "title": "ERROR: Didn't find op for builtin opcode 'MUL' version '5'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): model built with tf2.5 converted to tflite with tf_nightly 2.7.0-dev20210705\r\n\r\n\r\n**Output from tflite benchmark**\r\n\r\n```\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [model_float16.tflite]\r\nEnable op profiling: [1]\r\nLoaded model model_float16.tflite\r\nERROR: Didn't find op for builtin opcode 'MUL' version '5'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\r\n\r\nERROR: Registration failed.\r\n\r\nFailed to initialize the interpreter\r\nBenchmarking failed.\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nThe benchmark run was the prebuilt one provided on https://www.tensorflow.org/lite/performance/measurement#ios_benchmark_app.\r\nI can't use any earlier version of TensorFlow to convert the model because of recent additions to tf_nightly\r\n", "comments": ["This is an intended behavior. You need to use the same or higher version used for conversion when invoking the converted graph.", "I little confused, from what I understand the I am using the same or higher version of TensorFlow. Should I build the benchmark from source since the pre-built ones don't seem to be working, or maybe they aren't the newest version of nightly like I was lead to believe? Maybe I am missing something?"]}, {"number": 50640, "title": "[determinism] Enhance r2.6 release notes", "body": "This PR enhances `RELEASE.md` in the r2.6 branch by including info about new op-determinism features.\r\n\r\n@reedwm @sanjoy @nluehr ", "comments": []}, {"number": 50639, "title": "[INTEL MKL] Fixed a unit test failure when both cuda and oneDNN are enabled.", "body": "All the mkl tests (ex.  AutoMixedPrecisionTest.test_conv3d('mkl')) in //tensorflow/python/grappler:auto_mixed_precision_test_gpu fail in the gpu build when the env variable TF_ENABLE_ONEDNN_OPTS=1 is set.  The problem is caused by the tests trying to call the AutoMixedPrecisionMkl (bf16) grappler pass with gpu ops, which is not supported. The fix is to assign ops to CPU when testing AutoMixedPrecisionMkl.", "comments": []}, {"number": 50638, "title": "Fixing a build failure in a CI test", "body": "This PR fixes a CI build failure in the test \r\n//tensorflow/core/tpu/kernels:sharding_util_ops_test. This [log](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/25746/artifact/dnnl_test.log/*view*/) shows the build failure details. ", "comments": ["The commit got in (https://github.com/tensorflow/tensorflow/commit/795f6b8fd73ddfd3a5bec956a15594d11dfb8cc8). The test should be able to build now. I'll close this PR. Thank you again for submitting the fix!"]}, {"number": 50635, "title": "Converting FasterRCNN from tf -> tflite fails: could not rewrite use of immutable bound input", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): `tensorflow==2.5.0` installed via pip\r\n\r\n### 2. Code\r\n\r\nFull code to reproduce my issue: https://github.com/florianletsch/torch2tflite-failing\r\n\r\nI am trying to convert a Faster-RCNN with Resnet backbone following these 3 steps:  \r\n\r\n1. PyTorch -> ONNX\r\n2. ONNX -> Tensorflow\r\n3. Tensorflow -> tflite\r\n\r\nStep 3 takes a while (~18 minutes) and then fails. This is the code of step 3:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('assets/tfsavedmodel')\r\ntflite_model = converter.convert()\r\n```\r\n\r\nError message:\r\n\r\n```\r\nloc(callsite(callsite(\"onnx_tf_prefix_If_1347@__inference___call___15681\" at \"StatefulPartitionedCall@__inference_signature_wrapper_16224\") at \"StatefulPartitionedCall\")): error: could not rewrite use of immutable bound input\r\nTraceback (most recent call last):\r\n  File \"/home/florian/projects/torch2tflite-failing/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 291, in toco_convert_protos\r\n    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n  File \"/home/florian/projects/torch2tflite-failing/venv/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py\", line 32, in wrapped_toco_convert\r\n    return _pywrap_toco_api.TocoConvert(\r\nException: <unknown>:0: error: loc(callsite(callsite(\"onnx_tf_prefix_If_1347@__inference___call___15681\" at \"StatefulPartitionedCall@__inference_signature_wrapper_16224\") at \"StatefulPartitionedCall\")): could not rewrite use of immutable bound input\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n```\r\n\r\n### The op node where it fails\r\n\r\nThe error message mentions op node `If_1347`. According to the onnx model, the then/else branches of that node contain the following results of PyTorch's JIT:\r\n\r\n```\r\n  %2632 = If[else_branch = <graph torch-jit-export2>, then_branch = <graph torch-jit-export1>](%2631)\r\n\r\n(...)\r\n\r\ngraph torch-jit-export1 {\r\n  %2633 = Constant[value = <Tensor>]()\r\n  %2634 = ConstantOfShape[value = <Tensor>](%2633)\r\n  return %2634\r\n}\r\n\r\ngraph torch-jit-export2 {\r\n  %2635 = ReduceMax[keepdims = 0](%2622)\r\n  %2636 = Cast[to = 1](%2626)\r\n  %2637 = Constant[value = <Scalar Tensor []>]()\r\n  %2638 = Cast[to = 1](%2637)\r\n  %2639 = Add(%2635, %2638)\r\n  %2640 = Mul(%2636, %2639)\r\n  %2641 = Unsqueeze[axes = [1]](%2640)\r\n  %2642 = Add(%2622, %2641)\r\n  %2643 = Unsqueeze[axes = [0]](%2642)\r\n  %2644 = Unsqueeze[axes = [0]](%2624)\r\n  %2645 = Unsqueeze[axes = [0]](%2644)\r\n  %2646 = Constant[value = <Tensor>]()\r\n  %2647 = Constant[value = <Tensor>]()\r\n  %2648 = NonMaxSuppression(%2643, %2645, %2646, %2647)\r\n  %2649 = Constant[value = <Tensor>]()\r\n  %2650 = Gather[axis = 1](%2648, %2649)\r\n  %2651 = Squeeze[axes = [1]](%2650)\r\n  return %2651\r\n}\r\n```\r\n\r\nI don't understand what's going on here, is there a pointer that explains the issue hidden here?\r\n\r\n### Logs\r\nLogs of my 3 conversion steps can be found here: https://github.com/florianletsch/torch2tflite-failing/tree/main/logs\r\n\r\n### Converter issue or an unrelated problem?\r\nIs this an issue or the tflite converter? Or is this likely an issue of an earlier step in the conversion pipeline? I am grateful for any pointers.\r\n", "comments": ["Mutable variable support is now available in the nightly when converting using from_saved_model.\r\nYou need to set this flag to True for now.\r\n\r\nconverter.experimental_enable_resource_variables = True", "This solved it, thank you!"]}, {"number": 50634, "title": "Can't build tensorflow lite wheel in docker image for aarch64", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- TensorFlow version: latest from github\r\n\r\n**Describe the problem**\r\nFollowing the instructions from [here](https://www.tensorflow.org/lite/guide/build_cmake_pip) I tried to build a python wheel for tensorflow lite for the aarch64 (mendel 4.0/coral dev board) architecture. \r\n\r\n`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON38   tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh aarch64`\r\n\r\nWhen I run the suggested command an error occurs which indicates that cmake can't find the correct compiler. \r\n\r\n```\r\nCMakeFiles/cmTC_8c684.dir/testCCompiler.c.o  -o cmTC_8c684 \r\n    aarch64-linux-gnu-gcc: fatal error: -fuse-linker-plugin, but liblto_plugin.so not found\r\n    compilation terminated.\r\n    CMakeFiles/cmTC_8c684.dir/build.make:89: recipe for target 'cmTC_8c684' failed\r\n    make[1]: *** [cmTC_8c684] Error 1\r\n    make[1]: Leaving directory '/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeTmp'\r\n    Makefile:124: recipe for target 'cmTC_8c684/fast' failed\r\n    make: *** [cmTC_8c684/fast] Error 2\r\n    \r\n    \r\n\r\n  \r\n\r\n  CMake will not be able to correctly generate this project.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:40 (project)\r\n\r\n\r\nCMake Error at CMakeLists.txt:40 (project):\r\n  The CMAKE_CXX_COMPILER:\r\n\r\n    /workspace/tensorflow/lite/tools/cmake/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\r\n\r\n  is not a full path to an existing compiler tool.\r\n\r\n  Tell CMake where to find the compiler by setting either the environment\r\n  variable \"CXX\" or the CMake cache entry CMAKE_CXX_COMPILER to the full path\r\n  to the compiler, or to the compiler name if it is in the PATH.\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeError.log\".\r\n\r\n```\r\nAfter some investigation it seems that for some reason the tar command has problems unpacking the downloaded toolchain. The error (if manually run) is \r\n`\r\ntar: gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-g++: Cannot hard link to \u2018gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-c++\u2019: Operation not supported\r\n`\r\nAny suggestions how to fix this issue?", "comments": ["I've just tried on master branch on my Linux machine and it works well.\r\n\r\nCould you share branch you used and your build environment?", "The problem was that I tried to built it on an azure machine learning instance inside a mounted networking file system and apparently the untar option does not work there properly as it does not support hard links. Solution was to run it outside the mounted file system. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50634\">No</a>\n"]}, {"number": 50633, "title": "Compiling a keras model during a callback causes TypeError: 'NoneType' object is not callable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary from pip/anaconda\r\n- TensorFlow version (use command below): Tried with the following \r\n  - v2.4.0-49-g85c8b2a817f 2.4.1\r\n  - v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n  - v1.12.1-59787-gd89220bfa4c 2.7.0-dev20210706 (tf-nightly)\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen compiling a model in a callback, the training breaks with the error `TypeError: 'NoneType' object is not callable`\r\nI'm pretty sure there is a regression here, as it works when using TensorFlow v <= 2.3\r\n\r\n**Describe the expected behavior**\r\n\r\nNo crash, and having the model recompiled with the new parameters passed during compilation. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MyCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epochs, logs=None):\r\n        # we recompile at the end of the first epoch\r\n        if epochs == 0:\r\n            self.model.compile(\r\n                optimizer=\"rmsprop\",\r\n                loss=\"mse\",\r\n            )\r\n\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(1,input_shape=(1,))])\r\nmy_callback = MyCallback()\r\nx = tf.random.uniform((100,1))\r\ny = tf.random.uniform((100,1))\r\n\r\nmodel.compile(optimizer=\"rmsprop\", loss=\"mse\")\r\n\r\nmodel.fit(x,y, callbacks=[my_callback], epochs=2)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nThe Traceback: \r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"compile_callback.py\", line 28, in <module>\r\n    model.fit(x,y, callbacks=[my_callback], epochs=2)\r\n  File \"/home/user/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\nTypeError: 'NoneType' object is not callable\r\n```\r\n", "comments": ["@Lescurel,\r\n\r\n It looks like the issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](https://github.com/keras-team/keras/issues) repository. Thanks!", "Done. Feel free to close the issue if needed. ", "@Lescurel ,\r\n\r\nPlease feel free to close the issue here and open new issue in keras repo.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50633\">No</a>\n"]}]