[{"number": 24229, "title": "[INTEL MKL]: ADD Tensorflow+MKL Singularity definition files", "body": "This PR adds singularity definition files for intel optimized tensorflow.\r\n\r\nDefinition files are created based of existing docker binaries from docker hub. This helps for easier maintenance", "comments": ["@karthikvadla Do we want recipes for `Python3` images as well?\r\nIf yes then we need to add files specific to below images:\r\n```\r\nintelaipg/latest-mkl-py3\r\nintelaipg/latest-devel-mkl-py3\r\n```", "@penpornk can you please review this PR when you get time. ", "Thank you for your PR and sorry for my delay! The scripts' contents look fine. I just have to make sure if this is the right place to put them, and about the license term. Looping in @thirupalanisamy and @tatianashp for more comments.", "Sure thank you. @ashahba can help with license things from our end if you need any changes.", "> @karthikvadla Do we want recipes for `Python3` images as well?\r\n> If yes then we need to add files specific to below images:\r\n> \r\n> ```\r\n> intelaipg/latest-mkl-py3\r\n> intelaipg/latest-devel-mkl-py3\r\n> ```\r\n\r\nIf you think that really helps others we can do it. or based on requirement we can open separate PR's.", "> Thank you for your PR and sorry for my delay! The scripts' contents look fine. I just have to make sure if this is the right place to put them, and about the license term. Looping in @thirupalanisamy and @tatianashp for more comments.\r\n\r\nApache 2.0 license is already part of our OSS approved list, we are good there.", "Ahh sorry! I clicked the wrong button! Will try to reopen it.", "It seems the reopen button is only on the author's side. Please reopen it yourself if needed. Sorry for the inconvenience!", "@ymodak Thank you!", "@penpornk should i create `singularity` sub-folder at below structure and add files?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker .\r\n\r\nI can add section for singularity too in readme, if you are okay with that.", "I don't think these files should live in tensorflow/tensorflow. The underlying dockerfiles are inintelaipg/intel-optimized-tensorflow? Then these files should be there as well. \r\n\r\nThat also makes the discussion about licenses and directories moot.", "> I don't think these files should live in tensorflow/tensorflow. The underlying dockerfiles are inintelaipg/intel-optimized-tensorflow? Then these files should be there as well.\r\n> \r\n> That also makes the discussion about licenses and directories moot.\r\n\r\noh this is https://github.com/tensorflow/tensorflow/blob/c1738f64a7aa900bb7de422c36ebd93686c46fee/tensorflow/tools/singularity/Singularty-devel-mkl#L22 reference to binary on docker hub https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/tags/ , not to any dockerfiles.\r\n\r\nthis singularity definition file uses existing docker binary to create new singularity container. \r\n"]}, {"number": 24228, "title": "Allow TensorFlow demo app to switch between devices", "body": "- ListView gui for model (float and quant)\r\n- ListView gui for devices (optional gpu, nnapi and cpu)\r\n- Add a few AAR options.\r\n", "comments": ["Moving this to an internal code review"]}, {"number": 24227, "title": "Device mapping: no known device tensorflow-gpu Windows 10", "body": "", "comments": ["\r\n(base) C:\\Users\\ShivamOO7>activate tensorflow-gpu\r\n\r\n(tensorflow-gpu) C:\\Users\\ShivamOO7>python\r\nPython 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:05:27) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2018-12-08 02:10:10.036043: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-12-08 02:10:10.125431: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce 820M major: 2 minor: 1 memoryClockRate(GHz): 1.25\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.65GiB\r\n2018-12-08 02:10:10.136463: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1455] Ignoring visible gpu device (device: 0, name: GeForce 820M, pci bus id: 0000:08:00.0, compute capability: 2.1) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.0.\r\n2018-12-08 02:10:10.150674: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-08 02:10:10.158996: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2018-12-08 02:10:10.166482: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\nDevice mapping: no known devices.\r\n2018-12-08 02:10:10.174779: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:288] Device mapping:\r\n![screenshot 139](https://user-images.githubusercontent.com/44320036/49672662-dd9c2580-fa91-11e8-85e5-adbc8b33ef6b.png)\r\n\r\nI am working on windows10.\r\nwhen i am run deviceQuery.exe\r\n\r\nMicrosoft Windows [Version 10.0.17134.407]\r\n(c) 2018 Microsoft Corporation. All rights reserved.\r\n\r\nC:\\>cd Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\demo_suite\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\demo_suite>deviceQuery.exe\r\ndeviceQuery.exe Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce 820M\"\r\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\r\n  CUDA Capability Major/Minor version number:    2.1\r\n  Total amount of global memory:                 2048 MBytes (2147483648 bytes)\r\n  ( 2) Multiprocessors, ( 48) CUDA Cores/MP:     96 CUDA Cores\r\n  GPU Max Clock rate:                            1250 MHz (1.25 GHz)\r\n  Memory Clock rate:                             900 Mhz\r\n  Memory Bus Width:                              64-bit\r\n  L2 Cache Size:                                 131072 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65535), 3D=(2048, 2048, 2048)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 32768\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  1536\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (65535, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 0 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 8 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce 820M\r\nResult = PASS\r\n\r\nand nvcc --version\r\n\r\nMicrosoft Windows [Version 10.0.17134.407]\r\n(c) 2018 Microsoft Corporation. All rights reserved.\r\n\r\nC:\\Users\\ShivamOO7>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Mon_Jan__9_17:32:33_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.60\r\n", "As mentioned in the output below. The recommended CUDA compute capability for TensorFlow GPU is 3.5. However there have been successful installations if built from sources for cuda compute capability 3.0 as well. However it looks like your cuda compute capability is lower than 3.5. Therefore, its ignoring your GPU device. \r\n>T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1455] Ignoring visible gpu device (device: 0, name: GeForce 820M, pci bus id: 0000:08:00.0, compute capability: 2.1) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.0.", "That means I will not be able to use the GPU.", "Unfortunately not with your current GPU configuration. I will close this issue. Feel free to reopen if have any follow up questions. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24225, "title": "Why there are two nnapi_delegate.cc with the same name?", "body": "In r1.12\r\n\r\nThere is one under the folder https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite/delegates/nnapi\r\n\r\nThe other one is \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite\r\n\r\nCan anybody explain what the two files are doing? Can they be named differently? Looks very confusing. \r\n", "comments": ["https://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite/delegates/nnapi is used in unit test. \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.12/tensorflow/contrib/lite is mostly used\r\n", "@tingxingdong Did you resolve your question? If yes, we can close this bug.", "We plan on transitioning to using the one in lite/delegates/nnapi. We should probably rename the one in lite/ to avoid confusion, thanks for the report."]}, {"number": 24224, "title": "Add float16 suport for scatter_max/scatter_min on gpu", "body": "\r\nThis fix tries to address the issue raised in #24219 where\r\nthere were no float16 supports for scatter_max/scatter_min on gpu.\r\nThis fix adds the float16 support for scatter_max/scatter_min on gpu.\r\n\r\nThis fix fixes #24219.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 24223, "title": "TensorflowLite conversion failed using embedding layer.", "body": "**System information**\r\n- Windows 10 version 1803 x64\r\n- TensorFlow installed from pip\r\n- tf-nightly 1.13.0.dev20181129\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2018-12-07 12:30:04.392840: I tensorflow/lite/toco/import_tensorflow.cc:1280] Converting unsupported operation: ResourceGather\r\n2018-12-07 12:30:04.393575: I tensorflow/lite/toco/import_tensorflow.cc:1329] Unable to determine output type for op: ResourceGather\r\n2018-12-07 12:30:04.518747: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 28 arrays (0 quantized)\r\n2018-12-07 12:30:04.519109: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 28 arrays (0 quantized)\r\n2018-12-07 12:30:04.519819: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 7 operators, 14 arrays (0 quantized)\r\n2018-12-07 12:30:04.707328: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 6 operators, 13 arrays (0 quantized)\r\n2018-12-07 12:30:04.707811: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 5 operators, 11 arrays (0 quantized)\r\n2018-12-07 12:30:04.708213: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)\r\n2018-12-07 12:30:04.708689: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1216 bytes, theoretical optimal value: 1216 bytes.\r\n2018-12-07 12:30:04.709343: E tensorflow/lite/toco/toco_tooling.cc:420] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, LOGISTIC. Here is a list of operators for which you will need custom implementations: ResourceGather.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\dorme\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\dorme\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Dorme\\Anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n  File \"c:\\users\\dorme\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\dorme\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"c:\\users\\dorme\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, LOGISTIC. Here is a list of operators for which you will need custom implementations: ResourceGather.\r\n```\r\n\r\nUsing an Embedding, Flatten, Dense and Dense layers for text seems impossible i'm also using the library tokenizer from keras for preprocessing the data.\r\n", "comments": ["@Darthremar   As per the above exception, I do notice that it is due to the unsupported ops by TF Lite. For any native TF ops(which you want to use in Lite) could you please try using extended runtime by passing  --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter() as asked.\r\nWe would encourage you to go through [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tf_ops_compatibility.md) link to get an idea on TF Lite supported ops. Also if you have any request to add an op to TF Lite, please use this [thread](https://github.com/tensorflow/tensorflow/issues/21526) to post and track your request. "]}, {"number": 24222, "title": "New files for Tensorflow Lite Micro Speech Example targetting the Eta\u2026", "body": "\u2026 Compute ECM3531 evaluation board", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it\n\nOn Fri, Dec 7, 2018 at 11:10 AM googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24222#issuecomment-445333441>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Aq2lOJJ63pWG0mCPovD4SimhUV45x5-kks5u2r05gaJpZM4ZI_Co>\n> .\n>\n", "CLA has been signed again ", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Is there something else we need to do to trigger the remaining checks that haven't completed? /cc @gunan ", "The failure in the `MacOS Python2 and CC` test appears to be unrelated to any code here:\r\n\r\n```\r\n==================== Test output for //tensorflow/core:platform_port_test:\r\nRunning main() from test_main.cc\r\n[==========] Running 5 tests from 3 test cases.\r\n[----------] Global test environment set-up.\r\n[----------] 2 tests from Port\r\n[ RUN      ] Port.AlignedMalloc\r\n[       OK ] Port.AlignedMalloc (0 ms)\r\n[ RUN      ] Port.GetCurrentCPU\r\ntensorflow/core/platform/port_test.cc:38: Failure\r\nExpected: (cpu) >= (0), actual: -1 vs 0\r\n[  FAILED  ] Port.GetCurrentCPU (0 ms)\r\n```", "So only the copybara check is required.  Is there a way to kick that off?\n\nAlos, what is the protocol here, should I reply to your emails on Gmail or\non Github?\n\n\nOn Sun, Dec 16, 2018 at 5:29 PM Pete Warden <notifications@github.com>\nwrote:\n\n> The failure in the MacOS Python2 and CC test appears to be unrelated to\n> any code here:\n>\n> ==================== Test output for //tensorflow/core:platform_port_test:\n> Running main() from test_main.cc\n> [==========] Running 5 tests from 3 test cases.\n> [----------] Global test environment set-up.\n> [----------] 2 tests from Port\n> [ RUN      ] Port.AlignedMalloc\n> [       OK ] Port.AlignedMalloc (0 ms)\n> [ RUN      ] Port.GetCurrentCPU\n> tensorflow/core/platform/port_test.cc:38: Failure\n> Expected: (cpu) >= (0), actual: -1 vs 0\n> [  FAILED  ] Port.GetCurrentCPU (0 ms)\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24222#issuecomment-447696185>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Aq2lOPBF2k45td88eNLz0I2yVqzYTRbuks5u5vN-gaJpZM4ZI_Co>\n> .\n>\n", "I believe somebody internal should take care of the copybara check once the tests pass, and I've just seen that Gunhan was able to kick off tests by adding the kokoro:force-run flag, so I've tried doing the same to see if we can rerun the tests. Thanks for your patience, hopefully we'll have this through soon!", "Thanks Pete.  Looks like the checks passed!", "The stub .sh file caused an error internally because it didn't have a copyright notice, so I've removed that and will rerun the tests.", "Should I check in the  test_ecm3531_binary.sh file with the copyright added?  ", "No, I removed it already so I think we're good.", "There are 3 checks now that are failing but earlier today it showed that all the tests passed.  What gives?", "One point about removing the test_ecm3531_binary.sh file is that the\nmakefile complains about not finding the file and will report that the make\nfailed.  It will generate the .elf file though.  So this script was added\njust to satisfy the makefile and not confuse the user who may else thin\nthat there was a compilation error.  So it will be good to have this file\nchecked in.  But I dont know if the checks that the tensorflow build is now\ncomplaining about is related to this or not.\n\nmake: tensorflow/lite/experimental/micro/testing/test_ecm3531_binary.sh:\nCommand not found\n/home/hari/tensorflow_fork/tensorflow/tensorflow/lite/experimental/micro/tools/make/Makefile:171:\nrecipe for target 'test_micro_speech' failed\nmake: *** [test_micro_speech] Error 127\n\n\nOn Mon, Dec 17, 2018 at 2:19 PM Pete Warden <notifications@github.com>\nwrote:\n\n> The stub .sh file caused an error internally because it didn't have a\n> copyright notice, so I've removed that and will rerun the tests.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24222#issuecomment-448019303>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Aq2lOC4_apfrJxD7MW6BLq2CJP126r1Iks5u6BhegaJpZM4ZI_Co>\n> .\n>\n", "The previous errors were for Windows but now the MacOS and Ubuntu builds show errors.   None of the errors are related to the files I checked in.  So how does one make this process converge?", "There were build breakages merged to master branch.\r\nNow they are fixed, I will retrigger your tests.", "Thanks.  This time the \"MacOS Contrib\" check has failed as earlier, but the \"Ubuntu contrib\" test which passed before now fails.  But two of the earlier checks that failed, now pass (\"MacOS Python2\" and \"Ubuntu Python3\")."]}, {"number": 24221, "title": "`tf.estimator.Estimator` methods `.evaluate`/`.train` calls deplete CPU RAM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Null\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu`\r\n- TensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): Null\r\n- GCC/Compiler version (if compiling from source): Null\r\n- CUDA/cuDNN version: cudNN 7.4.1\r\n- GPU model and memory: NVIDIA GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nWhile using a `tf.estimator.Estimator` class, each separate call of `.train`/`.evaluate` results on extra CPU RAM permanent usage. In my use case, every time I call `.evaluate` I get another 200MB RAM allocated on my CPU (even though I'm training on the GPU) RAM, which then never gets free. This ultimately results in `MemoryError` since after a few training/validation rounds the RAM gets full.\r\n\r\n**Describe the expected behavior**\r\nMemory used by these methods should be released when done.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nEXAMPLE_TYPE = Tuple[str, List[int], List[int], List[int], int]\r\nDATASET_TYPE = List[EXAMPLE_TYPE]\r\n\r\nclass ExampleGenerator:\r\n    def __init__(self, train: DATASET_TYPE, validation: DATASET_TYPE):\r\n        self._data = {\r\n            'train': train,\r\n            'validation': validation,\r\n        }\r\n\r\n    def generate(self, source: str, shuffle: bool = False) -> Generator[\r\n        EXAMPLE_TYPE, None, None]:\r\n        \"\"\"\r\n        Yields examples\r\n\r\n        :param source: Source to generate from (train/validation)\r\n        :return: Example\r\n        \"\"\"\r\n\r\n        if shuffle:\r\n            random.shuffle(self._data[source])\r\n\r\n        for example in self._data[source]:\r\n            yield example\r\n\r\ndef input_fn(example_generator: ExampleGenerator, source: str, batch_size: int):\r\n    dataset = tf.data.Dataset.from_generator(generator=lambda: example_generator.generate(source, shuffle=True),\r\n                                             output_types=(tf.string, tf.int32, tf.int32, tf.int32, tf.int32),\r\n                                             output_shapes=(\r\n                                                 tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([None]),\r\n                                                 tf.TensorShape([None]), tf.TensorShape([])))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.padded_batch(batch_size,\r\n                                   padded_shapes=([], [None], [None], [None], []))\r\n\r\n    dataset = dataset.prefetch(buffer_size=batch_size)\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n\r\n    eid, text, pos, dep, label = iterator.get_next()\r\n\r\n    return {'eid': eid, 'text': text, 'text_pos': pos, 'text_dep': dep}, label\r\n\r\ndef model(...):\r\n    ...\r\n\r\nfor _ in range(params['epochs']):\r\n    classifier.train(input_fn=lambda: input_fn(example_generator, source='train', batch_size=params['batch_size']))\r\n    classifier.evaluate(input_fn=lambda: input_fn(example_generator, source='validation', batch_size=params['batch_size']))\r\n```\r\n\r\nTo figure out why I get `MemoryError`s, I have used the Python interactive mode, and called `.evaluate(...)` and `.train(...)` while observing RAM usage, after each call, another 200MB RAM got occupied on the CPU RAM, eventually depleting all available RAM.", "comments": ["Apologies for the delay in response. Can you please build against latest version of TF and test again.\r\nNote that TF 1.13.0-rc0 comes pre-built cuda 10 binaries, thus you may have to upgrade your cuda version. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I'll try to run the code again and monitor its RAM when I'll have time, bit busy"]}, {"number": 24220, "title": "No Device Mapped with Tensorflow-gpu - Ubuntu 18.04", "body": "## System information\r\n- OS Platform and Distribution: Ubuntu 18.04.1 LTS x86_64\r\n- TensorFlow installed from (source or binary): Miniconda (used cmd: 'conda install -c anaconda tensorflow-gpu')\r\n- TensorFlow version: Tensorflow-gpu (Tried both 1.12.0 and 1.9.0, and got the same result)\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.0\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: As below.\r\n\r\n## CPU and GPU info\r\nCPU: Intel i7-8750H (12) @ 4.100GHz\r\nGPU: NVIDIA GeForce GTX 1070 Mobile\r\nGPU: Intel Device 3e9b\r\n\r\n\r\n## Describe the problem\r\n\r\nIt looks like in the 'Device Mapping' section as below, no GPU is linked, though no error msg noted in code. Strangely though, the GPU name/mode has been detected. Would you please help?\r\n\r\nDetails as below.\r\n\r\nPython 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import os\r\n>>> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n>>> tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2018-12-08 00:44:56.363230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-12-08 00:44:56.363327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-08 00:44:56.363349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-12-08 00:44:56.363370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-12-08 00:44:56.363659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6738 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n2018-12-08 00:44:56.371266: I tensorflow/core/common_runtime/direct_session.cc:288] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n\r\n<tensorflow.python.client.session.Session object at 0x7f693a136ef0>\r\n\r\n\r\n\r\n\r\n## Provide the exact sequence of commands / steps that you executed before running into the problem\r\n Pretty much followed this post:\r\nhttps://mc.ai/install-conda-tensorflow-gpu-and-keras-on-ubuntu-18-04/\r\n\r\n\r\n\r\n## Any other info / logs\r\n\r\n## GPU Driver Info \r\n\r\nSat Dec  8 00:56:11 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.57                 Driver Version: 410.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   46C    P3    24W /  N/A |    936MiB /  8119MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1142      G   /usr/lib/xorg/Xorg                           535MiB |\r\n|    0      1313      G   /usr/bin/gnome-shell                         217MiB |\r\n|    0      1941      G   ...quest-channel-token=4040395960093113594   181MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n## cuDNN ver\r\n\r\ncudnn-9.0-linux-x64-v7.1.tgz \r\n\r\n\r\n## Full info for CUDA\r\n\r\n$ ./deviceQuery\r\n./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GTX 1070 with Max-Q Design\"\r\n  CUDA Driver Version / Runtime Version          10.0 / 9.0\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 8120 MBytes (8513978368 bytes)\r\n  (16) Multiprocessors, (128) CUDA Cores/MP:     2048 CUDA Cores\r\n  GPU Max Clock rate:                            1266 MHz (1.27 GHz)\r\n  Memory Clock rate:                             4004 Mhz\r\n  Memory Bus Width:                              256-bit\r\n  L2 Cache Size:                                 2097152 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Supports Cooperative Kernel Launch:            Yes\r\n  Supports MultiDevice Co-op Kernel Launch:      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.0, CUDA Runtime Version = 9.0, NumDevs = 1\r\nResult = PASS\r\n\r\n\r\n## Check if cards are recognised on the bus:\r\n$ lspci | grep -i nvidia\r\n01:00.0 VGA compatible controller: NVIDIA Corporation GP104M [GeForce GTX 1070 Mobile] (rev a1)\r\n01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n\r\n\r\n## Check Drivers:\r\n$ dmesg | grep NVRM\r\n[    3.286908] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  410.57  Tue Sep 18 23:25:09 CDT 2018 (using threaded interrupts)\r\n\r\n\r\nThanks very much in advance!\r\n\r\n", "comments": ["This means that your GPU is located and linked\r\n> Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6738 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)", "Thanks ymodak. But, then why it's 'GPU: 0'?\r\n", "That's just your device name. If you run **nvidia-smi** in your terminal you should be able to see it.", "Ah, I see. Thanks a lot. I'll close the issue then.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24219, "title": "FP16 support on GPU for scatter_max", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  v1.12.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nscatter_max supports fp32, fp64 on GPU and fp16, fp32, fp64 on CPU.\r\nIt will be nice if scatte_max can support fp16 on GPU.\r\n\r\n**Will this change the current api? How?**\r\nno\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to use scatter_max with fp16 in gpu.\r\n\r\n**Any Other info.**\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/device:GPU:0\"):\r\n  # dtype=tf.float16 won't work in this code sample\r\n  u = tf.constant([[1,6,8], [2,4,9], [3,5,7]]*3, dtype=tf.float32)\r\n  r = tf.get_variable(\"r\", [4, 3], dtype=tf.float32)\r\n  y = tf.scatter_max(r, [0,0,0,1,1,2,2,3,3], u)\r\n\r\n  with tf.Session() as sess:\r\n    tf.initialize_all_variables().run()\r\n    print(sess.run(y))\r\n```\r\n", "comments": ["Added PR #24224 for float16 support."]}, {"number": 24218, "title": "Compilation error when building tfcompile on Windows 10", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): r1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.15\r\n- GCC/Compiler version (if compiling from source): VS 2015 14.0.25431.01 Update 3\r\n- CUDA/cuDNN version: 9.0/7.3.1\r\n- GPU model and memory: GTX 1080, 8GBs\r\n\r\n**Describe the current behavior**\r\nChecked out the release 1.12 branch of the TF repo, configured bazel (configuration is below) and ran \r\n`bazel build -c opt --config=opt //tensorflow/compiler/aot:tfcompile`\r\n\r\nCompilation fails at `.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338`, it seems a static assert is failing: \r\n```\r\nstd::pair<uint64, uint64> Encode() const {\r\n    static_assert(sizeof(*this) == 16, \"\"); //<<<<<<<<<<<<<<<<< \r\n    uint64 upper = Pack(kind(), size_);\r\n    uint64 lower = entry_param_number_;\r\n    return {upper, lower};\r\n  }\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe code compiles.\r\n\r\n**Other info / logs**\r\n\r\nBazel config:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Users/1/Anaconda3/envs/tensorflow/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Users/1/Anaconda3/envs/tensorflow/lib/site-packages\"\r\nbuild --python_path=\"C:/Users/1/Anaconda3/envs/tensorflow/python.exe\"\r\nbuild:ignite --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/cuDNN/7.3.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"5.2,6.1\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=/arch:AVX2\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --config monolithic\r\nbuild --copt=-w --host_copt=-w\r\nbuild --verbose_failures\r\nbuild --distinct_host_configuration=false\r\nbuild --experimental_shortened_obj_file_path=true\r\nbuild --define=no_tensorflow_py_deps=true\r\nbuild --define=override_eigen_strong_inline=true\r\nbuild:v2 --define=tf_api_version=2\r\n```", "comments": ["I do not think we have XLA support on windows yet.\r\nReassigning to Justin to reroute the issue in XLA team.\r\nJustin, is anyone working on XLA on windows?", "I don't believe anyone is working on XLA support on Windows, though I would certainly review a PR that fixed this.  I believe this static_assert *is* checking something important.", "As a side question, then: Is the fact that windows is not currently supported written anywhere in the documentation?\r\nI went through the pages of the [XLA documentation](https://www.tensorflow.org/xla/overview) and found no mention of specific OSes or supported configurations.\r\nThis is IMO a **very** important information to have there. Had I not posted this question, I would have never known Windows is not supported, and I posted this only after several attempts following multiple guides and with multiple TF versions. That's a lot of time spent on the lack of a \"currently we only support builds on Linux\" anywhere I could find in the docs. Or, if that's not possible for whatever reason, please at least include a compiler check that gives a *maningful* error message instead of an empty string.", "Vistual Studio does not pack adjacent bitfields if they are of different type. In this case, \"kind\" is of an enum type, treated as an unsigned int (32 bits), and \"size\" is uint64. The resulting class is 8+16 bytes in size. The fix is to declare \"kind\" also as uint64 and typecast the assignments. But even when you fix that you will run into other issues with files generated by the protocol buffer compiler. I haven't chased those down.\r\n\r\n", "@farimani Is this still an issue with new TF version? Could you check whether the issue persists? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@jvishnuvardhan I'm trying to buikd TensorFlow 1.13.0-rc2, and I still have this issue.", "@imnotkind Please open a new issue by filling the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md) and providing as many details as possible. Thanks!", "@jvishnuvardhan since I'm also still having the issue with 1.13-rc2 I opened a new issue with the full current configuration."]}, {"number": 24217, "title": " After compiling tensorflow for golang, it was found that python tensorflow could not be imported normally.", "body": "when i import tensorflow ,it happend errors.\r\npython 3.6.5, macOS\r\n----> 1 import tensorflow as tf\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py in <module>\r\n     22\r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25\r\n     26 try:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>\r\n     47 import numpy as np\r\n     48\r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50\r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75\r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKNSt3__112basic_stringIcNS4_11char_traitsIcEENS4_9allocatorIcEEEEPNS4_6vectorIPNS_6DeviceENS8_ISF_EEEE\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: /Users/wqq/go/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow_framework.so\r\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so", "comments": ["@wangqiqianggithub  We understand that you are having tensorflow import issues with golang. In order to proceed assisting you, request you to fill [this](https://github.com/tensorflow/tensorflow/issues/new/choose) template which helps us to look into this.\r\n\r\nAlso, please go through the below link which has clear instructions on using TF with Go. \r\n[Tensorflow in Go](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md)\r\n\r\n", "In fact, I have solved this problem. In the process, I learned that setting environment variables is not very scientific when compiling golang packages, which may affect the import of tensorflow by other languages. After I commented out the following two environment variables, my python was able to use tensorflow normally.`#settting path for golang-tensorflow\r\n  #export DYLD_LIBRARY_PATH=${GOPATH}/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow\r\n  #export LIBRARY_PATH=${GOPATH}/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow`\r\nWe might want to do the first, which is copy the compiled file into /usr/local/lib\u3002\r\n`sudo cp ${GOPATH}/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow.so /usr/local/lib`\r\nthe parper link is [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go](url).\r\nthank you all the same,be happy. @harshini-gadige "]}, {"number": 24216, "title": "Can't use tf.gradients to get the gradients of a tf.gather_np op", "body": "I wanna get the gradients of a tf.gather(params, indices) as for indices, but when I do gradients = tf.gradients(ys=out_1, xs=indices). Gradients is None.\r\n\r\n```\r\nparams = tf.constant(np.array([[1,2], [3,4]]))\r\nindices = tf.constant(np.array([[[0,0],[0,1]],[[1,1],[0,1]]]))\r\nw = tf.constant(np.random.randn(4,3), dtype=tf.float32)\r\nwith tf.Session() as sess:\r\n    out_1 = tf.gather_nd(params=params, indices=indices)\r\n    print(sess.run(out_1))\r\n    gradients = tf.gradients(ys=out_1, xs=indices)\r\n    print(gradients)\r\n    print(sess.run(gradients))\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-39-c4fb622d33f8> in <module>()\r\n     10     gradients = tf.gradients(ys=out_1, xs=indices)\r\n     11     print(gradients)\r\n---> 12     print(sess.run(gradients))\r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1135     # Create a fetch handler to take care of the structure of fetches.\r\n   1136     fetch_handler = _FetchHandler(\r\n-> 1137         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\r\n   1138 \r\n   1139     # Run request and get response.\r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in __init__(self, graph, fetches, feeds, feed_handles)\r\n    469     \"\"\"\r\n    470     with graph.as_default():\r\n--> 471       self._fetch_mapper = _FetchMapper.for_fetch(fetches)\r\n    472     self._fetches = []\r\n    473     self._targets = []\r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in for_fetch(fetch)\r\n    259     elif isinstance(fetch, (list, tuple)):\r\n    260       # NOTE(touts): This is also the code path for namedtuples.\r\n--> 261       return _ListFetchMapper(fetch)\r\n    262     elif isinstance(fetch, collections.Mapping):\r\n    263       return _DictFetchMapper(fetch)\r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in __init__(self, fetches)\r\n    368     \"\"\"\r\n    369     self._fetch_type = type(fetches)\r\n--> 370     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n    371     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)\r\n    372 \r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in <listcomp>(.0)\r\n    368     \"\"\"\r\n    369     self._fetch_type = type(fetches)\r\n--> 370     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]\r\n    371     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)\r\n    372 \r\n\r\nC:\\Users\\11198\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py in for_fetch(fetch)\r\n    256     if fetch is None:\r\n    257       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\r\n--> 258                                                                  type(fetch)))\r\n    259     elif isinstance(fetch, (list, tuple)):\r\n    260       # NOTE(touts): This is also the code path for namedtuples.\r\n\r\nTypeError: Fetch argument None has invalid type <class 'NoneType'>\r\n\r\n\r\n", "comments": ["tf.gather_nd returns A `Tensor` out_1, and out_1 is not a function of indices.", "> tf.gather_nd returns A `Tensor` out_1, and out_1 is not a function of indices.\r\n\r\nIf there is anyway to calculate the gradient of tf.gather_nd function?  I see there is a `def _GatherNdGrad(op, grad):` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L355", "@KrisHan999 This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Closing this support issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). Thanks!"]}, {"number": 24214, "title": "[Tensorrt] Cannot convert when the batch dimension is changed", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm trying to convert seq2seq model graph to trt graph. \r\nBut it seems that the Reshape OP cannot be converted when the batch dimension is changed.\r\nIf possible, do  you have any plan to provide support for this feature?\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@paduck86 You would have to use the dynamic mode if you have dynamic shapes ( set `is_dynamic_op=True` when you call `create_inference_graph`). Could you please try that and let us know if that changes anything?\r\n\r\nWe are still working on improving TF-TRT with dynamic shapes (e.g. caching). This would affect a lot of models such as object detection and seq2seq. @trevor-m \r\n", "Thank you very much for your reply.\r\nBut there's no change after setting is_dynamic_op=True.\r\nI am particularly struggling with attention module because the module change batch size on 3d*2d matmul operation.\r\nI'll try again tomorrow and if there's another change, I'll answer it again.", "Could you post the log. A small repro would be great too.\r\n\r\nDoes the conversion crash or some of engines fail during build/execution?", "TensorRT does not support any modification of the batch dimension. They are planning to add support in the future, but it will not be available for a while.\r\n\r\nFor 3d*2d matmul, I have a workaround that involve changing your model. This will avoid the reshapes on the batch dimension. Are you using tf.layers.Dense()?\r\n\r\nTry replacing your layers like so:\r\n```\r\ndef linear_proj(x, hidden_size, name='q'):\r\n      with tf.variable_scope(name):\r\n        kernel = tf.get_variable(\"kernel\", shape=[x.get_shape().as_list()[-1], hidden_size])\r\n        with tf.variable_scope(\"Tensordot\", reuse=tf.AUTO_REUSE):\r\n          kernel = tf.expand_dims(kernel, 0)\r\n          return tf.nn.conv1d(x, kernel, 1, 'VALID', data_format='NWC')\r\n\r\n# Instead of:\r\n#y = tf.layers.Dense(hidden_size, use_bias=False, name=\"q\")(x)\r\n# Use this:\r\ny = linear_proj(x, hidden_size)\r\n```\r\nYou will need to use TF1.13 or later (use master branch for best results).", "Closing this. Let us know if comment above is not clear."]}, {"number": 24213, "title": "Whether Tf has the operations table?", "body": "I want to know about TF, whether it has the operations table?\r\nCan any one tell me to know about it?\r\nThanks.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "> This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nSorry about that, thank you."]}, {"number": 24212, "title": "Bazel test fail. Error deduced conflicting types. ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Mac OS Mojave 10.14.1 (18B75)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: branch r1.12\r\n- Python version: 3.6.6 \r\n- Installed using virtualenv? pip? conda?: building is in a virtualenv created by `pyenv virtualenv 3.6.6 my_tf_build_env`\r\n- Bazel version (if compiling from source): from source. 0.15.0\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version:  no cuda, build only for cpu\r\n- GPU model and memory: Intel Core i7, 16 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI was following the tutorial to build tensorflow from source.  I was on the `r1.12` branch, root directory, typed `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`. Error Message:\r\n\r\n![image](https://user-images.githubusercontent.com/1506580/49625289-efd57f80-fa0f-11e8-9ac5-e66542c8ff04.png)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ncommands sequence is same as this [tutorial](https://www.tensorflow.org/install/source).\r\nIn the `./configure` part, my configuration is like below:\r\n\r\n![image](https://user-images.githubusercontent.com/1506580/49625409-61adc900-fa10-11e8-80bf-687a5ca32db0.png)\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Added a PR #24237 for the fix.", "> Added a PR #24237 for the fix.\r\n\r\nThank you. "]}, {"number": 24211, "title": "something between tf 1.2.1 -> 1.12.0 causing call stack issues in embedded python", "body": "https://github.com/pythonnet/pythonnet/issues/792\r\n\r\nThis code running in pythonnet now crashes:\r\n\r\n```\r\nimport tensorflow as tf\r\nflags = tf.app.flags\r\nflags.DEFINE_integer(\"batch_size\", 32, \"Number of samples per batch\")\r\n```\r\n\r\nin scriptcs:\r\n\r\n```\r\n> #r C:\\Python\\Python35_64b\\lib\\site-packages\\Python.Runtime.dll\r\n> using Python.Runtime;\r\n> PythonEngine.Initialize();\r\n> dynamic tf = Py.Import(\"tensorflow\");\r\n> tf.app.flags.DEFINE_integer(\"batch_size\", 32, \"Number of samples per batch\");\r\nValueError : call stack is not deep enough\r\n```\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Closing this issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). Thanks!"]}, {"number": 24210, "title": "Saving Tesnors as Image in C++ api", "body": "Hi ,\r\nIn tensorflow C++ api I want to save the output tensor as an image to a local file.\r\n `Status run_status = session->Run({{input_layer, resized_tensor}}, {output_layer}, {}, &outputs);`\r\nAfter running session my output is actually a tensor of size 1,720,1280,1. How do I directly save it to a local binary file or image without using opencv?\r\n\r\n\r\n- TensorFlow version :1.8\r\n- Are you willing to contribute it (Yes/No): Yes\r\n- Ubuntu 14.0.4\r\n\r\n\r\n\r\n", "comments": ["Same problem", "> In tensorflow C++ api I want to save the output tensor as an image to a local file.\r\n> `Status run_status = session->Run({{input_layer, re`\r\n\r\n@hondaathma Did you solve the problem?\r\nIf so, please share the solution with me.", "@sukritiramesh Can you please take a look? Thanks!", "> @sukritiramesh Can you please take a look? Thanks!\r\n\r\n@ymodak @sukritiramesh I have the same problem than the creator of the post. \r\nIs there a solution to solve the problem? ", "@ricardobnjunior I could not solve the problem...so instead I std::cout (printed) to a text file all the values.\r\nI am having trouble building tensorflow with opencv or converted the tensor to cv::Mat. This si the only solution that works for me", "Take a look at similar [attempt](https://github.com/PatWie/tensorflow-cmake/issues/1) made by another user. Closing this issue since its not a bug/feature request. Feel free to reach out to [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) community for follow up questions related to this issue. Thanks!"]}, {"number": 24209, "title": "Tensorflow-GPU ImportError: DLL load failed", "body": "### System information\r\n- *OS:* Windows 10\r\n- **TensorFlow version**: Tensorflow v 1.12\r\n- **Python version**: 3.6.0\r\n- **CUDA/cuDNN version**: CUDA 10.0, cuDNN 7.4.1.5\r\n- **GPU model and memory**: 2080 ti\r\n\r\nI'm trying to get Tensorflow set-up on my machine. I've followed about 10 different tutorials but always come back to this same issue. I see that a bunch of other people have the same issue and just downgraded their CUDA to 9.0 due to Tensorflow not supporting 10.0. My problem is that the 2080 ti is only supported with CUDA 10.0\r\n\r\nIs anyone else using one of the RTX cards and have they had any of the same issues? My log is below.\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n", "comments": ["If you want to use CUDA 10, then you have to build TF from sources yourself. You can refer this [tutorial](https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/) created by other user who attempts to do the same.", "@Cavan09 \r\nI suggest you use CUDA 9.0", "@baihualinxin Do you know if I can use 9.0 with a 2080 ti. When I install 9.0 Nvidia throws a warning that my GPU is not supported and required 10.0.\r\n\r\n@ymodak I'll give this tutorial a try and let you know if it worked, thanks :)", "@ymodak \r\nSo I've finally got everything built after about 5 different configurations. Now when I hit the last step of the tutorial where I verify the install, I get the following error:\r\n\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\cmacphail\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nAny ideas?", "> @Cavan09\r\n> I suggest you use CUDA 9.0\r\n\r\n\r\nI installed CUDA9 and got this issue too.\r\n\r\nOS: Windows 10\r\nTensorFlow version: Tensorflow v 1.12\r\nPython version: 3.6\r\nCUDA/cuDNN version: CUDA 9, cuDNN 7\r\n\r\nAnyone know how to solve it?", "Okay I just got everything working....\r\n\r\nSo my setup is:\r\nTensorflow 1.12 (Build from source myself, using bazel)\r\nCUDA: 10.0\r\ncuDNN: 7.3.1\r\nPython: 3.6\r\nProtoc: 3.6.0 (started with 3.6.1 but had an issue with it)\r\n\r\nWhat i did was follow the tutorial mentioned above by @ymodak to build tensorflow from source. If you are using any new graphics cards (10xx or 20xx) make sure you check the comment section of the tutorial. There is a bug with the new cards which requires a patch. The bug is in the eigen part of tensorflow, which requires you to copy a patch into the third party folder. There is also a file, workspace.bzl, in tensorflow\\tensorflow\\tensorflow\\ which requires a small update to point to the patch file. This is provided in the comment section. I got stuck on this for a while as there is another workspace file in all caps, that is the wrong one!\r\n\r\nOnce I get all that working and tensorflow built successfully, I had the above issue. In order to fix it I had to downgrade my Protoc to 3.6.0.\r\n\r\nThere were a few warnings thrown, but I could properly run tensorflow and everything worked in my testing by comparison to previous runs I had done :)\r\n", "@tynefung \r\nWhat is your system setup? If it's similar to mine, you should look into building from source, take a look at my above comment and if you have any questions or get stuck I can help you out :)", "@Cavan09  \r\nIt worked after I rebooted.\r\nI think my issue is just caused by PATH.       \r\nT^T\r\nThank you for your kindly reply!\r\n\r\n", "Closing since this is resolved. Thanks @Cavan09 for sharing your workflow. "]}, {"number": 24208, "title": "Can Tensorflow (not lite) run on Android NDK C++", "body": "It looks like features like Dropout are not supported on Tensorflow Lite + Android NDK.\r\n\r\nI've got a Keras model that uses GPU, Dropout, CuDnnLSTM layer and I want to convert it to a tflite file or run it on Tensorflow for Android.\r\n\r\nIs this possible? Is there a guide that tells me what is and is not supported on tflite?", "comments": ["Yes. Here is an example of using TensorFlow from C++ on Android. \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24207, "title": "[tf.data] Adds eager coverage for MakeBatchedFeaturesDatasetTest", "body": "This PR enables `MakeBatchedFeaturesDatasetTest` to work in the eager mode. \r\n\r\ncc'ed @rachellim ", "comments": ["@shivaniag Could you please help review this PR when you have time?", "@feihugis Thank you for your contribution, there are efforts going on adding eager coverage for all dataset tests. I am closing this as this is covered with the bigger changes that are pending to be merged. "]}, {"number": 24206, "title": "Add subdirectory-output support and change official releases", "body": "This PR does a few things:\r\n\r\n* Add slice option `dockerfile_subdirectory` for https://github.com/tensorflow/tensorflow/pull/24180, a slice option that places any Dockerfile using the slice into a subdirectory of the Dockerfiles directory.\r\n* Rearrange releases and slices slightly. The `devel` images are now going to be built nightly, to best support master, and are tagged `devel-*`.", "comments": []}, {"number": 24203, "title": "Markdown formatting issue in tf.keras.layers.Layer docs", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: r1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\r\n\r\n\r\n**Describe the documentation issue**\r\nBelow the 'Arguments' list, it looks like there are supposed to be lists of read-only properties and mutable properties, but they render as paragraphs rather than lists.\r\n\r\nSeems like the issue is that the parser splits the docstring into sections based on a magic list of keywords, `detail_keywords = '|'.join(['Args', 'Arguments', 'Fields', 'Returns', 'Yields', 'Raises', 'Attributes'])`, and that list does not include the strings 'Read-only properties', or 'Mutable properties'. So it treats everything below \"Arguments\" as part of the list of arguments, and trips over itself.\r\n\r\n(There's also some question as to whether these lists should be there at all, or whether this information should be in the [Properties](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#properties) section)\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nNot with my current understanding of the docs system. I was able to find a script for generating a markdown index of the docs locally (`tensorflow/tools/docs/generate2.py`), but when I ran it, I got [a markdown file that didn't seem to have the issue I see on the site](https://gist.github.com/colinmorris/0a180ea671e12ccdb65fe6cc9f0319fa). Maybe it's because I used `generate2.py` but the docs on the site are generated by `generate.py`? Maybe the lists are fine in GitHub-flavoured markdown but not whatever engine is used when converting the docs from md to html for the site? (I'll file a separate issue about this meta-documentation issue)", "comments": ["Apologies for the delay in response. You are right about this. Thanks for reporting.", "This should be fixed by tomorrow or day-after tomorrow."]}, {"number": 24202, "title": "Saved model incompatible betweeen v1.4 and v1.8", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Ubuntu 14.04 / Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source / Binary\r\n- TensorFlow version (use command below): 1.4 / 1.8\r\n- Python version: 2.7 / 2.7\r\n- Bazel version (if compiling from source): 0.5.4 / 0.14.0\r\n- GCC/Compiler version (if compiling from source): 4.8.4 / 7.3.0\r\n- CUDA/cuDNN version: 8.0/6.0.21 / None\r\n- GPU model and memory: Nvidia GTX1080 + 32GB RAM / No GPU + 16GB RAM\r\n\r\n\r\nI trained a network using a machine running Ubuntu 14.04 + TF v1.4 (running on GPU), and then tried to restore the model on another machine running Ubuntu 18.04 + TF v.18 (running on CPU). Note that I don't restore the .meta file; rather I run the same Python code on both machines to build the graph, and then restore the .data-00000-of-00001 file.\r\n\r\nThe problem is that even though there are no errors in running the script, the output of the network is completely different as compared to when run on the original machine (the output is mostly 0 everywhere). Is there anything I can change to fix this issue? I thought saved models were cross-compatible for the same major release (v1.x)\r\n\r\nIn the info above, I have listed the details in the format <SPEC FOR 14.04 MACHINE> / <SPEC FOR 18.04 MACHINE>.\r\n", "comments": ["Issue resolved. Was unrelated to the TensorFlow saved model.\r\n\r\nUbuntu 18.04 comes with OpenCV 3.4, which parses the EXIF meta info in images, so basically my images were being rotated from their original orientation before being input to the network. This is what was causing the different outputs."]}, {"number": 24201, "title": "#23976 : Resolve ImportError: cannot import name cloud on s390x ", "body": "As per comments in #23976, adding check for s390x ", "comments": ["@gunan checked logs for failing CI builds. Could see error  `AttributeError: class Tokenizer has no attribute '__mro__'`\r\nNot sure if its related to the code patch introduced in this PR", "@Nayana-ibm  @gunan  Please address the test failures.", "@gunan @meteorcloudy Checked logs of failing CI builds - GPU Python3, MacOS Contrib etc.  Tests are failing with - `error code 134` or `ImportError: No module named keras_preprocessing`\r\n\r\nDoesn't look like its related to the code patch introduced in this PR. Could you please guide me on this? Merging is blocked due to these failures. ", "@gunan @meteorcloudy When this can be merged?", "@Nayana-ibm sorry for the delay, I'm trying to re-submit this change internally.", "@meteorcloudy Thanks "]}, {"number": 24200, "title": "TypeError: Expected binary or unicode string, got None", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 16.04\r\n- TensorFlow version (use command below):\r\n- Python version 3.5:\r\n- CUDA/cuDNN version9.0:\r\n- GPU model and memory: tesla k80\r\n\r\n\r\n", "comments": ["``` python\r\nmodel {\r\n  ssd {\r\n    num_classes: 90\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.2\r\n        max_scale: 0.95\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.3333\r\n        reduce_boxes_in_lowest_layer: true\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 300\r\n        width: 300\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.8\r\n        kernel_size: 3\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              stddev: 0.03\r\n              mean: 0.0\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_inception_v2'\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            stddev: 0.03\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          train: true,\r\n          scale: true,\r\n          center: true,\r\n          decay: 0.9997,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid {\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      hard_example_miner {\r\n        num_hard_examples: 3000\r\n        iou_threshold: 0.99\r\n        loss_type: CLASSIFICATION\r\n        max_negatives_per_positive: 3\r\n        min_negatives_per_image: 0\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 24\r\n  optimizer {\r\n    rms_prop_optimizer: {\r\n      learning_rate: {\r\n        exponential_decay_learning_rate {\r\n          initial_learning_rate: 0.004\r\n          decay_steps: 800720\r\n          decay_factor: 0.95\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n      decay: 0.9\r\n      epsilon: 1.0\r\n    }\r\n  }\r\n  fine_tune_checkpoint: \"~/models/research/object_detection/ssd_inception_v2_coco_2018_01_28/model.ckpt.*\"\r\n  from_detection_checkpoint: true\r\n  # Note: The below line limits the training process to 200K steps, which we\r\n  # empirically found to be sufficient enough to train the pets dataset. This\r\n  # effectively bypasses the learning rate schedule (the learning rate will\r\n  # never decay). Remove the below line to train indefinitely.\r\n  num_steps: 200000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"~/models/research/object_detection/training/coco_train.record-?????-of-00100\"\r\n  }\r\n  label_map_path: \"~/models/research/object_detection/data/mscoco_label_map.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 8000\r\n  # Note: The below line limits the evaluation process to 10 evaluations.\r\n  # Remove the below line to evaluate indefinitely.\r\n  max_evals: 10\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"~/models/research/object_detection/training/coco_val.record-?????-of-00010\"\r\n  }\r\n  label_map_path: \"~/models/research/object_detection/data/mscoco_label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n\r\n```", "``` python\r\npython model_main.py --model_\r\ndir=~/models/research/object_detection/train/ \\ --pipeline_config_path=~/models/research/ob\r\nject_detection/samples/configs/ssd_inception_v2_coco.config \\ --alsologtostderr\r\n```", "and i have\r\n``` python\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/home/taras_saveniuk/.local/lib/python3.5/site-packages/tensorflow/python/platform/\r\napp.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 71, in main\r\n    FLAGS.sample_1_of_n_eval_on_train_examples))\r\n  File \"/home/taras_saveniuk/models/research/object_detection/model_lib.py\", line 536, in c\r\nreate_estimator_and_inputs\r\n    config_override=config_override)\r\n  File \"/home/taras_saveniuk/models/research/object_detection/utils/config_util.py\", line 9\r\n5, in get_configs_from_pipeline_file\r\n    proto_str = f.read()\r\n  File \"/home/taras_saveniuk/.local/lib/python3.5/site-packages/tensorflow/python/lib/io/fi\r\nle_io.py\", line 125, in read\r\n    self._preread_check()\r\n  File \"/home/taras_saveniuk/.local/lib/python3.5/site-packages/tensorflow/python/lib/io/fi\r\nle_io.py\", line 85, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/home/taras_saveniuk/.local/lib/python3.5/site-packages/tensorflow/python/util/comp\r\nat.py\", line 61, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got None\r\n```", "Apologies for the delay in response. This issue is more suitable for TensorFlow Models repo. Please post it on models repo from [here](https://github.com/tensorflow/models/issues/new). Thanks!", "How did you resolve this?  I am having the same error report.\r\n\r\nAppreciate a reply.   Thanks\r\n\r\n(tlearn) ramesh@sentry-ramesh:~/projects/perid/tensorflow/models/research$ python object_detection/model_main.py \\ --pipeline_config_path=$/home/ramesh/projects/perid/tensorflow/models/training/models/ssd_mobilenet_v1_coc.config \\ --model_dir=$/home/ramesh/projects/perid/tensorflow/models/training/models \\ --num_train_steps=$2000 \\ --sample_1_of_n_eval_examples=$2 \\ --alsologtostderr\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 71, in main\r\n    FLAGS.sample_1_of_n_eval_on_train_examples))\r\n  File \"/home/ramesh/projects/perid/tensorflow/models/research/object_detection/model_lib.py\", line 566, in create_estimator_and_inputs\r\n    config_override=config_override)\r\n  File \"/home/ramesh/projects/perid/tensorflow/models/research/object_detection/utils/config_util.py\", line 95, in get_configs_from_pipeline_file\r\n    proto_str = f.read()\r\n  File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 125, in read\r\n    self._preread_check()\r\n  File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 85, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 61, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got None\r\n", "> How did you resolve this? I am having the same error report.\r\n> \r\n> Appreciate a reply. Thanks\r\n> \r\n> (tlearn) ramesh@sentry-ramesh:~/projects/perid/tensorflow/models/research$ python object_detection/model_main.py \\ --pipeline_config_path=$/home/ramesh/projects/perid/tensorflow/models/training/models/ssd_mobilenet_v1_coc.config \\ --model_dir=$/home/ramesh/projects/perid/tensorflow/models/training/models \\ --num_train_steps=$2000 \\ --sample_1_of_n_eval_examples=$2 \\ --alsologtostderr\r\n> Traceback (most recent call last):\r\n> File \"object_detection/model_main.py\", line 109, in \r\n> tf.app.run()\r\n> File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n> _sys.exit(main(argv))\r\n> File \"object_detection/model_main.py\", line 71, in main\r\n> FLAGS.sample_1_of_n_eval_on_train_examples))\r\n> File \"/home/ramesh/projects/perid/tensorflow/models/research/object_detection/model_lib.py\", line 566, in create_estimator_and_inputs\r\n> config_override=config_override)\r\n> File \"/home/ramesh/projects/perid/tensorflow/models/research/object_detection/utils/config_util.py\", line 95, in get_configs_from_pipeline_file\r\n> proto_str = f.read()\r\n> File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 125, in read\r\n> self._preread_check()\r\n> File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 85, in _preread_check\r\n> compat.as_bytes(self.__name), 1024 * 512, status)\r\n> File \"/home/ramesh/anaconda3/envs/tlearn/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 61, in as_bytes\r\n> (bytes_or_text,))\r\n> TypeError: Expected binary or unicode string, got None\r\n\r\nWere you able to solve this? @rasastry "]}, {"number": 24199, "title": "Python 3.6.7 DLL  load failed on GPU version", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro Build 17134\r\n- TensorFlow installed from (source or binary): Package (pip install tensorflow-gpu)\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA 9/CuDNN 7.4.1.5\r\n- GPU model and memory: GTX 1070ti 416.34\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have been working with some code in keras that works fine with the cpu version of tensorflow (pip install tensorflow)  Since each epoch was taking a minute to run, I wanted to throw my 1070ti at the problem. I installed tensorflow-gpu (pip install tensorflow-gpu) along with CUDA and CuDNN.  I then did a quick restart but when I went to run my code again it didn't make keras use the gpu version of tensorflow.  I uninstalled tensorflow (pip uninstall tensorflow) gave it another restart and now getting the following error with the DLL load failing.  Reinstalling the cpu version of Tensorflow getting it working again but only on the CPU.  I can't see what I have done wrong but any help is appreciated.  I have a look through some of the past issues but the solutions only seem to be for the CPU package not importing.  Might have missed some finesse there so please do point out the obvious.\r\n\r\n**Any other info / logs**\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Did you set up the environment variables as well after tensorflow-gpu installation? Please take a look at [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup) to know more and confirm.", "![image](https://user-images.githubusercontent.com/24856132/49603581-96277380-f983-11e8-81df-12bf1a7399a7.png)\r\n\r\n\r\nYeah, think I did it right, but not done this thing before so let me know if I set it up wrong.", "The required dll files should be in the \\bin folder. So you have to add a path to it. \r\nTensorFlow will not load without the cuDNN64_7.dll file. This file should be in the cuDNN folder under bin. You can copy this file and paste it  on C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\n\r\nNow create new path to locate those files:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\n", "\r\n\r\nAh ok, I have already installed cudnn by putting the files into the correctly so I clicked edit on the Path system variable and added the path to the variable and saved it.  See attached screenshot, do let me know if I did it wrong, \r\n![image](https://user-images.githubusercontent.com/24856132/49607590-587c1800-f98e-11e8-83ec-2df2933a9426.png)\r\n", "@mrry Can you please take a look? Thanks!", "Sorry, I won't have time to look at this.", "Been playing around with my code and swapped Keras to try to use CNTK-GPU and it has managed to achieve some partial success so I think I set CUDA up correctly.  It is still limited to 5.7% of the GPU capacity, Going to investigate that but I think that is an indication the issue isn't with how I set up CUDA.", "@ymodak Any chance anyone else can potentially help cause I would really like to use tensorflow because of the better community support and cause keras support multi-gpu with tensorflow only as far as I can tell.", "@gunan Can you please take a look? Thanks!", "This is either a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\r\nOr missing MSVC redisttributable installation.", "\n\n\n  \n  \n  \n    \n    \tSince I have an i7-4790 which I believe supports AVX that would\nmean it is an issue with MSVC then.  What would be the correct way to\nfix it as I do believe I have it correctly installed.  Can provide\nscreenshots when I am home in a couple hours.\n    \t\n    \t\n\n    \t\n    \n  \n\n\n\n\nOn Thu, Dec 13, 2018 at 7:00 PM +0000, \"Gunhan Gulsoy\"\n<notifications@github.com> wrote:\n\n\n\n\n\n\n\n\n\n\n\n\nThis is either a duplicate of #19584\n\nOr missing MSVC redisttributable installation.\n\n\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\n\n\n\n\n>>> \"Gunhan Gulsoy\" <notifications@github.com>\n2018-12-13T17:57:10.009314 >>>\nThis is either a duplicate of\nhttps://github.com/tensorflow/tensorflow/issues/19584\nOr missing MSVC redisttributable installation.\n\n-- \nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/24199#issuecomment-447058907\n", "@gunan Screenshot of all installed MSVC versions.  Is there a specific one I am missing?\r\n![image](https://user-images.githubusercontent.com/24856132/49975802-6bb35880-ff37-11e8-9ee0-271122639816.png)\r\n", "You need Microsoft visual C++ redistributable 2015 update 3, if I am not mistaken.\r\nWe need to find a way to add a check to our library to not fail and print a cryptic message, but actually inform users of this is the missing library.\r\n\r\nSorry for the inconvenience.", "@gunan so I just gave that a try, had to uninstall the 2017 versions to get the 2015 one to install correctly.  I then reinstalled tensorflow-gpu via pip.  Trying import tensorflow-gpu gave the exact same traceback so I don't think that had done much?  Have you got any other ideas of what might be wrong?\r\n![image](https://user-images.githubusercontent.com/24856132/50001864-1e1a0880-ff96-11e8-9a9e-404b12271ff8.png)\r\n![image](https://user-images.githubusercontent.com/24856132/50001985-89fc7100-ff96-11e8-91ae-964c241b77bf.png)\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n```\r\n\r\nSorry to be a pain by not being an easy fix.\r\n", "```>>> import tensorflowTraceback (most recent call last):  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>    from tensorflow.python.pywrap_tensorflow_internal import *  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>    _pywrap_tensorflow_internal = swig_import_helper()  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module    return load_dynamic(name, filename, file)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic    return _load(spec)ImportError: DLL load failed: The specified module could not be found.During handling of the above exception, another exception occurred:Traceback (most recent call last):  File \"<pyshell#0>\", line 1, in <module>    import tensorflow  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>    from tensorflow.python import pywrap_tensorflow  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>    raise ImportError(msg)ImportError: Traceback (most recent call last):  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>    from tensorflow.python.pywrap_tensorflow_internal import *  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>    _pywrap_tensorflow_internal = swig_import_helper()  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module    return load_dynamic(name, filename, file)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic    return _load(spec)ImportError: DLL load failed: The specified module could not be found.```", "```>>> import tensorflowTraceback (most recent call last):  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>    from tensorflow.python.pywrap_tensorflow_internal import *  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>    _pywrap_tensorflow_internal = swig_import_helper()  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module    return load_dynamic(name, filename, file)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic    return _load(spec)ImportError: DLL load failed: The specified module could not be found.During handling of the above exception, another exception occurred:Traceback (most recent call last):  File \"<pyshell#0>\", line 1, in <module>    import tensorflow  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>    from tensorflow.python import pywrap_tensorflow  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>    raise ImportError(msg)ImportError: Traceback (most recent call last):  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>    from tensorflow.python.pywrap_tensorflow_internal import *  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>    _pywrap_tensorflow_internal = swig_import_helper()  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module    return load_dynamic(name, filename, file)  File \"C:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic    return _load(spec)ImportError: DLL load failed: The specified module could not be found.```", "Ah sorry hit the wrong button", "Sorry, I am out of ideas. @ymodak, at one point, we had a diagnosis script to run on windows, do we have a link to that?\r\nIt should be referenced a lot in old issues.", "@gunan Can you please confirm if this is the correct link [Dependency Walker](https://github.com/tensorflow/tensorflow/issues/23277#issuecomment-434878832) for windows diagnostics?\r\n@Naith123 Also refer #23277", "Yes, it is.\nPlease try running that on your pywrap_tensorflow_internal.dll\n\n\nOn Fri, Dec 14, 2018 at 11:35 AM ymodak <notifications@github.com> wrote:\n\n> @gunan <https://github.com/gunan> Can you confirm if this is the correct\n> link Dependency Walker\n> <https://github.com/tensorflow/tensorflow/issues/23277#issuecomment-434878832>\n> for windows diagnostics?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24199#issuecomment-447431056>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOQVcrXCSVz5KN5xv2BymsZWhH_djks5u4_18gaJpZM4ZGvUj>\n> .\n>\n", "so this is a little weird @gunan, I went into the tensorflow folder in appdata and I could not for the life of me find pywrap_tensorflow_internal.dll\r\n\r\n![image](https://user-images.githubusercontent.com/24856132/50043747-b2a56900-0071-11e9-80be-f0e6e143b078.png)\r\n\r\nI ran it on the _pywrap_tensorflow_internal.pyd file as the issue @ymodak  linked to showed and got a whole host of errors. Everything came up with Error opening file, The system cannot find the file specified (2)  Not sure if I did it wrong or if there is something seriously wrong with my PC.\r\nAPI-MS-WIN-CORE-APIQUERY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-APPCOMPAT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-APPCOMPAT-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-APPINIT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-ATOMS-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-COMM-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-CONSOLE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-CONSOLE-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-CONSOLE-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-CONSOLE-L2-2-0.DLL\r\nAPI-MS-WIN-CORE-CONSOLE-L3-2-0.DLL\r\nAPI-MS-WIN-CORE-CRT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-CRT-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-DATETIME-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-DATETIME-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-DATETIME-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-DEBUG-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-DEBUG-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-DELAYLOAD-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-DELAYLOAD-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-ERRORHANDLING-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-ERRORHANDLING-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-ERRORHANDLING-L1-1-3.DLL\r\nAPI-MS-WIN-CORE-FIBERS-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-FIBERS-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-FIBERS-L2-1-1.DLL\r\nAPI-MS-WIN-CORE-FILE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-FILE-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-FILE-L1-2-1.DLL\r\nAPI-MS-WIN-CORE-FILE-L1-2-2.DLL\r\nAPI-MS-WIN-CORE-FILE-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-FILE-L2-1-1.DLL\r\nAPI-MS-WIN-CORE-FILE-L2-1-2.DLL\r\nAPI-MS-WIN-CORE-FILE-L2-1-3.DLL\r\nAPI-MS-WIN-CORE-HANDLE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-HEAP-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-HEAP-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-HEAP-OBSOLETE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-INTERLOCKED-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-IO-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-IO-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-JOB-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-JOB-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-KERNEL32-LEGACY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-KERNEL32-LEGACY-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-KERNEL32-LEGACY-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-KERNEL32-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-LARGEINTEGER-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-LIBRARYLOADER-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-LIBRARYLOADER-L1-2-1.DLL\r\nAPI-MS-WIN-CORE-LIBRARYLOADER-L1-2-2.DLL\r\nAPI-MS-WIN-CORE-LIBRARYLOADER-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-LOCALIZATION-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-LOCALIZATION-L1-2-2.DLL\r\nAPI-MS-WIN-CORE-LOCALIZATION-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-LOCALIZATION-OBSOLETE-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-LOCALIZATION-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-MEMORY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-MEMORY-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-MEMORY-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-MEMORY-L1-1-3.DLL\r\nAPI-MS-WIN-CORE-MISC-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-NAMEDPIPE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-NAMEDPIPE-L1-2-1.DLL\r\nAPI-MS-WIN-CORE-NAMEDPIPE-L1-2-2.DLL\r\nAPI-MS-WIN-CORE-NAMESPACE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-NORMALIZATION-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PATH-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PERFCOUNTERS-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PRIVATEPROFILE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PROCESSENVIRONMENT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PROCESSENVIRONMENT-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-PROCESSSNAPSHOT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PROCESSTHREADS-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PROCESSTHREADS-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-PROCESSTHREADS-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-PROCESSTHREADS-L1-1-3.DLL\r\nAPI-MS-WIN-CORE-PROCESSTOPOLOGY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PROFILE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PSAPI-ANSI-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PSAPI-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-REALTIME-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-REGISTRY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-REGISTRY-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-REGISTRY-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-REGISTRYUSERSPECIFIC-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-RTLSUPPORT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SHLWAPI-LEGACY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SHLWAPI-OBSOLETE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SIDEBYSIDE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-STRING-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-STRING-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-STRING-L2-1-1.DLL\r\nAPI-MS-WIN-CORE-STRING-OBSOLETE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-STRINGANSI-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SYNCH-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SYNCH-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-SYNCH-L1-2-1.DLL\r\nAPI-MS-WIN-CORE-SYSINFO-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SYSINFO-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-SYSINFO-L1-2-1.DLL\r\nAPI-MS-WIN-CORE-SYSINFO-L1-2-3.DLL\r\nAPI-MS-WIN-CORE-SYSTEMTOPOLOGY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SYSTEMTOPOLOGY-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-THREADPOOL-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-THREADPOOL-LEGACY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-THREADPOOL-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-TIMEZONE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-URL-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-UTIL-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-VERSION-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-VERSION-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-VERSION-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-VERSIONANSI-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-VERSIONANSI-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-WINDOWSERRORREPORTING-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINDOWSERRORREPORTING-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-WINDOWSERRORREPORTING-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-WINRT-ERROR-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WOW64-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WOW64-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-XSTATE-L2-1-0.DLL\r\nAPI-MS-WIN-CRT-CONIO-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-CONVERT-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-ENVIRONMENT-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-FILESYSTEM-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-HEAP-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-LOCALE-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-MATH-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-PROCESS-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-RUNTIME-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-STDIO-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-STRING-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-TIME-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-UTILITY-L1-1-0.DLL\r\nAPI-MS-WIN-DEVICES-CONFIG-L1-1-1.DLL\r\nAPI-MS-WIN-EVENTING-CLASSICPROVIDER-L1-1-0.DLL\r\nAPI-MS-WIN-EVENTING-CONSUMER-L1-1-0.DLL\r\nAPI-MS-WIN-EVENTING-CONSUMER-L1-1-1.DLL\r\nAPI-MS-WIN-EVENTING-CONTROLLER-L1-1-0.DLL\r\nAPI-MS-WIN-EVENTING-PROVIDER-L1-1-0.DLL\r\nAPI-MS-WIN-GDI-INTERNAL-UAP-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-APPCONTAINER-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-AUDIT-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-AUDIT-L1-1-1.DLL\r\nAPI-MS-WIN-SECURITY-BASE-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-BASE-L1-2-0.DLL\r\nAPI-MS-WIN-SECURITY-BASE-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-CAPABILITY-L1-1-0.DLL\r\nAPI-MS-WIN-SERVICE-CORE-L1-1-0.DLL\r\nAPI-MS-WIN-SERVICE-CORE-L1-1-1.DLL\r\nAPI-MS-WIN-SERVICE-CORE-L1-1-2.DLL\r\nAPI-MS-WIN-SERVICE-MANAGEMENT-L1-1-0.DLL\r\nAPI-MS-WIN-SERVICE-MANAGEMENT-L2-1-0.DLL\r\nAPI-MS-WIN-SERVICE-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-SERVICE-PRIVATE-L1-1-2.DLL\r\nAPI-MS-WIN-SERVICE-PRIVATE-L1-1-3.DLL\r\nAPI-MS-WIN-SERVICE-PRIVATE-L1-1-4.DLL\r\nAPI-MS-WIN-SERVICE-WINSVC-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-PATH-L1-1-0.DLL\r\nAPI-MS-WIN-SHELL-SHELLCOM-L1-1-0.DLL\r\nAPI-MS-WIN-SHELL-SHELLFOLDERS-L1-1-0.DLL\r\nAPI-MS-WIN-STATESEPARATION-HELPERS-L1-1-0.DLL\r\nAPI-MS-WIN-STORAGE-EXPORTS-EXTERNAL-L1-1-0.DLL\r\nAPI-MS-WIN-STORAGE-EXPORTS-INTERNAL-L1-1-0.DLL\r\nCUDART64_90.DLL\r\nCUFFT64_90.DLL\r\nCURAND64_90.DLL\r\nCUSOLVER64_90.DLL\r\nAPI-MS-WIN-APPMODEL-IDENTITY-L1-2-0.DLL\r\nAPI-MS-WIN-APPMODEL-RUNTIME-INTERNAL-L1-1-0.DLL\r\nAPI-MS-WIN-APPMODEL-RUNTIME-INTERNAL-L1-1-1.DLL\r\nAPI-MS-WIN-APPMODEL-RUNTIME-INTERNAL-L1-1-2.DLL\r\nAPI-MS-WIN-APPMODEL-RUNTIME-L1-1-0.DLL\r\nAPI-MS-WIN-APPMODEL-RUNTIME-L1-1-1.DLL\r\nAPI-MS-WIN-APPMODEL-STATE-L1-2-0.DLL\r\nAPI-MS-WIN-APPMODEL-UNLOCK-L1-1-0.DLL\r\nAPI-MS-WIN-BASE-UTIL-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-CALENDAR-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-COM-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-COM-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-COM-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-COM-L2-1-1.DLL\r\nAPI-MS-WIN-CORE-COM-MIDLPROXYSTUB-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-COM-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-COM-PRIVATE-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-COM-PRIVATE-L1-2-0.DLL\r\nAPI-MS-WIN-CORE-DEBUG-MINIDUMP-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-FEATURESTAGING-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-FIBERS-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-FIRMWARE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-KERNEL32-PRIVATE-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-KERNEL32-PRIVATE-L1-1-2.DLL\r\nAPI-MS-WIN-CORE-LIBRARYLOADER-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-LOCALIZATION-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-LOCALREGISTRY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-MARSHAL-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-MEMORY-L1-1-5.DLL\r\nAPI-MS-WIN-CORE-PRIVATEPROFILE-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-PROCESSTOPOLOGY-OBSOLETE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PSM-APP-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PSM-APPNOTIFY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-PSM-KEY-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-QUIRKS-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-REALTIME-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-REGISTRY-L2-1-0.DLL\r\nAPI-MS-WIN-CORE-REGISTRY-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-SHUTDOWN-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-TOOLHELP-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINRT-ERROR-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-WINRT-ERRORPRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINRT-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINRT-PROPERTYSETPRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINRT-PROPERTYSETPRIVATE-L1-1-1.DLL\r\nAPI-MS-WIN-CORE-WINRT-REGISTRATION-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINRT-ROBUFFER-L1-1-0.DLL\r\nAPI-MS-WIN-CORE-WINRT-STRING-L1-1-0.DLL\r\nAPI-MS-WIN-COREUI-SECRUNTIME-L1-1-0.DLL\r\nAPI-MS-WIN-CRT-MULTIBYTE-L1-1-0.DLL\r\nAPI-MS-WIN-DEVICES-QUERY-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-ADVAPI32-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-ADVAPI32-L2-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-KERNEL32-L2-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-NORMALIZ-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-OLE32-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-SHELL32-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-SHLWAPI-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-SHLWAPI-L2-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-USER32-L1-1-0.DLL\r\nAPI-MS-WIN-DOWNLEVEL-VERSION-L1-1-0.DLL\r\nAPI-MS-WIN-DWMAPI-L1-1-0.DLL\r\nAPI-MS-WIN-DX-D3DKMT-L1-1-0.DLL\r\nAPI-MS-WIN-DX-D3DKMT-L1-1-1.DLL\r\nAPI-MS-WIN-DX-D3DKMT-L1-1-2.DLL\r\nAPI-MS-WIN-DX-D3DKMT-L1-1-3.DLL\r\nAPI-MS-WIN-DX-D3DKMT-L1-1-4.DLL\r\nAPI-MS-WIN-EVENTING-LEGACY-L1-1-0.DLL\r\nAPI-MS-WIN-EVENTING-OBSOLETE-L1-1-0.DLL\r\nAPI-MS-WIN-EVENTING-TDH-L1-1-0.DLL\r\nAPI-MS-WIN-EVENTLOG-LEGACY-L1-1-0.DLL\r\nAPI-MS-WIN-GDI-DPIINFO-L1-1-0.DLL\r\nAPI-MS-WIN-HTTP-TIME-L1-1-0.DLL\r\nAPI-MS-WIN-MM-JOYSTICK-L1-1-0.DLL\r\nAPI-MS-WIN-MM-MISC-L1-1-0.DLL\r\nAPI-MS-WIN-MM-MISC-L2-1-0.DLL\r\nAPI-MS-WIN-MM-MME-L1-1-0.DLL\r\nAPI-MS-WIN-MM-TIME-L1-1-0.DLL\r\nAPI-MS-WIN-NETWORKING-INTERFACECONTEXTS-L1-1-0.DLL\r\nAPI-MS-WIN-NTUSER-RECTANGLE-L1-1-0.DLL\r\nAPI-MS-WIN-NTUSER-SYSPARAMS-L1-1-0.DLL\r\nAPI-MS-WIN-OLE32-IE-L1-1-0.DLL\r\nAPI-MS-WIN-OOBE-NOTIFICATION-L1-1-0.DLL\r\nAPI-MS-WIN-POWER-BASE-L1-1-0.DLL\r\nAPI-MS-WIN-POWER-SETTING-L1-1-0.DLL\r\nAPI-MS-WIN-RO-TYPERESOLUTION-L1-1-0.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-CLIPBOARD-L1-1-0.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-PRIVATE-L1-1-0.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-PRIVATE-L1-1-1.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-PRIVATE-L1-1-4.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-SHELL-L1-1-0.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-SYNCH-L1-1-0.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-WINDOW-L1-1-0.DLL\r\nAPI-MS-WIN-RTCORE-NTUSER-WINEVENT-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-ACCESSHLPR-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-ACTIVEDIRECTORYCLIENT-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-ACTIVEDIRECTORYCLIENT-L1-1-1.DLL\r\nAPI-MS-WIN-SECURITY-CREDENTIALS-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-CREDENTIALS-L2-1-0.DLL\r\nAPI-MS-WIN-SECURITY-CRYPTOAPI-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-GROUPPOLICY-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-ISOLATEDCONTAINER-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-LOGON-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-LOGON-L1-1-1.DLL\r\nAPI-MS-WIN-SECURITY-LSALOOKUP-ANSI-L2-1-0.DLL\r\nAPI-MS-WIN-SECURITY-LSALOOKUP-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-LSALOOKUP-L1-1-1.DLL\r\nAPI-MS-WIN-SECURITY-LSALOOKUP-L1-1-2.DLL\r\nAPI-MS-WIN-SECURITY-LSALOOKUP-L2-1-0.DLL\r\nAPI-MS-WIN-SECURITY-LSAPOLICY-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-PROVIDER-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-SDDL-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-SDDLPARSECOND-L1-1-0.DLL\r\nAPI-MS-WIN-SECURITY-SYSTEMFUNCTIONS-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-COMHELPERS-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-OBSOLETE-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-REGISTRY-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-REGISTRY-L1-1-1.DLL\r\nAPI-MS-WIN-SHCORE-SCALING-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-SCALING-L1-1-1.DLL\r\nAPI-MS-WIN-SHCORE-STREAM-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-STREAM-WINRT-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-SYSINFO-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-THREAD-L1-1-0.DLL\r\nAPI-MS-WIN-SHCORE-UNICODEANSI-L1-1-0.DLL\r\nAPI-MS-WIN-SHELL-ASSOCIATIONS-L1-1-1.DLL\r\nAPI-MS-WIN-SHELL-CHANGENOTIFY-L1-1-0.DLL\r\nAPI-MS-WIN-SHELL-NAMESPACE-L1-1-0.DLL\r\nAPI-MS-WIN-SHELL-SHDIRECTORY-L1-1-0.DLL\r\nAPI-MS-WIN-SHLWAPI-IE-L1-1-0.DLL\r\nAPI-MS-WIN-SHLWAPI-WINRT-STORAGE-L1-1-1.DLL\r\nEMCLIENT.DLL\r\nEXT-MS-MF-PAL-L2-1-0.DLL\r\nEXT-MS-ONECORE-APPCHROMEAPI-L1-1-0.DLL\r\nEXT-MS-ONECORE-APPDEFAULTS-L1-1-0.DLL\r\nEXT-MS-ONECORE-APPMODEL-EMCLIENT-L1-1-0.DLL\r\nEXT-MS-ONECORE-APPMODEL-STATEREPOSITORY-INTERNAL-L1-1-1.DLL\r\nEXT-MS-ONECORE-DCOMP-L1-1-0.DLL\r\nEXT-MS-ONECORE-HLINK-L1-1-0.DLL\r\nEXT-MS-ONECORE-ORIENTATION-L1-1-0.DLL\r\nEXT-MS-ONECORE-SHELLCHROMEAPI-L1-1-0.DLL\r\nEXT-MS-ONECORE-SHELLCHROMEAPI-L1-1-1.DLL\r\nEXT-MS-WIN-ADVAPI32-LSA-L1-1-0.DLL\r\nEXT-MS-WIN-ADVAPI32-MSI-L1-1-0.DLL\r\nEXT-MS-WIN-ADVAPI32-NPUSERNAME-L1-1-0.DLL\r\nEXT-MS-WIN-ADVAPI32-NTMARTA-L1-1-0.DLL\r\nEXT-MS-WIN-ADVAPI32-PSM-APP-L1-1-0.DLL\r\nEXT-MS-WIN-ADVAPI32-REGISTRY-L1-1-0.DLL\r\nEXT-MS-WIN-ADVAPI32-REGISTRY-L1-1-1.DLL\r\nEXT-MS-WIN-ADVAPI32-SAFER-L1-1-0.DLL\r\nEXT-MS-WIN-APPCOMPAT-AEINV-L1-1-0.DLL\r\nEXT-MS-WIN-APPCOMPAT-AEPIC-L1-1-0.DLL\r\nEXT-MS-WIN-APPCOMPAT-APPHELP-L1-1-0.DLL\r\nEXT-MS-WIN-APPMODEL-APPEXECUTIONALIAS-L1-1-0.DLL\r\nEXT-MS-WIN-APPMODEL-APPEXECUTIONALIAS-L1-1-1.DLL\r\nEXT-MS-WIN-APPMODEL-DAXCORE-L1-1-0.DLL\r\nEXT-MS-WIN-APPMODEL-DEPLOYMENT-L1-1-0.DLL\r\nEXT-MS-WIN-APPMODEL-RESTRICTEDAPPCONTAINER-INTERNAL-L1-1-0.DLL\r\nEXT-MS-WIN-APPMODEL-STATE-EXT-L1-2-0.DLL\r\nEXT-MS-WIN-APPMODEL-USERCONTEXT-L1-1-0.DLL\r\nEXT-MS-WIN-APPMODEL-VIEWSCALEFACTOR-L1-1-0.DLL\r\nEXT-MS-WIN-APPXDEPLOYMENTCLIENT-APPXDEPLOY-L1-1-0.DLL\r\nEXT-MS-WIN-APPXDEPLOYMENTCLIENT-APPXDEPLOYONECORE-L1-1-0.DLL\r\nEXT-MS-WIN-AUDIOCORE-PAL-L1-2-0.DLL\r\nEXT-MS-WIN-AUTHZ-CONTEXT-L1-1-0.DLL\r\nEXT-MS-WIN-AUTHZ-REMOTE-L1-1-0.DLL\r\nEXT-MS-WIN-COM-CLBCATQ-L1-1-0.DLL\r\nEXT-MS-WIN-COM-COML2-L1-1-1.DLL\r\nEXT-MS-WIN-COM-OLE32-L1-1-0.DLL\r\nEXT-MS-WIN-COM-OLE32-L1-1-1.DLL\r\nEXT-MS-WIN-COM-OLE32-L1-1-2.DLL\r\nEXT-MS-WIN-COM-OLE32-L1-1-3.DLL\r\nEXT-MS-WIN-COM-OLE32-L1-1-4.DLL\r\nEXT-MS-WIN-COM-OLE32-L1-1-5.DLL\r\nEXT-MS-WIN-COM-PSMREGISTER-L1-1-0.DLL\r\nEXT-MS-WIN-COM-SUSPENDRESILIENCY-L1-1-0.DLL\r\nEXT-MS-WIN-CORE-WINRT-REMOTE-L1-1-0.DLL\r\nEXT-MS-WIN-DESKTOPAPPX-L1-1-2.DLL\r\nEXT-MS-WIN-DEVMGMT-DM-L1-1-1.DLL\r\nEXT-MS-WIN-DEVMGMT-POLICY-L1-1-0.DLL\r\nEXT-MS-WIN-DEVMGMT-POLICY-L1-1-1.DLL\r\nEXT-MS-WIN-DIRECT2D-DESKTOP-L1-1-0.DLL\r\nEXT-MS-WIN-DOMAINJOIN-NETJOIN-L1-1-0.DLL\r\nEXT-MS-WIN-DWMAPI-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-DWMAPIDXGI-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-EDPUTIL-POLICY-L1-1-0.DLL\r\nEXT-MS-WIN-EDPUTIL-POLICY-L1-1-1.DLL\r\nEXT-MS-WIN-ELS-ELSCORE-L1-1-0.DLL\r\nEXT-MS-WIN-EVENTING-RUNDOWN-L1-1-0.DLL\r\nEXT-MS-WIN-FAMILYSAFETY-CHILDACCOUNT-L1-1-0.DLL\r\nEXT-MS-WIN-FECLIENT-ENCRYPTEDFILE-L1-1-0.DLL\r\nEXT-MS-WIN-FECLIENT-ENCRYPTEDFILE-L1-1-1.DLL\r\nEXT-MS-WIN-FIREWALLAPI-WEBPROXY-L1-1-0.DLL\r\nEXT-MS-WIN-FVEAPI-QUERY-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-CLIPPING-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-DC-CREATE-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-DC-L1-2-0.DLL\r\nEXT-MS-WIN-GDI-DC-L1-2-1.DLL\r\nEXT-MS-WIN-GDI-DEVCAPS-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-DRAW-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-DRAW-L1-1-1.DLL\r\nEXT-MS-WIN-GDI-DRAW-L1-1-2.DLL\r\nEXT-MS-WIN-GDI-DRAW-L1-1-3.DLL\r\nEXT-MS-WIN-GDI-FONT-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-FONT-L1-1-1.DLL\r\nEXT-MS-WIN-GDI-FONT-L1-1-2.DLL\r\nEXT-MS-WIN-GDI-FONT-L1-1-3.DLL\r\nEXT-MS-WIN-GDI-GDIPLUS-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-INTERNAL-DESKTOP-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-INTERNAL-DESKTOP-L1-1-1.DLL\r\nEXT-MS-WIN-GDI-INTERNAL-DESKTOP-L1-1-2.DLL\r\nEXT-MS-WIN-GDI-METAFILE-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-METAFILE-L1-1-1.DLL\r\nEXT-MS-WIN-GDI-METAFILE-L1-1-2.DLL\r\nEXT-MS-WIN-GDI-PATH-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-PRINT-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-PRIVATE-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-RENDER-L1-1-0.DLL\r\nEXT-MS-WIN-GDI-WCS-L1-1-0.DLL\r\nEXT-MS-WIN-GPAPI-GROUPPOLICY-L1-1-0.DLL\r\nEXT-MS-WIN-GUI-DUI70-L1-1-0.DLL\r\nEXT-MS-WIN-IMM-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-APPCOMPAT-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-DATETIME-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-ERRORHANDLING-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-FILE-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-LOCALIZATION-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-PACKAGE-CURRENT-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-PACKAGE-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-PACKAGE-L1-1-2.DLL\r\nEXT-MS-WIN-KERNEL32-QUIRKS-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-QUIRKS-L1-1-1.DLL\r\nEXT-MS-WIN-KERNEL32-REGISTRY-L1-1-0.DLL\r\nEXT-MS-WIN-KERNEL32-SIDEBYSIDE-L1-1-0.DLL\r\nEXT-MS-WIN-KERNELBASE-PROCESSTHREAD-L1-1-0.DLL\r\nEXT-MS-WIN-MININPUT-INPUTHOST-L1-1-0.DLL\r\nEXT-MS-WIN-MPR-MULTIPLEPROVIDERROUTER-L1-1-0.DLL\r\nEXT-MS-WIN-MRMCORER-RESMANAGER-L1-1-0.DLL\r\nEXT-MS-WIN-NETWORKING-WCMAPI-L1-1-0.DLL\r\nEXT-MS-WIN-NETWORKING-WLANSTORAGE-L1-1-0.DLL\r\nEXT-MS-WIN-NTDSAPI-ACTIVEDIRECTORYCLIENT-L1-1-0.DLL\r\nEXT-MS-WIN-NTDSAPI-ACTIVEDIRECTORYCLIENT-L1-1-1.DLL\r\nEXT-MS-WIN-NTUSER-DC-ACCESS-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-DIALOGBOX-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-DRAW-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-DRAW-L1-1-1.DLL\r\nEXT-MS-WIN-NTUSER-DRAW-L1-1-2.DLL\r\nEXT-MS-WIN-NTUSER-GUI-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-KEYBOARD-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-MENU-L1-1-2.DLL\r\nEXT-MS-WIN-NTUSER-MESSAGE-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-MESSAGE-L1-1-1.DLL\r\nEXT-MS-WIN-NTUSER-MESSAGE-L1-1-2.DLL\r\nEXT-MS-WIN-NTUSER-MISC-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-MISC-L1-2-0.DLL\r\nEXT-MS-WIN-NTUSER-MISC-L1-5-0.DLL\r\nEXT-MS-WIN-NTUSER-MISC-L1-5-1.DLL\r\nEXT-MS-WIN-NTUSER-MIT-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-MOUSE-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-PRIVATE-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-PRIVATE-L1-1-1.DLL\r\nEXT-MS-WIN-NTUSER-PRIVATE-L1-2-0.DLL\r\nEXT-MS-WIN-NTUSER-PRIVATE-L1-3-2.DLL\r\nEXT-MS-WIN-NTUSER-RECTANGLE-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-ROTATIONMANAGER-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-SERVER-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-STRING-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-SYNCH-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-UICONTEXT-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-WINDOW-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-WINDOW-L1-1-1.DLL\r\nEXT-MS-WIN-NTUSER-WINDOW-L1-1-2.DLL\r\nEXT-MS-WIN-NTUSER-WINDOW-L1-1-3.DLL\r\nEXT-MS-WIN-NTUSER-WINDOWCLASS-L1-1-0.DLL\r\nEXT-MS-WIN-NTUSER-WINDOWSTATION-L1-1-0.DLL\r\nEXT-MS-WIN-ODBC-ODBC32-L1-1-0.DLL\r\nEXT-MS-WIN-OLE32-BINDCTX-L1-1-0.DLL\r\nEXT-MS-WIN-OLE32-IE-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-OLE32-OLEAUTOMATION-L1-1-0.DLL\r\nEXT-MS-WIN-OLEACC-L1-1-1.DLL\r\nEXT-MS-WIN-PRINTER-PRNTVPT-L1-1-0.DLL\r\nEXT-MS-WIN-PROFILE-EXTENDER-L1-1-0.DLL\r\nEXT-MS-WIN-PROFILE-USERENV-L1-1-0.DLL\r\nEXT-MS-WIN-PROFILE-USERENV-L1-1-1.DLL\r\nEXT-MS-WIN-RAS-RASAPI32-L1-1-0.DLL\r\nEXT-MS-WIN-RAS-TAPI32-L1-1-1.DLL\r\nEXT-MS-WIN-RDR-DAVHLPR-L1-1-0.DLL\r\nEXT-MS-WIN-RESOURCES-DEPLOYMENT-L1-1-0.DLL\r\nEXT-MS-WIN-RESOURCES-LANGUAGEOVERLAY-L1-1-0.DLL\r\nEXT-MS-WIN-RO-TYPERESOLUTION-L1-1-0.DLL\r\nEXT-MS-WIN-RPC-SSL-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-GDI-DEVCAPS-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-GDI-OBJECT-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-GDI-RGN-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-GDI-RGN-L1-1-1.DLL\r\nEXT-MS-WIN-RTCORE-MINUSER-INPUT-L1-1-1.DLL\r\nEXT-MS-WIN-RTCORE-MINUSER-PRIVATE-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-CURSOR-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-DC-ACCESS-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-DPI-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-DPI-L1-2-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-IAM-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-INTEGRATION-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-SYNCH-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-SYSCOLORS-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-SYSPARAMS-L1-1-0.DLL\r\nEXT-MS-WIN-RTCORE-NTUSER-WINDOW-EXT-L1-1-0.DLL\r\nEXT-MS-WIN-SECUR32-TRANSLATENAME-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-CAPAUTHZ-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-CAPAUTHZ-L1-1-1.DLL\r\nEXT-MS-WIN-SECURITY-CHAMBERS-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-CREDUI-INTERNAL-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-CREDUI-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-CREDUI-L1-1-1.DLL\r\nEXT-MS-WIN-SECURITY-CRYPTUI-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-CRYPTUI-L1-1-1.DLL\r\nEXT-MS-WIN-SECURITY-EFS-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-EFSWRT-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-EFSWRT-L1-1-1.DLL\r\nEXT-MS-WIN-SECURITY-NGC-LOCAL-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-WINSCARD-L1-1-0.DLL\r\nEXT-MS-WIN-SECURITY-WINSCARD-L1-1-1.DLL\r\nEXT-MS-WIN-SESSION-USERMGR-L1-1-0.DLL\r\nEXT-MS-WIN-SESSION-USERTOKEN-L1-1-0.DLL\r\nEXT-MS-WIN-SESSION-WINSTA-L1-1-0.DLL\r\nEXT-MS-WIN-SESSION-WTSAPI32-L1-1-0.DLL\r\nEXT-MS-WIN-SETUPAPI-INF-L1-1-0.DLL\r\nEXT-MS-WIN-SETUPAPI-INF-L1-1-1.DLL\r\nEXT-MS-WIN-SHELL-DIRECTORY-L1-1-0.DLL\r\nEXT-MS-WIN-SHELL-EMBEDDEDMODE-L1-1-0.DLL\r\nEXT-MS-WIN-SHELL-SHELL32-L1-2-0.DLL\r\nEXT-MS-WIN-SHELL-SHLWAPI-L1-1-0.DLL\r\nEXT-MS-WIN-SHELL-TABBEDTITLEBAR-L1-1-0.DLL\r\nEXT-MS-WIN-SHELL32-SHELLCOM-L1-1-0.DLL\r\nEXT-MS-WIN-SHELL32-SHELLFOLDERS-L1-1-0.DLL\r\nEXT-MS-WIN-SMBSHARE-BROWSERCLIENT-L1-1-0.DLL\r\nEXT-MS-WIN-STORAGE-SENSE-L1-1-0.DLL\r\nEXT-MS-WIN-STORAGE-SENSE-L1-2-0.DLL\r\nEXT-MS-WIN-SXS-OLEAUTOMATION-L1-1-0.DLL\r\nEXT-MS-WIN-TSF-MSCTF-L1-1-0.DLL\r\nEXT-MS-WIN-TSF-MSCTF-L1-1-1.DLL\r\nEXT-MS-WIN-UI-VIEWMANAGEMENT-L1-1-0.DLL\r\nEXT-MS-WIN-USP10-L1-1-0.DLL\r\nEXT-MS-WIN-WER-UI-L1-1-0.DLL\r\nEXT-MS-WIN-WER-XBOX-L1-1-0.DLL\r\nEXT-MS-WIN-WINRT-DEVICE-ACCESS-L1-1-0.DLL\r\nEXT-MS-WIN-WINRT-STORAGE-L1-1-0.DLL\r\nEXT-MS-WIN-WLAN-ONEXUI-L1-1-0.DLL\r\nEXT-MS-WIN-WLAN-SCARD-L1-1-0.DLL\r\nEXT-MS-WINDOWSCORE-DEVICEINFO-L1-1-0.DLL\r\nIESHIMS.DLL\r\n\r\n\r\nEverything on this list came up fine.\r\n\r\nSHCORE.DLL\r\nSHLWAPI.DLL\r\nIEFRAME.DLL\r\n_PYWRAP_TENSORFLOW_INTERNAL.PYD\r\nADVAPI32.DLL\r\nBCRYPTPRIMITIVES.DLL\r\nCFGMGR32.DLL\r\nCRYPTBASE.DLL\r\nCUBLAS64_90.DLL\r\nCUDNN64_7.DLL\r\nDEVRTL.DLL\r\nGDI32.DLL\r\nKERNEL32.DLL\r\nKERNELBASE.DLL\r\nMSVCP140.DLL\r\nMSVCRT.DLL\r\nNTDLL.DLL\r\nNVCUDA.DLL\r\nNVFATBINARYLOADER.DLL\r\nPYTHON36.DLL\r\nRPCRT4.DLL\r\nSECHOST.DLL\r\nSETUPAPI.DLL\r\nSHELL32.DLL\r\nUSER32.DLL\r\nVCRUNTIME140.DLL\r\nVERSION.DLL\r\nWIN32U.DLL\r\nWS2_32.DLL\r\nACLUI.DLL\r\nACTIVEDS.DLL\r\nADSLDPC.DLL\r\nADVPACK.DLL\r\nAEPIC.DLL\r\nAPPHELP.DLL\r\nAUTHZ.DLL\r\nBCP47LANGS.DLL\r\nBCP47MRM.DLL\r\nBCRYPT.DLL\r\nBROWCLI.DLL\r\nCABINET.DLL\r\nCERTCA.DLL\r\nCERTENROLL.DLL\r\nCHAKRA.DLL\r\nCHARTV.DLL\r\nCLDAPI.DLL\r\nCOMBASE.DLL\r\nCOMCTL32.DLL\r\nCOMCTL32.DLL\r\nCOMDLG32.DLL\r\nCOML2.DLL\r\nCONCRT140.DLL\r\nCONTACTACTIVATION.DLL\r\nCOREMESSAGING.DLL\r\nCOREUICOMPONENTS.DLL\r\nCREDUI.DLL\r\nCRYPT32.DLL\r\nCRYPTNET.DLL\r\nCRYPTNGC.DLL\r\nCRYPTSP.DLL\r\nCRYPTTPMEKSVC.DLL\r\nCRYPTUI.DLL\r\nCSCAPI.DLL\r\nD2D1.DLL\r\nD3D11.DLL\r\nD3D12.DLL\r\nD3DSCACHE.DLL\r\nDAVHLPR.DLL\r\nDBGCORE.DLL\r\nDBGENG.DLL\r\nDBGHELP.DLL\r\nDBGMODEL.DLL\r\nDCOMP.DLL\r\nDEVMGR.DLL\r\nDEVOBJ.DLL\r\nDFSCLI.DLL\r\nDHCPCSVC.DLL\r\nDHCPCSVC6.DLL\r\nDMENTERPRISEDIAGNOSTICS.DLL\r\nDNSAPI.DLL\r\nDPAPI.DLL\r\nDRVSTORE.DLL\r\nDSCLIENT.DLL\r\nDSPARSE.DLL\r\nDSREG.DLL\r\nDSROLE.DLL\r\nDUI70.DLL\r\nDUSER.DLL\r\nDWMAPI.DLL\r\nDWRITE.DLL\r\nDXGI.DLL\r\nDXILCONV.DLL\r\nEAPPCFG.DLL\r\nEAPPPRXY.DLL\r\nEDGEISO.DLL\r\nEDPAUDITAPI.DLL\r\nEDPUTIL.DLL\r\nEFSADU.DLL\r\nEFSCORE.DLL\r\nEFSUTIL.DLL\r\nEFSWRT.DLL\r\nELSCORE.DLL\r\nESENT.DLL\r\nFECLIENT.DLL\r\nFIREWALLAPI.DLL\r\nFLTLIB.DLL\r\nFMS.DLL\r\nFWBASE.DLL\r\nFWPOLICYIOMGR.DLL\r\nGDIPLUS.DLL\r\nHID.DLL\r\nIEADVPACK.DLL\r\nIEAPFLTR.DLL\r\nIERTUTIL.DLL\r\nIEUI.DLL\r\nIMAGEHLP.DLL\r\nIMGUTIL.DLL\r\nIMM32.DLL\r\nINETCOMM.DLL\r\nIPHLPAPI.DLL\r\nLINKINFO.DLL\r\nLOGONCLI.DLL\r\nMFC42U.DLL\r\nMI.DLL\r\nMIUTILS.DLL\r\nMLANG.DLL\r\nMMDEVAPI.DLL\r\nMPR.DLL\r\nMPRMSG.DLL\r\nMRMCORER.DLL\r\nMSASN1.DLL\r\nMSCTF.DLL\r\nMSFEEDS.DLL\r\nMSHTML.DLL\r\nMSI.DLL\r\nMSILTCFG.DLL\r\nMSIMG32.DLL\r\nMSISO.DLL\r\nMSLS31.DLL\r\nMSOERT2.DLL\r\nMSVCP110_WIN.DLL\r\nMSVCP_WIN.DLL\r\nNCRYPT.DLL\r\nNETAPI32.DLL\r\nNETUTILS.DLL\r\nNEWDEV.DLL\r\nNGCRECOVERY.DLL\r\nNSI.DLL\r\nNTASN1.DLL\r\nNTDSAPI.DLL\r\nNTSHRUI.DLL\r\nOCCACHE.DLL\r\nOLE32.DLL\r\nOLEACC.DLL\r\nOLEAUT32.DLL\r\nOLEDLG.DLL\r\nONEX.DLL\r\nPKEYHELPER.DLL\r\nPOLICYMANAGER.DLL\r\nPOWRPROF.DLL\r\nPRINTUI.DLL\r\nPROFAPI.DLL\r\nPROPSYS.DLL\r\nPUIAPI.DLL\r\nRASAPI32.DLL\r\nRASMAN.DLL\r\nREGAPI.DLL\r\nRMCLIENT.DLL\r\nRTUTILS.DLL\r\nSAMCLI.DLL\r\nSAMLIB.DLL\r\nSCECLI.DLL\r\nSECUR32.DLL\r\nSHDOCVW.DLL\r\nSLC.DLL\r\nSPFILEQ.DLL\r\nSPINF.DLL\r\nSPPC.DLL\r\nSRPAPI.DLL\r\nSRVCLI.DLL\r\nSSPICLI.DLL\r\nTBS.DLL\r\nTEXTINPUTFRAMEWORK.DLL\r\nTOKENBINDING.DLL\r\nTPMCOREPROVISIONING.DLL\r\nTWINAPI.APPCORE.DLL\r\nTWINAPI.DLL\r\nUIAUTOMATIONCORE.DLL\r\nURLMON.DLL\r\nUSERDATATYPEHELPERUTIL.DLL\r\nUSERENV.DLL\r\nUXTHEME.DLL\r\nVAULTCLI.DLL\r\nVIRTDISK.DLL\r\nW32TOPL.DLL\r\nWEBAUTHN.DLL\r\nWEBIO.DLL\r\nWEBSERVICES.DLL\r\nWEBSOCKET.DLL\r\nWER.DLL\r\nWEVTAPI.DLL\r\nWINBRAND.DLL\r\nWINDOWSCODECS.DLL\r\nWINDOWSPERFORMANCERECORDERCONTROL.DLL\r\nWINHTTP.DLL\r\nWININET.DLL\r\nWINIPCFILE.DLL\r\nWINMM.DLL\r\nWINMMBASE.DLL\r\nWINMSIPC.DLL\r\nWINNSI.DLL\r\nWINSPOOL.DRV\r\nWINSTA.DLL\r\nWINTRUST.DLL\r\nWKSCLI.DLL\r\nWLANAPI.DLL\r\nWLDAP32.DLL\r\nWMICLNT.DLL\r\nWPAXHOLDER.DLL\r\nWPCWEBFILTER.DLL\r\nWTSAPI32.DLL\r\nWUCEFFECTS.DLL\r\nXMLLITE.DLL\r\n", "@ymodak  I had a look at the second issues you linked.  I checked his solution and when I did a search for the cudnn64_7.dll it was located in both the bins for CUDA 10 and CUDA 9.0 as well as MATLAB and CNTK\r\n\r\n![image](https://user-images.githubusercontent.com/24856132/50043878-cb168300-0073-11e9-9823-86b6d2be1e46.png)\r\n", "With the latest screenshot, I am fairly certain this is because TF tries to load cuda 10.\r\nCould you try removing CUDA 10 from your PATH and retry TF?", "Hi @gunan thanks for the reply, I just checked the path variable, as you can see only CUDA 9 is currently there. I uninstalled CUDA 10 a day prior to creating this issue and I guess the folder is just leftovers that the nvidia uninstaller did not remove.\r\nPATH System Variable\r\n![image](https://user-images.githubusercontent.com/24856132/50121599-2a7dba00-0251-11e9-91f1-22ae2c8127dc.png)\r\n\r\nSystem Variables\r\n![image](https://user-images.githubusercontent.com/24856132/50121628-41241100-0251-11e9-9cd0-c63e2255f63a.png)\r\n\r\nPATH User Variables\r\n![image](https://user-images.githubusercontent.com/24856132/50121649-4c773c80-0251-11e9-96c5-841dd8f72158.png)\r\n", "Screenshots only include partial information, please use terminal outputs\nto share information.\nAnd we cannot find the important information by searching, so please share\ntext instead.\nFor larger outputs, please paste them to pastebin, so the thread is still\neasy to follow.\n\nNow, for Path, sure system path is important, but to me it looks like you\nmay have CUDA also available through anaconda, or your \"user\" path.\nSo what is the output of `echo %PATH%` in your windows commandline.\n\nAlso, the errors dependency walker output is probably the key to the issue\non your machine.\nThat throwing errors means it tried to look into the libraries TF needs\nwhen starting up, and not all of them was found in the locations that TF\ncan find.\nWhat were the errors from that?\nRemember, no screenshots, and please use pastebin if the output is too long.\n\nThis is definitely in your machine setup (by machine, I mean cuda\ninstallations on your machine).\n\n\nOn Mon, Dec 17, 2018 at 3:15 PM Naith123 <notifications@github.com> wrote:\n\n> Hi @gunan <https://github.com/gunan> thanks for the reply, I just checked\n> the path variable, as you can see only CUDA 9 is currently there. I\n> uninstalled CUDA 10 a day prior to creating this issue and I guess the\n> folder is just leftovers that the nvidia uninstaller did not remove.\n> PATH System Variable\n> [image: image]\n> <https://user-images.githubusercontent.com/24856132/50121599-2a7dba00-0251-11e9-91f1-22ae2c8127dc.png>\n>\n> System Variables\n> [image: image]\n> <https://user-images.githubusercontent.com/24856132/50121628-41241100-0251-11e9-9cd0-c63e2255f63a.png>\n>\n> PATH User Variables\n> [image: image]\n> <https://user-images.githubusercontent.com/24856132/50121649-4c773c80-0251-11e9-96c5-841dd8f72158.png>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24199#issuecomment-448034707>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOSQNATmAkixvG3HrZiodOMcREF7tks5u6CWWgaJpZM4ZGvUj>\n> .\n>\n", "Hi Gunan, sorry for the delay in getting back to you. Something came up, please see the linked pastebin for output for the echo %PATH% command.\r\n\r\nhttps://pastebin.com/9AS5ycH1\r\n\r\nI haven't managed to get dependency walker to not crash since the first time I ran it.  I will keep trying and let you know as soon as it starts working again.", "and of course, as soon as I try to do it again, it works fine.  Just typical of my last week.  Please see the linked pastebin for the output.\r\n\r\nhttps://pastebin.com/kyEhjnXx\r\n\r\n\r\nThe first 7 entries came up with a whole bunch of info, time stamps, file sizes.  Everything below that up to FECLIENT.DLL had the error 'Error Opening File. The System cannot find the file specifced (2).  Then there were some correctly linked files up to IESHIMS.DLL which had the same error as before and then everything else seemed fine.", "Apologies lost track of this. @Naith123 Is this still an issue for you?", "@ymodak sorry about getting back to you, managed to get it fixed as of today,  looks like it is working now, the solution seemed to be to uninstall nsight from control panel and then reinstall CUDA 9, that seemed to be the solution in the end.  Kind thanks for both yours and @gunan's help! "]}, {"number": 24197, "title": "Building Tensorflow lite on ARM64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Python version: 2.7.15rc/3.6.7\r\n- Bazel version (if compiling from source): 0.20.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04) 7.3.0\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to install tensorflow lite using bazel. I'm trying to compile it on AWS ARM 64 bit machine. \r\nI compiled tehnsorflow using the command:\r\n`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... --incompatible_remove_native_http_archive=false`\r\nDuring the compilation I received this error message:\r\n\r\n```\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvB5cxx11EPKc\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n```\r\n", "comments": ["Did you make sure to have all of your dependencies installed like CUDA? Have you tried building using [this tutorial](https://gist.github.com/Brainiarc7/6d6c3f23ea057775b72c52817759b25c)? ", "I don't have CUDA on the server. I just want to compile tensorflow lite for my ARM device (It has Cortex A72 processor).", "@liorko87 you saw\r\n```\r\nImportError: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws11Environment6GetEnvB5cxx11EPKc\r\n```\r\nbecause platform dependent part of AWS SDK is not enabled for ARM64. I have a patch for this, see the [PR](https://github.com/tensorflow/tensorflow/pull/22856). What you can do is to either disable AWS SDK or try my patch.", "@freedomtan after I ran your build command `bazel build //tensorflow/tools/pip_package:build_pip_package` the installation succeeded.\r\nIn order to build the `whl` package, use this SO [answer](https://stackoverflow.com/a/45861241/4193208)."]}, {"number": 24196, "title": "Possibly buffer overflow in tf.nn.conv2d on GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5.3\r\n- CUDA/cuDNN version: 9.0 / 7.2.1\r\n- GPU model and memory: Tesla M60 (also tested on GeForce GTX TITAN Black)\r\n\r\n**Describe the current behavior**\r\nI've created a tensor set to zero, except a single entry at [:,-1,-1,0] that was set to a large number 1e10. I've then convolved the tensor with a random kernel. When the batch size is small (e.g. 5) then the output at [:,0,0,:] is zero (because the input array is zero there), but when the batch size is large (e.g. 100) then these entries contain numbers significantly larger than zero. This only happens on the GPU.\r\n**Describe the expected behavior**\r\nThe expected behavior should be zero regardless if running on CPU or GPU, because windows at these locations contain only zero entries. Because the only non-zero entry is outside the window used to compute the value, then this is most likely a buffer overflow (or some other memory access issue).\r\n**Code to reproduce the issue**\r\nIf the following does not immediately produces the bug, then try increasing the batch size from 100 to something greater.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef test(sess):\r\n\tw = np.random.uniform(size=(5,5,64,64)).astype(np.float32)\r\n\tx_t = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 64))\r\n\tw_t = tf.constant(w)\r\n\to_t = tf.nn.conv2d(x_t, w_t, [1,1,1,1], 'VALID', data_format='NHWC')\r\n\tx_good = np.zeros((5,32,32, 64), dtype=np.float32)\r\n\tx_good[:,-1,-1,0] = 1e10\r\n\tx_bad = np.zeros((100,32,32, 64), dtype=np.float32)\r\n\tx_bad[:,-1,-1,0] = 1e10\r\n\to_good = sess.run(o_t, feed_dict={x_t : x_good})\r\n\to_bad = sess.run(o_t, feed_dict={x_t : x_bad})\r\n\tprint('Number nonzero (good): ', np.count_nonzero(o_good[:,0,0,:]))\r\n\tprint('Number nonzero (bad): ', np.count_nonzero(o_bad[:,0,0,:]))\r\n\tif np.count_nonzero(o_bad[:,0,0,:]) > 0:\r\n\t\tprint(o_bad[0,0,0,:])\r\n\r\nwith tf.Session() as sess:\r\n\twith tf.device('cpu:0'):\r\n\t\tprint('Testing on cpu -- should succeed')\r\n\t\ttest(sess)\r\n\twith tf.device('gpu:0'):\r\n\t\tprint('Testing on gpu -- typically fails')\r\n\t\ttest(sess)\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24196\">No</a>\n", "Closing out this issue because it hasn't been updated in the last year.  Please reopen if this is still relevant.", "This is still an issue both on latest TF1 and TF2. See attached Colab notebooks demonstrating the issue:\r\nTF1: https://colab.research.google.com/drive/1eomQKlLSBvZsyDLU3EM_Bv74imxT_RwU?usp=sharing\r\nTF2: https://colab.research.google.com/drive/1mlHwozUWUyMv4ShGlhIYMzkxqtLrGcny?usp=sharing"]}, {"number": 24195, "title": "Added comment to Decoder.finalize", "body": "Hi. I'm Korean student, recently studying tensorflow.\r\n\r\nAt first, I'm sorry about my poor english.\r\n\r\nI found a curious thing that only `finalize` method in `contrib.seq2seq.Decoder` doesn't have any comment in documents.\r\n\r\nSo, I tried to write some comments on it, usually copying from comments from other methods.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed the indivisual CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "I changed comment of `sequence_lengths`", "Nagging Reviewer @ebrevdo, @lmthang: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@smilu97 Hi, could you please look into the changes requested by ebrevdo ?", "Oh.. I already commited new one as ebrevdo requested, but I forgot to write some comment about this.\r\nor did I missed something?", "This failed pylint at C0301[line-too-long], so I fixed some"]}]