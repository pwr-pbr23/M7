[{"number": 41526, "title": "fix eager execution", "body": "Fix #41493.", "comments": ["Not the good reviewer for this unfortunately.", "Fix #42424"]}, {"number": 41524, "title": "data input into model.fit() for multiple dense(2,)  layers ", "body": "```\r\ndef func(img_batch, lb_batch):\r\n  lbs = tf.one_hot(lb_batch,depth=2)\r\n  return img_batch, lbs\r\n\r\ntrain_Data = train_ds.map(func)\r\n\r\nmodel.fit(train_Data,steps_per_epoch=400,validation_steps=40,\r\n                    epochs=50,verbose=1))\r\n```\r\n\r\nError:\r\nThe model is expecting a 2 separate arrays , but the custom funtion return single array with shape (2,2).\r\n\r\nHere the lbs in func is retrun single array with shape (2,2), how can i make the lb into seperate array with each having shape (1,2)?", "comments": ["Any updates ?", "@Samjith888 \r\nI ran the above shared code, its incomplete hence i [face a different error](https://colab.research.google.com/gist/Saduf2019/e51869bb8cbc7b062aca0256c6149de9/untitled284.ipynb), please share all dependencies and complete indented code and tf version such that we can replicate the issue faced or a colab gist with the error to analyse. Thanks!", "> ```\r\n> def func(img_batch, lb_batch):\r\n>   lbs = tf.one_hot(lb_batch,depth=2)\r\n>   return img_batch, lbs\r\n> \r\n> train_Data = train_ds.map(func)\r\n> \r\n> model.fit(train_Data,steps_per_epoch=400,validation_steps=40,\r\n>                     epochs=50,verbose=1))\r\n> ```\r\n> \r\n> Error:\r\n> The model is expecting a 2 separate arrays , but the custom funtion return single array with shape (2,2).\r\n> \r\n> Here the lbs in func is retrun single array with shape (2,2), how can i make the lb into seperate array with each having shape (1,2)?\r\n\r\n@Saduf2019  please look at the updated issue ,, there  i have simplified the use case\r\n", "@Samjith888 \r\nI ran the code again its incomplete, please share a colab gist and the tf version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41524\">No</a>\n"]}, {"number": 41523, "title": "Train with multi-gpu with MirroredStrategy for dataset issue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):2.2.0\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.2 and cuDNN 7.6.5\r\n- GPU model and memory: Tesla T4 15109MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\nGPU details:\r\n\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:3', device_type='XLA_GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\r\n PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\r\n\r\n**Describe the current behavior**\r\nI followed the blog for the distributed training with multi gpu.When we train with single gpu this works but with multi gpu i have added mirrored strategy but it throw the error.\r\n\r\n**Describe the expected behavior**\r\nFollowed this while dev but gpus are not getting used \r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#train_and_evaluate_the_model\r\n\r\n**Standalone code to reproduce the issue**\r\nNotebook and custom transformers are available here:\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-18-285f5b632916> in <module>\r\n      2 eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn)\r\n      3 \r\n----> 4 tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    470         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    471 \r\n--> 472   return executor.run()\r\n    473 \r\n    474 \r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)\r\n    611       tf.compat.v1.logging.info(\r\n    612           'Running training and evaluation locally (non-distributed).')\r\n--> 613       return self.run_local()\r\n    614 \r\n    615     # Distributed case.\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)\r\n    712         max_steps=self._train_spec.max_steps,\r\n    713         hooks=train_hooks,\r\n--> 714         saving_listeners=saving_listeners)\r\n    715 \r\n    716     eval_result = listener_for_eval.eval_result or _EvalResult(\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    347 \r\n    348       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 349       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    350       logging.info('Loss for final step: %s.', loss)\r\n    351       return self\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1178   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1179     if self._train_distribution:\r\n-> 1180       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1181     else:\r\n   1182       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1240       self._config._train_distribute.configure(self._config.session_config)\r\n   1241       return self._actual_train_model_distributed(\r\n-> 1242           self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n   1243     # pylint: enable=protected-access\r\n   1244 \r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py in _actual_train_model_distributed(self, strategy, input_fn, hooks, saving_listeners)\r\n   1324                   labels,  # although this will be None it seems\r\n   1325                   ModeKeys.TRAIN,\r\n-> 1326                   self.config))\r\n   1327           loss = strategy.reduce(\r\n   1328               _get_loss_reduce_op_for_reporting(),\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   2288       kwargs = {}\r\n   2289     with self._container_strategy().scope():\r\n-> 2290       return self._call_for_each_replica(fn, args, kwargs)\r\n   2291 \r\n   2292   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(self, fn, args, kwargs)\r\n    768 \r\n    769     return _call_for_each_replica(self._container_strategy(), self._devices,\r\n--> 770                                   fn, args, kwargs)\r\n    771 \r\n    772   def _configure(self,\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _call_for_each_replica(distribution, devices, fn, args, kwargs)\r\n    199     for t in threads:\r\n    200       t.should_run.set()\r\n--> 201     coord.join(threads)\r\n    202 \r\n    203   return values.regroup(tuple(t.main_result for t in threads))\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\r\n    387       self._registered_threads = set()\r\n    388       if self._exc_info_to_raise:\r\n--> 389         six.reraise(*self._exc_info_to_raise)\r\n    390       elif stragglers:\r\n    391         if ignore_live_threads:\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\r\n    295     \"\"\"\r\n    296     try:\r\n--> 297       yield\r\n    298     except:  # pylint: disable=bare-except\r\n    299       self.request_stop(ex=sys.exc_info())\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_strategy.py in run(self)\r\n    996               self._var_scope, reuse=self.replica_id > 0), \\\r\n    997           variable_scope.variable_creator_scope(self.variable_creator_fn):\r\n--> 998         self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    999         self.done = True\r\n   1000     finally:\r\n\r\n/usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    263       except Exception as e:  # pylint:disable=broad-except\r\n    264         if hasattr(e, 'ag_error_metadata'):\r\n--> 265           raise e.ag_error_metadata.to_exception(e)\r\n    266         else:\r\n    267           raise\r\n\r\nTypeError: in user code:\r\n\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1170 _call_model_fn  *\r\n        model_fn_results = self._model_fn(features=features, **kwargs)\r\n    /media/ephemeral0/combined_estimator.py:39 model_fn  *\r\n        if spec.train_op:\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:924 if_stmt\r\n        basic_symbol_names, composite_symbol_names)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:962 tf_if_stmt\r\n        error_checking_orelse)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507 new_func\r\n        return func(*args, **kwargs)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:1177 cond\r\n        return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/ops/cond_v2.py:91 cond_v2\r\n        op_return_value=pred)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:958 error_checking_orelse\r\n        basic_symbol_names + composite_symbol_names)\r\n    /usr/lib/environs/e-a-2019.03-py-3.7.3/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py:295 _verify_tf_cond_vars\r\n        ' branches.\\n\\n{}'.format(name, str(e)))\r\n\r\n    TypeError: \"train_op_list\" does not have the same nested structure in the TRUE and FALSE branches.\r\n    \r\n    The two structures don't have the same nested structure.\r\n    \r\n    First structure: type=list str=[<tf.Tensor 'Adam/Identity:0' shape=() dtype=int64>]\r\n    \r\n    Second structure: type=NoneType str=None\r\n    \r\n    More specifically: Substructure \"type=list str=[<tf.Tensor 'Adam/Identity:0' shape=() dtype=int64>]\" is a sequence, while substructure \"type=NoneType str=None\" is not\r\n    Entire first structure:\r\n    [.]\r\n    Entire second structure:\r\n    .\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523\">No</a>\n", "The resolution did not help in case of distribution of operations.\r\nSo here  i have 3 estimators and i have combined it to one mega estimator.how to use distributed gpu in that case.", "@ymodak any suggestions here where we have custom estimator how the mirrored strategy can distribute the loss.Any rules to be followed?", "@guptapriya Need your valuable inputs as well i followed your directions for distributed training and saw that its not working when we use custom estimator.Any inputs?", "@nikitamaia any suggestions here?", "Hi @Akpadhy, please provide minimal reproducible code to help with the debugging process. \r\nAdditionally, looking at the stack trace I'm not sure this error is distribution strategy specific. A quick google search for this error message shows several cases on stack overflow that are not related to training with MirroredStrategy.\r\n\r\nYou are able to run your code without problems if you are not using a distribution strategy?", "@nikitamaia Yes this works without any issue when i run on single gpu without distribution strategy.When i use distribution it does not understand optimisers.\r\ngit link : [https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL.ipynb](url)\r\nI have given minimal code for reproducibility. There is a git repo included\u00a0which is easy to access.", "Here are a couple of things to try:\r\n-  Firstly try disabling tf.compat.v1.disable_eager_execution() as we don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). I noticed this line in several of your scripts.\r\n- As a test, have you tried running with fewer Estimators and seeing what happens? Basically passing in 2 estimators to your CombinedEstimator instead of 7. \r\n\r\nAdditionally, can you explain what you mean by \"it does not understand optimisers\" ?", "Thanks for you response @nikitamaia .\r\n\r\nI have tried that disabling eager mode as you suggested and also tried with 2 estimator and it did not work out same error again.I have made a separate notebook for that.\r\n\r\nHere is the link : [git repo](https://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_WOT_EAGER.ipynb)\r\n\r\nWhat i meant when i said it does not understand optimiser because training optimiser spec come different in single and multi gpu.I have tried to print the spec and below are the findings i got training optimiser it is expecting it to be same but single gpu its coming different and multi gpu it is different:\r\n\r\n  #Single GPU: <class 'tensorflow.python.framework.ops.Operation'>\r\n  #Multi GPU: <class 'tensorflow.python.framework.ops.Tensor'>\r\n   print(type(spec.train_op))\r\n   #Single GPU: name: \"Adam/update_0_9/AssignAddVariableOp\" \r\n   #Multi GPU: Tensor(\"Adam/Identity:0\", shape=(), dtype=int64, device=/replica:0/task:0/device:GPU:0)\r\n                   ", "@nikitamaia @guptapriya \r\nIs it the issue if we combine multiple estimator we cant use multi gpu or possible?", "> @nikitamaia @guptapriya\r\n> Is it the issue if we combine multiple estimator we cant use multi gpu or possible?\r\n\r\n\r\n\r\n> Here are a couple of things to try:\r\n> \r\n> * Firstly try disabling tf.compat.v1.disable_eager_execution() as we don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). I noticed this line in several of your scripts.\r\n> * As a test, have you tried running with fewer Estimators and seeing what happens? Basically passing in 2 estimators to your CombinedEstimator instead of 7.\r\n> \r\n> Additionally, can you explain what you mean by \"it does not understand optimisers\" ?\r\n\r\nThanks for you response @nikitamaia .\r\n\r\nI have tried that disabling eager mode as you suggested and also tried with 2 estimator and it did not work out same error again.I have made a separate notebook for that.\r\n\r\nHere is the link : git repo\r\n\r\nWhat i meant when i said it does not understand optimiser because training optimiser spec come different in single and multi gpu.I have tried to print the spec and below are the findings i got training optimiser it is expecting it to be same but single gpu its coming different and multi gpu it is different:\r\n\r\n#Single GPU: <class 'tensorflow.python.framework.ops.Operation'>\r\n#Multi GPU: <class 'tensorflow.python.framework.ops.Tensor'>\r\nprint(type(spec.train_op))\r\n#Single GPU: name: \"Adam/update_0_9/AssignAddVariableOp\"\r\n#Multi GPU: Tensor(\"Adam/Identity:0\", shape=(), dtype=int64, device=/replica:0/task:0/device:GPU:0)", "@nikitamaia Do you have any suggestions after the update you asked to make it.", "Please note that Estimator API support for MirroredStrategy is limited at this point, [as noted here. \r\n](https://www.tensorflow.org/guide/distributed_training#types_of_strategies)\r\n\r\nA couple more things to try:\r\n- what happens when you pass in just one estimator to `CombinedEstimator` ?\r\n- And what happens when you train a single estimator (removing `CombinedEstimator` altogether) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Please note that Estimator API support for MirroredStrategy is limited at this point, [as noted here. ](https://www.tensorflow.org/guide/distributed_training#types_of_strategies)\r\n> \r\n> A couple more things to try:\r\n> \r\n> * what happens when you pass in just one estimator to `CombinedEstimator` ?\r\n> * And what happens when you train a single estimator (removing `CombinedEstimator` altogether)\r\n\r\nI tried with single estimator same issue i see here as well.Here is the example notebook.\r\n\r\nhttps://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_WOT_EAGER_one_estm.ipynb", "I tried to run that notebook but I'm not able to train the estimator with TF 2.3, even without a distribution strategy. I get the following error message, suggesting that there's something wrong with the input. \r\n\r\n```\r\nInvalidArgumentError: Input to reshape is a tensor with 256 values, but the requested shape has 1\r\n\t [[node Reshape (defined at /.../tensorflow2.0/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/model_fn.py:338) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node Reshape:\r\n Mean (defined at /.../tensorflow_model-master/model_fns.py:22)\r\n\r\n```\r\n\r\nI'm seeing a variation of this error when I try to run any of the notebooks in that repo without distribution strategy. If I use TF 2.2 I still see a variation of this error message.", "> I tried to run that notebook but I'm not able to train the estimator with TF 2.3, even without a distribution strategy. I get the following error message, suggesting that there's something wrong with the input.\r\n> \r\n> ```\r\n> InvalidArgumentError: Input to reshape is a tensor with 256 values, but the requested shape has 1\r\n> \t [[node Reshape (defined at /.../tensorflow2.0/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/model_fn.py:338) ]]\r\n> \r\n> Errors may have originated from an input operation.\r\n> Input Source operations connected to node Reshape:\r\n>  Mean (defined at /.../tensorflow_model-master/model_fns.py:22)\r\n> ```\r\n> \r\n> I'm seeing a variation of this error when I try to run any of the notebooks in that repo without distribution strategy. If I use TF 2.2 I still see a variation of this error message.\r\n\r\nPlease clone the repo now and check \r\n\r\nHere is the notebook without distribution:\r\nhttps://github.com/Akpadhy/tensorflow_model/blob/master/GPU_Testing_DL_wot_distribution.ipynb", "Okay this time I was able to train without distribution, and also reproduce the error message with distribution. However I tried to run the single estimator without using the combined estimator and I was unable to train it successfully, even without distribution. \r\n\r\nIt's difficult to debug this case since you have a lot of custom code. If your combined estimator is also of type estimator, then I don't see any obvious reasons why it should fail with distributed training. I think you might need to just do some debugging based on the error message.\r\n\r\nLooking at the stack trace for the combined estimator case, the message doesn't seem to be be distribution strategy specific. I found a similar message in this issue here #33491\r\nAlso this [stack overflow post ](https://stackoverflow.com/questions/59585574/autograph-in-tensorflow-1-15-gives-typeerror-in-conditional-statement-with-value)\r\n\r\nThe message seems very similar to the type error in the docs for [tf.nest.assert_same_structure ](https://www.tensorflow.org/api_docs/python/tf/nest/assert_same_structure)\r\nWhich only happens when `check_types=True` so I'm guessing that when you run with distribution strategy something causes check_types to get set to True.\r\n\r\nSomething else I noticed is that when I ran the code in colab with MirroredStrategy and a single GPU, I saw this warning:\r\n```\r\nWARNING: AutoGraph could not transform <function _combine_distributed_scaffold.<locals>.<lambda> at 0x7f7df039bea0> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n      lambda scaffold: scaffold.ready_op, args=(grouped_scaffold,))\r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n<combined_estimator.CombinedEstimator at 0x7f7df1f08ac8>\r\n```\r\n\r\nWhich makes me think that the problem you're seeing is related to autograph (which you'll notice is referenced in the stack trace also). Hope this helps you to debug.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41523\">No</a>\n"]}, {"number": 41522, "title": "remove unused loss var in momentum_test.py", "body": "remove unused loss var in momentum_test.py in `testSparseNesterovMomentum`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41522) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41522) for more info**.\n\n<!-- ok -->"]}, {"number": 41521, "title": "Model titled with int8 is not quantized(Artistic Style Transfer with TensorFlow Lite)", "body": "I am trying to compile the model [Style transform model (int8)](https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite) to edgetpu version, but the error message say that model is not quantized.\r\nCan anybody tell me how to fix it?\r\n\r\n\r\nI have downloaded 2 int8 models at the bottom of https://www.tensorflow.org/lite/models/style_transfer/overview\r\nStyle prediction model (int8) : https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/prediction/1?lite-format=tflite\r\nStyle transform model (int8) : https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/int8/transfer/1?lite-format=tflite\r\n\r\nI try to convert these 2 model to _edgetpu version by typing following commands in my terminal.\r\n>edgetpu_compiler magenta_arbitrary-image-stylization-v1-256_int8_prediction_1.tflite\r\n>edgetpu_compiler magenta_arbitrary-image-stylization-v1-256_int8_transfer_1.tflite\r\n\r\nThe compile for first file : *_prediction_1.tflite is OK.\r\nHowever, the compile for  *_transfer_1.tflite reports error:\r\n**********************************************************************\r\nEdge TPU Compiler version 14.1.317412892\r\nInvalid model: magenta_arbitrary-image-stylization-v1-256_int8_transfer_1.tflite\r\nModel not quantized\r\n**********************************************************************", "comments": ["Was able to reproduce the issue, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e7cd2b91834feae2f57366378fd88e05/41521.ipynb). Thanks!", "This is expected error as the style transform model is not fully quantized (some operations are not quantized). \r\nYou will need to change the model to be able to get fully quantized version.\r\n"]}, {"number": 41520, "title": "Generators not compatible with ragged tensors when using model.fit", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: cuDNN 10\r\n- GPU model and memory: V100, 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen providing model.fit with a python generator that creates ragged inputs for a keras model, while the outputs of the generator will allow model.fit to train, if one provides the generator to model.fit, it will not. Below, I have attached a standalone script showing in synthetic data, how when the generator is incorporated into a for loop, the model will fit on the outputs of the generator but if one provides the generator to the model.fit command, it will not train.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should train given the python generator according to the docs.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef turn_into_ragged(x):\r\n    return tf.RaggedTensor.from_row_lengths(tf.concat(\r\n        [tf.RaggedTensor.from_row_lengths(np.concatenate(sample, axis=0), list(map(len, sample))) for sample in x],\r\n        axis=0), list(map(len, x)))\r\n\r\ndef get_batches_m(x,y,batch_size=10,random=False):\r\n    \"\"\" Return a generator that yields batches from vars. \"\"\"\r\n    #batch_size = len(x) // n_batches\r\n    if len(x[0]) % batch_size == 0:\r\n        n_batches = (len(x[0]) // batch_size)\r\n    else:\r\n        n_batches = (len(x[0]) // batch_size) + 1\r\n\r\n    sel = np.asarray(list(range(x[0].shape[0])))\r\n    if random is True:\r\n        np.random.shuffle(sel)\r\n\r\n    for ii in range(0, n_batches * batch_size, batch_size):\r\n        # If we're not on the last batch, grab data with size batch_size\r\n        if ii != (n_batches - 1) * batch_size:\r\n            sel_ind=sel[ii: ii + batch_size]\r\n        else:\r\n            sel_ind = sel[ii:]\r\n\r\n        x_out = [turn_into_ragged(var[sel_ind]) for var in x]\r\n        y_out = [var[sel_ind] for var in y]\r\n\r\n        yield tuple(x_out),tuple(y_out)\r\n\r\ndef generate_syn_data(n_i=2000, n_s=30, n_t=200, shape=(10, 10, 3)):\r\n    values = np.random.uniform(0, 1, (n_i, ) + shape).astype(np.float32)\r\n    idx_t = np.random.choice(n_t, n_i)\r\n    _, l = np.unique(idx_t, return_counts=True)\r\n    rt0 = tf.RaggedTensor.from_row_lengths(values, l)\r\n    idx_s = np.random.choice(n_s, n_t)\r\n    _, l = np.unique(idx_s, return_counts=True)\r\n    rt1 = tf.RaggedTensor.from_row_lengths(rt0, l)\r\n    y = tf.constant(np.eye(2)[np.random.choice(2, n_s)].astype(np.float32))\r\n    return  rt1, y\r\n\r\ndef basic_ragged_graph(input_shapes):\r\n    ragged_inputs = [tf.keras.layers.Input(shape=(None, None) + shape, dtype=tf.float32, ragged=True) for shape in input_shapes]\r\n    sample_aggregation = tf.concat([tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=(1, 2)))(ragged_input) for ragged_input in ragged_inputs], axis=1)\r\n    logits = tf.keras.layers.Dense(units=2, activation=None)(tf.keras.layers.Flatten()(sample_aggregation))\r\n    return tf.keras.Model(inputs=ragged_inputs, outputs=[logits])\r\n\r\n#create simple model that takes 2 ragged inputs and returns 1 output\r\ntile_shape = (10, 10, 3)\r\nmodel = basic_ragged_graph([tile_shape,tile_shape])\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\r\n\r\n#generate synthetic data for inputs and outputs\r\nx1,x2 = generate_syn_data()[0].numpy(),generate_syn_data()[0].numpy()\r\ny = generate_syn_data()[1].numpy()\r\n\r\n#create generator objeect to batch and convert data to tf raggged\r\ntrain_gen = get_batches_m([x1,x2],[y],batch_size=5,random=True)\r\n#this method uses generator and outputs x_train,y_train that work\r\nfor x_train, y_train in train_gen:\r\n    model.fit(x_train, y_train)\r\n\r\n#however, when one provides this generator to the model.fit, the model will not train\r\ntrain_gen = get_batches_m([x1,x2],[y],batch_size=5,random=True)\r\nmodel.fit(train_gen)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Apparently, this may be related to an issue described here:  #41419 . I'm wondering if this is a similar problem where the output of the generator is cast to a tensor even though the output of the generator is a ragged tensor.", "@sidhomj \r\nI ran the code shared and face [this error](https://colab.research.google.com/gist/Saduf2019/e955df428d9ad1721c58b70037702ad7/untitled286.ipynb), please let us know if it confirms your issue.", "Yes, this is the error I receive.\r\n\r\nSent from my iPhone\r\n\r\nOn Jul 20, 2020, at 4:32 AM, Saduf2019 <notifications@github.com> wrote:\r\n\r\n\ufeff\r\n\r\n      External Email - Use Caution\r\n\r\n\r\n\r\n@sidhomj<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fsidhomj&data=02%7C01%7Cjsidhom1%40jhmi.edu%7C1fc0306f4ee44905c9e008d82c878015%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637308307763337636&sdata=ah%2BL%2FAu7slJdyKzXDs%2BUElZWbeiZlTRjhx1WYL2boO4%3D&reserved=0>\r\nI ran the code shared and face this error<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fcolab.research.google.com%2Fgist%2FSaduf2019%2Fe955df428d9ad1721c58b70037702ad7%2Funtitled286.ipynb&data=02%7C01%7Cjsidhom1%40jhmi.edu%7C1fc0306f4ee44905c9e008d82c878015%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637308307763337636&sdata=aC9eGKCyPfNr3LLy2kVkMN34ieLRIiFITbt9zCzdep8%3D&reserved=0>, please let us know if it confirms your issue.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F41520%23issuecomment-660885729&data=02%7C01%7Cjsidhom1%40jhmi.edu%7C1fc0306f4ee44905c9e008d82c878015%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637308307763347628&sdata=maLPzx5Rp9JNnLNbQJe2hfrTXU3D1L1ok6ALwkpt164%3D&reserved=0>, or unsubscribe<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAGVMZTG2ILLE2NRIHIL2QG3R4P6LLANCNFSM4O7IMY3A&data=02%7C01%7Cjsidhom1%40jhmi.edu%7C1fc0306f4ee44905c9e008d82c878015%7C9fa4f438b1e6473b803f86f8aedf0dec%7C0%7C0%7C637308307763347628&sdata=I%2Bv6GwV%2Bz0KMtpRQYrhoHyhSry0i1Ju%2F2bfN9ClG4O8%3D&reserved=0>.\r\n", "Python generators work well with numpy arrays, but for TensorFlow composite tensor types like RaggedTensor and SparseTensor, we would recommend you use tf.data.Datasets. You can read more about how to use Datasets in your input pipeline here: https://www.tensorflow.org/guide/data", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41520\">No</a>\n"]}, {"number": 41518, "title": "Restoring  trained ResNet50v2 model excluding last layer", "body": "I have posted the question on stack overflow[Link](https://stackoverflow.com/questions/62946756/restoring- #tensorflow-resnet50v2-model-without-last-layer) \r\n\r\nI am very confused about the documentation as I did not find how do I load the checkpoint of one model to different model.", "comments": ["I have posted an answer that you might find useful [here](https://stackoverflow.com/a/62965660/13952116)", "Thank you for your answer.Is there any way to directly restore weights to the new model excluding last layer rather than first restoring to pre-trained model. ", "You must have saved the weights as an .h5 file so when you load it, you have to have the original model in full instantiated before you modify the network.", "Yes, I have saved it as .h5 file.  I have loaded the weights in original model but now can you please guide me how to modify this original model to get features from `Dense(2048)` but not from `Dense(10)`.", "I have tried to use\r\n`cnn_model.set_weights(pretrained_model.get_weights()[:-2])` but got the following error\r\n\r\nValueError: You called `set_weights(weights)` on layer \"sequential_1\" with a weight list of length 270, but the layer was expecting 272 weights. Provided weights: [array([[[[-1.26184434e-01,  3.86615060e-02, -2.74...\r\n\r\n", "Could you link this all in a colab file with the link to download the .h5 files? I'll see what error might be there.", "https://colab.research.google.com/drive/1DUy2zJ_4glsOAsfV1968LSxQqgp_CZtm?usp=sharing", "\r\n\r\n[link for .h5 file](https://drive.google.com/file/d/10-gZ4MifhfV3woPSSgQ69XJ7npFyPUYO/view?usp=sharing)\r\n\r\n", "Please [check this out](https://colab.research.google.com/drive/1sHvRFjfnMp7t4ikZg2G7A0aUSxX6_Q46#scrollTo=AO1rph7Snhmb), I have seen to every way you can set model weights.", "> I have tried to use\r\n> `cnn_model.set_weights(pretrained_model.get_weights()[:-2])` but got the following error\r\n> \r\n> ValueError: You called `set_weights(weights)` on layer \"sequential_1\" with a weight list of length 270, but the layer was expecting 272 weights. Provided weights: [array([[[[-1.26184434e-01, 3.86615060e-02, -2.74...\r\n\r\nYou should use\r\n `cnn_model.set_weights(pretrained_model.get_weights()[:-1])`\r\nbecause if you use -2 you skip 2 layers", "Thank you for your support. \r\n", "I am going to try this and then will update you. ", "Sure", "If I define the Sequential model in your way then the command `cnn_model.set_weights(pretrained_model.get_weights()[:-2])` works but if I define Sequential model in my way (as mentioned in my colab link) then this command does not work. Is there any difference in defining Sequential model in both ways", "So in your nb, cnn_model is the same as pretrained_model so if you set reduced weights into the same network, there will be a mismatch", "Yes , it is same but I have not included the last dense layer ( `Dense(10)`) in cnn_model ", "Okay so I checked both model summaries and pretrained model has one dense layer Dense(10), and cnn_model also has one dense layer, Dense(2048). Cnn_model layers aren't an exact subset of pretrained_model weights", "Once again, Thank you for your help", "Hope all is resolved and you close this issue", "Sure"]}, {"number": 41517, "title": "Add minor TODO comment - test PR", "body": "Test PR", "comments": []}, {"number": 41516, "title": "Add commented arguments and new methods to FileSystem for Transactions", "body": "This PR is a continuation of series of PRs introducing Transactions API to FileSystems. In this PR new Transaction API methods have been added to FileSystem base class and existing method signatures are changed to add Transaction Token arguments in commented form. Previous PRs, #41431, #41433, #41434, #41462 and #41463 and this PR merged, I will open another PR to uncomment the extra arguments and enable new API. ", "comments": []}, {"number": 41515, "title": "[ROCm] ROCm support for MLIR generated kernels", "body": "This PR contains prototype code to add ROCm support for MLIR generated kernels.\r\n\r\nThe intent behind this PR is to share what we have done and invite feedback on whether or not we are going down the right path, and what changes we need to make.\r\n\r\nReviewing the PR on a per commit basis will work best\r\n\r\nNote this is prototype code, which is half-baked in that it is hardcoded to generate GPU binaries for the \"gfx900\" architecture, and does not yet have the ability to support gpu binaries for more than one gpu_arch. However it is fully functional, and the unit test `//tensorflow/core/kernels/mlir_generated:gpu_tanh_test` will successfully run and pass with it on the ROCm platform\r\n\r\n--------------------------\r\n\r\n/cc @whchung @chsigg @cheshire @nvining-work ", "comments": ["> hi Adrian,\r\n> \r\n> Thank you for your prompt feedback. will incorporate it in the next iteration of the prototype.\r\n> Please cc me when you push out the changes for `abs`, and I will make the corresponding ROCm updates in my prototype as well.\r\n> \r\n> thanks\r\n\r\nThis change has now landed:\r\nhttps://github.com/tensorflow/tensorflow/commit/a6cd18a1334be6b54784f1d65fe5d435d31d2bb8", "@deven-amd  Can you please resolve conflicts? Thanks!", "> > hi Adrian,\r\n> > Thank you for your prompt feedback. will incorporate it in the next iteration of the prototype.\r\n> > Please cc me when you push out the changes for `abs`, and I will make the corresponding ROCm updates in my prototype as well.\r\n> > thanks\r\n> \r\n> This change has now landed:\r\n> [a6cd18a](https://github.com/tensorflow/tensorflow/commit/a6cd18a1334be6b54784f1d65fe5d435d31d2bb8)\r\n\r\n@akuegel \r\nThank you for the heads up. I will update this prototype to incorporate the changes in the above commit (sometime next week, when our fork syncs up with the upstream repo and picks up the above commit in the process)\r\n\r\n@gbaned \r\nThe merge-conflicts will be gone once I update the prototype as noted above and push out that commit\r\nKeep in mind that (atleast for now) this PR represents prototype code that is not intended to be merged. Once we get to a stage where the changes in this PR are clean enough, I will remove the `[prototype]` prefix from the PR title. \r\n\r\nthanks", "Sorry, I was on vacation the last two weeks, and returned yesterday. Still catching up on emails.\r\nIt seems in the meantime there were some changes which have again lead to merge conflicts, can you please resolve these?\r\nBtw, by now the MLIR generated kernels for abs and tanh are enabled by default for the cuda code path, and this seems to have stuck. So it looks like this will become part of the next tensorflow release.", "@akuegel \r\n\r\nI have rebased the PR to resolve the merge conflicts. Please re-review. \r\n\r\nYou mentioned that these MLIR generated kernels (for tanh and abs) will become default (on the cuda path) in the next release. Can you shed some light on why we are going down this route (of using MLIR generated kernels on GPU, in the non-XLA path)? \r\n\r\nIs the eventual goal here to replace all/most Eigen generated kernels with MLIR generated ones? \r\n", "> You mentioned that these MLIR generated kernels (for tanh and abs) will become default (on the cuda path) in the next release. Can you shed some light on why we are going down this route (of using MLIR generated kernels on GPU, in the non-XLA path)?\r\n> \r\n> Is the eventual goal here to replace all/most Eigen generated kernels with MLIR generated ones?\r\n\r\nI don't know how much I can share, but there will be a public tech talk about this which was announced as part of the MLIR newsletter:\r\nhttps://llvm.discourse.group/t/mlir-news-13th-edition-8-7-2020/1478\r\n(section Tensorflow - Kernel Generator)", "> Can you shed some light on why we are going down this route (of using MLIR generated kernels on GPU, in the non-XLA path)?\r\n\r\nThere are a few reasons, but: we could express TF ops as composition and just reuse the code generator when building TF to have the kernel generated AOT for these compiled op. Ultimately having the JIT using the same code path as the AOT codegen for most computations reduces the risk of divergence and make it more transparent to switch from one to another.", "It seems there is yet another merge conflict in cubin_creator.cc, can you please resolve this?", "@akuegel \r\n\r\nI have rebased the PR to resolve the merge-conflict.\r\nThe unfortunate side effect of that is that the two associated unit-tests (abs and tanh) are no longer passing after the rebase. They both crash during the phase that genrates GPU kernel code objects from the input MLIR. The crash is in the MLIR back end, which has changed with the rebase.\r\n\r\nWas wondering whether this crash is ROCm specific or is it something you are seeing on the CUDA side too?\r\n\r\nthanks\r\n\r\nerror message and stack trace at the point of failure\r\n```\r\nERROR: /root/tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:123:1: compile tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.gfx803.hsaco failed (Segmentation fault): tf_to_gpu_binary failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary '--same_shape=0,1' '--unroll_factors=4' '--tile_sizes=256' '--arch=803' '--input=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/abs_f16.mlir' '--output=bazel-out/k8-opt/bin/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.gfx803.hsaco')\r\nExecution platform: @local_execution_config_platform//:platform\r\nPLEASE submit a bug report to  and include the crash backtrace.\r\n```\r\n\r\n\r\n\r\n```\r\n(gdb) where\r\n#0  0x0000555558d32a87 in mlir::detail::OperandStorage::OperandStorage(mlir::Operation*, mlir::ValueRange) ()\r\n#1  0x0000555558d3585a in mlir::Operation::create(mlir::Location, mlir::OperationName, llvm::ArrayRef<mlir::Type>, llvm::ArrayRef<mlir::Value>, mlir::MutableDictionaryAttr, llvm::ArrayRef<mlir::Block*>, unsigned int) ()\r\n#2  0x0000555558d35964 in mlir::Operation::create(mlir::Location, mlir::OperationName, llvm::ArrayRef<mlir::Type>, llvm::ArrayRef<mlir::Value>, mlir::MutableDictionaryAttr, llvm::ArrayRef<mlir::Block*>, mlir::RegionRange) ()\r\n#3  0x0000555558d35ba7 in mlir::Operation::create(mlir::OperationState const&) ()\r\n#4  0x0000555558d8b684 in mlir::OpBuilder::createOperation(mlir::OperationState const&) ()\r\n#5  0x0000555557facacb in mlir::GPUIndexIntrinsicOpLowering<mlir::gpu::BlockIdOp, mlir::ROCDL::BlockIdXOp, mlir::ROCDL::BlockIdYOp, mlir::ROCDL::BlockIdZOp>::matchAndRewrite(mlir::Operation*, llvm::ArrayRef<mlir::Value>, mlir::ConversionPatternRewriter&) const ()\r\n#6  0x0000555558af4f13 in mlir::ConversionPattern::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&) const ()\r\n#7  0x0000555558d284cb in mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::RewritePattern const&, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::RewritePattern const&)>, llvm::function_ref<void (mlir::RewritePattern const&)>, llvm::function_ref<mlir::LogicalResult (mlir::RewritePattern const&)>) ()\r\n#8  0x0000555558d28688 in mlir::PatternApplicator::matchAndRewrite(mlir::Operation*, mlir::PatternRewriter&, llvm::function_ref<bool (mlir::RewritePattern const&)>, llvm::function_ref<void (mlir::RewritePattern const&)>, llvm::function_ref<mlir::LogicalResult (mlir::RewritePattern const&)>) ()\r\n#9  0x0000555558afe3a9 in (anonymous namespace)::OperationLegalizer::legalize(mlir::Operation*, mlir::ConversionPatternRewriter&) ()\r\n#10 0x0000555558afef36 in (anonymous namespace)::OperationConverter::convertOperations(llvm::ArrayRef<mlir::Operation*>) ()\r\n#11 0x0000555558b0035c in mlir::applyFullConversion(llvm::ArrayRef<mlir::Operation*>, mlir::ConversionTarget&, mlir::OwningRewritePatternList const&) ()\r\n#12 0x0000555558b00470 in mlir::applyFullConversion(mlir::Operation*, mlir::ConversionTarget&, mlir::OwningRewritePatternList const&) ()\r\n#13 0x0000555557d1d330 in xla::mlir_gpu::(anonymous namespace)::LowerToROCDLPass::runOnOperation() ()\r\n#14 0x0000555558c51d08 in mlir::Pass::run(mlir::Operation*, mlir::AnalysisManager) ()\r\n#15 0x0000555558c51dda in mlir::OpPassManager::run(mlir::Operation*, mlir::AnalysisManager) ()\r\n#16 0x0000555558c51f2e in mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl()::{lambda(llvm::MutableArrayRef<mlir::OpPassManager>)#1}::operator()(llvm::MutableArrayRef<mlir::OpPassManager>) const ()\r\n#17 0x0000555558c5731e in mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl() ()\r\n#18 0x0000555558c51d08 in mlir::Pass::run(mlir::Operation*, mlir::AnalysisManager) ()\r\n#19 0x0000555558c51dda in mlir::OpPassManager::run(mlir::Operation*, mlir::AnalysisManager) ()\r\n#20 0x0000555558c595f2 in mlir::PassManager::run(mlir::ModuleOp) ()\r\n#21 0x0000555557d1e0fe in xla::mlir_gpu::LowerKernelBodiesToROCDL(mlir::ModuleOp) ()\r\n#22 0x0000555555a7e736 in tensorflow::kernel_gen::GenerateGpuBinaryForTfCode(llvm::StringRef, std::pair<int, int>, llvm::ArrayRef<unsigned int>, llvm::ArrayRef<unsigned int>, llvm::ArrayRef<unsigned int>) ()\r\n#23 0x00005555559f6d7d in main ()\r\n(gdb) quit\r\n\r\n```\r\n", "@akuegel \r\n\r\nfound the cause of crash....was failing to account (on the ROCDL side) the following change that got picked up when I rebased the PR on the tip.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/bcfb691a79a9e9c742ee9e4a7bf5af0a9edb1746#diff-d0fe1cfb091a0f7a5f03b710558059d7\r\n\r\nI have made the correponding changes on the ROCDL side, and the two unit tests are back to passing.\r\n\r\n\r\n", "@deven-amd  Can you please check build failures. Thanks!", "@akuegel \r\n\r\npushed out another rebase to resolve the `buildifier` errors in `Ubuntu Sanity`....please re-approve. thanks", "For some reason, it seems this fails on the ROCM Kokoro. The problem is that it fails to find the file bin2c.py on the path returned by repository_ctx.path(Label(\"//third_party/gpus/rocm:bin2c.py\"))\r\nI tried the whole day all kind of stuff whether I can make it work, but failed. Maybe you have an idea? My guess is that this is somehow related to remote execution, but I don't really know. And the only way I could try it out was to repeatedly start another Kokoro run (and then see it fail).", "> Maybe you have an idea? My guess is that this is somehow related to remote execution, but I don't really know.\r\n\r\nI do not quite understand how the remote exectution part works either :(\r\n@chsigg would you happen to know? thanks\r\n", "> > Maybe you have an idea? My guess is that this is somehow related to remote execution, but I don't really know.\r\n> \r\n> I do not quite understand how the remote exectution part works either :(\r\n> @chsigg would you happen to know? thanks\r\n\r\nI got some help internally. Essentially remote execution doesn't support file upload yet. I am now trying whether we can just use hexdump in the genrule, so that we don't need the bin2c tool."]}, {"number": 41514, "title": "Improve error message in shape check", "body": "This PR tries to address the issue raised in #41504 where the\r\nerror message of:\r\n```\r\nInvalidArgumentError: Shape [2,2,2,2,2,2,2,2,2,2,2,2] would have more than 2**63 - 1 elements [Op:BroadcastTo]\r\n```\r\nis not intuitive.\r\n\r\nThe issue is that the construction of error message in tensor_shape.cc\r\ndid not copy the complete shape dims.\r\n\r\nThis PR fixes the issue.\r\n\r\nThis PR fixes #41504.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["There is a test failure (`//tensorflow/python/kernel_tests:broadcast_to_ops_test`), can you check please?\r\n\r\n```\r\n======================================================================\r\nERROR: testBroadcastToInvalidShape (__main__.BroadcastToTest)\r\nBroadcastToTest.testBroadcastToInvalidShape\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/python/framework/ops.py\", line 1839, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 3 and 2 for '{{node BroadcastTo}} = BroadcastTo[T=DT_INT32, Tidx=DT_INT32](Const, BroadcastTo/shape)' with input shapes: [3], [12] and with input tensors computed as partial shapes: input[1] = [110,53,104,147,157,123,5,24,188,40,5,2].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/python/kernel_tests/broadcast_to_ops_test.py\", line 200, in testBroadcastToInvalidShape\r\n    v = array_ops.broadcast_to(constant_op.constant(x), output_shape)\r\n  File \".../tensorflow/python/ops/gen_array_ops.py\", line 855, in broadcast_to\r\n    \"BroadcastTo\", input=input, shape=shape, name=name)\r\n  File \".../tensorflow/python/framework/op_def_library.py\", line 776, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \".../tensorflow/python/framework/ops.py\", line 3518, in _create_op_internal\r\n    op_def=op_def)\r\n  File \".../tensorflow/python/framework/ops.py\", line 2002, in __init__\r\n    control_input_ops, op_def)\r\n  File \".../tensorflow/python/framework/ops.py\", line 1842, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Dimensions must be equal, but are 3 and 2 for '{{node BroadcastTo}} = BroadcastTo[T=DT_INT32, Tidx=DT_INT32](Const, BroadcastTo/shape)' with input shapes: [3], [12] and with input tensors computed as partial shapes: input[1] = [110,53,104,147,157,123,5,24,188,40,5,2].\r\n```", "Thanks @mihaimaruseac. I have updated the PR so that exception check captures both ` (ValueError, errors.InvalidArgumentError)`. This will allow validation caused by different routes. Please take a look and let me know if the issue still happens in internal tests."]}, {"number": 41513, "title": "tf 2.2.0 achieves much worse results than tf 2.1.0, same model", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Have I written custom code (as opposed to using a stock example script\r\nprovided in TensorFlow):\r\n\r\nYes, i have custom code.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nUbunto 18.04\r\n\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\nhappens on a mobile device:\r\n\r\nNA\r\n\r\nTensorFlow installed from (source or binary):\r\n\r\npip3 install tensorflow==2.2\r\n\r\nTensorFlow version (use command below):\r\n\r\n2.2 (versus 2.1)\r\n\r\nPython version:\r\n\r\n3.6.9\r\n\r\n\r\nBazel version (if compiling from source):\r\n\r\nNA\r\n\r\nGCC/Compiler version (if compiling from source):\r\n\r\nNA\r\n\r\nCUDA/cuDNN version:\r\n\r\nCUDA version 11.0 according to nvidia-smi\r\n\r\nGPU model and memory:\r\n\r\nJetson Nano (Tegra X1), Geforce RTX 2070\r\n\r\nExact command to reproduce:\r\n\r\nNA (running my custom tensorflow application)\r\n\r\n\r\n\r\n\r\nSo... i have a transformer model which was achieving great results with TensorFlow 2.1, and after updating to TensorFlow 2.2, the results are bad. ", "By bad results do you mean training takes longer or inferences are bad?", "Yes GPU is recognized. Inferences are bad :(\r\n", "Can you link a notebook showing infrences in both versions?", "I could make it a single-file script and include it right here?\r\nNo idea how to make a notebook that can use a specific version of tensorflow.\r\n", "Yes please do share, I'll run it with both versions", "Ok, here is a repro case. This is a new project, a work in progress. The aim of the network is to predict the next word,\r\ngiven a sequence of words. After a very short training the script will display pairs of input and output sequences. When using tf 2.1, the results are usually perfect. Not so with tf 2.2, as you will see.\r\n\r\nI could not attach a .py file so i renamed trans.py to trans.txt, here it is :\r\n\r\n[trans.txt](https://github.com/tensorflow/tensorflow/files/4942352/trans.txt)\r\n\r\n\r\nI hope this is of any use to you! Note that the loss goes down equally fast with tf 2.1 as with tf 2.2, but you will see that the final results are very different... So somehow the current loss function is not a good indicator of output quality. Anyhow, i think that tf 2.2 should give similar results as tf 2.1, except maybe faster?\r\n\r\n\r\n", "I tried both scripts out, [TF 2.2.0](https://gist.github.com/lordtt13/0a96255e78e5009bde394b3e588801e1) and [TF 2.1.0](https://gist.github.com/lordtt13/f94e4fba62d311e8483f17b4e0cd3a9e). A rather obvious suggestion would be to try out a bigger dataset because overfitting may have been causing these issues.", "at this point in time, it is exactly the overfitting that i want to see\nas a sort of verification that the model is doing what i expect it to do\n", "Ah yes, now i see your results... they are very much like mine!\nThanks for confirming. Overfitting or not, i would expect two versions of\ntensorflow to give similar results.\n", "Yeah, I'm stumped too.", "@notnot \r\nI ran the code on 2.2 andd 2.1 please find the [gist for 2.2](https://colab.research.google.com/gist/Saduf2019/7629336bee8b00a87187bfc17fed04e4/untitled307.ipynb) and [2.1](https://colab.research.google.com/gist/Saduf2019/277384e19026ed2d13e571de241a6cc7/untitled308.ipynb) please let us know if it confirms the issuereported.", "Yes this totally confirms it. Worst results i have ever seen, with tf 2.2,\nwhile the tf 2.1 results are perfect...\n", "@notnot please check with tf2.3, the issue has been fixed now, check [here](https://gist.github.com/lordtt13/c8e6870408c656dc40bc2d9b884ba0d6)", "Thanks for your issue. This is fixed TF 2.3 now.\r\nSee the [gist](https://colab.research.google.com/gist/ymodak/e77a894202fe7c7e4d769b7d8deab0ea/untitled307.ipynb)"]}, {"number": 41512, "title": "Tensorflow", "body": "When I try using keras, says that I dont have tensorflow 2.2.0 installed, but i do. Can someone help me? I already uninstalled tensorflow and the same thing happens. Also I reinstalled Anaconda and nothing.\r\n\r\n\r\nfrom keras.models import Sequential\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\nImportError: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py)\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-9c5e0a19b646>\", line 1, in <module>\r\n    from keras.models import Sequential\r\n\r\n  File \"C:\\Users\\luiz_\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 6, in <module>\r\n    'Keras requires TensorFlow 2.2 or higher. '\r\n\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`", "comments": ["Try using Keras from inside TF if you have TF installed.\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\n```", "I tried and this happened:\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-f05745da3b73>\", line 1, in <module>\r\n    from tensorflow.keras.models import Sequential\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\nImportError: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py)", "Try one thing, remove all your python installations and CUDA installation and download using [Anaconda](https://docs.anaconda.com/anaconda/install/linux/) and you can install TF2 with CUDA bindings using:\r\n```bash\r\nconda install -c anaconda tensorflow-gpu\r\n```", "I tried, didnt work...\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-f05745da3b73>\", line 1, in <module>\r\n    from tensorflow.keras.models import Sequential\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\nImportError: cannot import name 'pywrap_tensorflow' from 'tensorflow.python' (C:\\Users\\luiz_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py)", "Can you describe your system details:\r\n#### System information:\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "**Systeminformation:**\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Microsoft Windows 10\r\n**TensorFlow installed from (source or binary):** Binary\r\n**TensorFlow version (use command below):** TensorFlow 2.2.0\r\n**Python version:** 3.7.6\r\n**Bazel version (if compiling from source):**\r\n**GCC/Compiler version (if compiling from source):**\r\n**CUDA/cuDNN version:** 10.0.130\r\n**GPU model and memory:**GeForce MX130; 8Gb RAM", "So I don't think GeForce MX130 has CUDA support, [(check here)](https://developer.nvidia.com/cuda-gpus)\r\n\r\n[PyWrap Error resolved here](https://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example)", "Please always fill in issue template.\r\n\r\nPlease use ` ``` ` before and after code and error blocks to make them readable.\r\n\r\nPlease post output of `pip list`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41512\">No</a>\n"]}, {"number": 41511, "title": "S3 read only memory region and stat", "body": "@mihaimaruseac \r\nThis PR adds `tf_read_only_memory_region`, `NewReadOnlyMemoryRegionFromFile` and `Stat` ( because we need `GetFileSize` ).\r\nThank you for doing an import manually with the previous PR.\r\nCould you please tell me why the it failed the internal import ?", "comments": ["I am not sure why it didn't work before. It's very likely that it is because the import happens at file level (so if a file changed, you need to rebase back on master), unlike git which is at change level", "@mihaimaruseac \nThe CI failed again. Could you check it please ? Thank you", "Will import manually."]}, {"number": 41510, "title": "ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n_Yes_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  _dockerhub container 'latest'  Digest:  08901711826b185136886c7b8271b9fdbe86b8ccb598669781a1f5cb340184eb_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n   _v2.2.0-rc4-8-g2b96f3662b 2.2.0_\r\n- Python version:\r\n   _Python 3.6.9_\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\non line `grads = tape.jacobian(w, [x, y])`\r\n`ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.`\r\n\r\n**Describe the expected behavior**\r\n\r\nI was expecting to get the jacobian calculation:\r\n`grads == [dwdx, dwdy]`\r\nor similar (not the error)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n        import tensorflow as tf\r\n\r\n        # x,y,v potentially a large batch\r\n        x = tf.constant([[1.0],[5.0]])\r\n        y = tf.constant([[2.0],[6.0]])\r\n        v = tf.Variable([[1.0],[3.0]])\r\n\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            tape.watch(x)\r\n            tape.watch(y)\r\n            tape.watch(v)\r\n\r\n            # w1 and w2 a nonlinear function independent for each row\r\n            w1 = (x + y + v )\r\n            w2 = y * y * v\r\n            w = tf.concat([w1, w2], axis=1)\r\n\r\n            grads = tape.jacobian(w, [x, y])\r\n\r\n        dgradsdv = tape.jacobian(grads, v)\r\n\r\n        print(dgradsdv)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nLog\r\n```\r\n    grads = tape.jacobian(w, [x, y])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\", line 1113, in jacobian\r\n    self._push_tape()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\", line 844, in _push_tape\r\n    raise ValueError(\"Tape is still recording, This can happen if you try to \"\r\nValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape.\r\n```\r\nIf I substitute `tape.jacobian` by `tape.gradient` there is no error.\r\nI would like to know how to implement these Jacobians efficiently.  Thanks!", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/173981f159dcf4853852de2404bedf8a/41510.ipynb). Thanks!", "I tried something similar with nested GradientTape, had the same issue with TF v2.3.", "Hi @andrescodas, I think this was reported a while ago in #34260 but because there was no reproducible example or follow up from the author, it fell through the cracks.\r\n\r\n@yidiq7 can you provide example code for the issue you're facing?", "Hi @nikitamaia, I'm glad to help and provide more information if required.  I managed to use calls to tape.gradient to replace the need for jacobian, however I'm concerned that this workaround might be inefficient. It would be useful to me if you could point out when to use GradientTape.graident vs GradientTape.jacobian vs GradientTape.batch_gradient, particularly when performing automatic differentiation on a function with many outputs to many inputs , and a batch to calculate on that function\r\n", "Hi @andrescodas @nikitamaia, after some debugging, I found it was probably due to the indentation:\r\nIn the original code provided by @andrescodas:\r\n\r\n```\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    ...\r\n    grads = tape.jacobian(w, [x, y])\r\n```\r\n\r\n\r\nIt should be like this instead:\r\n\r\n```\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    ...\r\ngrads = tape.jacobian(w, [x, y])\r\n```\r\n\r\nNot sure if this really does what you want, but the error was gone.\r\n\r\n", "Hi @yidiq7, thanks for digging.\r\n\r\nAlthough the problem disappear with the indentation you suggest, the behavior is different.   Let me illustrate with the following examples:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ny = tf.constant(2.0,)\r\nv = tf.Variable(3.0,)\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(y)\r\n    tape.watch(v)\r\n\r\n    w =  y * y * v\r\n\r\n    grads = tape.gradient(w, y)  # 2*y*v\r\ndgradsdv = tape.gradient(grads, v)  # 2 * y\r\n\r\nprint(grads)  # prints tf.Tensor(12.0, shape=(), dtype=float32)\r\nprint(dgradsdv) # prints tf.Tensor(4.0, shape=(), dtype=float32)\r\n```\r\nThis code above is working as expected, according to the comments.  Observe that I used gradient instead of jacobian to avoid the `ValueError` issue.\r\n\r\nIf I use the indentation you suggest, the first gradient operation is not in the tape, and therefore I think the answer is correctly None, because the graph missed the last operation linking v to grads, see:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ny = tf.constant(2.0,)\r\nv = tf.Variable(3.0,)\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(y)\r\n    tape.watch(v)\r\n\r\n    w =  y * y * v\r\n\r\ngrads = tape.gradient(w, y)  # 2*y*v  # operation not taped !\r\ndgradsdv = tape.gradient(grads, v)  # ?\r\n\r\nprint(grads)  # prints tf.Tensor(12.0, shape=(), dtype=float32)\r\nprint(dgradsdv) # None\r\n```\r\n\r\nLastly, for the record:  The code below still raises `ValueError` and it is simpler than the one in the first post:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ny = tf.constant(2.0,)\r\nv = tf.Variable(3.0,)\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(y)\r\n    tape.watch(v)\r\n\r\n    w =  y * y * v\r\n\r\n    grads = tape.jacobian(w, y) \r\n\r\n```", "A nested GradientTape can give you correct results. (Note the indentation for jacobian here)\r\nAlso persistent=True is not required in this case.\r\n\r\nI am not sure if this is the most efficient way to do this though.\r\n```python\r\nimport tensorflow as tf\r\n\r\ny = tf.constant(2.0,)\r\nv = tf.Variable(3.0,)\r\n\r\nwith tf.GradientTape() as tt:\r\n    tt.watch(v)\r\n    with tf.GradientTape() as t:\r\n        t.watch(y)\r\n        w =  y * y * v\r\n\r\n    grads = t.jacobian(w, y)  # 2*y*v  \r\ndgradsdv = tt.jacobian(grads, v)  # 2*y\r\n\r\nprint(grads) # <tf.Tensor: shape=(), dtype=float32, numpy=12.0>\r\nprint(dgradsdv) # <tf.Tensor: shape=(), dtype=float32, numpy=4.0>\r\n```\r\n\r\n", "@andrescodas does the suggestion from @yidiq7 work for you?", "Hi @nikitamaia ,\r\n\r\n> @andrescodas does the suggestion from @yidiq7 work for you?\r\n\r\nIt computes the correct answer.  However:\r\n\r\n- It is an open question whether it is the most efficient way (or preferred way) to compute a batch_jacobian of many targerts with respect to many sources.\r\n- It seems to be bug that GradientTape.jacobian raises ValueError while recording within the gradient context.\r\n- The message `ValueError: Tape is still recording, This can happen if you try to re-enter an already-active tape` is misleading because the user piece of code does not leave and re-enter the tape context.", "Agreed, that is a misleading message and I think a bug. It is currently being investigated and I can update this thread when there's a fix. For now, use the workaround from @yidiq7 and I'll post any updates here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41510\">No</a>\n"]}, {"number": 41509, "title": "Add string_print_f fuzzer", "body": "@mihaimaruseac ", "comments": ["@grasskin Can you please address Ubuntu Sanity errors? Thanks!", "@grasskin Still, Ubuntu Sanity errors appearing, Can you fix those?. Thanks!", "Will import manually"]}, {"number": 41508, "title": "[DNM] Added full integer quantization support for TFLM magic_wand on zephyr_riscv", "body": "This pull request adds support to run integer quantized model of TFLM magic_wand on zephyr_riscv.\r\n* Added code in `magic_wand/train/train.py` to support int8 quantized model (use argument `--fully-quantized`)\r\n* Added code in `magic_wand/train/train_magic_wand_model.ipynb` to reflect the change in `magic_wand/train/train.py`\r\n* Added int8 quantized model data in `magic_wand/magic_wand_model_data.cc` (uncomment the line `// #define USE_INT8_QUANTIZED` to use int8 quantized model)\r\n\r\nThe following changes ONLY applies to zephyr_riscv (`magic_wand/zephyr_riscv/src`)\r\n* Modified `magic_wand/zephyr_riscv/Makefile.inc` to use platform specific `main_functions.cc` and `.h` on zephyr_riscv\r\n* Added missing ops in `main_functions.cc` (DEQUANTIZE, QUANTIZE, and RESHAPE) for support of int8 quantized model\r\n* Added inline assembly in `main_functions.cc` to measure elapsed CPU cycle counts (NOT actual time due to hardware limitation, the clock speed is fixed at 100 MHz) while running main functions (uncomment the line `// #define ZEPHYR_RISCV_PROFILE` to enable it)\r\n\r\nThe above changes related to zephyr_riscv are both tested on Xilinx Arty A7-35T programmed with Litex/VexRiscv SoC that runs Zephyr RISC-V and simulated on Renode with the Renode scripts (*.resc) provided by Antmicro, see [antmicro/litex-vexriscv-tensorflow-lite-demo](https://github.com/antmicro/litex-vexriscv-tensorflow-lite-demo) for more details.\r\n\r\nThe following table shows the number of CPU (100 MHz) cycles for each main function to run on both Xilinx Arty A7 and Renode\r\n\r\n|                       | Before `Invoke()` | `Invoke()` | `PredictGesture()` | `HandleOutput()` |\r\n|-------------------|-----------------:|----------:|----------------:|--------------:|\r\n| float32 (Arty A7) | 275M            | 18.5M    | 8k             | 350          |\r\n| float32 (Renode)  | N/A             | 10M      | 1.5k           | 10           |\r\n| int8 (Arty A7)    | 275M            | 5.6M     | 8k             | 350          |\r\n| int8 (Renode)     | N/A             | 3M       | 1.8k           | 10           |\r\n\r\n#### Note\r\n* Cycle counts are averaged and rounded for clarity\r\n* Before `Invoke()`: The time to collect enough data for inference, N/A for Renode because the data is fed from a file, not determined by the actual sensor.\r\n* Actual speed may be faster because printing out messages adds overhead to the system (`#define ZEPHYR_RISCV_PROFILE` is uncommented in the experiment)", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/41508\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "Noting that this PR is not going to be merged since we are not currently able to accept changes to the examples.", "> Noting that this PR is not going to be merged since we are not currently able to accept changes to the examples.\r\n\r\nAgree, thanks for coming back to this PR @advaitjain "]}, {"number": 41507, "title": "GV100 Tensorflow 2.0.0 not working", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["![image](https://user-images.githubusercontent.com/14827177/87822046-d5e05d80-c83e-11ea-990b-991d5d08ce82.png)\r\n", "How have you compiled TF?", "Yes\r\nprint(tf.__version__)\r\n2.0.0\r\nf.test.gpu_device_name()\r\n' '", "I meant did you install it using conda or from source or from binaries?", "you should build tensorflow image by yourself in order to use old deprecated version of nvidia-driver and cuda", "@yskale \r\nPlease provide with detailed information for us to help you, we see that you have not filled the issue template. steps followed before you faced this issue.Thanks!", "@lordtt13 , I have installed it using pip.\r\n@alanpurple Can give list down the steps for it.", "x86_64\r\nNAME=\"SLES\"\r\nVERSION=\"15\"\r\nVERSION_ID=\"15\"\r\nPRETTY_NAME=\"SUSE Linux Enterprise Server 15\"\r\nID=\"sles\"\r\nID_LIKE=\"suse\"\r\nANSI_COLOR=\"0;32\"\r\nCPE_NAME=\"cpe:/o:suse:sles:15\"\r\n\r\nKernel - > 4.12.14-150.47-default\r\nGCC - > gcc (SUSE Linux) 7.5.0\r\n\r\n![image](https://user-images.githubusercontent.com/14827177/87972869-a8412180-ca95-11ea-917b-d2c896bb2bf7.png)\r\n\r\n", "I am able to run the cuda sample programs but the gpu are not visible in python for torch and tensorflow.\r\nI have installed torch using coda and for diiferent version of torch and cuda but do not see the gpu. ", "@yskale I think there is a mismatch in CUDA drivers. Please check the tested build configurations [here](https://www.tensorflow.org/install/source#gpu).  Please check the CUDA driver version for `TF2.0`. As you have used `pip` binaries, Tensorflow2.0 will be looking for `*CUDA10.0` folder for related drivers and you have `*CUDA10.0`, so there is a mismatch.\r\n\r\nCan you please uninstall cuda10.1, uninstall TF, and follow the instructions on TF website ( install cuda10.0 and then install TF).\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.0.0 | 2.7, 3.3-3.7 | GCC 7.3.1 | Bazel 0.26.1 | 7.4 | 10.0\r\n\r\nAlternatively, you can install TF2.2 (which requires CUDA10.1) and see whether that solves your problem. TF2.2 is much better than TF2.0.\r\n \r\n\r\nPlease let us know how it progresses. Thanks!", "Thanks for the information @jvishnuvardhan , I was able to figure out the problem and fix it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41507\">No</a>\n"]}, {"number": 41506, "title": "C++ TensorFlow Build doesn't find TensorFlow .h files that exist.  Also \"Please update your includePath\"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS Oracle VM Virtualbox inside Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Binary https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz\r\n- TensorFlow version: 1.15.0\r\n- Python version: Actually I'm using the C++ 0.29.0 extension in Visual Studio Code 1.47.1, but my Python version is Python 3.8.2\r\n- Installed using virtualenv? pip? conda?: No installed from TensorFlow C++ binary libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a?\r\n- GPU model and memory: NVIDIA GeForce GTS 965M with 2,048 MB GDDR5 memory.  32 GB RAM.\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFails to compile in C++ in Visual Studio Code test program, testing for correct installation of TensorFlow, because of errors on two TensorFlow #include.  See the two compile errors below and the three Visual Studio Code warnings after those, which give more information.\r\n\r\nI have to use TensorFlow for C++ NOT TensorFlow for Python because I am trying to do a C++ online course project capstone where I have decided to use TensorFlow to do a convolutional neural network to classify dog and cat pictures.  I cannot use Python for that because it is a C++ course.\r\n\r\nWhen I use #include <tensorflow/c/c_api.h>\r\n```\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ g++ test.cpp\r\ntest.cpp:2:10: fatal error: tensorflow/c/c_api.h: No such file or directory\r\n    2 | #include <tensorflow/c/c_api.h>\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ \r\n```\r\nWhen I use #include \"/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h\"\r\n```\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ g++ test.cpp\r\nIn file included from test.cpp:2:\r\n/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h:22:10: fatal error: tensorflow/c/tf_attrtype.h: No such file or directory\r\n   22 | #include \"tensorflow/c/tf_attrtype.h\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$\r\n```\r\n\r\nI installed TensorFlow for C++ on Ubuntu 20.04 LTS using these instructions: https://www.tensorflow.org/install/lang_c\r\n\r\nBelow are the three warnings I get from Visual Studio Code before compiling, which give more information than the compile error does:\r\n```\r\n#include <tensorflow/c/c_api.h>\r\n```\r\nWARNING for the above code: cannot open source file \"tensorflow/c/c_api.h\"C/C++(1696)\r\n\r\n```\r\n#include \"/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h\"\r\n```\r\nWARNING for the above code: #include errors detected. Please update your includePath. Squiggles are disabled for this translation unit (/home/tlroot/Documents/C++/Capstone/CppND-Capstone-Hello-World/src/test.cpp).C/C++(1696)\r\n\r\nWARNING for the above code: cannot open source file \"tensorflow/c/tf_attrtype.h\" (dependency of \"/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h\")C/C++(1696)\r\n\r\nHere is the build example program from https://www.tensorflow.org/install/lang_c that tests if the TensorFlow install works:\r\n```\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n#include \"/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h\"\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n```\r\nNote: #include <stdio.h> WORKS but neither TensorFlow #include works.  I have run C++ projects in this setup and those programs worked.  See an example: https://github.com/ProfHariSeldon/CppND-System-Monitor.  So I know the C++ works what isn't working is installing TensorFlow.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFollow instructions to install C++ TensorFlow from: https://www.tensorflow.org/install/lang_c.  See below steps:\r\n\r\n1. Download https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz\r\n\r\n2. Untar libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz in cd /usr/local/lib/\r\n\r\n3. cd /usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0\r\n\r\n4. sudo ldconfig\r\nNo error messages for this, but I opened Visual Studio Code and \"sudo ldconfig\" did not fix the #include errors\r\n\r\n5. cd cd /usr/local/lib/\r\n\r\n6. sudo ldconfig\r\nNo error messages for this, but I opened Visual Studio Code and \"sudo ldconfig\" did not fix the #include errors\r\n\r\n7. Since \"sudo ldconfig\" did not work I tried the other way the installation instructions (https://www.tensorflow.org/install/lang_c) recommended:\r\n```\r\ntlroot@tlroot-VirtualBox:~$ export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib\r\ntlroot@tlroot-VirtualBox:~$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib\r\ntlroot@tlroot-VirtualBox:~$ echo $LIBRARY_PATH\r\n:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib\r\ntlroot@tlroot-VirtualBox:~$ echo $LD_LIBRARY_PATH\r\n:/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/lib\r\ntlroot@tlroot-VirtualBox:~$\r\n```\r\nNo error messages for this, but I opened Visual Studio Code and those two export did not fix the #include errors\r\n\r\nI also noticed that the LIBRARY_PATH and LD_LIBRARY_PATH are forgotten when I reboot my Ubuntu 20.04 LTS Oracle VM Virtualbox.\r\n```\r\ntlroot@tlroot-VirtualBox:~$ echo $LIBRARY_PATH\r\ntlroot@tlroot-VirtualBox:~$ echo $LD_LIBRARY_PATH\r\ntlroot@tlroot-VirtualBox:~$\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is the example program I am using to test my TensorFlow C++ installation in Visual Studio Code:\r\n```\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n#include \"/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h\"\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n```\r\nNote: #include <stdio.h> WORKS but neither TensorFlow #include works.  I have run C++ projects in this setup and those programs worked.  See an example: https://github.com/ProfHariSeldon/CppND-System-Monitor.  So I know the C++ works what isn't working is installing TensorFlow.\r\n\r\nThese both exist so why can't the compiler find them?:\r\n/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/tf_attrtype.h\r\n/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h\r\n\r\nSee below:\r\n\r\n/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/c_api.h exists:\r\n```\r\n/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\n#ifndef TENSORFLOW_C_C_API_H_\r\n#define TENSORFLOW_C_C_API_H_\r\n\r\n#include <stddef.h>\r\n#include <stdint.h>\r\n\r\n#include \"tensorflow/c/tf_attrtype.h\"\r\n#include \"tensorflow/c/tf_datatype.h\"\r\n#include \"tensorflow/c/tf_status.h\"\r\n#include \"tensorflow/c/tf_tensor.h\"\r\n\r\n// --------------------------------------------------------------------------\r\n// C API for TensorFlow.\r\n//\r\n// The API leans towards simplicity and uniformity instead of convenience\r\n// since most usage will be by language specific wrappers.\r\n//\r\n// Conventions:\r\n// * We use the prefix TF_ for everything in the API.\r\n// * Objects are always passed around as pointers to opaque structs\r\n//   and these structs are allocated/deallocated via the API.\r\n// * TF_Status holds error information.  It is an object type\r\n//   and therefore is passed around as a pointer to an opaque\r\n//   struct as mentioned above.\r\n// * Every call that has a TF_Status* argument clears it on success\r\n//   and fills it with error info on failure.\r\n// * unsigned char is used for booleans (instead of the 'bool' type).\r\n//   In C++ bool is a keyword while in C99 bool is a macro defined\r\n//   in stdbool.h. It is possible for the two to be inconsistent.\r\n//   For example, neither the C99 nor the C++11 standard force a byte\r\n//   size on the bool type, so the macro defined in stdbool.h could\r\n//   be inconsistent with the bool keyword in C++. Thus, the use\r\n//   of stdbool.h is avoided and unsigned char is used instead.\r\n// * size_t is used to represent byte sizes of objects that are\r\n//   materialized in the address space of the calling process.\r\n// * int is used as an index into arrays.\r\n// * Deletion functions are safe to call on nullptr.\r\n//\r\n// Questions left to address:\r\n// * Might at some point need a way for callers to provide their own Env.\r\n// * Maybe add TF_TensorShape that encapsulates dimension info.\r\n//\r\n// Design decisions made:\r\n// * Backing store for tensor memory has an associated deallocation\r\n//   function.  This deallocation function will point to client code\r\n//   for tensors populated by the client.  So the client can do things\r\n//   like shadowing a numpy array.\r\n// * We do not provide TF_OK since it is not strictly necessary and we\r\n//   are not optimizing for convenience.\r\n// * We make assumption that one session has one graph.  This should be\r\n//   fine since we have the ability to run sub-graphs.\r\n// * We could allow NULL for some arguments (e.g., NULL options arg).\r\n//   However since convenience is not a primary goal, we don't do this.\r\n// * Devices are not in this API.  Instead, they are created/used internally\r\n//   and the API just provides high level controls over the number of\r\n//   devices of each type.\r\n\r\n// Macro to control visibility of exported symbols in the shared library (.so,\r\n// .dylib, .dll).\r\n// This duplicates the TF_EXPORT macro definition in\r\n// tensorflow/core/platform/macros.h in order to keep this .h file independent\r\n// of any other includes.\r\n#ifdef SWIG\r\n#define TF_CAPI_EXPORT\r\n#else\r\n#if defined(_WIN32)\r\n#ifdef TF_COMPILE_LIBRARY\r\n#define TF_CAPI_EXPORT __declspec(dllexport)\r\n#else\r\n#define TF_CAPI_EXPORT __declspec(dllimport)\r\n#endif  // TF_COMPILE_LIBRARY\r\n#else\r\n#define TF_CAPI_EXPORT __attribute__((visibility(\"default\")))\r\n#endif  // _WIN32\r\n#endif  // SWIG\r\n\r\n#ifdef __cplusplus\r\nextern \"C\" {\r\n#endif\r\n\r\n// --------------------------------------------------------------------------\r\n// TF_Version returns a string describing version information of the\r\n// TensorFlow library. TensorFlow using semantic versioning.\r\nTF_CAPI_EXPORT extern const char* TF_Version(void);\r\n\r\n// --------------------------------------------------------------------------\r\n// TF_Buffer holds a pointer to a block of data and its associated length.\r\n// Typically, the data consists of a serialized protocol buffer, but other data\r\n// may also be held in a buffer.\r\n//\r\n// By default, TF_Buffer itself does not do any memory management of the\r\n// pointed-to block.  If need be, users of this struct should specify how to\r\n// deallocate the block by setting the `data_deallocator` function pointer.\r\ntypedef struct TF_Buffer {\r\n  const void* data;\r\n  size_t length;\r\n  void (*data_deallocator)(void* data, size_t length);\r\n} TF_Buffer;\r\n\r\n// Makes a copy of the input and sets an appropriate deallocator.  Useful for\r\n// passing in read-only, input protobufs.\r\nTF_CAPI_EXPORT extern TF_Buffer* TF_NewBufferFromString(const void* proto,\r\n                                                        size_t proto_len);\r\n\r\n// Useful for passing *out* a protobuf.\r\nTF_CAPI_EXPORT extern TF_Buffer* TF_NewBuffer(void);\r\n\r\nTF_CAPI_EXPORT extern void TF_DeleteBuffer(TF_Buffer*);\r\n\r\nTF_CAPI_EXPORT extern TF_Buffer TF_GetBuffer(TF_Buffer* buffer);\r\n\r\n// --------------------------------------------------------------------------\r\n// TF_SessionOptions holds options that can be passed during session creation.\r\ntypedef struct TF_SessionOptions TF_SessionOptions;\r\n\r\n// Return a new options object.\r\nTF_CAPI_EXPORT extern TF_SessionOptions* TF_NewSessionOptions(void);\r\n\r\n// Set the target in TF_SessionOptions.options.\r\n// target can be empty, a single entry, or a comma separated list of entries.\r\n// Each entry is in one of the following formats :\r\n// \"local\"\r\n// ip:port\r\n// host:port\r\nTF_CAPI_EXPORT extern void TF_SetTarget(TF_SessionOptions* options,\r\n                                        const char* target);\r\n\r\n// Set the config in TF_SessionOptions.options.\r\n// config should be a serialized tensorflow.ConfigProto proto.\r\n// If config was not parsed successfully as a ConfigProto, record the\r\n// error information in *status.\r\nTF_CAPI_EXPORT extern void TF_SetConfig(TF_SessionOptions* options,\r\n                                        const void* proto, size_t proto_len,\r\n                                        TF_Status* status);\r\n\r\n// Destroy an options object.\r\nTF_CAPI_EXPORT extern void TF_DeleteSessionOptions(TF_SessionOptions*);\r\n\r\n// TODO(jeff,sanjay):\r\n// - export functions to set Config fields\r\n\r\n// --------------------------------------------------------------------------\r\n// The new graph construction API, still under development.\r\n\r\n// Represents a computation graph.  Graphs may be shared between sessions.\r\n// Graphs are thread-safe when used as directed below.\r\ntypedef struct TF_Graph TF_Graph;\r\n\r\n// Return a new graph object.\r\nTF_CAPI_EXPORT extern TF_Graph* TF_NewGraph(void);\r\n\r\n// Destroy an options object.  Graph will be deleted once no more\r\n// TFSession's are referencing it.\r\nTF_CAPI_EXPORT extern void TF_DeleteGraph(TF_Graph*);\r\n\r\n// Operation being built. The underlying graph must outlive this.\r\ntypedef struct TF_OperationDescription TF_OperationDescription;\r\n\r\n// Operation that has been added to the graph. Valid until the graph is\r\n// deleted -- in particular adding a new operation to the graph does not\r\n// invalidate old TF_Operation* pointers.\r\ntypedef struct TF_Operation TF_Operation;\r\n\r\n// Represents a specific input of an operation.\r\ntypedef struct TF_Input {\r\n  TF_Operation* oper;\r\n  int index;  // The index of the input within oper.\r\n} TF_Input;\r\n\r\n// Represents a specific output of an operation.\r\ntypedef struct TF_Output {\r\n  TF_Operation* oper;\r\n  int index;  // The index of the output within oper.\r\n} TF_Output;\r\n\r\n// TF_Function is a grouping of operations with defined inputs and outputs.\r\n// Once created and added to graphs, functions can be invoked by creating an\r\n// operation whose operation type matches the function name.\r\ntypedef struct TF_Function TF_Function;\r\n\r\n// Function definition options. TODO(iga): Define and implement\r\ntypedef struct TF_FunctionOptions TF_FunctionOptions;\r\n\r\n// Sets the shape of the Tensor referenced by `output` in `graph` to\r\n// the shape described by `dims` and `num_dims`.\r\n//\r\n// If the number of dimensions is unknown, `num_dims` must be set to\r\n// -1 and `dims` can be null. If a dimension is unknown, the\r\n// corresponding entry in the `dims` array must be -1.\r\n//\r\n// This does not overwrite the existing shape associated with `output`,\r\n// but merges the input shape with the existing shape.  For example,\r\n// setting a shape of [-1, 2] with an existing shape [2, -1] would set\r\n// a final shape of [2, 2] based on shape merging semantics.\r\n//\r\n// Returns an error into `status` if:\r\n//   * `output` is not in `graph`.\r\n//   * An invalid shape is being set (e.g., the shape being set\r\n//     is incompatible with the existing shape).\r\nTF_CAPI_EXPORT extern void TF_GraphSetTensorShape(TF_Graph* graph,\r\n                                                  TF_Output output,\r\n                                                  const int64_t* dims,\r\n                                                  const int num_dims,\r\n                                                  TF_Status* status);\r\n\r\n// Returns the number of dimensions of the Tensor referenced by `output`\r\n// in `graph`.\r\n//\r\n// If the number of dimensions in the shape is unknown, returns -1.\r\n//\r\n// Returns an error into `status` if:\r\n//   * `output` is not in `graph`.\r\nTF_CAPI_EXPORT extern int TF_GraphGetTensorNumDims(TF_Graph* graph,\r\n                                                   TF_Output output,\r\n                                                   TF_Status* status);\r\n\r\n// Returns the shape of the Tensor referenced by `output` in `graph`\r\n// into `dims`. `dims` must be an array large enough to hold `num_dims`\r\n// entries (e.g., the return value of TF_GraphGetTensorNumDims).\r\n//\r\n// If the number of dimensions in the shape is unknown or the shape is\r\n// a scalar, `dims` will remain untouched. Otherwise, each element of\r\n// `dims` will be set corresponding to the size of the dimension. An\r\n// unknown dimension is represented by `-1`.\r\n//\r\n// Returns an error into `status` if:\r\n//   * `output` is not in `graph`.\r\n//   * `num_dims` does not match the actual number of dimensions.\r\nTF_CAPI_EXPORT extern void TF_GraphGetTensorShape(TF_Graph* graph,\r\n                                                  TF_Output output,\r\n                                                  int64_t* dims, int num_dims,\r\n                                                  TF_Status* status);\r\n\r\n// Operation will only be added to *graph when TF_FinishOperation() is\r\n// called (assuming TF_FinishOperation() does not return an error).\r\n// *graph must not be deleted until after TF_FinishOperation() is\r\n// called.\r\nTF_CAPI_EXPORT extern TF_OperationDescription* TF_NewOperation(\r\n    TF_Graph* graph, const char* op_type, const char* oper_name);\r\n\r\n// Specify the device for `desc`.  Defaults to empty, meaning unconstrained.\r\nTF_CAPI_EXPORT extern void TF_SetDevice(TF_OperationDescription* desc,\r\n                                        const char* device);\r\n\r\n// The calls to TF_AddInput and TF_AddInputList must match (in number,\r\n// order, and type) the op declaration.  For example, the \"Concat\" op\r\n// has registration:\r\n//   REGISTER_OP(\"Concat\")\r\n//       .Input(\"concat_dim: int32\")\r\n//       .Input(\"values: N * T\")\r\n//       .Output(\"output: T\")\r\n//       .Attr(\"N: int >= 2\")\r\n//       .Attr(\"T: type\");\r\n// that defines two inputs, \"concat_dim\" and \"values\" (in that order).\r\n// You must use TF_AddInput() for the first input (since it takes a\r\n// single tensor), and TF_AddInputList() for the second input (since\r\n// it takes a list, even if you were to pass a list with a single\r\n// tensor), as in:\r\n//   TF_OperationDescription* desc = TF_NewOperation(graph, \"Concat\", \"c\");\r\n//   TF_Output concat_dim_input = {...};\r\n//   TF_AddInput(desc, concat_dim_input);\r\n//   TF_Output values_inputs[5] = {{...}, ..., {...}};\r\n//   TF_AddInputList(desc, values_inputs, 5);\r\n\r\n// For inputs that take a single tensor.\r\nTF_CAPI_EXPORT extern void TF_AddInput(TF_OperationDescription* desc,\r\n                                       TF_Output input);\r\n\r\n// For inputs that take a list of tensors.\r\n// inputs must point to TF_Output[num_inputs].\r\nTF_CAPI_EXPORT extern void TF_AddInputList(TF_OperationDescription* desc,\r\n                                           const TF_Output* inputs,\r\n                                           int num_inputs);\r\n\r\n// Call once per control input to `desc`.\r\nTF_CAPI_EXPORT extern void TF_AddControlInput(TF_OperationDescription* desc,\r\n                                              TF_Operation* input);\r\n\r\n// Request that `desc` be co-located on the device where `op`\r\n// is placed.\r\n//\r\n// Use of this is discouraged since the implementation of device placement is\r\n// subject to change. Primarily intended for internal libraries\r\nTF_CAPI_EXPORT extern void TF_ColocateWith(TF_OperationDescription* desc,\r\n                                           TF_Operation* op);\r\n\r\n// Call some TF_SetAttr*() function for every attr that is not\r\n// inferred from an input and doesn't have a default value you wish to\r\n// keep.\r\n\r\n// `value` must point to a string of length `length` bytes.\r\nTF_CAPI_EXPORT extern void TF_SetAttrString(TF_OperationDescription* desc,\r\n                                            const char* attr_name,\r\n                                            const void* value, size_t length);\r\n// `values` and `lengths` each must have lengths `num_values`.\r\n// `values[i]` must point to a string of length `lengths[i]` bytes.\r\nTF_CAPI_EXPORT extern void TF_SetAttrStringList(TF_OperationDescription* desc,\r\n                                                const char* attr_name,\r\n                                                const void* const* values,\r\n                                                const size_t* lengths,\r\n                                                int num_values);\r\nTF_CAPI_EXPORT extern void TF_SetAttrInt(TF_OperationDescription* desc,\r\n                                         const char* attr_name, int64_t value);\r\nTF_CAPI_EXPORT extern void TF_SetAttrIntList(TF_OperationDescription* desc,\r\n                                             const char* attr_name,\r\n                                             const int64_t* values,\r\n                                             int num_values);\r\nTF_CAPI_EXPORT extern void TF_SetAttrFloat(TF_OperationDescription* desc,\r\n                                           const char* attr_name, float value);\r\nTF_CAPI_EXPORT extern void TF_SetAttrFloatList(TF_OperationDescription* desc,\r\n                                               const char* attr_name,\r\n                                               const float* values,\r\n                                               int num_values);\r\nTF_CAPI_EXPORT extern void TF_SetAttrBool(TF_OperationDescription* desc,\r\n                                          const char* attr_name,\r\n                                          unsigned char value);\r\nTF_CAPI_EXPORT extern void TF_SetAttrBoolList(TF_OperationDescription* desc,\r\n                                              const char* attr_name,\r\n                                              const unsigned char* values,\r\n                                              int num_values);\r\nTF_CAPI_EXPORT extern void TF_SetAttrType(TF_OperationDescription* desc,\r\n                                          const char* attr_name,\r\n                                          TF_DataType value);\r\nTF_CAPI_EXPORT extern void TF_SetAttrTypeList(TF_OperationDescription* desc,\r\n                                              const char* attr_name,\r\n                                              const TF_DataType* values,\r\n                                              int num_values);\r\nTF_CAPI_EXPORT extern void TF_SetAttrPlaceholder(TF_OperationDescription* desc,\r\n                                                 const char* attr_name,\r\n                                                 const char* placeholder);\r\n\r\n// Set a 'func' attribute to the specified name.\r\n// `value` must point to a string of length `length` bytes.\r\nTF_CAPI_EXPORT extern void TF_SetAttrFuncName(TF_OperationDescription* desc,\r\n                                              const char* attr_name,\r\n                                              const char* value, size_t length);\r\n\r\n// Set `num_dims` to -1 to represent \"unknown rank\".  Otherwise,\r\n// `dims` points to an array of length `num_dims`.  `dims[i]` must be\r\n// >= -1, with -1 meaning \"unknown dimension\".\r\nTF_CAPI_EXPORT extern void TF_SetAttrShape(TF_OperationDescription* desc,\r\n                                           const char* attr_name,\r\n                                           const int64_t* dims, int num_dims);\r\n// `dims` and `num_dims` must point to arrays of length `num_shapes`.\r\n// Set `num_dims[i]` to -1 to represent \"unknown rank\".  Otherwise,\r\n// `dims[i]` points to an array of length `num_dims[i]`.  `dims[i][j]`\r\n// must be >= -1, with -1 meaning \"unknown dimension\".\r\nTF_CAPI_EXPORT extern void TF_SetAttrShapeList(TF_OperationDescription* desc,\r\n                                               const char* attr_name,\r\n                                               const int64_t* const* dims,\r\n                                               const int* num_dims,\r\n                                               int num_shapes);\r\n// `proto` must point to an array of `proto_len` bytes representing a\r\n// binary-serialized TensorShapeProto.\r\nTF_CAPI_EXPORT extern void TF_SetAttrTensorShapeProto(\r\n    TF_OperationDescription* desc, const char* attr_name, const void* proto,\r\n    size_t proto_len, TF_Status* status);\r\n// `protos` and `proto_lens` must point to arrays of length `num_shapes`.\r\n// `protos[i]` must point to an array of `proto_lens[i]` bytes\r\n// representing a binary-serialized TensorShapeProto.\r\nTF_CAPI_EXPORT extern void TF_SetAttrTensorShapeProtoList(\r\n    TF_OperationDescription* desc, const char* attr_name,\r\n    const void* const* protos, const size_t* proto_lens, int num_shapes,\r\n    TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_SetAttrTensor(TF_OperationDescription* desc,\r\n                                            const char* attr_name,\r\n                                            TF_Tensor* value,\r\n                                            TF_Status* status);\r\nTF_CAPI_EXPORT extern void TF_SetAttrTensorList(TF_OperationDescription* desc,\r\n                                                const char* attr_name,\r\n                                                TF_Tensor* const* values,\r\n                                                int num_values,\r\n                                                TF_Status* status);\r\n\r\n// `proto` should point to a sequence of bytes of length `proto_len`\r\n// representing a binary serialization of an AttrValue protocol\r\n// buffer.\r\nTF_CAPI_EXPORT extern void TF_SetAttrValueProto(TF_OperationDescription* desc,\r\n                                                const char* attr_name,\r\n                                                const void* proto,\r\n                                                size_t proto_len,\r\n                                                TF_Status* status);\r\n\r\n// If this function succeeds:\r\n//   * *status is set to an OK value,\r\n//   * a TF_Operation is added to the graph,\r\n//   * a non-null value pointing to the added operation is returned --\r\n//     this value is valid until the underlying graph is deleted.\r\n// Otherwise:\r\n//   * *status is set to a non-OK value,\r\n//   * the graph is not modified,\r\n//   * a null value is returned.\r\n// In either case, it deletes `desc`.\r\nTF_CAPI_EXPORT extern TF_Operation* TF_FinishOperation(\r\n    TF_OperationDescription* desc, TF_Status* status);\r\n\r\n// TF_Operation functions.  Operations are immutable once created, so\r\n// these are all query functions.\r\n\r\nTF_CAPI_EXPORT extern const char* TF_OperationName(TF_Operation* oper);\r\nTF_CAPI_EXPORT extern const char* TF_OperationOpType(TF_Operation* oper);\r\nTF_CAPI_EXPORT extern const char* TF_OperationDevice(TF_Operation* oper);\r\n\r\nTF_CAPI_EXPORT extern int TF_OperationNumOutputs(TF_Operation* oper);\r\nTF_CAPI_EXPORT extern TF_DataType TF_OperationOutputType(TF_Output oper_out);\r\nTF_CAPI_EXPORT extern int TF_OperationOutputListLength(TF_Operation* oper,\r\n                                                       const char* arg_name,\r\n                                                       TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern int TF_OperationNumInputs(TF_Operation* oper);\r\nTF_CAPI_EXPORT extern TF_DataType TF_OperationInputType(TF_Input oper_in);\r\nTF_CAPI_EXPORT extern int TF_OperationInputListLength(TF_Operation* oper,\r\n                                                      const char* arg_name,\r\n                                                      TF_Status* status);\r\n\r\n// In this code:\r\n//   TF_Output producer = TF_OperationInput(consumer);\r\n// There is an edge from producer.oper's output (given by\r\n// producer.index) to consumer.oper's input (given by consumer.index).\r\nTF_CAPI_EXPORT extern TF_Output TF_OperationInput(TF_Input oper_in);\r\n\r\n// Get the number of current consumers of a specific output of an\r\n// operation.  Note that this number can change when new operations\r\n// are added to the graph.\r\nTF_CAPI_EXPORT extern int TF_OperationOutputNumConsumers(TF_Output oper_out);\r\n\r\n// Get list of all current consumers of a specific output of an\r\n// operation.  `consumers` must point to an array of length at least\r\n// `max_consumers` (ideally set to\r\n// TF_OperationOutputNumConsumers(oper_out)).  Beware that a concurrent\r\n// modification of the graph can increase the number of consumers of\r\n// an operation.  Returns the number of output consumers (should match\r\n// TF_OperationOutputNumConsumers(oper_out)).\r\nTF_CAPI_EXPORT extern int TF_OperationOutputConsumers(TF_Output oper_out,\r\n                                                      TF_Input* consumers,\r\n                                                      int max_consumers);\r\n\r\n// Get the number of control inputs to an operation.\r\nTF_CAPI_EXPORT extern int TF_OperationNumControlInputs(TF_Operation* oper);\r\n\r\n// Get list of all control inputs to an operation.  `control_inputs` must\r\n// point to an array of length `max_control_inputs` (ideally set to\r\n// TF_OperationNumControlInputs(oper)).  Returns the number of control\r\n// inputs (should match TF_OperationNumControlInputs(oper)).\r\nTF_CAPI_EXPORT extern int TF_OperationGetControlInputs(\r\n    TF_Operation* oper, TF_Operation** control_inputs, int max_control_inputs);\r\n\r\n// Get the number of operations that have `*oper` as a control input.\r\n// Note that this number can change when new operations are added to\r\n// the graph.\r\nTF_CAPI_EXPORT extern int TF_OperationNumControlOutputs(TF_Operation* oper);\r\n\r\n// Get the list of operations that have `*oper` as a control input.\r\n// `control_outputs` must point to an array of length at least\r\n// `max_control_outputs` (ideally set to\r\n// TF_OperationNumControlOutputs(oper)). Beware that a concurrent\r\n// modification of the graph can increase the number of control\r\n// outputs.  Returns the number of control outputs (should match\r\n// TF_OperationNumControlOutputs(oper)).\r\nTF_CAPI_EXPORT extern int TF_OperationGetControlOutputs(\r\n    TF_Operation* oper, TF_Operation** control_outputs,\r\n    int max_control_outputs);\r\n\r\n// TF_AttrMetadata describes the value of an attribute on an operation.\r\ntypedef struct TF_AttrMetadata {\r\n  // A boolean: 1 if the attribute value is a list, 0 otherwise.\r\n  unsigned char is_list;\r\n\r\n  // Length of the list if is_list is true. Undefined otherwise.\r\n  int64_t list_size;\r\n\r\n  // Type of elements of the list if is_list != 0.\r\n  // Type of the single value stored in the attribute if is_list == 0.\r\n  TF_AttrType type;\r\n\r\n  // Total size the attribute value.\r\n  // The units of total_size depend on is_list and type.\r\n  // (1) If type == TF_ATTR_STRING and is_list == 0\r\n  //     then total_size is the byte size of the string\r\n  //     valued attribute.\r\n  // (2) If type == TF_ATTR_STRING and is_list == 1\r\n  //     then total_size is the cumulative byte size\r\n  //     of all the strings in the list.\r\n  // (3) If type == TF_ATTR_SHAPE and is_list == 0\r\n  //     then total_size is the number of dimensions\r\n  //     of the shape valued attribute, or -1\r\n  //     if its rank is unknown.\r\n  // (4) If type == TF_ATTR_SHAPE and is_list == 1\r\n  //     then total_size is the cumulative number\r\n  //     of dimensions of all shapes in the list.\r\n  // (5) Otherwise, total_size is undefined.\r\n  int64_t total_size;\r\n} TF_AttrMetadata;\r\n\r\n// Returns metadata about the value of the attribute `attr_name` of `oper`.\r\nTF_CAPI_EXPORT extern TF_AttrMetadata TF_OperationGetAttrMetadata(\r\n    TF_Operation* oper, const char* attr_name, TF_Status* status);\r\n\r\n// Fills in `value` with the value of the attribute `attr_name`.  `value` must\r\n// point to an array of length at least `max_length` (ideally set to\r\n// TF_AttrMetadata.total_size from TF_OperationGetAttrMetadata(oper,\r\n// attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrString(TF_Operation* oper,\r\n                                                     const char* attr_name,\r\n                                                     void* value,\r\n                                                     size_t max_length,\r\n                                                     TF_Status* status);\r\n\r\n// Get the list of strings in the value of the attribute `attr_name`.  Fills in\r\n// `values` and `lengths`, each of which must point to an array of length at\r\n// least `max_values`.\r\n//\r\n// The elements of values will point to addresses in `storage` which must be at\r\n// least `storage_size` bytes in length.  Ideally, max_values would be set to\r\n// TF_AttrMetadata.list_size and `storage` would be at least\r\n// TF_AttrMetadata.total_size, obtained from TF_OperationGetAttrMetadata(oper,\r\n// attr_name).\r\n//\r\n// Fails if storage_size is too small to hold the requested number of strings.\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrStringList(\r\n    TF_Operation* oper, const char* attr_name, void** values, size_t* lengths,\r\n    int max_values, void* storage, size_t storage_size, TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrInt(TF_Operation* oper,\r\n                                                  const char* attr_name,\r\n                                                  int64_t* value,\r\n                                                  TF_Status* status);\r\n\r\n// Fills in `values` with the value of the attribute `attr_name` of `oper`.\r\n// `values` must point to an array of length at least `max_values` (ideally set\r\n// TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,\r\n// attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrIntList(TF_Operation* oper,\r\n                                                      const char* attr_name,\r\n                                                      int64_t* values,\r\n                                                      int max_values,\r\n                                                      TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrFloat(TF_Operation* oper,\r\n                                                    const char* attr_name,\r\n                                                    float* value,\r\n                                                    TF_Status* status);\r\n\r\n// Fills in `values` with the value of the attribute `attr_name` of `oper`.\r\n// `values` must point to an array of length at least `max_values` (ideally set\r\n// to TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,\r\n// attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrFloatList(TF_Operation* oper,\r\n                                                        const char* attr_name,\r\n                                                        float* values,\r\n                                                        int max_values,\r\n                                                        TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrBool(TF_Operation* oper,\r\n                                                   const char* attr_name,\r\n                                                   unsigned char* value,\r\n                                                   TF_Status* status);\r\n\r\n// Fills in `values` with the value of the attribute `attr_name` of `oper`.\r\n// `values` must point to an array of length at least `max_values` (ideally set\r\n// to TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,\r\n// attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrBoolList(TF_Operation* oper,\r\n                                                       const char* attr_name,\r\n                                                       unsigned char* values,\r\n                                                       int max_values,\r\n                                                       TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrType(TF_Operation* oper,\r\n                                                   const char* attr_name,\r\n                                                   TF_DataType* value,\r\n                                                   TF_Status* status);\r\n\r\n// Fills in `values` with the value of the attribute `attr_name` of `oper`.\r\n// `values` must point to an array of length at least `max_values` (ideally set\r\n// to TF_AttrMetadata.list_size from TF_OperationGetAttrMetadata(oper,\r\n// attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrTypeList(TF_Operation* oper,\r\n                                                       const char* attr_name,\r\n                                                       TF_DataType* values,\r\n                                                       int max_values,\r\n                                                       TF_Status* status);\r\n\r\n// Fills in `value` with the value of the attribute `attr_name` of `oper`.\r\n// `values` must point to an array of length at least `num_dims` (ideally set to\r\n// TF_Attr_Meta.size from TF_OperationGetAttrMetadata(oper, attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrShape(TF_Operation* oper,\r\n                                                    const char* attr_name,\r\n                                                    int64_t* value,\r\n                                                    int num_dims,\r\n                                                    TF_Status* status);\r\n\r\n// Fills in `dims` with the list of shapes in the attribute `attr_name` of\r\n// `oper` and `num_dims` with the corresponding number of dimensions. On return,\r\n// for every i where `num_dims[i]` > 0, `dims[i]` will be an array of\r\n// `num_dims[i]` elements. A value of -1 for `num_dims[i]` indicates that the\r\n// i-th shape in the list is unknown.\r\n//\r\n// The elements of `dims` will point to addresses in `storage` which must be\r\n// large enough to hold at least `storage_size` int64_ts.  Ideally, `num_shapes`\r\n// would be set to TF_AttrMetadata.list_size and `storage_size` would be set to\r\n// TF_AttrMetadata.total_size from TF_OperationGetAttrMetadata(oper,\r\n// attr_name).\r\n//\r\n// Fails if storage_size is insufficient to hold the requested shapes.\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrShapeList(\r\n    TF_Operation* oper, const char* attr_name, int64_t** dims, int* num_dims,\r\n    int num_shapes, int64_t* storage, int storage_size, TF_Status* status);\r\n\r\n// Sets `value` to the binary-serialized TensorShapeProto of the value of\r\n// `attr_name` attribute of `oper`'.\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrTensorShapeProto(\r\n    TF_Operation* oper, const char* attr_name, TF_Buffer* value,\r\n    TF_Status* status);\r\n\r\n// Fills in `values` with binary-serialized TensorShapeProto values of the\r\n// attribute `attr_name` of `oper`. `values` must point to an array of length at\r\n// least `num_values` (ideally set to TF_AttrMetadata.list_size from\r\n// TF_OperationGetAttrMetadata(oper, attr_name)).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrTensorShapeProtoList(\r\n    TF_Operation* oper, const char* attr_name, TF_Buffer** values,\r\n    int max_values, TF_Status* status);\r\n\r\n// Gets the TF_Tensor valued attribute of `attr_name` of `oper`.\r\n//\r\n// Allocates a new TF_Tensor which the caller is expected to take\r\n// ownership of (and can deallocate using TF_DeleteTensor).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrTensor(TF_Operation* oper,\r\n                                                     const char* attr_name,\r\n                                                     TF_Tensor** value,\r\n                                                     TF_Status* status);\r\n\r\n// Fills in `values` with the TF_Tensor values of the attribute `attr_name` of\r\n// `oper`. `values` must point to an array of TF_Tensor* of length at least\r\n// `max_values` (ideally set to TF_AttrMetadata.list_size from\r\n// TF_OperationGetAttrMetadata(oper, attr_name)).\r\n//\r\n// The caller takes ownership of all the non-null TF_Tensor* entries in `values`\r\n// (which can be deleted using TF_DeleteTensor(values[i])).\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrTensorList(TF_Operation* oper,\r\n                                                         const char* attr_name,\r\n                                                         TF_Tensor** values,\r\n                                                         int max_values,\r\n                                                         TF_Status* status);\r\n\r\n// Sets `output_attr_value` to the binary-serialized AttrValue proto\r\n// representation of the value of the `attr_name` attr of `oper`.\r\nTF_CAPI_EXPORT extern void TF_OperationGetAttrValueProto(\r\n    TF_Operation* oper, const char* attr_name, TF_Buffer* output_attr_value,\r\n    TF_Status* status);\r\n\r\n// Returns the operation in the graph with `oper_name`. Returns nullptr if\r\n// no operation found.\r\nTF_CAPI_EXPORT extern TF_Operation* TF_GraphOperationByName(\r\n    TF_Graph* graph, const char* oper_name);\r\n\r\n// Iterate through the operations of a graph.  To use:\r\n// size_t pos = 0;\r\n// TF_Operation* oper;\r\n// while ((oper = TF_GraphNextOperation(graph, &pos)) != nullptr) {\r\n//   DoSomethingWithOperation(oper);\r\n// }\r\nTF_CAPI_EXPORT extern TF_Operation* TF_GraphNextOperation(TF_Graph* graph,\r\n                                                          size_t* pos);\r\n\r\n// Write out a serialized representation of `graph` (as a GraphDef protocol\r\n// message) to `output_graph_def` (allocated by TF_NewBuffer()).\r\n// `output_graph_def`'s underlying buffer will be freed when TF_DeleteBuffer()\r\n// is called.\r\n//\r\n// May fail on very large graphs in the future.\r\nTF_CAPI_EXPORT extern void TF_GraphToGraphDef(TF_Graph* graph,\r\n                                              TF_Buffer* output_graph_def,\r\n                                              TF_Status* status);\r\n\r\n// Returns the serialized OpDef proto with name `op_name`, or a bad status if no\r\n// such op exists. This can return OpDefs of functions copied into the graph.\r\nTF_CAPI_EXPORT extern void TF_GraphGetOpDef(TF_Graph* graph,\r\n                                            const char* op_name,\r\n                                            TF_Buffer* output_op_def,\r\n                                            TF_Status* status);\r\n\r\n// Returns the serialized VersionDef proto for this graph.\r\nTF_CAPI_EXPORT extern void TF_GraphVersions(TF_Graph* graph,\r\n                                            TF_Buffer* output_version_def,\r\n                                            TF_Status* status);\r\n\r\n// TF_ImportGraphDefOptions holds options that can be passed to\r\n// TF_GraphImportGraphDef.\r\ntypedef struct TF_ImportGraphDefOptions TF_ImportGraphDefOptions;\r\n\r\nTF_CAPI_EXPORT extern TF_ImportGraphDefOptions* TF_NewImportGraphDefOptions(\r\n    void);\r\nTF_CAPI_EXPORT extern void TF_DeleteImportGraphDefOptions(\r\n    TF_ImportGraphDefOptions* opts);\r\n\r\n// Set the prefix to be prepended to the names of nodes in `graph_def` that will\r\n// be imported into `graph`. `prefix` is copied and has no lifetime\r\n// requirements.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetPrefix(\r\n    TF_ImportGraphDefOptions* opts, const char* prefix);\r\n\r\n// Set the execution device for nodes in `graph_def`.\r\n// Only applies to nodes where a device was not already explicitly specified.\r\n// `device` is copied and has no lifetime requirements.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetDefaultDevice(\r\n    TF_ImportGraphDefOptions* opts, const char* device);\r\n\r\n// Set whether to uniquify imported operation names. If true, imported operation\r\n// names will be modified if their name already exists in the graph. If false,\r\n// conflicting names will be treated as an error. Note that this option has no\r\n// effect if a prefix is set, since the prefix will guarantee all names are\r\n// unique. Defaults to false.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetUniquifyNames(\r\n    TF_ImportGraphDefOptions* opts, unsigned char uniquify_names);\r\n\r\n// If true, the specified prefix will be modified if it already exists as an\r\n// operation name or prefix in the graph. If false, a conflicting prefix will be\r\n// treated as an error. This option has no effect if no prefix is specified.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsSetUniquifyPrefix(\r\n    TF_ImportGraphDefOptions* opts, unsigned char uniquify_prefix);\r\n\r\n// Set any imported nodes with input `src_name:src_index` to have that input\r\n// replaced with `dst`. `src_name` refers to a node in the graph to be imported,\r\n// `dst` references a node already existing in the graph being imported into.\r\n// `src_name` is copied and has no lifetime requirements.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddInputMapping(\r\n    TF_ImportGraphDefOptions* opts, const char* src_name, int src_index,\r\n    TF_Output dst);\r\n\r\n// Set any imported nodes with control input `src_name` to have that input\r\n// replaced with `dst`. `src_name` refers to a node in the graph to be imported,\r\n// `dst` references an operation already existing in the graph being imported\r\n// into. `src_name` is copied and has no lifetime requirements.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsRemapControlDependency(\r\n    TF_ImportGraphDefOptions* opts, const char* src_name, TF_Operation* dst);\r\n\r\n// Cause the imported graph to have a control dependency on `oper`. `oper`\r\n// should exist in the graph being imported into.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddControlDependency(\r\n    TF_ImportGraphDefOptions* opts, TF_Operation* oper);\r\n\r\n// Add an output in `graph_def` to be returned via the `return_outputs` output\r\n// parameter of TF_GraphImportGraphDef(). If the output is remapped via an input\r\n// mapping, the corresponding existing tensor in `graph` will be returned.\r\n// `oper_name` is copied and has no lifetime requirements.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddReturnOutput(\r\n    TF_ImportGraphDefOptions* opts, const char* oper_name, int index);\r\n\r\n// Returns the number of return outputs added via\r\n// TF_ImportGraphDefOptionsAddReturnOutput().\r\nTF_CAPI_EXPORT extern int TF_ImportGraphDefOptionsNumReturnOutputs(\r\n    const TF_ImportGraphDefOptions* opts);\r\n\r\n// Add an operation in `graph_def` to be returned via the `return_opers` output\r\n// parameter of TF_GraphImportGraphDef(). `oper_name` is copied and has no\r\n// lifetime requirements.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefOptionsAddReturnOperation(\r\n    TF_ImportGraphDefOptions* opts, const char* oper_name);\r\n\r\n// Returns the number of return operations added via\r\n// TF_ImportGraphDefOptionsAddReturnOperation().\r\nTF_CAPI_EXPORT extern int TF_ImportGraphDefOptionsNumReturnOperations(\r\n    const TF_ImportGraphDefOptions* opts);\r\n\r\n// TF_ImportGraphDefResults holds results that are generated by\r\n// TF_GraphImportGraphDefWithResults().\r\ntypedef struct TF_ImportGraphDefResults TF_ImportGraphDefResults;\r\n\r\n// Fetches the return outputs requested via\r\n// TF_ImportGraphDefOptionsAddReturnOutput(). The number of fetched outputs is\r\n// returned in `num_outputs`. The array of return outputs is returned in\r\n// `outputs`. `*outputs` is owned by and has the lifetime of `results`.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefResultsReturnOutputs(\r\n    TF_ImportGraphDefResults* results, int* num_outputs, TF_Output** outputs);\r\n\r\n// Fetches the return operations requested via\r\n// TF_ImportGraphDefOptionsAddReturnOperation(). The number of fetched\r\n// operations is returned in `num_opers`. The array of return operations is\r\n// returned in `opers`. `*opers` is owned by and has the lifetime of `results`.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefResultsReturnOperations(\r\n    TF_ImportGraphDefResults* results, int* num_opers, TF_Operation*** opers);\r\n\r\n// Fetches any input mappings requested via\r\n// TF_ImportGraphDefOptionsAddInputMapping() that didn't appear in the GraphDef\r\n// and weren't used as input to any node in the imported graph def. The number\r\n// of fetched mappings is returned in `num_missing_unused_input_mappings`. The\r\n// array of each mapping's source node name is returned in `src_names`, and the\r\n// array of each mapping's source index is returned in `src_indexes`.\r\n//\r\n// `*src_names`, `*src_indexes`, and the memory backing each string in\r\n// `src_names` are owned by and have the lifetime of `results`.\r\nTF_CAPI_EXPORT extern void TF_ImportGraphDefResultsMissingUnusedInputMappings(\r\n    TF_ImportGraphDefResults* results, int* num_missing_unused_input_mappings,\r\n    const char*** src_names, int** src_indexes);\r\n\r\n// Deletes a results object returned by TF_GraphImportGraphDefWithResults().\r\nTF_CAPI_EXPORT extern void TF_DeleteImportGraphDefResults(\r\n    TF_ImportGraphDefResults* results);\r\n\r\n// Import the graph serialized in `graph_def` into `graph`.  Returns nullptr and\r\n// a bad status on error. Otherwise, returns a populated\r\n// TF_ImportGraphDefResults instance. The returned instance must be deleted via\r\n// TF_DeleteImportGraphDefResults().\r\nTF_CAPI_EXPORT extern TF_ImportGraphDefResults*\r\nTF_GraphImportGraphDefWithResults(TF_Graph* graph, const TF_Buffer* graph_def,\r\n                                  const TF_ImportGraphDefOptions* options,\r\n                                  TF_Status* status);\r\n\r\n// Import the graph serialized in `graph_def` into `graph`.\r\n// Convenience function for when only return outputs are needed.\r\n//\r\n// `num_return_outputs` must be the number of return outputs added (i.e. the\r\n// result of TF_ImportGraphDefOptionsNumReturnOutputs()).  If\r\n// `num_return_outputs` is non-zero, `return_outputs` must be of length\r\n// `num_return_outputs`. Otherwise it can be null.\r\nTF_CAPI_EXPORT extern void TF_GraphImportGraphDefWithReturnOutputs(\r\n    TF_Graph* graph, const TF_Buffer* graph_def,\r\n    const TF_ImportGraphDefOptions* options, TF_Output* return_outputs,\r\n    int num_return_outputs, TF_Status* status);\r\n\r\n// Import the graph serialized in `graph_def` into `graph`.\r\n// Convenience function for when no results are needed.\r\nTF_CAPI_EXPORT extern void TF_GraphImportGraphDef(\r\n    TF_Graph* graph, const TF_Buffer* graph_def,\r\n    const TF_ImportGraphDefOptions* options, TF_Status* status);\r\n\r\n// Adds a copy of function `func` and optionally its gradient function `grad`\r\n// to `g`. Once `func`/`grad` is added to `g`, it can be called by creating\r\n// an operation using the function's name.\r\n// Any changes to `func`/`grad` (including deleting it) done after this method\r\n// returns, won't affect the copy of `func`/`grad` in `g`.\r\n// If `func` or `grad` are already in `g`, TF_GraphCopyFunction has no\r\n// effect on them, but can establish the function->gradient relationship\r\n// between them if `func` does not already have a gradient. If `func` already\r\n// has a gradient different from `grad`, an error is returned.\r\n//\r\n// `func` must not be null.\r\n// If `grad` is null and `func` is not in `g`, `func` is added without a\r\n// gradient.\r\n// If `grad` is null and `func` is in `g`, TF_GraphCopyFunction is a noop.\r\n// `grad` must have appropriate signature as described in the doc of\r\n// GradientDef in tensorflow/core/framework/function.proto.\r\n//\r\n// If successful, status is set to OK and `func` and `grad` are added to `g`.\r\n// Otherwise, status is set to the encountered error and `g` is unmodified.\r\nTF_CAPI_EXPORT extern void TF_GraphCopyFunction(TF_Graph* g,\r\n                                                const TF_Function* func,\r\n                                                const TF_Function* grad,\r\n                                                TF_Status* status);\r\n\r\n// Returns the number of TF_Functions registered in `g`.\r\nTF_CAPI_EXPORT extern int TF_GraphNumFunctions(TF_Graph* g);\r\n\r\n// Fills in `funcs` with the TF_Function* registered in `g`.\r\n// `funcs` must point to an array of TF_Function* of length at least\r\n// `max_func`. In usual usage, max_func should be set to the result of\r\n// TF_GraphNumFunctions(g). In this case, all the functions registered in\r\n// `g` will be returned. Else, an unspecified subset.\r\n//\r\n// If successful, returns the number of TF_Function* successfully set in\r\n// `funcs` and sets status to OK. The caller takes ownership of\r\n// all the returned TF_Functions. They must be deleted with TF_DeleteFunction.\r\n// On error, returns 0, sets status to the encountered error, and the contents\r\n// of funcs will be undefined.\r\nTF_CAPI_EXPORT extern int TF_GraphGetFunctions(TF_Graph* g, TF_Function** funcs,\r\n                                               int max_func, TF_Status* status);\r\n\r\n// Note: The following function may fail on very large protos in the future.\r\n\r\nTF_CAPI_EXPORT extern void TF_OperationToNodeDef(TF_Operation* oper,\r\n                                                 TF_Buffer* output_node_def,\r\n                                                 TF_Status* status);\r\n\r\ntypedef struct TF_WhileParams {\r\n  // The number of inputs to the while loop, i.e. the number of loop variables.\r\n  // This is the size of cond_inputs, body_inputs, and body_outputs.\r\n  const int ninputs;\r\n\r\n  // The while condition graph. The inputs are the current values of the loop\r\n  // variables. The output should be a scalar boolean.\r\n  TF_Graph* const cond_graph;\r\n  const TF_Output* const cond_inputs;\r\n  TF_Output cond_output;\r\n\r\n  // The loop body graph. The inputs are the current values of the loop\r\n  // variables. The outputs are the updated values of the loop variables.\r\n  TF_Graph* const body_graph;\r\n  const TF_Output* const body_inputs;\r\n  TF_Output* const body_outputs;\r\n\r\n  // Unique null-terminated name for this while loop. This is used as a prefix\r\n  // for created operations.\r\n  const char* name;\r\n} TF_WhileParams;\r\n\r\n// Creates a TF_WhileParams for creating a while loop in `g`. `inputs` are\r\n// outputs that already exist in `g` used as initial values for the loop\r\n// variables.\r\n//\r\n// The returned TF_WhileParams will have all fields initialized except\r\n// `cond_output`, `body_outputs`, and `name`. The `body_outputs` buffer will be\r\n// allocated to size `ninputs`. The caller should build `cond_graph` and\r\n// `body_graph` starting from the inputs, and store the final outputs in\r\n// `cond_output` and `body_outputs`.\r\n//\r\n// If `status` is OK, the caller must call either TF_FinishWhile or\r\n// TF_AbortWhile on the returned TF_WhileParams. If `status` isn't OK, the\r\n// returned TF_WhileParams is not valid, and the caller should not call\r\n// TF_FinishWhile() or TF_AbortWhile().\r\n//\r\n// Missing functionality (TODO):\r\n// - Gradients\r\n// - Reference-type inputs\r\n// - Directly referencing external tensors from the cond/body graphs (this is\r\n//   possible in the Python API)\r\nTF_CAPI_EXPORT extern TF_WhileParams TF_NewWhile(TF_Graph* g, TF_Output* inputs,\r\n                                                 int ninputs,\r\n                                                 TF_Status* status);\r\n\r\n// Builds the while loop specified by `params` and returns the output tensors of\r\n// the while loop in `outputs`. `outputs` should be allocated to size\r\n// `params.ninputs`.\r\n//\r\n// `params` is no longer valid once this returns.\r\n//\r\n// Either this or TF_AbortWhile() must be called after a successful\r\n// TF_NewWhile() call.\r\nTF_CAPI_EXPORT extern void TF_FinishWhile(const TF_WhileParams* params,\r\n                                          TF_Status* status,\r\n                                          TF_Output* outputs);\r\n\r\n// Frees `params`s resources without building a while loop. `params` is no\r\n// longer valid after this returns. Either this or TF_FinishWhile() must be\r\n// called after a successful TF_NewWhile() call.\r\nTF_CAPI_EXPORT extern void TF_AbortWhile(const TF_WhileParams* params);\r\n\r\n// Adds operations to compute the partial derivatives of sum of `y`s w.r.t `x`s,\r\n// i.e., d(y_1 + y_2 + ...)/dx_1, d(y_1 + y_2 + ...)/dx_2...\r\n//\r\n// `dx` are used as initial gradients (which represent the symbolic partial\r\n// derivatives of some loss function `L` w.r.t. `y`).\r\n// `dx` must be nullptr or have size `ny`.\r\n// If `dx` is nullptr, the implementation will use dx of `OnesLike` for all\r\n// shapes in `y`.\r\n// The partial derivatives are returned in `dy`. `dy` should be allocated to\r\n// size `nx`.\r\n//\r\n// Gradient nodes are automatically named under the \"gradients/\" prefix. To\r\n// guarantee name uniqueness, subsequent calls to the same graph will\r\n// append an incremental tag to the prefix: \"gradients_1/\", \"gradients_2/\", ...\r\n// See TF_AddGradientsWithPrefix, which provides a means to specify a custom\r\n// name prefix for operations added to a graph to compute the gradients.\r\n//\r\n// WARNING: This function does not yet support all the gradients that python\r\n// supports. See\r\n// https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md\r\n// for instructions on how to add C++ more gradients.\r\nTF_CAPI_EXPORT void TF_AddGradients(TF_Graph* g, TF_Output* y, int ny,\r\n                                    TF_Output* x, int nx, TF_Output* dx,\r\n                                    TF_Status* status, TF_Output* dy);\r\n\r\n// Adds operations to compute the partial derivatives of sum of `y`s w.r.t `x`s,\r\n// i.e., d(y_1 + y_2 + ...)/dx_1, d(y_1 + y_2 + ...)/dx_2...\r\n// This is a variant of TF_AddGradients that allows to caller to pass a custom\r\n// name prefix to the operations added to a graph to compute the gradients.\r\n//\r\n// `dx` are used as initial gradients (which represent the symbolic partial\r\n// derivatives of some loss function `L` w.r.t. `y`).\r\n// `dx` must be nullptr or have size `ny`.\r\n// If `dx` is nullptr, the implementation will use dx of `OnesLike` for all\r\n// shapes in `y`.\r\n// The partial derivatives are returned in `dy`. `dy` should be allocated to\r\n// size `nx`.\r\n// `prefix` names the scope into which all gradients operations are being added.\r\n// `prefix` must be unique within the provided graph otherwise this operation\r\n// will fail. If `prefix` is nullptr, the default prefixing behaviour takes\r\n// place, see TF_AddGradients for more details.\r\n//\r\n// WARNING: This function does not yet support all the gradients that python\r\n// supports. See\r\n// https://www.tensorflow.org/code/tensorflow/cc/gradients/README.md\r\n// for instructions on how to add C++ more gradients.\r\nTF_CAPI_EXPORT void TF_AddGradientsWithPrefix(TF_Graph* g, const char* prefix,\r\n                                              TF_Output* y, int ny,\r\n                                              TF_Output* x, int nx,\r\n                                              TF_Output* dx, TF_Status* status,\r\n                                              TF_Output* dy);\r\n\r\n// Create a TF_Function from a TF_Graph\r\n//\r\n// Params:\r\n//  fn_body - the graph whose operations (or subset of whose operations) will be\r\n//            converted to TF_Function.\r\n//  fn_name - the name of the new TF_Function. Should match the operation\r\n//            name (OpDef.name) regexp [A-Z][A-Za-z0-9_.\\\\-/]*.\r\n//            If `append_hash_to_fn_name` is false, `fn_name` must be distinct\r\n//            from other function and operation names (at least those\r\n//            registered in graphs where this function will be used).\r\n//  append_hash_to_fn_name - Must be 0 or 1. If set to 1, the actual name\r\n//                           of the function will be `fn_name` appended with\r\n//                           '_<hash_of_this_function's_definition>'.\r\n//                           If set to 0, the function's name will be `fn_name`.\r\n//  num_opers - `num_opers` contains the number of elements in the `opers` array\r\n//              or a special value of -1 meaning that no array is given.\r\n//              The distinction between an empty array of operations and no\r\n//              array of operations is necessary to distinguish the case of\r\n//              creating a function with no body (e.g. identity or permutation)\r\n//              and the case of creating a function whose body contains all\r\n//              the nodes in the graph (except for the automatic skipping, see\r\n//              below).\r\n//  opers - Array of operations to become the body of the function or null.\r\n//          - If no array is given (`num_opers`  = -1), all the\r\n//          operations in `fn_body` will become part of the function\r\n//          except operations referenced in `inputs`. These operations\r\n//          must have a single output (these operations are typically\r\n//          placeholders created for the sole purpose of representing\r\n//          an input. We can relax this constraint if there are\r\n//          compelling use cases).\r\n//          - If an array is given (`num_opers` >= 0), all operations\r\n//          in it will become part of the function. In particular, no\r\n//          automatic skipping of dummy input operations is performed.\r\n//  ninputs - number of elements in `inputs` array\r\n//  inputs - array of TF_Outputs that specify the inputs to the function.\r\n//           If `ninputs` is zero (the function takes no inputs), `inputs`\r\n//           can be null. The names used for function inputs are normalized\r\n//           names of the operations (usually placeholders) pointed to by\r\n//           `inputs`. These operation names should start with a letter.\r\n//           Normalization will convert all letters to lowercase and\r\n//           non-alphanumeric characters to '_' to make resulting names match\r\n//           the \"[a-z][a-z0-9_]*\" pattern for operation argument names.\r\n//           `inputs` cannot contain the same tensor twice.\r\n//  noutputs - number of elements in `outputs` array\r\n//  outputs - array of TF_Outputs that specify the outputs of the function.\r\n//            If `noutputs` is zero (the function returns no outputs), `outputs`\r\n//            can be null. `outputs` can contain the same tensor more than once.\r\n//  output_names - The names of the function's outputs. `output_names` array\r\n//                 must either have the same length as `outputs`\r\n//                 (i.e. `noutputs`) or be null. In the former case,\r\n//                 the names should match the regular expression for ArgDef\r\n//                 names - \"[a-z][a-z0-9_]*\". In the latter case,\r\n//                 names for outputs will be generated automatically.\r\n//  opts - various options for the function, e.g. XLA's inlining control.\r\n//  description - optional human-readable description of this function.\r\n//  status - Set to OK on success and an appropriate error on failure.\r\n//\r\n// Note that when the same TF_Output is listed as both an input and an output,\r\n// the corresponding function's output will equal to this input,\r\n// instead of the original node's output.\r\n//\r\n// Callers must also satisfy the following constraints:\r\n// - `inputs` cannot refer to TF_Outputs within a control flow context. For\r\n//   example, one cannot use the output of \"switch\" node as input.\r\n// - `inputs` and `outputs` cannot have reference types. Reference types are\r\n//   not exposed through C API and are being replaced with Resources. We support\r\n//   reference types inside function's body to support legacy code. Do not\r\n//   use them in new code.\r\n// - Every node in the function's body must have all of its inputs (including\r\n//   control inputs). In other words, for every node in the body, each input\r\n//   must be either listed in `inputs` or must come from another node in\r\n//   the body. In particular, it is an error to have a control edge going from\r\n//   a node outside of the body into a node in the body. This applies to control\r\n//   edges going from nodes referenced in `inputs` to nodes in the body when\r\n//   the former nodes are not in the body (automatically skipped or not\r\n//   included in explicitly specified body).\r\n//\r\n// Returns:\r\n//  On success, a newly created TF_Function instance. It must be deleted by\r\n//  calling TF_DeleteFunction.\r\n//\r\n//  On failure, null.\r\nTF_CAPI_EXPORT extern TF_Function* TF_GraphToFunction(\r\n    const TF_Graph* fn_body, const char* fn_name,\r\n    unsigned char append_hash_to_fn_name, int num_opers,\r\n    const TF_Operation* const* opers, int ninputs, const TF_Output* inputs,\r\n    int noutputs, const TF_Output* outputs, const char* const* output_names,\r\n    const TF_FunctionOptions* opts, const char* description, TF_Status* status);\r\n\r\n// Similar to TF_GraphToFunction but allows specifying control outputs of the\r\n// function.\r\n//\r\n//  The arguments of TF_GraphToFunction have the same meaning, but the new\r\n//  arguments are as follows:\r\n//\r\n//    ncontrol_outputs: Number of control outputs of the function.\r\n//    control_outputs: vector of TF_Operation objects to be marked as control\r\n//      outputs of the function. Operations marked as control outputs are\r\n//      guaranteed to execute.\r\n//    control_output_names: Optional. If not nullptr, vector of strings, one\r\n//      per control output, with their names to be added to the function's\r\n//      OpDef.\r\nTF_CAPI_EXPORT extern TF_Function* TF_GraphToFunctionWithControlOutputs(\r\n    const TF_Graph* fn_body, const char* fn_name,\r\n    unsigned char append_hash_to_fn_name, int num_opers,\r\n    const TF_Operation* const* opers, int ninputs, const TF_Output* inputs,\r\n    int noutputs, const TF_Output* outputs, const char* const* output_names,\r\n    int ncontrol_outputs, const TF_Operation* const* control_outputs,\r\n    const char* const* control_output_names, const TF_FunctionOptions* opts,\r\n    const char* description, TF_Status* status);\r\n\r\n// Returns the name of the graph function.\r\n// The return value points to memory that is only usable until the next\r\n// mutation to *func.\r\nTF_CAPI_EXPORT extern const char* TF_FunctionName(TF_Function* func);\r\n\r\n// Write out a serialized representation of `func` (as a FunctionDef protocol\r\n// message) to `output_func_def` (allocated by TF_NewBuffer()).\r\n// `output_func_def`'s underlying buffer will be freed when TF_DeleteBuffer()\r\n// is called.\r\n//\r\n// May fail on very large graphs in the future.\r\nTF_CAPI_EXPORT extern void TF_FunctionToFunctionDef(TF_Function* func,\r\n                                                    TF_Buffer* output_func_def,\r\n                                                    TF_Status* status);\r\n\r\n// Construct and return the function whose FunctionDef representation is\r\n// serialized in `proto`. `proto_len` must equal the number of bytes\r\n// pointed to by `proto`.\r\n// Returns:\r\n//  On success, a newly created TF_Function instance. It must be deleted by\r\n//  calling TF_DeleteFunction.\r\n//\r\n//  On failure, null.\r\nTF_CAPI_EXPORT extern TF_Function* TF_FunctionImportFunctionDef(\r\n    const void* proto, size_t proto_len, TF_Status* status);\r\n\r\n// Sets function attribute named `attr_name` to value stored in `proto`.\r\n// If this attribute is already set to another value, it is overridden.\r\n// `proto` should point to a sequence of bytes of length `proto_len`\r\n// representing a binary serialization of an AttrValue protocol\r\n// buffer.\r\nTF_CAPI_EXPORT extern void TF_FunctionSetAttrValueProto(TF_Function* func,\r\n                                                        const char* attr_name,\r\n                                                        const void* proto,\r\n                                                        size_t proto_len,\r\n                                                        TF_Status* status);\r\n\r\n// Sets `output_attr_value` to the binary-serialized AttrValue proto\r\n// representation of the value of the `attr_name` attr of `func`.\r\n// If `attr_name` attribute is not present, status is set to an error.\r\nTF_CAPI_EXPORT extern void TF_FunctionGetAttrValueProto(\r\n    TF_Function* func, const char* attr_name, TF_Buffer* output_attr_value,\r\n    TF_Status* status);\r\n\r\n// Frees the memory used by the `func` struct.\r\n// TF_DeleteFunction is a noop if `func` is null.\r\n// Deleting a function does not remove it from any graphs it was copied to.\r\nTF_CAPI_EXPORT extern void TF_DeleteFunction(TF_Function* func);\r\n\r\n// Attempts to evaluate `output`. This will only be possible if `output` doesn't\r\n// depend on any graph inputs (this function is safe to call if this isn't the\r\n// case though).\r\n//\r\n// If the evaluation is successful, this function returns true and `output`s\r\n// value is returned in `result`. Otherwise returns false. An error status is\r\n// returned if something is wrong with the graph or input. Note that this may\r\n// return false even if no error status is set.\r\nTF_CAPI_EXPORT extern unsigned char TF_TryEvaluateConstant(TF_Graph* graph,\r\n                                                           TF_Output output,\r\n                                                           TF_Tensor** result,\r\n                                                           TF_Status* status);\r\n\r\n// TODO(josh11b): Register OpDef, available to all operations added\r\n// to this graph.\r\n\r\n// --------------------------------------------------------------------------\r\n// API for driving Graph execution.\r\n\r\ntypedef struct TF_Session TF_Session;\r\n\r\n// Return a new execution session with the associated graph, or NULL on\r\n// error. Does not take ownership of any input parameters.\r\n//\r\n// *`graph` must be a valid graph (not deleted or nullptr). `graph` will be be\r\n// kept alive for the lifetime of the returned TF_Session. New nodes can still\r\n// be added to `graph` after this call.\r\nTF_CAPI_EXPORT extern TF_Session* TF_NewSession(TF_Graph* graph,\r\n                                                const TF_SessionOptions* opts,\r\n                                                TF_Status* status);\r\n\r\n// This function creates a new TF_Session (which is created on success) using\r\n// `session_options`, and then initializes state (restoring tensors and other\r\n// assets) using `run_options`.\r\n//\r\n// Any NULL and non-NULL value combinations for (`run_options, `meta_graph_def`)\r\n// are valid.\r\n//\r\n// - `export_dir` must be set to the path of the exported SavedModel.\r\n// - `tags` must include the set of tags used to identify one MetaGraphDef in\r\n//    the SavedModel.\r\n// - `graph` must be a graph newly allocated with TF_NewGraph().\r\n//\r\n// If successful, populates `graph` with the contents of the Graph and\r\n// `meta_graph_def` with the MetaGraphDef of the loaded model.\r\nTF_CAPI_EXPORT extern TF_Session* TF_LoadSessionFromSavedModel(\r\n    const TF_SessionOptions* session_options, const TF_Buffer* run_options,\r\n    const char* export_dir, const char* const* tags, int tags_len,\r\n    TF_Graph* graph, TF_Buffer* meta_graph_def, TF_Status* status);\r\n\r\n// Close a session.\r\n//\r\n// Contacts any other processes associated with the session, if applicable.\r\n// May not be called after TF_DeleteSession().\r\nTF_CAPI_EXPORT extern void TF_CloseSession(TF_Session*, TF_Status* status);\r\n\r\n// Destroy a session object.\r\n//\r\n// Even if error information is recorded in *status, this call discards all\r\n// local resources associated with the session.  The session may not be used\r\n// during or after this call (and the session drops its reference to the\r\n// corresponding graph).\r\nTF_CAPI_EXPORT extern void TF_DeleteSession(TF_Session*, TF_Status* status);\r\n\r\n// Run the graph associated with the session starting with the supplied inputs\r\n// (inputs[0,ninputs-1] with corresponding values in input_values[0,ninputs-1]).\r\n//\r\n// Any NULL and non-NULL value combinations for (`run_options`,\r\n// `run_metadata`) are valid.\r\n//\r\n//    - `run_options` may be NULL, in which case it will be ignored; or\r\n//      non-NULL, in which case it must point to a `TF_Buffer` containing the\r\n//      serialized representation of a `RunOptions` protocol buffer.\r\n//    - `run_metadata` may be NULL, in which case it will be ignored; or\r\n//      non-NULL, in which case it must point to an empty, freshly allocated\r\n//      `TF_Buffer` that may be updated to contain the serialized representation\r\n//      of a `RunMetadata` protocol buffer.\r\n//\r\n// The caller retains ownership of `input_values` (which can be deleted using\r\n// TF_DeleteTensor). The caller also retains ownership of `run_options` and/or\r\n// `run_metadata` (when not NULL) and should manually call TF_DeleteBuffer on\r\n// them.\r\n//\r\n// On success, the tensors corresponding to outputs[0,noutputs-1] are placed in\r\n// output_values[]. Ownership of the elements of output_values[] is transferred\r\n// to the caller, which must eventually call TF_DeleteTensor on them.\r\n//\r\n// On failure, output_values[] contains NULLs.\r\nTF_CAPI_EXPORT extern void TF_SessionRun(\r\n    TF_Session* session,\r\n    // RunOptions\r\n    const TF_Buffer* run_options,\r\n    // Input tensors\r\n    const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs,\r\n    // Output tensors\r\n    const TF_Output* outputs, TF_Tensor** output_values, int noutputs,\r\n    // Target operations\r\n    const TF_Operation* const* target_opers, int ntargets,\r\n    // RunMetadata\r\n    TF_Buffer* run_metadata,\r\n    // Output status\r\n    TF_Status*);\r\n\r\n// Set up the graph with the intended feeds (inputs) and fetches (outputs) for a\r\n// sequence of partial run calls.\r\n//\r\n// On success, returns a handle that is used for subsequent PRun calls. The\r\n// handle should be deleted with TF_DeletePRunHandle when it is no longer\r\n// needed.\r\n//\r\n// On failure, out_status contains a tensorflow::Status with an error\r\n// message. *handle is set to nullptr.\r\nTF_CAPI_EXPORT extern void TF_SessionPRunSetup(\r\n    TF_Session*,\r\n    // Input names\r\n    const TF_Output* inputs, int ninputs,\r\n    // Output names\r\n    const TF_Output* outputs, int noutputs,\r\n    // Target operations\r\n    const TF_Operation* const* target_opers, int ntargets,\r\n    // Output handle\r\n    const char** handle,\r\n    // Output status\r\n    TF_Status*);\r\n\r\n// Continue to run the graph with additional feeds and fetches. The\r\n// execution state is uniquely identified by the handle.\r\nTF_CAPI_EXPORT extern void TF_SessionPRun(\r\n    TF_Session*, const char* handle,\r\n    // Input tensors\r\n    const TF_Output* inputs, TF_Tensor* const* input_values, int ninputs,\r\n    // Output tensors\r\n    const TF_Output* outputs, TF_Tensor** output_values, int noutputs,\r\n    // Target operations\r\n    const TF_Operation* const* target_opers, int ntargets,\r\n    // Output status\r\n    TF_Status*);\r\n\r\n// Deletes a handle allocated by TF_SessionPRunSetup.\r\n// Once called, no more calls to TF_SessionPRun should be made.\r\nTF_CAPI_EXPORT extern void TF_DeletePRunHandle(const char* handle);\r\n\r\n// --------------------------------------------------------------------------\r\n// The deprecated session API.  Please switch to the above instead of\r\n// TF_ExtendGraph(). This deprecated API can be removed at any time without\r\n// notice.\r\n\r\ntypedef struct TF_DeprecatedSession TF_DeprecatedSession;\r\n\r\nTF_CAPI_EXPORT extern TF_DeprecatedSession* TF_NewDeprecatedSession(\r\n    const TF_SessionOptions*, TF_Status* status);\r\nTF_CAPI_EXPORT extern void TF_CloseDeprecatedSession(TF_DeprecatedSession*,\r\n                                                     TF_Status* status);\r\nTF_CAPI_EXPORT extern void TF_DeleteDeprecatedSession(TF_DeprecatedSession*,\r\n                                                      TF_Status* status);\r\nTF_CAPI_EXPORT extern void TF_Reset(const TF_SessionOptions* opt,\r\n                                    const char** containers, int ncontainers,\r\n                                    TF_Status* status);\r\n// Treat the bytes proto[0,proto_len-1] as a serialized GraphDef and\r\n// add the nodes in that GraphDef to the graph for the session.\r\n//\r\n// Prefer use of TF_Session and TF_GraphImportGraphDef over this.\r\nTF_CAPI_EXPORT extern void TF_ExtendGraph(TF_DeprecatedSession*,\r\n                                          const void* proto, size_t proto_len,\r\n                                          TF_Status*);\r\n\r\n// See TF_SessionRun() above.\r\nTF_CAPI_EXPORT extern void TF_Run(TF_DeprecatedSession*,\r\n                                  const TF_Buffer* run_options,\r\n                                  const char** input_names, TF_Tensor** inputs,\r\n                                  int ninputs, const char** output_names,\r\n                                  TF_Tensor** outputs, int noutputs,\r\n                                  const char** target_oper_names, int ntargets,\r\n                                  TF_Buffer* run_metadata, TF_Status*);\r\n\r\n// See TF_SessionPRunSetup() above.\r\nTF_CAPI_EXPORT extern void TF_PRunSetup(TF_DeprecatedSession*,\r\n                                        const char** input_names, int ninputs,\r\n                                        const char** output_names, int noutputs,\r\n                                        const char** target_oper_names,\r\n                                        int ntargets, const char** handle,\r\n                                        TF_Status*);\r\n\r\n// See TF_SessionPRun above.\r\nTF_CAPI_EXPORT extern void TF_PRun(TF_DeprecatedSession*, const char* handle,\r\n                                   const char** input_names, TF_Tensor** inputs,\r\n                                   int ninputs, const char** output_names,\r\n                                   TF_Tensor** outputs, int noutputs,\r\n                                   const char** target_oper_names, int ntargets,\r\n                                   TF_Status*);\r\n\r\ntypedef struct TF_DeviceList TF_DeviceList;\r\n\r\n// Lists all devices in a TF_Session.\r\n//\r\n// Caller takes ownership of the returned TF_DeviceList* which must eventually\r\n// be freed with a call to TF_DeleteDeviceList.\r\nTF_CAPI_EXPORT extern TF_DeviceList* TF_SessionListDevices(TF_Session* session,\r\n                                                           TF_Status* status);\r\n\r\n// Lists all devices in a TF_Session.\r\n//\r\n// Caller takes ownership of the returned TF_DeviceList* which must eventually\r\n// be freed with a call to TF_DeleteDeviceList.\r\nTF_CAPI_EXPORT extern TF_DeviceList* TF_DeprecatedSessionListDevices(\r\n    TF_DeprecatedSession* session, TF_Status* status);\r\n\r\n// Deallocates the device list.\r\nTF_CAPI_EXPORT extern void TF_DeleteDeviceList(TF_DeviceList* list);\r\n\r\n// Counts the number of elements in the device list.\r\nTF_CAPI_EXPORT extern int TF_DeviceListCount(const TF_DeviceList* list);\r\n\r\n// Retrieves the full name of the device (e.g. /job:worker/replica:0/...)\r\n// The return value will be a pointer to a null terminated string. The caller\r\n// must not modify or delete the string. It will be deallocated upon a call to\r\n// TF_DeleteDeviceList.\r\n//\r\n// If index is out of bounds, an error code will be set in the status object,\r\n// and a null pointer will be returned.\r\nTF_CAPI_EXPORT extern const char* TF_DeviceListName(const TF_DeviceList* list,\r\n                                                    int index,\r\n                                                    TF_Status* status);\r\n\r\n// Retrieves the type of the device at the given index.\r\n//\r\n// The caller must not modify or delete the string. It will be deallocated upon\r\n// a call to TF_DeleteDeviceList.\r\n//\r\n// If index is out of bounds, an error code will be set in the status object,\r\n// and a null pointer will be returned.\r\nTF_CAPI_EXPORT extern const char* TF_DeviceListType(const TF_DeviceList* list,\r\n                                                    int index,\r\n                                                    TF_Status* status);\r\n\r\n// Retrieve the amount of memory associated with a given device.\r\n//\r\n// If index is out of bounds, an error code will be set in the status object,\r\n// and -1 will be returned.\r\nTF_CAPI_EXPORT extern int64_t TF_DeviceListMemoryBytes(\r\n    const TF_DeviceList* list, int index, TF_Status* status);\r\n\r\n// Retrieve the incarnation number of a given device.\r\n//\r\n// If index is out of bounds, an error code will be set in the status object,\r\n// and 0 will be returned.\r\nTF_CAPI_EXPORT extern uint64_t TF_DeviceListIncarnation(\r\n    const TF_DeviceList* list, int index, TF_Status* status);\r\n\r\n// --------------------------------------------------------------------------\r\n// Load plugins containing custom ops and kernels\r\n\r\n// TF_Library holds information about dynamically loaded TensorFlow plugins.\r\ntypedef struct TF_Library TF_Library;\r\n\r\n// Load the library specified by library_filename and register the ops and\r\n// kernels present in that library.\r\n//\r\n// Pass \"library_filename\" to a platform-specific mechanism for dynamically\r\n// loading a library. The rules for determining the exact location of the\r\n// library are platform-specific and are not documented here.\r\n//\r\n// On success, place OK in status and return the newly created library handle.\r\n// The caller owns the library handle.\r\n//\r\n// On failure, place an error status in status and return NULL.\r\nTF_CAPI_EXPORT extern TF_Library* TF_LoadLibrary(const char* library_filename,\r\n                                                 TF_Status* status);\r\n\r\n// Get the OpList of OpDefs defined in the library pointed by lib_handle.\r\n//\r\n// Returns a TF_Buffer. The memory pointed to by the result is owned by\r\n// lib_handle. The data in the buffer will be the serialized OpList proto for\r\n// ops defined in the library.\r\nTF_CAPI_EXPORT extern TF_Buffer TF_GetOpList(TF_Library* lib_handle);\r\n\r\n// Frees the memory associated with the library handle.\r\n// Does NOT unload the library.\r\nTF_CAPI_EXPORT extern void TF_DeleteLibraryHandle(TF_Library* lib_handle);\r\n\r\n// Get the OpList of all OpDefs defined in this address space.\r\n// Returns a TF_Buffer, ownership of which is transferred to the caller\r\n// (and can be freed using TF_DeleteBuffer).\r\n//\r\n// The data in the buffer will be the serialized OpList proto for ops registered\r\n// in this address space.\r\nTF_CAPI_EXPORT extern TF_Buffer* TF_GetAllOpList(void);\r\n\r\n// TF_ApiDefMap encapsulates a collection of API definitions for an operation.\r\n//\r\n// This object maps the name of a TensorFlow operation to a description of the\r\n// API to generate for it, as defined by the ApiDef protocol buffer (\r\n// https://www.tensorflow.org/code/tensorflow/core/framework/api_def.proto)\r\n//\r\n// The ApiDef messages are typically used to generate convenience wrapper\r\n// functions for TensorFlow operations in various language bindings.\r\ntypedef struct TF_ApiDefMap TF_ApiDefMap;\r\n\r\n// Creates a new TF_ApiDefMap instance.\r\n//\r\n// Params:\r\n//  op_list_buffer - TF_Buffer instance containing serialized OpList\r\n//    protocol buffer. (See\r\n//    https://www.tensorflow.org/code/tensorflow/core/framework/op_def.proto\r\n//    for the OpList proto definition).\r\n//  status - Set to OK on success and an appropriate error on failure.\r\nTF_CAPI_EXPORT extern TF_ApiDefMap* TF_NewApiDefMap(TF_Buffer* op_list_buffer,\r\n                                                    TF_Status* status);\r\n\r\n// Deallocates a TF_ApiDefMap.\r\nTF_CAPI_EXPORT extern void TF_DeleteApiDefMap(TF_ApiDefMap* apimap);\r\n\r\n// Add ApiDefs to the map.\r\n//\r\n// `text` corresponds to a text representation of an ApiDefs protocol message.\r\n// (https://www.tensorflow.org/code/tensorflow/core/framework/api_def.proto).\r\n//\r\n// The provided ApiDefs will be merged with existing ones in the map, with\r\n// precedence given to the newly added version in case of conflicts with\r\n// previous calls to TF_ApiDefMapPut.\r\nTF_CAPI_EXPORT extern void TF_ApiDefMapPut(TF_ApiDefMap* api_def_map,\r\n                                           const char* text, size_t text_len,\r\n                                           TF_Status* status);\r\n\r\n// Returns a serialized ApiDef protocol buffer for the TensorFlow operation\r\n// named `name`.\r\nTF_CAPI_EXPORT extern TF_Buffer* TF_ApiDefMapGet(TF_ApiDefMap* api_def_map,\r\n                                                 const char* name,\r\n                                                 size_t name_len,\r\n                                                 TF_Status* status);\r\n\r\n// --------------------------------------------------------------------------\r\n// Kernel definition information.\r\n\r\n// Returns a serialized KernelList protocol buffer containing KernelDefs for all\r\n// registered kernels.\r\nTF_CAPI_EXPORT extern TF_Buffer* TF_GetAllRegisteredKernels(TF_Status* status);\r\n\r\n// Returns a serialized KernelList protocol buffer containing KernelDefs for all\r\n// kernels registered for the operation named `name`.\r\nTF_CAPI_EXPORT extern TF_Buffer* TF_GetRegisteredKernelsForOp(\r\n    const char* name, TF_Status* status);\r\n\r\n// --------------------------------------------------------------------------\r\n// In-process TensorFlow server functionality, for use in distributed training.\r\n// A Server instance encapsulates a set of devices and a Session target that\r\n// can participate in distributed training. A server belongs to a cluster\r\n// (specified by a ClusterSpec), and corresponds to a particular task in a\r\n// named job. The server can communicate with any other server in the same\r\n// cluster.\r\n\r\n// In-process TensorFlow server.\r\ntypedef struct TF_Server TF_Server;\r\n\r\n// Creates a new in-process TensorFlow server configured using a serialized\r\n// ServerDef protocol buffer provided via `proto` and `proto_len`.\r\n//\r\n// The server will not serve any requests until TF_ServerStart is invoked.\r\n// The server will stop serving requests once TF_ServerStop or\r\n// TF_DeleteServer is invoked.\r\nTF_CAPI_EXPORT extern TF_Server* TF_NewServer(const void* proto,\r\n                                              size_t proto_len,\r\n                                              TF_Status* status);\r\n\r\n// Starts an in-process TensorFlow server.\r\nTF_CAPI_EXPORT extern void TF_ServerStart(TF_Server* server, TF_Status* status);\r\n\r\n// Stops an in-process TensorFlow server.\r\nTF_CAPI_EXPORT extern void TF_ServerStop(TF_Server* server, TF_Status* status);\r\n\r\n// Blocks until the server has been successfully stopped (via TF_ServerStop or\r\n// TF_ServerClose).\r\nTF_CAPI_EXPORT extern void TF_ServerJoin(TF_Server* server, TF_Status* status);\r\n\r\n// Returns the target string that can be provided to TF_SetTarget() to connect\r\n// a TF_Session to `server`.\r\n//\r\n// The returned string is valid only until TF_DeleteServer is invoked.\r\nTF_CAPI_EXPORT extern const char* TF_ServerTarget(TF_Server* server);\r\n\r\n// Destroy an in-process TensorFlow server, frees memory. If server is running\r\n// it will be stopped and joined.\r\nTF_CAPI_EXPORT extern void TF_DeleteServer(TF_Server* server);\r\n\r\n// Register a listener method that processes printed messages.\r\n//\r\n// If any listeners are registered, the print operator will call all listeners\r\n// with the printed messages and immediately return without writing to the\r\n// logs.\r\nTF_CAPI_EXPORT extern void TF_RegisterLogListener(\r\n    void (*listener)(const char*));\r\n\r\n#ifdef __cplusplus\r\n} /* end extern \"C\" */\r\n#endif\r\n\r\n#endif  // TENSORFLOW_C_C_API_H_\r\n```\r\n\r\n/usr/local/lib/libtensorflow-cpu-linux-x86_64-1.15.0/include/tensorflow/c/tf_attrtype.h exists\r\n```\r\n/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n#ifndef TENSORFLOW_C_TF_ATTRTYPE_H_\r\n#define TENSORFLOW_C_TF_ATTRTYPE_H_\r\n\r\n#ifdef __cplusplus\r\nextern \"C\" {\r\n#endif\r\n\r\n// TF_AttrType describes the type of the value of an attribute on an operation.\r\ntypedef enum TF_AttrType {\r\n  TF_ATTR_STRING = 0,\r\n  TF_ATTR_INT = 1,\r\n  TF_ATTR_FLOAT = 2,\r\n  TF_ATTR_BOOL = 3,\r\n  TF_ATTR_TYPE = 4,\r\n  TF_ATTR_SHAPE = 5,\r\n  TF_ATTR_TENSOR = 6,\r\n  TF_ATTR_PLACEHOLDER = 7,\r\n  TF_ATTR_FUNC = 8,\r\n} TF_AttrType;\r\n\r\n#ifdef __cplusplus\r\n} /* end extern \"C\" */\r\n#endif\r\n\r\n#endif  // TENSORFLOW_C_TF_ATTRTYPE_H_\r\n```", "comments": ["You need to use the `-I` compile flags to include more directories which contain header files", "> \r\n> \r\n> You need to use the `-I` compile flags to include more directories which contain header files\r\n\r\nI tried again.  I'm pretty sure the below output means it's working.  If so we can close this bug report.\r\n```\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ gcc hello_tf.c -ltensorflow -o hello_tf\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.1: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.1: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.1: .dynsym local symbol at index 5 (>= sh_info of 3)\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ gcc -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow -o hello_tf\r\n/usr/bin/ld: /usr/local/lib/libtensorflow.so: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.1: .dynsym local symbol at index 3 (>= sh_info of 3)\r\n/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.1: .dynsym local symbol at index 4 (>= sh_info of 3)\r\n/usr/bin/ld: /usr/local/lib//libtensorflow_framework.so.1: .dynsym local symbol at index 5 (>= sh_info of 3)\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$ ./hello_tf\r\nHello from TensorFlow C library version 1.15.0\r\ntlroot@tlroot-VirtualBox:~/Documents/C++/Capstone/CppND-Capstone-Hello-World/src$\r\n```\r\n\r\nThe problem was I untared the file with Archive Manager instead of sudo tar:\r\n```\r\nsudo tar -C /usr/local -xzf https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.15.0.tar.gz\r\n```\r\n\r\nSorry.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41506\">No</a>\n"]}, {"number": 41505, "title": "Mixed Precision training is ~10 times slower", "body": "Mixed Precision training is too slow when I use `mixed_float16` policy\r\n**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\n- **Ubuntu 18.04:**\r\n- **TensorFlow installed from pip3:**\r\n- **TensorFlow version 2.2.0:**\r\n- **Python version 3.7:**\r\n- **CUDA version 10.1 cuDNN version 7.5:**\r\n- **GPU model and memory RTX 2080ti 11gb:**\r\n\r\nI've designed a segmentation model, which consists of different regular layers like BatchNorm, Conv2D, Activation, etc.. Haven't designed and used any custom layer.\r\nI'm using binary cross-entropy as a loss function and also put `float32` dtype on my network's outputs (like to official doc says), which is a sigmoid function. Data loading is with `float32` as well but **when I train with mixed precision, it gets super slow ~6-10 times**. I use the same network with the same batch size, without changing anything else. I've calculated the time between the processes and here is what it looks like\r\n\r\n- **Feedforward is slower ~10x**\r\n- **Loss Computing is slower ~6x**\r\n- **Gradients computing is slower ~6x**\r\n\r\nI assume it should be faster than the `float32` policy, but it turns out not. It's super slow. At least I'm expecting it should not be slower and the main advantage would be to take a bigger **batch size**.", "comments": ["I think I've found the problem. **It's because of the resize operation I've got**.\r\nIt turns out **tf.image.resize** or **keras.Upsample2D** (or any sampling approach, which uses image resize from keras backend), breaks the `mixed-precision` training regime.\r\n\r\nYou can use **Conv2D** or **Pooling2D** for **downsampling** and **Con2DTranspose** for upsampling to avoid this problem.\r\n\r\nWhat about this issue? In many situations, we have to use a resize operation. How to make it work as well?", "Have you tried using keras.layers.Reshape?", "No, I haven't tried. \r\nBut how the reshape is going to fill the missing values? e.g. interpolation aims to upsample (bigger resolution) and fill additional sampling values.", "If you are interpolating then reshape is useless and as you suggested to gain best performance Conv2DTranspose will be the best way to go when upsampling. A doubt though, why would you explicitly require tf.image.resize or keras.Upsample2D if you can achieve the same with better performance using Conv2DTranspose?\r\n", "Also are you sure that tf.image.resize supports mixed policy?", "I couldn't find a note which would tell that `tf.image.resize` does not support `mixed-precision`. Also, **Conv2DTransponse** is not always a good choice. I tried both, but Conv2DTranspose is really worst than the interpolation (in my task).", "Then van you use tf.image.resize and Conv2DTranspose in ciinjunction? i.e. Using Conv2DTranspose as part of the model and tf.image.resize outside of it.", "I don't think I can use it. it's inside my decoder and each layer has an upsampling op.", "Ah shoot, I'm stumped.", "Here is an issue which was opened a couple of years ago \"**Provide GPU implementations for resize image methods**\" #533\r\nClosed without any solution.", "@Hazarapet,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar thank you for your response.\r\nI'm sorry but I can't share the code. It's a model, containing regular layers like **batchnorm**, **conv2d**, **activation**, and **sampling** layers and **nothing else**. I don't have any complicated custom implementations in my model. The main problem is related to sampling operations. I'm guessing image resize does not have GPU support for `mixed_float16`. If I replace them with Conv2d and Conv2dTranspose, the training process gets really fast.", "@Hazarapet,\r\nWithout a sample code it is difficult for us to pinpoint the issue. Is it possible for you to provide a mock code which replicates the behavior? So that it'll will allow us to determine the source of the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Note this may have been fixed by https://github.com/tensorflow/tensorflow/commit/67d15573a776119d5a544ed266dc2514ae13c3b5."]}, {"number": 41504, "title": "Misleading error message in tf.broadcast_to", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen running `tf.broadcast_to`, the input shown in the error message for an invalid argument is different from the given input.  `[110, 53, 104, 147, 157, 123, 5, 24, 188, 40, 5, 2]` (given) vs `[2,2,2,2,2,2,2,2,2,2,2,2]` (error message).\r\n\r\n**Describe the expected behavior**\r\nCorrect error message\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.constant([1, 2, 3])\r\ntf.broadcast_to([x, [110, 53, 104, 147, 157, 123, 5, 24, 188, 40, 5, 2])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n`InvalidArgumentError: Shape [2,2,2,2,2,2,2,2,2,2,2,2] would have more than 2**63 - 1 elements [Op:BroadcastTo]`\r\n", "comments": ["So the error isn't that misleading, you're getting it because you're broadcasting it to too big a tensor. \r\n```\r\nInvalidArgumentError: Shape [2,2,2,2,2,2,2,2,2,2,2,2] would have more than 2**63 - 1 elements [Op:BroadcastTo]\r\n```\r\n appears when the number of elements has exceeded the given limit. You will have more than 2**63 - 1 elements when you broadcast it to the given shape [110, 53, 104, 147, 157, 123, 5, 24, 188, 40, 5, 2] or the upper limit of shapes [2,2,2,2,2,2,2,2,2,2,2,2].", "The issue is that the construction of error message in tensor_shape.cc does not copy all dims of shape, only the current dim of the shape (when error check happens). Created a PR #41514 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41504\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41504\">No</a>\n"]}, {"number": 41503, "title": "'ValueError: need more than 0 values to unpack' in Tensorflow object-detection model training", "body": "I've been dealing with an issue training my Tensorflow custom object-detection model, following [this tutorial](https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/)\r\n\r\nI keep getting `ValueError: need more than 0 values to unpack` I will attached the full error script.\r\n\r\nI am using my mac, running Catalina 10.15.5, with Python 2.7.17, and Tensorflow 1.15.0, which was pip installed.\r\n\r\nMy directory in terms of applicable files and folders:\r\n\r\n- Desktop\r\n    - models\r\n        - research \r\n            - object_detection\r\n                - legacy\r\n                    - train.py\r\n                - training\r\n                    - object-detection.pbtxt\r\n                    - ssdlite_mobilenet_v3_small_320x320_coco.config\r\n    - Object-Detection\r\n        - data/\r\n            - Test_labels.csv\r\n            - Train_labels.csv\r\n        - Camera pictures/\r\n            - Test/\r\n                - testingimages.jpg\r\n            - Train/\r\n                - trainingimages.jpg\r\n            - ...myimages.jpg\r\n        - training\r\n        - xml_to_csv.py\r\n        - generate_TFrecord.py'\r\n\r\nHere is my .config script:\r\n```\r\n# SSDLite with Mobilenet v3 large feature extractor.\r\n# Trained on COCO14, initialized from scratch.\r\n# TPU-compatible.\r\n# Users should configure the fine_tune_checkpoint field in the train config as\r\n# well as the label_map_path and input_path fields in the train_input_reader and\r\n# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n# should be configured.\r\n\r\nmodel {\r\n  ssd {\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n    num_classes: 1\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    encode_background_as_zeros: true\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.2\r\n        max_scale: 0.95\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.3333\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 320\r\n        width: 320\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.8\r\n        kernel_size: 3\r\n        use_depthwise: true\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n        class_prediction_bias_init: -4.6\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              stddev: 0.03\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            train: true,\r\n            scale: true,\r\n            center: true,\r\n            decay: 0.97,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_mobilenet_v3_small'\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      use_depthwise: true\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            stddev: 0.03\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          train: true,\r\n          scale: true,\r\n          center: true,\r\n          decay: 0.97,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          alpha: 0.75,\r\n          gamma: 2.0\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n          delta: 1.0\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    normalize_loc_loss_by_codesize: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n        use_static_shapes: true\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 24\r\n  sync_replicas: true\r\n  startup_delay_steps: 0\r\n  replicas_to_aggregate: 32\r\n  num_steps: 800000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.4\r\n          total_steps: 800000\r\n          warmup_learning_rate: 0.13333\r\n          warmup_steps: 2000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"data/train.record\"\r\n  }\r\n  label_map_path: \"training/object-detection.pbtxt\"\r\n}\r\n\r\neval_config: {\r\n  num_examples: 18\r\n  # Note: The below line limits the evaluation process to 10 evaluations.\r\n  # Remove the below line to evaluate indefinitely.\r\n  max_evals: 10\r\n}\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"data/test.record\"\r\n  }\r\n  label_map_path: \"training/object-detection.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n}\r\n```\r\nHere is my object-detection.pbtxt script:\r\n```\r\nitem {\r\n  id: 1\r\n  name: 'Raspi'\r\n}\r\n```\r\nHere is my train.py script\r\n```\r\nimport functools\r\nimport json\r\nimport os\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.python.util.deprecation import deprecated\r\n\r\n\r\nfrom object_detection.builders import dataset_builder\r\nfrom object_detection.builders import graph_rewriter_builder\r\nfrom object_detection.builders import model_builder\r\nfrom object_detection.legacy import trainer\r\nfrom object_detection.utils import config_util\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\r\nflags.DEFINE_integer('task', 0, 'task id')\r\nflags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\r\nflags.DEFINE_boolean('clone_on_cpu', False,\r\n                     'Force clones to be deployed on CPU.  Note that even if '\r\n                     'set to False (allowing ops to run on gpu), some ops may '\r\n                     'still be run on the CPU if they have no GPU kernel.')\r\nflags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\r\n                     'replicas.')\r\nflags.DEFINE_integer('ps_tasks', 0,\r\n                     'Number of parameter server tasks. If None, does not use '\r\n                     'a parameter server.')\r\nflags.DEFINE_string('train_dir', '',\r\n                    'Directory to save the checkpoints and training summaries.')\r\n\r\nflags.DEFINE_string('pipeline_config_path', '',\r\n                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\r\n                    'file. If provided, other configs are ignored')\r\n\r\nflags.DEFINE_string('train_config_path', '',\r\n                    'Path to a train_pb2.TrainConfig config file.')\r\nflags.DEFINE_string('input_config_path', '',\r\n                    'Path to an input_reader_pb2.InputReader config file.')\r\nflags.DEFINE_string('model_config_path', '',\r\n                    'Path to a model_pb2.DetectionModel config file.')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n\r\n@deprecated(None, 'Use object_detection/model_main.py.')\r\ndef main(_):\r\n  assert FLAGS.train_dir, '`train_dir` is missing.'\r\n  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\r\n  if FLAGS.pipeline_config_path:\r\n    configs = config_util.get_configs_from_pipeline_file(\r\n        FLAGS.pipeline_config_path)\r\n    if FLAGS.task == 0:\r\n      tf.gfile.Copy(FLAGS.pipeline_config_path,\r\n                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\r\n                    overwrite=True)\r\n  else:\r\n    configs = config_util.get_configs_from_multiple_files(\r\n        model_config_path=FLAGS.model_config_path,\r\n        train_config_path=FLAGS.train_config_path,\r\n        train_input_config_path=FLAGS.input_config_path)\r\n    if FLAGS.task == 0:\r\n      for name, config in [('model.config', FLAGS.model_config_path),\r\n                           ('train.config', FLAGS.train_config_path),\r\n                           ('input.config', FLAGS.input_config_path)]:\r\n        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\r\n                      overwrite=True)\r\n\r\n  model_config = configs['model']\r\n  train_config = configs['train_config']\r\n  input_config = configs['train_input_config']\r\n\r\n  model_fn = functools.partial(\r\n      model_builder.build,\r\n      model_config=model_config,\r\n      is_training=True)\r\n\r\n  def get_next(config):\r\n    return dataset_builder.make_initializable_iterator(\r\n        dataset_builder.build(config)).get_next()\r\n\r\n  create_input_dict_fn = functools.partial(get_next, input_config)\r\n\r\n  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\r\n  cluster_data = env.get('cluster', None)\r\n  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\r\n  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\r\n  task_info = type('TaskSpec', (object,), task_data)\r\n\r\n  # Parameters for a single worker.\r\n  ps_tasks = 0\r\n  worker_replicas = 1\r\n  worker_job_name = 'lonely_worker'\r\n  task = 0\r\n  is_chief = True\r\n  master = ''\r\n\r\n  if cluster_data and 'worker' in cluster_data:\r\n    # Number of total worker replicas include \"worker\"s and the \"master\".\r\n    worker_replicas = len(cluster_data['worker']) + 1\r\n  if cluster_data and 'ps' in cluster_data:\r\n    ps_tasks = len(cluster_data['ps'])\r\n\r\n  if worker_replicas > 1 and ps_tasks < 1:\r\n    raise ValueError('At least 1 ps task is needed for distributed training.')\r\n\r\n  if worker_replicas >= 1 and ps_tasks > 0:\r\n    # Set up distributed training.\r\n    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\r\n                             job_name=task_info.type,\r\n                             task_index=task_info.index)\r\n    if task_info.type == 'ps':\r\n      server.join()\r\n      return\r\n\r\n    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\r\n    task = task_info.index\r\n    is_chief = (task_info.type == 'master')\r\n    master = server.target\r\n\r\n  graph_rewriter_fn = None\r\n  if 'graph_rewriter_config' in configs:\r\n    graph_rewriter_fn = graph_rewriter_builder.build(\r\n        configs['graph_rewriter_config'], is_training=True)\r\n\r\n  trainer.train(\r\n      create_input_dict_fn,\r\n      model_fn,\r\n      train_config,\r\n      master,\r\n      task,\r\n      FLAGS.num_clones,\r\n      worker_replicas,\r\n      FLAGS.clone_on_cpu,\r\n      ps_tasks,\r\n      worker_job_name,\r\n      is_chief,\r\n      FLAGS.train_dir,\r\n      graph_hook_fn=graph_rewriter_fn)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\nFull error script:\r\n```\r\n$object_detection jp3spinelli$ python legacy/train.py --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v3_small_coco.config --logtostderr\r\n\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse object_detection/model_main.py.\r\nW0717 13:07:39.904257 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py:250: main (from __main__) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse object_detection/model_main.py.\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py:265: create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nW0717 13:07:39.919519 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py:265: create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0717 13:07:39.935688 4548259264 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\nW0717 13:07:39.944289 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:175: map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0717 13:07:40.005249 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:175: map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\r\n\r\nW0717 13:07:40.027405 4548259264 module_wrapper.py:139] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\r\n\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:48: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\r\nW0717 13:07:42.934035 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/builders/dataset_builder.py:48: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nW0717 13:07:42.989105 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/box_list_ops.py:231: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0717 13:07:43.008011 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/box_list_ops.py:231: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\nW0717 13:07:43.557287 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0717 13:07:43.561017 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0717 13:07:43.562012 4548259264 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/training/input.py:752: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nTraceback (most recent call last):\r\n  File \"legacy/train.py\", line 186, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"legacy/train.py\", line 182, in main\r\n    graph_hook_fn=graph_rewriter_fn)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py\", line 290, in train\r\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n  File \"/Users/jp3spinelli/Desktop/models/research/object_detection/legacy/deployment/model_deploy.py\", line 192, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/object_detection-0.1-py2.7.egg/object_detection/legacy/trainer.py\", line 180, in _create_losses\r\n    train_config.use_multiclass_scores)\r\nValueError: need more than 0 values to unpack\r\n```", "comments": ["Why don't you try out [these tutorials](https://github.com/tensorflow/models/tree/master/research/object_detection/colab_tutorials)", "@jp3spinelli \r\n\r\nI think this is more related to models repo.Please, raise an issue in models repo by filling issue template from [here](https://github.com/tensorflow/models/issues/new/choose).Also, please share related code so it helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41503\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41503\">No</a>\n"]}, {"number": 41502, "title": "Modify smart_resize in keras.preprocessing.image", "body": "Rewrite the crop part for `smart_resize` to avoid if-else. The target height (width) is always the smaller of original height (width) or the height (width) calculated from width (height). \r\nThis change makes `smart_resize` compatible with `tf.data.Dataset`. \r\n\r\nFixes #41501 ", "comments": []}, {"number": 41501, "title": "smart_resize in keras preprocessing not compatible with Dataset from tf.data", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.4.0-dev20200717\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nUsing `smart_resize` on Dataset object:\r\n```\r\nsize = (200, 200)\r\nds = ds.map(lambda img: smart_resize(img, size))\r\n``` \r\nthrows error\r\n```\r\nOperatorNotAllowedInGraphError: in user code:\r\n\r\n    <ipython-input-4-1b8d37623c29>:4 None  *\r\n        lambda image: tf.keras.preprocessing.image.smart_resize(image, size))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py:126 smart_resize  **\r\n        if target_ratio < img_ratio:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:878 __bool__\r\n        self._disallow_bool_casting()\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:491 _disallow_bool_casting\r\n        self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:480 _disallow_in_graph_mode\r\n        \" this function with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThis should work according to documentation (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/preprocessing/image/smart_resize)\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1_awoHwxurYy0kM2UNQaUvHuOaMHInRxE?usp=sharing\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nIMG_SIZE=224\r\nsize = [IMG_SIZE, IMG_SIZE]\r\n\r\nnp_image = np.random.rand(32, size[0], size[1], 3)\r\nds_train = tf.data.Dataset.from_tensor_slices(np_image)\r\nds_train = ds_train.map(lambda image: tf.keras.preprocessing.image.smart_resize(image, size))\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41501\">No</a>\n"]}, {"number": 41500, "title": "fit the model with features on GPU (AssertionError: Could not compute output Tensor(\"dense_3/Identity:0\", shape=(None, 1), dtype=float32)) ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nTF version 2.2\r\n\r\n**Issue occurs when we call model.fit after creating keras model. We are using tfhub bert layer for its word embeddings ,setting trainable =False** as below:-\r\n```\r\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=False)\r\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\r\n\r\nbert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)\r\n```\r\n\r\n```\r\nX_train_pooled_output.shape(50,000,768)  and X_test_pooled_output.shape(20,000, 768) batch_size = 32\r\nmode3.fit(X_train_pooled_output, y_train, epochs=nb_epoch,batch_size=batch_size,verbose=1,validation_data = (X_test_pooled_output,y_test),callbacks=callback_list)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/21074002/87811649-9cc6de00-c87c-11ea-9f77-0d705e230c5e.png)\r\n\r\n", "comments": ["thanks the issue is resolved now. hence closing the issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41500\">No</a>\n", "Facing the same issue. What has been the resolution provided for this?", "no resolution. We took a workaround in our code.", "> no resolution. We took a workaround in our code.\r\n\r\nWhat was the workaround? I am currently encountering the same/related issue."]}, {"number": 41499, "title": "Potential overflow in tf.broadcast_to", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen passing a large positive value (`2660446024`) to `shape` to `tf.broadcast_to`, it throws an `InvalidArgumentException`  but the error message  `Dimension -1634521272 must be >= 0 ` complains about a number that was not actually given. It seems there is a potential overflow going on internally. \r\n\r\n**Describe the expected behavior**\r\nCorrect error message\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.constant([1, 2, 3])\r\ntf.broadcast_to(x, [2660446024])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n`InvalidArgumentError: Dimension -1634521272 must be >= 0 [Op:BroadcastTo]`\r\n\r\n", "comments": ["I will take a look at this, thank you for filing the issue.", "I tried in colab with TF versions 2.2, 2.3-rc1 and nightly versions(`2.4.0-dev20200719`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f56e33799db581c4cf8655dc2ca94fc9/untitled.ipynb).Thanks!", "@mjkim720,\r\nSorry for the delayed response. Upon running the code with **`Tensorflow Version 2.4.1`**, we see a meaningful error:\r\n\r\n> InvalidArgumentError: Incompatible shapes: [3] vs. [2660446024] [Op:BroadcastTo] \r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/04e9271376e50d80a43c80abd2e66899/untitled.ipynb#scrollTo=xvFpAG4AcJ2J). \r\nPlease let us know if we can close this issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41499\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41499\">No</a>\n"]}, {"number": 41498, "title": "Update version numbers for TensorFlow 2.3.0-rc2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.3.0-rc1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.3.0rc1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": ["Can I ask you why we have two strings `2.3.0-rc1` and `2.3.0rc1`?"]}, {"number": 41497, "title": "Refactor gcs part 1 random access file", "body": "@mihaimaruseac \r\nThis PR refactors `gcs` to use the `core` implementation. The change is:\r\n- Get various envs\r\n- Use `ram_file_block_cache` https://github.com/tensorflow/tensorflow/blob/2629e4f9a4d982448494c880402f603cf559c488/tensorflow/core/platform/cloud/gcs_file_system.cc#L826\r\n- This variable can be controlled by env so I drop it https://github.com/tensorflow/tensorflow/blob/2629e4f9a4d982448494c880402f603cf559c488/tensorflow/core/platform/cloud/gcs_file_system.cc#L788\r\n- Merge 2 `RandomAccessFile` into one. https://github.com/tensorflow/tensorflow/blob/2629e4f9a4d982448494c880402f603cf559c488/tensorflow/core/platform/cloud/gcs_file_system.cc#L261 and https://github.com/tensorflow/tensorflow/blob/2629e4f9a4d982448494c880402f603cf559c488/tensorflow/core/platform/cloud/gcs_file_system.cc#L289\r\n- I am missing this part https://github.com/tensorflow/tensorflow/blob/2629e4f9a4d982448494c880402f603cf559c488/tensorflow/core/platform/cloud/gcs_file_system.cc#L991-L1003 will be added in later PR.", "comments": ["@mihaimaruseac \r\nReady to review", "This still fails to build internally\r\n\r\n```\r\ntensorflow/c/experimental/filesystem/plugins/gcs/gcs_filesystem.cc:165:3: error: expected unqualified-id\r\n  gcs_file->buffer_start = start;\r\n```", "Apologize for the error. I hope everything is good now.", "Hmm, still fails\r\n\r\n```\r\ntensorflow/c/experimental/filesystem/plugins/gcs/gcs_filesystem.cc:164:3: error: 'exclusive_locks_required' attribute cannot be applied to a statement\r\n  ABSL_EXCLUSIVE_LOCKS_REQUIRED(gcs_file->buffer_mutex);\r\n  ^                                                    ~\r\nabsl/base/thread_annotations.h:166:18: note: expanded from macro 'ABSL_EXCLUSIVE_LOCKS_REQUIRED'\r\n  __attribute__((exclusive_locks_required(__VA_ARGS__)))\r\n                 ^\r\n1 error generated.\r\n```", "`ABSL_EXCLUSIVE_LOCKS_REQUIRED` is usually used for member function of a class which uses some variable member guarded by another member mutex. I dont know how to use it outside of class scope. Maybe we should remove all the `ABSL_GUARDED_BY` ?", "One more question: Why the status of `import/copybara` is passed ? It is really strange to me.", "For the first question, let me look more into it.\r\n\r\nFor the copybara question, that check only states that the change has been imported internally. There are no tests being run on `import/copybara` (the internal tests are triggered upon internal approval of the converted code)", "What if we use `absl::MutexLock l(&gcs_file->buffer_mutex);` instead of the absl macro?", "I got an idea. In the `core` implementation, `FillBuffer` is used only one time. So I think the best solution is to remove `FillBuffer` and just write that code directly into `Read`.\r\n\r\n", "> What if we use `absl::MutexLock l(&gcs_file->buffer_mutex);` instead of the absl macro?\r\n\r\n`Read` is holding `absl::MutexLock l(&gcs_file->buffer_mutex);`, so it may cause error if we try to lock `gcs_file->buffer_mutex` again.", "Good point. We can make it an internal function of the struct or just inline. Let's go with inlining for now since it has only one use"]}, {"number": 41496, "title": "RGB to YUV: Conflicting information between example and description", "body": "https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv\r\n\r\nThe documentation says this function is only well defined if the values are between 0 and 1, but the example uses an input with values greater than 1.", "comments": ["Hi, Can I create a PR to fix this? Thank you!", "PR to fix this issue is here, https://github.com/tensorflow/tensorflow/pull/41565\r\nPlease take a look, thank you!", "This was resolved in `tf-nightly` with this PR https://github.com/tensorflow/tensorflow/pull/42270.\r\n\r\nPlease check the page here https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv?version=nightly\r\n\r\nI am closing this issue as this was already resolved. Thanks!"]}, {"number": 41495, "title": "Does TensorFlow Profiler step-time consider overlapping?", "body": "In TensorFlow Profiler overview page, the \"step-time graph\" consists of 8 parts of time, such as \"Input\", \"Host compute\", \"Device compute\". Is it from wall-clock time view, or does it consider each part separately?\r\nFor example, if I use tensorflow to invoke a matrix multiply operation on GPU which lasts for 3ms in CUDA kernel, at the same time I use tensorflow to invoke another operator on CPU which lasts for 2ms. Then 2ms of the first GPU computing is overlapped(hided) by the CPU computing, and only the last 1ms is observed. In TensorFlow Profiler, will it show \"Device compute\" as \"3ms\" or as \"1ms\"?", "comments": ["I do not work on TensorFlow any more.\n\nOn Fri, Jul 31, 2020 at 9:59 AM gowthamkpr <notifications@github.com> wrote:\n\n> Assigned #41495 <https://github.com/tensorflow/tensorflow/issues/41495>\n> to @prb12 <https://github.com/prb12>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41495#event-3609420683>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACYDJGJGFCI2PVGIEKMZ3ELR6LZ6PANCNFSM4O6KRBPQ>\n> .\n>\n", "@gaoteng-git,\r\nSorry for the delayed response. Can you please try executing [**`Tensorflow Profiler`** ](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) with the latest **`Tensorflow Version (2.5)`** and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]