[{"number": 53701, "title": "The GRU/LSTM function bugs when using TF2.7", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.7\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.5\r\n- GPU model and memory:NVIDIA Geforce 1070, 8GB\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHello\uff0cWhen I used the GRU/LSTM model to accomplish the text classification with IMDB dataset\uff0cthere were some problems  in model  fitting process.This is the dataset shape information.\r\n![datasets](https://user-images.githubusercontent.com/4904955/148642698-a1fe2d86-ed76-404e-a89b-34a122a21618.png)\r\nWhen I set the GRU layer with  return_sequences=True:\r\n\r\n![GRU](https://user-images.githubusercontent.com/4904955/148642812-1fd832e2-7d8f-4bc4-94bc-443f0ed4677a.png)\r\n\r\n![1-True](https://user-images.githubusercontent.com/4904955/148642730-781a87a5-67f1-426f-a3a3-30e70698967c.png)\r\n\r\nThen I removed the return_sequences=False,the code was seemed to be normal,but the gradient was't decreased:\r\n\r\n![1-false](https://user-images.githubusercontent.com/4904955/148642765-bffba379-bd36-4d8d-bcc0-1667afff676c.png)\r\n\r\n\r\nThe LSTM performance was also similar with the GRU. If I used the two layers GRU with the last GRU's return_sequences=False\uff0cthe performance of model was same as the single GRU/LSTM layer.\r\n![M-RNN](https://user-images.githubusercontent.com/4904955/148642917-8adb0da9-b3d3-4a96-8000-ac2764170bdf.png)\r\n\r\n![M-RNN2](https://user-images.githubusercontent.com/4904955/148642905-e30f9f6b-4483-4bc5-b4a9-5b9dc396da92.png)\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nI asked my teammates who used **tf2.6.2**,they said all the problem I mentioned above didn't occur when they ran these codes. But some people who used  **tf2.7** found the same problem. I also checked the different systems like linux in colab or docker,it seems that the bugs could happen when using tf2.7.I has updated my code with zip attachment,\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe Code is here:\r\n[imdb.zip](https://github.com/tensorflow/tensorflow/files/7833147/imdb.zip)\r\n\r\n", "comments": ["Hi @hayatejp !\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "> Hi @hayatejp ! Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 . Thanks!\r\n\r\nHi mohantym ,I will post the issue on Keras,Thank you for your support\r\n", "Ok @hayatejp ! Can you please close this issue here as it will be tracked in keras repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53701\">No</a>\n", "> Ok @hayatejp ! Can you please close this issue here as it will be tracked in keras repo. Thanks!\r\n\r\nOK~"]}, {"number": 53700, "title": "Why is tensorflow2.6 so much slower than pytorch1.10.0+cu113?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n- TensorFlow installed from binary:\r\n- TensorFlow version --2.6:\r\n- Python 3.8:\r\n- GPU model R3090 and memory 24G:\r\n\r\ntensorflow 2.6 code\uff1a\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import preprocessing\r\nfrom tensorflow.keras.applications.vgg19 import VGG19\r\n\r\nprint(tf.__version__)\r\n\r\nmodel = VGG19(weights='imagenet')\r\nimg = preprocessing.image.load_img('008.jpg', target_size=(224, 224))\r\nx = preprocessing.image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx /= 255.\r\nfor _ in range(5):\r\n    t1 = time.time()\r\n    preds = model.predict(x)\r\n    print(preds.shape)\r\n    t2 = time.time()\r\n    print(t2 - t1)\r\n```\r\n\r\noutput:\r\n```\r\n2.6.0\r\n(1, 1000)\r\n2.8845417499542236\r\n(1, 1000)\r\n0.044057369232177734\r\n(1, 1000)\r\n0.027636051177978516\r\n(1, 1000)\r\n0.04877758026123047\r\n(1, 1000)\r\n0.027869701385498047\r\n```\r\n\r\npytorch code:\r\n```\r\nimport torch\r\nimport torchvision\r\nimport time\r\nimport thop\r\n\r\nprint(torch.__version__)\r\n\r\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n\r\nmodel = torchvision.models.vgg19(pretrained=False)\r\nmodel.eval().to(device)\r\ninputs = torch.rand((1, 3, 224, 224)).to(device)\r\nwith torch.no_grad():\r\n    for _ in range(5):\r\n        t1 = time.time()\r\n        outputs = model(inputs)\r\n        print(outputs.shape)\r\n        t2 = time.time()\r\n        print(t2 - t1)\r\n```\r\n\r\noutput:\r\n```\r\n1.10.0+cu113\r\ntorch.Size([1, 1000])\r\n0.0020911693572998047\r\ntorch.Size([1, 1000])\r\n0.0019617080688476562\r\ntorch.Size([1, 1000])\r\n0.001947164535522461\r\ntorch.Size([1, 1000])\r\n0.001924753189086914\r\ntorch.Size([1, 1000])\r\n0.0019559860229492188\r\n```\r\nIs this my problem or the tensorflow problem?", "comments": ["Check #42475.", "> Check #42475.\r\n\r\nthanks."]}, {"number": 53699, "title": "ValueError: Exception encountered when calling layer \"transformer_decoder_3\" (type TransformerDecoder).  Could not find matching concrete function to call loaded from the SavedModel", "body": "<em>I got this error, while trying to load my transformer model. </em>\r\n\r\n\r\nModel:\r\n>Layer (type)          ==         Output Shape    ==     Param #   ==  Connected to                     \r\n>=============================================================================================>=====\r\n> encoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                               \r\n >                                                                                                 \r\n >positional_embedding_6 (Positi    == (None, None, 256)==   645120  ==    ['encoder_inputs[0][0]']         \r\n >onalEmbedding)                                                                                   \r\n  >                                                                                                \r\n >decoder_inputs (InputLayer)      ==  [(None, None)]  ==     0    ==       []                               \r\n  >                                                                                                \r\n >transformer_encoder_3 (Transfo    == (None, None, 256) ==  3155456   ==  ['positional_embedding_6[0][0]'] \r\n >rmerEncoder)                                                                                     \r\n >                                                                                                 \r\n >model_7 (Functional)      ==     (None, None, 2500) ==  6547140  ==   ['decoder_inputs[0][0]',         \r\n  >                                                                                                                 'transformer_encoder_3[0][0]']  \r\n   >                                                                                               \r\n>==================================================================================================\r\n\r\nError:\r\n>ValueError: Exception encountered when calling layer \"transformer_decoder_3\" (type TransformerDecoder).\r\n>\r\n>Could not find matching concrete function to call loaded from the SavedModel. Got:\r\n > Positional arguments (4 total):\r\n   > * Tensor(\"inputs:0\", shape=(None, None, 256), dtype=float32)\r\n   > * Tensor(\"encoder_outputs:0\", shape=(None, None, 256), dtype=float32)\r\n   > * None\r\n   > * False\r\n  >Keyword arguments: {}\r\n>\r\n> Expected these arguments to match one of the following 2 option(s):\r\n>\r\n>Option 1:\r\n > Positional arguments (4 total):\r\n  >  * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='inputs')\r\n   > * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='encoder_outputs')\r\n    >* TensorSpec(shape=(None, None), dtype=tf.bool, name='mask')\r\n    >* False\r\n  >Keyword arguments: {}\r\n>\r\n>Option 2:\r\n > Positional arguments (4 total):\r\n  >  * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='inputs')\r\n   > * TensorSpec(shape=(None, None, 256), dtype=tf.float32, name='encoder_outputs')\r\n    >* TensorSpec(shape=(None, None), dtype=tf.bool, name='mask')\r\n    >* True\r\n > Keyword arguments: {}\r\n>\r\n>Call arguments received:\r\n > \u2022 args=('tf.Tensor(shape=(None, None, 256), dtype=float32)',)\r\n > \u2022 kwargs={'encoder_outputs': 'tf.Tensor(shape=(None, None, 256), dtype=float32)', 'training': 'None'}\r\n\r\nI also got this warning, while saving model.\r\n>WARNING:absl:Found untraced functions such as embedding_12_layer_call_fn, embedding_12_layer_call_and_return_conditional_losses, embedding_13_layer_call_fn, embedding_13_layer_call_and_return_conditional_losses, multi_head_attention_9_layer_call_fn while saving (showing 5 of 150). These >functions will not be directly callable after loading.\r\n\r\n\r\n**Important Notice**\r\n\r\nGOOGLE COLAB\r\nhttps://colab.research.google.com/drive/1vhJiMvCnxT7y4KhMv7wez_zBqwI-7yqu?usp=sharing\r\n\r\n\r\n", "comments": ["@MrBuzzy4 ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53699\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53699\">No</a>\n", "@MrBuzzy4 I'm facing with the same problem, did you solved it?"]}, {"number": 53698, "title": "Converter for Tile operation", "body": "- ConvertTile operator is implemented so that both parameters (tensor and multiplier) can be passed as tensors and weights. A unit test with 6 tests and all combinations of passed parameters is provided.\r\n- New macro #define CHECK_INPUT_SIZE (size, exp_size, node_def) unifies the testing of the number of parameters passed. It is used in 8 different places.\r\n- The helper function template <typename T> void AdjustVectorByDims(...) makes writing tests much easier (especially tests with parameters that can be passed as tensors and weights). For example, today we can use some parameters like tensors with predefined dimensions, but with no real numbers. An attempt to pass the same parameters with the same values as weights leads to the crash. We see a similar crash when the vector providing the values assigned to the tensor is not empty, but its length does not match the number of values defined by the dimensions. AdjustVectorByDims fixes these two problems and is called by default in the situations described, but these calls can also be blocked when such failures are intentional.", "comments": ["Thanks @andrei5055 for the PR! Tagging @bixia1 for review.", "Shall we close this PR as it is replaced by [PR53718](https://github.com/tensorflow/tensorflow/pull/53718)?", "Yes, this one should be closed, it was replaced by [PR53718](https://github.com/tensorflow/tensorflow/pull/53718)\r\n\r\n", "@andrei5055 Any update on this PR? Please. Thanks!", "> @andrei5055 Any update on this PR? Please. Thanks!\r\n\r\n@gbaned : This one was closed and replaced by [#53718](https://github.com/tensorflow/tensorflow/pull/53718) and [#54424\r\n](https://github.com/tensorflow/tensorflow/pull/54424)which were also closed. Currently, only [#54427](https://github.com/tensorflow/tensorflow/pull/54427) should be considered.", "Closing this one because the latest version is in [PR#54427](https://github.com/tensorflow/tensorflow/pull/54427)"]}, {"number": 53696, "title": "TensorBoard doesn't work on IOS/IpadOS. Requires apple icons. ", "body": "\r\nAfter starting the server it gives the following result:\r\n\r\nW0107 19:43:36.780149 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404\r\nW0107 19:43:36.798506 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404\r\nW0107 19:43:36.832389 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404\r\nW0107 19:43:36.849392 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404\r\nW0107 19:43:36.900132 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404\r\nW0107 19:43:36.915132 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404\r\nW0107 19:43:36.948019 139768956442176 application.py:556] path /apple-touch-icon-precomposed.png not found, sending 404\r\nW0107 19:43:36.965894 139768956442176 application.py:556] path /apple-touch-icon.png not found, sending 404\r\n\r\nAnd in afterwards you can not access tensorboard on apple devices (tested on Iphone 8, iphone 11, ipad 2018). but on android/chome(macos) works great", "comments": ["@NikitaGordia \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),Thanks!", " no. it is clear", "@NikitaGordia This issue is more related to TensorBoard ,could you please post this issue in [TensorBoard](https://github.com/tensorflow/tensorboard/issues) repo to get the right help?Thank you!", "Yeah, thank you, I missed it. Just moved to the tensorboard repo", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53696\">No</a>\n"]}, {"number": 53695, "title": "Fix crash of tf.sparse.split when axis is not a scalar.", "body": "This PR tries to fix the crash of tf.sparse.split when axis is not a scalar, as was raised in #53660.\r\n\r\nThis PR fixes #53660.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac  for the review. The PR has been updated. Please take a look."]}, {"number": 53694, "title": "Add complex data type support to tf.sparse.to_dense", "body": "This PR tries to fix the issue raised in #53653 where `tf.sparse.to_dense` does not support complex64 or complex128\r\n(`tf.sparse.from_dense` support complex dtypes).\r\n\r\nThis PR adds complex64/complex128 support for `tf.sparse.to_dense`.\r\n\r\nThis PR fixes #53653.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 53693, "title": "Will experimental_distribute_dataset or experimental_distribute_datasets_from_function distribute same data in more than one batch ?", "body": "\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: 2.2\r\n-   **Python version**: 3.8\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**: \r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nam using tensorflow 2.2, Noticed same data being distributed in multiple batches/data shard . total batch count and no of records in each batch matched to expected count but the records present in each batch isn't unique.  Strategy - Multiworker mirror strategy\r\n\r\n### Source code / logs\r\nI won't be able to share the codes here. but am using multiworker mirror strategy with unbatch, batch and autoshard policy of data.\r\n", "comments": ["@kabilan6 ,\r\nWe see that you are using tf version 2.2, which is not actively supported, please update to v2.7 and let us know if you are using same issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I tried upgrade to v2.7, but i'm getting this error\r\nAttributeError: 'Tensor' object has no attribute '_keras_mask'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield                   \r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 699, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in user code:\r\n\r\nsame code runs fine with v2.2", "@kabilan6 ,\r\nPlease take a look at this link [1](https://github.com/tensorflow/tensorflow/issues/42403#issuecomment-751622363) and [2](https://github.com/tensorflow/tensorflow/issues/36624) with the similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53693\">No</a>\n"]}, {"number": 53692, "title": "NNAPI/Hexagon Support Through CMAKE on RB5 Arm Linux Platform", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu 18.04 Qualcomm Linaro Release\r\n- Qualcomm RB5\r\n- Source\r\n- TensorFlow version: v2.6\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source):  \r\n- GCC/Compiler version (if compiling from source): gcc 7.5\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: Adreno GPU 650\r\n\r\nHello, I am having trouble understanding the tutorials and getting delegate support for Tensorflow lite c++/c library on the Qualcomm RB5.\r\n\r\nI have been using ```CMAKE / GCC``` installation natively on board.\r\n\r\nI have the following flags enabled ```-DTFLITE_ENBLE_XNN_PACK -DTFLITE_-DTFLITE_ENABLE_GPU -DTFLITE_ENABLE_NNAPI```\r\n\r\nAnd I am able to compile a working(ish) library and do inferencing. However the only delegate that works is the XNN_PACK delegate. The GPU delegate causes the program to fail when specifying it (maybe not linking correct adreno library to the inferencing code)...could use some help there)\r\n\r\nBut mostly I am concerned about building NNAPI and HEXAGON delegate support. The RB5 has hexagon libraries already installed and I will also install the libhexagon_nn_skel.so libraries.\r\n\r\nThere is no documentation using cmake to enable hexagon support in the TensorFlow lite library and I see no flags in the CMakeLists.txt.\r\n\r\nI have downloaded the Hexagon SDK and will copy the android_ndk _rc19 workspace to somewhere on board and set the path. Will this enable NNAPI correctly? Because now it does not believe it skips that compilation because it cannot find the correct headers (not installed yet)\r\n\r\n\r\n\r\n\r\nBasically I have the following questions:\r\n\r\n\r\n1) Will moving the android_rc19_ndk to a directory on board the RB5 and setting ANDROID_NDK_HOME successfully compile the NNAPI delegate ?\r\n\r\n2) How can I enable hexagon support with the CMake Installation of the TfLite ? All hexagon libraries are on board.\r\n\r\n3) What are the correct library links to use to successfully enable a GPU delegate on the Adreno GPU? I see existing support documented on the tensorflow website for this.\r\n\r\n4) If none of the three on board are viable - Would building with Bazel on board work? Or do we need to do a cross compilation, if so how?\r\n\r\nThank you for taking the time to read this. I really enjoy using tensorflow lite and I know the RB5 community would really enjoy a documented process to do this correctly and unlock all of it's potential.\r\n\r\n\r\n", "comments": ["Hi @sachinprasadhs ! Could you please look at this issue?", "Can you have a look into [this](https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_c_api) document on Hexagon delegate C API and also check these [FAQs](https://www.tensorflow.org/lite/performance/hexagon_delegate#faq) on Hexagon delegate. Let us know if there is anything missing which you are looking for. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53692\">No</a>\n"]}, {"number": 53690, "title": "Training time is varying between docker and without docker ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04. \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.4/8.2\r\n- GPU model and memory:A6000 & 48GB\r\n\r\n**Describe the current behavior**\r\nIf I use docker image tensorflow/tensorflow:latest-gpu container for training Convolution based neural network it is taking less time compared to Tensorflow installed using pip install tensorflow-gpu==2.7. \r\n\r\n**Describe the expected behavior**\r\nIf we use docker image/ installed using pip also the training should not change much\r\n\r\nI am using tf.distribute.MirroredStrategy() to use all GPU's. How to resolve this issue?\r\n", "comments": ["@rameshkunasi \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Sorry... We are using in built database, We cannot give that data and code here. If you suggest any experiment I can do and share the results", "@rameshkunasi If you're unable to provide  a code snippet of the reported issue , then it would be difficult for us to reproduce this one from our end .Could you please post this issue in [TF discussion](https://discuss.tensorflow.org/) forum where there is a larger community to get you the right help? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53689, "title": "The absence of ruy profiler header in gather.h file", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: x\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe `gather.h` file of reference ops uses ruy profiler but it does not include the header file.\r\nWhen I include `gather.h` file alone, it occurs an error because `ruy` has not been declared.\r\n\r\n**Any other info / logs**\r\n\r\n```\r\ntensorflow/lite/kernels/internal/reference/gather.h:31:3: error: \u2018ruy\u2019 has not been declared\r\n   ruy::profiler::ScopeLabel label(\"Gather\");\r\n```\r\n\r\nCan I add the ruy header file to gather.h file?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f6331b7c3da432f21ce12675b15e60ec59933e47/tensorflow/lite/kernels/internal/reference/gather.h#L18-L30\r\n\r\nIn other files in the same case, it explicitly contains a header file as shown below.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f6331b7c3da432f21ce12675b15e60ec59933e47/tensorflow/lite/kernels/internal/reference/exp.h#L20-L29", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53689\">No</a>\n"]}, {"number": 53688, "title": "LocallyConnected layers default initializer glorot_uniform determines fanning incorrectly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7 running NGC tensorflow_21.07-tf2-py3 container\r\n- TensorFlow installed from (source or binary): binary, Nvidia container\r\n- TensorFlow version (use command below): 2.5.0+nv\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 11.5\r\n- GPU model and memory: Nvidia V100, 32 GB\r\n\r\n**Describe the current behavior**\r\nThe Keras LocallyConnectednD layers all use default kernel_initializer glorot_uniform, just like their Conv counterparts. However, with the default implementation (1), the parameter tensor for e.g. LocallyConnected1D ends up being three-dimensional input-size * kernel-size * filters. In short, this means that the initialization values for a layer are vastly different if you just switch from Conv1D to LocallyConnected1D, to the point where gradients can easily vanish.\r\n\r\n**Describe the expected behavior**\r\nWhile a locally connected layer and a convolutional layer will have different training dynamics, the underlying argument for the Xavier initialization should result in initial tensors of similar magnitude, since the number of terms in the sum for each activation is identical.\r\n\r\nGlorotUniform is a wrapper for VarianceScaling. VarianceScaling will use _compute_fans internally, which just stacks additional input dimensions on top in a multiplicative manner (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/initializers/initializers_v2.py#L1011). This has been confirmed in the master branch, as indicated in the link.\r\n\r\nDue to compat reasons, I guess changing the default at this point could be hard, but with a clean slate the expected behavior would be that the default init for Conv layers and LocallyConnected layers would be identical. At this point, maybe a warning and comments in the docs for both LocallyConnected layers and the initializers on how the fanning is actually determined.\r\n\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): adding a warning mechanism in the locally connected layers if used with a default initializer. This is not ideal, as even customized code could assume another definition of fanning.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1bLUD52-syj01zjHyVXhGzdYZggLFM-kf\r\n\r\n", "comments": ["Hi @cnettel ! \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53688\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53688\">No</a>\n"]}, {"number": 53687, "title": "[PluggableDevice] Add Variant DEVICE_DEFAULT registration for ResourceScatterUpdate and ResourceGather", "body": "When `ResourceScatterUpdate` and `ResourceGather` run with `DT_VARIANT`, everything is executing on the CPU. Pluggable devices shouldn't have to reimplement this logic.", "comments": ["Hi,\r\n\r\nCould someone review this PR? This is something that we need for the release of our pluggable device plugin.\r\n\r\nThank you!"]}, {"number": 53686, "title": "API generation makes inheritance relationship confusing after r2.6", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: Python 3.6.8\r\n\r\n**Describe the current behavior**\r\nI'm trying to select different algorithms by different optimizer, and got following unexpected behavior:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.optimizer_v2 import adam\r\nfrom tensorflow.python.keras.optimizer_v2 import optimizer_v2\r\n\r\nopt = tf.keras.optimizers.Adam(1E-3)\r\ncond = isinstance(opt, optimizer_v2.OptimizerV2) # It expects true.\r\n```\r\n\r\nBefore r2.6, everything works fine. But after r2.6, it will get false instead. I noticed that there is an API-gen process when exporting the APIs. And I make other tests.\r\n\r\n```python\r\nopt = tf.keras.optimizers.Adam(1E-3)\r\ncond = isinstance(opt, optimizer_v2.OptimizerV2) # 2.6: false, 2.5: true\r\ncond = isinstance(opt, tf.keras.optimizers.Optimizer) # 2.6: true, 2.5: true\r\n\r\nopt = adam.Adam(1E-3)\r\ncond = isinstance(opt, optimizer_v2.OptimizerV2) # 2.6: true, 2.5: true\r\ncond = isinstance(opt, tf.keras.optimizers.Optimizer) # 2.6: false, 2.5: true\r\n```\r\n\r\nIt seems that the APIs have an independent type system after r2.6. It makes code hard to manage when we need to extend some functionalities.\r\n", "comments": ["@Lifann \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53686\">No</a>\n"]}, {"number": 53685, "title": "How to debug xla .", "body": "I merge the features of dynamic_partition in XLA from master(TF2.8) to TF1.15, and then the features report a error:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Expected instruction to have shape equal to (s64[40960]{0}, s64[40960]{0}), actual shape is (s64[<=40960]{0}, s64[<=40960]{0}):\r\n%tuple.78 = (s64[<=40960]{0}, s64[<=40960]{0}) tuple(s64[40960]{0} %dynamic-slice.64, s64[40960]{0} %dynamic-slice.67), metadata={op_name=\"XLA_Retvals\"}\r\n\r\nFailed after pipeline-start\r\n\t [[{{node cluster_0_1/xla_compile}}]]\r\n\t [[cluster_0_1/merge_oidx_1/_7]]\r\n  (1) Internal: Expected instruction to have shape equal to (s64[40960]{0}, s64[40960]{0}), actual shape is (s64[<=40960]{0}, s64[<=40960]{0}):\r\n%tuple.78 = (s64[<=40960]{0}, s64[<=40960]{0}) tuple(s64[40960]{0} %dynamic-slice.64, s64[40960]{0} %dynamic-slice.67), metadata={op_name=\"XLA_Retvals\"}\r\n\r\nFailed after pipeline-start\r\n\t [[{{node cluster_0_1/xla_compile}}]]\r\n0 successful operations.\r\n```\r\n\r\nHow can I debug this error ? Please give me some ideas, thanks~!", "comments": ["@zhaozheng09 ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53684, "title": "framework not found TensorFlowLite", "body": null, "comments": ["Hi @bhesaniyakrunal ! Could you please fill the [template](https://github.com/tensorflow/tensorflow/issues/new/choose) as it would help expedite the issue? Attaching relevant [thread](https://stackoverflow.com/questions/9458739/ld-warning-directory-not-found-for-option) for reference though.", "+1 to what @mohantym said. Specifically it's unclear how to reproduce the issue. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53684\">No</a>\n"]}, {"number": 53683, "title": "tf.data.dataset gather element", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 2.7\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to do something like tf.gather on tf.data.Dataset. \r\nFor example, \r\ndataset = tf.data.Dataset.from_tensor_slices(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])\r\nchoice_dataset = tf.data.Dataset.range(3).repeat(2) # Define a dataset containing [0, 1, 2, 0, 1, 2]\r\n\r\nI want it to output a dataset having ['a', 'b', 'c', 'a', 'b', 'c']\r\n\r\nThis is somewhat similar to choose_from_datasets, but that method choose from multiple datasets not element in the dataset\r\n\r\n**Will this change the current api? How?** Add new method to tf.data.Dataset\r\n\r\n**Who will benefit with this feature?** Tensorflow dataset user\r\n\r\n**Any Other info.**\r\n I am not sure if there is a workaround. Please correct me if I am wrong.\r\n", "comments": ["\"This is somewhat similar to choose_from_datasets\" like FilterDataset ?", "@nviwch Do you want to replace the original `dataset` with only first three elements of that dataset? Do you want the updated dataset as ['a', 'b', 'c', 'a', 'b', 'c','a', 'b', 'c'] in the above example? Thanks!", "Not just first three, it is just an example. I want it to behave like tf.gather\r\n![image](https://user-images.githubusercontent.com/19923010/149432382-1ecb6c05-a834-459e-b6db-9d0dd013047b.png)\r\nFor example,\r\ndataset = tf.data.Dataset.from_tensor_slices(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])\r\nif choice_dataset contain [5, 7, 2, 1, 0, 0, 3], I want to get a dataset with ['f', 'h', 'c', 'b', 'a', 'a', 'd']\r\n", "Would it work to use `tf.gather` directly? i.e. instead of\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])\r\ndataset = dataset.gather(...)\r\n```\r\n\r\ndo\r\n\r\n```\r\ndata = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\r\ndata.gather(...)\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\n```\r\n\r\nDoing gather inside of tf.data would be expensive since Datasets are Iterables without support for random access.", "One of the reason why I want to keep it in dataset is to cache and then train with combination of the cached data.\r\n\r\nFor example, data = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'], but I want to feed all combination pairs as training data, i.e.\r\n['a', 'b'], ['a', 'c'], ['a', 'd'], ..., ['b', 'c'], ...\r\n\r\nHow to do this with efficient dataset pipline?", "You can do this efficiently by generating the index pairs you want, then using `map` to translate them to examples:\r\n\r\n```python\r\ndata = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\r\nindices = [[1, 2], [1, 3], [1, 4], ... [2, 3], ...]\r\ndataset = tf.data.Dataset.from_tensor_slices(indices)\r\n\r\ndef lookup_indices(indices):\r\n  return data.gather(indices)\r\n\r\ndataset = dataset.map(lookup_indices)\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53682, "title": "NNAPI on android 11 fails with movenet fp16 and int8 tflite models", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): aarch64 Android 11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI have issues while working with movenet tflite models on android 11 NNAPI 1.3\r\nthe movenet models used are sourced from tfhub:\r\n\r\n1. [https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4](url)\r\n2. [https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4](url) \r\n\r\n\r\nthe above two singlepose movenet lighting tflite models are float16 and INT8 respectively, and I was trying to perform benchmarking of the same using the prebuilt benchmark model for android_aarch64 sourced from tflite website:\r\n[https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/android_aarch64_benchmark_model](url)\r\n\r\nI am easily able to benchmark the models on CPU and GPU, but when I try to run it on NNAPI, the benchmarking fails, which is interesting because even if the model is not supported by NNAPI delegate, it should fallback on CPU which is not happening. These models fail to execute on NNAPI CPU as well which is strange.\r\n\r\nlog for lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite:\r\n> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [nnapi-reference]\r\nNNAPI accelerators available: [nnapi-reference]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nERROR: NN API returned error ANEURALNETWORKS_BAD_DATA at line 992 while adding operation.\r\nERROR: Node number 303 (TfLiteNnapiDelegate) failed to prepare.\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply NNAPI delegate.\r\nBenchmarking failed.\r\n\r\nNode 303 is a GatherNd node, but I am not sure why it fails at this node because there are two more GatherNd nodes which come before node 303.\r\n\r\nlog for lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite:\r\n\r\n>  $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [nnapi-reference]\r\nNNAPI accelerators available: [nnapi-reference]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nERROR: NN API returned error ANEURALNETWORKS_BAD_DATA at line 992 while adding operation.\r\nERROR: Node number 162 (TfLiteNnapiDelegate) failed to prepare.\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply NNAPI delegate.\r\nBenchmarking failed.\r\n\r\nNode 162 for this model is a separable_conv2d/bias node.\r\n\r\nthe logcat files for both the operations are attached in logs section. \r\nI get the same results if i remove the '--nnapi_accelerator_name=nnapi-reference', or add '--nnapi_allow_fp16=true' parameter, I still get the same benchmarking failed issue as above.\r\n\r\nthese movenet models work well with CPU and GPU:\r\n\r\n1. lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite CPU 4 threads:\r\n> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  --num_threads=4\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]\r\n#threads used for CPU inference: [4]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nThe input model file size (MB): 2.89484\r\nInitialized session in 4.574ms.\r\n\r\n\r\n2. lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  GPU:\r\n\r\n> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite  --use_gpu=1\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]\r\nUse gpu: [1]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nARG_MAX: Operation is not supported.\r\nCAST: Operation is not supported.\r\nCONCATENATION: OP is supported, but tensor type isn't matched!\r\nFLOOR_DIV: Operation is not supported.\r\nGATHER_ND: Operation is not supported.\r\nMUL: OP is supported, but tensor type isn't matched!\r\nPACK: OP is supported, but tensor type isn't matched!\r\nRESHAPE: OP is supported, but tensor type isn't matched!\r\nSUB: OP is supported, but tensor type isn't matched!\r\nUNPACK: Operation is not supported.\r\n100 operations will run on the GPU, and the remaining 57 operations will run on the CPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nExplicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\nThe input model file size (MB): 2.89484\r\nInitialized session in 2957.12ms.\r\n\r\n\r\n\r\n3. lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite CPU 4 threads:\r\n\r\n> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --num_threads=4\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]\r\n#threads used for CPU inference: [4]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nThe input model file size (MB): 4.75851\r\nInitialized session in 3.211ms.\r\n\r\n\r\n\r\n4. lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite GPU:\r\n\r\n> $ ./android_aarch64_benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --use_gpu=1\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]\r\nUse gpu: [1]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nARG_MAX: Operation is not supported.\r\nCAST: Operation is not supported.\r\nCONCATENATION: OP is supported, but tensor type isn't matched!\r\nDEQUANTIZE:\r\nFLOOR_DIV: Operation is not supported.\r\nGATHER_ND: Operation is not supported.\r\nMUL: OP is supported, but tensor type isn't matched!\r\nPACK: OP is supported, but tensor type isn't matched!\r\nRESHAPE: OP is supported, but tensor type isn't matched!\r\nSUB: OP is supported, but tensor type isn't matched!\r\nUNPACK: Operation is not supported.\r\n245 operations will run on the GPU, and the remaining 52 operations will run on the CPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nExplicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.\r\nThe input model file size (MB): 4.75851\r\nInitialized session in 2013.4ms.\r\n\r\n\r\nto make sure that NNAPI is working for other models, I used a mobilenetv2 fp16 and int8 model from tfhub\r\n1. mobilenetv2-coco_fp16 : [https://tfhub.dev/sayakpaul/lite-model/mobilenetv2-coco/fp16/1](url)\r\n2. mobilenetv2-coco_int8 : [https://tfhub.dev/sayakpaul/lite-model/mobilenetv2-coco/int8/1](url)\r\nand i face no issues running NNAPI CPU.\r\n\r\noutput for mobilenetv2-coco/fp16 for NNAPI CPU:\r\n\r\n> $ ./android_aarch64_benchmark_model --graph=lite-model_mobilenetv2-coco_fp16_1.tflite   --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_mobilenetv2-coco_fp16_1.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [nnapi-reference]\r\nNNAPI accelerators available: [nnapi-reference]\r\nLoaded model lite-model_mobilenetv2-coco_fp16_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 11 delegate kernels.\r\nThe input model file size (MB): 4.2551\r\nInitialized session in 99.601ms.\r\n\r\n\r\nSo there is something wrong in movenet models which is failing when using NNAPi instead of falling back onto CPU. One reason i can think of, after analysing the logfile is due to tensor type not matching for CONCATENATION op, but not sure.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe movenet models, even if not entirely supported on NNAPI should fallback on CPU. If fallback is disabled, but graph is forced through NNAPI CPU, it should still give similar results as it would have when running simply on CPU, but that is not observed.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nlog files:\r\n[lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite_android_nnapi_logcat.txt](https://github.com/tensorflow/tensorflow/files/7825438/lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite_android_nnapi_logcat.txt)\r\n[lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite_android_nnapi_logcat.txt](https://github.com/tensorflow/tensorflow/files/7825446/lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite_android_nnapi_logcat.txt)\r\n\r\n", "comments": ["@suyash-narain ,\r\nCan you please confirm the tensorflow version you are using.Thanks", "Hi @tilakrayal ,\r\nThe tensorflow version being used is \"2.1.0\"", "Hi @suhrid-s \r\n\r\nThe 2.1.0 version was already more than 2 years old (it was released back in Jan 2020). \r\nCould you try to upgrade to the newest version and see if the problem still exists?", "if the benchmark_model is downloaded recently, it should be Tensorflow 2.8 or later.", "@suyash-narain ,\r\nAs mentioned can you please try to execute the code in latest v2.7 and let us know if you are facing same issue.Thanks!", "what is the best method to upgrade tensorflow on android 11? I am using the preinstalled version 2.1 on the same.\r\nfrom what I know, in android 11, the benchmark_model should have same tf runtime as that installed on the android, right? or does the tf runtime of the binary or app doesn't matter?\r\n", "@suyash-narain if you download a recent nightly build of benchmark_model, you are using post-2.8 tflite.\r\n1. there is not tf runtime in Android. Yes, there is a tflite shared library in Android, but current benchmark_model doesn't use it.\r\n2. currently, tflite benchmark_model use statically linked tflite interpreter/runtime.\r\n\r\nSo, get benchmark_model nightly from https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary, then you can use newer tflite. ", "@suyash-narain ,\r\nPlease take a look at this [link](https://www.tensorflow.org/lite/guide/build_android) to update to latest stable version .Thanks", "I tried the same int8 and float16 models with the latest nightly benchmark_model android_arch64 for android 11, and i used the following commands for nnapi-cpu, and i get the following results:\r\n\r\n1. lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite\r\n> $ ./benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [nnapi-reference]\r\nNNAPI accelerators available: [nnapi-reference]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_int8_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nNNAPI delegate created.\r\nINFO: Replacing 141 node(s) with delegate (TfLiteNnapiDelegate) node, yielding 15 partitions.\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 8 delegate kernels.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nThe input model file size (MB): 2.89484\r\nInitialized session in 59.758ms.\r\n\r\n\r\n2.  lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite\r\n> $ ./benchmark_model --graph=lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite --use_nnapi=1 --nnapi_accelerator_name=nnapi-reference\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [nnapi-reference]\r\nNNAPI accelerators available: [nnapi-reference]\r\nLoaded model lite-model_movenet_singlepose_lightning_tflite_float16_4.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nNNAPI delegate created.\r\nThough NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Replacing 269 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 13 partitions.\r\nThe input model file size (MB): 4.75851\r\nInitialized session in 109.192ms.\r\n\r\n\r\nfor INT8, I see the model is partially executed by NNAPI delegate, whereas the float16 model is not at all executed by the NNAPI delegate even though I am explicitly applying NNAPI CPU. Is there a reason? ", "I face the same issue if i run this model https://tfhub.dev/sayakpaul/lite-model/mobilenetv2-coco/fp16/1?lite-format=tflite, i see the same message as before for fp16 models:\r\n\r\n> Graph: [lite-model_mobilenetv2-coco_fp16_1.tflite]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [nnapi-reference]\r\nNNAPI accelerators available: [nnapi-reference]\r\nLoaded model lite-model_mobilenetv2-coco_fp16_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nNNAPI delegate created.\r\nThough NNAPI delegate is explicitly applied, the model graph will not be executed by the delegate.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Replacing 211 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 43 partitions.\r\nThe input model file size (MB): 4.2551\r\nInitialized session in 133.522ms.\r\n\r\nis it the same for every fp16 model that it won't be supported on NNAPI? i don't see this issue with an old nightly benchmark model for android, which i downloaded on 1/28/2021, which i think used tf 2.4.1\r\n\r\nI even used the --disable_nnapi_cpu=0 argument, and yet still got the same result with nightly", "i will create a new ticket for fp16 issue https://github.com/tensorflow/tensorflow/issues/53758#issue-1102815789 and close this one"]}, {"number": 53681, "title": "[TF:TRT] Fix oss build", "body": "Directly referring to `//third_party/tensorrt/plugins/...` in the OSS build does not work as the archive must be downloaded. So we should refer to the `@tensorrt_oss_archive//` repository to find these targets. A previous commit attempted to add an option `use_efficient_nms_plugin` for turning off/on the use of a plugin in `@tensorrt_oss_archive`, but made the mistake above. This change fixes the issue. It moves the `config_setting` to the TFTRT `BUILD` file. It also updates the `tensorrt_cc_test` test which uses the plugin (a change made when we first introduced TRT OSS archive). Tested with and without `--define=use_efficient_nms_plugin=1`.", "comments": ["@bixia1  for review when available", "So, the crux of the problem seemed to be we needed to make a conditional with conjunction condition. You can't nest `select` statements, so I used what  was recommended in [the bazel docs](https://docs.bazel.build/versions/main/configurable-attributes.html#combining-selects) for creating intermediate target. There is a section on \"AND chaining\", but `if_tensorrt` doesn't seem to be a normal config style setting, so I opted for the the first option. ", "I need to modify the BUILD file to make it work for inside google. In the process of merging this."]}, {"number": 53679, "title": "[TF:TRT] Remove redundant NodeDef attr helpers", "body": "Removes a class in `convert_nodes.cc` that is supposed to help with parsing NodeDef attributes. However, the class uses `TF_CHECK`-style error checking and can potentially cause a crash during conversion (although unlikely). This change replaces uses of the class with uses of helper functions from TF's NodeDef utilities.", "comments": ["Depends on https://github.com/tensorflow/tensorflow/pull/53678", "@bixia1 for review when available", "I should rebase?", "Yes, please rebase. There is conflict.", "updated"]}, {"number": 53678, "title": "[TF:TRT] Remove dead conversion helpers", "body": "This change removes a set of helper functions in convert_nodes.cc which are no longer used.", "comments": []}, {"number": 53677, "title": "Update c_api.h", "body": "Fixes # https://github.com/tensorflow/tensorflow/issues/53628", "comments": []}, {"number": 53676, "title": "[2.8 backport] Resolve bug with re-saving Estimator-exported SavedModel", "body": "# Change description\r\nSort signature inputs by input key instead of tensor name when loading a TF1 model in TF2.\r\n\r\nSignatures are saved using nest.flatten, which sorts the keys alphabetically for determinism. This can cause a mismatch between the lifted signature function's arg_keywords and structured_input_signature, resulting in users not being able to re-save the signature.\r\n\r\nDetailed example below.\r\n\r\nSay there is a SavedModel signature with the inputs:\r\n```\r\n  inputs['a'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: input_2:0\r\n    inputs['b'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 2)\r\n        name: input_1:0\r\n```\r\n\r\nWhen this signature is loaded using the TF2 API, it becomes a concrete function with arg keywords = `['b', 'a']` (sorted by tensor name)\r\n\r\nThe `structured_input_signature` becomes `([], {'b': spec_for_b, 'a': spec_for_a})`\r\n\r\nWhen this function is exported as a signature in a new SavedModel, the structured inputs are flattened and zipped with the arg keywords. However, the flattened `structured_input_signature` is `[spec_for_a, spec_for_b]`, since `nest.flatten` sorts the keys for determinism. This results in the wrong spec being mapped to the wrong arguments.\r\n\r\nPiperOrigin-RevId: 417889747\r\nChange-Id: Icdce72a43033027c18f5530068f2d6fd47e8e32d", "comments": []}, {"number": 53675, "title": "[2.7 backport] Resolve bug with re-saving Estimator-exported SavedModel", "body": "# Change description\r\nSort signature inputs by input key instead of tensor name when loading a TF1 model in TF2.\r\n\r\nSignatures are saved using nest.flatten, which sorts the keys alphabetically for determinism. This can cause a mismatch between the lifted signature function's arg_keywords and structured_input_signature, resulting in users not being able to re-save the signature.\r\n\r\nDetailed example below.\r\n\r\nSay there is a SavedModel signature with the inputs:\r\n```\r\n  inputs['a'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: input_2:0\r\n    inputs['b'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 2)\r\n        name: input_1:0\r\n```\r\n\r\nWhen this signature is loaded using the TF2 API, it becomes a concrete function with arg keywords = `['b', 'a']` (sorted by tensor name)\r\n\r\nThe `structured_input_signature` becomes `([], {'b': spec_for_b, 'a': spec_for_a})`\r\n\r\nWhen this function is exported as a signature in a new SavedModel, the structured inputs are flattened and zipped with the arg keywords. However, the flattened `structured_input_signature` is `[spec_for_a, spec_for_b]`, since `nest.flatten` sorts the keys for determinism. This results in the wrong spec being mapped to the wrong arguments.\r\n\r\nPiperOrigin-RevId: 417889747\r\nChange-Id: Icdce72a43033027c18f5530068f2d6fd47e8e32d", "comments": []}, {"number": 53674, "title": "[2.6 backport] Resolve bug with re-saving Estimator-exported SavedModel", "body": "# Change description\r\nSort signature inputs by input key instead of tensor name when loading a TF1 model in TF2.\r\n\r\nSignatures are saved using nest.flatten, which sorts the keys alphabetically for determinism. This can cause a mismatch between the lifted signature function's arg_keywords and structured_input_signature, resulting in users not being able to re-save the signature.\r\n\r\nDetailed example below.\r\n\r\nSay there is a SavedModel signature with the inputs:\r\n```\r\n  inputs['a'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: input_2:0\r\n    inputs['b'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 2)\r\n        name: input_1:0\r\n```\r\n\r\nWhen this signature is loaded using the TF2 API, it becomes a concrete function with arg keywords = `['b', 'a']` (sorted by tensor name)\r\n\r\nThe `structured_input_signature` becomes `([], {'b': spec_for_b, 'a': spec_for_a})`\r\n\r\nWhen this function is exported as a signature in a new SavedModel, the structured inputs are flattened and zipped with the arg keywords. However, the flattened `structured_input_signature` is `[spec_for_a, spec_for_b]`, since `nest.flatten` sorts the keys for determinism. This results in the wrong spec being mapped to the wrong arguments.\r\n\r\nPiperOrigin-RevId: 417889747\r\nChange-Id: Icdce72a43033027c18f5530068f2d6fd47e8e32d", "comments": []}, {"number": 53673, "title": "[2.5 backport] Resolve bug with re-saving Estimator-exported SavedModel", "body": "# Change description\r\nSort signature inputs by input key instead of tensor name when loading a TF1 model in TF2.\r\n\r\nSignatures are saved using nest.flatten, which sorts the keys alphabetically for determinism. This can cause a mismatch between the lifted signature function's arg_keywords and structured_input_signature, resulting in users not being able to re-save the signature.\r\n\r\nDetailed example below.\r\n\r\nSay there is a SavedModel signature with the inputs:\r\n```\r\n  inputs['a'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: input_2:0\r\n    inputs['b'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 2)\r\n        name: input_1:0\r\n```\r\n\r\nWhen this signature is loaded using the TF2 API, it becomes a concrete function with arg keywords = `['b', 'a']` (sorted by tensor name)\r\n\r\nThe `structured_input_signature` becomes `([], {'b': spec_for_b, 'a': spec_for_a})`\r\n\r\nWhen this function is exported as a signature in a new SavedModel, the structured inputs are flattened and zipped with the arg keywords. However, the flattened `structured_input_signature` is `[spec_for_a, spec_for_b]`, since `nest.flatten` sorts the keys for determinism. This results in the wrong spec being mapped to the wrong arguments.\r\n\r\nPiperOrigin-RevId: 417889747\r\nChange-Id: Icdce72a43033027c18f5530068f2d6fd47e8e32d", "comments": []}, {"number": 53672, "title": "Higher validation and test loss and lower accuracy using tf.data.dataset with tfrecords than using numpy", "body": "**System information**\r\n- OS Platform and Distribution : Kaggle and Colab gpu/tpu\r\n- TensorFlow version (use command below): 2.4 on Kaggle, 2.7 on Colab\r\n- GPU model and memory:tesla p100, colab tpu, kaggle tpu\r\n\r\nHi, I'm training and testing an LSTM network using kaggle notebooks. \r\nTo improve time performance I have converted my numpy dataset to TFRecords, but the results are very differents respects results obtained with numpy dataset. I noticed this behavior either on GPU and TPU on Kaggle and Google Colab. \r\nI want to specify that the hyperparametrs and the dataset order is the same with numpy method and td.data.\r\nI repeated the tests on differents file of the dataset many times but each time the validation and test accuracy are lower  and validation and test loss are higher using tf.data rsepect to using numpy.\r\n\r\nWith tf.data:\r\n![accuracy](https://user-images.githubusercontent.com/48318112/148437400-86932751-279e-4ae3-96de-2a9c7c3589e3.png)\r\n![loss](https://user-images.githubusercontent.com/48318112/148437401-a0529b6b-71f7-4d56-8604-86914565cc2f.png)\r\n\r\nWith numpy dataset:\r\n![accuracy](https://user-images.githubusercontent.com/48318112/148437518-2eb912f8-5f37-496a-a0e6-a393b9b64d2a.png)\r\n![loss](https://user-images.githubusercontent.com/48318112/148437522-f79ecc38-6681-42df-91ab-57fb00c0afdc.png)\r\n\r\nExecution with tf.data:\r\n```\r\nEpoch 1/18\r\n11929/11929 [==============================] - 323s 26ms/step - loss: 0.1282 - binary_accuracy: 0.9558 - auc: 0.8314 - precision: 0.7457 - recall: 0.6855 - val_loss: 1.6054 - val_binary_accuracy: 0.6283 - val_auc: 0.6912 - val_precision: 0.9549 - val_recall: 0.0453\r\n2022-01-06 15:14:59.141498: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 146116, Output num: 3\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1641482099.138121196\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 146116, Output num: 3\",\"grpc_status\":3}\r\nEpoch 2/18\r\n11929/11929 [==============================] - 303s 25ms/step - loss: 0.0450 - binary_accuracy: 0.9862 - auc: 0.8506 - precision: 0.8013 - recall: 0.7681 - val_loss: 1.4894 - val_binary_accuracy: 0.7084 - val_auc: 0.8023 - val_precision: 0.9523 - val_recall: 0.2626\r\nEpoch 3/18\r\n11929/11929 [==============================] - 307s 26ms/step - loss: 0.0294 - binary_accuracy: 0.9915 - auc: 0.8516 - precision: 0.8214 - recall: 0.8078 - val_loss: 1.4176 - val_binary_accuracy: 0.7219 - val_auc: 0.8154 - val_precision: 0.9737 - val_recall: 0.2922\r\nEpoch 4/18\r\n11929/11929 [==============================] - 308s 26ms/step - loss: 0.0186 - binary_accuracy: 0.9947 - auc: 0.8534 - precision: 0.8326 - recall: 0.8298 - val_loss: 1.2668 - val_binary_accuracy: 0.7506 - val_auc: 0.8195 - val_precision: 0.9890 - val_recall: 0.3619\r\nEpoch 5/18\r\n11929/11929 [==============================] - 306s 26ms/step - loss: 0.0142 - binary_accuracy: 0.9961 - auc: 0.8542 - precision: 0.8387 - recall: 0.8378 - val_loss: 1.2077 - val_binary_accuracy: 0.7710 - val_auc: 0.8313 - val_precision: 0.9954 - val_recall: 0.4125\r\nEpoch 6/18\r\n11929/11929 [==============================] - 304s 25ms/step - loss: 0.0126 - binary_accuracy: 0.9965 - auc: 0.8548 - precision: 0.8432 - recall: 0.8397 - val_loss: 1.1289 - val_binary_accuracy: 0.7836 - val_auc: 0.8471 - val_precision: 0.9974 - val_recall: 0.4442\r\nEpoch 7/18\r\n11929/11929 [==============================] - 303s 25ms/step - loss: 0.0094 - binary_accuracy: 0.9975 - auc: 0.8555 - precision: 0.8470 - recall: 0.8455 - val_loss: 0.9592 - val_binary_accuracy: 0.8196 - val_auc: 0.8885 - val_precision: 0.9983 - val_recall: 0.5365\r\nEpoch 8/18\r\n11929/11929 [==============================] - 304s 26ms/step - loss: 0.0085 - binary_accuracy: 0.9978 - auc: 0.8559 - precision: 0.8478 - recall: 0.8463 - val_loss: 0.7648 - val_binary_accuracy: 0.8320 - val_auc: 0.9281 - val_precision: 0.9983 - val_recall: 0.5686\r\nEpoch 9/18\r\n11929/11929 [==============================] - 305s 26ms/step - loss: 0.0070 - binary_accuracy: 0.9982 - auc: 0.8561 - precision: 0.8498 - recall: 0.8481 - val_loss: 0.6531 - val_binary_accuracy: 0.8376 - val_auc: 0.9435 - val_precision: 0.9986 - val_recall: 0.5828\r\nEpoch 10/18\r\n11929/11929 [==============================] - 304s 25ms/step - loss: 0.0062 - binary_accuracy: 0.9984 - auc: 0.8565 - precision: 0.8506 - recall: 0.8491 - val_loss: 0.6175 - val_binary_accuracy: 0.8421 - val_auc: 0.9462 - val_precision: 0.9987 - val_recall: 0.5943\r\nEpoch 11/18\r\n11929/11929 [==============================] - 307s 26ms/step - loss: 0.0048 - binary_accuracy: 0.9987 - auc: 0.8567 - precision: 0.8514 - recall: 0.8501 - val_loss: 0.5576 - val_binary_accuracy: 0.8626 - val_auc: 0.9529 - val_precision: 0.9990 - val_recall: 0.6470\r\nEpoch 12/18\r\n11929/11929 [==============================] - 309s 26ms/step - loss: 0.0044 - binary_accuracy: 0.9988 - auc: 0.8568 - precision: 0.8520 - recall: 0.8514 - val_loss: 0.4787 - val_binary_accuracy: 0.8744 - val_auc: 0.9641 - val_precision: 0.9992 - val_recall: 0.6773\r\nEpoch 13/18\r\n11929/11929 [==============================] - 308s 26ms/step - loss: 0.0040 - binary_accuracy: 0.9989 - auc: 0.8568 - precision: 0.8526 - recall: 0.8521 - val_loss: 0.4774 - val_binary_accuracy: 0.8818 - val_auc: 0.9635 - val_precision: 0.9991 - val_recall: 0.6965\r\nEpoch 14/18\r\n11929/11929 [==============================] - 306s 26ms/step - loss: 0.0036 - binary_accuracy: 0.9990 - auc: 0.8568 - precision: 0.8528 - recall: 0.8522 - val_loss: 0.4594 - val_binary_accuracy: 0.8837 - val_auc: 0.9663 - val_precision: 0.9992 - val_recall: 0.7011\r\nEpoch 15/18\r\n11929/11929 [==============================] - 307s 26ms/step - loss: 0.0035 - binary_accuracy: 0.9990 - auc: 0.8568 - precision: 0.8529 - recall: 0.8517 - val_loss: 0.4588 - val_binary_accuracy: 0.8946 - val_auc: 0.9615 - val_precision: 0.9992 - val_recall: 0.7294\r\nEpoch 16/18\r\n11929/11929 [==============================] - 305s 26ms/step - loss: 0.0031 - binary_accuracy: 0.9991 - auc: 0.8568 - precision: 0.8534 - recall: 0.8529 - val_loss: 0.3980 - val_binary_accuracy: 0.9048 - val_auc: 0.9642 - val_precision: 0.9994 - val_recall: 0.7553\r\nEpoch 17/18\r\n11929/11929 [==============================] - 304s 26ms/step - loss: 0.0030 - binary_accuracy: 0.9991 - auc: 0.8569 - precision: 0.8532 - recall: 0.8528 - val_loss: 0.3670 - val_binary_accuracy: 0.9098 - val_auc: 0.9685 - val_precision: 0.9993 - val_recall: 0.7683\r\nEpoch 18/18\r\n11929/11929 [==============================] - 306s 26ms/step - loss: 0.0028 - binary_accuracy: 0.9992 - auc: 0.8569 - precision: 0.8533 - recall: 0.8526 - val_loss: 0.3948 - val_binary_accuracy: 0.9091 - val_auc: 0.9655 - val_precision: 0.9992 - val_recall: 0.7667\r\n3497/3497 [==============================] - 83s 23ms/step - loss: 0.4007 - binary_accuracy: 0.9079 - auc: 0.9650 - precision: 0.9993 - recall: 0.7645\r\n2022-01-06 16:43:10.007313: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1641487390.006754880\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3\",\"grpc_status\":3}\r\n```\r\nOnly with Kaggle TPU i have the following error as you can see just after first epoch validation and during testing, but the training run correctly and the program don't crash:\r\n\r\n```\r\nW ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1641487390.006754880\",\"description\":\"Error received from peer ipv4:10.0.0.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 2652130, Output num: 3\",\"grpc_status\":3}\r\n```\r\nThis problem occour also without this error on Kaggle GPU and Colab, so this error is specific on platform TPU Kaggle and is not related with the problem.\r\n\r\nIn the following link you can find my code with tfrecord write and read and all the process to get the dataset from tfrecord, model creation, fitting and testing:\r\n[link code](https://colab.research.google.com/drive/1goQZL3xRU2vkBos04PZkWL2o1qHv8qul?usp=sharing)\r\n\r\nSeems like to with tf.data the training start with lower validation accuracy value and increase more slowly compared to numpy dataset, so with a lot of epochs maybe also with tf.data can reach numpy accuracy level, is this comportament normal using tf.data or not?\r\n", "comments": ["@marzdeveloper I tried to replicate this issue on colab using TF [v2.7.0](https://colab.research.google.com/gist/sushreebarsa/9dbe6bd35372f742d1085de59c632df3/tf.ipynb#scrollTo=kbOvr83M16wU) , [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/761f60c0874a16364fa3d5538cfd8658/tf.ipynb#scrollTo=kbOvr83M16wU) and faced a different error as `NameError: name 'GCS_DS_PATH' is not defined` ,could you please have a look at the attached gists ? Please let us know if I am missing something to reproduce this issue. Thank you!", "@sushreebarsa Now i published a part of dataset, in the attached code now you can find the GCS_DS_PATH definition which contains tfrecord files. While in this [link ](https://drive.google.com/drive/folders/1EKCf4YH1fau5Zpr78dCGNH6VmT5FnY5v?usp=sharing) you can find the same part in numpy, before the tfrecord transformation. So if you want run with tfrecord dataset just run the code in \"Main with TFRecords\" section. To run with numpy dataset please run the code in \"Main with Numpy\" but first download the dataset in this comment and obviously change the value of \"path\" variable in the code. If you want check the validity of conversion from numpy to TFRecords please run the code in \"From numpy to TFRecords\" section. If you need anything else let me know. Thank you!", "Hi @marzdeveloper !\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Hi @mohantym !\r\nThank you, but shouldn't this be a tensorflow.data problem?", "@marzdeveloper !Sorry for the late response!  Model should provide same accuracy irrespective of Dataset type. It has been seen earlier that  accuracy ,loss and performance differs when  numpy arrays are used instead of tensors. So I think it should be more appropriate for keras repo. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53672\">No</a>\n"]}, {"number": 53670, "title": "[PluggableDevice] Add DEVICE_DEFAULT registration to MakeWeakResourceHandle", "body": null, "comments": ["Hi,\r\n\r\nCould someone review this PR? This is something that we need for the release of our pluggable device plugin.\r\n\r\nThank you!"]}, {"number": 53669, "title": "[PluggableDevice] Link `pywrap_tensorflow_internal` with `kernels_experimental`", "body": "Because Windows python builds don't come with `tensorflow_framework.dll`, they don't currently expose any of the experimental kernel functions which are necessary to support resource operations in pluggable devices.", "comments": ["Reaching out to you @penpornk for help finding the right reviews going for @PatriceVignola. Pat is an engineer in my team actively working on TF2 pluggable device for DirectML. He has been proposing a few PRs to move us along with our plan to support TF2 on Windows through the pluggable device interface e.g. other than this PR there is also #53484 for example. @jstoecker", "@penpornk Would you please help look into this PR or point us to someone who could help review and approve this change? It is currently blocking our plug-in development on Windows build. Thanks! @saxenasaurabh ", "> This PR seems to break our [Ubuntu CPU](https://source.cloud.google.com/results/invocations/652958ff-7e74-43d9-9be4-da7e296356a0), [Linux GPU](https://source.cloud.google.com/results/invocations/16a006fa-5359-4759-86b5-31180d378a01), and [MacOS CPU](https://source.cloud.google.com/results/invocations/e5976f9c-17bd-4c7e-83a4-9738d8e678cf) builds. Can we try adding the dependency just for Windows? Thank you!\r\n\r\nDone!", "@PatriceVignola Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned I fixed the buildifier warnings."]}, {"number": 53668, "title": "Compiling tensorflow/core/kernels/bincount_op.cc failed: (Exit 1): clang failed: error executing command", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.7\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 4.2.2\r\n- GCC/Compiler version (if compiling from source): 10.1.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nTry to build the libtensorflowlite.so with select-tf-ops. I add `\"//tensorflow/lite/delegates/flex:delegate\"` to tensorflow/lite/BUILD. Then the Compilation failed.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n1. git clone https://github.com/tensorflow/tensorflow\r\n2. cd tensorflow && ./configure\r\n3. vim tensorflow/lite/BUILD && add \"//tensorflow/lite/delegates/flex:delegate\" to the deps of tensorflowlite\r\n4. bazel build -c opt --cxxopt=--std=c++11 --fat_apk_cpu=arm64-v8a --config=android_arm64 //tensorflow/lite:libtensorflowlite.so --verbose_failures\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nFull log:\r\n\r\n> INFO: Options provided by the client:\r\n>   Inherited 'common' options: --isatty=1 --terminal_columns=146\r\n> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:\r\n>   Inherited 'common' options: --experimental_repo_remote_exec\r\n> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:\r\n>   'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\n> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.tf_configure.bazelrc:\r\n>   'build' options: --action_env PYTHON_BIN_PATH=/root/miniconda2/envs/py36/bin/python3 --action_env PYTHON_LIB_PATH=/root/miniconda2/envs/py36/lib/python3.6/site-packages --python_path=/root/miniconda2/envs/py36/bin/python3 --define=PREFIX=/data/offline/deps/prefix --action_env ANDROID_NDK_HOME=/data/offline/deps/android-ndk-r21e --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.2 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/opt/android-sdk\r\n> INFO: Reading rc options for 'build' from /data/offline/deps/tensorflow/.bazelrc:\r\n>   'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\n> INFO: Found applicable config definition build:short_logs in file /data/offline/deps/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n> INFO: Found applicable config definition build:v2 in file /data/offline/deps/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n> INFO: Found applicable config definition build:android_arm64 in file /data/offline/deps/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n> INFO: Found applicable config definition build:android in file /data/offline/deps/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false\r\n> INFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (76 packages loaded, 4130 targets configured).\r\n> INFO: Found 1 target...\r\n> ERROR: /data/offline/deps/tensorflow/tensorflow/core/kernels/BUILD:6848:11: Compiling tensorflow/core/kernels/bincount_op.cc failed: (Exit 1): clang failed: error executing command \r\n>   (cd /root/.cache/bazel/_bazel_root/c8bef3cfd9e610c81b071433025995b0/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     ANDROID_BUILD_TOOLS_VERSION=30.0.2 \\\r\n>     ANDROID_NDK_API_LEVEL=21 \\\r\n>     ANDROID_NDK_HOME=/data/offline/deps/android-ndk-r21e \\\r\n>     ANDROID_SDK_API_LEVEL=30 \\\r\n>     ANDROID_SDK_HOME=/opt/android-sdk \\\r\n>     LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64 \\\r\n>     PATH=/data/offline/deps/cmdline-tools/tools:/data/offline/deps/cmdline-tools/tools/bin:/data/offline/deps/cmdline-tools/platform-tools:/root/miniconda2/envs/py36/bin:/root/miniconda2/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/usr/local/cuda-11.0/bin:/usr/local/cuda/bin:/root/.ft:/root/.local/bin:/usr/local/bin:/root/bin:/usr/local/git/bin \\\r\n>     PWD=/proc/self/cwd \\\r\n>     PYTHON_BIN_PATH=/root/miniconda2/envs/py36/bin/python3 \\\r\n>     PYTHON_LIB_PATH=/root/miniconda2/envs/py36/lib/python3.6/site-packages \\\r\n>     TF2_BEHAVIOR=1 \\\r\n>   external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=21' -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig '-Werror=return-type' '-Werror=int-to-pointer-cast' '-Werror=pointer-to-int-cast' '-Werror=implicit-function-declaration' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/bincount_op.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/bincount_op.pic.o' -fPIC '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DSUPPORT_SELECTIVE_REGISTRATION -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/gif -iquote bazel-out/arm64-v8a-opt/bin/external/gif -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/bin/external/nsync -iquote external/libjpeg_turbo -iquote bazel-out/arm64-v8a-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/bin/external/double_conversion -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -iquote external/png -iquote bazel-out/arm64-v8a-opt/bin/external/png -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/bin/external/highwayhash -iquote external/icu -iquote bazel-out/arm64-v8a-opt/bin/external/icu -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/arm64-v8a-opt/bin/external/gemmlowp -isystem external/gif -isystem bazel-out/arm64-v8a-opt/bin/external/gif -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/png -isystem bazel-out/arm64-v8a-opt/bin/external/png -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/icu/icu4c/source/common -isystem bazel-out/arm64-v8a-opt/bin/external/icu/icu4c/source/common -w '--std=c++11' '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/bincount_op.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/bincount_op.pic.o)\r\n> Execution platform: @local_execution_config_platform//:platform\r\n> Target //tensorflow/lite:libtensorflowlite.so failed to build\r\n> INFO: Elapsed time: 942.540s, Critical Path: 321.94s\r\n> INFO: 1130 processes: 48 internal, 1082 local.\r\n> FAILED: Build did NOT complete successfully\r\n> \r\n> ", "comments": ["Hi @Saduf2019 ! Could you please look at this issue?", "Change \"//tensorflow/lite/kernels:builtin_ops_all_linked\", to \"//tensorflow/lite/kernels:builtin_ops\", solve this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53668\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53668\">No</a>\n"]}]