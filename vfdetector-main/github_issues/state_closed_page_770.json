[{"number": 30455, "title": "loop problem in defining the network", "body": "\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state:**\r\n\r\nEach data A has the shape of [height, width, channel] and is saved in TFrecord file.\r\nI use an iterator to read batches from my TFrecord file. The input of my network is a 4-dimensional tensor A_batch with shape [batch, height, width, channel]. \r\n\r\nLet Network() represents the network i define and Output represents the output of the network:\r\nOutput = Network(A_batch)\r\n\r\nEach data A has the shape of [height, width, channel]. I want to have different trainable ops on each channel like (assume channel=3):\r\n```\r\n\r\n    ops_0 on A[ : , : , 0]\r\n    ops_1 on A[ : , : , 1]\r\n    ops_2 on A[ : , : , 2]\r\n\r\n```\r\n\r\nHowever, the input A_batch with the shape [batch, height, width, channel]. It means i need to loop on batch (assume channel=3):\r\n\r\n```\r\nfor m in range(batch)\r\n    ops_0 on A_batch [ m , : , :\uff0c0 ]\r\n    ops_1 on A_batch [ m , : , :\uff0c1 ]\r\n    ops_2 on A_batch [ m , : , :\uff0c2 ]\r\n\r\n```\r\n**Note that after trainable ops, original data becomes trainable variable.**\r\n\r\n**Question 1:**\r\nIn defining the network, is it allowed to separate batch data to perform different trainable ops in loop? \r\n\r\n**Question 2:**\r\nIn defining the network, is it allowed to assign trainable variable to a tensor to form another trainable variable? \r\n\r\nfor example, let A_batch_operated denotes the trainable variable after performing trainable ops_0, ops_1,and ops_2 on batch data A_batch :\r\n\r\n```\r\nfor m in range(batch)\r\n    A_batch_operated [ m , : , :\uff0c0 ]= ops_0 (A_batch [ m , : , :\uff0c0 ] )\r\n    A_batch_operated [ m , : , :\uff0c1 ]= ops_1 (A_batch [ m , : , :\uff0c1 ] )\r\n    A_batch_operated [ m , : , :\uff0c2 ]= ops_2 (A_batch [ m , : , :\uff0c2 ] )\r\n\r\n```\r\n**Question 3:**\r\nCan i add a trainable ops on the new trainable variable on A_batch_operated?  like:\r\n\r\nOutput= ops_3 (A_batch_operated)\r\n\r\n\r\nThank you\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nSome researchers of image/video processing\r\nAny Other info.\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 30454, "title": "curses is not supported on this machine (please install/reinstall curses for an optimal experience)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\r\n", "Spam issue"]}, {"number": 30453, "title": "[TF 2.0] categorical_column_with_vocabulary_list not usable in custom training loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nOutside of `fit`, e.g in a custom training loop, `categorical_column_with_vocabulary_list` results in an error. I have provided a modified version of [Classifying Structured data](https://www.tensorflow.org/beta/tutorials/keras/feature_columns) which demonstrates this.\r\n\r\nThe error is `ValueError: Column dtype and SparseTensors dtype must be compatible. key: thal, column dtype: <dtype: 'string'>, tensor dtype: <dtype: 'int32'>`\r\n\r\n**Describe the expected behavior**\r\n\r\nCode runs without causing an error\r\n\r\n**Code to reproduce the issue**\r\n\r\nIt should be directly copy-paste-able \r\n\r\n```\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n\r\ndef df_to_dataset(df, shuffle=True, batch_size=32):\r\n    df = df.copy()\r\n    labels = df.pop('target')\r\n    ds = tf.data.Dataset.from_tensor_slices(\r\n        (dict(df), labels)\r\n    )\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(df))\r\n    ds = ds.batch(batch_size)\r\n    return ds\r\n\r\n\r\ndef generate_features():\r\n    feature_columns = []\r\n    feature_layer_inputs = {}\r\n\r\n\r\n    thal = tf.feature_column.categorical_column_with_vocabulary_list(\r\n          'thal', ['fixed', 'normal', 'reversible'])\r\n    thal_one_hot = tf.feature_column.indicator_column(thal)\r\n    feature_columns.append(thal_one_hot)\r\n    feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n\r\n    return feature_columns, feature_layer_inputs\r\n\r\n\r\ndef create_model(feature_columns, feature_layer_inputs):\r\n    input_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n    inputs = input_layer(feature_layer_inputs)\r\n\r\n    l1 = tf.keras.layers.Dense(128, activation='relu')(inputs)\r\n    l2 = tf.keras.layers.Dense(128, activation='relu')(l1)\r\n\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(l2)\r\n\r\n    model = tf.keras.Model(\r\n        inputs=[v for v in feature_layer_inputs.values()],\r\n        outputs=[output]\r\n    )\r\n    return model\r\n\r\n\r\ndef make_loss(loss_object):\r\n    def loss(model, x, y):\r\n        y_pred = model(x)\r\n        return loss_object(y_true=y, y_pred=y_pred)\r\n    return loss\r\n\r\n\r\ndef grad(model, inputs, targets, loss):\r\n    with tf.GradientTape() as tape:\r\n        loss_value = loss(model, inputs, targets)\r\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n\r\n\r\ndef fit(epochs, train_ds, model, optimizer, loss_obj):\r\n    loss = make_loss(loss_obj)\r\n    for epoch in range(epochs):\r\n        for i, (x, y) in enumerate(train_ds):\r\n            loss_values, grad_values = grad(model, x, y, loss)\r\n            optimizer.apply_gradients(zip(grad_values, model.trainable_variables))\r\n\r\n\r\nif __name__ == '__main__':\r\n    URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\n    df = pd.read_csv(URL)\r\n    CUSTOM_TRAINING = True\r\n\r\n    train, test = train_test_split(df, test_size=0.2)\r\n    train, val = train_test_split(train, test_size=0.2)\r\n\r\n    # hardcoded stuff\r\n    batch_size = 32\r\n    train_ds = df_to_dataset(train, batch_size=batch_size)\r\n\r\n    # Create model and features\r\n    feature_columns, feature_layer_inputs = generate_features()\r\n    model = create_model(feature_columns, feature_layer_inputs)\r\n\r\n    if CUSTOM_TRAINING:\r\n        print('Trying custom training')\r\n        bce = tf.keras.losses.BinaryCrossentropy()\r\n        adam = tf.keras.optimizers.Adam()\r\n        fit(epochs=5, train_ds=train_ds,\r\n            model=model, optimizer=adam, loss_obj=bce)\r\n    else:\r\n        print('Using pre-defined fit')\r\n        model.compile(optimizer='adam',\r\n                      loss='binary_crossentropy',\r\n                      metrics=['accuracy'])\r\n        model.fit(train_ds, epochs=5)\r\n```\r\n\r\nIf you flip the `CUSTOM_TRAINING` variable between `True` and `False` (line 72) you'll see what I mean. \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n1) The complete (relevant) stacktrace is \r\n\r\n```\r\n  File \"/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py\", line 1758, in <module>\r\n    main()\r\n  File \"/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py\", line 1752, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py\", line 1147, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py\", line 90, in <module>\r\n    model=model, optimizer=adam, loss_obj=bce)\r\n  File \"/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py\", line 65, in fit\r\n    loss_values, grad_values = grad(model, x, y, loss)\r\n  File \"/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py\", line 57, in grad\r\n    loss_value = loss(model, inputs, targets)\r\n  File \"/Users/ian.quah/PycharmProjects/tf2/datasets/issues.py\", line 50, in loss\r\n    y_pred = model(x)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 712, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 753, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 895, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 712, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 474, in call\r\n    self._state_manager)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 4299, in get_dense_tensor\r\n    return transformation_cache.get(self, state_manager)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 2562, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 4238, in transform_feature\r\n    transformation_cache, state_manager)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 3714, in get_sparse_tensors\r\n    transformation_cache.get(self, state_manager), None)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 2562, in get\r\n    transformed = column.transform_feature(self, state_manager)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 3692, in transform_feature\r\n    return self._transform_input_tensor(input_tensor)\r\n  File \"/anaconda3/envs/mlpl/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 3668, in _transform_input_tensor\r\n    self.key, self.dtype, input_tensor.dtype))\r\nValueError: Column dtype and SparseTensors dtype must be compatible. key: thal, column dtype: <dtype: 'string'>, tensor dtype: <dtype: 'int32'>\r\n```\r\n\r\n2)  Placing a debugger on line 50 leads me to `feature_column_v2.py` specifically `_transform_input_tensor`\r\n\r\nThe `input_tensor` arg to `_transform_input_tensor` is \r\n\r\n```\r\nSparseTensor(indices=tf.Tensor(\r\n[[ 0  0]\r\n [ 1  0]\r\n [ 2  0]\r\n [ 3  0]\r\n [ 4  0]\r\n [ 5  0]\r\n [ 6  0]\r\n [ 7  0]\r\n [ 8  0]\r\n [ 9  0]\r\n [10  0]\r\n [11  0]\r\n [12  0]\r\n [13  0]\r\n [14  0]\r\n [15  0]\r\n [16  0]\r\n [17  0]\r\n [18  0]\r\n [19  0]\r\n [20  0]\r\n [21  0]\r\n [22  0]\r\n [23  0]\r\n [24  0]\r\n [25  0]\r\n [26  0]\r\n [27  0]\r\n [28  0]\r\n [29  0]\r\n [30  0]\r\n [31  0]], shape=(32, 2), dtype=int64), values=tf.Tensor(\r\n[49 34 58 46 59 47 55 58 41 68 62 51 61 46 39 48 37 41 51 59 51 70 60 57\r\n 54 60 52 44 65 49 44 59], shape=(32,), dtype=int32), dense_shape=tf.Tensor([32  1], shape=(2,), dtype=int64))\r\n```\r\n\r\nwhich seems strange? It's like it forgot that it had transformed those variables? ", "comments": ["Reproduced the error with TF Version 2.0.beta1", "Thanks! \r\n\r\n1) Does anyone have an intuition as to why this happens in a custom loop but not in `fit`?\r\n\r\n2) Does anyone have a quick fix? ", "Adding @rohan100jain who is the expert on feature columns.", "Is this still an open issue? ", "facing same issue.\r\n\r\nValueError: Column dtype and SparseTensors dtype must be compatible. key: opt_categories, column dtype: <dtype: 'int64'>, tensor dtype: <dtype: 'float32'>", "I am also facing the same issue. Any help...", "This got resolved for me. The datatype of the vocabulary list passed to the method, the data in the column of the record and the data type that we specify in the model have to be the same. Was able to find the issue, it got resolved.", "same issue with tensorflow 1.14\r\n    ValueError: Column dtype and SparseTensors dtype must be compatible. key: adt_sev_flag, column dtype: <dtype: 'int64'>, tensor dtype: <dtype: 'float64'>", "> This got resolved for me. The datatype of the vocabulary list passed to the method, the data in the column of the record and the data type that we specify in the model have to be the same. Was able to find the issue, it got resolved.\r\n\r\nhow did you solved it?", "The have you checked if the datatype of the data in the vocab list, the datatype of the input data and the data type of the feature_column declaration are same. In my case, the vocab list that I passed had infered to a different datatype and I passed it into the categorical_column_with_vocabulary_list layer incorrectly.", "I have tried this example in tf-nightly, which seems to have been resolved. It's probably a Keras issue instead of FeatureColumn. Can anyone help to verify?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30453\">No</a>\n"]}, {"number": 30452, "title": "Update kissfft to fix stdint.h issue on Windows", "body": "This fix tries to address the issue raised in #30447\r\nwhere kissfft compile error on Windows 10 + VS 2019.\r\nThis fix updates kissfft to the latest commit to fix the issue.\r\n\r\nThis fix fixes #30447.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["It looks like the new version is causing the following error:\r\n`tensorflow/lite/experimental/microfrontend/lib/window_util.c(39): error C2065: 'M_PI': undeclared identifier`\r\n\r\nCan you try adding a local WINDOW_PI definition in that file and changing the one place in the code to use it? That's what we had to do for this similar case:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.cc#L23", "@petewarden, undefined M_PI is unrelated to kissfft and should be fixed by now."]}, {"number": 30451, "title": "Add warning to dropout that it violates Google patent US9406017B2", "body": "\r\n## Description of issue (what needs changing):\r\n\r\nGoogle who to a large extent runs this project has patented dropout.  See https://patents.google.com/patent/US9406017B2/en\r\n\r\nThis patent is not listed as a pledged patent in Google's list of patents that are thus *not* covered by the [Google Open Patent Non-Assertion Pledge](https://www.google.com/patents/opnpledge/patents/)\r\n\r\nSince this could cause serious issues for users, it needs to be documented.\r\n", "comments": ["It is possible that the current dropout implementations have been designed to work around this patent.  If not, working around the patent should be considered.", "To the extent any Tensorflow contributor, including Google, holds any patents that may read on\r\ncontributions to Tensorflow, those patents are licensed as per the terms of the Apache 2 license in the LICENSE file"]}, {"number": 30450, "title": "tensorboard 1.14.0 has requirement setuptools", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Spam issue, nothing included, github user has no actual content"]}, {"number": 30449, "title": "Error during build on Windows ( SET PATH error)", "body": "**System information**\r\n- Windows 10\r\n- Compiled from source\r\n- TensorFlow version 1.14\r\n- Python version 3.6\r\n- Installed using virtualenv? No . pip? Yes:\r\n- Bazel version : 0.27.1\r\n- CUDA/cuDNN version: N/A ( built for CPU) \r\n- GPU model and memory: N/A ( CPU build) \r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nSo during the installation , everything goes rather well  , but it fails on what seems to be the last step . \r\n\r\n``````\r\nE:\\tf\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n``````\r\n Result : \r\n\r\n``````\r\nWARNING: E:/tf/tensorflow/tensorflow/python/BUILD:3469:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tf/tensorflow/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tf/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: E:/tf/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: E:/tf/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: E:/tf/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: E:/tf/tensorflow/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 2 targets configured).\r\nINFO: Found 1 target...\r\nERROR: E:/tf/tensorflow/tensorflow/lite/toco/BUILD:54:1: ProtoCompile tensorflow/lite/toco/model_flags_pb2.py failed (Exit -1073741795): protoc.exe failed: error executing command\r\n  cd C:/users/admin/_bazel_admin/meeqa4gn/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files\\Docker\\Docker\\Resources\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\PuTTY\\;C:\\Program Files\\nodejs\\;C:\\Program Files (x86)\\GtkSharp\\2.12\\bin;E:\\Program Files\\Golem\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\90\\Tools\\binn\\;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\130\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\140\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\140\\DTS\\Binn\\;C:\\mingw64\\bin;C:\\Program Files\\Microsoft SQL Server\\120\\Tools\\Binn\\;C:\\wamp64\\bin\\php\\php7.2.4;C:\\ProgramData\\ComposerSetup\\bin;C:\\Program Files (x86)\\Yarn\\bin\\;C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\admin\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\admin\\AppData\\Roaming\\npm;C:\\Program Files\\Heroku\\bin;C:\\Program Files\\Docker Toolbox;F:\\opencv\\bin;C:\\Program Files\\MPICH2\\bin;C:\\Program Files\\LAMMPS 64-bit 7Dec2018\\bin;C:\\Users\\admin\\AppData\\Roaming\\LINKS\\bin;C:\\Users\\admin\\AppData\\Roaming\\Composer\\vendor\\bin;C:\\Program Files\\Git\\bin;C:\\Users\\admin\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\admin\\AppData\\Local\\Yarn\\bin;\r\n    SET PYTHON_BIN_PATH=C:/Users/admin/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/admin/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  bazel-out/x64_windows-opt/bin/external/protobuf_archive/protoc.exe --python_out=bazel-out/x64_windows-py2-opt/bin -I. -I. -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/python -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/python tensorflow/lite/toco/model_flags.proto\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 38.214s, Critical Path: 7.35s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n``````\r\n\r\nI'm pretty sure the problem comes from the SET PATH= ... line , here's the output of it alone if i copy-paste it into the terminal : \r\n ``````\r\nC:\\msys64\\bin : The term 'C:\\msys64\\bin' is not recognized as the name of a cmdlet, function, script file, or operable\r\nprogram. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\r\nAt line:1 char:30\r\n+   SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\ProgramData\\DockerDeskt ...\r\n+                              ~~~~~~~~~~~~~\r\n    + CategoryInfo          : ObjectNotFound: (C:\\msys64\\bin:String) [], CommandNotFoundException\r\n    + FullyQualifiedErrorId : CommandNotFoundException\r\n\r\nC:\\ProgramData\\DockerDesktop\\version-bin : The term 'C:\\ProgramData\\DockerDesktop\\version-bin' is not recognized as\r\nthe name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was\r\nincluded, verify that the path is correct and try again.\r\nAt line:1 char:44\r\n+ ... sr\\bin;C:\\msys64\\bin;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Prog ...\r\n+                          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    + CategoryInfo          : ObjectNotFound: (C:\\ProgramData\\DockerDesktop\\version-bin:String) [], CommandNotFoundExc\r\n   eption\r\n    + FullyQualifiedErrorId : CommandNotFoundException\r\n``````\r\nand a whole lot of those. \r\nI'm using Powershell , too , i don't know if it's important\r\n\r\n", "comments": ["Just to verify did you follow instructions from [TensorFlow](https://www.tensorflow.org/install/pip) website .Please, let us know. Thanks", "I followed those from here : https://www.tensorflow.org/install/source_windows\r\nLiterally by the word , i needed to build it without AVX as my CPU doesn't support  it", "The line that starts with `SET PATH` is just setting the environment up for the `protoc` command. The actual failing command is this:\r\n`\r\nbazel-out/x64_windows-opt/bin/external/protobuf_archive/protoc.exe --python_out=bazel-out/x64_windows-py2-opt/bin -I. -I. -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/python -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/python tensorflow/lite/toco/model_flags.proto\r\n`\r\n\r\nCould you copy paste the full bazel command output using pastebin, or an attachment file?", "https://pastebin.com/bqLsRj6L\r\nNo output at all ...", "Sorry, should have clarified. \r\nI was asking the full terminal output and your inputs for the `configure.py` script run then the subsequent `bazel build ....` command.", "@unknownazazel \r\n\r\nPlease, let us know if the issue still persists. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30449\">No</a>\n"]}, {"number": 30448, "title": "Keras has memory leak when passing in dataset object to `predict(...)` function", "body": "**Summary**\r\nPerformance degrades quickly and memory increases consistently when calling the Keras `predict` function in a loop with a dataset object. This does not happen when passing `predict` a numpy array, or when passing in a tensor from a dataset iterator.\r\n\r\n**System information**\r\n- Have I written custom code: Minimally reproducible example below uses only stock 1.14.0 code.\r\n- OS Platform and Distribution: Ubuntu 18.04 / Linux Mint 19.1\r\n- TensorFlow installed from (source or binary): `pip install tensorflow-gpu` (example not using GPU, `CUDA_VISIBLE_DEVICES=-1`)\r\n- TensorFlow version (use command below): `v1.14.0-rc1-22-gaf24dc9 1.14.0`\r\n- Python version: `3.7.3`\r\n\r\n**Describe the current behavior**\r\nLooping over `model.predict(x=mydataset)` in a continuous loop degrades in performance after a few hundred iterations. The minimally reproducible example below starts at ~0.04s per loop iteration and within about a minute of running is near 0.5s per loop iteration. Memory continues to climb.\r\n\r\nThis does not happen when passing in a numpy array to `model.predict(x=myndarray)`. The problem is also less severe when passing in `tf.data.Iterator` rather than a `tf.data.Dataset`. If you pass an iterator the performance will continue to degrade at a fifth to a tenth the rate. \r\n\r\nThe cause of the difference between the dataset performance and the iterator performance is likely at `training_utils.py:1314` where Keras creates a new iterator for each `predict` loop. \r\n\r\nThe issue is completely ameliorated when passing predict the tensor produced from `tf.data.make_one_shot_iterator(mydataset).get_next()`. In this case no additional dataset operations appear to be created by keras in the `predict` loop.\r\n\r\n**Describe the expected behavior**\r\nMultiple calls to `predict` should not degrade in performance over time when passing in a dataset. \r\n\r\n**Code to reproduce the issue**\r\nThis code reproduces the issue and is copy/paste runnable, performance will degrade significantly within ~30 seconds running this example.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nSIZE = 5000\r\n\r\ninp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')\r\nx = tf.keras.layers.Dense(units=SIZE)(inp)\r\n\r\nmodel = tf.keras.Model(inputs=inp, outputs=x)\r\n\r\nnp_data = np.random.rand(1, SIZE)\r\nds = tf.data.Dataset.from_tensor_slices(np_data).batch(1).repeat()\r\n\r\ndebug_time = time.time()\r\nwhile True:\r\n    model.predict(x=ds, steps=1)\r\n    print('Processing time {:.2f}'.format(time.time() - debug_time))\r\n    debug_time = time.time()\r\n```\r\n\r\nThis example demonstrates passing a numpy array does not have the same issue.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nSIZE = 5000\r\n\r\ninp = tf.keras.layers.Input(shape=(SIZE,), dtype='float32')\r\nx = tf.keras.layers.Dense(units=SIZE)(inp)\r\n\r\nmodel = tf.keras.Model(inputs=inp, outputs=x)\r\n\r\nnp_data = np.random.rand(1, SIZE)\r\n\r\ndebug_time = time.time()\r\nwhile True:\r\n    model.predict(x=np_data)  # using numpy array directly\r\n    print('Processing time {:.2f}'.format(time.time() - debug_time))\r\n    debug_time = time.time()\r\n```\r\n\r\nThis issue started at SO at: https://stackoverflow.com/questions/56910950/keras-predict-loop-memory-leak-using-tf-data-dataset-but-not-with-a-numpy-array\r\n\r\nI decided to post it here when I realized that `predict` is creating a new iterator each predict loop iteration, and works when the get_next tensor is passed in directly.", "comments": ["I could able to reproduce the issue by provided code snippet on Colab with Tensorflow 1.14.0. Thanks!", "Indeed, this is because in v1 making a dataset iterator adds ops to the graph.\r\n`print('Processing time {:.2f}, {} ops on graph'.format(time.time() - debug_time, len(inp.graph.get_operations())))`\r\n\r\nIn v2 (`tf.enable_v2_behavior()`), you'll see that there is no accumulation of ops and run time does not increase over subsequent `model.predict` calls.\r\n\r\nIt's not obvious why the performance drops off so quickly since the dataset iterator doesn't add that many. In any event though, as long as you use 2.0 you should be fine.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30448\">No</a>\n", "I still have this problem with TF 2.0. "]}, {"number": 30447, "title": "kissfft compile errors due to old archived version of kissfft", "body": "external \"kissfft\" fails to compile on Windows 10\r\n\r\n**System Information**\r\nWindows 10 v1903 on desktop computer \r\nBuilding Latest git pull of Tensorflow (7/6/2019)\r\nMSys64\r\nPython 3.6.8 :: Anaconda, Inc.\r\nCUDA 10.1\r\ncuDNN 7\r\nBazel 0.24.1\r\nMS Visual Studio 2019 Community\r\nNVIDIA GeForce GTX 850M\r\nTF_CUDA_COMPUTE_CAPABILITIES=\"5.0\"\r\n\r\nBuild Command:\r\nbazel build --config=opt --config=v2 --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nProblem:\r\nWhen building Tensorflow from source, I get the following errors:\r\n\r\nc:\\users\\lawrence\\_bazel_lawrence\\jc3cdzjy\\execroot\\org_tensorflow\\external\\kissfft\\kiss_fft.h(60): error C2061: syntax error: identifier 'int16_t'\r\n...\r\n\r\nThe problem is line 46 of kiss_fft.h:\r\n\"#include <sys/types.h>\"\r\nwhich is an attempt to define int16_t. This doesn't work in Windows, and is depricated in unix. The correct approach is:\r\n\"#include <stdint.h>\"\r\n\r\nNOTE: The author of kissfft has fixed this in recent distributions (see https://github.com/mborgerding/kissfft/blob/master/kiss_fft.h) so updating the tensorflow external archive with a more recent version of kissfft would resolve this issue.\r\n", "comments": ["Added PR #30452 for the fix."]}, {"number": 30446, "title": "[TF 2.0 API Docs] tf.image.image_gradients", "body": "Added usage example for image_gradients. The issue is raised in the link https://github.com/tensorflow/tensorflow/issues/30445", "comments": []}, {"number": 30445, "title": "[TF 2.0 API Docs] tf.image.image_gradients", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/image_gradients\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Usage example\r\n\r\nNo usage example provided\r\n\r\n### Submit a pull request?\r\n\r\nYes\r\nhttps://github.com/tensorflow/tensorflow/pull/30446", "comments": ["@imransalam Thanks for contributing through PRs. ", "Automatically closing this out since I understand it to be resolved by the https://github.com/tensorflow/tensorflow/pull/30446 (merged already), but please let me know if I'm mistaken.Thanks!"]}, {"number": 30444, "title": "ctc_* api support custom blank_index", "body": "since `ctc_loss_v2(labels, logits, label_length, logit_length, logits_time_major=True, unique=None, blank_index=None, name=None) ` has supporting `blank_index`,  I think the `ctc_greed_decode_v2` and `ctc_beam_search_decode_v2` also should support it.", "comments": ["@zh794390558 ,\r\nThank you for reaching out. Can you please confirm if you are referring to `tf.nn.ctc_greedy_decoder\r\n` instead of `ctc_greed_decode_v2`, as we couldn't find this `ctc_greed_decode_v2`.", "@ravikyram  Sorry, ` tf.nn.ctc_greedy_decoder` is correct.", "@zh794390558,\r\nSorry for the delayed response. Can you please specify the use cases in which adding `ctc_greedy_decoder` will be helpful? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 30443, "title": "Densnet169 produces memory leak in TF2.0 but works well with latest TF1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview\r\n- Python version:3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: colab version\r\n- GPU model and memory: colab version\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nDensnet169 crashes with memory error even with datagenerator and a batchsize of 8.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you please elaborate the issue .In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "When using this version of tf \r\n$ pip install tf-nightly-gpu-2.0-preview\r\n'''\r\nmodel=tf.keras.applications.densenet.DenseNet169(include_top=True, weights='imagenet', input_tensor=None, input_shape=(224,224,3), pooling=None, classes=1000)\r\n\r\nhistory = model.fit_generator(\r\n      train_data_generator,\r\n      steps_per_epoch=train_count/batch_size ,\r\n      epochs=300,\r\n      validation_data=val_data_generator,\r\n      validation_steps=val_count/batch_size,\r\n      verbose=1,callbacks=callbacks,initial_epoch=initial_epoch)\r\n'''\r\nIn the above snippet, the first epoch takes a long time to start and results in memory error restarting the kernel. I use tf.keras image datagenerator with minimum batch size  of 8 and size(224,224,3) . \r\nRAM- 16GB\r\nGPU- 12GB", "I am not able to reproduce the issue with the provided code(train_data_generator is not defined).Can you please provide full code snippet to reproduce the issue.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30442, "title": "License used in tensorflow repos", "body": "Hello Tensorflow Community, \r\n\r\nI am not sure this is the right place to ask this question, but if I were to create my own repository that uses tensorflow and does not modify or add to tensorflow, which license am I required to use for the code that uses tensorflow?  If this is not the correct place to ask this, please direct me to the correct place. \r\n\r\nThanks\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "TensorFlow is Apache 2 licensed, and you may use it under the terms of that license. If you have any specific queries, I refer you to the Apache License FAQ http://www.apache.org/foundation/license-faq.html. I can't give you legal advice, but you can find a lot of information on the web about using the Apache license."]}, {"number": 30441, "title": "EntityTooLarge when saving estimator checkpoint too big to s3.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.0 (Final)\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): ('v1.11.0-0-gc19e29306c', '1.11.0')\r\n- Python version: Python 2.7.15\r\n\r\n**Describe the current behavior**\r\nI build a tf.estimator.LinearClassifier with 5 categorical_column_with_hash_bucket(hash_bucket_size=100\\*1024\\*1024).\r\nWhen passing model_dir with a local file system, it saved successfully.\r\nWhen passing model_dir with a s3 path, it failed with exception EntityTooLarge.\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport logging\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.lib.io import file_io\r\n\r\n\r\ndef test_tf_estimator(bucket='tensorflow-testing'):\r\n  data_file = ['s3://%s/input_data.txt'%(bucket)]\r\n  label_name = 'click_label'\r\n  feas = 'fea1,fea2,fea3,fea4,fea5'.split(',')\r\n  input_data_schema = [label_name] + feas\r\n  input_data_dtypes = [ \r\n    [0.0 if col==label_name else '']\r\n      for col in input_data_schema \r\n  ]\r\n  input_data_sep = '\\t'\r\n  def gen_input_data(line_per_file=2000):\r\n    for path in data_file :\r\n      with file_io.FileIO(path, 'w') as fout:\r\n        for i in range(line_per_file):\r\n          label = int(i % 10 == 0)\r\n          cols = [label] + [ '%s_%s'%(col, i%107) for col in feas ] \r\n          fout.write('%s\\n'%(input_data_sep.join(map(str, cols))))\r\n  gen_input_data()\r\n  def parse_csv(value):\r\n    columns = tf.decode_csv(value, record_defaults=input_data_dtypes, field_delim=input_data_sep, use_quote_delim=False)\r\n    features = dict(zip(input_data_schema, columns))\r\n    label = features[label_name]\r\n    return features, label\r\n  def input_fn(batch_size=1000):\r\n    dataset = tf.data.TextLineDataset(data_file)\r\n    dataset = dataset.prefetch(buffer_size=batch_size*10)\r\n    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_csv, num_parallel_batches=4, batch_size=batch_size))\r\n    iterator = dataset.make_one_shot_iterator()\r\n    features, labels = iterator.get_next()\r\n    return features, labels\r\n  fids = [ tf.feature_column.categorical_column_with_hash_bucket(fname, 100*1024*1024) for fname in feas ]\r\n\r\n  feature_columns = fids\r\n  partitioner = None\r\n  partitioner = tf.fixed_size_partitioner(16, axis=0) \r\n  estimator = tf.estimator.LinearClassifier(feature_columns, model_dir='s3://%s/model_dir'%(bucket), partitioner=partitioner)\r\n  estimator.train(input_fn=input_fn, steps=1)\r\n  estimator.evaluate(input_fn=input_fn, steps=1)\r\n\r\nif __name__ == \"__main__\":\r\n  test_tf_estimator()\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWhen passing model_dir with a local file system, it saved successfully.\r\n```\r\ntensorflow-testing/model_dir/\r\n|-- [ 124]  checkpoint\r\n|-- [  64]  eval\r\n|-- [7.6M]  events.out.tfevents.1562343007.9-21-165-119\r\n|-- [4.4M]  graph.pbtxt\r\n|-- [   8]  model.ckpt-0.data-00000-of-00002\r\n|-- [5.9G]  model.ckpt-0.data-00001-of-00002\r\n|-- [ 22K]  model.ckpt-0.index\r\n|-- [2.2M]  model.ckpt-0.meta\r\n|-- [   8]  model.ckpt-1.data-00000-of-00002\r\n|-- [5.9G]  model.ckpt-1.data-00001-of-00002\r\n|-- [ 22K]  model.ckpt-1.index\r\n`-- [2.2M]  model.ckpt-1.meta\r\n\r\n1 directory, 11 files\r\n```\r\n\r\nWhen passing model_dir with a s3 path, it failed with exception EntityTooLarge.\r\n```\r\n2019-07-05 23:56:40.450188: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28\r\n2019-07-05 23:57:22.245940: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28\r\nTraceback (most recent call last):\r\n  File \"test_s3.py\", line 82, in <module>\r\n    main()\r\n  File \"test_s3.py\", line 77, in main\r\n    test_tf_estimator()\r\n  File \"test_s3.py\", line 67, in test_tf_estimator\r\n    estimator.train(input_fn=input_fn, steps=1)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1215, in _train_model_default\r\n    saving_listeners)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1406, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 807, in create_session\r\n    hook.after_create_session(self.tf_sess, self.coord)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 567, in after_create_session\r\n    self._save(session, global_step)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 598, in _save\r\n    self._get_saver().save(session, self._save_path, global_step=step)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1433, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnknownError: EntityTooLarge: Unable to parse ExceptionName: EntityTooLarge Message: \r\n     [[{{node save/SaveV2_1}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/ShardedFilename_1, save/SaveV2_1/tensor_names, save/SaveV2_1/shape_and_slices, save/Identity_487, save/Identity_489, save/Identity_491, save/Identity_493, save/Identity_495, save/Identity_497, save/Identity_499, save/Identity_501, save/Identity_503, save/Identity_505, save/Identity_507, save/Identity_509, save/Identity_511, save/Identity_513, save/Identity_515, save/Identity_517, save/Identity_519, save/Identity_521, save/Identity_523, save/Identity_525, save/Identity_527, save/Identity_529, save/Identity_531, save/Identity_533, save/Identity_535, save/Identity_537, save/Identity_539, save/Identity_541, save/Identity_543, save/Identity_545, save/Identity_547, save/Identity_549, save/Identity_551, save/Identity_553, save/Identity_555, save/Identity_557, save/Identity_559, save/Identity_561, save/Identity_563, save/Identity_565, save/Identity_567, save/Identity_569, save/Identity_571, save/Identity_573, save/Identity_575, save/Identity_577, save/Identity_579, save/Identity_581, save/Identity_583, save/Identity_585, save/Identity_587, save/Identity_589, save/Identity_591, save/Identity_593, save/Identity_595, save/Identity_597, save/Identity_599, save/Identity_601, save/Identity_603, save/Identity_605, save/Identity_607, save/Identity_609, save/Identity_611, save/Identity_613, save/Identity_615, save/Identity_617, save/Identity_619, save/Identity_621, save/Identity_623, save/Identity_625, save/Identity_627, save/Identity_629, save/Identity_631, save/Identity_633, save/Identity_635, save/Identity_637, save/Identity_639, save/Identity_641, save/Identity_643, save/Identity_645, save/Identity_647, save/Identity_649, save/Identity_651, save/Identity_653, save/Identity_655, save/Identity_657, save/Identity_659, save/Identity_661, save/Identity_663, save/Identity_665, save/Identity_667, save/Identity_669, save/Identity_671, save/Identity_673, save/Identity_675, save/Identity_677, save/Identity_679, save/Identity_681, save/Identity_683, save/Identity_685, save/Identity_687, save/Identity_689, save/Identity_691, save/Identity_693, save/Identity_695, save/Identity_697, save/Identity_699, save/Identity_701, save/Identity_703, save/Identity_705, save/Identity_707, save/Identity_709, save/Identity_711, save/Identity_713, save/Identity_715, save/Identity_717, save/Identity_719, save/Identity_721, save/Identity_723, save/Identity_725, save/Identity_727, save/Identity_729, save/Identity_731, save/Identity_733, save/Identity_735, save/Identity_737, save/Identity_739, save/Identity_741, save/Identity_743, save/Identity_745, save/Identity_747, save/Identity_749, save/Identity_751, save/Identity_753, save/Identity_755, save/Identity_757, save/Identity_759, save/Identity_761, save/Identity_763, save/Identity_765, save/Identity_767, save/Identity_769, save/Identity_771, save/Identity_773, save/Identity_775, save/Identity_777, save/Identity_779, save/Identity_781, save/Identity_783, save/Identity_785, save/Identity_787, save/Identity_789, save/Identity_791, save/Identity_793, save/Identity_795, save/Identity_797, save/Identity_799, save/Identity_801, save/Identity_803, save/Identity_805, save/Identity_807, save/Identity_809, save/Identity_811, save/Identity_813, save/Identity_815, save/Identity_817, save/Identity_819, save/Identity_821, save/Identity_823, save/Identity_825, save/Identity_827, save/Identity_829, save/Identity_831, save/Identity_833, save/Identity_835, save/Identity_837, save/Identity_839, save/Identity_841, save/Identity_843, save/Identity_845, save/Identity_847, save/Identity_849, save/Identity_851, save/Identity_853, save/Identity_855, save/Identity_857, save/Identity_859, save/Identity_861, save/Identity_863, save/Identity_865, save/Identity_867, save/Identity_869, save/Identity_871, save/Identity_873, save/Identity_875, save/Identity_877, save/Identity_879, save/Identity_881, save/Identity_883, save/Identity_885, save/Identity_887, save/Identity_889, save/Identity_891, save/Identity_893, save/Identity_895, save/Identity_897, save/Identity_899, save/Identity_901, save/Identity_903, save/Identity_905, save/Identity_907, save/Identity_909, save/Identity_911, save/Identity_913, save/Identity_915, save/Identity_917, save/Identity_919, save/Identity_921, save/Identity_923, save/Identity_925, save/Identity_927, save/Identity_929, save/Identity_931, save/Identity_933, save/Identity_935, save/Identity_937, save/Identity_939, save/Identity_941, save/Identity_943, save/Identity_945, save/Identity_947, save/Identity_949, save/Identity_951, save/Identity_953, save/Identity_955, save/Identity_957, save/Identity_959, save/Identity_961, save/Identity_963, save/Identity_965, save/Identity_967, save/Identity_969, save/Identity_971)]]\r\n\r\nCaused by op u'save/SaveV2_1', defined at:\r\n  File \"test_s3.py\", line 82, in <module>\r\n    main()\r\n  File \"test_s3.py\", line 77, in main\r\n    test_tf_estimator()\r\n  File \"test_s3.py\", line 67, in test_tf_estimator\r\n    estimator.train(input_fn=input_fn, steps=1)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1215, in _train_model_default\r\n    saving_listeners)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1406, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 215, in finalize\r\n    self._saver.build()\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1106, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1143, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 369, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 343, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 284, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 202, in save_op\r\n    tensors)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1690, in save_v2\r\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/data/home/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): EntityTooLarge: Unable to parse ExceptionName: EntityTooLarge Message: \r\n     [[{{node save/SaveV2_1}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](save/ShardedFilename_1, save/SaveV2_1/tensor_names, save/SaveV2_1/shape_and_slices, save/Identity_487, save/Identity_489, save/Identity_491, save/Identity_493, save/Identity_495, save/Identity_497, save/Identity_499, save/Identity_501, save/Identity_503, save/Identity_505, save/Identity_507, save/Identity_509, save/Identity_511, save/Identity_513, save/Identity_515, save/Identity_517, save/Identity_519, save/Identity_521, save/Identity_523, save/Identity_525, save/Identity_527, save/Identity_529, save/Identity_531, save/Identity_533, save/Identity_535, save/Identity_537, save/Identity_539, save/Identity_541, save/Identity_543, save/Identity_545, save/Identity_547, save/Identity_549, save/Identity_551, save/Identity_553, save/Identity_555, save/Identity_557, save/Identity_559, save/Identity_561, save/Identity_563, save/Identity_565, save/Identity_567, save/Identity_569, save/Identity_571, save/Identity_573, save/Identity_575, save/Identity_577, save/Identity_579, save/Identity_581, save/Identity_583, save/Identity_585, save/Identity_587, save/Identity_589, save/Identity_591, save/Identity_593, save/Identity_595, save/Identity_597, save/Identity_599, save/Identity_601, save/Identity_603, save/Identity_605, save/Identity_607, save/Identity_609, save/Identity_611, save/Identity_613, save/Identity_615, save/Identity_617, save/Identity_619, save/Identity_621, save/Identity_623, save/Identity_625, save/Identity_627, save/Identity_629, save/Identity_631, save/Identity_633, save/Identity_635, save/Identity_637, save/Identity_639, save/Identity_641, save/Identity_643, save/Identity_645, save/Identity_647, save/Identity_649, save/Identity_651, save/Identity_653, save/Identity_655, save/Identity_657, save/Identity_659, save/Identity_661, save/Identity_663, save/Identity_665, save/Identity_667, save/Identity_669, save/Identity_671, save/Identity_673, save/Identity_675, save/Identity_677, save/Identity_679, save/Identity_681, save/Identity_683, save/Identity_685, save/Identity_687, save/Identity_689, save/Identity_691, save/Identity_693, save/Identity_695, save/Identity_697, save/Identity_699, save/Identity_701, save/Identity_703, save/Identity_705, save/Identity_707, save/Identity_709, save/Identity_711, save/Identity_713, save/Identity_715, save/Identity_717, save/Identity_719, save/Identity_721, save/Identity_723, save/Identity_725, save/Identity_727, save/Identity_729, save/Identity_731, save/Identity_733, save/Identity_735, save/Identity_737, save/Identity_739, save/Identity_741, save/Identity_743, save/Identity_745, save/Identity_747, save/Identity_749, save/Identity_751, save/Identity_753, save/Identity_755, save/Identity_757, save/Identity_759, save/Identity_761, save/Identity_763, save/Identity_765, save/Identity_767, save/Identity_769, save/Identity_771, save/Identity_773, save/Identity_775, save/Identity_777, save/Identity_779, save/Identity_781, save/Identity_783, save/Identity_785, save/Identity_787, save/Identity_789, save/Identity_791, save/Identity_793, save/Identity_795, save/Identity_797, save/Identity_799, save/Identity_801, save/Identity_803, save/Identity_805, save/Identity_807, save/Identity_809, save/Identity_811, save/Identity_813, save/Identity_815, save/Identity_817, save/Identity_819, save/Identity_821, save/Identity_823, save/Identity_825, save/Identity_827, save/Identity_829, save/Identity_831, save/Identity_833, save/Identity_835, save/Identity_837, save/Identity_839, save/Identity_841, save/Identity_843, save/Identity_845, save/Identity_847, save/Identity_849, save/Identity_851, save/Identity_853, save/Identity_855, save/Identity_857, save/Identity_859, save/Identity_861, save/Identity_863, save/Identity_865, save/Identity_867, save/Identity_869, save/Identity_871, save/Identity_873, save/Identity_875, save/Identity_877, save/Identity_879, save/Identity_881, save/Identity_883, save/Identity_885, save/Identity_887, save/Identity_889, save/Identity_891, save/Identity_893, save/Identity_895, save/Identity_897, save/Identity_899, save/Identity_901, save/Identity_903, save/Identity_905, save/Identity_907, save/Identity_909, save/Identity_911, save/Identity_913, save/Identity_915, save/Identity_917, save/Identity_919, save/Identity_921, save/Identity_923, save/Identity_925, save/Identity_927, save/Identity_929, save/Identity_931, save/Identity_933, save/Identity_935, save/Identity_937, save/Identity_939, save/Identity_941, save/Identity_943, save/Identity_945, save/Identity_947, save/Identity_949, save/Identity_951, save/Identity_953, save/Identity_955, save/Identity_957, save/Identity_959, save/Identity_961, save/Identity_963, save/Identity_965, save/Identity_967, save/Identity_969, save/Identity_971)]]\r\n```\r\n", "comments": ["Looks like a limitation on s3 storage capacity. See https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html", "Is it possible to use a Multipart Upload instead of put when detecting a large upload?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "I think the codes above is clearly enough.\r\nIt looks like the implementation of file_io module has choose to use a PUT method which suffers when uploading object too large. \r\n\r\nOr do I miss anything?\r\nIs there any document or api guilds to manipulate the s3 options to use a Multipart Upload?"]}, {"number": 30440, "title": "mpi_portable_platform.h not found because MPI_LIB_IS_OPENMPI variable is always set to True", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Linux 7.6 (Maipo) \r\n- TensorFlow installed from (source or binary): Issue when trying to build from source.\r\n- TensorFlow version: Master Branch (as of 7/4/2019)\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: Nope.\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): 5.5\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```./configure``` with ```mpi``` enabled.\r\n\r\n```bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\n\r\nSorry, the log is somehow lost. But the error is about ```mpi_portable_platform.h``` not found. My system has ```mpich``` installed but not ```open_mpi```. The source file in ```tensorflow/third_party/mpi/mpi.bzl``` contains:\r\n\r\n```\r\ndef mpi_hdr():\r\n    MPI_LIB_IS_OPENMPI = True\r\n    hdrs = []\r\n    if MPI_LIB_IS_OPENMPI:\r\n        hdrs = [\"mpi.h\", \"mpi_portable_platform.h\"]  #When using OpenMPI\r\n    else:\r\n        hdrs = [\"mpi.h\", \"mpio.h\", \"mpicxx.h\"]  #When using MVAPICH\r\n    return hdrs\r\n```\r\n\r\nIt seems that the variable ```MPI_LIB_IS_OPENMPI``` is always set to ```True``` which won't work if one does not use ```open mpi```. I set the variable to ```False``` and the error goes away.\r\n\r\n\r\n", "comments": ["This line will be changed if the `./configure` script detects your MPICH installation. See https://github.com/tensorflow/tensorflow/blob/master/configure.py#L1201.", "> This line will be changed if the `./configure` script detects your MPICH installation. See https://github.com/tensorflow/tensorflow/blob/master/configure.py#L1201.\r\n\r\nThank you. Yes, I was confused why this hasn't been the case. \r\n\r\nWhen I ran `./configure`, the script did detect the location of my MPICH installation (i.e. rather than me having to manually put in the location). Given this, I still got this error down the road so I was a bit baffled.\r\n\r\nThe error was gone only when I manually set the variable to false.", "Try change configure script and add debug toggle to see why that line wasn\u2019t executed.", "@YuMan-Tam Did you try as suggested by byronyi.\r\nLooks like your issue got resolved.Please, let me know if i am wrong.Thanks!", "> @YuMan-Tam Did you try as suggested by byronyi.\r\n> Looks like your issue got resolved.Please, let me know if i am wrong.Thanks!\r\n\r\n@byronyi: Thank you for the suggestion. I did not yet have a chance to test this alternative but will post update when I do.\r\n\r\n@ravikyram: The issue is resolved in the sense that I got rid of error by manually changing the source file.   But I still haven't did what @byronyi suggested to figure out why MPICH was not correctly detected by the configuration script. Would you like to me close the issue for now?", "Ping @jbedorf; mind to take a look?", "@YuMan-Tam  did you get a chance to look as suggested by @byronyi .Thanks!", "Looks like the problem is that this line fails:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L1201\r\nThe replace command looks for the string without spaces around the ` = ` symbol, while the `mpi.bzl` file has the line with spaces. I'll push a fix later this week. ", "> @YuMan-Tam did you get a chance to look as suggested by @byronyi .Thanks!\r\n\r\n@ravikyram: Sorry for my late reply. I believe @jbedorf has located the root cause of this issue. The same problem persists in the configure script for the 2.0 version of tensorflow. \r\n\r\nIn fact, I am not sure how to do the `debug toggle` as suggested by @byronyi. However, I could investigate further (a.k.a. confirm the finding from @jbedorf) if this is still helpful.  "]}, {"number": 30439, "title": "Fix build with make Linux target", "body": "This PR fixes building with TARGET=linux and custom CFLAGS.", "comments": ["@johang Could you please resolve the conflicts? Thanks!", "@gbaned, done."]}, {"number": 30438, "title": "Calibration with Post-training quantization fails on networks that have multiple placeholders", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to use post-training quantisation with a calibration data-set to capture the activation ranges. The network is an unrolled lstm-based model. This model has multiple placeholders (3 ph) and the post-training quantisation process breaks because of that.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt is expected the post-training quantisation method to accept networks with multiple placeholders. Trying the same code as below in other networks that has only one placeholder works as expected (although the reference implementations of quantise/de-quantise which are located at the beginning and at the end of the network is extremely slow, but this is another issue).\r\n\r\n**Code to reproduce the issue**\r\n```\r\ndef representative_dataset_gen():\r\n    for _ in range(1):\r\n        input_node = np.array(np.random.rand(1,16,19,26), dtype=np.float32)\r\n        new_state_c = np.array(np.random.rand(1,2048), dtype=np.float32)\r\n        new_state_h = np.array(np.random.rand(1,2048), dtype=np.float32)\r\n        yield [input_node,new_state_c,new_state_h]\r\n```\r\n\r\n```\r\ndef load_graph_and_post_train():\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(args.graph, \r\n        input_arrays=[ \"input_node\",\"previous_state_c\",\"previous_state_h\" ], \r\n        output_arrays=[\"logits\"], \r\n        input_shapes={\"input_node\": [1,16,19,26], \"previous_state_c\":[1,2048], \"previous_state_h\":[1,2048]} )\r\n\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_dataset_gen\r\n\r\n    tflite_quant_model = converter.convert()\r\n    open(\"./models/post_train_calibration_model.tflite\", \"wb\").write(tflite_quant_model)\r\n\r\nif __name__ == '__main__':\r\n    parser = ArgumentParser(description='post-training quantisation')\r\n    parser.add_argument('-graph', required=True, type=str, help='The pb model')\r\n    args = parser.parse_args()\r\n    load_graph_and_post_train()\r\n```\r\n\r\n\r\n**Other info / logs**\r\n2019-07-05 14:03:13.996584: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-05 14:03:14.003163: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192860000 Hz\r\n2019-07-05 14:03:14.007918: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6b0b650 executing computations on platform Host. Devices:\r\n2019-07-05 14:03:14.007953: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0705 14:03:15.397149 140670832994048 deprecation_wrapper.py:119] From post-trainin-quantisation-calib.py:96: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\nW0705 14:03:15.397516 140670832994048 deprecation_wrapper.py:119] From post-trainin-quantisation-calib.py:96: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n\r\n2019-07-05 14:03:15.401800: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-07-05 14:03:15.401952: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-07-05 14:03:31.757150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-07-05 14:03:31.757222: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 388 nodes (-14), 510 edges (-14), time = 10271.6318ms.\r\n2019-07-05 14:03:31.757235: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 388 nodes (0), 510 edges (0), time = 5687.64893ms.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nterminate called after throwing an instance of 'std::out_of_range'\r\n  what():  _Map_base::at\r\nAbort (core dumped)\r\n", "comments": ["Changed the script a little bit. \r\n\r\nWhen I run post-training quantisation without the argument` converter.optimizations = [tf.lite.Optimize.DEFAULT]` then the script extracts a tflite model (obviously not quatnised). So the problem is probably not the placeholders and the shapes but the u8 optimizer ?\r\n\r\n```\r\ndef representative_dataset_gen():\r\n    for _ in range(1):\r\n        input_node = np.zeros([16,19,26], dtype=np.float32)\r\n        new_state_h= np.zeros([2048], dtype=np.float32)\r\n        new_state_c= np.zeros([2048], dtype=np.float32)\r\n        yield [[input_node], [new_state_h], [new_state_c]]\r\n```\r\n\r\n```\r\ndef load_graph_and_post_train():\r\n    print(\"loading graph ...\")\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(args.graph, \r\n        input_arrays=[ \"input_node\",\"previous_state_h\",\"previous_state_c\" ], \r\n        output_arrays=[\"logits\"], \r\n        input_shapes={\"input_node\": [1,16,19,26], \"previous_state_h\":[1,2048], \"previous_state_c\":[1,2048]} )\r\n\r\n    converter.representative_dataset = representative_dataset_gen\r\n    tflite_quant_model = converter.convert()\r\n    open(\"./models/post_train_calibration_model.tflite\", \"wb\").write(tflite_quant_model)\r\n```", "I have std::out_of_range error even with only one placeholder", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30438\">No</a>\n"]}, {"number": 30437, "title": "Remove \"global_step\" in usage example of CosineDecay and other related classes", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py\r\n\r\n## Description of issue (what needs changing):\r\nUsage example of CosineDecay, CosineDecayRestarts, LinearCosineDecay, NoisyLinearCosineDecay takes a parameter \"global_step\" which does not correspond to the definition. \r\n\r\n### Clear description\r\n\r\nThe script defines the various learning rate decay classes used for network training.\r\n\r\n### Usage example\r\n\r\nThere is a usage example. However, it does not correspond to the definition. \r\n\r\n### Submit a pull request?\r\n\r\nI can if required\r\n", "comments": ["@vsuryamurthy Can you submit a PR to update the docs? Thanks!", "Closing this issue as it was resolved by the  https://github.com/tensorflow/tensorflow/pull/30648 which was already merged. Thanks!"]}, {"number": 30436, "title": "tf.data.Dataset.list_files return is deterministic order when shuffle=False?", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\r\n\r\n## Description of issue (what needs changing):\r\nIn the doc above, it says \r\n```\r\nNOTE: The default behavior of this method is to return filenames in \r\na non-deterministic random shuffled order. \r\nPass a seed or shuffle=False to get results in a deterministic order.\r\n```\r\n\r\nSo if pass `shuffle=False`, it will return a deterministic order.\r\n\r\nBut if check source code of the function, it calls following function to get matching files.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L769\r\n\r\n```\r\n  @staticmethod\r\n  def list_files(file_pattern, shuffle=None, seed=None):\r\n      ...\r\n      matching_files = gen_io_ops.matching_files(file_pattern)\r\n```\r\n\r\nIf we check description of `gen_io_ops.matching_files`,  it says `Note also that the order of filenames returned can be non-deterministic.`\r\n\r\n```\r\n@tf_export('matching_files')\r\ndef matching_files(pattern, name=None):\r\n  r\"\"\"Returns the set of files matching one or more glob patterns.\r\n\r\n  Note that this routine only supports wildcard characters in the\r\n\r\n  basename portion of the pattern, not in the directory portion.\r\n\r\n  Note also that the order of filenames returned can be non-deterministic.\r\n\r\n  Args:\r\n    pattern: A `Tensor` of type `string`.\r\n      Shell wildcard pattern(s). Scalar or vector of type string.\r\n    name: A name for the operation (optional).\r\n\r\n  Returns:\r\n    A `Tensor` of type `string`.\r\n  \"\"\"\r\n```\r\n\r\nAnd also the document in https://www.tensorflow.org/api_docs/python/tf/io/matching_files.\r\n\r\n```\r\nDefined in generated file: python/ops/gen_io_ops.py.\r\n\r\nNote that this routine only supports wildcard characters in the basename portion of the pattern,\r\nnot in the directory portion. \r\nNote also that the order of filenames returned can be non-deterministic.\r\n```\r\n\r\nAnd also description in the function https://www.tensorflow.org/api_docs/python/tf/io/match_filenames_once \r\n\r\n```\r\nDefined in python/training/input.py.\r\n\r\nNOTE: The order of the files returned can be non-deterministic.\r\n```\r\n\r\nCheck source code of the fucntion\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/training/input.py#L63\r\n\r\nBoth `tf.io.matching_files` and  tf.io.match_filenames_once` call `gen_io_ops.matching_files`.\r\n\r\nI think it is quite confuse here.", "comments": ["@zhjunqin As `MatchingFilesOp` sorts the matched files [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matching_files_op.cc#L63), the order of filenames returned should be deterministic. PR #30543 has been submitted to update the related docs. "]}, {"number": 30435, "title": "Operation marked as not fetchable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04, Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13 and 1.14\r\n- Python version: 3\r\n- CUDA/cuDNN version: 10.0/7\r\n- GPU model and memory: Nvidia GTX 960M 4GB\r\n\r\n**Describe the current behavior**\r\nI wanted to reference a list item inside tf.while_loop using a loop variable. Instead of printing the list items one after another it says the following:\r\n```\r\nValueError: Operation 'while/Identity' has been marked as not fetchable.\r\n```\r\n\r\n**Describe the expected behavior**\r\nExpected behaviour is to print the list items.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nclass A():\r\n    def __init__(self):\r\n        self.lst = [1, 2, 3]\r\n        self.sess = tf.Session()\r\n        self.total_length = tf.constant(len(self.lst))\r\n\r\n    def loop(self, i):\r\n        pr = tf.print(i)\r\n        current_value = self.lst[i.eval(session=self.sess)]\r\n        with tf.control_dependencies([pr]):\r\n            i = tf.add(i, 1)\r\n        return [i]\r\n\r\n    def cond(self, i):\r\n        return tf.less(i, self.total_length) \r\n\r\n    def run(self):\r\n        i = tf.constant(0)\r\n        while_op = tf.while_loop(self.cond, self.loop, [i])\r\n        final_i = self.sess.run(while_op)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    obj = A()\r\n    obj.run()\r\n```\r\nI have tried eager execution as well but the same error pops up.\r\n\r\n**Other info / logs**\r\nThis issue was originally a comment by me in [this thread](https://github.com/tensorflow/tensorflow/issues/4094#issuecomment-502346438). It was confirmed by @mrry that this indeed was a bug. \r\n\r\nThanks for all the help!", "comments": ["Saurabh: This looks like a bug in V1 control flow (in graph mode) to me. Can you take a quick look and confirm? Thanks!", "This seems to be working as intended. Could you try the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass A():\r\n    def __init__(self):\r\n        self.lst = tf.constant([1, 2, 3])\r\n        self.sess = tf.Session()\r\n        self.total_length = tf.size(self.lst)\r\n\r\n    def loop(self, i):\r\n        pr = tf.print(i)\r\n        current_value = self.lst[i]\r\n        with tf.control_dependencies([pr]):\r\n            i = tf.add(i, 1)\r\n        return [i]\r\n\r\n    def cond(self, i):\r\n        return tf.less(i, self.total_length) \r\n\r\n    def run(self):\r\n        i = tf.constant(0)\r\n        while_op = tf.while_loop(self.cond, self.loop, [i])\r\n        final_i = self.sess.run(while_op)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    obj = A()\r\n    obj.run()\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30435\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30435\">No</a>\n", "Apologies for the confusion: I missed the `i.eval(session=self.sess)` inside the loop body, which is indeed not allowed.", "Oh okay, it works! Thanks!\r\nI actually wanted to know if it would be possible to index a pythonic list instead of using a tensorflow array.\r\n\r\nimport tensorflow as tf\r\n```\r\nclass A():\r\n    def __init__(self):\r\n        self.lst = [1, 2, 3]\r\n        self.sess = tf.Session()\r\n        self.total_length = tf.size(self.lst)\r\n\r\n    def loop(self, i):\r\n        pr = tf.print(i)\r\n        current_value = i.eval()\r\n        pr_2 = tf.print(self.lst[current_value])\r\n        with tf.control_dependencies([pr, pr_2]):\r\n            i = tf.add(i, 1)\r\n        return [i]\r\n\r\n    def cond(self, i):\r\n        return tf.less(i, self.total_length) \r\n\r\n    def run(self):\r\n        i = tf.constant(0)\r\n        while_op = tf.while_loop(self.cond, self.loop, [i])\r\n        final_i = self.sess.run(while_op)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    obj = A()\r\n    obj.run()\r\n```\r\nBut here it says:\r\n```\r\nCannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\r\n```\r\nI add the line `with self.sess.as_default()`  right before the `current_value` line which brings me back to the original error message. \r\nThank you for all your help!"]}, {"number": 30434, "title": "[XLA] Fix compilation errors in exhaustive test", "body": "When I compile the code in the exchaustive ops tests, I get an error.  The internet seems to think that this error is due to the code being out of spec for standard C++, as shown here: \r\n\r\nhttps://stackoverflow.com/questions/3052579/explicit-specialization-in-non-namespace-scope\r\n\r\nand\r\n\r\nhttps://stackoverflow.com/questions/2097811/c-syntax-for-explicit-specialization-of-a-template-function-in-a-template-clas\r\n\r\nThis change moves the template out of the enclosing class into a namespace, as the standard requires.\r\n\r\nIt also adds a constructor for the ErrorSpec class, which also fails to compile due to the use of the braces initialization.\r\n\r\nMy compiler version is `gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0`\r\n", "comments": ["done :)", "regarding the windows test failure, it seems very very unlikely that this change can have caused the failure:\r\n\r\n```\r\nAttributeError: Can't pickle local object 'TFRecordWriterCloseAndFlushTests.testFlush.<locals>.childProcess'\r\n```"]}, {"number": 30433, "title": "Unable to build micro_speech for bluepill target (region `FLASH' overflowed by 9656 bytes)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow version: r1.14\r\n- Python version:\r\n- GCC/Compiler version (if compiling from source):\r\narm-none-eabi-gcc (15:4.9.3+svn231177-1) 4.9.3 20150529 (prerelease)\r\narm-none-eabi-g++ (15:4.9.3+svn231177-1) 4.9.3 20150529 (prerelease)\r\n\r\n**Describe the problem**\r\nI am unable to generate the binary for micro_speech_bin. The error log says that the FLASH memory is not big enough. I have had a similar issue with different build tests. If I increase the size of the FLASH memory (say to 90K), the executables and .bin can be generated and tested with renode. However, these can now not be ported to the Blue Pill development board (I assume, our dev board has not arrived yet).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=bluepill micro_speech_bin```\r\n\r\n**Any other info / logs**\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/main.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/main.o\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,\r\n                 from tensorflow/lite/experimental/micro/examples/micro_speech/main.cc:22:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,\r\n                 from tensorflow/lite/experimental/micro/examples/micro_speech/main.cc:22:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/audio_provider.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/audio_provider.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/no_micro_features_data.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/no_micro_features_data.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/yes_micro_features_data.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/yes_micro_features_data.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/tiny_conv_micro_features_model_data.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/tiny_conv_micro_features_model_data.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/command_responder.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/command_responder.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_lut.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_lut.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.o\r\narm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/kiss_fft.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/kiss_fft.o\r\ncc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C\r\narm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/tools/kiss_fftr.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/tools/kiss_fftr.o\r\ncc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/micro_error_reporter.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_error_reporter.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/micro_mutable_op_resolver.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_mutable_op_resolver.o\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,\r\n                 from tensorflow/lite/experimental/micro/micro_mutable_op_resolver.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,\r\n                 from tensorflow/lite/experimental/micro/micro_mutable_op_resolver.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/simple_tensor_allocator.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/simple_tensor_allocator.o\r\nIn file included from ./tensorflow/lite/experimental/micro/simple_tensor_allocator.h:21:0,\r\n                 from tensorflow/lite/experimental/micro/simple_tensor_allocator.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/experimental/micro/simple_tensor_allocator.h:21:0,\r\n                 from tensorflow/lite/experimental/micro/simple_tensor_allocator.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/bluepill/debug_log.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/bluepill/debug_log.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/debug_log_numbers.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/debug_log_numbers.o\r\ntensorflow/lite/experimental/micro/debug_log_numbers.cc: In function 'char* {anonymous}::FastFloatToBufferLeft(float, char*)':\r\ntensorflow/lite/experimental/micro/debug_log_numbers.cc:117:53: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   const uint32_t u = *reinterpret_cast<uint32_t*>(&f);\r\n                                                     ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/micro_interpreter.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_interpreter.o\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_interpreter.h:20,\r\n                 from tensorflow/lite/experimental/micro/micro_interpreter.cc:15:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_interpreter.h:20,\r\n                 from tensorflow/lite/experimental/micro/micro_interpreter.cc:15:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/pooling.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/pooling.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/softmax.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/softmax.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/conv.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/conv.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/elementwise.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/elementwise.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.o\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,\r\n                 from tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc:13:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h:18,\r\n                 from ./tensorflow/lite/experimental/micro/kernels/all_ops_resolver.h:16,\r\n                 from tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc:13:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/kernels/fully_connected.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/fully_connected.o\r\narm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/c/c_api_internal.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/c/c_api_internal.o\r\ncc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/error_reporter.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/error_reporter.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/flatbuffer_conversions.o\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\n                 from tensorflow/lite/core/api/flatbuffer_conversions.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from ./tensorflow/lite/core/api/flatbuffer_conversions.h:24,\r\n                 from tensorflow/lite/core/api/flatbuffer_conversions.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/op_resolver.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/op_resolver.o\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from tensorflow/lite/core/api/op_resolver.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameTensorType(tflite::TensorType)':\r\n./tensorflow/lite/schema/schema_generated.h:374:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < TensorType_FLOAT32 || e > TensorType_INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameQuantizationDetails(tflite::QuantizationDetails)':\r\n./tensorflow/lite/schema/schema_generated.h:404:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < QuantizationDetails_NONE || e > QuantizationDetails_CustomQuantization) return \"\";\r\n         ^\r\nIn file included from ./tensorflow/lite/core/api/op_resolver.h:20:0,\r\n                 from tensorflow/lite/core/api/op_resolver.cc:16:\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOperator(tflite::BuiltinOperator)':\r\n./tensorflow/lite/schema/schema_generated.h:833:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOperator_ADD || e > BuiltinOperator_HARD_SWISH) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameBuiltinOptions(tflite::BuiltinOptions)':\r\n./tensorflow/lite/schema/schema_generated.h:1133:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < BuiltinOptions_NONE || e > BuiltinOptions_HardSwishOptions) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNamePadding(tflite::Padding)':\r\n./tensorflow/lite/schema/schema_generated.h:2296:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < Padding_SAME || e > Padding_VALID) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameActivationFunctionType(tflite::ActivationFunctionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2338:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < ActivationFunctionType_NONE || e > ActivationFunctionType_SIGN_BIT) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSHProjectionType(tflite::LSHProjectionType)':\r\n./tensorflow/lite/schema/schema_generated.h:2371:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSHProjectionType_UNKNOWN || e > LSHProjectionType_DENSE) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameFullyConnectedOptionsWeightsFormat(tflite::FullyConnectedOptionsWeightsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2401:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < FullyConnectedOptionsWeightsFormat_DEFAULT || e > FullyConnectedOptionsWeightsFormat_SHUFFLED4x16INT8) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameLSTMKernelType(tflite::LSTMKernelType)':\r\n./tensorflow/lite/schema/schema_generated.h:2431:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < LSTMKernelType_FULL || e > LSTMKernelType_BASIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCombinerType(tflite::CombinerType)':\r\n./tensorflow/lite/schema/schema_generated.h:2464:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CombinerType_SUM || e > CombinerType_SQRTN) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameMirrorPadMode(tflite::MirrorPadMode)':\r\n./tensorflow/lite/schema/schema_generated.h:2494:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < MirrorPadMode_REFLECT || e > MirrorPadMode_SYMMETRIC) return \"\";\r\n         ^\r\n./tensorflow/lite/schema/schema_generated.h: In function 'const char* tflite::EnumNameCustomOptionsFormat(tflite::CustomOptionsFormat)':\r\n./tensorflow/lite/schema/schema_generated.h:2521:9: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n   if (e < CustomOptionsFormat_FLEXBUFFERS || e > CustomOptionsFormat_FLEXBUFFERS) return \"\";\r\n         ^\r\ntensorflow/lite/core/api/op_resolver.cc: In function 'TfLiteStatus tflite::GetRegistrationFromOpCode(const tflite::OperatorCode*, const tflite::OpResolver&, tflite::ErrorReporter*, const TfLiteRegistration**)':\r\ntensorflow/lite/core/api/op_resolver.cc:29:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]\r\n       builtin_code < BuiltinOperator_MIN) {\r\n                    ^\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/kernel_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/kernel_util.o\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/internal/quantization_util.cc -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/internal/quantization_util.o\r\narm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/timers.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/timers.o\r\ncc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C\r\narm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.o\r\ncc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c: In function 'FastFloatToBufferLeft':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c:72:3: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n   const uint32_t u = *(uint32_t*)(&i);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c: In function 'StrCatInt32':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c:135:3: warning: variable length array 'number_string' is used [-Wvla]\r\n   char number_string[kFastToBufferSize];\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c: In function 'StrCatUInt32':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.c:142:3: warning: variable length array 'number_string' is used [-Wvla]\r\n   char number_string[kFastToBufferSize];\r\n   ^\r\narm-none-eabi-gcc -DNDEBUG -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -c tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.o\r\ncc1: warning: command line option '-std=gnu++11' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fno-rtti' is valid for C++/ObjC++ but not for C\r\ncc1: warning: command line option '-fpermissive' is valid for C++/ObjC++ but not for C\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c: In function 'HardFaultHandler':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:62:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(SCB->HFSR);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:64:5: warning: variable length array 'log' is used [-Wvla]\r\n     LOG_HEX32(SCB->CFSR);\r\n     ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:65:5: warning: variable length array 'log' is used [-Wvla]\r\n     LOG_HEX32(SCB_CFSR_IBUSERR_Msk);\r\n     ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:92:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(r0);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:93:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(r1);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:94:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(r2);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:95:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(r3);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:96:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(r12);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:97:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(lr);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:98:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(pc);\r\n   ^\r\ntensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.c:99:3: warning: variable length array 'log' is used [-Wvla]\r\n   LOG_HEX32(psr);\r\n   ^\r\narm-none-eabi-ar -r tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_error_reporter.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_mutable_op_resolver.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/simple_tensor_allocator.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/bluepill/debug_log.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/debug_log_numbers.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/micro_interpreter.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/pooling.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/softmax.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/conv.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/elementwise.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/fully_connected.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/c/c_api_internal.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/error_reporter.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/flatbuffer_conversions.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/core/api/op_resolver.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/kernel_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/kernels/internal/quantization_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/timers.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/strings.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/source/startup.o \r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. -Itensorflow/lite/experimental/micro/tools/make/downloads/ -Itensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/experimental/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/experimental/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -Itensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/include -Itensorflow/lite/experimental/micro/tools/make/downloads/kissfft -o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/main.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/audio_provider.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/feature_provider.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/no_micro_features_data.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/yes_micro_features_data.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/tiny_conv_micro_features_model_data.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/recognize_commands.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/command_responder.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_features_generator.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/micro_model_settings.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/fft_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/filterbank_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/frontend_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_lut.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/log_scale_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/noise_reduction_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/pcan_gain_control_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/examples/micro_speech/micro_features/window_util.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/kiss_fft.o tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/tools/make/downloads/kissfft/tools/kiss_fftr.o  tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a -T tensorflow/lite/experimental/micro/tools/make/downloads/stm32_bare_lib/stm32_linker_layout.lds -Wl,-Map=tensorflow/lite/experimental/micro/tools/make/gen/bluepill.map,--cref -Wl,--gc-sections -lm\r\n/usr/lib/gcc/arm-none-eabi/4.9.3/../../../arm-none-eabi/bin/ld: tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech section `.rodata' will not fit in region `FLASH'\r\n/usr/lib/gcc/arm-none-eabi/4.9.3/../../../arm-none-eabi/bin/ld: region `FLASH' overflowed by 9656 bytes\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/experimental/micro/examples/micro_speech/Makefile.inc:370: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/bin/micro_speech] Error 1", "comments": ["@KBacsa We see you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30433\">No</a>\n"]}, {"number": 30432, "title": "pasta.base.annotate.AnnotationError: Indent detection failed (line 300)", "body": "Hello,\r\nI use google colab and I have this error:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tf_upgrade_v2\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py\", line 139, in main\r\n    args.input_file, output_file, upgrade)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py\", line 40, in process_file\r\n    upgrader.process_file(in_filename, out_filename)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 900, in process_file\r\n    temp_file)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 960, in process_opened_file\r\n    self.update_string_pasta(\"\".join(lines), in_filename))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 916, in update_string_pasta\r\n    t = pasta.parse(text)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/__init__.py\", line 25, in parse\r\n    annotator.visit(t)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 220, in visit_Module\r\n    self.generic_visit(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 261, in generic_visit\r\n    self.visit(item)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 95, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 411, in visit_FunctionDef\r\n    self.visit(stmt)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 95, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 291, in visit_For\r\n    self.visit(stmt)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 95, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 290, in visit_For\r\n    for stmt in self.indented(node, 'body'):\r\n  File \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1224, in indented\r\n    'more than the outer indentation.' % cur_loc[0])\r\npasta.base.annotate.AnnotationError: Indent detection failed (line 300); inner indentation level is not more than the outer indentation.\r\n\r\nthanks for help me", "comments": ["@GuesmiAbd do you have an example of what you are trying to convert?\r\n\r\nit looks like you have a code, that has one line that is an extra indentation - whitespace", "@GuesmiAbd It looks like you have less indentation. Would you mind to copy here lines `299-301`?", "@lc0  You can see my code lines 297-301\r\n        for l in range(NUM_CLASSES):\r\n            total_seen_class[l] += np.sum((batch_label==l) & (batch_smpw>0))\r\n            total_correct_class[l] += np.sum((pred_val==l) & (batch_label==l) & (batch_smpw>0))\r\n        for b in range(batch_label.shape[0]):\r\n\t         _, uvlabel, _ = pc_util.point_cloud_label_to_surface_voxel_label_fast(aug_data[b,batch_smpw[b,:]>0,:], np.concatenate((np.expand_dims(batch_label[b,batch_smpw[b,:]>0],1),np.expand_dims(pred_val[b,batch_smpw[b,:]>0],1)),axis=1), res=0.02)", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Hello @ravikyram ,\r\nI'm trying to upgrade my code to TF2.0.First I install TF2.0alpha0-gpu  on google colab (python 3.6)then I typed the command  !tf_upgrade_v2 --infile train.py --outfile train_upgraded.py  to generate the upgraded version.\r\nThe link of the code:https://github.com/charlesq34/pointnet2/blob/master/scannet/train.py\r\n\r\nI got the following error:\r\nprocess_opened_file\r\nself.update_string_pasta(\"\".join(lines), in_filename))\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/tools/compatibility/ast_edits.py\", line 916, in update_string_pasta\r\nt = pasta.parse(text)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/init.py\", line 25, in parse\r\nannotator.visit(t)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\nsuper(AstAnnotator, self).visit(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\nsuper(BaseVisitor, self).visit(node)\r\nFile \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\nreturn visitor(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 47, in wrapped\r\nf(self, node, *args, **kwargs)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 220, in visit_Module\r\nself.generic_visit(node)\r\nFile \"/usr/lib/python3.6/ast.py\", line 261, in generic_visit\r\nself.visit(item)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\nsuper(AstAnnotator, self).visit(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\nsuper(BaseVisitor, self).visit(node)\r\nFile \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\nreturn visitor(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 95, in wrapped\r\nf(self, node, *args, **kwargs)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 411, in visit_FunctionDef\r\nself.visit(stmt)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\nsuper(AstAnnotator, self).visit(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\nsuper(BaseVisitor, self).visit(node)\r\nFile \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\nreturn visitor(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 95, in wrapped\r\nf(self, node, *args, **kwargs)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 291, in visit_For\r\nself.visit(stmt)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1194, in visit\r\nsuper(AstAnnotator, self).visit(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 132, in visit\r\nsuper(BaseVisitor, self).visit(node)\r\nFile \"/usr/lib/python3.6/ast.py\", line 253, in visit\r\nreturn visitor(node)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 95, in wrapped\r\nf(self, node, *args, **kwargs)\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 290, in visit_For\r\nfor stmt in self.indented(node, 'body'):\r\nFile \"/usr/local/lib/python3.6/dist-packages/pasta/base/annotate.py\", line 1224, in indented\r\n'more than the outer indentation.' % cur_loc[0])\r\npasta.base.annotate.AnnotationError: Indent detection failed (line 300); inner indentation level is not more than the outer indentation.\r\n\r\nThanks,\r\n\r\n", "@GuesmiAbd This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "@GuesmiAbd Were you able to resolve the issue?.Thanks!", "@GuesmiAbd Please double check indentation, used in code. Seems like authors combine tabs with whitespaces, that causing the issue.\r\n\r\n@soupytwist what do you think? ", "Yes, it's a bug in pasta due to mixed tabs + spaces. Opened: https://github.com/google/pasta/issues/70\r\n\r\nThanks for the report!", "@GuesmiAbd \r\nCan we close this issue and track the issue in pasta repository: google/pasta#70.Thanks!\r\n", "We are closing this issue and we can track the issue in  google/pasta#70.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30432\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30432\">No</a>\n", "Thank you for all!\r\nthe problem was solved."]}, {"number": 30431, "title": "Significantly reduced validation accuracy when switching from alpha0 to beta0/1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Code is strongly based on: [Stock example](https://www.tensorflow.org/beta/tutorials/images/transfer_learning ), but uses kaggle aerial-cactus-identification dataset. \r\n- OS Platform and Distribution: Google Colab\r\n- TensorFlow installed from: binary via `pip install tensorflow-gpu==2.0.0-beta0`\r\n- TensorFlow version: 2.0.0-beta0\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: N/A (Google Colab as of 05.07.2019)\r\n- GPU model and memory:  N/A (Google Colab as of 05.07.2019)\r\n\r\n**Describe the current behavior**\r\nWith beta 0 and 1 the training/validation history is as follows:\r\n![Beta 0/1](https://puu.sh/DOY9H/8ec1a1a7f5.png)\r\nThis is a very low validation accuracy. This can of course happen, even with code based on an example, the issue is that this only appears with beta0 and beta1 builds, not with alpha0 (see below) when using the exactly same code and training data.\r\n\r\n**Describe the expected behavior**\r\nWith alpha0 and the same code the history looks as follows:\r\n![Alpha 0](https://puu.sh/DOYss/b14cc7f979.png)\r\n\r\nAn upgrade of the tensorflow version should not affect the resulting accuracy in such a manner.\r\n\r\n**Code to reproduce the issue**\r\n1. Open this [Google Colab](https://colab.research.google.com/drive/14oaC63n1n3yymRyB90uMx3GNvzt-rGMT)\r\n2. Run the code with Alpha0\r\n![Choose Version](https://puu.sh/DOYdz/3c3efe344c.png)\r\n3. Make note of the training history plot.\r\n4. Restart the runtime.\r\n5. Run the code with Beta0 or Beta1\r\n![Choose Version](https://puu.sh/DOYeP/6a24c7c681.png)\r\n6. Make note of the training history plot.\r\n7. Observe that with no change to the code but using the beta0 instead of alpha0 the validation accuracy goes down from > 95% to < 90%, with beta1 even to < 80%\r\n\r\n**Other info / logs**\r\nMy best guess is that changes have been made to the pretrained MobileNetV2 or to the Adam optimizer, otherwise the drastic loss in accuracy is hard to explin.\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0 beta 1 and alpha 0 and was able to reproduce the issue.Thanks!", "Could be related to the changes in trainable flags between alpha and beta; @tanzhenyu , can you take a look?", "I encountered the same issue with beta1 version. I'm also using tf.keras.Model.fit() (but with distributed Dataset)\r\n\r\nHope you guys can get this bug fixed, since people experimenting with tf 2.0 will get unsatisfactory results from their models. \r\n\r\nCheers!\r\n\r\n", "I was able to reproduce this in colab. Will look into it.", "Hi,\r\nI also encountered this issue in a colab from a course demonstrating transfer learning with MobileNetV2.\r\nI ran the same exact code in alpha0 and then in beta1, and am seeing a reduction of 13.6% in validation accuracy in beta1.\r\n[Colab for Alpha0](https://colab.research.google.com/drive/1KszPPoBVwBoAGwgoSMtUjuLZMYRXzvWV#scrollTo=6XOGP0tExZvv)\r\n[Colab for Beta1](https://colab.research.google.com/drive/1-s53y_oAMaa1VCZl7JqpzNCAoVumk1Qj#scrollTo=6XOGP0tExZvv)\r\n\r\nI hope this aids in your diagnosis of the issue.\r\n", "In between these 2 versions, we have changed the semantics of setting `batch_norm.trainable = False` on a `BatchNormalization` layer.\r\n\r\nOld behavior: freeze weights, freeze updates, use training-time behavior during training (i.e. use the batch statistics are used for training-time normalization).\r\n\r\nNew behavior: freeze weights, freeze updates, use inference-time behavior during training (i.e. use the frozen batch statistics gamma and beta).\r\n\r\nIn general we expect the new behavior to work better for fine-tuning use cases. But in any case, it is *different*, so it's expected that you'd see different results.\r\n\r\nNote that you can always manually specify what behavior you want by explicitly passing the `training` argument in `call` for your BatchNormalization layers when building the model you use for training (and changing it when you use a different model that will be for evaluation and prediction only).", "Thanks for the clarification @fchollet, indeed I see the documentation of the change in the comments of the [BatchNormalization](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/normalization_v2.py#L26-L65) class. \r\n\r\nSo, since both @taotsetung and I observed this behavior with the MobileNetV2 model provided by tf.keras.applications.MobileNetV2, I presume this model was trained before the change (the weights throughout the model would have been updated based on the gradients of activations normalized using the training-time batch statistics and now don't \"recognize\" the activations normalized using the new behavior)? Could this explain the drastic drop in validation accuracy we observed and could the solution simply be to re-train the entire base model provided by tf.keras.applications.MobileNetV2 using the new behavior?\r\n\r\nI hope I am understanding this correctly, my apologies if I am way off.\r\n", "Never mind, I realized just after posting that my suggestion made no sense since the behavior when trainable = True did not change. Re-training the entire model (all layers are trainable) would not lead to any different results.", "Based on @fchollet explanation and @AbrahamSanders description, I am closing this issue. However, feel free to open a new issue if you notice similar performance issue with any other models. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30431\">No</a>\n"]}, {"number": 30430, "title": "Batch size affects prediction output in RNN layers (LSTM, GRU)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker\r\n- TensorFlow version (use command below): 1.12.3 and 2.0.0-beta1\r\n- Python version: 2.7.12 and 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe output during prediction from RNN layers, i.e. `LSTM` and `GRU` is dependent on the batch size when `stateful=False`. \r\n\r\n**Describe the expected behavior**\r\nThe predicted output tensor for a given input tensor should always produce the same result regardless of the size of the batch the sample is found in.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import LSTM, GRU\r\n\r\nif __name__ == \"__main__\":\r\n    with tf.device('/CPU:0'):\r\n        batches = [1] + list(range(1, 10))\r\n        shape = (1000, 512)\r\n        inputs = tf.keras.Input(shape=shape)\r\n        rnn = GRU(shape[-1]//2, return_sequences=True)(inputs)\r\n        model = tf.keras.Model(inputs=inputs, outputs=rnn)\r\n        results = []\r\n        for i, batch in enumerate(batches):\r\n            x = tf.ones((batch,) + shape)\r\n            y = model.predict_on_batch(x)\r\n            results.append(y[0])\r\n\r\n        for b, x in list(zip(batches, results))[1:]:\r\n            print(b, np.max(np.abs(results[0] - x)))\r\n        if not all(np.allclose(x, results[0]) for x in results[1:]):\r\n            raise ValueError(\"Varying batch size produces different results\")\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1 and 1.12 and was able to reproduce the issue.Thanks!", "Thanks for reporting the issue, let me take a look.", "I think it is because the numeric underflow when some certain prediction produce a very small number in the result. \r\n\r\nif you change your code with self.assertAllClose(x, results[0]), you will notice that most of time, there is only less than 1% of the result are different, and the diff is actually very small (~1e-5). If you add the atol and rtol to the assertAllClose or np.allclose, the error actually goes away.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30430\">No</a>\n", "If there is underflow, would that just be the particulars of a given computation? Shouldn't it be the same regardless of the batch size? Also, a diff of 1e-5 is not actually that small when the average values we're talking about are less than 1.", "1e-5 is 0.00001 which is quite small compare to 1. \r\n\r\nI think the indeterministic behavior is caused by floating point number, and we actually hit it a lot during unit test, which is why we sometimes have to give the test some atol and rtol.", "Whether or not 1e-5 is significant depends on the application. Setting greater tolerance values only removes errors from comparisons in test cases, it doesn't resolve the issue.\r\nWith respect to this behavior, it is deterministic. If you give the same sample in a batch with the same number of samples you always get the same value. The values returned by inference should not depend on the batch size, unless the other samples in the batch are somehow affecting each other.", "still have the same isssue on tf 2.3"]}, {"number": 30429, "title": "TPU Distribution not working for r1.13", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No, I have used XLNet training\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI tried to use TPUDistribution Strategy to use TPUv2 pods for training XLNet, but it gave me error(mentioned in logs below)\r\n**Describe the expected behavior**\r\nTPU Distribution Strategy should be able to implement this\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nNUM_TPUS = 4\r\ntpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver([os.environ['TPU_NAME']], zone=os.environ['TPU_ZONE'], project=os.environ['GCE_PROJECT_NAME'])\r\ntpu_strategy = tf.contrib.tpu.TPUDistributionStrategy(tpu_cluster_resolver)\r\nrun_config = tf.contrib.tpu.RunConfig(\r\n    cluster=tpu_cluster_resolver,\r\n    model_dir=OUTPUT_DIR,\r\n    save_checkpoints_steps=1000,\r\n    tpu_config=tf.contrib.tpu.TPUConfig(\r\n        iterations_per_loop=1000,\r\n        num_shards=8 * NUM_TPUS,\r\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2),\r\n    keep_checkpoint_max=5,\r\n    train_distribute=tpu_strategy\r\n)\r\n\r\nxlnet_config = xlnet.XLNetConfig(json_path=os.path.join(XLNET_PRETRAINED_DIR, 'xlnet_config.json'))\r\nxlnet_model = run_classifier.get_model_fn(num_classes)\r\nestimator = tf.contrib.tpu.TPUEstimator(\r\n        use_tpu=True,\r\n        model_fn=xlnet_model,\r\n        config=run_config,\r\n        params=None,\r\n        train_batch_size=TRAIN_BATCH_SIZE,\r\n        predict_batch_size=PREDICT_BATCH_SIZE,\r\n        eval_batch_size=EVAL_BATCH_SIZE)\r\n\r\ntrain_input_fn = run_classifier.file_based_input_fn_builder(\r\n    input_file=\"gs://input_file.tfrecord\",\r\n    seq_length=MAX_SEQ_LENGTH,\r\n    is_training=True,\r\n    drop_remainder=True)\r\n\r\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nI0705 07:33:40.742511 139772774643520 tpu_system_metadata.py:59] Querying Tensorflow master (grpc://a.b.c.d:8470) for TPU system metadata.\r\n\r\nINFO:tensorflow:Found TPU system:\r\n\r\nI0705 07:33:40.888471 139772774643520 tpu_system_metadata.py:120] Found TPU system:\r\n\r\nINFO:tensorflow:*** Num TPU Cores: 32\r\n\r\nI0705 07:33:40.890206 139772774643520 tpu_system_metadata.py:121] *** Num TPU Cores: 32\r\n\r\nINFO:tensorflow:*** Num TPU Workers: 4\r\n\r\nI0705 07:33:40.895273 139772774643520 tpu_system_metadata.py:122] *** Num TPU Workers: 4\r\n\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\n\r\nI0705 07:33:40.898422 139772774643520 tpu_system_metadata.py:124] *** Num TPU Cores Per Worker: 8\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4144468757051904050)\r\n\r\nI0705 07:33:40.905382 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4144468757051904050)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8307488279136102617)\r\n\r\nI0705 07:33:40.907173 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8307488279136102617)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15853259856546976898)\r\n\r\nI0705 07:33:40.908568 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15853259856546976898)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15507240626127054220)\r\n\r\nI0705 07:33:40.909704 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15507240626127054220)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9584144648126289353)\r\n\r\nI0705 07:33:40.910813 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9584144648126289353)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1596766444393261122)\r\n\r\nI0705 07:33:40.911922 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1596766444393261122)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 107997388316727553)\r\n\r\nI0705 07:33:40.913051 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 107997388316727553)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5933300785999976036)\r\n\r\nI0705 07:33:40.914163 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5933300785999976036)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17288917258161835028)\r\n\r\nI0705 07:33:40.915283 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17288917258161835028)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10754988339652322123)\r\n\r\nI0705 07:33:40.916407 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10754988339652322123)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 15827474055072985247)\r\n\r\nI0705 07:33:40.917562 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 15827474055072985247)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:CPU:0, CPU, -1, 17772760716915846644)\r\n\r\nI0705 07:33:40.918668 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:CPU:0, CPU, -1, 17772760716915846644)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:XLA_CPU:0, XLA_CPU, 17179869184, 1079066005188293108)\r\n\r\nI0705 07:33:40.919769 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:XLA_CPU:0, XLA_CPU, 17179869184, 1079066005188293108)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:0, TPU, 17179869184, 8833670584752429122)\r\n\r\nI0705 07:33:40.920902 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:0, TPU, 17179869184, 8833670584752429122)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:1, TPU, 17179869184, 9749399796990995666)\r\n\r\nI0705 07:33:40.922011 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:1, TPU, 17179869184, 9749399796990995666)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:2, TPU, 17179869184, 4972353230220870757)\r\n\r\nI0705 07:33:40.923123 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:2, TPU, 17179869184, 4972353230220870757)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:3, TPU, 17179869184, 513286311176106128)\r\n\r\nI0705 07:33:40.924228 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:3, TPU, 17179869184, 513286311176106128)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:4, TPU, 17179869184, 14096714381964779427)\r\n\r\nI0705 07:33:40.925353 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:4, TPU, 17179869184, 14096714381964779427)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:5, TPU, 17179869184, 11601778884065416558)\r\n\r\nI0705 07:33:40.926468 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:5, TPU, 17179869184, 11601778884065416558)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:6, TPU, 17179869184, 5715109243185029245)\r\n\r\nI0705 07:33:40.927602 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:6, TPU, 17179869184, 5715109243185029245)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:7, TPU, 17179869184, 293318216316046063)\r\n\r\nI0705 07:33:40.928760 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:7, TPU, 17179869184, 293318216316046063)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5955036069992402374)\r\n\r\nI0705 07:33:40.929885 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5955036069992402374)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, -1, 6513862482907012835)\r\n\r\nI0705 07:33:40.930993 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, -1, 6513862482907012835)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 17179869184, 7175494438094722165)\r\n\r\nI0705 07:33:40.932108 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 17179869184, 7175494438094722165)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:0, TPU, 17179869184, 14689466335568240094)\r\n\r\nI0705 07:33:40.933240 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:0, TPU, 17179869184, 14689466335568240094)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:1, TPU, 17179869184, 2413636980211343743)\r\n\r\nI0705 07:33:40.934341 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:1, TPU, 17179869184, 2413636980211343743)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:2, TPU, 17179869184, 8959053062706485002)\r\n\r\nI0705 07:33:40.935440 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:2, TPU, 17179869184, 8959053062706485002)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:3, TPU, 17179869184, 13668243215792653265)\r\n\r\nI0705 07:33:40.936621 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:3, TPU, 17179869184, 13668243215792653265)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:4, TPU, 17179869184, 916241737357705146)\r\n\r\nI0705 07:33:40.937787 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:4, TPU, 17179869184, 916241737357705146)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:5, TPU, 17179869184, 9081936198773042571)\r\n\r\nI0705 07:33:40.938918 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:5, TPU, 17179869184, 9081936198773042571)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:6, TPU, 17179869184, 17137058361537129430)\r\n\r\nI0705 07:33:40.940040 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:6, TPU, 17179869184, 17137058361537129430)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:7, TPU, 17179869184, 9871713588844769915)\r\n\r\nI0705 07:33:40.941166 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:7, TPU, 17179869184, 9871713588844769915)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13242834065860745167)\r\n\r\nI0705 07:33:40.942301 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13242834065860745167)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:CPU:0, CPU, -1, 14166663181551736188)\r\n\r\nI0705 07:33:40.943438 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:CPU:0, CPU, -1, 14166663181551736188)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:XLA_CPU:0, XLA_CPU, 17179869184, 2214898881771697800)\r\n\r\nI0705 07:33:40.944600 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:XLA_CPU:0, XLA_CPU, 17179869184, 2214898881771697800)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:0, TPU, 17179869184, 17428311216696528109)\r\n\r\nI0705 07:33:40.945710 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:0, TPU, 17179869184, 17428311216696528109)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:1, TPU, 17179869184, 5022423960481121783)\r\n\r\nI0705 07:33:40.946825 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:1, TPU, 17179869184, 5022423960481121783)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:2, TPU, 17179869184, 3367013141681705422)\r\n\r\nI0705 07:33:40.947974 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:2, TPU, 17179869184, 3367013141681705422)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:3, TPU, 17179869184, 11677911886123920679)\r\n\r\nI0705 07:33:40.949125 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:3, TPU, 17179869184, 11677911886123920679)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:4, TPU, 17179869184, 8941627580085686113)\r\n\r\nI0705 07:33:40.950233 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:4, TPU, 17179869184, 8941627580085686113)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:5, TPU, 17179869184, 3622957282534488438)\r\n\r\nI0705 07:33:40.951513 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:5, TPU, 17179869184, 3622957282534488438)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:6, TPU, 17179869184, 11366405219786765949)\r\n\r\nI0705 07:33:40.952693 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:6, TPU, 17179869184, 11366405219786765949)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:7, TPU, 17179869184, 4659665062672917141)\r\n\r\nI0705 07:33:40.953825 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:7, TPU, 17179869184, 4659665062672917141)\r\n\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16417307345580175404)\r\n\r\nI0705 07:33:40.955019 139772774643520 tpu_system_metadata.py:126] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16417307345580175404)\r\n\r\nINFO:tensorflow:Error recorded from training_loop: 'TPUDistributionStrategy' object has no attribute 'configure'\r\n\r\nI0705 07:33:40.956579 139772774643520 error_handling.py:70] Error recorded from training_loop: 'TPUDistributionStrategy' object has no attribute 'configure'\r\n\r\nINFO:tensorflow:training_loop marked as finished\r\n\r\nI0705 07:33:40.957767 139772774643520 error_handling.py:93] training_loop marked as finished\r\n\r\nWARNING:tensorflow:Reraising captured error\r\n\r\nW0705 07:33:40.958972 139772774643520 tf_logging.py:161] Reraising captured error\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<timed exec> in <module>\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n   2455     finally:\r\n   2456       rendezvous.record_done('training_loop')\r\n-> 2457       rendezvous.raise_errors()\r\n   2458 \r\n   2459   def evaluate(self,\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py in raise_errors(self, timeout_sec)\r\n    126       else:\r\n    127         logging.warn('Reraising captured error')\r\n--> 128         six.reraise(typ, value, traceback)\r\n    129 \r\n    130     for k, (typ, value, traceback) in kept_errors:\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n   2450           steps=steps,\r\n   2451           max_steps=max_steps,\r\n-> 2452           saving_listeners=saving_listeners)\r\n   2453     except Exception:  # pylint: disable=broad-except\r\n   2454       rendezvous.record_error('training_loop', sys.exc_info())\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    356 \r\n    357       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 358       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    359       logging.info('Loss for final step: %s.', loss)\r\n    360       return self\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1120   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1121     if self._train_distribution:\r\n-> 1122       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1123     else:\r\n   1124       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/home/jinamshah/venv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1181       return self\r\n   1182     else:\r\n-> 1183       self._config._train_distribute.configure(self._config.session_config)\r\n   1184       return self._actual_train_model_distributed(\r\n   1185           self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n\r\nAttributeError: 'TPUDistributionStrategy' object has no attribute 'configure'", "comments": ["@jinamshah Can you give us the complete code to reproduce the issue on our end. Thanks!", "@gadagashwini the rest of the code is proprietary. If you can tell me which part you need, I can help with that, but giving you the full code would not possible for me ", "@jinamshah I understand the privacy. Will it be possible to provide minimal code with which we can reproduce the mentioned issue. It will indeed help us to move faster. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30429\">No</a>\n"]}, {"number": 30428, "title": "Add missing closing paren in keras optimizer_v2 docs", "body": "", "comments": []}, {"number": 30427, "title": "FutureWarning: Deprecated numpy API calls in tf.python.framework.dtypes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): not really\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.5 (18F132)\r\n- TensorFlow installed from (source or binary): tensorflow==2.0.0b1 from https://pypi.org/\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nA plenty of FutureWarning errors:\r\n\r\n```\r\n/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tensorflow-2.0.0b1-py3.6-macosx-10.14-x86_64.egg/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/xxx/venv/lib/python3.6/site-packages/tb_nightly-1.14.0a20190603-py3.6.egg/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warnings.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow.python.framework.dtypes\r\n\r\nprint(\"Hello world\")\r\n```", "comments": ["@habernal We have executed your code in Google colab and in Jupyter notebook with TF version\r\n 2.0 beta 1 and numpy version 1.16.4.We did not get any warnings.Please upgrade your numpy version and check whether the warnings still persists.Thanks!", "Thanks, @ravikyram - I've double-checked the installed libraries and found the culprit.\r\n\r\nTF 2.0.b1 comes with numpy dependency 1.16.4 and it shows no warning. So it is correct you couldn't reproduce the bug, neither could I.\r\n\r\nHowever, it turned out that I had numpy 1.17.0rc1 installed in the project (no idea from which library it was linked as I certainly didn't install it by hand).\r\n\r\n**Numpy 1.17.0rc1** is responsible for complaining about these FutureWarnings. Perhaps not urgent to fix for now but once there is an upgrade to np 17, this will show up again, I guess.", "In fact, the numpy dependency is treated differently given the tool you install TF (`pip` versus `setuptools`). Both tested in a clean virtual environment with only these libraries installed at the beginning:\r\n\r\n``\r\npip==19.1.1\r\nsetuptools==41.0.1\r\nwheel==0.33.4\r\n``\r\n\r\n1. Installing TF using pip:\r\n\r\n```\r\n$ pip install tensorflow==2.0.0b1 --no-cache-dir\r\n...\r\nCollecting numpy<2.0,>=1.14.5 (from tensorflow==2.0.0b1)\r\n  Downloading https://files.pythonhosted.org/packages/...cc/numpy-1.16.4-cp36-cp....whl (13.9MB)\r\n...\r\n```\r\n\r\n2. Installing TF as dependendy in `setup.py` containing only:\r\n\r\n```\r\nfrom setuptools import setup\r\nsetup(\r\n    name='tf30427',\r\n    version='0.0.1',\r\n    install_requires=['tensorflow==2.0.0b1'],\r\n)\r\n```\r\n\r\nand then installing as\r\n\r\n```\r\n$ python setup.py install\r\n...\r\nSearching for numpy<2.0,>=1.14.5\r\nReading https://pypi.org/simple/numpy/\r\nDownloading https://files.pythonhosted.org/packages/e...17/numpy-1.17.0rc1-cp36-cp36...25e0\r\nBest match: numpy 1.17.0rc1\r\nProcessing numpy-1.17.0rc1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\r\nInstalling numpy-1.17.0rc1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl to /xxx/venv/lib/python3.6/site-packages\r\nAdding numpy 1.17.0rc1 to easy-install.pth file\r\n...\r\n```\r\n\r\nThey understand the requirement `numpy<2.0,>=1.14.5` differently: pip installs 1.16.4 while setuptools 1.17.0rc1.\r\n\r\nProposed fix:\r\n\r\nChange the dependency to `numpy<1.17,>=1.14.5` to stick with the 1.16 version regardless of the installation procedure.", "And here's the core of the problem: https://github.com/pypa/setuptools/issues/855\r\n\r\nas discovered by others, too: https://stackoverflow.com/q/54796975", "@habernal let me know if we can close this issue since we found the solution. Thanks!", "@habernal I think tensorflow could be updated to make it compatible with numpy 1.17+. Created a PR #30559 for the fix.", "Hello i am having the same error. Can u plese tell me what to do with instruction to solve this problem.Thanks", "@Akeaakar The issue has been fixed and merged into the master branch. In the next release of TF the issue should be gone.", "@Akeaakar And for the time being, `pip install \"numpy<1.17\"` to revert to numpy version 1.16.4", "With Ubuntu19.04, the combination of tensorflow-gpu 1.14.0 (or tensorflow-gpu 1.12.2) with numpy 1.17.0 has the same warnings as well. When change numpy version to 1.16.3, the warnings are gone.", "Or you can supress the warnings using the below code.\r\n\r\n```\r\nimport warnings\r\nimport tensorflow as tf\r\n\r\nwarnings.filterwarnings('ignore')\r\n```\r\n", "> Or you can supress the warnings using the below code.\r\n> \r\n> ```\r\n> import warnings\r\n> import tensorflow as tf\r\n> \r\n> warnings.filterwarnings('ignore')\r\n> ```\r\nseems this doesn't work for me. (numpy1.17.0 python3.7.3)\r\n", "> > Or you can supress the warnings using the below code.\r\n> > ```\r\n> > import warnings\r\n> > import tensorflow as tf\r\n> > \r\n> > warnings.filterwarnings('ignore')\r\n> > ```\r\n> \r\n> seems this doesn't work for me. (numpy1.17.0 python3.7.3)\r\n```\r\nimport warnings\r\nwarnings.filterwarnings('ignore',category=FutureWarning)\r\nimport tensorflow as tf\r\n```\r\nNeeds to be before the import, since that is where the warnings are coming from. Also added a restriction to only effect that warning category instead of silencing all warnings.", "> import warnings\r\n> warnings.filterwarnings('ignore',category=FutureWarning)\r\n> import tensorflow as tf\r\nIt works for me even under a virtual environment\r\n", "I have numpy in ./venv/lib/python3.7/site-packages (1.17.2). In both Jupyter and VS Code, I get the idea. I really dislike the idea of ignoring the warning. Any fixes would be greatly appreciated. ", "> pip install \"numpy<1.17\"\r\n\r\nAfter doing that, system installed `numpy-1.16.5`. But now `import tensorflow` says:\r\n`ImportError: Something is wrong with the numpy installation.`", "numpy: `pip install --upgrade numpy`\r\nkeras: ` conda install -c conda-forge keras`\r\n\r\nUpgrade your numpy and install keras by using the above command in Anaconda prompt.", "Hi, for reference, I got this fixed with these 2 version:\r\n```\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.0.0-rc1\r\n>>> import numpy as np\r\n>>> print(np.__version__)\r\n1.17.2\r\n>>> exit()\r\n```\r\n09/24/19, hope this helps", "Thanks @Namburger . The warnings are gone on 2.0.0-rc1.", "With the latest release of tensorflow (i.e., 2.0.0) and numpy 1.17.2, the warning is back.", "Can you provide a reproducer, @UndeadKernel ? I cannot reproduce:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0'\r\n>>> import numpy as np\r\n>>> np.__version__\r\n'1.17.2'\r\n>>> import tensorflow.python.framework.dtypes\r\n>>> \r\n```\r\n\r\nAlso tried from a file:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(\"TF\", tf.__version__)\r\nprint(\"NP\", np.__version__)\r\n\r\nimport tensorflow.python.framework.dtypes\r\n```\r\n\r\nbut no warnings:\r\n\r\n```\r\n(gh_numpty) mihaimaruseac@ankh:/tmp/gh_numpty$ python test.py \r\nTF 2.0.0\r\nNP 1.17.2\r\n```", "@mihaimaruseac, I can reproduce the problem with both examples of yours. That is, I see the following output:\r\n```\r\n2019-10-14 12:20:56.438892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nTF 2.0.0\r\nNP 1.17.2\r\n```\r\n\r\nI'm using arch. Arch, if I recall correctly, adapts Tensorflow to work with Python 3.7.4. Or is this version of Python already supported by Tensorflow?", "@UndeadKernel I'm running Arch too, but I'm not getting any warnings as of now (they were present earlier). My packages are:\r\n* python-tensorflow-opt-cuda: 2.0.0-2\r\n* python-numpy-openblas: 1.17.2-1", "This is indeed quite strange @rharish101, I have the same exact versions. I tried using `python-numpy` from the official repo and python-numpy-openblas` from the AUR and I see the same FutureWarning messages.", "@UndeadKernel Could you try reinstalling the TensorFlow package? It might have changed in the latest patch (2.0.0-2 as opposed to 2.0.0-1). I remember that I, too, saw those warnings in the last few days, but as of today, they are gone, with the only difference that I updated TensorFlow 9 days ago.", "@rharish101, thank for the suggestion.\r\nI noticed that `tensorboard` was not the same version as `tensorflow`. After reinstalling `tensorboard` (notice that this is \"tensorBoard\" and not \"TensorFlow\"), the warning went away.", "There is an official release of 2.0, you should not need to use the patched versions.", "To which patches are you referring @mihaimaruseac?", "2.0.0-1 and 2.0.0-2 mentioned in above comments\r\n\r\n`pip install tensorflow==2.0.0` should work (might need to upgrade pip to the latest version if you're on an old one, due to the change to be manylinux2010 compliant)", "try\r\n\r\npip3 [pip] install tf-nightly\r\n\r\nthe nightly build of tensorflow seems to have that problem sorted out, best of luck", "for `tensorflow==2.0.0` and `numpy==1.17.4`\r\n\r\nI don't get the error/warning.\r\n\r\nYou can try as:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0'\r\n>>> import numpy as np\r\n>>> np.__version__\r\n'1.17.4'\r\n>>> import tensorflow.python.framework.dtypes\r\n>>>\r\n```\r\n\r\n", "> @Akeaakar And for the time being, `pip install \"numpy<1.17\"` to revert to numpy version 1.16.4\r\n\r\nAnd also for anaconda:\r\n`conda install \"numpy<1.17\"`", "> And for the time being, `pip install \"numpy<1.17\"` to revert to numpy version 1.16.4\r\n\r\nAfter how many hours this was the one that solved my problem. I installed the TF that had the numpy version installed (18.0) and when i reverted it to 1.16.4 it works! thanks\r\n"]}, {"number": 30426, "title": "How to use CMake to add tf which builded by bazel into project?", "body": "Hello, I am confused by importing the tf into my project with CMake,  I compiled the tensorflow with bazel,without GPU support.\r\nI dont know how to write the CMakeLists. I am not so clear about the compiling result of the bazel, there are severl folders, how should I do to use these folders in my CMakeLists ?\r\nHope your reply, Thx :)", "comments": ["tensorflow : 1.14.0, I download it from release, and I use the Windows10. ", "I would recommend starting with CMakeLists found in the tensorflow/contrib/Cmake folder.\r\nThe bazel system might take a while to recreate in CMake if you were to start from scratch... :D", "Thx! I would try this way. \r\nMaybe I should use some preview versions  _(:\u0437)\u2220)_ ,  not the lastest \ud83d\ude2d ", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "So sorry about that, I will notice the problem next time.\r\nwe solve this problem of tensorflow with another solution. It looks like that my partner compile it with only c api. Our project is a Qt project, so we use the .pro file, instead of  cmake.\r\n\r\n**The Details**:\r\nOS: **Windows 10 profession edition(x64)**.\r\nTensorFlow verison: **1.14.0**, compiled from source.\r\n\r\nI compiled it successfully(with the commond ``` bazel.exe build  //tensorflow:libtensorflow_cc.so ```), I just cant use it with cmake. So sorry about  that I cant provide you the error output information because I had delete the CMakeLists that I used.\r\n\r\nif it's possible, could you please provide a CMakeLists template of the project which would use the cpp and tf. \r\n\r\nAnd thanks for your reply! \\(\uffe3\ufe36\uffe3*\\))", "@Junhuan-Peng ,\r\nThank you for the information. Building Tensorflow using CMake is not supported from TF Version, 1.11. Please refer this [link](https://www.tensorflow.org/install/source_windows#tested_build_configurations), for details.", "Closing the issue as Tensorflow could be installed successfully using Bazel."]}]