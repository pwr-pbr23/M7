[{"number": 34700, "title": "std::uniform_int_distribution<int8_t> is undefined in the C++17 standard, but TFLite violates this limitation.", "body": "As [the C++ reference](https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution) mentioned, std::uniform_int_distribution<int8_t> is undefined in the C++17.\r\nTherefore microsoft visual C++ 2017 will give the following build error when the code includes [benchmark_tflite_model.cc #L496]( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L496).\r\n\r\nIn fact, the recent TFLite model benchmark couldn't build on windows as [my CI environment](https://dev.azure.com/mlops/tensorflow/_build/results?buildId=548&view=logs&j=e1e4dfe0-fc62-5ca1-9c02-b15972c8e9c4&t=9da95ac0-03a9-5448-4a9d-063bdd2c2605&l=1058) shows.\r\n\r\n```sh\r\nbazel build -c opt --verbose_failures //tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\n\r\n```\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\random(2401): error C2338: invalid template argument for uniform_int_distribution: N4659 29.6.1.1 [rand.req.genl]/1e requires one of short, int, long, long long, unsigned short, unsigned int, unsigned long, or unsigned long long\r\ntensorflow/lite/tools/benchmark/benchmark_tflite_model.cc(496): note: see reference to class template instantiation 'std::uniform_int_distribution<uint8_t>' being compiled\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.16.27023\\include\\random(2401): error C2338: note: char, signed char, unsigned char, int8_t, and uint8_t are not allowed\r\n```\r\n\r\nWould you like to modify benchmark_tflite_model.cc?\r\n", "comments": ["Would you like to delete this topic? sorry..."]}, {"number": 34699, "title": "Dear Team after training model evaluation precision is nan for lots of catedories and some of having 0 how could i solve it because i am using faster rcnn resnet 101 model", "body": "Dear Team after training model evaluation precision is nan for lots of catedories and some of having 0 how could i solve it because i am using faster rcnn resnet 101 model", "comments": ["@DeveloperRachit ,\r\nCan you share a simple and standalone code to reproduce the issue? also mention Tensorflow version being used.Thanks!\r\n", "yes", "this is my config file faster_rcnn_resnet101.config\"", "model {\r\n  faster_rcnn {\r\n    num_classes: 545\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 350\r\n        width: 350\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_resnet101'\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n first_stage_nms_score_threshold: 0.1\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: true\r\n        dropout_keep_probability: 0.5\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n      }\r\n                                                                                                                            29,5          32%\r\n  second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.009999999776482582\r\n        iou_threshold: 0.6000000238418579\r\n        max_detections_per_class: 100\r\n        max_total_detections: 300\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 2\r\n    optimizer {\r\n        adam_optimizer: {\r\n            learning_rate {\r\n            exponential_decay_learning_rate: {initial_learning_rate:0.0001}\r\n            }\r\n        }\r\n    }\r\n  fine_tune_checkpoint: \"/data/sample-apps/deep_dive_demos/open_images_detection/preprocessing/faster_rcnn_resnet101_coco_2018_01_28/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  batch_queue_capacity: 50\r\n  gradient_clipping_by_norm: 10\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n                                   }\r\n  }\r\n}\r\n\r\neval_config: {\r\n  num_examples:2000\r\n  num_visualizations: 20\r\n\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/data/sample-apps/deep_dive_demos/open_images_detection/preprocessing/record.train\"\r\n  }\r\n  label_map_path: \"/data/sample-apps/deep_dive_demos/open_images_detection/preprocessing/models/research/object_detection/data/oid_v4_label_map.pbtxt\"\r\n}\r\n\r\n\r\neval_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"/data/sample-apps/deep_dive_demos/open_images_detection/preprocessing/record.test\"\r\n  }\r\n  label_map_path: \"/data/sample-apps/deep_dive_demos/open_images_detection/preprocessing/models/research/object_detection/data/oid_v4_label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 2\r\n}\r\n\r\n\r\n", "and rest of i am using train.py which is inside object detection api", "https://github.com/tensorflow/models/blob/master/research/object_detection/legacy/train.py\r\n\r\nthis is above link that i am using to train my datasets", "https://github.com/tensorflow/models/blob/master/research/object_detection/legacy/eval.py", "INFO:tensorflow:global step 258266: loss = 650309252677632.0000 (0.618 sec/step)\r\nINFO:tensorflow:global step 258266: loss = 650309252677632.0000 (0.618 sec/step)\r\nsome time losses is very high what is exact problem \r\ni am using GPU Quadro M5000 with 8GB Ram \r\nand having 32 GB CPU RAM", "@DeveloperRachit When TP+FP=0 : means that all instances were predicted as negative. You can try referring to the issue [here](https://github.com/tensorflow/models/issues/1621).\r\n\r\nYou can follow the tutorial thats mentioned [here1](https://github.com/vijendra1125/Tensorflow_Object_detection_API-Custom_Faster_RCNN) and [here2](https://medium.com/@vijendra1125/tensorflow-api-custom-object-detection-55444de6562d) to learn about object detection as the issue might be that your.\r\n\r\nPlease post these questions in stackoverflow as github is  only meant for bug/performance, build/install, feature request, docs related issues. Thank you!"]}, {"number": 34698, "title": "toco_from_protos: not found - breaking", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18 LTS\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\nPython version: 3.6\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```\r\n    # Read the saved model from disk. Assumes it is downloaded.\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(model_out)\r\n    lite_model = converter.convert()\r\n\r\n    # Format model file paths to store the file in the right directory:\r\n    lite_name = model_name + '.tflite'\r\n    out_path = model_out + '/' + lite_name\r\n\r\n    # Write the converted model to disk:\r\n    open(out_path, \"wb\").write(lite_model)\r\n```\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/[]/[]/[]/v1/[]/models/main.py\", line 124, in convert_lite\r\n    lite_model = converter.convert()\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 446, in convert\r\n    **converter_kwargs)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/[]/env/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n/bin/sh: 1: toco_from_protos: not found\r\n```\r\n\r\nThis happens, even after adding my python bin from the virtualenv to my $PATH variable.\r\n\r\nIt seems that the path to toco is hardcoded and therefore Tensorflow will never look for it anywhere else?\r\n\r\nIt's now not possible to use the converter in a Python virtualenv ( the recommended way to use Tensorflow )\r\n\r\nNote that when activating the virtualenv from the commandline, the toco and toco_from_protos commands do work. It is only when running the Python program with Tensorflow that Tensorflow is not able to find toco_from_protos.\r\n", "comments": ["Assigning this to gargn@ who is more familiar with the context.", "If you installed using `--user` instead of `sudo pip install`, then  you do not have any console commands in your path. You probably need to do `export PATH=$PATH:~/.local/bin` before running the converter. This is a pip oddity, not really a bug of our converter.\r\n\r\nReassigning to @aselle who has more knowledge about this.", "Installed using ```python -m pip install```, without the ```--user``` flag.\r\nAll the required python packages are contained in the virtual env.\r\n\r\nThe virtual environment bin is in the ```PATH``` variable, and the toco as well as toco_from_protos packages are present in this bin directory. ( They are available when called directly from the command line ).\r\n\r\nThe recommend way to install Tensorflow is in a virtualenv, as well as specifically with Python 3.6.", "How are you adding it to your path?", "I could not reproduce this. On linux you shouldn't have to manipulate your path and virtualenv should work. If you copy and paste this into a terminal that is exactly what I did. And it worked w/o any errors.\r\n```\r\nvirtualenv -p python3 ~/py3-for-repro\r\nsource ~/py3-for-repro/bin/activate\r\npip install --upgrade tensorflow==2.0.0\r\ncat > repro_test.py <<EOF;\r\nimport tensorflow as tf\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\r\ndef simple(x):\r\n  return tf.add(x,x)\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([simple.get_concrete_function()])\r\nconverter.convert()\r\nEOF\r\npython repro_test.py\r\n```\r\nThe only guesses I have without more info is you are not sourcing the virtualenv script?\r\n\r\n\r\n", "The ```virtualenv/bin/``` is added to the PATH variable in the row with other binary directories, in the .profile file, on the same linux user account which I also use to run the python program. \r\n\r\nDoes this clear up how it is added to the path?\r\n\r\nThe rest of Tensorflow works successfully, it would not work if I was not sourcing my virtualenv script because Tensorflow is only installed in the virtual environment.\r\n\r\nFor added info: I use ```python -m pip install``` and not the system installed pip to prevent conflicts between system level pip and the python packages in the virtual environment.\r\n\r\nIs there any more specific information that would help?\r\n\r\n", "I'm facing the same issue. Using TensorFlow 2.1 and virtual environments, adding the virtual env to PATH env solves the issue but this is not very handy and seems to broke then environment isolation. The toco_from_protos should be taken from the bin of the environment without the need to specify in the PATH", "I'm having the same issue with `tensorflow-gpu==2.1.0`\r\n\r\nAs a workaround, before converting to tf lite, I'm adding the virtualenv's bin path to the environment's `PATH`\r\n```python\r\n    bin_path = dirname(sys.executable)\r\n\r\n    if 'PATH' in os.environ:\r\n        os.environ['PATH'] += ':' + bin_path\r\n    else:\r\n        os.environ['PATH'] = bin_path\r\n```", "> I'm having the same issue with `tensorflow-gpu==2.1.0`\r\n> \r\n> As a workaround, before converting to tf lite, I'm adding the virtualenv's bin path to the environment's `PATH`\r\n> \r\n> ```python\r\n>     bin_path = dirname(sys.executable)\r\n> \r\n>     if 'PATH' in os.environ:\r\n>         os.environ['PATH'] += ':' + bin_path\r\n>     else:\r\n>         os.environ['PATH'] = bin_path\r\n> ```\r\n\r\nThat also didn't work for me. I'm using visual studio in combination with python 3.8.\r\nThe only thing that seemed to work was copying the file to the solutions folders.\r\n", "I`m facing the issue. My system is macos catalina, tensorflow==1.13.1, 1.14.0 also has been tried. Add the path of toco_from_protos can solve the issue. But if pip install tensorflow will not put the bin to right path, so should the TFLiteConverter call it like this?", "@igorhoogerwoord We see that you are using older version of tensorflow .Many bug have been fixed in latest version. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34698\">No</a>\n"]}, {"number": 34697, "title": "Keras model pickle-able but tf.keras model not pickle-able", "body": "**System information**\r\n- Windows 10 \r\n- Tensorflow 2.0 (CPU)\r\n- joblib 0.14.0\r\n- Python 3.7.5\r\n- Keras 2.3.1\r\n\r\nHello everybody! This is my first post so please forgive me if I have missed something. So I'm trying to use a genetic algorithm to train and evaluate multiple NN architectures so I need to parallelize them on a multi-core CPU. Therefore I have used joblib to try to parallelize this. However, I was stuck on my tf.keras code because it wasn't pickleable. After many hours of debugging I finally realised that the tf.keras models are not pickleable whereas keras models are.\r\n\r\n**Describe the current behavior**\r\nThe code below works but if you replaced keras with tf.keras, there will be an error:\r\n**Could not pickle the task to send it to the workers.**\r\n\r\n**Describe the expected behavior**\r\nMoving forward, tf.keras should be replacing keras and therefore tf.keras should also be pickleable.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n#The following is a simple code to illustrate the problem:\r\nfrom joblib import Parallel, delayed\r\nimport keras\r\nimport tensorflow as tf\r\n\r\ndef test():\r\n    model = keras.models.Sequential()\r\n    return\r\n\r\nParallel(n_jobs=8)(delayed(test)(i) for i in range(10)) #this works as intended\r\n\r\ndef test_tf():\r\n    model = tf.keras.models.Sequential()\r\n    return\r\n\r\nParallel(n_jobs=8)(delayed(test_tf)(i) for i in range(10)) #this will spit out the error above\r\n```\r\n\r\n**Other comments**\r\nI guess a quick fix would just be to replace all the existing code with tf.keras to just keras but seeing as keras support will be discontinued and absorbed by Tensorflow 2.0, I think this should be fixed.\r\n", "comments": ["@Edwin-Koh1 \r\n\r\nCan you please check with nightly version(`!pip install tf-nightly==2.1.0dev20191201 `) and see if the error still persists. There are lot of performance improvements in latest nightly versions. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@ravikyram I'm still seeing this issue on tensorflow==2.1.0:\r\n```python\r\nimport pickle\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    model_1 = tf.keras.Sequential((\r\n        tf.keras.layers.Dense(16, activation='relu'),\r\n        tf.keras.layers.Dense(1, activation='linear'),\r\n    ))\r\n\r\n    _ = model_1(tf.random.uniform((15, 3)))\r\n\r\n    model_2 = pickle.loads(pickle.dumps(model_1))\r\n\r\n    for w1, w2 in zip(model_1.get_weights(), model_2.get_weights()):\r\n        tf.debugging.assert_equal(w1, w2)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nresults in \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/hartikainen/github/rail-berkeley/softlearning-3/tests/test_pickle_keras_model.py\", line 25, in <module>\r\n    main()\r\n  File \"/Users/hartikainen/github/rail-berkeley/softlearning-3/tests/test_pickle_keras_model.py\", line 18, in main\r\n    model_2 = pickle.loads(pickle.dumps(model_1))\r\nTypeError: can't pickle weakref objects\r\n```\r\n\r\n```\r\n$ pip freeze | grep \"tf\\|tensor\"\r\ntensorboard==2.1.0\r\ntensorflow==2.1.0\r\ntensorflow-estimator==2.1.0\r\ntensorflow-probability==0.9.0\r\n$ python --version\r\nPython 3.7.5\r\n```", "I have tried on colab with TF version 2.1.0-rc2, 2.2.0-dev20200113 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9e8710dd7372a766ae81dd49fc3e461e/untitled553.ipynb). Thanks!", "@ravikyram, should keras functional models be picklable too or not? I'd assume if Sequential models are then functional models should too be? Or does functional models have some properties that make them harder to pickle?\r\n\r\n```\r\n$ python -m tests.test_pickle_keras_functional_model\r\n2020-01-17 16:47:08.567598: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-17 16:47:08.581327: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa0a55aa6c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-17 16:47:08.581362: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/hartikainen/github/rail-berkeley/softlearning-3/tests/test_pickle_keras_functional_model.py\", line 20, in <module>\r\n    main()\r\n  File \"/Users/hartikainen/github/rail-berkeley/softlearning-3/tests/test_pickle_keras_functional_model.py\", line 13, in main\r\n    model_2 = pickle.loads(pickle.dumps(model_1))\r\nTypeError: can't pickle _thread.RLock objects\r\n```", "Hi everyone,\r\nI'm trying to switch from standalone `keras` to `tensorflow.keras` as per the recommendation at https://keras.io/.\r\nI'm hitting the same exception as https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-575705599 with `joblib` (which uses `pickle` under the hood).\r\n\r\n**System information:**\r\n- Debian 10 (buster)\r\n- Python 3.7.6\r\n- joblib 0.14.1\r\n- tensorflow 2.1.0\r\n\r\n**Script to reproduce:**\r\n```py\r\nimport joblib\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(1, input_dim=42, activation='sigmoid'))\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\njoblib.dump(model, 'model.pkl')\r\n```\r\n\r\n**Output:**\r\n```py\r\nTypeError: can't pickle _thread.RLock objects\r\n```", "Here's a fix adapted from http://zachmoshe.com/2017/04/03/pickling-keras-models.html intended for solving the same issue back when Keras models used to not be pickleable.\r\n\r\n```python\r\nimport pickle\r\nimport tempfile\r\nfrom tensorflow.keras.models import Sequential, load_model, save_model, Model\r\nfrom tensorflow.keras.layers import Dense\r\n\r\n# Hotfix function\r\ndef make_keras_picklable():\r\n    def __getstate__(self):\r\n        model_str = \"\"\r\n        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\r\n            save_model(self, fd.name, overwrite=True)\r\n            model_str = fd.read()\r\n        d = {'model_str': model_str}\r\n        return d\r\n\r\n    def __setstate__(self, state):\r\n        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\r\n            fd.write(state['model_str'])\r\n            fd.flush()\r\n            model = load_model(fd.name)\r\n        self.__dict__ = model.__dict__\r\n\r\n\r\n    cls = Model\r\n    cls.__getstate__ = __getstate__\r\n    cls.__setstate__ = __setstate__\r\n\r\n# Run the function\r\nmake_keras_picklable()\r\n\r\n# Create the model\r\nmodel = Sequential()\r\nmodel.add(Dense(1, input_dim=42, activation='sigmoid'))\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n# Save\r\nwith open('model.pkl', 'wb') as f:\r\n    pickle.dump(model, f)\r\n```", "@epetrovski Should I call this code whenever I'm about to pickle a model or can I just call it at the beginning of my application (before creating the model)?", "> @epetrovski Should I call this code whenever I'm about to pickle a model or can I just call it at the beginning of my application (before creating the model)?\r\n\r\nYou can definitely just call it once at the beginning of your app after importing `tensorflow.keras.models.Model`. Executing the function adds two new methods `__getstate__()`and `__setstate__()` to the `tensorflow.keras.models.Model` class so it should work every time you want to pickle a member of the updated tf.keras Model class - ie. your own model.", "Here is an alternative to @epetrovski 's answer that does not require saving to a file:\r\n\r\n```python3\r\nimport pickle\r\n\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.python.keras.layers import deserialize, serialize\r\nfrom tensorflow.python.keras.saving import saving_utils\r\n\r\n\r\ndef unpack(model, training_config, weights):\r\n    restored_model = deserialize(model)\r\n    if training_config is not None:\r\n        restored_model.compile(\r\n            **saving_utils.compile_args_from_training_config(\r\n                training_config\r\n            )\r\n        )\r\n    restored_model.set_weights(weights)\r\n    return restored_model\r\n\r\n# Hotfix function\r\ndef make_keras_picklable():\r\n\r\n    def __reduce__(self):\r\n        model_metadata = saving_utils.model_metadata(self)\r\n        training_config = model_metadata.get(\"training_config\", None)\r\n        model = serialize(self)\r\n        weights = self.get_weights()\r\n        return (unpack, (model, training_config, weights))\r\n\r\n    cls = Model\r\n    cls.__reduce__ = __reduce__\r\n\r\n# Run the function\r\nmake_keras_picklable()\r\n\r\n# Create the model\r\nmodel = Sequential()\r\nmodel.add(Dense(1, input_dim=42, activation='sigmoid'))\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\n# Save\r\nwith open('model.pkl', 'wb') as f:\r\n    pickle.dump(model, f)\r\n```\r\n\r\nSource: https://docs.python.org/3/library/pickle.html#object.__reduce__\r\n\r\nI feel like maybe this could be added to Model? Are there any cases where this would not work?", "It seems that there are two attributes that are not pickable in Sequential class. This fix also worked for me: \r\n\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nclass PickableSequential(Sequential):\r\n    def __getstate__(self):\r\n        state = super().__getstate__()\r\n        state.pop(\"_trackable_saver\")\r\n        state.pop(\"_compiled_trainable_state\")\r\n        return state\r\n\r\nmodel = PickableSequential(Dense(10))\r\n\r\nimport pickle\r\n\r\npickle.dumps(model)\r\n~                     \r\n```", "I have tried in colab with TF version 2.2, nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/54b83c2f25a8c8fcbae0865f97c212ef/untitled31.ipynb).Thanks!", "> I have tried in colab with TF version 2.2, nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/54b83c2f25a8c8fcbae0865f97c212ef/untitled31.ipynb).Thanks!\r\n\r\n\r\n\r\nkeras model is pickable but tf.keras is not pickable , so the alternative solution for this is refer the below code:-\r\nI saw your colab notebook and made the required changes just copy the same code as below and you are done with resolving the error\r\n\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    model_1 = tf.keras.Sequential((\r\n        tf.keras.layers.Dense(16, activation='relu'),\r\n        tf.keras.layers.Dense(1, activation='linear'),\r\n    ))\r\n\r\n    _ = model_1(tf.random.uniform((15, 3)))\r\n    model_1.save('model_2.h5')\r\n    model_2 = tf.keras.models.load_model('model_2.h5')\r\n\r\n    for w1, w2 in zip(model_1.get_weights(), model_2.get_weights()):\r\n        tf.debugging.assert_equal(w1, w2)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()", "@Edwin-Koh1 \r\n\r\nAs per the suggestion from @lahsrahtidnap i have tried in colab and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/65ad581e74a22866f9078a7e0e34c3ef/untitled40.ipynb).Thanks!", "> Hi everyone,\r\n> I'm trying to switch from standalone `keras` to `tensorflow.keras` as per the recommendation at https://keras.io/.\r\n> I'm hitting the same exception as [#34697 (comment)](https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-575705599) with `joblib` (which uses `pickle` under the hood).\r\n> \r\n> **System information:**\r\n> \r\n> * Debian 10 (buster)\r\n> * Python 3.7.6\r\n> * joblib 0.14.1\r\n> * tensorflow 2.1.0\r\n> \r\n> **Script to reproduce:**\r\n> \r\n> ```python\r\n> import joblib\r\n> from tensorflow.keras.models import Sequential\r\n> from tensorflow.keras.layers import Dense\r\n> \r\n> model = Sequential()\r\n> model.add(Dense(1, input_dim=42, activation='sigmoid'))\r\n> model.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n> joblib.dump(model, 'model.pkl')\r\n> ```\r\n> \r\n> **Output:**\r\n> \r\n> ```python\r\n> TypeError: can't pickle _thread.RLock objects\r\n> ```\r\n\r\nUsing pickle or joblib will not solve your problem as tensorflow.keras dosen't support this .\r\nSo the alternative solution for this is : -\r\n\r\nconsidering your code:-\r\nreplace this line : - joblib.dump(model, 'model.pkl')\r\nwith: -\r\nto save the model use: -\r\n----->   model.save('new_model.h5')\r\nand if you want to load this model use : -\r\n----->   new_model = tf.keras.models.load_model('new_model.h5')", "> considering your code:-\r\n> replace this line : - joblib.dump(model, 'model.pkl')\r\n> with: -\r\n> to save the model use: -\r\n> -----> model.save('new_model.h5')\r\n> and if you want to load this model use : -\r\n> -----> new_model = tf.keras.models.load_model('new_model.h5')\r\n\r\nThis works in some cases, however, it does not help when a model is pickled as part of another function, in my case this happens when using the python ```multiprocessing``` library.", "@Edwin-Koh1 \r\n\r\nIs this still an issue?\r\nPlease, confirm.Thanks!", "Is it possible to dump Keras Sequential Model in byteIO container\r\n```\r\nbytes_container = BytesIO()\r\njoblib.dump(Keras_model, bytes_container, protocol=4)\r\n# Error\r\nTypeError: can't pickle _thread.RLock objects\r\n\r\npickle.dump(Keras_model, bytes_container, protocol=4)\r\n# Error\r\nTypeError: can't pickle _thread.RLock objects\r\n\r\ndill.dump(Keras_model, bytes_container, protocol=4)\r\n# Error\r\nTypeError: can't pickle tensorflow.python._tf_stack.StackSummary objects\r\n```\r\nor in a tempfile\r\n```\r\ntempfile.TemporaryFile().write(Keras_model)\r\n```\r\nor\r\n```\r\nsave_model(Keras_model, bytes_container)\r\n# Error\r\nTypeError: expected str, bytes or os.PathLike object, not _io.BytesIO\r\n```\r\n", "This worked perfectly, well you may not need to base64, for me to store in the database I did,everything in-memory, no touching disk\r\n```\r\nfrom io import BytesIO\r\nimport dill,base64,tempfile\r\n\r\n#Saving Model as base64\r\nmodel_json = Keras_model.to_json()\r\n\r\ndef Base64Converter(ObjectFile):\r\n    bytes_container = BytesIO()\r\n    dill.dump(ObjectFile, bytes_container)\r\n    bytes_container.seek(0)\r\n    bytes_file = bytes_container.read()\r\n    base64File = base64.b64encode(bytes_file)\r\n    return base64File\r\n\r\nbase64KModelJson = Base64Converter(model_json)  \r\nbase64KModelJsonWeights = Base64Converter(Keras_model.get_weights())  \r\n\r\n#Loading Back\r\nfrom joblib import load\r\nfrom keras.models import model_from_json\r\ndef ObjectConverter(base64_File):\r\n    loaded_binary = base64.b64decode(base64_File)\r\n    loaded_object = tempfile.TemporaryFile()\r\n    loaded_object.write(loaded_binary)\r\n    loaded_object.seek(0)\r\n    ObjectFile = load(loaded_object)\r\n    loaded_object.close()\r\n    return ObjectFile\r\n\r\nmodeljson = ObjectConverter(base64KModelJson)\r\nmodelweights = ObjectConverter(base64KModelJsonWeights)\r\nloaded_model = model_from_json(modeljson)\r\nloaded_model.set_weights(modelweights)\r\n\r\n```\r\n", "@hanzigs \r\nThis is a nice solution, thanks. Only be careful if you plan to continue training this model, as this method does not preserve the optimizer state.", "@JohannesAck \r\nYes, we have to compile with the optimizer before doing fit with new data, that shouldn't be time consuming,\r\n\r\nThe other way model.save is very hard to store in-memory.\r\n\r\nAnother way is we can do get_config() and from_config() with initialization and compile then fit has to be done for new data.", "@Edwin-Koh1 \r\n\r\nAny update on this issue please.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> \r\n> \r\n> > considering your code:-\r\n> > replace this line : - joblib.dump(model, 'model.pkl')\r\n> > with: -\r\n> > to save the model use: -\r\n> > -----> model.save('new_model.h5')\r\n> > and if you want to load this model use : -\r\n> > -----> new_model = tf.keras.models.load_model('new_model.h5')\r\n> \r\n> This works in some cases, however, it does not help when a model is pickled as part of another function, in my case this happens when using the python `multiprocessing` library.\r\n\r\n@JohannesAck, I think I might have a similar issue. I train a Keras model on GPU, save it using the TensorFlow SavedModel format using the Keras API, reload it in a new session und try to make predictions in parallel on multiple CPUs using the `multiprocessing` library and the `starmap` function. If I load the model before parallelizing predictions I get a pickling error (`TypeError: can't pickle _thread.RLock objects`). If I load the model within my prediction function each time and delete it at the end of each function call it hangs after a couple of predictions. Do you have any idea what might be going on here?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697\">No</a>\n", "This is not stale as far as I know.\n\nOn Tue, Aug 25, 2020, 5:26 AM tensorflow-butler[bot] <\nnotifications@github.com> wrote:\n\n> Are you satisfied with the resolution of your issue?\n> Yes\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697>\n> No\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-679941192>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANMPP2EF3PRKBYRYHOHY3TSCOGTXANCNFSM4JSZ4QSA>\n> .\n>\n", "Is this a WONTFIX after the issue has been closed now?\r\n\r\nIn my case I can't simply use `model.save()` because pickling is performed from an external tool to which my code merely provides a scikit-learn compatible model (my class provides a `get_clf` method). I could work around the issue as my code was (almost) Keras-compatible (not tf.keras), and with Keras 2.3.1 (TF 1.15.0) pickling works without issue.", "@mimxrt  if you are looking to use Keras models within a scikit-learn env, please check out [SciKeras](https://github.com/adriangb/scikeras) (full disclosure: I am the author). If you are just looking for a way to make Keras objects pickable, check https://github.com/tensorflow/tensorflow/pull/39609 and in particular https://github.com/tensorflow/tensorflow/pull/39609#issuecomment-683370566\r\n\r\nEdit: fixed link", "We are not actively working on this right now, but reopening since it is still an issue. ", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697\">No</a>\n", "This will get stalled again.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "What is the status of this issue? is there going to be a fix?", "> Here's a fix adapted from http://zachmoshe.com/2017/04/03/pickling-keras-models.html intended for solving the same issue back when Keras models used to not be pickleable.\r\n> \r\n> ```python\r\n> import pickle\r\n> import tempfile\r\n> from tensorflow.keras.models import Sequential, load_model, save_model, Model\r\n> from tensorflow.keras.layers import Dense\r\n> \r\n> # Hotfix function\r\n> def make_keras_picklable():\r\n>     def __getstate__(self):\r\n>         model_str = \"\"\r\n>         with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\r\n>             save_model(self, fd.name, overwrite=True)\r\n>             model_str = fd.read()\r\n>         d = {'model_str': model_str}\r\n>         return d\r\n> \r\n>     def __setstate__(self, state):\r\n>         with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\r\n>             fd.write(state['model_str'])\r\n>             fd.flush()\r\n>             model = load_model(fd.name)\r\n>         self.__dict__ = model.__dict__\r\n> \r\n> \r\n>     cls = Model\r\n>     cls.__getstate__ = __getstate__\r\n>     cls.__setstate__ = __setstate__\r\n> \r\n> # Run the function\r\n> make_keras_picklable()\r\n> \r\n> # Create the model\r\n> model = Sequential()\r\n> model.add(Dense(1, input_dim=42, activation='sigmoid'))\r\n> model.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n> \r\n> # Save\r\n> with open('model.pkl', 'wb') as f:\r\n>     pickle.dump(model, f)\r\n> ```\r\n\r\nOSError: Unable to create file (unable to open file: name = 'C:\\Users\\karti\\AppData\\Local\\Temp\\tmppjgdsxh4.hdf5', errno = 13, error message = 'Permission denied', flags = 13, o_flags = 302)\r\ngetting this error after following these steps", "> Here is an alternative to @epetrovski 's answer that does not require saving to a file:\r\n> \r\n> ```python\r\n> import pickle\r\n> from tensorflow.keras.models import Sequential, Model\r\n> from tensorflow.keras.layers import Dense\r\n> from tensorflow.python.keras.layers import deserialize, serialize\r\n> from tensorflow.python.keras.saving import saving_utils\r\n> def unpack(model, training_config, weights):\r\n>     restored_model = deserialize(model)\r\n>     if training_config is not None:\r\n>         restored_model.compile(\r\n>             **saving_utils.compile_args_from_training_config(\r\n>                 training_config\r\n>             )\r\n>         )\r\n>     restored_model.set_weights(weights)\r\n>     return restored_model\r\n> # Hotfix function\r\n> def make_keras_picklable():\r\n>     def __reduce__(self):\r\n>         model_metadata = saving_utils.model_metadata(self)\r\n>         training_config = model_metadata.get(\"training_config\", None)\r\n>         model = serialize(self)\r\n>         weights = self.get_weights()\r\n>         return (unpack, (model, training_config, weights))\r\n>     cls = Model\r\n>     cls.__reduce__ = __reduce__\r\n> # Run the function\r\n> make_keras_picklable()\r\n> # Create the model\r\n> model = Sequential()\r\n> model.add(Dense(1, input_dim=42, activation='sigmoid'))\r\n> model.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n> # Save\r\n> with open('model.pkl', 'wb') as f:\r\n>     pickle.dump(model, f)\r\n> ```\r\n> Source: https://docs.python.org/3/library/pickle.html#object.__reduce__\r\n> I feel like maybe this could be added to Model? Are there any cases where this would not work?\r\n\r\n\r\nThis works fine for me locally thank you, but when I deploy it through docker container then publishing the model to azure it throws this error that I don't know its cause:\r\nCan't get attribute 'unpack' on <module 'main' from '/opt/python/3.7.9/bin/gunicorn'>\r\n\r\nRequirements\r\nTensorFlow: 2.4\r\nKeras: 2.4.3", "> > Here is an alternative to @epetrovski 's answer that does not require saving to a file:\r\n> > ```python\r\n> > import pickle\r\n> > ```\r\n> \r\n> > from tensorflow.keras.models import Sequential, Model\r\n> > from tensorflow.keras.layers import Dense\r\n> > from tensorflow.python.keras.layers import deserialize, serialize\r\n> > from tensorflow.python.keras.saving import saving_utils\r\n> > def unpack(model, training_config, weights):\r\n> > restored_model = deserialize(model)\r\n> > if training_config is not None:\r\n> > restored_model.compile(\r\n> > **saving_utils.compile_args_from_training_config(\r\n> > training_config\r\n> > )\r\n> > )\r\n> > restored_model.set_weights(weights)\r\n> > return restored_model\r\n> > # Hotfix function\r\n> > def make_keras_picklable():\r\n> > ```\r\n> > def __reduce__(self):\r\n> >     model_metadata = saving_utils.model_metadata(self)\r\n> >     training_config = model_metadata.get(\"training_config\", None)\r\n> >     model = serialize(self)\r\n> >     weights = self.get_weights()\r\n> >     return (unpack, (model, training_config, weights))\r\n> > \r\n> > cls = Model\r\n> > cls.__reduce__ = __reduce__\r\n> > ```\r\n> > \r\n> > \r\n> > # Run the function\r\n> > make_keras_picklable()\r\n> > # Create the model\r\n> > model = Sequential()\r\n> > model.add(Dense(1, input_dim=42, activation='sigmoid'))\r\n> > model.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n> > # Save\r\n> > with open('model.pkl', 'wb') as f:\r\n> > pickle.dump(model, f)\r\n> > ```\r\n> > \r\n> > Source: https://docs.python.org/3/library/pickle.html#object.__reduce__\r\n> > \r\n> > I feel like maybe this could be added to Model? Are there any cases where this would not work?\r\n> > ```\r\n> \r\n> This works fine for me locally thank you, but when I deploy it through docker container then publishing the model to azure it throws this error that I don't know its cause:\r\n> Can't get attribute 'unpack' on <module 'main' from '/opt/python/3.7.9/bin/gunicorn'>\r\n> \r\n> Requirements\r\n> TensorFlow: 2.4\r\n> Keras: 2.4.3\r\n\r\nYes I also try this thing, and when I try to run my flask app.py it can't load the model and show the same error:\r\nCan't get attribute 'unpack' on <module 'main' from '/opt/python/3.7.9/bin/gunicorn'\r\nI \r\n\r\n", "I'm not sure what might be going on with Flask/Gunicorn, maybe posting a minimal reproducible example might help? An updated version of the \"hack\" in https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-627193883 might also help: https://github.com/adriangb/scikeras/blob/master/scikeras/_saving_utils.py", "The below code could be run successfully:\r\n\r\n```python\r\nimport joblib\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(1, input_dim=42, activation='sigmoid'))\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\njoblib.dump(model, 'model.pkl')\r\n```\r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/1c7881f8edc78a0f964775d517c54cca/gh_34697.ipynb) of the working code. \r\n\r\nPlease let me know if that can be considered as issue being resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rmothukuru have you tried running  `joblib.load('model.pkl')`? I am getting this error:\r\n\r\nTypeError: '<' not supported between instances of 'InputLayer' and 'Sequential'\r\n\r\nversion:\r\njoblib                  1.0.1\r\ntensorflow           2.4.1", "I tried on colab with  TF v2.5 and faced different error,please  find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ae213fa046b7653559d174fc6b518e09/untitled553.ipynb#scrollTo=8-Va2TRZY6-b)..Thanks !", "I still run into this issue with TF 2.5. For example, the following code raises the exception `TypeError: can't pickle weakref objects` (see stacktrace below):\r\n\r\n```python\r\nimport joblib\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nmodel = Sequential([Dense(1, input_shape=[42], activation='sigmoid')])\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\njoblib.dump(model, 'model.pkl')\r\n```\r\n\r\n<details>\r\n<summary>Stacktrace</summary>\r\n\r\n```stacktrace\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-2f1f59ec2087> in <module>()\r\n      6 model.add(Dense(1, input_dim=42, activation='sigmoid'))\r\n      7 model.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n----> 8 joblib.dump(model, 'model.pkl')\r\n\r\n21 frames\r\n/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)\r\n    522             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    523             if reduce is not None:\r\n--> 524                 rv = reduce(self.proto)\r\n    525             else:\r\n    526                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\nTypeError: can't pickle weakref objects\r\n```\r\n</details>\r\n", "There's a different error on TF 2.7.0.dev20210702:\r\nThe code posted by ageron just above leads to `TypeError: cannot pickle '_thread.RLock' object`\r\n\r\n<details><summary>Stacktrace</summary>\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/lukas/Desktop/EDEO-Sensing/untitled0.py\", line 15, in <module>\r\n    joblib.dump(model, 'model.pkl')\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 480, in dump\r\n    NumpyPickler(f, protocol=protocol).dump(value)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 487, in dump\r\n    self.save(obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 931, in save_list\r\n    self._batch_appends(obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 955, in _batch_appends\r\n    save(x)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 931, in save_list\r\n    self._batch_appends(obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 958, in _batch_appends\r\n    save(tmp[0])\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/site-packages/joblib/numpy_pickle.py\", line 282, in save\r\n    return Pickler.save(self, obj)\r\n\r\n  File \"/home/lukas/anaconda3/lib/python3.8/pickle.py\", line 578, in save\r\n    rv = reduce(self.proto)\r\n\r\nTypeError: cannot pickle '_thread.RLock' object\r\n```\r\n</details>", "I am trying to cave callback objects instead of models and getting the same error.", "@Edwin-Koh1 Could you please refer to the [link1](https://github.com/QData/TextAttack/issues/499) ,[link2](https://github.com/CognitiveScale/certifai-reference-models/pull/61) and let us know if it helps ? Thanks!", "Hey all, I believe the issue was resolved by @adriangb in [#39609](https://github.com/tensorflow/tensorflow/pull/39609) and the keras pull [#14748](https://github.com/keras-team/keras/pull/14748). Should be fixed in Keras and Tensorflow 2.6.0 but have not tested this yet.\r\n", "@Edwin-Koh1 , indeed, the problem seems fixed in TF 2.6.0! \ud83d\udc4d \r\n\r\nThis now works (while it failed in TF 2.5.1):\r\n\r\n```python\r\nimport joblib\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nmodel = Sequential([Dense(1, input_shape=[42], activation='sigmoid')])\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\njoblib.dump(model, 'model.pkl')\r\n```", "Glad we could fix it for you @ageron! It should also work with Functional models by the way.\r\n\r\n@Edwin-Koh1 should we close the issue?", "It works with joblib in TF 2.6.0, but not with pickle, however:\r\n\r\n```python\r\nimport pickle\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nmodel = Sequential([Dense(1, input_shape=[42], activation='sigmoid')])\r\nmodel.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\nwith open('model.pkl', 'wb') as f:\r\n  pickle.dump(model, f)\r\n```\r\n\r\nRaises:\r\n\r\n```exception\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-e391f4844d65> in <module>()\r\n      6 model.compile(optimizer='Nadam', loss='binary_crossentropy', metrics=['accuracy'])\r\n      7 with open('model.pkl', 'wb') as f:\r\n----> 8   pickle.dump(model, f)\r\n\r\nTypeError: can't pickle weakref objects\r\n```", "Can you try with `tf-nightly`? I'm wondering if the fix was actually _not_ in 2.6.0 and your example just happens to be broken in `2.5.1`. In [this colab notebook](https://colab.research.google.com/drive/1IEz5nNQo32qaS0Wi9lfyOqviOknrXKe-?usp=sharing), on `tf-nightly`, your example runs fine.", "@Edwin-Koh1 Could you please let us know if this issue is resolved for you ? If it is resolved please feel free to move this ticket to closed status .Thank you!", "@adriangb you're right, both the joblib and pickle code examples work with tf-nightly (while only the joblib example worked in 2.6.0). \ud83d\udc4d ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34697\">No</a>\n", "> This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\r\n\r\nIssue resolved"]}, {"number": 34696, "title": "Attention of the value_embeddings inputs", "body": "This [line](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/layers/dense_attention.py#L241) and this [one](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/layers/dense_attention.py#L368) should be:\r\n```\r\nvalue_embeddings = token_embedding(value_input)\r\n```\r\n", "comments": ["Similar issue [#34283](https://github.com/tensorflow/tensorflow/issues/34283). Thanks!", "This is fixed now. Thanks!"]}, {"number": 34695, "title": "Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "I have TF 2.0 on Ubuntu 18.04 LTS.\r\nMy GPU is the Nvidia RTX 2070 with driver 440.33.01.\r\nWhen I train a model everything is fine and it works correctly.\r\nBut after the model is saved as .h5 file and I load it back, I cannot predict anything and I got this log:\r\n\r\n```\r\n`Using TensorFlow backend.\r\n2019-11-29 00:59:27.178674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-29 00:59:27.220014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.220381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2019-11-29 00:59:27.222887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-29 00:59:27.259558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-29 00:59:27.278651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-29 00:59:27.284736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-29 00:59:27.330947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-29 00:59:27.359948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-29 00:59:27.426684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-29 00:59:27.426784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.427108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.427373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-29 00:59:27.427889: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-29 00:59:27.457927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\r\n2019-11-29 00:59:27.460601: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5169da0 executing computations on platform Host. Devices:\r\n2019-11-29 00:59:27.460654: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-29 00:59:27.567015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.567342: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x51fc6f0 executing computations on platform CUDA. Devices:\r\n2019-11-29 00:59:27.567353: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5\r\n2019-11-29 00:59:27.567459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.567704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2019-11-29 00:59:27.567727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-29 00:59:27.567735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-29 00:59:27.567743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-29 00:59:27.567749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-29 00:59:27.567756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-29 00:59:27.567763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-29 00:59:27.567770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-29 00:59:27.567801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.568059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.568291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-29 00:59:27.568720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-29 00:59:27.569778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-29 00:59:27.569787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-29 00:59:27.569790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-29 00:59:27.570517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.570901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-29 00:59:27.571160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7225 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-11-29 00:59:30.035047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-29 00:59:30.389311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-29 00:59:31.306557: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-29 00:59:31.307242: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-11-29 00:59:31.307791: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]`\r\n\r\n```\r\n\r\nOn Windows 10 it works correctly with the same loaded model. I tried also to downgrade TF to v. 1.14 but nothing changed.\r\nI installed cuda with aptitude. The strange thing is that I can train a model but not load it.\r\nThis is my test code for load and test a model:\r\n\r\n\r\n\r\n```\r\nfrom keras.models import load_model\r\n\r\nimport cv2\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodelN = load_model('../model/nvidiaModel.h5')\r\nwith open(\"../data/images/1574517989418963.jpg\", \"rb\") as f:\r\n    data = f.read()\r\n\r\n\r\ndef image_preprocess(image):\r\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\r\n    image = cv2.GaussianBlur(image, (3, 3), 0)\r\n    image = cv2.resize(image, (200, 66))  # Image input size of the Nvidia model architecture\r\n    image = (image / 127.5) - 1\r\n\r\n    return image\r\n\r\n\r\nimage = Image.open(BytesIO(bytearray(data)))\r\nimage = np.asarray(image)\r\nimage = image_preprocess(image)\r\nimage = np.array([image])\r\nsteering_angle = str(modelN.predict_classes(image))\r\nprint(steering_angle)\r\n```", "comments": ["@xXNicolaXx \r\n\r\nCan you help us with the supporting files to reproduce the issue in our environment. Thanks!", "> @xXNicolaXx\r\n> \r\n> Can you help us with the supporting files to reproduce the issue in our environment. Thanks!\r\n\r\nYeah, for sure! Thank you!\r\nThis is my project link: [https://github.com/xXNicolaXx/AutoPilotServer/](url)\r\nUnder the \"app\" package, you can find the class testModel.py which load my trained model and try to predic a value taking an image from my \"data\" folder.\r\nUnder the \"model\" package, there is the file for start the training (keras_model.py).\r\nIf you need something else I will be glad to do everything possible for solving this issue!\r\nThanks again.", "@xXNicolaXx \r\ncan you please add the below lines of code and check.Use tf.keras instead of Keras\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import load_mode\r\n```\r\nPlease, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/5dd8e139990c80ec08076311aa16e7dc/untitled431.ipynb) Is this the expected behavior?. Thanks!\r\n", "@ravikyram \r\nHi,\r\nthank you for your help.\r\nI try to import tensorflow as you did in the code with:\r\n`from tensorflow.keras.models import load_model`\r\nand even if it's underline in red, because he didn't find the module, it load the class. (Another option is to call the function directly inside the code with `tf.keras.models.load_model('my_model.h5)`\r\nAnyway, I still get this error.\r\nBut finally I find a solution for this, adding this code below(don't know why on Windows I don't need it:\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n```\r\nNow, it predicts the class with the loaded model.\r\nThank you so much for the support!\r\nHave a nice day,\r\nNicola.", "Thanks Nicola. It worked for me too.", "How about for C++?\r\nI have the same problem, but in C++", "> @ravikyram\r\n> Hi,\r\n> thank you for your help.\r\n> I try to import tensorflow as you did in the code with:\r\n> `from tensorflow.keras.models import load_model`\r\n> and even if it's underline in red, because he didn't find the module, it load the class. (Another option is to call the function directly inside the code with `tf.keras.models.load_model('my_model.h5)`\r\n> Anyway, I still get this error.\r\n> But finally I find a solution for this, adding this code below(don't know why on Windows I don't need it:\r\n> \r\n> ```\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> if gpus:\r\n>     try:\r\n>         # Currently, memory growth needs to be the same across GPUs\r\n>         for gpu in gpus:\r\n>             tf.config.experimental.set_memory_growth(gpu, True)\r\n>         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n>     except RuntimeError as e:\r\n>         # Memory growth must be set before GPUs have been initialized\r\n>         print(e)\r\n> ```\r\n> \r\n> Now, it predicts the class with the loaded model.\r\n> Thank you so much for the support!\r\n> Have a nice day,\r\n> Nicola.\r\n\r\nThank you man, for the ResNets project, I added the piece of code before everything in resnet_run_loop.py file and worked well."]}, {"number": 34694, "title": "API Documentation - Incorrect formatting for tanh activation function", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe arguments section of the tanh activation documentation is not formatted correctly.\r\n\r\nI am uncertain as to why, but suspect it may be due to a lack of a newline after the example block ([code here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/activations.py#L201-L221))\r\n", "comments": ["@benchislett , thanks for reporting, but the issue has actually already been fixed in the `master` branch [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py#L225-L243).", "Great! Thanks for letting me know."]}, {"number": 34693, "title": "Size and CombinedNMS Op support request for tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\nLike in the issue https://github.com/tensorflow/tensorflow/issues/33059 i want to be able to convert my model to TFLite and I have tried changing to: tf.image.non_max_suppression_padded and tf.image.non_max_suppression_with_scores but with both I still get errors\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-11-28 19:01:40.384887: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-11-28 19:01:40.385067: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-28 19:01:40.408717: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-28 19:01:40.408744: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 481 nodes (0), 688 edges (0), time = 4.814ms.\r\n2019-11-28 19:01:40.408760: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: Graph size after: 481 nodes (0), 688 edges (0), time = 5.216ms.\r\n2019-11-28 19:01:40.408769: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: Tiny_YOLOv3_inference_non_max_suppression_map_while_body_3226\r\n2019-11-28 19:01:40.408775: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-11-28 19:01:40.408781: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-11-28 19:01:40.408788: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: Tiny_YOLOv3_inference_non_max_suppression_map_while_cond_3225\r\n2019-11-28 19:01:40.408794: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-11-28 19:01:40.408802: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-11-28 19:01:41.252079: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-11-28 19:01:41.252177: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-28 19:01:41.456787: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-11-28 19:01:41.456818: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 359 nodes (-58), 584 edges (-58), time = 111.647ms.\r\n2019-11-28 19:01:41.456824: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 359 nodes (0), 584 edges (0), time = 26.561ms.\r\n2019-11-28 19:01:41.456829: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: Tiny_YOLOv3_inference_non_max_suppression_map_while_body_3226_frozen\r\n2019-11-28 19:01:41.456833: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 52 nodes (0), 55 edges (0), time = 1.265ms.\r\n2019-11-28 19:01:41.456838: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 52 nodes (0), 55 edges (0), time = 1.103ms.\r\n2019-11-28 19:01:41.456842: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: Tiny_YOLOv3_inference_non_max_suppression_map_while_cond_3225_frozen\r\n2019-11-28 19:01:41.456846: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 14 nodes (0), 8 edges (0), time = 0.617ms.\r\n2019-11-28 19:01:41.456850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 14 nodes (0), 8 edges (0), time = 0.394ms.\r\nTraceback (most recent call last):\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-c548bab089a8>\", line 1, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 446, in convert\r\n    **converter_kwargs)\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-11-28 19:01:43.495379: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-28 19:01:43.517231: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2712000000 Hz\r\n2019-11-28 19:01:43.518000: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d1e210d680 executing computations on platform Host. Devices:\r\n2019-11-28 19:01:43.518031: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-28 19:01:43.520442: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-28 19:01:43.520471: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-28 19:01:43.520503: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nl1lxl-107914): /proc/driver/nvidia/version does not exist\r\n2019-11-28 19:01:43.549801: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-11-28 19:01:43.549850: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-11-28 19:01:43.550020: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-11-28 19:01:43.550039: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Size\r\n2019-11-28 19:01:43.550121: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-11-28 19:01:43.550139: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550183: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-11-28 19:01:43.550193: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550205: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-11-28 19:01:43.550212: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550219: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-11-28 19:01:43.550225: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550232: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-11-28 19:01:43.550240: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550253: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-11-28 19:01:43.550270: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550275: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550279: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550283: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550287: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-11-28 19:01:43.550299: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-11-28 19:01:43.550308: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-11-28 19:01:43.550317: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-11-28 19:01:43.554013: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 226 operators, 430 arrays (0 quantized)\r\n2019-11-28 19:01:43.557143: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 226 operators, 430 arrays (0 quantized)\r\n2019-11-28 19:01:43.648439: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 111 operators, 212 arrays (0 quantized)\r\n2019-11-28 19:01:43.650428: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 111 operators, 212 arrays (0 quantized)\r\n2019-11-28 19:01:43.652457: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 111 operators, 212 arrays (0 quantized)\r\n2019-11-28 19:01:43.653866: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 111 operators, 212 arrays (0 quantized)\r\n2019-11-28 19:01:43.655915: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 22151168 bytes, theoretical optimal value: 22151168 bytes.\r\n2019-11-28 19:01:43.656807: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PACK, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: Size, TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"/home/brechard/miniconda3/envs/modeling/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/brechard/miniconda3/envs/modeling/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, EXPAND_DIMS, FILL, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PACK, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: Size, TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n```\r\n\r\nIf it helps, the function that I am trying to modify can be found in: \r\nhttps://github.com/Brechard/computer-vision-tf2/blob/master/src/models/detection/yolov3.py#L240\r\n\r\nmy modification right now looks like this:\r\n\r\n```\r\n    def non_max_suppression(self, outputs):\r\n\r\n        bboxes, confidence, class_prob = [], [], []\r\n        for o in outputs:\r\n            bboxes.append(tf.reshape(o[0], (tf.shape(o[0])[0], -1, tf.shape(o[0])[-1])))\r\n            confidence.append(tf.reshape(o[1], (tf.shape(o[1])[0], -1, tf.shape(o[1])[-1])))\r\n            class_prob.append(tf.reshape(o[2], (tf.shape(o[2])[0], -1, tf.shape(o[2])[-1])))\r\n\r\n        bbox = tf.concat(bboxes, axis=1)\r\n        confidence = tf.concat(confidence, axis=1)\r\n        class_probs = tf.concat(class_prob, axis=1)\r\n\r\n        scores = confidence * class_probs\r\n\r\n        # Process for each image of the batch\r\n        def _nms_single_image(args):\r\n            image_bbox, image_scores = args[0], args[1]\r\n            classes = tf.math.argmax(image_scores, axis=1)\r\n            new_img_scores = tf.math.reduce_max(image_scores, axis=1)\r\n\r\n            selected_items, selected_scores = tf.image.non_max_suppression_with_scores(\r\n                boxes=tf.reshape(image_bbox, (-1, 4)),\r\n                scores=new_img_scores,\r\n                max_output_size=100,\r\n                iou_threshold=self.iou_threshold,\r\n                score_threshold=self.score_threshold\r\n            )\r\n\r\n            return [tf.gather(image_bbox, selected_items), tf.gather(new_img_scores, selected_items),\r\n                    tf.gather(classes, selected_items)]\r\n\r\n        bboxes, scores, classes = tf.map_fn(_nms_single_image, elems=[bbox, scores],\r\n                                            dtype=[tf.float32, tf.float32, tf.int64])\r\n\r\n        return bboxes, scores, classes\r\n```\r\n", "comments": ["any update on this?\r\n", "@Brechard We see that you are using older version of tensorflow .Many bug have been fixed in latest version.  We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!\r\n\r\n", "@Brechard If this is still relevant, can you try using [`non_max_suppression_padded`](https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression_padded)? It has better support in TFLite, and performs better too. See [this issue](https://github.com/tensorflow/tensorflow/issues/51629#issuecomment-905074446) for some details.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34693\">No</a>\n"]}, {"number": 34692, "title": "Fix for compilation error in cmsis-nn/pooling.cc", "body": "There was a couple of compilation errors that needed to be fixed. One of the issues is that arm_avgpool_s8() returned void. I had to fix that in the CMSIS repo. Hence I had to step up the downloaded CMSIS commit id. On top of that I added two .c file that are utilized in CMSIS for MVE optimizations.", "comments": ["@petewarden @advaitjain, gentle ping for review", "@freddan80  Can you please resolve conflicts? Thanks!", "You will have to pull and resolve conflicts.  We recently moved micro out of the experimental directory, so your changes need to reflect the correct paths for tip-of-tree.", "I've landed an exact replica of this change internally to get around copybara errors."]}, {"number": 34691, "title": "keras Model with a dictionary as output: compile and fit expect layers names instead of the output dictionary's keys", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nUsing the functional Keras API, I build a model which has multiple outputs that are in a dictionary:\r\n```\r\ndef make_model():\r\n    inp = tf.keras.Input(1)\r\n    y = tf.keras.layers.Dense(1)(inp)\r\n    y_times_2 = tf.math.multiply(2., y)\r\n    model = tf.keras.Model(inputs=inp, outputs={'y':y, 'y_times_2':y_times_2})\r\n    return model\r\n```\r\nWhen compiling the model, I want to pass as the `loss` argument the following dictionary:\r\n`{'y':'mean_squared_error', 'y_times_2':'mean_squared_error'}` \r\nand as the `metrics` argument the following dictionary:\r\n`{'y':'mean_absolute_error', 'y_times_2':'mean_absolute_error'}`\r\nas it is both intuitive and suggested [by the documentation of `compile`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile) (see what is written for the `metrics` argument).\r\nSimilarly, for training the model, I want to pass as the `y` argument of `fit` the following dictionary:\r\n`{'y':y, 'y_times_2':2*y}`.\r\n\r\nIt turns out that none of this works and using any of those dictionaries throws an error, because, strangely, the API does not expect `'y'` and `'y_times2'` as keys, whereas those are the keys of the output of the model, but it expects the name of the layers producing the outputs! For the `y` output, the layer producing it is the `Dense` layer defined in the model. For `y_times2` the layer producing it is an instance of `TensorFlowOpLayer` produced by the call of `tf.math.multiply` in the model definition. So, for example, the correct dictionary to use as the `loss` argument of `compile` is:\r\n`{'dense':'mean_squared_error', 'tf_op_layer_Mul':'mean_squared_error'}`\r\n\r\nNote that given that `TensorFlowOpLayer` layers are named automatically (even if I use the `name` argument of `tf.math.multiply`, because the name of the `TensorFlowOpLayer` layer is not the same as the name of the op), it can be really difficult to know the proper names to pass to `compile` and `fit` in a more complex example.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behaviour should be as described in [the documentation of `compile`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile) and should use the output names.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\nx = np.random.normal(size=(64,))\r\ny = x\r\n\r\ndef make_model():\r\n    inp = tf.keras.Input(1)\r\n    y = tf.keras.layers.Dense(1)(inp)\r\n    y_times_2 = tf.math.multiply(2., y)\r\n    model = tf.keras.Model(inputs=inp, outputs={'y':y, 'y_times_2':y_times_2})\r\n    return model\r\n\r\nmodel = make_model()\r\n\r\nmodel.summary()\r\n\r\ntry:\r\n    print('Trying compilation with the output names')\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.1), \r\n                  loss={'y':'mean_squared_error', 'y_times_2':'mean_squared_error'})\r\nexcept Exception as e:\r\n    print('The compilation failed with the following message error:')\r\n    print(e)\r\n    print('Trying compilation with the layers names')\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.1),\r\n                  loss={'dense':'mean_squared_error', 'tf_op_layer_Mul':'mean_squared_error'})\r\n    \r\ntry:\r\n    print('\\nTrying training with the output names')\r\n    model.fit(x, {'y':y, 'y_times_2':2*y}, epochs=2)\r\nexcept Exception as e:\r\n    print('The training failed with the following message error:')\r\n    print(e)\r\n    print('Trying training with the layers names')\r\n    model.fit(x, {'dense':y, 'tf_op_layer_Mul':2*y}, epochs=2)\r\n    \r\nprint('\\n###################\\nAdd metrics to the example:')\r\n\r\nmodel = make_model()\r\n\r\nmodel.summary()\r\n\r\ntry:\r\n    print('Trying compilation with the output names')\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.1), \r\n                  loss={'dense_1':'mean_squared_error', 'tf_op_layer_Mul_1':'mean_squared_error'},\r\n                  metrics={'y':'mean_absolute_error', 'y_times_2':'mean_absolute_error'})\r\nexcept Exception as e:\r\n    print('The compilation failed with the following message error:')\r\n    print(e)\r\n    print('Trying compilation with the layers names')\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.1),\r\n                  loss={'dense_1':'mean_squared_error', 'tf_op_layer_Mul_1':'mean_squared_error'},\r\n                  metrics={'dense_1':'mean_absolute_error', 'tf_op_layer_Mul_1':'mean_absolute_error'})\r\n\r\nmodel.fit(x, {'dense_1':y, 'tf_op_layer_Mul_1':2*y}, epochs=2)\r\n```\r\n\r\nwith the following output:\r\n```\r\nUsing Tensorflow version 2.0.0 (git version v2.0.0-rc2-26-g64c3d382ca)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 1)]               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 2         \r\n_________________________________________________________________\r\ntf_op_layer_Mul (TensorFlowO [(None, 1)]               0         \r\n=================================================================\r\nTotal params: 2\r\nTrainable params: 2\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrying compilation with the output names\r\nThe compilation failed with the following message error:\r\nUnknown entries in loss dictionary: ['y', 'y_times_2']. Only expected following keys: ['dense', 'tf_op_layer_Mul']\r\nTrying compilation with the layers names\r\n\r\nTrying training with the output names\r\nThe training failed with the following message error:\r\nNo data provided for \"dense\". Need data for each key in: ['dense', 'tf_op_layer_Mul']\r\nTrying training with the layers names\r\nTrain on 64 samples\r\nEpoch 1/2\r\n64/64 [==============================] - 1s 9ms/sample - loss: 10.1650 - dense_loss: 2.0330 - tf_op_layer_Mul_loss: 8.1320\r\nEpoch 2/2\r\n64/64 [==============================] - 0s 62us/sample - loss: 7.1354 - dense_loss: 1.4271 - tf_op_layer_Mul_loss: 5.7084\r\n\r\n###################\r\nAdd metrics to the example:\r\nModel: \"model_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 1)]               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 2         \r\n_________________________________________________________________\r\ntf_op_layer_Mul_1 (TensorFlo [(None, 1)]               0         \r\n=================================================================\r\nTotal params: 2\r\nTrainable params: 2\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrying compilation with the output names\r\nThe compilation failed with the following message error:\r\nUnknown entries in metrics dictionary: ['y', 'y_times_2']. Only expected following keys: ['dense_1', 'tf_op_layer_Mul_1']\r\nTrying compilation with the layers names\r\nTrain on 64 samples\r\nEpoch 1/2\r\n64/64 [==============================] - 0s 5ms/sample - loss: 1.4019 - dense_1_loss: 0.2804 - tf_op_layer_Mul_1_loss: 1.1215 - dense_1_mean_absolute_error: 0.4108 - tf_op_layer_Mul_1_mean_absolute_error: 0.8217\r\nEpoch 2/2\r\n64/64 [==============================] - 0s 71us/sample - loss: 0.4672 - dense_1_loss: 0.0934 - tf_op_layer_Mul_1_loss: 0.3737 - dense_1_mean_absolute_error: 0.2408 - tf_op_layer_Mul_1_mean_absolute_error: 0.4816\r\n```", "comments": ["Could reproduce the issue with Tensorflow 2.0. \r\nPlease see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/7606fa53332f78d64286bae0e90d7b2a/untitled284.ipynb). Thanks!", "Given the documentation on keras, I don't think you're supposed to pass a dictionary for outputs. I've never seen keras code in the tensorflow docs that do that.", "As I said, [the documentation of `compile`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile) implies it, read what is written in  the `metrics` argument.", "You're correct about what's written in the metrics argument. However, that's for the metrics argument not the outputs argument. I could be wrong as I'm new to Keras myself, but again, I haven't seen anything that specifically does that for the outputs argument. I've only seen it for the losses and metric arguments. I believe that it attaches these two things to outputs by looking for the names of the tensors they're supposed to be attached to via the key you provide. I don't think that it directly matches keys between a dictionary for say outputs and a dictionary for metrics. Again, take with grain of salt though as I'm new myself!", "Also, if you're convinced, I suspect there's some bugs in the dictionary-style API as I've described it. See my issue I submitted here: #34713 ", "I see what you mean, and it seems that you are right. The doc you linked in your issue supports that.\r\n\r\n> I believe that it attaches these two things to outputs by looking for the names of the tensors they're supposed to be attached to via the key you provide.\r\n\r\nFrom my experiments (and the doc you linked in your issue), it looks for the name of the layer outputting the tensors, not the name of the tensors. And this is an information that can be difficult to find. Furthermore, like shown in my example above, there are layers for which you don't have total control over the name (I think this corner case was not taken in consideration when designing the API).\r\n\r\n If, as you said, Keras should not be given a dict as output, then my issue becomes a feature request. However it works when I simply `call` the model on some data. With my example code above, if you then execute `model(tf.expand_dims(x, axis=-1))` you get the desired output: a `dict` of `Tensor` objects with keys `'y'`and `'y_times_2'`. Being able to name the outputs and having those names don't match the names you give to the `loss` and `metrics` arguments of `compile` seems pretty counterintuitive. I think that, if the ouputs are named by using a dictionary as output, those names should override the name of the layers producing the outputs.", "@durandg12 This issue is resolved with the latest tf-nightly and 2.2.0-rc2. here is a [colab gist](https://colab.research.google.com/gist/goldiegadde/0ad3fed10b0cbfb3633ee3ff05dcad31/github-issue-34691.ipynb)\r\nCould you please verify and close this issue accordingly. ", "@goldiegadde I would not say the issue is totally solved, looking at the gist. Althought my main points are solved, the information printed by the keras API during training still uses the name of the tensor producing the output instead of the name of the output given by the user:\r\n```\r\nEpoch 1/2\r\n2/2 [==============================] - 0s 2ms/step - loss: 9.2561 - dense_loss: 1.8512 - tf_op_layer_Mul_loss: 7.4048\r\nEpoch 2/2\r\n2/2 [==============================] - 0s 1ms/step - loss: 6.7087 - dense_loss: 1.3417 - tf_op_layer_Mul_loss: 5.3670\r\n```", "@durandg12 I am not sure whether it is a workaround or not. I changed two lines in your code as shown below. With that change, now I can see the following output with the names given to the layers. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/7711f849ec186ce53fa8f8489845a7ea/github-issue-34691.ipynb).\r\n\r\n```\r\n    y = tf.keras.layers.Dense(1,name='y')(inp)\r\n    y_times_2 = tf.keras.layers.Lambda(lambda y:tf.math.multiply(2., y),name='y_times_2')(y)\r\n```\r\n\r\nOutput\r\n\r\n```\r\nEpoch 1/2\r\n2/2 [==============================] - 0s 4ms/step - loss: 18.1492 - y_loss: 3.6298 - y_times_2_loss: 14.5194 - y_mean_absolute_error: 1.6186 - y_times_2_mean_absolute_error: 3.2371\r\nEpoch 2/2\r\n2/2 [==============================] - 0s 6ms/step - loss: 13.7314 - y_loss: 2.7463 - y_times_2_loss: 10.9851 - y_mean_absolute_error: 1.4094 - y_times_2_mean_absolute_error: 2.8189\r\n<tensorflow.python.keras.callbacks.History at 0x7f6b66c5f390>\r\n```\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34691\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34691\">No</a>\n"]}, {"number": 34690, "title": "Using clip_by_value anywhere in the model graph (including in a metric) produces a model which cannot be saved as h5.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 and Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7.4 and 3.6.8\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUsing a custom metric function which employs tf.clip_by_value, then adding it to the model by model.add_loss, and attempting to save it using model.save results in a ValueError exception saying \"Unable to create group (name already exists) - full trace provided at the end.\r\n\r\nPlaying around with this model, the custom metric is irrelevant, it also happens with this:\r\n\r\n```python\r\nfrom tensorflow.keras import Model, layers\r\nimport tensorflow as tf\r\n\r\nx = layers.Input((10, 10, 1))\r\nt = tf.clip_by_value(x, 0, 1)\r\npred = layers.Dense(8, activation='relu')(t)\r\nmodel = Model(inputs=x, outputs=pred)\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\r\nmodel.save('bla.h5')\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI would expect the model to get saved without an issue.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nfrom tensorflow.keras import Model, layers\r\nimport tensorflow as tf\r\n\r\nx = layers.Input((10, 10, 1))\r\npred = layers.Dense(8, activation='relu')(x)\r\nmodel = Model(inputs=x, outputs=pred)\r\n\r\ndef custom_metric(y_pred):\r\n    y_pred = tf.clip_by_value(y_pred, 0, 1)\r\n    return tf.reduce_mean(y_pred)\r\n\r\nmodel.add_metric(custom_metric(pred), name='val_custom', aggregation='mean')\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\r\nmodel.save('bla.h5')\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-11f183ffac31> in <module>\r\n----> 1 model.save('bla.h5')\r\n\r\n~/projects/tensorflow/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    973     \"\"\"\r\n    974     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n--> 975                       signatures, options)\r\n    976 \r\n    977   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n~/projects/tensorflow/env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    110           'or using `save_weights`.')\r\n    111     hdf5_format.save_model_to_hdf5(\r\n--> 112         model, filepath, overwrite, include_optimizer)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n\r\n~/projects/tensorflow/env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)\r\n    107     model_weights_group = f.create_group('model_weights')\r\n    108     model_layers = model.layers\r\n--> 109     save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n    110 \r\n    111     # TODO(b/128683857): Add integration tests between tf.keras and external\r\n\r\n~/projects/tensorflow/env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in save_weights_to_hdf5_group(f, layers)\r\n    623 \r\n    624   for layer in layers:\r\n--> 625     g = f.create_group(layer.name)\r\n    626     weights = _legacy_weights(layer)\r\n    627     weight_values = K.batch_get_value(weights)\r\n\r\n~/projects/tensorflow/env/lib/python3.6/site-packages/h5py/_hl/group.py in create_group(self, name, track_order)\r\n     66             name, lcpl = self._e(name, lcpl=True)\r\n     67             gcpl = Group._gcpl_crt_order if track_order else None\r\n---> 68             gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)\r\n     69             return Group(gid)\r\n     70 \r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5g.pyx in h5py.h5g.create()\r\n\r\nValueError: Unable to create group (name already exists)\r\n```\r\n\r\n", "comments": ["Issue is replicating with Tensorflow 2.0.\r\nPlease see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/4c0112dd8bb56b7ae29c0b39b02931e3/untitled283.ipynb). Thanks!", "@feature-engineer,\r\n The Model will be Saved successfully if you use `model.save('bla')` for saving the Model. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/d95c4d04bb6c37ad8326e4401d63f110/untitled283.ipynb). Thanks!\r\n", "@feature-engineer,\r\nCan you please let us know if your issue is resolved. Thanks! ", "@rmothukuru That's certainly a workaround. However I would like to use the h5 format, and I think it should still be a bug to resolve if tensorflow isn't planning on deprecating this format. ", "@feature-engineer I think this was resolved in `tf-nightly`. I cannot reproduce the issue with `tf-nightly`. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ac8ce65ee413880f26d189b31a9b0f63/untitled283.ipynb). Thanks!\r\n\r\nPlease close this issue if it was resolved in `tf-nightly` for you. Thanks!", "Yes, it indeed seem to have been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34690\">No</a>\n", "It still does not work in tf 2.1...", "@fzyzcjy Please check with `tf-nightly`. There were lots of modifications after `TF2.1`, so the updated code is `tf-nightly`. Currently `TF2.2.0-rc2` released and i think the stable `TF2.2` will be released in near future. Thanks!\r\n\r\nPlease feel free to create a new issue with a standalone code to reproduce the error. Thanks!", "@jvishnuvardhan Sorry is it stable enough? I am afraid nightly will contain strange bugs...?", "@fzyzcjy I guess there will be a stable `TF2.2` in near future. It is currently `TF2.2.0-rc2`. Thanks!"]}, {"number": 34689, "title": "Keras scikit-learn wrapper not compatible with keras functional model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (use command below): TF version: 2.0, 1.15\r\n- Python version: 3.6 , 3.7\r\n\r\n**Describe the current behavior**\r\nwhen using a keras functional api model via the keras scikit-learn wrapper a crash occurs.\r\nsee: \r\nhttps://github.com/tensorflow/tensorflow/blob/13f2db1e7071ae109d2f51c7202867a154f587d2/tensorflow/python/keras/wrappers/scikit_learn.py#L241\r\n**Describe the expected behavior**\r\nmodel.predict() should work on all keras model types besides sequential \r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras\r\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\r\n\r\ndef build_model():\r\n  input = tf.keras.layers.Input(shape=(2,))\r\n  pred = tf.keras.layers.Dense(2, activation='softmax')(input)\r\n  model = tf.keras.models.Model(inputs=input, outputs=pred)\r\n  model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\r\n  return model\r\n\r\nX = np.array([[1,2],[3,1]])\r\nY = np.array([[1,0], [0,1]])\r\nmodel = build_model()\r\nmodel.fit(X, Y)\r\nprint(model.predict(X))  # this works\r\n\r\nmodel_wrapped = KerasClassifier(build_model)\r\nmodel_wrapped.fit(X, Y)\r\nmodel_wrapped.predict(X)  # this crashes\r\n```\r\nOutput: \r\nTrain on 2 samples\r\n2/2 [==============================] - 0s 62ms/sample - loss: 1.1024 - acc: 0.5000\r\n[[0.62487346 0.37512657]\r\n [0.8205698  0.17943017]]\r\nTrain on 2 samples\r\n2/2 [==============================] - 0s 64ms/sample - loss: 0.2733 - acc: 1.0000\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-14-48bacae97b80> in <module>()\r\n     19 model_wrapped = KerasClassifier(build_model)\r\n     20 model_wrapped.fit(X, Y)\r\n---> 21 model_wrapped.predict(X)  # this crashes\r\n     22 \r\n     23 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py in predict(self, x, **kwargs)\r\n    239     \"\"\"\r\n    240     kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\r\n--> 241     classes = self.model.predict_classes(x, **kwargs)\r\n    242     return self.classes_[classes]\r\n    243 \r\n\r\nAttributeError: 'Model' object has no attribute 'predict_classes'\r\n```", "comments": ["Could replicate the issue with Tf 2.0 and Tf 1.15.\r\nPlease take a look at the gist for [Tf 2.0](https://colab.sandbox.google.com/gist/gadagashwini/a75cf412027db7dd67d5684a738db5bf/untitled281.ipynb) and [Tf 1.15](https://colab.sandbox.google.com/gist/gadagashwini/1a6524d0fb5fcbdee556d27792bc2f42/untitled282.ipynb). Thanks!", "@karimmohraz,\r\nAs per this [Source Code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/wrappers/scikit_learn.py#L225-L242) of  `predict` method of `Keras Classifier Wrapper` for `Scikit-Learn` works only on `Keras Sequential` Model, not on `tf.keras.models.Model`. \r\n\r\nSo, this behavior is expected.", "We are aware that this is a current limitation of the implementation, but from a design standpoint this severely limits applicability for various use-cases. One major problem is that neither multiple output nor multiple input layers are supported by Keras sequential model, e.g. **BERT** or a mixture of softmax/sigmoid outputs. In our solution we also monkey-patched the functional model to support predict_classes which solves the issue. In our opinion it would make sense to either extend the functional model to support Sklearn wrapper or rewrite the Sklearn wrapper to not use predict_classes s.t. it can be used in a wider context. If required we could also submit a PR doing exactly that (if a resource constraint on your side is the reason you call this behavior as expected).", "I ran into the same issue; however, upon reflection, it makes sense to only support the `Sequential` model. It is always possible to wrap a functional graph into a custom Keras layer and add that to a sequential model; I've done this a while ago with residual layers, see [link](https://sebastianwallkoetter.wordpress.com/2018/04/08/layered-layers-residual-blocks-in-the-sequential-keras-api/) for an example.\r\n\r\nThis other limitation of `Sequential` vs `Model` is the number of inputs and outputs. As far as I am aware scikit learn doesn't natively support multiple inputs (not sure about outputs). I would have to write custom code to support multiple inputs on the scikit side AND on the tensorflow side to make this work. What is the desired usecase?\r\n\r\nOne bug in the `KerasClassifier` wrapper that I've noticed is that it doesn't declare `self._estimator_type = \"classifier\"` in the `__init__`, which makes the majority of scikit functions reject the estimator. I saw that #37201 addresses this, so I will just wait until it gets merged :)", "My use case is: \r\n- take a pretrained keras network (e.g. https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1) \r\n- add a classification layer on top for transfer learning\r\n- use scikitlearn GridsearchCV for hyperparameter tuning\r\n\r\nWe discarded the keras scikit wrapper as we also use multihead output and a custom scorer which requires too much monkey patching.\r\nA much simpler solution is to wrap the Bert classifier as a scikitlearn estimator which can be used by GridsearchCV.\r\nsee https://gist.github.com/karimmohraz/7dde3bfcc0786b56673461bf20cfdf5b", "Hi everyone,\r\n\r\nFrom discussion with the Keras team, it looks like #37201 may not be merged and instead the functionality may be made into a separate package that will be easier to maintain and have more flexibility. The wrappers would then be deprecated from Keras/TF. This is not final, but I created a package to test out the idea. \r\n\r\nLinks: \r\nPyPi: https://pypi.org/project/scikeras\r\nSource: https://github.com/adriangb/scikeras\r\n\r\nPlease take a look and let me know if this satisfies the use cases in this PR. This should allow for input splitting/output joining similar to the gist that @karimmohraz references above, except it is done in a more programmatic way.\r\n\r\nAny input is welcome.", "from keras.wrappers.scikit_learn import KerasClassifier\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n def build_classifier():\r\n    classifier = Sequential()\r\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\r\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\r\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\r\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n    return classifier\r\nclassifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\r\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\r\nwhen i run those code in spyder(python 3.7.7) I got an exception like:\r\n\r\n  File \"<ipython-input-2-57e958f5fe78>\", line 5\r\n    def build_classifier():\r\n    ^\r\nIndentationError: unexpected indent\r\n\r\nwhats is about? please help me :( ", "In Python, indentations do matter. try this:\r\n\r\n```\r\ndef build_classifier():\r\n    classifier = Sequential()\r\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\r\n    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\r\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\r\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\nreturn classifier\r\n```", "This Python package seems to support also the functional API: https://pypi.org/project/scikeras/", "> This Python package seems to support also the functional API: https://pypi.org/project/scikeras/\r\n\r\nYep it does! It also supports subclassed models and quite a few other things.", "Oops, it seems I missed part of above discussions. Great work, thank you!", "I think maybe this issue can be closed @gbaned ?", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200818`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/79b8500741d40b00e1274eddebe7a537/untitled260.ipynb).Thanks!", "@karimmohraz,\r\nCan you please confirm if we can close this issue with respect to [Adrian's comment](https://github.com/tensorflow/tensorflow/issues/34689#issuecomment-612645946)? Thanks! ", "you can go ahead and close this issue. We have already removed the keras scikit wrapper from our project.\r\nWe will evaluate Adrian's package in due time.", "Ok for me, thanks\n\nrmothukuru ***@***.***> schrieb am Fr., 9. Apr. 2021, 13:07:\n\n> @karimmohraz <https://github.com/karimmohraz>,\n> Can you please confirm if we can close this issue with respect to Adrian's\n> comment\n> <https://github.com/tensorflow/tensorflow/issues/34689#issuecomment-612645946>?\n> Thanks!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34689#issuecomment-816606405>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKEY3UBVBPYTK3AKE72DQH3TH3NYTANCNFSM4JSUY63Q>\n> .\n>\n", "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_weight_boosting.py in _validate_estimator(self)\r\n    454         if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\r\n    455             raise ValueError(\"%s doesn't support sample_weight.\"\r\n--> 456                              % self.base_estimator_.__class__.__name__)\r\n    457 \r\n    458     def _boost(self, iboost, X, y, sample_weight, random_state):\r\n\r\nValueError: KerasClassifier doesn't support sample_weight.\r\n\r\n\r\n\r\ni am getting following error when I use neural network as a base classifier in adaboost. how we can solve this problem"]}, {"number": 34688, "title": "Failed to load nodes from frozen graph file(.pb)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Python binary(pip install tensorflow==1.7.0)\r\n- TensorFlow version (use command below): 1.7.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.10.0(never used bazel)\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version: Not used CUDA/cuDNN\r\n- GPU model and memory: Not used GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I tried to load frozen graph(.pb) as graph_def, I couldn't get success to run [import_graph_def](https://github.com/tensorflow/tensorflow/blob/9156fcc7a8a901ca9e553297da8237a313255bd8/tensorflow/python/framework/importer.py#L402).\r\nI received a ValueError: **ValueError: graph_def is invalid at node 'inpaint_net/conv1/kernel/Assign': Input tensor 'inpaint_net/conv1/kernel:0' Cannot convert a tensor of type float32 to an input of type float32_ref.**\r\nI found similar issues from #3628 and #24062, but I can not resolve with @barbolo's suggested [code](https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-272147744).\r\n\r\n**Describe the expected behavior**\r\nIt should load gragh_def object by using [tf.import_graph_def](https://github.com/tensorflow/tensorflow/blob/9156fcc7a8a901ca9e553297da8237a313255bd8/tensorflow/python/framework/importer.py#L402) function.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\ninput = 'model/snapmodel-160000.pb'\r\nwith tf.Session() as sess:\r\n    print(\"load graph\")\r\n    with gfile.FastGFile(input,'rb') as f:\r\n       graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    f = open(node_file, 'a')\r\n    # fix nodes\r\n    for node in graph_def.node:\r\n        if node.op == 'RefSwitch':\r\n            node.op = 'Switch'\r\n            for index in range(len(node.input)):\r\n                node.input[index] = node.input[index] + '/read'\r\n        elif node.op == 'AssignSub':\r\n            node.op = 'Sub'\r\n            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n        elif node.op == 'AssignAdd':\r\n            node.op = 'Add'\r\n            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n        elif node.op == 'Assign':\r\n            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n    with sess.graph.as_default() as tf_graph:\r\n        tf.import_graph_def(graph_def, name='')\r\n```\r\n\r\n**Other info / logs**\r\nThere is full log:\r\n```\r\n/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2019-11-28 05:03:22.022135: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nload graph\r\nTraceback (most recent call last):\r\n  File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 667, in import_graph_def\r\n    op._add_input(source_tensor, dtype=input_type)\r\n  File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1898, in _add_input\r\n    (tensor.dtype.name, dtype.name))\r\nTypeError: Cannot convert a tensor of type float32 to an input of type float32_ref\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Volumes/Work/2019_Work/Inpainting/tensorflow-onnx/convert.py\", line 68, in <module>\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/mac/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 671, in import_graph_def\r\n    node, 'Input tensor %r %s' % (input_name, te)))\r\nValueError: graph_def is invalid at node 'inpaint_net/conv1/kernel/Assign': Input tensor 'inpaint_net/conv1/kernel:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n```\r\n\r\nPlease give me a help for resolving this issue asap.\r\nThank you", "comments": ["@alekdev2019, Thanks for reporting issue.\r\nCould you please try against latest TF version 1.15.0. \r\nLet us know how it progresses. Thanks!", "Helo @gadagashwini \r\nThank you for your reply.\r\nBut I should use TF 1.7.0 because [model](https://github.com/JiahuiYu/generative_inpainting) was trained by TF 1.7.0.\r\nI've tested [this project](https://github.com/JiahuiYu/generative_inpainting) with another TF version, but it only work with TF 1.7.0.\r\nPlease let me know how I can resolve above issue.\r\nThank you\r\n", "@alekdev2019, Could you post the .pb file to reproduce the reported issue. Thanks!", "@gadagashwini , there is my frozen_graph file path:\r\nhttps://drive.google.com/open?id=1tnMg5GVn7R8gU-X5yiORQN8wKsr2X8lW\r\n\r\nThank you.", "@alekdev2019, tried to replicate the reported issue but received different error message. \r\nPlease see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/098f8779983524ad7a18fa24cd31c843/untitled291.ipynb) and provide more information. Thanks! ", "@gadagashwini , I have checked your logs and not sure if you copied pb file at the path correctly.\r\nIf you copied the file correctly, then it seems like dependency's version issue.\r\nThere are my dependencies:\r\n```\r\npip list\r\nPackage             Version   \r\n------------------- ----------\r\nabsl-py             0.8.1     \r\nasn1crypto          1.2.0     \r\nastor               0.8.0     \r\nattrs               19.3.0    \r\nbleach              1.5.0     \r\ncertifi             2019.11.28\r\ncffi                1.13.2    \r\nchardet             3.0.4     \r\ncoverage            4.5.4     \r\ncryptography        2.8       \r\nfuture              0.18.2    \r\ngast                0.3.2     \r\ngrpcio              1.25.0    \r\nhtml5lib            0.9999999 \r\nidna                2.8       \r\nimportlib-metadata  1.1.0     \r\nKeras-Applications  1.0.8     \r\nKeras-Preprocessing 1.1.0     \r\nMarkdown            3.1.1     \r\nmock                3.0.5     \r\nmore-itertools      7.2.0     \r\nnumpy               1.17.4    \r\npackaging           19.2      \r\nparameterized       0.7.0     \r\npip                 19.3.1    \r\npluggy              0.13.1    \r\nprotobuf            3.11.1    \r\npy                  1.8.0     \r\npycparser           2.19      \r\npyOpenSSL           19.1.0    \r\npyparsing           2.4.5     \r\nPySocks             1.7.1     \r\npytest              5.3.1     \r\npytest-cov          2.8.1     \r\npytest-runner       5.2       \r\nPyYAML              5.1.2     \r\nrequests            2.22.0    \r\nsetuptools          42.0.1    \r\nsix                 1.13.0    \r\ntensorboard         1.7.0     \r\ntensorflow          1.7.0     \r\ntermcolor           1.1.0     \r\ntyping-extensions   3.7.4.1   \r\nurllib3             1.25.7    \r\nwcwidth             0.1.7     \r\nWerkzeug            0.16.0    \r\nwheel               0.33.6    \r\nzipp                0.6.0 \r\n```\r\n\r\nAbout pip packages, please check `protobuf` and `numpy` versions.\r\nPlease let me know if my env resolve your problems.\r\nThank you", "@alekdev2019 May be this [comment](https://github.com/tensorflow/tensorflow/issues/3628#issuecomment-272149052) might help you to solve your problem.\r\nUnfortunately I don't have a great solution for this yet, partly because I haven't been able to create a simple reproducible test case.\r\n\r\nAlso would recommend you to post this question in stackoverflow where the community may help. Thanks!"]}, {"number": 34687, "title": "Wrong inputs shape for 1D-inputs in keras with run_eagerly=True", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): default v2 for Colab\r\n- TensorFlow version (use command below): v2.0.0-0-g64c3d382ca 2.0.0\r\n- Python version: default for Colab\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: Colab\r\n- GPU model and memory: Colab\r\n\r\n**Describe the current behavior**\r\nWhen compiling keras model with run_eagerly=True and passing 1D-inputs, model reshapes it into 2D.\r\nWhen compiling with run_eagerly=False everything works as expected: layer obtains 1D-input.\r\n\r\n**Describe the expected behavior**\r\nModel should not corrupt inputs shape.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1LHtQRE1CnjkZCOqGe-O7VbBKHWSTCtJS\r\n", "comments": ["Still got this issue in v2.1.0-rc1-0-g064e1535a7 || 2.1.0-rc1 ", "@shkarupa-alex I think this was resolved in recent `tf-nightly`. Please change the first line of your code to `!pip install tf-nightly` and run the colab. Here is the output when `run_eagerly=True` was selected.\r\n\r\n```\r\nThis should be (None,) because of 1D-input:  (2,)\r\n1/1 [==============================] - 0s 1ms/step - loss: 4.0000\r\n<tensorflow.python.keras.callbacks.History at 0x7ff3319e4518>\r\n```\r\n\r\nFor the same option (`run_eagerly=True`), earlier output is as follows which was not correct.\r\n\r\n```\r\nTrain on 2 samples\r\nWARNING:tensorflow:Model was constructed with shape Tensor(\"input_2:0\", shape=(None,), dtype=float32) for input (None,), but it was re-called on a Tensor with incompatible shape (2, 1).\r\nThis should be (None,) because of 1D-input:  (2, 1)\r\nWARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\r\n2/2 [==============================] - 0s 5ms/sample - loss: 4.0000\r\n<tensorflow.python.keras.callbacks.History at 0x7fcb970bc4e0>\r\n```\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34687\">No</a>\n"]}, {"number": 34686, "title": "tf.cast with division impose different influence on python2 v.s. python3", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.5, 2.7.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n\r\n**Describe the current behavior**\r\nWhen I run a tf.cast() including a division('/'), the change of random seed is different on python2 and python3\r\nFor example, if I run the following code\r\n```\r\nimport tensorflow as tf\r\ntf.set_random_seed(1)\r\n\r\nwith tf.Session() as sess:\r\n    print('tf random_normal:{}'.format(sess.run(tf.random_normal([1, 2]))))\r\na = tf.cast(tf.constant(1)/tf.constant(2), tf.float32)\r\nwith tf.Session() as sess:\r\n    print('tf random_normal:{}'.format(sess.run(tf.random_normal([1, 2]))))\r\n```\r\nI get following result on python2.7.10\r\n```\r\ntf random_normal:[[-0.67086124  0.22357143]]\r\ntf random_normal:[[-0.3143593  0.6476281]]\r\n```\r\nBut I get following result on python3.6.5\r\n```\r\ntf random_normal:[[-0.67086124  0.22357143]]\r\ntf random_normal:[[-0.21253194  0.47261432]]\r\n```\r\nAt first, py2 and py3 create the same random_normal. But after run the tf.cast, they get different result.\r\n\r\n**Describe the expected behavior**\r\nThe tf.random_normal() of py2 and py3 with same seed should get the same result.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\ntf.set_random_seed(1)\r\n\r\nwith tf.Session() as sess:\r\n    print('tf random_normal:{}'.format(sess.run(tf.random_normal([1, 2]))))\r\na = tf.cast(tf.constant(1)/tf.constant(2), tf.float32)\r\nwith tf.Session() as sess:\r\n    print('tf random_normal:{}'.format(sess.run(tf.random_normal([1, 2]))))\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could see different result after tf.cast in Python 3 and Python 2 with Tf 1.12. \r\nPlease see the gist for Python3 [here](https://colab.sandbox.google.com/gist/gadagashwini/e2fce6a4c6ad349fc70137f7ec656a88/untitled280.ipynb).\r\nPlease see the gist for Python2 [here](https://colab.sandbox.google.com/gist/gadagashwini/e099e0a14835a61670c16fcf9fee3122/untitled280.ipynb). Thanks!", "Could reproduce the issue with TF Version 1.15 as well.", "I am closing this issue as `python2` is not supported anymore with `Tensorflow`. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34686\">No</a>\n"]}, {"number": 34685, "title": "add dtypes to range dataset op. fix #33414", "body": "Hi @jsimsa ,\r\nThis PR is just allow to specify the type of the tf.data.Dataset.range function, see issue #33414\r\n\r\nI also add test code to range_dataset_op_test.cc and range_test.py.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34685) for more info**.\n\n<!-- need_author_cla -->", "Hi, @jsimsa  thank you for your comments, I updated the code and **please review**.\r\n\r\nAnd I have a doubt, I add output_type param this way for compatible the previous code: \r\n```python\r\ndef range(*args, output_type=dtypes.int64):\r\n```\r\nso the call must be like this:\r\n```python\r\n# work\r\nrange(start, stop, step, output_type=tf.int64) \r\n# not work\r\nrange(start, stop, step, tf.int64) \r\n```\r\nI don't think define interface this way is good, do you have any suggestions?\r\n", "Hi, @jsimsa , thank you for your guide, I updated the test code.\r\n\r\nI am in doubt that why I could only pass the test code with eager mode while graph mode fail:\r\n```python\r\n# eager only, test passed\r\n@combinations.generate(combinations.times(\r\n    test_base.eager_only_combinations(),\r\n    combinations.combine(output_type=[dtypes.int32, dtypes.int64, \r\n        dtypes.float32, dtypes.float64, dtypes.string])))\r\n\r\n# eager and graph, test failed\r\n@combinations.generate(combinations.times(\r\n    test_base.default_test_combinations(),\r\n    combinations.combine(output_type=[dtypes.int32, dtypes.int64, \r\n        dtypes.float32, dtypes.float64, dtypes.string])))\r\n```\r\n\r\nCould you please point me in the direction, thank you in advance.", "What is the test failure with graph mode?", "Hi, @jsimsa , I found that:\r\n```python\r\n# next_ type defined as int64, \r\n# so the out_tensors is dtypes.int64\r\n# and we can get output_dtype with   dataset()->output_dtypes()[0]\r\nout_tensors->emplace_back(next_);\r\n```\r\nHow to convert **next_** from **int64** to **output_dtype** here?", "You should be able to do this:\r\n\r\n```\r\n...\r\nTensor result(dataset()->output_types()[0], dataset()->output_shapes()[0]);\r\nswitch (dataset()->output_types()[0]) {\r\n#define HANDLE_TYPE(type)                                         \\\r\n  case DataTypeToEnum<type>::value: {                             \\\r\n    result->scalar<type>()() = static_cast<type>(next_);          \\\r\n    break;                                                        \\\r\n  }\r\nTF_CALL_NUMBER_TYPES(HANDLE_TYPE);\r\n#undef HANDLE_TYPE\r\n  default:\r\n    return errors::InvalidArgument(\"Unsupported data type: \", DataTypeString(dataset()->output_types()[0]));\r\n}\r\nout_tensors->push_back(std::move(result));\r\n...\r\n```", "Hi @jsimsa , thank you.\r\n\r\n I fixed the problem, please review. ", "@ljwh thank you for your contribution, please sign CLA.", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34685) for more info**.\n\n<!-- ok -->", "@ljwh  Could you please check failed build errors? Thanks!", "Hi, @gbaned thank you for your reminder.\r\n\r\nHi, @jsimsa, I updated the code for backwards compatible to CI test, **please review**.\r\n\r\nLooks like the CI would run after pr approved.\r\n", "Hi, @jsimsa \r\n```python\r\n  # Support in python3 but not support in python2(Syntax error)\r\n  def range(*args, output_type=dtypes.int64):\r\n\r\n  # So, I changed this func and considered the backwards compatible:\r\n  def range(*args, **kwargs):\r\n```\r\nAlso updated some code format for pylint, **Please review**\r\n", "hi @gbaned , @jsimsa \r\nwhy ci does not run? Need help...", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34685) for more info**.\n\n<!-- need_author_cla -->", "@ljwh  thank you for your contribution, please sign CLA.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34685) for more info**.\n\n<!-- ok -->", "Hi, @jsimsa \r\nI updated the api golden files(by run api compatibility test).\r\nPlease review.\r\n", "Hi, @jsimsa \r\nCI build/test looks good, How to **trigger the copybara** ?", "@gbaned could you please unblock this PR? thank you", "@gbaned could you please **trigger the copybara** ? thank you", "@jsimsa, @ljwh There is an automatic merge process that seems to be stalled for now. Thank you!", "Hi, @gbaned \r\nHow is that going? Thank you", "@jsimsa @gbaned  thank you !", "@gbaned what is this blocked on?\r\n\r\nEDIT: the PR is now going through the internal review process"]}, {"number": 34684, "title": "[TFMicro][RFC] Create a utility that takes model and generates model-data and ops-re\u2026", "body": "Saves the developer from knowing the model architecture or the operations used in the model. \r\n\r\n- Currently the model data to CC conversion happens through the xxd\r\n  utility. But the other piece that is dependent on the model is the\r\n  ops-resolver. Having to understand a model to generate the\r\n  ops-resolver is additional overhead\r\n- This utility does both of the above processing in a single go\r\n\r\nWould something like this be of interest?\r\n\r\nI also have some thoughts about including additional annotations in the model, being converted, so it can be used for placing specific components in the model in faster/slower memory. But that's in the future.\r\n\r\nTODOs:\r\n- This is done as an example for Request For Comments for micro-speech, can be extended to other applications\r\n- Relies on `flatc` utility (as other utilities within the repo rely on this). Open to suggestions for using any native Python library for parsing the model", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34684) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34684) for more info**.\n\n<!-- ok -->", "@petewarden would like your inputs here", "@kedars Can you please resolve conflicts? Thanks!", "> @kedars Can you please resolve conflicts? Thanks!\r\n\r\nDone", "@petewarden Can you please take a look on this PR? Thanks!", "@kedars Can you please resolve conflicts? Thanks!", "CC @AdityaHPatwardhan ", "@tensorflowbutler  Sorry for the delayed update, This MR is still worked on, will update today itself.", "@AdityaHPatwardhan, @kedars  Any update on this PR, Please. Thanks!", "Yes, I have started working on this, I will update the MR this week.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@AdityaHPatwardhan, @kedars Any update on this PR, Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 34682, "title": "Allow to specify the type of the tf.data.Dataset.range function", "body": "Hi @jsimsa ,\r\nThis PR is just allow to specify the type of the tf.data.Dataset.range function, see issue  #33414 \r\n\r\nI also add test code to range_dataset_op_test.cc and range_test.py.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34682) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34682) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 34681, "title": "Skipping optimization due to error while loading function libraries: Invalid argument", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04 docker image\r\n- TensorFlow installed from (source or binary):\r\n```pip install tensorflow-gpu=2.0```\r\n- TensorFlow version (use command below):\r\nGIT_VERSION: v2.0.0-rc2-26-g64c3d38 VERSION: 2.0.0\r\n- Python version:\r\n3.7.3\r\n- CUDA/cuDNN version:\r\nCUDA: 10.0, cuDNN: 7.6.2.24-1\r\n- GPU model and memory:\r\nTesla P100 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen training LSTMs using the tf.keras API I get a warning:\r\n```\r\nW tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_2111_2291_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_2921' and '__inference___backward_cudnn_lstm_with_fallback_2111_2291' both implement 'lstm_9ed5de36-e395-476d-91c9-2cd0a6da489a' but their signatures do not match.\r\n```\r\n**Describe the expected behavior**\r\nShould probably not give any warnings\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nbatch_size = 16\r\nnum_batches = 4\r\nnum_timesteps = 100\r\nnum_features = 40\r\nnum_targets = 2\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.LSTM(2, input_shape=(None, num_features), return_sequences=True))\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.summary()\r\n\r\n\r\nX = tf.random.normal((batch_size * num_batches, num_timesteps, num_features))\r\ny = tf.random.normal((batch_size * num_batches, num_timesteps, num_targets))\r\n\r\nmodel.fit(X, y)\r\n```\r\n**Other info / logs**\r\n```\r\npython reproduce.py\r\n2019-11-28 10:07:13.562280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-28 10:07:13.988578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:13.999849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:13:00.0\r\n2019-11-28 10:07:13.999984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.061208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:1b:00.0\r\n2019-11-28 10:07:14.061465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-28 10:07:14.062923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-28 10:07:14.064178: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-28 10:07:14.064527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-28 10:07:14.066268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-28 10:07:14.067651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-28 10:07:14.071730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-28 10:07:14.071863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.072508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.073136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.073750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.074320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-11-28 10:07:14.074596: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-11-28 10:07:14.084185: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494140000 Hz\r\n2019-11-28 10:07:14.087023: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5618f79346f0 executing computations on platform Host. Devices:\r\n2019-11-28 10:07:14.087055: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-28 10:07:14.233596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.235446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.237594: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5618f7996f10 executing computations on platform CUDA. Devices:\r\n2019-11-28 10:07:14.237621: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2019-11-28 10:07:14.237630: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2019-11-28 10:07:14.237933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.238529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:13:00.0\r\n2019-11-28 10:07:14.238608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.239161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:1b:00.0\r\n2019-11-28 10:07:14.239197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-28 10:07:14.239214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-28 10:07:14.239226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-28 10:07:14.239241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-28 10:07:14.239257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-28 10:07:14.239272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-28 10:07:14.239288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-28 10:07:14.239349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.239942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.240527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.241117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.241680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-11-28 10:07:14.241720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-28 10:07:14.243465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-28 10:07:14.243489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\r\n2019-11-28 10:07:14.243502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N\r\n2019-11-28 10:07:14.243514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N\r\n2019-11-28 10:07:14.243666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.244276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.244873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.245455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:13:00.0, compute capability: 6.0)\r\n2019-11-28 10:07:14.245932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-28 10:07:14.246534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15216 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:1b:00.0, compute capability: 6.0)\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nlstm (LSTM)                  (None, None, 2)           344\r\n=================================================================\r\nTotal params: 344\r\nTrainable params: 344\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain on 64 samples\r\n2019-11-28 10:07:16.655379: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_2111_2291_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_2921' and '__inference___backward_cudnn_lstm_with_fallback_2111_2291' both implement 'lstm_9ed5de36-e395-476d-91c9-2cd0a6da489a' but their signatures do not match.\r\n2019-11-28 10:07:16.787844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n64/64 [==============================] - 3s 40ms/sample - loss: 1.0611\r\n```\r\n", "comments": ["Seems to be discussed in #30263, which is closed. Apparently because the warning can be disregarded. I still think that it should be removed, so keeping this open for now.", "@torjush ,\r\nWhen tried running the given code, i did not face any warnings. Please find the [gist](https://colab.sandbox.google.com/gist/oanush/a576f83e677d81035aa57a94c48e4d8c/34681.ipynb) of colab, kindly share your gist if the issue is faced. Thanks!", "After carefully rereading the comments of #30263, I see that there is indeed a fix for this coming in the next release, ref: https://github.com/tensorflow/tensorflow/issues/30263#issuecomment-548053368. Closing this issue.", "I met the same problem as you in version 2.0.0, is there any solution other than upgrading the version?"]}, {"number": 34680, "title": "Generator function didn't preserve needed nodes, copying old replacements back in instead.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nubuntu 16.04 / \r\npython 3.6 /\r\ntf 1.14.0 /\r\nonnx 1.6.0 \r\n\r\n**Describe the current behavior**\r\nI want to convert siamfc network pb model to onnx model.\r\nI build tensorflow siamfc project  from [https://github.com/bilylee/SiamFC-TensorFlow.git](url).\r\nafter a lot of step ,I got .ckpt.* file successfully, then I freeze to pb file.\r\nthis is a part of network below:\r\n![image](https://user-images.githubusercontent.com/16054460/69796416-f9e82380-1208-11ea-8859-d74889fb0917.png)\r\nthen,I need to convert to onnx model.\r\nby this cmd:\r\npython -m tf2onnx.convert --input=./siamfc-edit.pb --inputs=inference/examplar_input:0,inference/instance_input:0 --outputs=inference/detection/add:0 --opset=11 --continue_on_error --fold_const --output=siamfc-edit.onnx --verbose\r\nI think this cmd is correct, because I can convert a lot of other network successfully.\r\nbut when it convert , a lot of warning come up ,and never stop.\r\nso that I can't convert to onnx .the warning is below \r\n\r\n......\r\n2019-11-28 17:19:56.308150: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n2019-11-28 17:19:56.309946: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv2/b2/BatchNorm/beta/read/_10__cf__10 to be preserved.\r\n2019-11-28 17:19:56.309960: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv2/b2/BatchNorm/gamma/read/_11__cf__11 to be preserved.\r\n2019-11-28 17:19:56.309975: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv2/b2/BatchNorm/moving_mean/read/_12__cf__12 to be preserved.\r\n2019-11-28 17:19:56.309983: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv2/b2/BatchNorm/moving_variance/read/_13__cf__13 to be preserved.\r\n2019-11-28 17:19:56.310000: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n2019-11-28 17:19:56.319616: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv3/BatchNorm/beta/read/_15__cf__15 to be preserved.\r\n2019-11-28 17:19:56.319670: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv3/BatchNorm/gamma/read/_16__cf__16 to be preserved.\r\n2019-11-28 17:19:56.319678: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv3/BatchNorm/moving_mean/read/_17__cf__17 to be preserved.\r\n2019-11-28 17:19:56.319686: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv3/BatchNorm/moving_variance/read/_18__cf__18 to be preserved.\r\n2019-11-28 17:19:56.319718: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n2019-11-28 17:19:56.323429: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv4/b2/BatchNorm/beta/read/_25__cf__25 to be preserved.\r\n2019-11-28 17:19:56.323456: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv4/b2/BatchNorm/gamma/read/_26__cf__26 to be preserved.\r\n2019-11-28 17:19:56.323464: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv4/b2/BatchNorm/moving_mean/read/_27__cf__27 to be preserved.\r\n2019-11-28 17:19:56.323471: W tensorflow/tools/graph_transforms/transform_utils.cc:441] Expected convolutional_alexnet/conv4/b2/BatchNorm/moving_variance/read/_28__cf__28 to be preserved.\r\n2019-11-28 17:19:56.323479: W tensorflow/tools/graph_transforms/transform_utils.cc:448] Generator function didn't preserve needed nodes, copying old replacements back in instead.\r\n......\r\n\r\nso what's wrong with it or me?\r\n\r\nI expect to know what's this warning mean,I found there are a lot of identity op as input op share for other op. \r\n![image](https://user-images.githubusercontent.com/16054460/69797447-ad9de300-120a-11ea-9dd0-cd01138d4fd2.png)\r\ndoes it because of this?\r\n\r\nI look forward to your reply . thanks a lot.", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34680\">No</a>\n"]}, {"number": 34678, "title": "What value should I assign to the polling sizes (p) in main.py", "body": "It seems that there is no specific assigned value to the pooling sizes in main.py. I only figured out the last element should be 20.\r\n![\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_15749326291298](https://user-images.githubusercontent.com/30167606/69793254-06697d80-1203-11ea-89eb-50dc3d61e027.png)\r\n", "comments": ["Thanks a lot!"]}, {"number": 34677, "title": "Suspected memory leak when loading multiple models", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: **Yes**\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **Windows 10**\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: **NA**\r\n- **TensorFlow installed from (source or binary)**: **binary wheel via PyPI**\r\n- **TensorFlow version (use command below)**: **2.0.0**\r\n- **Python version**: **3.6.2**\r\n- **Bazel version (if compiling from source)**: **NA**\r\n- **GCC/Compiler version (if compiling from source)**: **NA**\r\n- **CUDA/cuDNN version**: **NA**\r\n- **GPU model and memory**: **NA**\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI'm suspecting a memory leak when loading multiple models(running on cpu only).\r\nWhen im running infinite loop that keeps creating the same model while using the same variable the memory (private bytes and working  set)  of the process keep increasing. At some points the working set seems to free some memory, but the trend is that the memory keeps on rising,\r\n\r\nThis trend happens even though I call gc.collect() on every iteration.\r\nIn addition - using gc.get_objects() I can see that every iteration leaks exactly 1928 new objects. Using objgraph the leaked objects are:\r\n\r\nIf I add  tf.keras.backend.clear_session() between iterations it does clear the memory and the graph of the memory is flat.\r\nCan someone please explain this behavior (and the mechanism) and approve that it is the correct flow when working with multiple models.\r\n\r\n**Describe the expected behavior**\r\nThe memory shouldnt increase on each interation\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport gc\r\nimport objgraph\r\n\r\ndef mem_stat():\r\n  objs = gc.get_objects()\r\n  print(\"total objects count\", len(objs))\r\n\r\nc = 1\r\nwhile True:\r\n  print(\"----------- iter\", c)\r\n  model = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  \r\n  gc.collect()\r\n  \r\n  print(\"mem stat after model creation:\")\r\n  mem_stat()\r\n  objgraph.show_growth(limit=30)\r\n  c += 1\r\n```\r\n\r\nwith the clear session:\r\n```\r\nimport tensorflow as tf\r\nimport gc\r\nimport objgraph\r\n\r\n\r\ndef mem_stat():\r\n  objs = gc.get_objects()\r\n  print(\"total objects count\", len(objs))\r\n\r\n\r\nc = 1\r\nwhile True:\r\n  print(\"----------- iter\", c)\r\n  model = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dropout(0.2),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  tf.keras.backend.clear_session()\r\n  gc.collect()\r\n  \r\n  print(\"mem stat after model creation:\")\r\n  mem_stat()\r\n  objgraph.show_growth(limit=30)\r\n  c += 1\r\n```\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nAttaching perfmon screenshot:\r\n\r\n![image](https://user-images.githubusercontent.com/43209657/69791462-457cdc00-11cd-11ea-8103-6ac73532be68.png)\r\n\r\n![image](https://user-images.githubusercontent.com/43209657/69791948-42362000-11ce-11ea-9ee3-17fbc67da753.png)\r\n\r\n\r\n", "comments": ["@bard1988 \r\n\r\nCan you please share the code with proper indentation or colab link to reproduce the issue .Thanks!", "> @bard1988\r\n> \r\n> Can you please share the code with proper indentation or colab link to reproduce the issue .Thanks!\r\n\r\nHi,\r\nPlease see the edited code (hope its good now)", "I have tried on colab with TF version 2.0, 2.1.0-dev20191201 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/ed055e5767cc83c0e74d447831f0a66e/untitled432.ipynb). Thanks!\r\n", "@bard1988 Just a quick question to understand more details about your issue.\r\n1. what is your objective? Do you want to make several model graphs or want to update same graph with different parameters? \r\n2. What is your use case?\r\n\r\nLooks like you are making many model graphs which is why it increases number of objects every iteration. Thanks!", "@bard1988 Can you please respond to my earlier response? Thanks!\r\n\r\nPlease close the issue If this was already resolved. Thanks!", "@jv\r\n\r\n> @bard1988 Just a quick question to understand more details about your issue.\r\n> \r\n> 1. what is your objective? Do you want to make several model graphs or want to update same graph with different parameters?\r\n> 2. What is your use case?\r\n> \r\n> Looks like you are making many model graphs which is why it increases number of objects every iteration. Thanks!\r\n\r\n hi, sorry for the late reply. \r\n\r\nour usage is a server that serves different models, and we want to keep only several models in memory, not all of them. this means that we wish to clear the ones that are bit being used.\r\n\r\nthe script here shows that deleting the model isnt enough for clearing the memory, have to use the clear session. ", "Clear session removes parts of the model that get stored in the global graph, so if you are running an infinite loop with no clear_session, this is expected. You can read more here: https://github.com/tensorflow/tensorflow/issues/28844#issuecomment-499758695"]}, {"number": 34676, "title": "When building a custom model using keras.Model, a warning was triggered", "body": "My environment:\r\nwindow10\r\nCUDA10.0\r\ntensorflow-gpu 1.14\r\n\r\nWhen I built a more complex model today, I triggered the following warning:\r\n\r\n> W1128 15: 22: 59.781456 32404 ag_logging.py:145] Entity <bound method DiMP.call of <__ main __. DiMP object at 0x0000024606A21208 >> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY = 10`) and attach the full output. Cause: (unicode error) 'utf-8' codec can't decode byte 0xca in position 250: invalid continuation byte (tmp56zh3z7b.py, line 15)\r\n\r\nI don't know if this warning will affect the model operation, and the content of the warning says that it needs to be reported. So I came here.\r\nBecause there are a lot of engineering components, I can't determine which part triggered this warning. I won't upload all the code for the time being. If I need to upload code, I will not refuse.\r\nHere is the entire print:\r\n\r\n> C:\\software\\Anaconda3\\envs\\CV_env\\python.exe C:/Users/stars_ocean/Desktop/DiMP_TF/models/DiMP.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1128 15:22:57.572051 32404 deprecation_wrapper.py:119] From C:\\Users\\stars_ocean\\Desktop\\DiMP_TF\\models\\backbone\\MobileNet_V3_By_Model.py:4: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\r\n\r\n> used backbone layers: [5, 12]\r\n> W1128 15:22:57.689736 32404 deprecation_wrapper.py:119] From C:\\Users\\stars_ocean\\Desktop\\DiMP_TF\\models\\classifiter\\Initializer.py:24: The name tf.keras.initializers.RandomNormal is deprecated. Please use tf.compat.v1.keras.initializers.RandomNormal instead.\r\n> \r\n> W1128 15:22:57.691731 32404 deprecation_wrapper.py:119] From C:\\Users\\stars_ocean\\Desktop\\DiMP_TF\\models\\classifiter\\Optimizer.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n> \r\n> 2019-11-28 15:22:57.692470: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n> 2019-11-28 15:22:58.922915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\n> pciBusID: 0000:01:00.0\r\n> 2019-11-28 15:22:58.923123: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n> 2019-11-28 15:22:58.926473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n> 2019-11-28 15:22:58.926787: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n> 2019-11-28 15:22:58.930528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\n> name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\n> pciBusID: 0000:01:00.0\r\n> 2019-11-28 15:22:58.930820: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n> 2019-11-28 15:22:58.934215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n> 2019-11-28 15:22:59.447520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-11-28 15:22:59.447669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n> 2019-11-28 15:22:59.447755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n> 2019-11-28 15:22:59.453515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2995 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> W1128 15:22:59.781456 32404 ag_logging.py:145] Entity <bound method DiMP.call of <__main__.DiMP object at 0x0000024606A21208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: (unicode error) 'utf-8' codec can't decode byte 0xca in position 250: invalid continuation byte (tmp56zh3z7b.py, line 15)\r\n> W1128 15:23:05.219440 32404 deprecation.py:323] From C:\\software\\Anaconda3\\envs\\CV_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Use tf.where in 2.0, which has the same broadcast rule as np.where\r\n> 2019-11-28 15:23:06.199208: W tensorflow/core/common_runtime/eager/context.cc:371] Added two functions with the same name: __inference_conv_and_transpose_10931\r\n> \r\n> Process finished with exit code 0\r\n> ", "comments": ["I know why that warning is triggered. Because i am in function\r\n`def call (self, inputs, training = None, mask = None):`\r\nThis is caused by a long paragraph with \"\"\"(omitted)\"\"\".\r\nI deleted those comments and they didn't trigger the warning.\r\nI think this warning is very unreasonable. I think adding a function description will not cause errors in the code, so it should not trigger any warning."]}, {"number": 34675, "title": "Feature request: lock some of the layers alternately in training", "body": "For example, a model with 3 dense layers named L0, L1, L2. Train it in below steps:\r\n\r\n1, Set all layers trainable, train it for 10 epochs.\r\n2, Set L0 not trainable, train it for 10 epochs.\r\n3, Set L0 trainable, set L1 not trainable, train it for 10 epochs.\r\n4, Set L1 trainable, set L2 not trainable, train it for 10 epochs.\r\n5, Like step 2, set L2 trainable, set L0 not trainable, train it for 10 epochs.\r\n6, Like step 3.\r\n.......\r\n\r\nIt seems to train in this way is better than to train all the layers together. I think to reduce the trainable layers may reduce the gradient disappearance.\r\n\r\nBut for each time to alter the layers I have to recompile the whole model. I want there is some feature to do it automatically something like dropout.\r\n\r\nIf there already has this feature, please notice me.", "comments": ["@saintthor You can take layers from a model instance and freeze them. To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. And yes for this, you have to compile the model again.\r\n\r\nAs of now there is no feature that does what you are expecting to do.", "> @saintthor You can take layers from a model instance and freeze them. To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. And yes for this, you have to compile the model again.\r\n> \r\n> As of now there is no feature that does what you are expecting to do.\r\n\r\nTo freeze is to set Layer.trainable = False?  I did it before. thanks.", "Recompiling is the recommended way to do this, or you can look at creating custom layers/optimizers for this. However, it is too complicated a use-case to bake in to the keras lib at this time."]}, {"number": 34674, "title": "`py_function` in eager mode with Tensors fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-8709-gddde447 2.0.0-dev20190813\r\n- Python version: 3.6.9\r\n\r\n## Current Behaviour\r\n\r\n`tf.py_function` fails when run in eager mode with `tf.Tensor`s from `tf.keras.Input`s\r\n\r\n## Expected Behariour\r\n\r\nSame as in graph mode.\r\n\r\n## Code to reproduce the issue\r\n```python\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\nx = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\ny = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\n\r\ndef f(x, y):\r\n    return x + y\r\n\r\nz = tf.py_function(f, (x, y), tf.float32)\r\n```\r\n\r\n## Stack trace\r\n```\r\nTraceback (most recent call last):\r\n  File \"../site-packages/tensorflow_core/python/ops/gen_script_ops.py\", line 53, in eager_py_func\r\n    token, \"is_async\", is_async, \"Tout\", Tout)\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"pfk.py\", line 11, in <module>\r\n    z = tf.py_function(f, (x, y), tf.float32)\r\n  File \"../site-packages/tensorflow_core/python/ops/script_ops.py\", line 404, in eager_py_func\r\n    return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\r\n  File \"../site-packages/tensorflow_core/python/ops/script_ops.py\", line 293, in _internal_py_func\r\n    name=name)\r\n  File \"../site-packages/tensorflow_core/python/ops/gen_script_ops.py\", line 59, in eager_py_func\r\n    ctx=_ctx)\r\n  File \"../site-packages/tensorflow_core/python/ops/gen_script_ops.py\", line 113, in eager_py_func_eager_fallback\r\n    _attr_Tin, input = _execute.convert_to_mixed_eager_tensors(input, _ctx)\r\n  File \"../site-packages/tensorflow_core/python/eager/execute.py\", line 277, in convert_to_mixed_eager_tensors\r\n    types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access\r\n  File \"../site-packages/tensorflow_core/python/eager/execute.py\", line 277, in <listcomp>\r\n    types = [t._datatype_enum() for t in v]  # pylint: disable=protected-access\r\nAttributeError: 'Tensor' object has no attribute '_datatype_enum'\r\n```", "comments": ["Workaround: wrap it in a `Lambda`, though this is highly unexpected/unintuitive.\r\n\r\n```python\r\ndef run(args):\r\n    return tf.py_function(f, args, tf.float32)\r\n\r\nz = tf.keras.layers.Lambda(run)([x, y])\r\n```", "I could reproduce the issue with Tensorflow 2.0.\r\nPlease see the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3a4d6e2b631dbf57791b55c689142a0c/untitled279.ipynb). Thanks!", "Wrapping any graph computation in a Lambda layer or a custom Layer (putting code in the `call` body) is the correct way to do it. There is a mechanism called the TensorFlowOpLayer which tries to wrap \"bare\" computations in layers, but it is best-effort only. So your workaround isn't really a workaround.", "Ack, was about to say \"I'd prefer a more helpful error message\" then I realized the top traceback says almost exactly that - just not where I normally look for the underlying cause...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34674\">No</a>\n"]}, {"number": 34673, "title": "what is the model inception5h?", "body": "With what data is this model trained?\r\n\r\nHow do I keep training it?", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "@molo32 The inception model is trained using the Imagenet dataset.\r\nSee https://github.com/tensorflow/models/tree/master/research/inception#inception-in-tensorflow\r\nhttp://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\nYou may use transfer learning approach to continue training further.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34672, "title": "You must feed a value for placeholder tensor 'Placeholder' with dtype int32", "body": "```\r\n    import tensorflow as tf  \r\n\r\n    d1 = tf.placeholder(tf.int32)\r\n    d2 = tf.add(6, 2, name=\"Add_these_numbers2\")\r\n    d3 = tf.add(d1, d2, name=\"res5\")\r\n    d4 = tf.add(d1, d3, name=\"res5\")\r\n    \r\n    with tf.Session() as sess:\r\n       # writer = tf.summary.FileWriter(\"output\", sessi.graph)\r\n        print(sess.run(d3))\r\n        print(sess.run(d4,feed_dict={d1:0}))\r\n```\r\n\r\n\r\nwhy do you give me this error?\r\n\r\n\r\nuse python 3 with google colab gpu\r\nF", "comments": ["@molo32 ,\r\nTensor will produce an error unless [value is fed ](https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder#for_example)with the `feed_dict `optional argument, line   `print(sess.run(d3))` throws an error as the value is not fed.\r\nTry running the code\r\n```\r\n import tensorflow as tf  \r\n d1 = tf.placeholder(tf.int32)\r\n d2 = tf.add(6, 2, name=\"Add_these_numbers2\")\r\n d3 = tf.add(d1, d2, name=\"res5\")\r\n d4 = tf.add(d1, d3, name=\"res5\")\r\n    \r\n with tf.Session() as sess:\r\n       # writer = tf.summary.FileWriter(\"output\", sessi.graph)\r\n       #print(sess.run(d3)) (remove this line or comment it)\r\n      print(sess.run(d4,feed_dict={d1:0}))\r\n```\r\nkindly find the [gist](https://colab.sandbox.google.com/gist/oanush/b3889cf62e4f582311f34fe67d38c292/34672.ipynb) of colab.Thanks!", "@molo32 ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34671, "title": "how can i visualize activation of a layer?", "body": "python 3\r\ngoogle colab\r\ngpu", "comments": ["@molo32 \r\nPlease refer the [link](https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0) for visualization activation in CNN using keras. Let us know if it helped.\r\nMeanwhile Can you please provide more information issue? Thanks!", "@molo32 ,\r\nany update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34670, "title": "How to stop the process while sess.run in TensorFlowInferenceInterface?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm now developing the application with TensorFlowInferenceInterface in android. What I tried to find was stop function that stop the running calculation by setting some flag that can make calculation quit. However, the only thing I found was `TensorFlowInferenceInterface.close()` that couldn't stop running calculation. \r\n\r\nIs there any feature to set flag to stop the process? If there is already related information, I'm sorry and please let me know :)\r\n\r\n**Will this change the current api? How?**\r\nLittle bit?\r\n\r\n**Who will benefit with this feature?**\r\nAndroid developers who are running the Tensorflow can make their application more responsive to the user.\r\n\r\n**Any Other info.**\r\nI run TensorFlowInferenceInterface with `rxJava` to serve it in background.\r\n\r\nThank you so much!", "comments": ["There's no way to do this. Note that TFLite actually has a cancellation API for doing this, though it's not yet exposed in Java. Is there a reason you have to use TFMobile (used by TensorFlowInferenceInterface) rather than TFLite?", "@jdduke I see. We used TFMobile because TFLite doesn't provide much operations related to the Transformer as I searched correctly. Actually, we will use TFLite if the related operations are implemented. Thank you for your reply :)"]}, {"number": 34669, "title": "armeabi-v7a libtensorflowlite_jni.so\uff1asignal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xeef5445f", "body": "11-26 06:29:37.207 25216 25216 F DEBUG   : Revision: '1234'\r\n11-26 06:29:37.207 25216 25216 F DEBUG   : ABI: 'arm'\r\n11-26 06:29:37.207 25216 25216 F DEBUG   : pid: 24454, tid: 24528, name: RxCachedThreadS  \r\n11-26 06:29:37.207 25216 25216 F DEBUG   : signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xeef5445f\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     r0  eef5445f  r1  cd222804  r2  cd222800  r3  00000000\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     r4  0000005a  r5  00000000  r6  cd2234c0  r7  d1ec0f28\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     r8  00000000  r9  eef5445f  r10 00000168  r11 cd2234c0\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     ip  00000009  sp  d1ec0e68  lr  00000004  pc  cd725ede", "comments": ["11-26 06:29:37.207 25216 25216 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n11-26 06:29:37.207 25216 25216 F DEBUG   : Build fingerprint: \r\n11-26 06:29:37.207 25216 25216 F DEBUG   : Revision: '1234'\r\n11-26 06:29:37.207 25216 25216 F DEBUG   : ABI: 'arm'\r\n11-26 06:29:37.207 25216 25216 F DEBUG   : pid: 24454, tid: 24528, name: RxCachedThreadS  >>> com.tensorflow.jni <<<\r\n11-26 06:29:37.207 25216 25216 F DEBUG   : signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0xeef5445f\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     r0  eef5445f  r1  cd222804  r2  cd222800  r3  00000000\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     r4  0000005a  r5  00000000  r6  cd2234c0  r7  d1ec0f28\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     r8  00000000  r9  eef5445f  r10 00000168  r11 cd2234c0\r\n11-26 06:29:37.207 25216 25216 F DEBUG   :     ip  00000009  sp  d1ec0e68  lr  00000004  pc  cd725ede\r\n11-26 06:29:37.292 25216 25216 F DEBUG   : \r\n11-26 06:29:37.292 25216 25216 F DEBUG   : backtrace:\r\n11-26 06:29:37.292 25216 25216 F DEBUG   :     #00 pc 00093ede  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/lib/arm/libtensorflowlite_jni.so\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #01 pc 0009536b  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/lib/arm/libtensorflowlite_jni.so\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #02 pc 00092bc3  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/lib/arm/libtensorflowlite_jni.so\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #03 pc 000ce64d  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/lib/arm/libtensorflowlite_jni.so\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #04 pc 000080af  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+26)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #05 pc 00037691  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.odex (offset 0x2f000) (org.tensorflow.lite.NativeInterpreterWrapper.run+120)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #06 pc 00416b75  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #07 pc 003f0397  /system/lib/libart.so (art_quick_invoke_static_stub+222)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #08 pc 000a103b  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+154)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #09 pc 001e83bd  /system/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+232)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #10 pc 001e309d  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+776)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #11 pc 003ebe93  /system/lib/libart.so (MterpInvokeStatic+130)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #12 pc 00409b14  /system/lib/libart.so (ExecuteMterpImpl+14612)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #13 pc 00b15e72  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (org.tensorflow.lite.NativeInterpreterWrapper.run+166)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #14 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.293 25216 25216 F DEBUG   :     #15 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #16 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #17 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #18 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #19 pc 00b15858  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #20 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #21 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #22 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #23 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #24 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #25 pc 00b15842  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (org.tensorflow.lite.Interpreter.run+36)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #26 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.294 25216 25216 F DEBUG   :     #27 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #28 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #29 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #30 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #31 pc 004a1b36  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (com.tensorflow.jni.core.nlp.offline.LocalNLP.process+234)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #32 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #33 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.295 25216 25216 F DEBUG   :     #34 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #35 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #36 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #37 pc 0047f032  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (com.tensorflow.jni.core.nlp.NlpScheduler.lambda$requestLocal$0$NlpScheduler+42)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #38 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #39 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #40 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #41 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #42 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #43 pc 0047e83a  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (com.tensorflow.jni.core.nlp.NlpScheduler$$Lambda$0.subscribe+8)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #44 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #45 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #46 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #47 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #48 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.296 25216 25216 F DEBUG   :     #49 pc 00a1e2e0  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (io.reactivex.internal.operators.observable.ObservableCreate.subscribeActual+20)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #50 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #51 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #52 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #53 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #54 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #55 pc 00246994  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (io.reactivex.Observable.subscribe+30)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #56 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #57 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #58 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #59 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #60 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #61 pc 00a28e00  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (io.reactivex.internal.operators.observable.ObservableSubscribeOn$SubscribeTask.run+12)\r\n11-26 06:29:37.297 25216 25216 F DEBUG   :     #62 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #63 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #64 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #65 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #66 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #67 pc 005e9120  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (io.reactivex.Scheduler$DisposeTask.run+18)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #68 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #69 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #70 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #71 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #72 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #73 pc 00a36e5e  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (io.reactivex.internal.schedulers.ScheduledRunnable.run+26)\r\n11-26 06:29:37.298 25216 25216 F DEBUG   :     #74 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #75 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #76 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #77 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #78 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #79 pc 00a36d2a  /data/app/com.tensorflow.jni-kNNGQ77u2Lr-Bq2GxxZZEg==/oat/arm/base.vdex (io.reactivex.internal.schedulers.ScheduledRunnable.call)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #80 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #81 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #82 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #83 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #84 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #85 pc 001130fa  /system/framework/boot-core-oj.vdex (java.util.concurrent.FutureTask.run+62)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #86 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #87 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #88 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.299 25216 25216 F DEBUG   :     #89 pc 003eb3cd  /system/lib/libart.so (MterpInvokeSuper+1096)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #90 pc 00409a14  /system/lib/libart.so (ExecuteMterpImpl+14356)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #91 pc 00115776  /system/framework/boot-core-oj.vdex (java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run+38)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #92 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #93 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #94 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #95 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #96 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #97 pc 001170e0  /system/framework/boot-core-oj.vdex (java.util.concurrent.ThreadPoolExecutor.runWorker+162)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #98 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #99 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #100 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #101 pc 003eaeb7  /system/lib/libart.so (MterpInvokeVirtual+442)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #102 pc 00409994  /system/lib/libart.so (ExecuteMterpImpl+14228)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #103 pc 0011640e  /system/framework/boot-core-oj.vdex (java.util.concurrent.ThreadPoolExecutor$Worker.run+4)\r\n11-26 06:29:37.300 25216 25216 F DEBUG   :     #104 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #105 pc 001cc2e3  /system/lib/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+146)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #106 pc 001e3087  /system/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+754)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #107 pc 003eba85  /system/lib/libart.so (MterpInvokeInterface+1020)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #108 pc 00409b94  /system/lib/libart.so (ExecuteMterpImpl+14740)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #109 pc 000ca806  /system/framework/boot-core-oj.vdex (java.lang.Thread.run+12)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #110 pc 001c7a15  /system/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3885687342+352)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #111 pc 001cc22f  /system/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+82)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #112 pc 003de6fb  /system/lib/libart.so (artQuickToInterpreterBridge+890)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #113 pc 0041b0ff  /system/lib/libart.so (art_quick_to_interpreter_bridge+30)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #114 pc 00416b75  /system/lib/libart.so (art_quick_invoke_stub_internal+68)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #115 pc 003f0293  /system/lib/libart.so (art_quick_invoke_stub+226)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #116 pc 000a1029  /system/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+136)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #117 pc 0034f9dd  /system/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+52)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #118 pc 00350769  /system/lib/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+316)\r\n11-26 06:29:37.301 25216 25216 F DEBUG   :     #119 pc 0037230d  /system/lib/libart.so (art::Thread::CreateCallback(void*)+900)\r\n11-26 06:29:37.302 25216 25216 F DEBUG   :     #120 pc 000639f5  /system/lib/libc.so (__pthread_start(void*)+22)\r\n11-26 06:29:37.302 25216 25216 F DEBUG   :     #121 pc 0001df65  /system/lib/libc.so (__start_thread+22)", "@yanceyblog,\r\nPlease Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "@yanceyblog, Provide more information about the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]