[{"number": 26070, "title": "Model Memory requirements", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWant to print memory requirement of the model given a particular input size, batch size and a Structure of a model. It would cause Optimal GPU usage and also give a fair idea on what batch size should be.\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nEveryone who doesn't have a lot of compute and needs to know more specifics of the model\r\n**Any Other info.**\r\n", "comments": ["@Geeks-Sid,\r\nSorry for the delayed response. Can you please let us know if [Tensorflow Profiler](https://www.tensorflow.org/guide/profiler) is what you are looking for? Thanks! ", "Switched to torch in March 2019. Sorry for the issue."]}, {"number": 26069, "title": "Many operations don't support uint64", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): b'unknown' 1.13.0-rc1\r\n- Python version: Python 3.7.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI've encountered several operations that support int64 but not uint64, without any clear reasoning. `tf.equal`, `tf.fill`, `tf.where`, and `tf.stack`, for example, give errors like:\r\n\r\n    InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Pack' used by node stack (defined at <stdin>:4) with these attrs: [T=DT_UINT64, axis=0, N=2]\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nFunctions that work for int64s should also work for uint64s when the behavior would be the same.\r\n\r\n**Code to reproduce the issue**\r\n\r\nhttps://gist.github.com/hjfreyer/31ab2dd2d85d1a509272af1c5e011dde\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@hjfreyer Most of the operations in TF does not support uint64. There are some historical and reality reasons as far as I know. One is the binary size which could be really big if all signed and unsigned int (8/16/32/64) are enabled. Also, some integer types does not work on GPU for certain math operations yet. \r\n\r\nBy default, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64` are supported for most of the ops. `uint32` and `uint64` are supported for `bitwise` ops additionally. There are also some ops that are merely memory/type manipulation (like `cast`) so `uint32` and `uint64` are supported as well.\r\n\r\nFrom the list you provide, `tf.equal`, `tf.fill`, `tf.where`, and `tf.stack`, my guess is that they will not be supported, unless `uint32` and `uint64` are added to the default list and applies to almost all other ops.\r\n\r\nThis might take some time and might need guidance from api team I believe.", "(For API owners) We can take a look when there is a specific proposal to review.", "> @hjfreyer Most of the operations in TF does not support uint64. There are some historical and reality reasons as far as I know. One is the binary size which could be really big if all signed and unsigned int (8/16/32/64) are enabled. Also, some integer types does not work on GPU for certain math operations yet.\r\n> \r\n> By default, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64` are supported for most of the ops. `uint32` and `uint64` are supported for `bitwise` ops additionally. There are also some ops that are merely memory/type manipulation (like `cast`) so `uint32` and `uint64` are supported as well.\r\n> \r\n> From the list you provide, `tf.equal`, `tf.fill`, `tf.where`, and `tf.stack`, my guess is that they will not be supported, unless `uint32` and `uint64` are added to the default list and applies to almost all other ops.\r\n> \r\n> This might take some time and might need guidance from api team I believe.\r\n\r\n@yongtang is there some definitive list what is to be supported, or what shouldn't?\r\ne.g. I just stumbled over the fact that for dtype `tf.uint16` operator == is not implemented \u2026 something I would file as a bug, however if it is `wontfix`, then this should be documented somewhere \u2026\r\n\r\nI just took a look at more combinations:\r\n```python \r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.errors_impl import NotFoundError\r\n\r\ndtypes = set([dtype for dtype in tf.dtypes.__dict__.values() if isinstance(dtype, tf.dtypes.DType)]) - {tf.resource, tf.variant}\r\n\r\nfor dtype in dtypes:\r\n    a = tf.zeros(1, dtype=dtype)\r\n    try:\r\n        print(dtype, a == a)\r\n    except NotFoundError:\r\n        print(dtype, 'operator == not implemented')\r\n```\r\n```\r\n> TF_CPP_MIN_LOG_LEVEL=3 python test.py\r\n<dtype: 'float32'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'float64'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'int32'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'uint8'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'int16'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'int8'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'string'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'complex64'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'int64'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'bool'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'qint8'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'quint8'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'qint32'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'bfloat16'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'qint16'> operator == not implemented\r\n<dtype: 'quint16'> operator == not implemented\r\n<dtype: 'uint16'> operator == not implemented\r\n<dtype: 'complex128'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'float16'> tf.Tensor([ True], shape=(1,), dtype=bool)\r\n<dtype: 'uint32'> operator == not implemented\r\n<dtype: 'uint64'> operator == not implemented\r\n```\r\n\r\nSo `qint16`, `quint16`, `uint16`, `uint32`, `uint64` don't even have an equality operator implemented \u2026 that makes those datatypes very rudimentary. ", "@csachs I added a PR #38288 for uint16, uint32, and uint64 for tf.math.equal and tf.math.not_equal.", "@hjfreyer \r\n\r\nI am not seeing any issue with TF version ,1.15,2.x.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d8387b0dd947a2537fd55d3aec87267f/untitled123.ipynb).Please,verify once and close the issue.Thanks!", "@ravikyram \r\nThe problems raised in my comment remain open.\r\nFurthermore, there seems a regression, in earlier versions:\r\n`tf.zeros(1, tf.qint16)` worked, now it doesn't.\r\nShould I open another issue?", "@csachs The tf.zeros issue is being addressed in PR #41421 (tf.zeros uses FillOp implicitly).", "@csachs \r\n\r\nThe tf.zeros issue is being addressed in PR #41421 also got merged. Can you please verify once.Thanks!", "Added a PR #41795 to cover qint8/quint8/qint16/quint16 for `tf.math.[equal|not_equal]`.", "> @csachs\r\n> \r\n> The tf.zeros issue is being addressed in PR #41421 also got merged. Can you please verify once.Thanks!\r\n\r\nThank you @ravikyram , using version 2.4.0-dev20200728 , the `tf.zeros` issue is fixed.\r\n\r\nHowever the larger code snippet above does not run (producing a new error); I guess it will resolve with https://github.com/tensorflow/tensorflow/pull/41795 .", "Most of the dtypes in https://github.com/tensorflow/tensorflow/issues/26069#issuecomment-604608722 are already supported now except for `tf.qint32` (with `tf.zeros`). Added a PR #46313 to add `tf.qint32` for `tf.zeros`.", "The support of tf.qint16 and tf.quint16 for tf.stack is being added in #46404", "@hjfreyer,\r\nWith @yongtang's PRs, many operations support uint64 now (**`Tensorflow Version 2.4.1`**). Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/4f4124c514714df8fe4456a3c589209f/gh_26069.ipynb) of the Working Code. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26069\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26069\">No</a>\n"]}, {"number": 26068, "title": "Incorrect predictions while exporting keras model to android", "body": "I trained the Keras model on State Farm Distracted Driver Dataset and exported it to **.pb** graph file with the code given below:\r\n\r\n```\r\ndef export_model(saver, model, input_node_names, output_node_name):\r\n    tf.train.write_graph(K.get_session().graph_def, 'out', \\\r\n        MODEL_NAME + '_graph.pbtxt')\r\n\r\n    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\r\n\r\n    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\r\n        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\r\n        \"save/restore_all\", \"save/Const:0\", \\\r\n        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\r\n\r\n    input_graph_def = tf.GraphDef()\r\n    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\r\n        input_graph_def.ParseFromString(f.read())\r\n\r\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n            input_graph_def, input_node_names, [output_node_name],\r\n            tf.float32.as_datatype_enum)\r\n\r\n    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n\r\n    print(\"graph saved!\")\r\n```\r\n\r\n The problem is when I run that exported model on android, it always predicts wrong classes. The same code below predicts the correct labels when given .pb file of model trained on MNIST Dataset from [source.](https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/blob/master/mnistandroid/app/src/main/assets/opt_mnist_convnet-keras.pb).\r\n\r\n```\r\npublic class MainActivity extends AppCompatActivity {\r\n    ImageView imageView;\r\n    Button button1;\r\n    TextView label;\r\n    private static final int PixelWidth = 100;\r\n    //private List<Classifier> mClassifiers = new ArrayList<>();\r\n    public static final int cam_req = 999; // used in OpenCamera Function\r\n    public Classifier c11;\r\n    private static final int PICK_IMAGE = 1;\r\n    Uri imageUri;\r\n    private boolean isLoaded= true;\r\n    private float[] imageNormalizedPixels;\r\n    private int [] imageBitmapPixels;\r\n    private byte[] bytearray;\r\n\r\n    @Override\r\n    protected void onCreate(Bundle savedInstanceState) {\r\n        c11 = new ModelClassifier();\r\n        loadModel();\r\n\r\n        super.onCreate(savedInstanceState);\r\n        setContentView(R.layout.activity_main);\r\n        imageView = (ImageView) findViewById(R.id.imageView);\r\n        label = (TextView)findViewById(R.id.label_output);\r\n        button1 = (Button) findViewById(R.id.button);\r\n        button1.setOnClickListener(new View.OnClickListener() {\r\n                                       @Override\r\n                                       public void onClick(View v) {\r\n                                           openGallery();\r\n                                       }\r\n                                   }\r\n\r\n        );\r\n\r\n\r\n    }\r\n\r\n    private void loadModel() {\r\n        //The Runnable interface is another way in which you can implement multi-threading other than extending the\r\n        // //Thread class due to the fact that Java allows you to extend only one class. Runnable is just an interface,\r\n        // //which provides the method run.\r\n        // //Threads are implementations and use Runnable to call the method run().\r\n        new Thread(new Runnable() {\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    c11=ModelClassifier.create(getAssets(), \"Keras\", \"opt_keras-10grey.pb\", \"labels.txt\", PixelWidth,\r\n                          \"input_3\", \"softmax_1/Softmax\", false);\r\n\r\n                } catch (final Exception e) {\r\n                    //if they aren't found, throw an error!\r\n                    label.setText(\"Model not loaded\");\r\n                    isLoaded = false;\r\n                    throw new RuntimeException(\"Error initializing classifiers!\", e);\r\n                }\r\n            }\r\n        }).start();\r\n    }\r\n\r\n\r\n\r\n    private void openGallery() {\r\n        if (isLoaded) {\r\n            Intent gallery = new Intent(Intent.ACTION_PICK, MediaStore.Images.Media.INTERNAL_CONTENT_URI);\r\n            startActivityForResult(gallery, PICK_IMAGE);\r\n        }\r\n\r\n    }\r\n\r\n    public void OpenCamera(View view) {\r\n\r\n        if (isLoaded) {\r\n\r\n            Intent intnt = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);\r\n            startActivityForResult(intnt, cam_req);\r\n        }\r\n    }\r\n\r\n\r\n    @Override\r\n    protected void onActivityResult(int requestCode, int resultCode, Intent Data) {\r\n        super.onActivityResult(requestCode, resultCode, Data);\r\n        if (resultCode == RESULT_OK && requestCode == PICK_IMAGE) {\r\n\r\n            try {\r\n                final Uri imageUri = Data.getData();\r\n                final InputStream imageStream = getContentResolver().openInputStream(imageUri);\r\n                final Bitmap selectedImage = BitmapFactory.decodeStream(imageStream);\r\n                ImageView imageView = (ImageView) findViewById(R.id.imageView);\r\n                imageView.setImageBitmap(selectedImage);\r\n                imageView.invalidate();\r\n                BitmapDrawable drawable = (BitmapDrawable) imageView.getDrawable();\r\n\r\n                Bitmap image = Bitmap.createScaledBitmap(selectedImage, PixelWidth, PixelWidth, false);\r\n\r\n                float [] pixels = getPixels(image);\r\n\r\n                if (pixels!=null) {\r\n                    System.out.println(\"It is gallery image\");\r\n\r\n                    final Classification returned = c11.recognize(pixels);\r\n                    System.out.println(\"this is the predicted class: \"+returned.getLabel());\r\n                    label.setText(returned.getLabel());\r\n\r\n\r\n                }\r\n\r\n            } catch (FileNotFoundException e) {\r\n                e.printStackTrace();\r\n            }\r\n\r\n   }\r\n\r\n\r\n    public float[] getPixels(Bitmap bitmap)\r\n    {\r\n\r\n        // Get 100x100 pixel data from bitmap\r\n        int[] pixels = new int[PixelWidth * PixelWidth];\r\n        bitmap.getPixels(pixels, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n        float[] retPixels = new float[pixels.length];\r\n        for (int i = 0; i < pixels.length; ++i) {\r\n            // Set 0 for white and 255 for black pixel\r\n            int pix = pixels[i];\r\n            retPixels[i] = pix /255f;\r\n        }\r\n        return retPixels;\r\n    }\r\n\r\n    }\r\n\r\n```\r\n**ModelClassifier.java** class is given below:\r\n\r\n```\r\n\r\n\r\npublic class ModelClassifier implements Classifier {\r\n\r\n    private static final float THRESHOLD = 0.1f;\r\n    private TensorFlowInferenceInterface tfHelper;\r\n\r\n    private String name;\r\n    private String InputName;\r\n    private String OutputName;\r\n\r\n    private int inputsize;\r\n\r\n    private boolean feedKeepProb;\r\n\r\n    private List<String> Labels;\r\n    private float[] Output;\r\n\r\n    private String OutputNames[];\r\n\r\n\r\n   /*public  ModelClassier(Context a){\r\n       Intent a1=a;\r\n    }*/\r\n\r\n    private static List<String> readLabels(AssetManager am, String fileName) throws IOException {\r\n        BufferedReader br = new BufferedReader(new InputStreamReader(am.open(fileName)));\r\n\r\n        String line;\r\n        List<String> labels = new ArrayList<>();\r\n        while ((line = br.readLine()) != null) {\r\n            labels.add(line);\r\n        }\r\n\r\n        br.close();\r\n        return labels;\r\n    }\r\n\r\n    public static ModelClassifier create(AssetManager assetManager, String name,\r\n                                         String modelPath, String labelFile, int inputSize, String inputName, String outputName,\r\n                                         boolean feedKeepProb) throws IOException {\r\n\r\n        ModelClassifier c = new ModelClassifier();\r\n\r\n\r\n        c.name = name;\r\n\r\n        c.InputName = inputName;\r\n        c.OutputName = outputName;\r\n\r\n\r\n        c.Labels = readLabels(assetManager, labelFile);\r\n        System.out.println(\"These are the labels\"+c.Labels.toString());\r\n\r\n        c.tfHelper = new TensorFlowInferenceInterface(assetManager, modelPath);\r\n        System.out.println(\"ModelPath is++++++++++\" + modelPath);\r\n        int numClasses = 10;\r\n\r\n\r\n        c.inputsize = inputSize;\r\n\r\n\r\n        c.OutputNames = new String[]{outputName};\r\n\r\n        c.OutputName = outputName;\r\n        c.Output = new float[numClasses];\r\n\r\n        c.feedKeepProb = feedKeepProb;\r\n        System.out.println(\"Model is finally loaded++++++++++\");\r\n        return c;\r\n    }\r\n\r\n\r\n    //@Override\r\n    public String name() {\r\n        return name;\r\n    }\r\n\r\n\r\n    @Override\r\n    public Classification recognize(float[] pixels) {\r\n\r\n        //using the interface\r\n        //give it the input name, raw pixels from the drawing,\r\n        //input size\r\n        tfHelper.feed(InputName, pixels, new long[]{1, inputsize, inputsize, 1});\r\n\r\n        //probabilities\r\n        if (feedKeepProb) {\r\n            tfHelper.feed(\"keep_prob\", new float[]{1});\r\n            System.out.println(\"Infeedprob++++++++++\");\r\n        }\r\n        //get the possible outputs\r\n        tfHelper.run(OutputNames);\r\n\r\n\r\n        tfHelper.fetch(OutputName, Output);\r\n        System.out.println(\"afterfetch++++++++++\");\r\n        // Find the best classification\r\n        //for each output prediction\r\n        //if its above the threshold for accuracy we predefined\r\n        //write it out to the view\r\n        Classification ans = new Classification();\r\n        for (int i = 0; i < Output.length; ++i) {\r\n            if (Output[i] > THRESHOLD && Output[i] > ans.getConf()) {\r\n                ans.update(Output[i], Labels.get(i));\r\n            }\r\n\r\n        }\r\n\r\n        return ans;\r\n    }\r\n\r\n```\r\n**Note that model input shape is [100,100,1]**\r\n\r\nPlease tell me where I am making mistake. I have tried training on different number of classes but the outputs are completely different. \r\n\r\n\r\n\r\n\r\n", "comments": ["Do you have any example of model usage in Python?", "Yes. Although when I export the model through transfer learning model works fine on android. It gives completely wrong predictions on model trained above. I can't figure out what is the problem.\r\n", "Apologies for the delay, @AzharSindhi is this still an issue with the latest TFLite release?", "@jdduke I haven't checked it on the latest release. You can close this issue now.\r\n\r\nThank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26068\">No</a>\n"]}, {"number": 26067, "title": "Updated description & example usage", "body": "Made changes per issue #25844:\r\n- Updated description to mention tf.Keras model. \r\n- Added the missing line in the example usage for SavedModel and tf.keras model.", "comments": ["This PR has been approved but 2 checks failed. What's the next step on this?", "@margaretmz could you please check build errors", "@dynamicwebpaige please advise the next steps on this PR, which has been approved a long time ago but 2 checks were not successful. Thanks!"]}, {"number": 26066, "title": "Added ```python for usage examples", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26066) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26066) for more info**.\n\n<!-- ok -->"]}, {"number": 26065, "title": "Reformatted variable names.", "body": "", "comments": ["@dynamicwebpaige gentle ping to resolve conflicts.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26064, "title": "Reformatted example variables, fixed typo.", "body": "", "comments": []}, {"number": 26063, "title": "Format usage examples in docstring for Python.", "body": "", "comments": ["@dynamicwebpaige  please resolve merge conflicts\r\n", "@dynamicwebpaige gentle ping to resolve the conflicts", "> @dynamicwebpaige gentle ping to resolve the conflicts\r\n\r\ngentle ping", "It has been 25 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26062, "title": "Added the Padding and Activation scenarios", "body": "This was one of the TODO items", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@haozha111 , thanks for pointing this out, i have made the changes as suggested by you, kindly check and approve.", "@haozha111 Sure, made the change as per the comments, kindly check and approve", "@haozha111 , thanks for the feedback, i have updated the code as per your suggestion. Kindly check and approave.", "@haozha111 , sorry for the overlook, i have fixed the issue, kindly check and approve.", "@haozha111 , I have updated the code as per the suggestions, kindly check and approve,\r\n\r\nRegards\r\nAmit"]}, {"number": 26061, "title": "Cannot load a Keras model with a custom initializer/regularizer/constraint function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION == '2.0.0-dev20190222'\r\ntf.version.GIT_VERSION == 'v1.12.0-8615-g74016a0d51'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI cannot load a model containing a custom initializer, or a custom regularizer, or a custom constraint, if they are defined as regular functions (rather than by subclassing the appropriate classes).\r\n\r\n**Describe the expected behavior**\r\nI expect it to work, since the model otherwise works fine, and is saved correctly.\r\n\r\n**Code to reproduce the issue**\r\nThe following model uses a custom initializer, and a custom regularizer, and a custom constraint, it works fine, saves fine, but cannot be loaded. You can try using only one at a time, they all fail.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ndef my_glorot_initializer(shape, dtype=tf.float32):\r\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\r\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\r\n\r\ndef my_l1_regularizer(weights):\r\n    return tf.reduce_sum(tf.abs(0.01 * weights))\r\n\r\ndef my_positive_weights(weights):\r\n    return tf.nn.relu(weights)\r\n\r\nX_train = np.random.randn(100, 2)\r\ny_train = np.random.randn(100, 1)\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Dense(1,\r\n                       kernel_regularizer=my_l1_regularizer,\r\n                       kernel_constraint=my_positive_weights,\r\n                       kernel_initializer=my_glorot_initializer),\r\n])\r\n\r\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\r\nmodel.fit(X_train, y_train, epochs=2)\r\nmodel.save(\"my_model.h5\")\r\nmodel = keras.models.load_model(\r\n    \"my_model.h5\",\r\n    custom_objects={\r\n       \"my_l1_regularizer\": my_l1_regularizer(0.01),\r\n       \"my_positive_weights\": my_positive_weights,\r\n       \"my_glorot_initializer\": my_glorot_initializer,\r\n    })\r\n```\r\n\r\n**Other info / logs**\r\nHere's the stacktrace:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-e9ce6b82aa4f> in <module>\r\n     31        \"my_l1_regularizer\": my_l1_regularizer(0.01),\r\n     32        \"my_positive_weights\": my_positive_weights,\r\n---> 33        \"my_glorot_initializer\": my_glorot_initializer,\r\n     34     })\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model(filepath, custom_objects, compile)\r\n    214     model_config = json.loads(model_config.decode('utf-8'))\r\n    215     model = model_config_lib.model_from_config(model_config,\r\n--> 216                                                custom_objects=custom_objects)\r\n    217\r\n    218     # set weights\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)\r\n     53                     '`Sequential.from_config(config)`?')\r\n     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n---> 55   return deserialize(config, custom_objects=custom_objects)\r\n     56\r\n     57\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n     77       module_objects=globs,\r\n     78       custom_objects=custom_objects,\r\n---> 79       printable_module_name='layer')\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    190             custom_objects=dict(\r\n    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 192                 list(custom_objects.items())))\r\n    193       with CustomObjectScope(custom_objects):\r\n    194         return cls.from_config(cls_config)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)\r\n    349     for layer_config in layer_configs:\r\n    350       layer = layer_module.deserialize(layer_config,\r\n--> 351                                        custom_objects=custom_objects)\r\n    352       model.add(layer)\r\n    353     if not model.inputs and build_input_shape:\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n     77       module_objects=globs,\r\n     78       custom_objects=custom_objects,\r\n---> 79       printable_module_name='layer')\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    192                 list(custom_objects.items())))\r\n    193       with CustomObjectScope(custom_objects):\r\n--> 194         return cls.from_config(cls_config)\r\n    195     else:\r\n    196       # Then `cls` may be a function returning a class.\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in from_config(cls, config)\r\n    414         A layer instance.\r\n    415     \"\"\"\r\n--> 416     return cls(**config)\r\n    417\r\n    418   def compute_output_shape(self, input_shape):\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in __init__(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\r\n    930     self.activation = activations.get(activation)\r\n    931     self.use_bias = use_bias\r\n--> 932     self.kernel_initializer = initializers.get(kernel_initializer)\r\n    933     self.bias_initializer = initializers.get(bias_initializer)\r\n    934     self.kernel_regularizer = regularizers.get(kernel_regularizer)\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py in get(identifier)\r\n    176   elif isinstance(identifier, six.string_types):\r\n    177     config = {'class_name': str(identifier), 'config': {}}\r\n--> 178     return deserialize(config)\r\n    179   elif callable(identifier):\r\n    180     return identifier\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py in deserialize(config, custom_objects)\r\n    165       module_objects=globals(),\r\n    166       custom_objects=custom_objects,\r\n--> 167       printable_module_name='initializer')\r\n    168\r\n    169\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    199       custom_objects = custom_objects or {}\r\n    200       with CustomObjectScope(custom_objects):\r\n--> 201         return cls(**cls_config)\r\n    202   elif isinstance(identifier, six.string_types):\r\n    203     function_name = identifier\r\n\r\nTypeError: my_glorot_initializer() missing 1 required positional argument: 'shape'\r\n```", "comments": ["@omalleyt12 -- can you comment on the proper format for serializing+deserializing of initializers?", "@karmel Sure\r\n\r\n@ageron Thanks for the issue, looks like all custom_objects are being treated as classes rather than functions. I'll work on the fix, a quick workaround would be to turn your functions into classes, with the current function as the class's `__call__` method", "A fix should be available in the latest nightly :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26061\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26061\">No</a>\n"]}, {"number": 26059, "title": "Tensorflow-gpu import error (solved)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win 10\r\n\r\n- TensorFlow installed from (source or binary):Source? Not to sure just using pip3\r\n- TensorFlow version: tensorflow-gpu-1.12.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip3 inside a virtualenv environment.\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory: GTX 1080 8192 MB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen attempting to do anything involving tensorflow it errors.  Errors while running \"import tensorflow as tf\"  I looked at some older issues that were similar but trying to do what they said did not solve my problem.  First attempt was them saying use an older version of tensorflow but to do so I would need to downgrade CUDA to 9.0 not sure if that's what I should do.  I have tried to uninstall reinstall to no avail.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nC:\\>pip install --upgrade pip\r\nC:\\>pip install virtualenv\r\nC:\\>virtualenv --system-site-packages -p \"C:\\Users\\Kathy\\AppData\\Local\\Programs\\Python\\Python36\\python.exe\" ./venv\r\nC:\\>.\\venv\\Scripts\\activate\r\n(venv) C:\\>pip3 install tensorflow-gpu\r\n(venv) C:\\>python\r\n>>>import tensorflow as tf\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```", "comments": ["TensorFlow 1.13.0-rc2 has released which supports cuda 10. You might want to give it a try instead.", "```\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Kathy\\venv\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nafter running running pip3 uninstall tensorflow-gpu then pip3 install tf-nightly-gpu", "Did you update the path for cuda, cudnn as per the [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup)?", "I believe so https://i.imgur.com/bc9AImq.png\r\n\r\nI tried adding them to the user variables as well.  cudnn64_7.dll is inside the CUDA bin folder do I need to create a separate folder for it and then path to that as well? Or do I need to name the dll files explicitly in the path?", "You need to add sub folders: ```bin``` and ```lib``` to the path. I see that ```lib``` folder is not added to the path. ", "I have this same exact issue too. Win10, only I am using CUDA 9 toolkit. \r\n\r\nI just added \r\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib`\r\nper ymodak's instruction, but that did not work. \r\n\r\nWhat else could it be?", "https://i.imgur.com/m4azsmP.png Thank you ymodak this was it for me.  rygo6 I would suggest you uninstall everything and follow the guide ymodak posted (https://www.tensorflow.org/install/gpu#windows_setup) from start to finish and if that doesn't work then create your own issue.\r\n\r\nI don't know how to tag it as solved sorry", "@CrabBucket Glad it worked for you. I will close this issue now since its solved.\r\n@rygo6 Can you please post issue by providing all the information asked in the [build/installation](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md) template? Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 26058, "title": "Updated hierarchical_tree_broadcaster.cc", "body": "Fixed uninitialised warning", "comments": []}, {"number": 26057, "title": "Updated test_util.cc", "body": "Fixed warning of comparison", "comments": ["Nagging Reviewer @jianlijianli: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 26056, "title": "Updated model_test.cc", "body": "Fixed Typo error", "comments": []}, {"number": 26055, "title": "Updated tf_ops_compatibility.md", "body": "Fixed typo error", "comments": []}, {"number": 26054, "title": "Updated profile_buffer_test.cc", "body": "Fixed the warning in the file", "comments": []}, {"number": 26053, "title": "Updated profile_buffer.h", "body": "Fixed the warning in the file", "comments": []}, {"number": 26052, "title": "Updated custom_operators.md", "body": "Typo error fixed", "comments": []}, {"number": 26051, "title": "Update TF_MINOR_VERSION to 13.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26051) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26051) for more info**.\n\n<!-- ok -->", "I am not in charge of version change. Please find another reviewer.", "We take care of this during the release process. I will close this PR."]}, {"number": 26050, "title": "Failed to convert object of type <class 'dict'> to Tensor.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code for Model Regression\r\n- I used ANACONDA platform with Tensorflow to Develop DNNRegressor Estimator Mode\r\n\r\nDuring i run code, i found some error during training model as per below \r\n\r\n```\r\nWARNING:tensorflow:From C:\\Users\\thana\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From C:\\Users\\thana\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nINFO:tensorflow:Calling model_fn.\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    526     try:\r\n--> 527       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    528     except TypeError:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in <listcomp>(.0)\r\n    526     try:\r\n--> 527       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    528     except TypeError:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\compat.py in as_bytes(bytes_or_text, encoding)\r\n     60     raise TypeError('Expected binary or unicode string, got %r' %\r\n---> 61                     (bytes_or_text,))\r\n     62 \r\n\r\nTypeError: Expected binary or unicode string, got {'TOL': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:36' shape=(?,) dtype=float64>}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-19-b26ed31a0e2e> in <module>\r\n----> 1 model.train(input_fn=input_func,steps=1000)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    352 \r\n    353       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 354       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    355       logging.info('Loss for final step: %s.', loss)\r\n    356       return self\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1205       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1206     else:\r\n-> 1207       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1208 \r\n   1209   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1235       worker_hooks.extend(input_hooks)\r\n   1236       estimator_spec = self._call_model_fn(\r\n-> 1237           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n   1238       global_step_tensor = training_util.get_global_step(g)\r\n   1239       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1193 \r\n   1194     logging.info('Calling model_fn.')\r\n-> 1195     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1196     logging.info('Done calling model_fn.')\r\n   1197 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _model_fn(features, labels, mode, config)\r\n    654           config=config,\r\n    655           batch_norm=batch_norm,\r\n--> 656           shared_state_manager=shared_state_manager)\r\n    657 \r\n    658     super(DNNRegressor, self).__init__(\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config, use_tpu, batch_norm, shared_state_manager)\r\n    313           labels=labels,\r\n    314           optimizer=optimizer,\r\n--> 315           logits=logits)\r\n    316 \r\n    317 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py in create_estimator_spec(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)\r\n    237           self._create_tpu_estimator_spec(\r\n    238               features, mode, logits, labels, optimizer, train_op_fn,\r\n--> 239               regularization_losses))\r\n    240       return tpu_estimator_spec.as_estimator_spec()\r\n    241     except NotImplementedError:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py in _create_tpu_estimator_spec(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)\r\n   1480 \r\n   1481       training_loss, unreduced_loss, weights, _ = self.create_loss(\r\n-> 1482           features=features, mode=mode, logits=logits, labels=labels)\r\n   1483       if regularization_losses:\r\n   1484         regularization_loss = math_ops.add_n(regularization_losses)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py in create_loss(***failed resolving arguments***)\r\n   1379     labels = _check_dense_labels_match_logits_and_reshape(\r\n   1380         labels=labels, logits=logits,\r\n-> 1381         expected_labels_dimension=self._logits_dimension)\r\n   1382     labels = math_ops.to_float(labels)\r\n   1383     if self._loss_fn:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\head.py in _check_dense_labels_match_logits_and_reshape(labels, logits, expected_labels_dimension)\r\n    303         'returns labels.')\r\n    304   with ops.name_scope(None, 'labels', (labels, logits)) as scope:\r\n--> 305     labels = sparse_tensor.convert_to_tensor_or_sparse_tensor(labels)\r\n    306     if isinstance(labels, sparse_tensor.SparseTensor):\r\n    307       raise ValueError(\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py in convert_to_tensor_or_sparse_tensor(value, dtype, name)\r\n    277     return value\r\n    278   return ops.internal_convert_to_tensor(\r\n--> 279       value, dtype=dtype, name=name)\r\n    280 \r\n    281 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1144 \r\n   1145     if ret is None:\r\n-> 1146       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1147 \r\n   1148     if ret is NotImplemented:\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    227                                          as_ref=False):\r\n    228   _ = as_ref\r\n--> 229   return constant(v, dtype=dtype, name=name)\r\n    230 \r\n    231 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    206   tensor_value.tensor.CopyFrom(\r\n    207       tensor_util.make_tensor_proto(\r\n--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    210   const_tensor = g.create_op(\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    529       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n    530                       \"Contents: %s. Consider casting elements to a \"\r\n--> 531                       \"supported type.\" % (type(values), values))\r\n    532     tensor_proto.string_val.extend(str_values)\r\n    533     return tensor_proto\r\n\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'TOL': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:36' shape=(?,) dtype=float64>}. Consider casting elements to a supported type.\r\n\r\n```\r\nI don't sure that what kind of error that i m facing. \r\nWhat should i do to solve this problem ?\r\n\r\nThank you in advance for some suggestion and great advise\r\n\r\nBest regards,\r\nNes\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Thank you very much sir.\r\n\r\nAfter i check in more detail. I found this problem is come from i specify label data more than 1 ( 4 labor data) in DNNRegressor Estimator. \r\n\r\nNow i try to find out the way to use DNNRegressor to predict more than 1 output. \r\nCould you advise how to do this thing in DNNRegressor estimator sir ?\r\n\r\nThank you in advance^^\r\n\r\n", "[```tf.estimator.DNNRegressor```](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor#__init__) has one of the argument as ```label_dimension```, you can try setting it to an appropriate number to pass multiple outputs."]}, {"number": 26049, "title": "official/utils/misc/distribution_utils.py uses the wrong API for OneDeviceStrategy resulting in runtime python error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): b'unknown' 1.13.0-rc2\r\n- Python version: Python 3.7.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nReceive an attribute not found error on tf.distribute.OneDeviceStrategy(\"device:CPU:0\")\r\n**Describe the expected behavior**\r\nNot see this error.\r\n**Code to reproduce the issue**\r\nRun cifar10_main.py on a macbook without an NVIDIA GPU, I imagine this would also happen on linux if no GPU was detected.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Filed in wrong project."]}, {"number": 26048, "title": "Check failure and silent failures with incorrect usage of tf.custom_gradient (in eager mode).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-8779-g2ae06ca491 1.13.0-dev20190223 (as well as 1.12.0)\r\n- Python version: Python 3.6.4 :: Anaconda, Inc.\r\n\r\nWhen `tf.custom_gradient` is used incorrectly (in this case, the returned `grad` function returns an empty list, the script segfaults.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.custom_gradient\r\ndef identity(x):\r\n    def grad(dy):\r\n        return []  # This return value is wrong!\r\n    return x, grad\r\n\r\nx = tf.Variable(1.0)\r\nwith tf.GradientTape() as t:\r\n    y = identity(x)\r\nt.gradient(y, [x])\r\n```\r\n\r\nThe `t.gradient` call fails with \r\n\r\n```\r\n2019-02-23 18:09:14.621207: F ./tensorflow/c/eager/tape.h:642] Check failed: state.op_tape.empty() \r\nAbort trap: 6\r\n```\r\n\r\nI think it'd be preferable to raise an exception instead of crashing.\r\n\r\nIf I instead return too many values from `grad`, then the script runs, but this is most likely a bug and should probably raise an exception.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.custom_gradient\r\ndef identity(x):\r\n    def grad(dy):\r\n        return 1.0, 2.0  # Too many return values!\r\n    return x, grad\r\n\r\nx = tf.Variable(1.0)\r\nwith tf.GradientTape() as t:\r\n    y = identity(x)\r\nt.gradient(y, [x])\r\n```\r\n\r\nFYI @alextp ", "comments": []}, {"number": 26047, "title": "Switching between TF 1.x and TF 2.0 in the API browser loses the context", "body": "**System information**\r\n- TensorFlow version: from \r\n- Doc Link:\r\n\r\n\r\n**Describe the documentation issue**\r\nWhen I'm browsing the documentation, I often need to switch from TF 1.x to TF 2.0 or vice versa. Unfortunately, the context is lost when I do that.\r\nFor example, visit the `tf.constant` documentation for TF 2.0:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/constant\r\nThen from the drop down menu at the top (labeled \"API r2.0\"), select any TF 1.x version. Notice that you do not land on the `tf.constant` page.\r\nThe same is true in the reverse direction.\r\n\r\nThis also makes it hard to find TF 2.0 documentation by searching: I generally land on a TF 1.x page, when I switch to TF 2.0 the context is lost.\r\n\r\nOf course, many functions have been removed or moved, but this should not be a problem, since most of them are still available in `tf.compat.v1`.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nWhy not, but I would need someone to point me to the code that handles this.", "comments": ["I just noticed that the context is lost whenever I switch to any other TF version, not just to/from TF 2.0. I think I recall that context used to be preserved (I may be wrong). I didn't notice it until now because I was usually only interested in the latest TF version, so I didn't need to switch often. Now I do, and I think as TF 2.0 usage grows, people are going to need to switch more often too.", "Hey Aur\u00e9lien,\r\nIt's a good point and something we've discussed, but not prepared to invest the site infra in (at least at this time with the run-up to TF 2).\r\nWhen TF 2 is out of preview and released, that will be the default experience for tensorflow.org. We will continue to host some older 1.x API docs, but for narrative docs, we will point users off to our [tensorflow/docs](https://github.com/tensorflow/docs) repo, which will contain the navigable markdown docs in a release branch.\r\n", "@ageron please confirm if you still face the issue", "Moving this to closed status as there is no response"]}, {"number": 26046, "title": "Tensorflow 1.13.0 reports wrong version", "body": "**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.13.0\r\n- Python version: 3.5.2\r\n- Bazel version : 0.22.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: cuda10.0 cudnn7.4.2.24\r\n- GPU model and memory: GTX1070\r\n\r\nTensorflow compiled and built without issue. But when created wheel with command\r\n\r\n```\r\n ./bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOME/opt/cuda_test/cuda10/tensorflow_pkg\r\n\r\n```\r\nThe wheel created was called tensorflow-**1.12.0**-cp35-cp35m-linux_x86_64.whl\r\n\r\nI changed 1.12.0 to 1.13.0 manually and install it with pip\r\n```\r\ncd $HOME/tensorflow_pkg\r\npip3 install --no-cache-dir ./tensorflow-1.13.0-cp35-cp35m-linux_x86_64.whl --user\r\nProcessing ./tensorflow-1.13.0-cp35-cp35m-linux_x86_64.whl\r\nRequirement already satisfied: absl-py>=0.1.6 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.7.0)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (1.0.5)\r\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (1.11.0)\r\nRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (1.0.6)\r\nRequirement already satisfied: google-pasta>=0.1.2 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.1.4)\r\nRequirement already satisfied: gast>=0.2.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.2.2)\r\nRequirement already satisfied: protobuf>=3.6.1 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (3.6.1)\r\nRequirement already satisfied: numpy<2.0,>=1.14.5 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.16.1)\r\nRequirement already satisfied: grpcio>=1.8.6 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.18.0)\r\nRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (0.31.1)\r\nRequirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.13.0rc0)\r\nRequirement already satisfied: termcolor>=1.1.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.1.0)\r\nRequirement already satisfied: astor>=0.6.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.7.1)\r\nRequirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.12.2)\r\nRequirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.0) (2.9.0)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.0) (40.8.0)\r\nRequirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0) (2.0.0)\r\nRequirement already satisfied: werkzeug>=0.11.10 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0) (0.14.1)\r\nRequirement already satisfied: markdown>=2.6.8 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0) (3.0.1)\r\nRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0) (4.0.4)\r\nInstalling collected packages: tensorflow\r\n**Successfully installed tensorflow-1.12.0**\r\n\r\n```\r\nFrom last sentence of ouput above python still sees tensorflow-1.12.0\r\n\r\nNow\r\n```\r\n$ python3\r\nPython 3.5.2 (default, Nov 12 2018, 13:43:14) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n```\r\n>>> import tensorflow as tf\r\nLimited tf.compat.v2.summary API due to missing TensorBoard installation\r\n\r\n```\r\n```\r\n", "comments": ["Same situation ...", "Same issue ..", "Same issue here...", "same issue to me:\r\nubuntu 18.04.2 LTS\r\npython 3.6.7\r\nBazel version : 0.22.0\r\nNO GPU", "And the PyPI hasn't been updated yet.", "same issue,but my tensorboard run properly.\r\nmaybe you can try compile your code in terminal,\r\nthen try \r\ntensorboard --logdir=\"./graphs\" --port number(anyone you want)\r\nand open the address you set in browser(ie.http://localhost:10086/)", "Is it because TF_MINOR_VERSION is still 12? I have submitted #26051 to update it.", "You can change the version by editing tensorflow-1.13.0/tensorflow/tools/pip_package/setup.py and change line 48 to _VERSION = '1.13.0' (typo in the original?) But the tensorboard error persists.\r\n\r\n```\r\nLimited tf.compat.v2.summary API due to missing TensorBoard installation\r\n```", "Yeah, it is because you need more than renaming the file to update the version.\r\nDid you build it from the branch r1.13? ", "@gunan \r\n\r\n I downloaded the 1.13 release tarball  from https://github.com/tensorflow/tensorflow/releases Now 1.13 appears to have been removed from the server (unreleased?) the newest  version on the link has been  back to 1.13rc2 since maybe last night.", "I made a mistake with that, and that tarball was created with the master version. 1,13,1 is being prepared which will have the correct sources.", "@gunan \r\n\r\nIt works now, tf 1.13.1 compiled and installed successfully. Thanks!"]}, {"number": 26045, "title": "tensorflow 1.12.0 install package requires protobuf>=3.6.1 but comes with protobuf==3.6.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nIf you download the Python 2.7 CPU-only official whl file (following https://www.tensorflow.org/install/pip) https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.12.0-cp27-none-linux_x86_64.whl, you will see that it requires protobuf>=3.6.1.\r\n\r\nHowever, after you extract it and see the include files, they are protobuf==3.6.0.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe protobuf version should be consistent.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nwget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.12.0-cp27-none-linux_x86_64.whl\r\nunzip tensorflow-1.12.0-cp27-none-linux_x86_64.whl\r\nless tensorflow-1.12.0.dist-info/metadata.jsoin \r\nless tensorflow-1.12.0.data/purelib/tensorflow/include/google/protobuf/stubs/common.h\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Ideally it should be consistent, but most packages are expected to be compatible between patch release versions.\r\nUnless this is causing a breakage for you, this is not a problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26045\">No</a>\n"]}, {"number": 26044, "title": "Needs upgrade Russian's translation of page: /tutorials/keras/basic_classification", "body": "**System information**\r\n- TensorFlow version: not important\r\n- Doc Link: https://www.tensorflow.org/tutorials/keras/basic_classification\r\n\r\n**Describe the documentation issue**\r\nWhen I was reading the Russian translation of this documentation page, I found a few places with not a natural way of using the Russian language(I'm a native Russian speaker), also I noticed a few small typo mistakes. I carefully read the whole page and compared with the original article on English and made some updates. The PR with these changes will be sent in the next couple of minutes after opening this issue. PR with updates: https://github.com/tensorflow/docs/pull/345", "comments": []}, {"number": 26043, "title": "Update contributing.md to point to addons", "body": "Since we are migrating out of `contrib` with TensorFlow 2.0\r\n\r\nWhat do you think @dynamicwebpaige and @seanpmorgan", "comments": []}, {"number": 26042, "title": "Tensorflow r1.12 CPU-only build fails on Ubuntu 18.04", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Not yet installed, trying to build from source\r\n- TensorFlow version: r1.12\r\n- Python version: 2.7.15rc1\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhile bulding tensorflow r1.12 CPU only for Python 2.7, the build fails with the following error:\r\n\r\n`ERROR: /home/davide/Downloads/tensorflow/tensorflow/python/estimator/api/BUILD:12:1: Executing genrule //tensorflow/python/estimator/api:estimator_python_api_gen failed (Exit 1)`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git \r\ncd tensorflow\r\ngit checkout r1.12\r\n./configure\r\n```\r\n\r\nThis is the configuration file (`.tf_configure.bazelrc`):\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild:ignite --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```\r\n\r\nBecause I'm using bazel 0.19.2, I opened the file .bazelrc and added at the beginning the command `import /home/davide/Downloads/tensorflow/tools/bazel.rc`\r\n\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/davide/Downloads/tensorflow/tensorflow/python/estimator/api/BUILD:12:1: Executing genrule //tensorflow/python/estimator/api:estimator_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 510, in <module>\r\n    main()\r\n  File \"/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 503, in main\r\n    importlib.import_module(args.package)\r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow.python.estimator.estimator_lib\r\n  File \"/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/estimator/estimator_lib.py\", line 22, in <module>\r\n    from tensorflow.python.estimator.canned.baseline import BaselineClassifier\r\n  File \"/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/estimator/canned/baseline.py\", line 50, in <module>\r\n    from tensorflow.python.estimator import estimator\r\nImportError: cannot import name estimator\r\ntf.estimator package not installed.\r\ntf.estimator package not installed.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 8147.753s, Critical Path: 232.48s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 6805 processes: 6805 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["as shown in the error message\r\n```\r\nfrom tensorflow.python.estimator import estimator\r\nImportError: cannot import name estimator\r\ntf.estimator package not installed.\r\ntf.estimator package not installed.\r\n```\r\nYou may want to either `pip install tensorflow-estimator==1.2.0` or `pip install tensorflow==1.2.0` before building your own version.", "Ok, I will try. In the meantime I have successfully build version 1.11 without installing `tensorflow-estimator==1.2.0` or `tensorflow==1.2.0`.", "@Davide-sd Is this still an issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26042\">No</a>\n", "Just want to add that I ran into this exact same problem (installing 1.12 from source) with the same error. The error message is not great but this is actually about version mismatch. For me the issue was that I had previously pip installed tensorflow 1.13. The way I fixed it was simply `pip uninstall tensorflow` and `pip uninstall tensorflow-estimator`."]}, {"number": 26041, "title": "tf-nightly-gpu-2.0-preview dataset and tf.function error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview\r\n- Python version: Python 3 in Google Colab\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: Tesla K80\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using GPU runtime in Google Colab with tf-nightly-gpu-2.0-preview tensorflow version, creating a dataset with tensorflow datasets inside a tf.function decorated function results in an exception regarding incompatible device types. \r\n\r\n**Describe the expected behavior**\r\n\r\n Expected behavior is not receiving an exception. Removing tf.function from the train method results in correct behavior. The code also works well without GPU support (tf-nightly-2.0-preview).\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self._layer1 = tf.keras.layers.Dense(20, activation='relu')\r\n        self._layer2 = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, x, training):\r\n        x = self._layer1(x)\r\n        x = self._layer2(x)\r\n        return x\r\n\r\n\r\ndef create_dataset():\r\n    def process(features):\r\n        image, label = features['image'], features['label']\r\n        return tf.reshape(image, [-1]) / np.float32(255.0), label\r\n\r\n    data_builder = tfds.builder('mnist')\r\n    dataset = data_builder.as_dataset(split=tfds.Split.TRAIN)\r\n    dataset = (\r\n        dataset.map(process)\r\n        .batch(32)\r\n        .repeat(1)\r\n    )\r\n\r\n    return dataset\r\n\r\n  \r\navg_loss = tf.metrics.Mean()\r\n\r\n  \r\n@tf.function\r\ndef train(model, optimizer):\r\n    dataset = create_dataset()\r\n    step = 0\r\n    for images, labels in dataset:\r\n        step += 1\r\n        with tf.GradientTape() as tape:\r\n            logits = model(images, True)\r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n                logits=logits, labels=labels)\r\n            loss = tf.math.reduce_mean(loss)\r\n            \r\n        avg_loss.update_state(loss)\r\n        \r\n        grads = tape.gradient(\r\n            loss, model.trainable_variables)\r\n        optimizer.apply_gradients(\r\n            zip(grads, model.trainable_variables))\r\n        \r\n        if tf.equal(step % 20, 0):\r\n            tf.print(avg_loss.result())\r\n            avg_loss.reset_states()\r\n            \r\n\r\nNUM_EPOCHS = 2\r\nmodel = MyModel()\r\noptimizer = tf.keras.optimizers.Adam()\r\nfor _ in range(NUM_EPOCHS):\r\n    train(model, optimizer)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nLink to the original Google Colab [file](https://gist.github.com/Mrpatekful/92f274756dffd6aab2993e401b7fb7af)\r\n\r\nThe encountered exception:\r\n\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-43-03d2bfd1799c> in <module>()\r\n     59 optimizer = tf.keras.optimizers.Adam()\r\n     60 for _ in range(NUM_EPOCHS):\r\n---> 61     train(model, optimizer)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    436         # Lifting succeeded, so variables are initialized and we can run the\r\n    437         # stateless function.\r\n--> 438         return self._stateless_fn(*args, **kwds)\r\n    439     else:\r\n    440       canon_args, canon_kwds = self._canonicalize_function_inputs(args, kwds)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1251     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1252     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1253     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1254 \r\n   1255   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    537     \"\"\"\r\n    538     return self._call_flat(\r\n--> 539         (t for t in nest.flatten((args, kwargs))\r\n    540          if isinstance(t, (ops.Tensor,\r\n    541                            resource_variable_ops.ResourceVariable))))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    590     # Only need to override the gradient in graph mode and when we have outputs.\r\n    591     if context.executing_eagerly() or not self.outputs:\r\n--> 592       outputs = self._inference_function.call(ctx, args)\r\n    593     else:\r\n    594       self._register_gradient()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    380             attrs=(\"executor_type\", executor_type,\r\n    381                    \"config_proto\", config),\r\n--> 382             ctx=ctx)\r\n    383       # Replace empty list with None\r\n    384       outputs = outputs or None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     64     else:\r\n     65       message = e.message\r\n---> 66     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     67   except TypeError as e:\r\n     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_925768]\r\n```", "comments": ["Any progress on this issue ? I'm facing similar issue here https://github.com/tensorflow/agents/issues/19\r\nI thought setting the device placement be the solution but I finally can't verify it.", "I believe @jsimsa has fix for this in the works.", "@Mrpatekful could you sync past https://github.com/tensorflow/tensorflow/commit/11f13a90545ff793762191da02e27669934c9e1c and see whether the problem goes away? Thanks.", "ran into the same problem when run tf-agents example code, \r\nnightly build as of '2.0.0-dev20190322' still have the same issues\r\n", "I'm too facing this. It triggers when moving to the second variable [1] in\r\nfor i in tf.range(100):\r\nCommands under loop run correctly for i = 0, \r\nfor i in tf.range(100) is inside the function.\r\n\r\nBuilt 2.0 from source around 20th March (+ or - 2 days). Thanks", "I cannot reproduce the issue using Python 3 Google colab, running:\r\n\r\n`!pip install tf-nightly-gpu-2.0-preview` (this installs tf-nightly-gpu-2.0-preview-2.0.0.dev20190326)\r\n\r\nfollowed by:\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self._layer1 = tf.keras.layers.Dense(20, activation='relu')\r\n        self._layer2 = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, x, training):\r\n        x = self._layer1(x)\r\n        x = self._layer2(x)\r\n        return x\r\n\r\n\r\ndef create_dataset():\r\n    def process(features):\r\n        image, label = features['image'], features['label']\r\n        return tf.reshape(image, [-1]) / np.float32(255.0), label\r\n\r\n    data_builder = tfds.builder('mnist')\r\n    data_builder.download_and_prepare()\r\n    dataset = data_builder.as_dataset(split=tfds.Split.TRAIN)\r\n    dataset = (\r\n        dataset.map(process)\r\n        .batch(32)\r\n        .repeat(1)\r\n    )\r\n\r\n    return dataset\r\n\r\n  \r\navg_loss = tf.metrics.Mean()\r\n\r\n  \r\n@tf.function\r\ndef train(model, optimizer):\r\n    dataset = create_dataset()\r\n    step = 0\r\n    for images, labels in dataset:\r\n        step += 1\r\n        with tf.GradientTape() as tape:\r\n            logits = model(images, True)\r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n                logits=logits, labels=labels)\r\n            loss = tf.math.reduce_mean(loss)\r\n            \r\n        avg_loss.update_state(loss)\r\n        \r\n        grads = tape.gradient(\r\n            loss, model.trainable_variables)\r\n        optimizer.apply_gradients(\r\n            zip(grads, model.trainable_variables))\r\n        \r\n        if tf.equal(step % 20, 0):\r\n            tf.print(avg_loss.result())\r\n            avg_loss.reset_states()\r\n            \r\n\r\nNUM_EPOCHS = 2\r\nmodel = MyModel()\r\noptimizer = tf.keras.optimizers.Adam()\r\nfor _ in range(NUM_EPOCHS):\r\n    train(model, optimizer)\r\n```\r\n\r\n@bayesian and @caissalover please create a separate issue with instructions on how to reproduce the issue you are seeing. I am closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26041\">No</a>\n", "I went into this error when using `tf.keras.layers.GRU`. Switching to `tf.compat.v1.keras.layers.CuDNNGRU` resolved the problem."]}, {"number": 26040, "title": "Support Tensorflow Lite for Windows", "body": "This change is not fully porting of Tensorflow Lite since some fixes for dependency projects are required on Windows. ex: gemmlowp, farmhash. However I could build libtensorflow-lite.a using this patch and some small fixes to the dependencies.\r\n\r\nhttps://github.com/mattn/webcam-detect-tflite\r\n\r\nThis is example app to detect objects on real time camera streaming using TensorFlow Lite on Windows.\r\n\r\n![image](https://user-images.githubusercontent.com/10111/53289235-e2da5800-37d6-11e9-9bfc-2db20b47e5aa.png)\r\n\r\n", "comments": ["This is a patch for gemmlowp and farmhash.\r\n\r\n```diff\r\ndiff -ur downloads.orig/farmhash/src/farmhash.cc downloads/farmhash/src/farmhash.cc\r\n--- downloads.orig/farmhash/src/farmhash.cc\t2017-09-14 05:13:37.000000000 +0900\r\n+++ downloads/farmhash/src/farmhash.cc\t2019-02-24 01:17:52.840988000 +0900\r\n@@ -114,7 +114,7 @@\r\n \r\n #if defined(FARMHASH_UNKNOWN_ENDIAN) || !defined(bswap_64)\r\n \r\n-#ifdef _MSC_VER\r\n+#ifdef _WIN32\r\n \r\n #undef bswap_32\r\n #undef bswap_64\r\ndiff -ur downloads.orig/gemmlowp/internal/platform.h downloads/gemmlowp/internal/platform.h\r\n--- downloads.orig/gemmlowp/internal/platform.h\t2019-02-24 02:14:49.938967500 +0900\r\n+++ downloads/gemmlowp/internal/platform.h\t2019-02-24 00:53:31.840022600 +0900\r\n@@ -69,12 +69,21 @@\r\n   return max_threads;\r\n }\r\n \r\n+#ifdef __MINGW64__\r\n+inline double real_time_in_seconds() {\r\n+  __int64 wintime;\r\n+  GetSystemTimeAsFileTime((FILETIME *)&wintime);\r\n+  wintime -= 116444736000000000;  // 1jan1601 to 1jan1970\r\n+  return wintime / 10000000 + wintime % 10000000 * 100 * 1e-9;\r\n+}\r\n+#else\r\n inline double real_time_in_seconds() {\r\n   __int64 wintime;\r\n   GetSystemTimeAsFileTime((FILETIME *)&wintime);\r\n   wintime -= 116444736000000000i64;  // 1jan1601 to 1jan1970\r\n   return wintime / 10000000i64 + wintime % 10000000i64 * 100 * 1e-9;\r\n }\r\n+#endif\r\n \r\n #else\r\n inline void *aligned_alloc(size_t alignment, size_t size) {\r\n```", "What compiler+version are you using? We've been able to successfully build with Visual Studio for some time now, though I believe only with more recent versions.", "I use mingw64 compiler (which is bundled on msys2/mingw64)\r\n\r\n```\r\ngcc (Rev2, Built by MSYS2 project) 8.3.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n", "<del>The changes for `mmap_allocation.cc` is not required. So I'll remove  this.</del>\r\n\r\nSorry, this is required change.", "Taking a step back, I'm wondering if we have a broader problem. Should the [//third_party/tensorflow:windows condition](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/BUILD#L167) be satisfied when building with MinGW? Does TensorFlow support building with MinGW? I'd prefer to fix this at the build rule level, if we can, rather than sprinkling more #ifdefs int he code.", "TensorFlow does not support MinGW. https://github.com/tensorflow/tensorflow/issues/22840#issuecomment-429502287\r\n\r\nBut mingw can build tensorflow-lite using tensorflow/lite/tools/make/Makefile.", "@jdduke Fixed as you pointed. Thanks. Still not possible to build benchmark.\r\n\r\n```\r\ng++ -O3 -DNDEBUG -fPIC  --std=c++11 -fext-numeric-literals -D__LITTLE_ENDIAN__ -I. -Ic:/dev/tensorflow/tensorflow/lite/tools/make/../../../../../ -Ic:/dev/tensorflow/tensorflow/lite/tools/make/../../../../../../ -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/ -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/eigen -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/absl -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -Ic:/dev/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\\r\n-o c:/dev/tensorflow/tensorflow/lite/tools/make/gen/windows_x86_64/bin/benchmark_model \\\r\n c:/dev/tensorflow/tensorflow/lite/tools/make/gen/windows_x86_64/lib/benchmark-lib.a  -lstdc++ -lpthread -lm -lz\r\nc:/dev/tensorflow/tensorflow/lite/tools/make/gen/windows_x86_64/lib/benchmark-lib.a(benchmark_plus_flex_main.o):benchmark_plus_flex_main.cc:(.text+0x3b): undefined reference to `tflite::InitTensorFlow()'\r\ncollect2.exe: error: ld returned 1 exit status\r\nmingw32-make: *** [tensorflow\\lite\\tools\\make\\Makefile:254: c:/dev/tensorflow/tensorflow/lite/tools/make/gen/windows_x86_64/bin/benchmark_model] Error 1\r\n```", "We should exclude benchmark_plus_flex_main.cc from `BENCHMARK_SRCS`.", "Reverted change for c_api_types.h", "@mattn can you please resolve conflicts ", "Fixed conflict. Also updated patch for third-party libraries.\r\n\r\n```diff\r\ndiff -ur downloads.orig/farmhash/src/farmhash.cc downloads/farmhash/src/farmhash.cc\r\n--- downloads.orig/farmhash/src/farmhash.cc\t2019-05-05 22:39:31.645761900 +0900\r\n+++ downloads/farmhash/src/farmhash.cc\t2019-05-05 22:46:58.122055200 +0900\r\n@@ -114,7 +114,7 @@\r\n \r\n #if defined(FARMHASH_UNKNOWN_ENDIAN) || !defined(bswap_64)\r\n \r\n-#ifdef _MSC_VER\r\n+#ifdef _WIN32\r\n \r\n #undef bswap_32\r\n #undef bswap_64\r\n@@ -216,7 +216,7 @@\r\n   return shift == 0 ? val : ((val >> shift) | (val << (64 - shift)));\r\n }\r\n \r\n-#if defined(_MSC_VER) && defined(FARMHASH_ROTR)\r\n+#if defined(_WIN32) && defined(FARMHASH_ROTR)\r\n \r\n STATIC_INLINE uint32_t Rotate32(uint32_t val, int shift) {\r\n   return sizeof(unsigned long) == sizeof(val) ?\r\n```\r\n", "FYI, I sent small fix to farmhash https://github.com/google/farmhash/pull/32\r\n\r\nSo we will be possible to build tensorflow-lite with mingw compiler. (without patches)\r\n", "https://github.com/google/farmhash/pull/32 was merged."]}]