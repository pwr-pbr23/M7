[{"number": 18374, "title": "Feed_dict slow in Google compute engine", "body": " _preds = sess.run(preds, feed_dict={img_placeholder: X})\r\n\r\n\r\nwhile running in computer it took around 1-2 seconds while running in compute engine cost more than 50 s \r\n", "comments": ["Hi @PIRANAVARUBAN, please fill the issue template making sure to include an example to reproduce the issue you're facing or else we won't be able to properly help you. \r\nBy the bit of information you shared though there are many aspects to consider including your GCE setup/config before confirming it is actually a TensorFlow bug.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18373, "title": "CI nightly libtensorflow stuck", "body": "@asimshankar @alextp the nightly libtensorflow build on the CI server seems to have been stuck for a few days now due to the Mac build. Is there a way to resolve this in order to have the binaries built for the recent changes to the eager C API? I'm currently cross-compiling on my machine but that is very expensive and so this would really help! Thanks a lot! :)\r\n\r\n[http://ci.tensorflow.org/job/nightly-libtensorflow/](http://ci.tensorflow.org/job/nightly-libtensorflow/)", "comments": ["I restarted that job, but I believe the nightly jobs are in the process of being migrated away from this Jenkins instance? I could be wrong (there seem to be no online Mac slaves).\r\n\r\n@av8ramit @case540 would know better.\r\n", "Thanks for filing @eaplatanios. I'm working right now to take care of this.", "@av8ramit Thanks! It seems it's all good now! :)\r\n\r\n@asimshankar Thanks for helping with this! If indeed you migrate those jobs away, do you plan to provide a way to obtain nightly builds for the dynamic libraries (i.e., `libtensorflow.so` and `libtensorflow_framework.so`)?", "@eaplatanios yes we will definitely provide a way for access to those in a cleaner way as well.", "That's great! Thanks @av8ramit! :)", "@av8ramit Is there any update on when a new way to access the precompiled binaries might become available?", "https://github.com/tensorflow/tensorflow#continuous-build-status"]}, {"number": 18372, "title": "Is Docker image tag \"latest-devel-py3\" Python2?", "body": "I'd like to use Tensorflow with Python3 on Docker.\r\nI set my Dockerfile below then build and run.\r\n\r\n`FROM tensorflow/tensorflow:latest-devel-py3`\r\n\r\nI entered in a shell on Docker and typed next command.\r\n\r\n```\r\n# python --version\r\nPython 2.7.12\r\n```\r\n\r\n\r\nIs the image tag of \"latest-devel-py3\" Python2?\r\n\r\nHowever, I tried to build and run using \"latest-py3\".\r\nThis response was,\r\n```\r\n# python --version\r\nPython 3.5.2\r\n```\r\n\r\nAnyway, could anyone describe me about the difference between \"latest-devel-py3\" and \"latest-py3\"?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Python 3 was added to Dockerfile in #6030. I noticed that compared with `Dockerfile`, the `Dockerfile.devel` does not have \r\n```\r\n# RUN ln -s /usr/bin/python3 /usr/bin/python#\r\n```\r\nThat is the reason `python --version` output difference. (python 2 in latest-devel-py3 and python 3 in latest-py3)\r\n\r\nI don't know if that was intentional in #6030, though.", "This omission was a mistake. I've patched it [here](https://github.com/tensorflow/tensorflow/pull/18391). Once in I'll re-upload the packages. \r\n\r\nTemporarily in those containers running any script with python3 invocation will work. Thanks for catching @MatsumotoHiroko ", "```\r\n# docker run -it  tensorflow/tensorflow:latest-py3 bash\r\n# python --version\r\nPython 3.5.2\r\n```\r\n\r\nShould be fixed now. Please comment if there are still issues. "]}, {"number": 18371, "title": "compile tf-lite error", "body": "I want to **cross compile** some components of **tf-lite**.So I do these:\r\n```\r\nbazel build tensorflow/contrib/lite/arm_build:test_only \\\r\n      --crosstool_top=@local_config_arm_compiler//:toolchain \\\r\n      --cpu=armeabi\\\r\n      --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n      --verbose_failures\r\n```\r\nin my **arm_build** folder the **BUILD** file is:\r\n```\r\npackage(default_visibility = [\"//visibility:public\"])\r\nload(\"//tensorflow/contrib/lite:build_def.bzl\", \"tflite_copts\")\r\nlicenses([\"notice\"])  # Apache 2.0\r\ncc_binary(\r\n    name = \"test_only\",\r\n    srcs = [\r\n        \"test.cc\",\r\n    ],\r\n    copts = [ \"-O3\",],\r\n    linkopts = [\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/contrib/lite:context\",\r\n        \"//tensorflow/contrib/lite:framework\",\r\n        \"//tensorflow/contrib/lite:schema_fbs_version\",\r\n    ],\r\n    linkstatic=1,\r\n)\r\n```\r\nthe **test.cc** is:\r\n```\r\n#include <stdio.h>\r\nint main(){\r\n  printf(\"---\");\r\n  return 0;\r\n}\r\n```\r\nActually I just want to compile the \"context,framework and schema_fbs_version\" components.\r\nI got like this \"**error: '_Float128' does not name a type**\"\r\n![a](https://user-images.githubusercontent.com/14851411/38535744-98ca9ada-3c74-11e8-8ad3-e4e3ef9d7c40.png)\r\nIf compiled with native gcc ,it is okay.\r\n**gcc version 7.3.1 20180312 (GCC)**\r\ncross-toolchain **4.9.3 raspberry pi** which was downloaded by the **Bazel(version 0.11.1**)automatically.\r\nOS:  4.15.15-1-ARCH\r\nCan anyone help?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I managed to bypass this issue by replaced a higher toolchain version than 4.9.3 . Thanks."]}, {"number": 18370, "title": "typo and readability fixes in CPU section", "body": "Fixed a typo in the Tuning MKL section, and modified punctuation for intra_op_parallelism_threads section for easier readability.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 18369, "title": "JNI Session memory leak", "body": "Curly brace required instead of right bracket for code display on getting started guide\r\n\r\n[session_jni.cc.zip](https://github.com/tensorflow/tensorflow/files/1892574/session_jni.cc.zip)\r\n\r\nJNI Session memory leak for load the configuration\r\n", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Mind handling the CLA before I take a look?", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this change out because it's merging from r1.6. Feel free to reopen with the change and CLA."]}, {"number": 18368, "title": "Fix code block rendering issue in adding_an_op.md", "body": "In adding_an_op.md, html `<code>` was used in markdown for code blocks. However, this does not work very well as some of the code blocks includes incorrect rendering.\r\n\r\nThis fix converts html into \"```c++\" (backticks) so that the rendering could be fixed.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 18367, "title": "Fix incorrect math equation renderings in random_fourier_features.py", "body": "This fix fixes incorrect math equation renderings for markdown in random_fourier_features.py. The issue is that \"```\" backtick should not be added when mathjax quote is used (\"\\\\(\" or \"$$\").\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18366, "title": "Branch 192210794", "body": "Manually merged following files:\r\n\r\n\ttensorflow/BUILD\r\n\ttensorflow/compiler/tests/BUILD\r\n\ttensorflow/compiler/xla/service/hlo_evaluator.cc\r\n\ttensorflow/contrib/autograph/operators/control_flow.py\r\n\ttensorflow/contrib/autograph/pyct/inspect_utils_test.py\r\n\ttensorflow/core/grappler/costs/op_level_cost_estimator.cc\r\n\ttensorflow/core/grappler/costs/op_level_cost_estimator.h\r\n\ttensorflow/core/grappler/optimizers/arithmetic_optimizer.cc\r\n\ttensorflow/core/kernels/cudnn_rnn_ops.cc\r\n\ttensorflow/python/keras/_impl/keras/applications/resnet50.py\r\n\ttensorflow/stream_executor/cuda/cuda_dnn.cc\r\n", "comments": ["OK. All the test passed. Waiting for CLA response."]}, {"number": 18365, "title": "Branch 192193752", "body": "Manually merged following files, PTAL\r\n\r\n\ttensorflow/BUILD\r\n\ttensorflow/compiler/tests/BUILD\r\n\ttensorflow/compiler/xla/service/hlo_evaluator.cc\r\n\ttensorflow/contrib/autograph/operators/control_flow.py\r\n\ttensorflow/contrib/autograph/pyct/inspect_utils_test.py\r\n\ttensorflow/core/grappler/costs/op_level_cost_estimator.cc\r\n\ttensorflow/core/grappler/costs/op_level_cost_estimator.h\r\n\ttensorflow/core/grappler/optimizers/arithmetic_optimizer.cc\r\n\ttensorflow/core/kernels/cudnn_rnn_ops.cc\r\n\ttensorflow/python/keras/_impl/keras/applications/resnet50.py\r\n\ttensorflow/stream_executor/cuda/cuda_dnn.cc\r\n", "comments": ["I think the py3 builds are already failing and broken by internal changes", "Somehow the cla is not replying", "hmm I see, let's wait for a fix before we push then? We don't want to break the github master. clabot seems to do that when there are too many commits in a PR.", "Understood. The build corp has pushed a change to disabled the test. How can I keep this PR and push a new version to it with a new baseline CL closer to head?", "Can you just create and push a commit that disables that test?", "created https://github.com/tensorflow/tensorflow/pull/18366, which includes few more changes. After doing several merge, I am doing efficiently :)"]}, {"number": 18364, "title": "unified flip_* and random_flip_* functions", "body": "The code for flip_left_right / flip_up_down and random_flip... resp. differs only in the index in which the array is reversed. Therefore I have unified these two pairs of functions into non-public `_flip` and `_random_flip` functions.", "comments": []}, {"number": 18363, "title": "Won't build on Ubuntu 16, CUDA 9.0", "body": "I've tried to build TF from source, a number of times over the span of a week, each day pulling the latest TF.  I've tried gcc and I've tried clang.  With gcc it fails near the end.  If needed I'll open a separate issue for clang (the error is very different)\r\n\r\nFirst I'll give configuration, then the FAIL output.\r\n\r\nConfiguration\r\n--------------\r\n - bazel 0.11.1\r\n - python3.5 (/home/ubuntu/envs/med/lib/python3.5/site-packages)\r\n - jemalloc\r\n - No Google Cloud Platform support\r\n - No Hadoop File System support\r\n - Amazon S3 File System support enabled\r\n - No Apache Kafka Platform support\r\n - No XLA JIT support\r\n - No GDR support\r\n - No VERBS support\r\n - No OpenCL SYCL support\r\n - CUDA support\r\n - CUDA SDK 9.0\r\n - cuDNN version 7.0\r\n - TensorRT support will be enabled\r\n - NCCL 1.3\r\n - Nvidia compute capability 7.0\r\n - nvcc will be used as CUDA compiler\r\n - MPI support will be enabled for TensorFlow.\r\n - optimization flags during compilation when bazel option \"--config=opt\": -march=native\r\n - Not configuring the WORKSPACE for Android builds.\r\n\r\n\r\nLatest fail output:\r\n-----------------\r\n... (many warnings before this)\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __device__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(169): warning: __host__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(169): warning: __device__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __host__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __device__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\nINFO: From Compiling tensorflow/core/kernels/data/repeat_dataset_op.cc:\r\ntensorflow/core/kernels/data/repeat_dataset_op.cc: In member function 'virtual void tensorflow::{anonymous}::RepeatDatasetOp::MakeDataset(tensorflow::OpKernelContext*, tensorflow::DatasetBase*, tensorflow::DatasetBase**)':\r\ntensorflow/core/kernels/data/repeat_dataset_op.cc:45:61: warning: 'count' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n         : GraphDatasetBase(ctx), count_(count), input_(input) {\r\n                                                             ^\r\ntensorflow/core/kernels/data/repeat_dataset_op.cc:36:11: note: 'count' was declared here\r\n     int64 count;\r\n           ^\r\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_bitwise_and.cu.cc:\r\n./tensorflow/core/kernels/cwise_ops.h(169): warning: __host__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(169): warning: __device__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __host__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __device__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(169): warning: __host__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(169): warning: __device__ annotation on a defaulted function(\"scalar_left\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __host__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\n./tensorflow/core/kernels/cwise_ops.h(199): warning: __device__ annotation on a defaulted function(\"scalar_right\") is ignored\r\n\r\nINFO: From ProtoCompile tensorflow/core/debug/debugger_event_metadata.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/contrib/mpi/mpi_msg.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/python/framework/cpp_shape_inference.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/debug/debug_service.pb.cc:\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nbazel-out/k8-py3-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/core/grappler/optimizers/graph_optimizer_stage.cc:\r\ntensorflow/core/grappler/optimizers/graph_optimizer_stage.cc: In function 'tensorflow::Status tensorflow::grappler::GetTensorProperties(const tensorflow::grappler::GraphOptimizerContext&, const string&, tensorflow::OpInfo::TensorProperties*)':\r\ntensorflow/core/grappler/optimizers/graph_optimizer_stage.cc:60:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   if (num_outputs == 0 || port > num_outputs - 1) {\r\n                                ^\r\nERROR: /home/ubuntu/downloads/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)\r\nIn file included from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34:0,\r\n                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:\r\n./tensorflow/contrib/mpi/mpi_utils.h: In member function 'const int tensorflow::MPIUtils::GetSourceID(const string&) const':\r\n./tensorflow/contrib/mpi/mpi_utils.h:49:11: error: 'FATAL' was not declared in this scope\r\n       LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << task_id;\r\n           ^\r\n./tensorflow/contrib/mpi/mpi_utils.h:49:16: error: 'LOG' was not declared in this scope\r\n       LOG(FATAL) << \"Failed to convert worker name to MPI index: \" << task_id;\r\n                ^\r\nIn file included from ./tensorflow/core/lib/strings/strcat.h:27:0,\r\n                 from ./tensorflow/core/lib/strings/str_util.h:23,\r\n                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,\r\n                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,\r\n                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:372:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:381:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2688.392s, Critical Path: 200.30s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Forgot to mention:\r\n - AWS p3.2xlarge\r\n - Ubuntu 16.04.4 LTS\r\n\r\n```\r\n> nvidia-smi\r\nMon Apr  9 22:00:23 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   32C    P0    33W / 300W |      0MiB / 16152MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n", "Incidentally, here's the FAIL output when using clang:\r\n\r\nuested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:59:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<float> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex64)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\nIn file included from tensorflow/core/kernels/reduction_ops_gpu_bool.cu.cc:20:\r\nIn file included from ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<double> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:60:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<double> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex128)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\n2 warnings generated when compiling for sm_70.\r\nINFO: From Compiling tensorflow/core/kernels/reduction_ops_half_prod_max_min.cu.cc:\r\nIn file included from tensorflow/core/kernels/reduction_ops_half_prod_max_min.cu.cc:20:\r\nIn file included from ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<float> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:59:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<float> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex64)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\nIn file included from tensorflow/core/kernels/reduction_ops_half_prod_max_min.cu.cc:20:\r\nIn file included from ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<double> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:60:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<double> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex128)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\n2 warnings generated when compiling for sm_70.\r\nINFO: From Compiling tensorflow/core/kernels/softmax_op.cc:\r\nIn file included from tensorflow/core/kernels/softmax_op.cc:26:\r\n./tensorflow/core/kernels/softmax_op_functor.h:60:45: warning: unused variable 'depth_dim' [-Wunused-variable]\r\n    Eigen::IndexList<Eigen::type2index<1> > depth_dim;\r\n                                            ^\r\n1 warning generated.\r\nINFO: From Compiling tensorflow/core/kernels/l2loss_op_gpu.cu.cc:\r\nIn file included from tensorflow/core/kernels/l2loss_op_gpu.cu.cc:20:\r\nIn file included from ./tensorflow/core/kernels/l2loss_op.h:18:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<float> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:59:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<float> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex64)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\nIn file included from tensorflow/core/kernels/l2loss_op_gpu.cu.cc:20:\r\nIn file included from ./tensorflow/core/kernels/l2loss_op.h:18:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<double> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:60:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<double> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex128)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\n2 warnings generated when compiling for sm_70.\r\nINFO: From Compiling tensorflow/core/kernels/softmax_op_gpu.cu.cc:\r\nIn file included from tensorflow/core/kernels/softmax_op_gpu.cu.cc:21:\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:23:\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<float> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:59:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<float> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex64)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\nIn file included from tensorflow/core/kernels/softmax_op_gpu.cu.cc:21:\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:23:\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:\r\nIn file included from external/eigen_archive/Eigen/Core:382:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/Meta.h:187:84: warning: control reaches end of non-void function [-Wreturn-type]\r\n  static T quiet_NaN() { assert(false && \"quiet_NaN not supported for this type\"); }\r\n                                                                                   ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:180:39: note: in instantiation of member function 'Eigen::internal::device::numeric_limits<std::complex<double> >::quiet_NaN' requested here\r\n    return numext::numeric_limits<T>::quiet_NaN();\r\n                                      ^\r\n./tensorflow/core/kernels/reduction_ops.h:60:1: note: in instantiation of member function 'Eigen::GenericNumTraits<std::complex<double> >::quiet_NaN' requested here\r\nFIX_MEAN_IDENTITY(complex128)\r\n^\r\n./tensorflow/core/kernels/reduction_ops.h:53:35: note: expanded from macro 'FIX_MEAN_IDENTITY'\r\n      return Eigen::NumTraits<T>::quiet_NaN();                  \\\r\n                                  ^\r\n2 warnings generated when compiling for sm_70.\r\nINFO: From Compiling tensorflow/contrib/lite/toco/toco.cc:\r\nIn file included from tensorflow/contrib/lite/toco/toco.cc:20:\r\nIn file included from ./tensorflow/contrib/lite/toco/model.h:26:\r\nIn file included from ./tensorflow/contrib/lite/toco/runtime/types.h:18:\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:4750:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]\r\n    c128 = _mm_set1_epi8 (128);\r\n           ~~~~~~~~~~~~~  ^~~\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:5431:27: warning: implicit conversion from 'int' to 'char' changes value from 128 to -128 [-Wconstant-conversion]\r\n    c128 = _mm_set1_epi8 (128);\r\n           ~~~~~~~~~~~~~  ^~~\r\n2 warnings generated.\r\nERROR: /home/ubuntu/downloads/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:58:6: error: explicit specialization of 'CopyTensorData<Eigen::ThreadPoolDevice>' after instantiation\r\nvoid CopyTensorData<CPUDevice>(void* dst, void* src, size_t size) {\r\n     ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:165:3: note: implicit instantiation first required here\r\n  CopyTensorData<Device>((void*)buffer, (void*)input->tensor_data().data(),\r\n  ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:71:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, int>' after instantiation\r\nGENERATE_ACCUMULATE(int);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:72:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, long long>' after instantiation\r\nGENERATE_ACCUMULATE(long long);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:73:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, float>' after instantiation\r\nGENERATE_ACCUMULATE(float);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\n4 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2303.291s, Critical Path: 108.87s\r\nFAILED: Build did NOT complete successfully", "If I remove MPI support from the configuration, it seems to succeed.", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I don't know what the MPI support does and I'm not sure who owns it, because there's no entry in the CODEOWNERS file. I'm marking this community support and tagging @yongtang, @klshrinidhi.", "Added a PR #18907 which should fix the build issue on CUDA+MPI.", "I got error in the final link pywrap_tensorflow_internal.so\r\n\r\nmy env:\r\n\r\nLD_LIBRARY_PATH=/usr/local/openmpi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nPATH=/usr/local/openmpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\n\r\nthere is libmpi.so.40 in :/usr/local/openmpi/lib!!!\r\nroot@u10-154-66-206:/usr/local/openmpi/lib# ll libmpi.so*\r\nlrwxrwxrwx 1 root root      16 Jun 15 07:30 libmpi.so -> libmpi.so.40.0.0*\r\nlrwxrwxrwx 1 root root      16 Jun 15 07:30 libmpi.so.40 -> libmpi.so.40.0.0*\r\n-rwxr-xr-x 1 root root 1148024 Jun 15 07:30 libmpi.so.40.0.0*\r\n\r\nstill got the error:\r\n\r\nERROR: /data/tensorflow-1.8.0/tensorflow/tools/api/generator/BUILD:27:1: Executing genrule //tensorflow/tools/api/generator:python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libmpi.so.40: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.util import tf_decorator\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/root/.cache/bazel/_bazel_root/0a9f54458d3a3a35b3d60e9a3ec7b986/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libmpi.so.40: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n"]}, {"number": 18362, "title": "Branch 192145052", "body": "Manually merged following files. Since this is my first rotation, please take a close look before LGTM this PR. Thanks.\r\n\r\n        tensorflow/BUILD\r\n\ttensorflow/compiler/xla/service/hlo_evaluator.cc\r\n\ttensorflow/core/grappler/costs/op_level_cost_estimator.cc\r\n\ttensorflow/core/grappler/costs/op_level_cost_estimator.h\r\n\ttensorflow/core/grappler/optimizers/arithmetic_optimizer.cc\r\n\ttensorflow/core/kernels/cudnn_rnn_ops.cc\r\n\ttensorflow/python/keras/_impl/keras/applications/resnet50.py\r\n\ttensorflow/stream_executor/cuda/cuda_dnn.cc\r\n", "comments": ["Error introduced in tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc while merge. I will redo a new PR and probably abandon this one."]}, {"number": 18361, "title": "Android: Not a valid TensorFlow Graph serialization: Invalid GraphDef", "body": "I have an issue with loading custom model into Android app. \r\nI'm using `LinearClassifier` and the function `export_savedmodel()` for saving the model.\r\nIt looks like this\r\n```\r\nmodel_path = model.export_savedmodel(export_dir_base=\"model\", serving_input_receiver_fn=serving_input_fn)\r\n```\r\n\r\nThen from the path, I'm getting the file `saved_model.pb` and put it into the `assets` folder of the Android app. Then I try to load the model\r\n```\r\nval inferenceInterface = TensorFlowInferenceInterface(assets, \"saved_model.pb\")\r\n```\r\nHowever, at runtime IOException is thrown:\r\n```\r\nFailed to load model from 'saved_model.pb'\r\n...\r\nAndroid: Not a valid TensorFlow Graph serialization: Invalid GraphDef\r\n```\r\n\r\nDoes anybody know what can be the issue? Seems like either the model is exported wrong or the issue with the implementation of `TensorFlowInferenceInterface`. Did somebody have experience loading models in Android app?\r\n\r\nModel export is located [here](https://github.com/dkhmelenko/fitness-ml/blob/master/simple/Fitness%20Classification.ipynb)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sure, sorry for incomplete data. Here they are:\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from Anaconda\r\nTensorFlow version: 1.4.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: Described in the first comment", "The TensorFlow Mobile API only supports \"frozen graphs\" (where all model parameters are \"frozen\" as constants in the graph) and not the [SavedModel](https://www.tensorflow.org/programmers_guide/saved_model#build_and_load_a_savedmodel) format at this time.\r\n\r\nSee https://www.tensorflow.org/mobile/prepare_models for details on creating frozen models suitable for TensorFlow mobile. I believe changes are in progress to make it easier to created frozen models as well as models in a format usable by [TFLite](https://www.tensorflow.org/mobile/) directly from the SavedModel format.\r\n\r\nHope that helps.", "@asimshankar Thank you for your help! \r\n\r\nI'm trying to freeze my model and I'm getting the error \r\n```\r\nTypeError: names_to_saveables must be a dict mapping string names to Tensors/Variables. Not a variable: Tensor(\"linear/linear_model/bias_weights:0\", shape=(18,), dtype=float32)\r\n```\r\n\r\nI looked on  [StackOverflow](https://stackoverflow.com/search?q=names_to_saveables+must+be+a+dict+) and seems this issue happens to many people and there is no answer to that.\r\n\r\nDo you know what can be the reason? \r\n\r\nThe code for creating the model is very simple\r\n```python\r\ngender = tf.feature_column.categorical_column_with_vocabulary_list(\"gender\", [\"f\", \"m\"])\r\ngoal = tf.feature_column.categorical_column_with_vocabulary_list(\"goal\", [\"fit\", \"lose\", \"muscle\"])\r\nlevel = tf.feature_column.categorical_column_with_vocabulary_list(\"level\", [\"begin\", \"middle\", \"advance\"])\r\n\r\nfeature_columns = [gender,goal,level]\r\ninput_fn = tf.estimator.inputs.pandas_input_fn(x=x_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)\r\nmodel = tf.estimator.LinearClassifier(feature_columns=feature_columns, n_classes=18, model_dir=\"export\")\r\n\r\nmodel.train(input_fn=input_fn, steps=100)\r\n```", "I created a separate issue https://github.com/tensorflow/tensorflow/issues/18523 for the comment above, because it's a different issue. You can ignore the comment about."]}, {"number": 18359, "title": "Tensorflow import error for tf.contrib.data.python.ops.threadpool(used for benchmark script)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I am trying to run tf_cnn_benchmarks provided by tensorflow\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Redhat linux 7.4\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0\r\n- **GCC/Compiler version (if compiling from source)**:c++ (GCC) 5.3.0\r\n- **CUDA/cuDNN version**:cuda-toolkit/9.0.176 nccl/2.1.2-1 anaconda cuDNN/9.0v7\r\n- **GPU model and memory**: Tesla k40, memory 55gb\r\n- **Exact command to reproduce**:  python tf_cnn_benchmarks.py \r\n\r\n### Describe the problem\r\nI am trying to run the benchmark provided in this link([https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py])\r\nIt says it can not import threadpool. Threadpool is inside tf.contrib.data API which may be deprecated. \r\n\r\n\r\n\r\n### Source code / logs\r\nI am including the trace below:\r\n\r\n```\r\n>>> import tensorflow\r\n>>> from tensorflow.contrib.data.python.ops import threadpool\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name threadpool\r\n```\r\n\r\n", "comments": ["The `tensorflow.contrib.data.python.ops.threadpool` module was added after TensorFlow 1.5 (in ee40d87af8e6c24e6e84ff64e4932c38d6ccfcf7), so you'll need to upgrade to TensorFlow 1.7 or later to use that particular version of the benchmarking script."]}, {"number": 18358, "title": "Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: dropout_1/cond/dropout/random_uniform/max = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>](dropout_1/cond/Switch:1)", "body": "I used Transfer Learning to learn a set of features. So used these 3 final layers on my transfer learning task.\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense_1 (Dense)              (None, 256)               6422784   \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 256)               0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 2)                 514       \r\n=================================================================\r\nTotal params: 6,423,298\r\nTrainable params: 6,423,298\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\n All workout on ipythonNotebook but after making it into a .pb file and loading into Android Studio. This error occurs\r\n\r\n> **Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: dropout_1/cond/dropout/random_uniform/max = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>](dropout_1/cond/Switch:1)**\r\n\r\n**The same code works on those models which have input on their first layer**\r\n_Example: conv2d_input_1_ which was my previous task which didn't relate to transfer learning but as soon as the model changed the code didn't work. I think tensorflowAndroid wants the input layer to feed the images but our transfer learning model doesn't have the input layer to feed and is showing problem everytime\r\nI checked the version of tensorflow and tensorflow on android and matched the version but still no luck. Could you please help me!!\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "System information\r\nHave I written custom code : yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.4.0\r\nPython version: 2.7.13\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: see below", "@codexponent : Could you share a [minimal, verifiable, complete](https://stackoverflow.com/help/mcve) example to reproduce the problem? The error message is suggesting that somehow the graph is providing an input to a \"constant\" node, which shouldn't be possible. Details on reproducing this problem will be helpful.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "After converting .ckpt to .pb, when I used the pb files, I had the same problem, there are my error \"ValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node rpn_path_drop/case/cond/Const}} = Constdtype=DT_FLOAT, value=Tensor<type: float shape: [] values:1>(rpn_path_drop/case/cond/Switch:1)\"."]}, {"number": 18357, "title": "Bug: GPU resources not released appropriately when graph is reset & session is closed", "body": "Have I written custom code: Forked network arch for FasterRCNN from `https://github.com/endernewton/tf-faster-rcnn`\r\nOS Platform and Distribution: Ubuntu 16.04.4, x86_64 GNU/Linux\r\nTensorFlow installed from: \r\nBazel version: 0.5.2\r\nCUDA/cuDNN version: CUDA V8.0.61, release 8.0\r\nGPU model and memory: `Nvidia Tesla K80`, 12 GB memory\r\nExact procedure to reproduce: Load `graph` and `net` in GPU memory, use `tf.reset_default_graph()` followed by `sess.close()`, GPU memory not freed as seen through `nvidia-smi`\r\n______________________________________________________________________________________________\r\nPossible duplicate but re-opening here since it doesn't appear to have been resolved & there's no way to re-open the previously filed ones:\r\n\r\nVersions Used/Tried:\r\n - System: aws EC2 (ubuntu) (with p2.xlarge elastic GPU instance)\r\n - GPU: `NVIDIA Tesla K80`\r\n - Tensorflow versions: 1.3.0, 1.5.5 (tried on both)\r\n - CUDA Version: 8.0, 9.0 (tried on both)\r\n\r\nCalling `tf.reset_graph()` and `sess.close()` doesn't free GPU as seen using `nvidia-smi`. \r\nIt might be possible that nvidia-smi doesn't update but in that case, there wouldn't be resource errors on subsequent code trying to run on the GPU as seen here:\r\n![image](https://user-images.githubusercontent.com/15898956/38519614-6ad30aec-3c0e-11e8-895e-b9b2d20dab17.png)\r\n\r\nWhat i tried after this was to use a subprocess within my script to kill the previous processes occupying resources on the GPU as seen here:\r\n![image](https://user-images.githubusercontent.com/15898956/38519591-55817534-3c0e-11e8-8bae-51d91e20b98e.png)\r\nAlthough this works, it seems like incredibly bad practice & I shouldn't be doing this.\r\n\r\nWhat I tried then was to use the `numba` host API for interacting with the GPU to shut down processes i.e. clear `current_context` and then clear deallocations from the GPU like this:\r\n`from numba import cuda\r\ncurrent_context = get_context(devnum=0)\r\ncurrent_context.reset()\r\ncuda.current_context().deallocations.clear()`\r\n\r\nWhen this didn't work out, as a last resort, i tried to use this:\r\n`from numba import cuda\r\ncuda.gpus[0].numba.cuda.cudadrv.devices.reset()` \r\nwhich works but results in a substantial memory leak everytime it runs which implies that after running my code a few times, the leak accumulated to a large enough value to again give me  resource errors.\r\n\r\n___________________________________________________________________________________________________\r\n\r\nContext: I am trying to deploy a deep learning model using a flask API. Since this is a pipeline with multiple computation graphs, I cannot afford to keep all of those in memory so i need to do something like this:\r\n1) Upload data & store it\r\n2) Build Graph\r\n2) Run Inference on stored data\r\n3) Remove graph from memory\r\n4) Build new graph\r\n5) Run Inference on stored data again\r\n.\r\n.\r\n....and so on\r\n--------------------------------------------------------------------------------------------\r\nWhat I suspect might be going on under the hood:\r\n          It's possible that `tf.reset_graph()` frees memory but doesn't remove the actual process holding onto that chunk of memory in the GPU in which case it makes sense for `nvidia-smi` to still show me the PID occupying memory in the GPU. But shouldn't `tf.reset_graph()` followed by `sess.close()` be freeing the GPU entirely?\r\n\r\n----------\r\n\r\nAny help on the issue will be appreciated.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler I have made the requested updates.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18356, "title": "Failed to build for iOS using Xcode 9.3: thread-local storage is not supported for the current target", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: Xcode 9.3: `Apple LLVM version 9.1.0 (clang-902.0.39.1)`\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: `tensorflow/contrib/makefile/build_all_ios.sh -g /path/to/model.pb`\r\n\r\n### Describe the problem\r\nAs of Xcode 9.3 (was working fine on 9.2), compiling TF for iOS using `build_all_ios.sh` fails, complaining that \"thread-local storage is not supported for the current target\". This is related to #12573, which introduced the `thread_local` attribute for iOS builds.\r\n\r\n### Source code / logs\r\n```\r\n$ tensorflow/contrib/makefile/build_all_ios.sh -g /path/to/model.pb\r\n[...]\r\ngcc --std=c++11 -DIS_SLIM_BUILD -fno-exceptions -DNDEBUG -O3 -DANDROID_TYPES=__ANDROID_TYPES_FULL__ -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION -mios-simulator-version-min=9.0 -arch i386 -mno-sse -fembed-bitcode -D__thread=thread_local -DUSE_GEMM_FOR_CONV -Wno-c++11-narrowing -DTF_LEAN_BINARY -D__ANDROID_TYPES_FULL__ -fno-exceptions -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator11.3.sdk -MT /Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/local_device.o -MMD -MP -MF /Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/dep/ios_I386//tensorflow/core/common_runtime/local_device.Td -I. -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/ -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/fft2d -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/common_runtime/local_device.cc -o /Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/local_device.o\r\nIn file included from tensorflow/core/common_runtime/local_device.cc:18:\r\nIn file included from ./tensorflow/core/common_runtime/local_device.h:19:\r\nIn file included from ./tensorflow/core/common_runtime/device.h:35:\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:21:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from /Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:79:\r\nIn file included from /Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:58:\r\n/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/SimpleThreadPool.h:153:5: error:\r\n      thread-local storage is not supported for the current target\r\n    EIGEN_THREAD_LOCAL PerThread per_thread;\r\n    ^\r\n/Users/json/everalbum/ios-sdk/submodules/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note:\r\n      expanded from macro 'EIGEN_THREAD_LOCAL'\r\n#define EIGEN_THREAD_LOCAL static __thread\r\n                                  ^\r\n<command line>:6:18: note: expanded from here\r\n#define __thread thread_local\r\n```", "comments": ["@petewarden here's an iOS build issue potentially related to a change you made.", "I changed `-D__thread=thread_local \\` to `-D__thread= \\` in tensorflow/contrib/makefile/Makefile (for the `i386` architecture only) and that's an okay workaround for now. It (probably) re-introduces the problem that #12573 solved for that architecture, but at least for my use that's acceptable until this gets a proper solution.", "+1\r\nAny update on this ?", "@sadlerjw I am working on Mac sierra 10.13.5 (17F77) , and what I did is here:\r\nchecked out tensorflow 1.9.0-rc1\r\ndid export ANDROID_TYPES=\"-D__ANDROID_TYPES_FULL__\"\r\nand then launched the script tensorflow/contrib/makefile/buils_all_ios.sh .\r\n\r\nOn building stage for the arch i386 I got the error:\r\n```\r\ntensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/SimpleThreadPool.h:153:5: error: \r\n      thread-local storage is not supported for the current target\r\n    EIGEN_THREAD_LOCAL PerThread per_thread;\r\n    ^\r\n/pj/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note: \r\n      expanded from macro 'EIGEN_THREAD_LOCAL'\r\n#define EIGEN_THREAD_LOCAL static __thread\r\n                                  ^\r\n<command line>:3:18: note: expanded from here\r\n#define __thread thread_local\r\n                 ^\r\nIn file included from tensorflow/core/common_runtime/local_device.cc:18:\r\nIn file included from ./tensorflow/core/common_runtime/local_device.h:19:\r\nIn file included from ./tensorflow/core/common_runtime/device.h:35:\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:20:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from /pj/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:79:\r\nIn file included from /pj/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:59:\r\n/pj/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:326:5: error: \r\n      thread-local storage is not supported for the current target\r\n    EIGEN_THREAD_LOCAL PerThread per_thread_;\r\n    ^\r\n/pj/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/ThreadLocal.h:15:35: note: \r\n      expanded from macro 'EIGEN_THREAD_LOCAL'\r\n#define EIGEN_THREAD_LOCAL static __thread\r\n                                  ^\r\n<command line>:3:18: note: expanded from here\r\n#define __thread thread_local\r\n                 ^\r\n2 errors generated.\r\nmake: *** [pj/tensorflow/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/local_device.o] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'i386 compilation failed.'\r\ni386 compilation failed.\r\n+ exit 1\r\n\r\n```", "I'm up for sadlerjw's fix because when most people use this library i386 is more about just making sure one of the simulators compiles, but it is unnecessary for testing or production, just that it passes automated testing and whatnot.", "sadlerjw's fix does not work for tensorflow 1.10 + xcode 9.4.1. It fixed for Eigen, but there is another similar error:\r\ntensorflow/core/util/work_sharder.cc:23:23: error: thread-local storage is not supported for the current target\r\n", "@sadlerjw 's fix doesn't seem to work anymore on TF 1.10. Getting the same error like @robinqhuang \r\n\r\nSpecifically it's line 23 in `tensorflow/tensorflow/core/util/work_sharder.cc`\r\n\r\n```\r\n/* ABSL_CONST_INIT */ thread_local int per_thread_max_parallism = 1000000;\r\n```\r\n\r\nI fixed it by replacing `thread_local` with `__thread` since it is defined as empty in @sadlerjw 's fix for i386 and that's what we want\r\n\r\n```\r\n/* ABSL_CONST_INIT */ __thread int per_thread_max_parallism = 1000000;\r\n```", "I had the same problem as @eaigner on TF 1.11, and used the same workaround", "Thanks @eaigner for the workaround. I think it was resolved.  Please open a new ticket if you see a similar issue. Thanks!", "I am facing a similar issue. I tried both @eaigner and @sadlerjw solutions but I am still getting the error. Anyone else facing same problem?"]}, {"number": 18355, "title": "Cannot return string from tf.data map function", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nThe map method of tf.data does not allow returning a string because no implicit conversion to a tensor is made. Instead, an error `AttributeError: 'str' object has no attribute 'get_shape'` is raised.\r\n\r\nAccording to @mrry this is a bug introduced in TF 1.5. For a reference, see the comments at: https://stackoverflow.com/questions/49668252/returning-strings-in-tf-data-dataset-map-method this is also the Q&A that originally noted this problem.\r\n\r\n### Source code / logs\r\nTo reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef map_fn(x):\r\n    return x*2, 'foo'\r\n\r\ndataset = tf.data.Dataset.range(5)\r\ndataset = dataset.map(map_fn)\r\n```\r\n\r\nError trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/username/tensorflow-remote/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 790, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/home/username/tensorflow-remote/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1597, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"/home/username/tensorflow-remote/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py\", line 486, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/home/username/tensorflow-remote/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py\", line 321, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/home/username/tensorflow-remote/local/lib/python2.7/site-packages/tensorflow/python/framework/function.py\", line 338, in _create_definition_if_needed_impl\r\n    outputs = self._func(*inputs)\r\n  File \"/home/username/tensorflow-remote/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1585, in tf_map_func\r\n    ret, [t.get_shape() for t in nest.flatten(ret)])\r\nAttributeError: 'str' object has no attribute 'get_shape'\r\n```\r\n", "comments": ["@jsimsa I think this regression crept in at 82fa1e1ae5b2f8af642979fafb1cab455db1882f, which added SparseTensor support, because the new logic assumes that all return values from a map function are either `tf.Tensor` or `tf.SparseTensor`, whereas the old logic had a conversion pass. Perhaps some variant of `convert_to_tensor_or_sparse_tensor()` is needed here?", "Nagging Assignee @jsimsa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This has been fixed in https://github.com/tensorflow/tensorflow/commit/5a53c9b54d8781032ebf2cf26f93da3b2a33d1e4"]}, {"number": 18354, "title": "LookupError: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: b'v1.3.0-rc1-3011-gd86448938' 1.3.0\r\n- **Python version**: Python 3.6.4 \r\n-Bazel version: N/A\r\n-CUDA/cuDNN version: N/A\r\n-GPU model and memory: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninp = tf.ones((1, 2, 2, 5))\r\nshape = (3, 3, 5, 1)\r\nfilters = tf.Variable(tf.zeros_initializer()(shape))\r\nconv = tf.nn.depthwise_conv2d(inp, filters, strides=[1, 1, 1, 1], padding=\"SAME\")\r\nvalue_layer = tf.layers.Dense(\r\n    units=1, kernel_initializer=tf.zeros_initializer(), use_bias=False, activation=None)\r\nval = value_layer.apply(conv)\r\n\r\ntrainer = tf.train.GradientDescentOptimizer(1e-4)\r\nv_grads, _ = zip(*trainer.compute_gradients(val))\r\nv_gradsf = tf.concat([tf.reshape(e, [-1, 1]) for e in v_grads], axis=0)\r\nvweights = tf.random_uniform((50, 1))\r\ndot = tf.matmul(tf.transpose(v_gradsf), vweights)\r\nv_hess, _ = zip(*trainer.compute_gradients(dot))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nprint(sess.run((v_grads, dot, v_hess)))\r\n```\r\n```\r\nLookupError: No gradient defined for operation 'gradients/depthwise_grad/DepthwiseConv2dNativeBackpropFilter' (op type: DepthwiseConv2dNativeBackpropFilter)\r\n```\r\nI'm not sure if this is a bug or feature request. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "This is a feature request to implement second order gradients `tf.nn.depthwise_conv2d`.\r\n\r\n/CC @zhangyaobit, do you know if there are any plans for this or how difficult this would be?\r\n\r\n", "Also happens for `tf.nn.separable_conv2d` and `tf.contrib.layers.conv2d_in_plane` which I think use the same gradient function.", "@reedwm I am also looking forward to this feature - second order gradients `tf.nn.depthwise_conv2d`. Any updates on this?", "Marking this as contribution welcome; please feel free to contribute on this before we get a chance to work on it!", "@reedwm I am also looking forward for this feature. It would be nice for this op to support float16 precision.", "i am working on it.", "i have created a pl https://github.com/tensorflow/tensorflow/pull/20782", "Although a fix for this was merged in https://github.com/tensorflow/tensorflow/commit/6d96cc4d05fd09e5663853a795bcd9a5b01f1732\r\n\r\nIt seems as if this issue has returned at some point since I'm getting it with the exact same repro-steps as in this ticket.\r\nSee this colab for quick proof:\r\nhttps://colab.research.google.com/drive/1s-0Co367Os762TzId5L2_kT1miLWaiMd\r\n\r\n", "See [this comment](https://github.com/tensorflow/tensorflow/pull/20782#issuecomment-479999811). This will be in the next stable release.", "Closing this issue since this is fixed in the master branch."]}, {"number": 18353, "title": "InvalidArgumentError in tensorflow reduce_sum gradient when compiling from sources (still a problem in TF 1.7)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: source, without AVX support\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: CMake 3.10.3\r\n- **GCC/Compiler version (if compiling from source)**: MSVC 2017\r\n- **CUDA/cuDNN version**: CPU only\r\n- **GPU model and memory**: CPU only\r\n- **Exact command to reproduce**:\r\nUsing the same example provided in [#12177](https://github.com/tensorflow/tensorflow/issues/12177) and following the suggestions in the comment:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(tf.float32, [2,3])\r\ny = tf.placeholder(tf.float32, [2,3])\r\nW = tf.Variable(tf.random_normal([3,3], stddev=0.01))\r\n\r\nz = tf.matmul(x, W)\r\n# works when removing the reduce_sum op\r\nz = tf.reduce_sum(tf.multiply(z, y), axis=1, keep_dims=True)\r\n\r\noptimizer = tf.train.AdamOptimizer()\r\ntrain_op = optimizer.minimize(z)\r\n\r\nXdata = np.array([[1,2,3],[4,5,6]])\r\nYdata = np.array([[2,3,4],[3,4,5]])\r\n\r\nop = tf.get_default_graph().get_operation_by_name(\"gradients/Sum_grad/Reshape\")\r\nwith tf.Session() as sess:\r\n    for input_ in op.inputs:\r\n        print(input_, sess.run(input_, feed_dict={x: Xdata, y: Ydata}))\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(train_op, feed_dict={ x: Xdata, y: Ydata })\r\n    print('done')\r\n```\r\nResults:\r\n- Build from source\r\n```\r\nTensor(\"gradients/Fill:0\", shape=(2, 1), dtype=float32) [[1.]\r\n [1.]]\r\nTensor(\"gradients/Sum_grad/DynamicStitch:0\", shape=(?,), dtype=int32) [2 1 0 ... 0 0 0]\r\n2018-04-09 14:59:09.801431: F C:\\Users\\hcheng\\tensorflow\\tensorflow\\core\\framework\\tensor_shape.cc:243] Check failed: ndims_byte() < MaxDimensions() (unsigned char value 254 vs. 254)Too many dimensions in tensor\r\n```\r\n- Precompiled binary\r\n```\r\nTensor(\"gradients/Fill:0\", shape=(2, 1), dtype=float32) [[1.]\r\n [1.]]\r\nTensor(\"gradients/Sum_grad/DynamicStitch:0\", shape=(?,), dtype=int32) [2 1]\r\ndone\r\n```\r\n\r\n", "comments": ["/CC @mrry, any ideas?", "Well, it looks like something is trying to make a shape of rank `0 - 2`. Since our binaries don't crash in the same way, we'll need some help to track down the problem. Can you try running the script with the environment variable `TF_CPP_MIN_VLOG_LEVEL=1` set, and post the log that results? That should tell us what op failed.", "Please find the result in the link below. Thanks.\r\n[result.txt](https://github.com/tensorflow/tensorflow/files/1895252/result.txt)\r\n\r\n", "That seems to be a different failure (a Python `InvalidArgumentError` exception) than the one you described in the initial issue (a fatal error, leading to a process crash). Can you reproduce the process crash version with that environment variable set as well?", "Yes. It is kind of random actually; sometimes it's the InvalidArgumentError and sometimes it's the \"ndims_byte() < MaxDimensions()\" check fail. Please find the log below for the \"ndims_byte() < MaxDimensions()\":\r\n[result_2.log](https://github.com/tensorflow/tensorflow/files/1895426/result_2.log)\r\n", "It looks like the DynamicStitch op is giving an incorrect output in some cases. Can you replace the last part of the program with the following and let us know what the three values are when it crashes?\r\n\r\n```python\r\nop = tf.get_default_graph().get_operation_by_name(\"gradients/Sum_grad/DynamicStitch\")\r\nwith tf.Session() as sess:\r\n    for input_ in op.inputs:\r\n        print(input_, sess.run(input_, feed_dict={x: Xdata, y: Ydata}))\r\n    print(op.outputs[0], sess.run(op.outputs[0], feed_dict={x: Xdata, y: Ydata}))\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(train_op, feed_dict={ x: Xdata, y: Ydata })\r\n    print('done')\r\n```", "Please find the logs below, thanks.\r\n[Check failed.log](https://github.com/tensorflow/tensorflow/files/1898919/Check.failed.log)\r\n[InvalidArgumentError.log](https://github.com/tensorflow/tensorflow/files/1898920/InvalidArgumentError.log)\r\n", "I'm afraid I haven't had time to reproduce this, and it's not an area that I work on, so I'm going to unassign myself (and hopefully someone else can pick up the issue, if it's still a problem).", "Is this still an issue with the latest Tensorflow version ?\r\n", "I am having the same issue.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18352, "title": "Placeholder not replaced by import_graph_def input_map argument", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have written custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: \r\n\r\n### Problem Description\r\nI'm having trouble connecting a placeholder in a GraphDef loaded from a file to a dataset provider using the`input_map` argument to `tf.import_graph_def`, and I don't think it's behaving as expected.\r\n\r\nI first set up a data source, referenced in a tensor called `images`. Then I load a GraphDef from a file that contains a Placeholder called `batch`. Then I call\r\n\r\n      tf.import_graph_def(quantized_graph_def, input_map={'batch': images}, name='')\r\n\r\nto connect them together. When I run this I get the error:\r\n\r\n    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor \r\n    'batch_1' with dtype float and shape [100,224,224,3]\r\n                 [[Node: batch_1 = Placeholder[dtype=DT_FLOAT, shape=[100,224,224,3], \r\n    _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nAccording to the documentation for `tf.import_graph_def`:\r\n\r\n> `input_map`: A dictionary mapping input names (as strings) in graph_def to Tensor objects. The values of the named input tensors in the imported graph will be re-mapped to the respective Tensor values.\r\n\r\nAs I understand it, the `input_map` argument should connect the two graphs, but that doesn't seem to be working. See related article [\"Connecting Two Graphs Together using `import_graph_def`\"](https://blog.konpat.me/tf-connecting-two-graphs-together/). I believe that I am doing the same thing as in the article.\r\n\r\n### Source Code/Logs\r\n\r\nThis script borrows heavily from [`models/research/slim/eval_image_classifier.py`](https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py).\r\n\r\nFirst I open a graph\r\n\r\n    with tf.Graph().as_default():\r\n\r\nThen I set up a dataset provider and preprocessing function using the `slim` API\r\n\r\n      # Select the dataset\r\n      # Create a dataset provider that loads data from the dataset\r\n      # Select the preprocessing function\r\n      ...\r\n      image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n\r\n      images, labels = tf.train.batch(\r\n          [image, label],\r\n          batch_size=batch_size,\r\n          num_threads=num_preprocessing_threads,\r\n          capacity=5 * batch_size)\r\n\r\nThen I import a graph from a GraphDef from a file and load it into the current graph using `import_graph_def`\r\n\r\n      quantized_graph_def = graph_pb2.GraphDef()\r\n      with tf.gfile.FastGFile(path.join(cwd(), quantized_graph_filename), 'rb') as f:\r\n        quantized_graph_def.ParseFromString(f.read())\r\n      tf.import_graph_def(quantized_graph_def, input_map={'batch': images}, name='')\r\n\r\nThen I set up the metrics and call `slim.evaluation.evaluate_once` to process the batches\r\n\r\n      # Define the metrics:\r\n      names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n          'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\r\n          'Recall_5': slim.metrics.streaming_recall_at_k(\r\n              logits, labels, 5),\r\n      })\r\n\r\n      ...\r\n\r\n      slim.evaluation.evaluate_once(\r\n          master=master,\r\n          checkpoint_path=checkpoint_path,\r\n          logdir=log_dir,\r\n          num_evals=num_batches,\r\n          eval_op=list(names_to_updates.values()))\r\n\r\nWhen I run this I get the following error:\r\n\r\n    Caused by op 'batch_1', defined at:\r\n      File \"vanilla_vgg.py\", line 319, in <module>\r\n        import_quantized_graph_with_imagenet()\r\n      File \"vanilla_vgg.py\", line 251, in import_quantized_graph_with_imagenet\r\n        tf.import_graph_def(quantized_graph_def, input_map={'batch': images}, name='')\r\n      File \"/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n        return func(*args, **kwargs)\r\n      File \"/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 553, in import_graph_def\r\n        op_def=op_def)\r\n      File \"/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n        op_def=op_def)\r\n      File \"/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_1' with dtype float and shape [100,224,224,3]\r\n             [[Node: batch_1 = Placeholder[dtype=DT_FLOAT, shape=[100,224,224,3], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nThe GraphDef that I am loading has a Placeholder op with the name `batch`, with the same shape and dtype as the tensor `images`. For reference, running `print(images)` returns:\r\n\r\n    Tensor(\"batch:0\", shape=(100, 224, 224, 3), dtype=float32)\r\n\r\nI have also tried using `batch:0` and `batch_1` as the key to the `input_map` but neither works.\r\n\r\nThe `evaluate_once` function runs a batch of images in a single function call, so I cannot simply call `images.eval()` and pass the result to `evaluate_once` because it would only run the first batch. So the two graphs must be connected and able to be run with a single invocation.\r\n", "comments": ["Update on this, I realized that 'batch' in the graph that I'm importing has control inputs that are not being mapped with `input_map`. Relevant issue may be [here](https://github.com/tensorflow/tensorflow/issues/7508).\r\n\r\nThe `batch` port has two tensor outputs and three control dependencies. Looking at the graph in tensorboard after the `import_graph_def` call, the tensor inputs are mapped correctly but the control dependencies are left dangling. How do I map these?", "According to [this comment](https://github.com/tensorflow/tensorflow/issues/7508#issuecomment-363964163), there is a way to map control inputs using the `c_api` module.\r\n\r\nI have tried: \r\n\r\n    scoped_options = c_api_util.ScopedTFImportGraphDefOptions()\r\n    options = scoped_options.options\r\n\r\n    tf.import_graph_def(quantized_graph_def,\r\n        input_map={'batch': images},\r\n        name='')\r\n    c_api.TF_ImportGraphDefOptionsRemapControlDependency(options, 'batch', images)\r\n\r\nresulting in:\r\n\r\n    Traceback (most recent call last):\r\n      File \"vanilla_vgg.py\", line 419, in <module>\r\n        import_quantized_graph_with_imagenet()\r\n      File \"vanilla_vgg.py\", line 281, in import_quantized_graph_with_imagenet\r\n        c_api.TF_ImportGraphDefOptionsRemapControlDependency(options, 'batch', images)\r\n    TypeError: in method 'TF_ImportGraphDefOptionsRemapControlDependency', argument 3 of type 'TF_Operation *'\r\n\r\nThen I tried:\r\n\r\n    c_api.TF_ImportGraphDefOptionsRemapControlDependency(options, 'batch', images._as_tf_output().oper)\r\n\r\nresulting in:\r\n\r\n    File \"/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 556, in _as_tf_output\r\n      assert self.op._c_op\r\n    AssertionError\r\n\r\nHow do I get a handle to a `c_op` from the input tensor `images`?", "Any ideas on this?", "You can run the previous graph_def through a filter that removes all control dependencies. If you are doing inference with no loops, usually control dependencies only exist to do debugging. Does that help?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18351, "title": "TF Lite Documentation and Raspberry Pi Demo Request", "body": "Hi All,\r\n\r\nI want to request further documentation for running TF Lite on a Raspberry Pi.\r\nI was really impressed by the SSD demo shown by Andrew Selle during the TensorFlow Dev Summit and would love to see documentation to reimplement something like that.\r\n\r\nAt some point the tensorflow/contrib/lite/README.md stated something about a Raspberry Pi demo app coming soon, but it is no longer there. Is this something that you still plan to support?\r\n\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: master\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "Sorry, it is on my list to open source this.", "This is a dupe of #19110.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Dupliate."]}, {"number": 18350, "title": "Add support for tf.repeat (equivalent to np.repeat)", "body": "This PR tries to address the feature request proposed in #8246 to add the support for tf.repeat that is equivalent to np.repeat.\r\n\r\nThis PR is just to pick up the work of #15224 which added tf.repeat but closed and didn't merge.", "comments": ["Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This change is fine for API review", "Can this be implemented in terms of tf.tile or tf.contrib.framework.broadcast_to?", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Closing since I think this can be implemented in terms of existing ops.", "Wait, @alextp there are lots of tensorflow ops that can be implemented in terms of existing ops (e.g. expand_dims). But implementing repeat is a hassle, prone to errors, and not easily readable. Please clarify why this merge request is getting rejected?", "I think you can just write the python code and get the same performance in\nthis case, without having to write new C++ kernels, which add to the\ncomplexity, maintenance cost, and binary size of tensorflow.\n\nOn Thu, Jul 26, 2018 at 7:50 PM Ethan Brooks <notifications@github.com>\nwrote:\n\n> Wait, @alextp <https://github.com/alextp> there are lots of tensorflow\n> ops that can be implemented in terms of existing ops (e.g. expand_dims).\n> But implementing repeat is a hassle, prone to errors, and not easily\n> readable. Please clarify why this merge request is getting rejected?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18350#issuecomment-408295099>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxR3y0hXcKo_77csZnhYkyTCKVK2zks5uKn_agaJpZM4TM5ar>\n> .\n>\n\n\n-- \n - Alex\n", "And by this I mean that if you send a PR which implements this in terms of\nother ops I'll happily accept it.\n\nOn Fri, Jul 27, 2018 at 8:17 AM Alexandre Passos <apassos@google.com> wrote:\n\n> I think you can just write the python code and get the same performance in\n> this case, without having to write new C++ kernels, which add to the\n> complexity, maintenance cost, and binary size of tensorflow.\n>\n> On Thu, Jul 26, 2018 at 7:50 PM Ethan Brooks <notifications@github.com>\n> wrote:\n>\n>> Wait, @alextp <https://github.com/alextp> there are lots of tensorflow\n>> ops that can be implemented in terms of existing ops (e.g. expand_dims).\n>> But implementing repeat is a hassle, prone to errors, and not easily\n>> readable. Please clarify why this merge request is getting rejected?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/18350#issuecomment-408295099>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAATxR3y0hXcKo_77csZnhYkyTCKVK2zks5uKn_agaJpZM4TM5ar>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n", "Got it. That makes a lot of sense. Thanks."]}, {"number": 18349, "title": "Allow access to keras.utils.model_to_dot API", "body": "\r\n`model_to_dot` is not currently accessible from TensorFlow Keras. So visualizing the graph directly using commands  like` SVG(model_to_dot(model).create(prog='dot', format='svg'))` is not possible. \r\n\r\nAdd `@tf_export('keras.utils.model_to_dot')` in  vis_utils.py\r\n\r\nAdd `from tensorflow.python.keras._impl.keras.utils.vis_utils import model_to_dot` in \r\n'tensorflow/python/keras/_impl/keras/utils/__init__.py'\r\n\r\nAdd `from tensorflow.python.keras._impl.keras.utils.vis_utils import model_to_dot` in \r\n'tensorflow/python/keras/utils//__init__.py'", "comments": ["@fchollet Can you take a look at this, please?", "Nagging Assignee @jhseu: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you @fchollet \r\n@jhseu could you please update label.", "It seems to be failing on Python2 for target //tensorflow/tools/api/tests:api_compatibility_test\r\nAny suggestions?", "Can you run the test locally? It'll show the message for how to update the goldens for API changes.", "@jhseu how do I run test locally? I have applied the proposed changes on the TensorFlow installation in my local machine. The `model_to_dot `API works as expected and I don't get any error messages.", "To be specific, the failing test here: `//tensorflow/tools/api/tests:api_compatibility_test`. We keep a list of APIs available to users, and that test failure will explain how to update the golden files.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you fix the conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Since the` /_impl/` folder was removed I think this PR is not valid anymore. I will close and create a new PR. Though still no idea how to resolve the API conflicts."]}, {"number": 18348, "title": "Fix some minor incorrect anchor links", "body": "This PR is to fix some incorrect anchor links as below.\r\n- Fix the incorrect anchor name \"PrepareLinux\", which cause cannot direct to correct section with link: https://www.tensorflow.org/install/install_sources#PrepareLinux \r\n- Fix https://www.tensorflow.org/install/install_linux#CommonInstallationProblems with https://www.tensorflow.org/install/install_linux#common_installation_problems since no this anchor defined actually;", "comments": ["Nagging Assignee @jhseu: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jhseu: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18347, "title": "Fixes to source_writer.cc.", "body": "- Fix memory leak in source_writer constructor.\r\n- FIx test data having .java extension causing issues with internal\r\n  linters. Changing to .txt extension.\r\n- Fix test data path not being correct internally.", "comments": ["I dont have really much experience with C++, so please make sure what Im doing makes sense :)", "Updated PR / Addressed comments. ty!"]}, {"number": 18346, "title": "Fix broken links in /extend/language_bindings ", "body": "This PR is to fix broken links in [TensorFlow in other languages](https://www.tensorflow.org/extend/language_bindings#graph_construction).\r\n\r\n- As we can see in above api guides as below, the [ `tensorflow/core/ops/ops.pbtxt` ] should link to a cite link of https://www.tensorflow.org/code/tensorflow/core/ops/ops.pbtxt according to the cite reference link in the [end of the doc](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/docs_src/extend/language_bindings.md) but it didn't.\r\n  The issue here is citation reference will not work on tensorflow.org when the citation text contains \"`\".\r\n![image](https://user-images.githubusercontent.com/1680977/38501893-9b14f882-3c40-11e8-9bc1-b814dc1ca1d2.png)\r\n- Remove the last two citation reference link since they are never referenced anywhere.", "comments": ["@jhseu it seems I cannot open the failed detailed log page with 404 error. Could you please kindly help have a look what's going wrong here?", "I can't imagine that it failed for any real reason. Merging."]}, {"number": 18345, "title": "Using Tensorflow with RNNs & batch normalisation", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Archlinux/Linux 4.14\r\n- **TensorFlow installed from**: source (will segfault before [7535f6b](https://github.com/tensorflow/tensorflow/commit/7535f6beb7ba95bf54e1513b0c2c51b844a7a49f))\r\n- **TensorFlow version**: b'unknown' 1.7.0 / latest master\r\n- **Python version**: 3.6\r\n- **Bazel version**: 0.11.1\r\n- **GCC/Compiler version**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.1/7.1.2\r\n- **GPU model and memory**: Titan XP/12GB\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\n\r\nWhen trying to run the source code below I get an error saying there's a cycle in my graph. This only seems to happen with XLA enabled and does not happen if I don't include the [additional required control dependency for batch normalisation](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization). Indeed this seems to occur whenever I use a dynamic RNN in combination with batch normalisation and XLA JIT support.\r\n\r\n#### Sample code to reproduce\r\n\r\n```Python\r\nimport tensorflow as tf                                                                                                                                                                                                                       \r\n\r\nwith tf.device('/cpu:0'):\r\n    xin = tf.placeholder(tf.float32, [None, 1, 1], name='input')\r\n    rnn_cell = tf.contrib.rnn.LSTMCell(1)\r\n    out, _ = tf.nn.dynamic_rnn(rnn_cell, xin, dtype=tf.float32)\r\n    out = tf.layers.batch_normalization(out, training=True)\r\n    out = tf.identity(out, name='output')\r\n\r\n    optimiser = tf.train.AdamOptimizer(.0001)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n        out = optimiser.minimize(out, global_step=tf.Variable(0, dtype=tf.float32), name='train_op')\r\n\r\nconfig = tf.ConfigProto(allow_soft_placement = False)\r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsample_in = [[[0]]]\r\nsess.run(out, feed_dict={xin: sample_in})\r\n```\r\n#### Output log\r\n```\r\n2018-04-03 13:09:24.326950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties:                                                                                                                          \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\r\n2018-04-03 13:09:24.326982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-04-03 13:09:24.512956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11366 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/thom/.python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"/home/thom/.python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/home/thom/.python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: Cycle detected when adding enter->frame edge: Edge from gradients/f_count to (null) would create a cycle.\r\n+-> (null)\r\n|   rnn/TensorArrayStack/TensorArrayGatherV3\r\n|   rnn/transpose_1\r\n|   batch_normalization/moments/mean\r\n|   batch_normalization/moments/Squeeze\r\n|   batch_normalization/AssignMovingAvg/sub\r\n|   batch_normalization/AssignMovingAvg/mul\r\n|   batch_normalization/AssignMovingAvg\r\n+-- gradients/f_count\r\n```\r\n\r\n### Workaround\r\n\r\nI seem to be able to workaround the issue by relying on [`tf.contrib.layers.batch_norm`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm) instead of [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) by setting the `updates_collections` parameter to `None` in order to force inlining of the update operation.\r\n\r\n#### Sample code with workaround\r\n\r\n```Python\r\nimport tensorflow as tf                                                                                                                                                                                                                       \r\n\r\nwith tf.device('/cpu:0'):\r\n    xin = tf.placeholder(tf.float32, [None, 1, 1], name='input')\r\n    rnn_cell = tf.contrib.rnn.LSTMCell(1)\r\n    out, _ = tf.nn.dynamic_rnn(rnn_cell, xin, dtype=tf.float32)\r\n    out = tf.contrib.layers.batch_norm(out, is_training=True, updates_collections=None)\r\n    out = tf.identity(out, name='output')\r\n\r\n    optimiser = tf.train.AdamOptimizer(.0001)\r\n    out = optimiser.minimize(out, global_step=tf.Variable(0, dtype=tf.float32), name='train_op')\r\n\r\nconfig = tf.ConfigProto(allow_soft_placement = False)\r\nsess = tf.Session(config=config)\r\nsess.run(tf.global_variables_initializer())\r\n\r\nsample_in = [[[0]]]\r\nsess.run(out, feed_dict={xin: sample_in})\r\n```\r\n\r\n### Additional information\r\n\r\n1. I have already submitted a question on [Stack Overflow](https://stackoverflow.com/questions/49630269/using-tensorflow-with-rnns-batch-normalisation) a week ago and have had no answers so far.\r\n2. Make sure you try and reproduce the issue on the tip of master or the sample will cause a segmentation fault.\r\n", "comments": ["Nagging Assignee @ebrevdo: It has been 150 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This bug is probably outdated.  I'll close for now -- I'm not able to replicate locally with a recent version of TF.  Reopen if it's still an issue."]}, {"number": 18344, "title": "[XLA] Disable int64 test for backends which don't support it", "body": "This test forces the data type to in64.  Not all platforms support int64, and this change disables the test for platforms which do not advertise int64 support.\r\n\r\n", "comments": []}]