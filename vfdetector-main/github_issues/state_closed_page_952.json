[{"number": 24873, "title": "tf.sparse.to_dense warns that you should use itself", "body": "\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/sparse/to_dense\r\n- Implemented here: https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/sparse_ops.py#L1115\r\n\r\n\r\n**Describe the documentation issue**\r\nHi,\r\nThe function [`tf.sparse.to_dense`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/sparse_ops.py#L1115), is a wrapper for `tf.sparse_to_dense`. However, [`tf.sparse_to_dense`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/ops/sparse_ops.py#L832) is deprecated and produces a warning that the user should use `tf.sparse.to_dense` instead.\r\n\r\nSo, the user uses the correct function `tf.sparse.to_dense` but still gets a warning that she should use it.\r\n\r\nExample code that produces that warning:\r\n`\r\ns = tf.SparseTensor(indices=[[1,1],[2,2],[3,3]], values=[4,5,6],dense_shape=[3,3])\r\ntf.sparse.to_dense(s)\r\n`\r\n\r\nresults in a warning:\r\nWARNING:tensorflow:From /home/urialon/.local/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\n\r\nSo I am not sure - is this the right usage, and just the warnings are wrong?\r\nThanks!", "comments": ["@urialon I think the issue has been fixed in #24037.", "Right, thanks!"]}, {"number": 24872, "title": "Unable to load native tensorflow", "body": " Mac OS X EI Captain.Python Anaconda 3.6, am getting the following error when i run \"from keras.models import Sequential\"\r\n\r\nUsing TensorFlow backend.\r\nImportError Traceback (most recent call last)\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in ()\r\n57\r\n---> 58 from tensorflow.python.pywrap_tensorflow_internal import *\r\n59 from tensorflow.python.pywrap_tensorflow_internal import version\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in ()\r\n27 return _mod\r\n---> 28 _pywrap_tensorflow_internal = swig_import_helper()\r\n29 del swig_import_helper\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n23 try:\r\n---> 24 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n25 finally:\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n241 else:\r\n--> 242 return load_dynamic(name, filename, file)\r\n243 elif type_ == PKG_DIRECTORY:\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n341 name=name, loader=loader, origin=path)\r\n--> 342 return _load(spec)\r\n343\r\n\r\nImportError: dlopen(/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\nReferenced from: /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\nExpected in: /usr/lib/libSystem.B.dylib\r\nin /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError Traceback (most recent call last)\r\nin ()\r\n----> 1 from keras.models import Sequential\r\n2 from keras.layers import Dense\r\n3 from keras.layers import LSTM\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/init.py in ()\r\n1 from future import absolute_import\r\n2\r\n----> 3 from . import utils\r\n4 from . import activations\r\n5 from . import applications\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/utils/init.py in ()\r\n4 from . import data_utils\r\n5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n7\r\n8 # Globally-importable utils.\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/utils/conv_utils.py in ()\r\n7 from six.moves import range\r\n8 import numpy as np\r\n----> 9 from .. import backend as K\r\n10\r\n11\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/backend/init.py in ()\r\n87 elif _BACKEND == 'tensorflow':\r\n88 sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89 from .tensorflow_backend import *\r\n90 else:\r\n91 # Try and load external backend.\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in ()\r\n3 from future import print_function\r\n4\r\n----> 5 import tensorflow as tf\r\n6 from tensorflow.python.framework import ops as tf_ops\r\n7 from tensorflow.python.training import moving_averages\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/init.py in ()\r\n22\r\n23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n25\r\n26 try:\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/init.py in ()\r\n47 import numpy as np\r\n48\r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n50\r\n51 from tensorflow.python.tools import component_api_helper\r\n\r\n/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in ()\r\n72 for some common reasons and solutions. Include the entire stack trace\r\n73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74 raise ImportError(msg)\r\n75\r\n76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\nFile \"/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in \r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in \r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"/Applications/Anaconda/anaconda/lib/python3.6/imp.py\", line 242, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"/Applications/Anaconda/anaconda/lib/python3.6/imp.py\", line 342, in load_dynamic\r\nreturn _load(spec)\r\nImportError: dlopen(/Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\nReferenced from: /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\nExpected in: /usr/lib/libSystem.B.dylib\r\nin /Applications/Anaconda/anaconda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["This error indicates that TF was installed incorrectly. Can you please provide following information? Thanks\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "TensorFlow 1.6 and above requires macOS 10.12.6 (Sierra) or later. That is a plausible reason for your build failure. You can try installing TensorFlow 1.5 for your current system configuration.\r\nThanks!", "I'm having the same issue trying to build Tensorflow 1.13.1 on 10.11.6 with bazel-0.18.1-darwin-x86_64.  Is there any known work-around?\r\n\r\n```\r\nERROR: /Users/davidlaxer/tensorflow/tensorflow/contrib/ignite/BUILD:148:1: Executing genrule //tensorflow/contrib/ignite:gen_igfs_ops_pygenrule failed (Trace/breakpoint trap): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\ndyld: lazy symbol binding failed: Symbol not found: _clock_gettime\r\n  Referenced from: /private/var/tmp/_bazel_davidlaxer/f9fe21ec5c09226e5ca0dce9376abe82/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ignite/../../../_solib_darwin_x86_64/_U_S_Stensorflow_Scontrib_Signite_Cgen_Ugen_Uigfs_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n\r\ndyld: Symbol not found: _clock_gettime\r\n  Referenced from: /private/var/tmp/_bazel_davidlaxer/f9fe21ec5c09226e5ca0dce9376abe82/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/contrib/ignite/../../../_solib_darwin_x86_64/_U_S_Stensorflow_Scontrib_Signite_Cgen_Ugen_Uigfs_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n\r\n/bin/bash: line 1: 74118 Trace/BPT trap: 5       bazel-out/host/bin/tensorflow/contrib/ignite/gen_gen_igfs_ops_py_wrappers_cc , '' 0 0 > bazel-out/darwin-opt/genfiles/tensorflow/contrib/ignite/python/ops/gen_igfs_ops.py\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 36273.121s, Critical Path: 8750.72s\r\nINFO: 4623 processes: 4623 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```"]}, {"number": 24871, "title": "Documentation on reducing the binary size of tflite library on iOS", "body": "**System information**\r\n- TensorFlow version: r1.12\r\n- Doc Link: https://www.tensorflow.org/lite/\r\n\r\n**Describe the documentation issue**\r\n\r\nThe tflite library seems to add around 2.5 MB to my iOS binary, which is far bigger than the 700 kB claimed in the documentation. Are there custom build options for tflite like there was with tfmobile that can help get this library size down? I haven't been able to find documentation on how to do this.\r\n\r\nFor context, I've tried using both the Pod and also used the build_ios_universal_lib.sh script, but the library size still seems to be around 2.5 MB.", "comments": ["Nevermind, after compiling with size optimization, this seems to be a non-issue.", "@lenaevans hi\uff0chow can you set the compiling with size optimization. I build it for android and it gets 3.6M.", "The biggest issue I found for both iOS and android is building for all architectures, which puts multiple copies of tensorflow in the app and thus inflates your size. If you build for specific architectures, you will probably see more reasonable app sizes. (See e.g. [this](https://proandroiddev.com/reducing-apk-size-by-using-abi-filters-and-apk-split-74a68a885f4e) for how to do that).", "@lenaevans  sorry \uff0ci did not make it clear. I mean the size of binary size of tflite library. The size of file  libtensorflowLite.so.", "Ohh hm, sorry can't really help there :(. I haven't tried to compile from source for android, the package in the package manager worked fine for me.", "@lenaevans Hi, i encounter same problem. I got a binary of libtensorflow-lite.a for arm64, it's around 7.5MB. Do you have any idea on how to reduce the size? Thanks in advance.", "I got the library from the build_ios_universal_lib.sh script, and then compiled in xcode with optimizations for space in the compiler settings. At first it looked a lot bigger than it actually was for me because I was looking at the size of the universal framework but once I looked at the size increases of the builds for the individual architectures, the size was a little over 1 MB.", "@lenaevans Hi, What did you mean by \"compiled in xcode with optimizations for space in the compiler settings\", you meant to set some settings in Xcode when compile project which use libtensorflow-lite.a  ?  if so, can you tell me what settings you specified ?  I'm a newbie in this area. Thanks a lot.", "Here's a diff of changes that worked for me to reduce the minimal binary size from ~2.9Mb to ~1.1Mb for Aarch64. Note that maybe it's not an optimal change and I can't guarantee it works for everyone. My binary was running happily just fine:\r\n\r\n```\r\ndiff --git a/tensorflow/lite/tools/make/Makefile b/tensorflow/lite/tools/make/Makefile\r\nindex 6d4b0c0ce3..f0c398e8a3 100644\r\n--- a/tensorflow/lite/tools/make/Makefile\r\n+++ b/tensorflow/lite/tools/make/Makefile\r\n@@ -55,11 +55,12 @@ LIBS := \\\r\n # There are no rules for compiling objects for the host system (since we don't\r\n # generate things like the protobuf compiler that require that), so all of\r\n # these settings are for the target compiler.\r\n-CXXFLAGS := -O3 -DNDEBUG -fPIC\r\n+CXXFLAGS := -Os -DNDEBUG -fPIC -ffunction-sections -fdata-sections -s\r\n CXXFLAGS += $(EXTRA_CXXFLAGS)\r\n CFLAGS := ${CXXFLAGS}\r\n CXXFLAGS += --std=c++11\r\n LDOPTS := -L/usr/local/lib\r\n+LDFLAGS += -Wl, --gc-sections\r\n ARFLAGS := -r\r\n TARGET_TOOLCHAIN_PREFIX :=\r\n CC_PREFIX :=\r\n```"]}, {"number": 24870, "title": "tensorflow installed but cannot be imported", "body": "**System information**\r\n- Microsoft Windows 10 Home Single Language\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.12.0 but also tried bit 1.5.0 and 1.11.0\r\n- Python version: Python 3.6.8 and also tried 3.5.2\r\n- Installed using virtualenv? pip? conda?: tried using pip and conda\r\n- CUDA/cuDNN version: cannot find a cuda or cudnn file\r\n- GPU model and memory: Nvidia geforce 840 - Total memory 6071 MB of which 2010MB is Video RAM\r\n\r\nThe installation of tensorflow using the `pip install tensorflow` command was successful but when I test the command `import tensorflow as tf` with `cmd.exe`, I get the following result:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Bruno\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\nModuleNotFoundError: No module named 'tensorflow.python'\r\n```\r\nI tried so many things that i am quite desperate. Can someone please suggest a solution?", "comments": ["I finally managed to install Tensorflow following the tutorial:\r\nhttps://www.youtube.com/watch?v=jaSD7Lfp_7M\r\n\r\nHope it can help you too.", "@prachm  I see that you are trying to install TF 2.0 preview version. Since this is a different issue than the original one, Can you please post a new issue describing your problem? Thanks!\r\nI will close this issue since it's resolved for the author. Thanks!"]}, {"number": 24868, "title": "TF tutorial for Custom Layers seems not work for multiple GPUs case", "body": "In the tutorial [Custom layers](https://www.tensorflow.org/tutorials/eager/custom_layers), it says to extend the `tf.keras.Layer` class. However, Keras layer doesn't have `_reuse` attribute, and it doesn't work for multiple GPUs case. I check that in the `tf.layers.conv` definition script, it inherits from two classes: \r\n\r\n```\r\n@tf_export('layers.Conv1D')\r\nclass Conv1D(keras_layers.Conv1D, base.Layer):\r\n```\r\n\r\nThe second class provides support for variable reuse.", "comments": []}, {"number": 24867, "title": "fix logic adding generated API directory to tensorflow.__path__", "body": "This reverts part of PR #24247 that broke logic in the v1 `__init__.py` template that sets up `tensorflow.__path` so that generated API modules can be imported directly.\r\n\r\nThe breakage can be reproduced in current `tf-nightly` by just doing:\r\n\r\n```\r\n>>> import tensorflow\r\n>>> tensorflow.compat\r\n<module 'tensorflow._api.v1.compat' from '/.../tensorflow/_api/v1/compat/__init__.pyc'>\r\n>>> import tensorflow.compat\r\n>>> tensorflow.compat\r\n<module 'tensorflow.compat' from '/.../tensorflow/python/compat/__init__.pyc'>\r\n```\r\nThe latter version of `tensorflow.compat` only contains utility functions and lacks any `v1` submodule, which is only in `_api/v1/compat/v1`, so this change manifests as `import tensorflow.compat.v1` failing with \"No module named v1\".  This is a particularly obvious failure, but it actually affects any top-level module that also exists under tensorflow/python, it's just that only a few of them are noticeably different.\r\n\r\nThe problem in the original change was that re-importing app via `from tensorflow.python.platform import app` actually changed which `app` module file was in use, and `app` was being used as an arbitrarily chosen module in logic below that adjusts the `__path__` attribute of the `tensorflow` to include the generated API directory, as determined by using `dirname(dirname(app.__file__))`.\r\n\r\nThe logic for this should be made more robust so it's not affected by seemingly unrelated changes like this, but for now I'm just reverting it (with an intermediate variable to make the usage more obvious) to get tf-nightly unbroken.", "comments": []}, {"number": 24866, "title": "Gradient descent in the body of while loop (v2) causes error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6, TensorFlow r1.12\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0-rc0\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): 10.0.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen using `tf.while_loop` (v1), we are able to call gradients in the body of the while loop. In other words, we were able to embed the training into our TensorFlow graph.\r\n\r\nIf we replace `tf.while_loop` with `while_v2.while_loop` then we end up getting an error:\r\n\r\n`In op 'train/update_W/ApplyGradientDescent', input types ([tf.float32, tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32, tf.float32])`\r\n\r\nSince this shows that gradient descent is being fed a list of length 3, but the placeholders have a length of 2, I think it might be something around the counter getting added. However, I'm not certain and any insights you guys have would be useful!\r\n\r\n**Describe the expected behavior**\r\nWe expect to get the same results when we use v1 or v2. We know that you guys are working on v2 of the while loops, and we're really excited for it. We just wanted to make certain this was on your radar as a use case, since we didn't see it in the RFC \ud83d\ude04 \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\ndef body(loopVar):\r\n    y = x * W\r\n    loss = tf.reduce_mean((y - y_)**2)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n    train_op = optimizer.minimize(loss, name='train')\r\n\r\n    with tf.control_dependencies([train_op]):\r\n        return loopVar + 1\r\n\r\nx = tf.placeholder(tf.float32, name='input')\r\ny_ = tf.placeholder(tf.float32, name='target')\r\nW = tf.Variable(5., name='W')\r\n\r\ncounter = tf.Variable(0, name='counter')\r\nz = while_v2.while_loop(lambda v: v < 6, body, [counter])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(W))\r\n    print(sess.run(z, feed_dict={x: [1.0, 2.0], y_:[2.0, 4.0]}))\r\n    print(sess.run(W))\r\n```\r\n\r\n(If we replace `while_v2.while_loop` with `tf.while_loop` then we get the expected results of seeing the value of W move towards 2)\r\n\r\n**Other info / logs**\r\n", "comments": ["while_v2 currently works only with ResourcesVariables, which aren't enabled by default in TF yet. Try calling `tf.enable_resource_variables()` at the beginning of your code or passing `use_resource=True` to the `tf.Variable` constructor. Please see https://stackoverflow.com/questions/40817665/whats-the-difference-between-variable-and-resourcevariable-in-tensorflow for more information on ResourceVariables.", "It worked! Thanks for your help! "]}, {"number": 24865, "title": "[TF2.0] Custom variables", "body": "Hello all,\r\n\r\nI'm testing new features of TF2.0. Especially, I'm interested in creating custom variables. According to [Variables RFC](https://github.com/tensorflow/community/blob/master/rfcs/20180817-variables-20.md) description, it must be easy to do:\r\n> tf.Variable will become an abstract base class with a well-defined interface and `a scoped factory to construct instances: 1. Users will be able to implement their own variable-like objects by subclassing tf.Variable and adding a scoped factory function to use those variables, 2. ...\r\n\r\nBut, I'm getting errors and because of lack of the documentation, I can not figure out either I do something wrong or this feature doesn't work at the moment.\r\n\r\nI'm using tensorflow from pip `tf-nightly-2.0-preview`.\r\n\r\nversion 1:\r\n```python\r\nIn [74]: class Variable(metaclass=tf.Variable): pass\r\nIn [75]: v = Variable(1.0)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-75-5f9f01fedcf6> in <module>\r\n----> 1 v = Variable(1.0)\r\n\r\nTypeError: 'ResourceVariable' object is not callable\r\n```\r\n\r\n\r\nversion 2:\r\n```python\r\nIn [78]: class Variable(tf.Variable):\r\n    ...:     pass\r\n    ...:\r\n\r\nIn [79]: v = Variable(10.)\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-79-9441da3b11b5> in <module>\r\n----> 1 v = Variable(10.)\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    200       return cls._variable_v2_call(*args, **kwargs)\r\n    201     else:\r\n--> 202       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    203\r\n    204\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __init__(self, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation)\r\n    395       RuntimeError: If eager execution is enabled.\r\n    396     \"\"\"\r\n--> 397     raise NotImplementedError\r\n    398\r\n    399   def __repr__(self):\r\n\r\nNotImplementedError:\r\n```\r\n\r\nI would be grateful for the clarity about the status of this RFC part and, in case it is implemented, some guidance how it should be done.\r\n\r\nThanks!", "comments": ["We don't have an interface for inheriting from Variable yet (the RFC isn't as fully implemented as it seems). There'll probably be another RFC for the inherit-from-variable interface; it is tricky as we need to maintain compatibility with checkpointing and distribution strategies and parameter servers and functions and other things which want to control variable creation and initialization.\r\n", "@awav I think we should close this? Or do you want to attach a cleaned-up version of the second alternative you have here?", "Closing, and removing from the project tracker.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24865\">No</a>\n"]}, {"number": 24864, "title": "Add support of collective ops for GDR", "body": "Fixes #18232.\r\n\r\nSigned-off-by: Bairen Yi <byronyi@clustar.ai>", "comments": ["@dubey Could you help to review? Thanks!", "Thanks for the contribution, @byronyi!  This is on my to-do list for this week; let me know if you need a faster response.", "@ymodak is testing passed for this? If so, let me know if you are ready to pull."]}, {"number": 24863, "title": "AttributeError: 'MirroredStrategy' object has no attribute 'read_var'", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: 10.0/7.3 (drivers 410.72)\r\n- GPU model and memory: Gtx 1080Ti 11178 MiB\r\n\r\n\r\n**Describe the current behavior**\r\n`AttributeError: 'MirroredStrategy' object has no attribute 'read_var'` when trying to use MirroredStrategy.\r\n\r\n**Describe the expected behavior**\r\nNo error. I just compiled the new master branch, and it was working before.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n n_gpus = 2\r\n strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=n_gpus) if n_gpus > 1 else None\r\n run_config = tf.estimator.RunConfig(\r\n     model_dir=model_dir,\r\n      save_checkpoints_steps=save_steps,\r\n      save_summary_steps=summary_steps,\r\n      train_distribute=strategy,\r\n      keep_checkpoint_max=None)\r\n#...  Use this RunConfig in EstimatorSpec ...\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\r\n  return f(*args, **kwds)\r\n2019-01-11 18:35:33.974554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-01-11 18:35:34.097498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.42GiB\r\n2019-01-11 18:35:34.098592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0, 1\r\n2019-01-11 18:35:34.099646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-11 18:35:34.099665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 1 \r\n2019-01-11 18:35:34.099694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N Y \r\n2019-01-11 18:35:34.099743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 1:   Y N \r\n2019-01-11 18:35:34.100040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:0 with 10468 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2019-01-11 18:35:34.100319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:1 with 10138 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2019-01-11 18:35:34.101973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0, 1\r\n2019-01-11 18:35:34.102046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-11 18:35:34.102073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 1 \r\n2019-01-11 18:35:34.102087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N Y \r\n2019-01-11 18:35:34.102132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 1:   Y N \r\n2019-01-11 18:35:34.102368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:0 with 10468 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2019-01-11 18:35:34.102507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/device:GPU:1 with 10138 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2019-01-11 18:35:34.180174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0, 1\r\n2019-01-11 18:35:34.180243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-11 18:35:34.180256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 1 \r\n2019-01-11 18:35:34.180265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N Y \r\n2019-01-11 18:35:34.180274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 1:   Y N \r\n2019-01-11 18:35:34.180495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10468 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2019-01-11 18:35:34.180773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10138 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n\r\n\"siknet.py\" in <module>\r\n  585:  tf.app.run()\r\n  ./siknet.py\r\n\"app.py\" in run\r\n  125:  _sys.exit(main(argv))\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\r\n\"siknet.py\" in main\r\n  578:  sync_replicas=FLAGS.sync_replicas\r\n  ./siknet.py\r\n\"utils.py\" in train_and_eval\r\n  681:  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  /home/rpng/github/siknet/utils.py\r\n\"training.py\" in train_and_evaluate\r\n  471:  return executor.run()\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/training.py\r\n\"training.py\" in run\r\n  610:  return self.run_local()\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/training.py\r\n\"training.py\" in run_local\r\n  711:  saving_listeners=saving_listeners)\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/training.py\r\n\"estimator.py\" in train\r\n  354:  loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\r\n\"estimator.py\" in _train_model\r\n 1181:  return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\r\n\"estimator.py\" in _train_model_distributed\r\n 1256:  self._train_distribution.read_var(global_step_tensor))\r\n  /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\r\nAttributeError: 'MirroredStrategy' object has no attribute 'read_var'\r\n```\r\n", "comments": ["Looks like you are using an older version of the tensorflow_estimator package. At head, that line has been updated to use \"strategy.extended.read_var\" which should work: https://github.com/tensorflow/estimator/blob/5875cd0acd21887c1eabd66d7ddfcd7af85a7618/tensorflow_estimator/python/estimator/estimator.py#L1213"]}, {"number": 24862, "title": "r1.13-rc0 cherry-pick request: fix races in TF-TRT integration", "body": "This fixes two data races and one conversion bug in TF-TRT integration:\r\n\r\n1. during INT8 calibration, when two sess.run()s fetch two different sets of ops and both of them trigger the computation of the TRTEngineOp, two different TRTEngineOp will be created, and dev_tensors_ will only be allocated once by the first op by the device's resource manager via LookupOrCreate(). As a result, the second TRTEngineOp will have a empty dev_tensors_ vector which will result in std::out_of_range error when the op tries to access it later.\r\n2. IExecutionContext::enqueu() is not thread-safe which cause one of the issues mentioned in\r\nhttps://github.com/tensorflow/tensorflow/issues/23853. This PR added a mutex to guard that call.\r\n3. IConstantLayer doesn't set the output data type automatically but defaults to FP32, this cause conversion of models that contain any compatible INT32 binary ops to fail. This PR fix that by explicitly setting the output datatype.", "comments": ["@pooyadavoodi @trevor-m Just FYI, please help to take a look.", "@aselle Hi Andrew, I just double checked, the CI build failure is irrelevant, please let me know if there are any questions. Thanks.", "@aselle I just double checked again, all the failures of required checks are not caused by this PR, just FYI.", "Thanks @aselle for the merge!"]}, {"number": 24861, "title": "r1.13-rc0 cherry-pick request: Fix --config=mkl performance regression", "body": "This fixes a ~4x slowdown (compared to r1.12) observed when running [DeepVariant](https://github.com/google/deepvariant) on cloud. If not fixed, we could see similarly huge slowdowns in other TF-MKL runs with models that have Conv2D following directly by FusedBatchNorm (and also Conv2D followed by Squeeze and BiasAdd).\r\n\r\nDetails:\r\nhttps://github.com/tensorflow/tensorflow/commit/ffde93148d82093c9cd8703e8d7d81bff67a1dc6 makes Grappler generate some _FusedConv2D nodes that doesn't have corresponding TF-MKL nodes. For example, MKL graph rewriter doesn't support _FusedConv2D with BatchNorm yet so any Conv2D that gets fused with BatchNorm will miss using MKL-DNN's convolutions, causing a big performance slowdown compared to r1.12.", "comments": []}, {"number": 24860, "title": "Dynload", "body": "Unify `tensorflow` and `tensorflow_gpu` into single package\r\nSee #22562 and #16184\r\nAs a person who started all that (original patch [here](https://gist.github.com/pkit/e06c2d23046a265a4f0fea302a5ce539)) I will take it to the finish line, hopefully.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@pkit can you please sign CLA ", "@rthadur already signed, you cannot submit PR if not signed...\r\n\"The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter\"\r\nI.e. the bot is not smart enough to understand that @superbobry is another author of a connected PR.", "@pkit thanks for doing this! Have you looked into the remaining cudart dependencies? I am still not sure if these can be safely removed.", "@superbobry yep, looked into it, doing it right now.\r\nWe have some cublas and cusolver leftovers in kernel solvers. Treating these first.\r\nMost of the cudart references are a red herring, essentially there are only 3 major places where it's used and mostly for memory management. I will isolate all the cudart MM into a wrapper object (similar to cupti wrapper) and point all the core/platform stuff there.\r\nThere seems to be some contrib stuff doing direct cuda MM, but these are easily treated once core wrapper is set up.", "Oh, yeah!\r\n```\r\n$ nm -C -u ./tensorflow-1.12.0.data/purelib/tensorflow/python/_pywrap_tensorflow_internal.so | grep '@@libcu' \r\n$\r\n```", "@yifeif please take a look at this PR.\r\n@yifeif has a few changes in flight internally, there may be some conflicts in the next few days.", "Thanks @pkit and @gunan. I have a pending change that does something similar to this PR, but also handles a few things so that it would work internally as well. Will drop a note once that is submitted.", "@yifeif 6 days passed, should I continue and rebase, or you will handle it?\r\nI will close the PR then.", "hi @pkit, the latest change that got pushed should remove cuda driver dynamic linking. I'm working on removing cuda runtime. ", "@yifeif I can read the code, thanks :)\r\nIf you're going to remove `libcudart`, `libcusolver` and last bits of `libcublas` from everywhere then probably this PR has no meaning. And needs to be closed. ", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "Nagging Reviewer @Artem-B, @yifeif, @timshen91: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Is this patch still relevant?", "@yifeif Do we still need this patch, or should we close the PR?", "All the explicit dependency of cuda has been removed from core. @pkit I'll close this PR. If you are looking into removing the rest, let's open discuss in a new thread/PR. @gunan is also looking into dynamically load the kernels, so we might not need to wrap kernel's cuda usage.", "@yifeif originally I needed only the `libcuda` out. Any additional things were just \"nice to have\", to make the deploy easier. So I'm good for now."]}, {"number": 24859, "title": "[TensorFlow Java] adding updateEdge function", "body": "This PR adds an updateEdge function for Java, addressing issue [#24231](https://github.com/tensorflow/tensorflow/issues/24231). \r\n\r\nNote: the TF_UpdateEdge function added to c_api.cc is from python_api.cc. \r\n\r\nCo-authored-by: Samantha Andow <samdow@fb.com>\r\nCo-authored-by: Irene Dea <irenedea@fb.com>", "comments": ["Thanks for the PR, but I do have some concerns/questions. Let's chat about that in the issue.", "Closing as per discussion in #24231 "]}, {"number": 24858, "title": "[Java] Graph environment decoupling in preparation of eager execution", "body": "This PR is the first of a series of pull requests that enables eager execution in the TensorFlow Java client. It might look ambitious at first but is pretty basic when understanding what is coming next.\r\n\r\nThis PR decouples the actual implementation for building operations, which is tightly bound to graph execution, from its public interface. Doing this in a single PR helps to get a better picture of the targeted architecture and discuss about it if needed. \r\n\r\nSo in brief:\r\n\r\n* `Operation` concrete class became a `GraphOperation` that implements the `Operation` interface by extending from the `AbstractOperation` abstract class\r\n* `OperationBuilder` concrete class became a `GraphOperationBuilder` that implements the `OperationBuilder` interface\r\n* `Graph` implements the `ExecutionEnvironment` interface to be used by the `Ops` classes\r\n\r\nOnce we are happy with this solution, we can start adding `Operation`, `OperationBuilder` and `ExecutionEnvironment` implementations for eager execution, which are pretty straightforward according to my previous POC.\r\n\r\nI tried to preserve backward compatibility as much as possible. I ended up with the following exceptions:\r\n\r\n* Constructor in `Output` is now package-private and accepts only `AbstractOperation` instances\r\n** Note: I could have play around to keep it public but for me, it sounds wrong to instantiate an `Output` directly instead of calling the available type-safe `Operation.output` method\r\n\r\n* `Scope.graph()` has been changed to `Scope.env()` to return an instance of `ExecutionEnvironment` instead instead of a `Graph`\r\n** Note: This can only impacts clients already using the `Ops` API and even there, unless they did something tricky, the `Ops` classes will take care of the change flawlessly.\r\n\r\nSee diagram below for an overview of the targeted architecture:\r\n\r\n![eager-diagrams](https://user-images.githubusercontent.com/10109534/54080672-2cd93880-42c3-11e9-992b-6a94db926cdc.jpg)\r\n", "comments": ["FYI, just applied a small change to this PR", "Nagging Reviewer @asimshankar: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied.", "I think this should be assigned to @sjamesr instead.", "Thanks for approving the PR, @sjamesr \r\n\r\nSince it was created a while ago, I had to rebase the code on current branch to get it compile, you might need to reapprove.\r\n\r\nAlso, I took that opportunity to rename `GraphNode*` classes to `GraphOperation*` for consistency with the upcoming eager implementation (i.e. `EagerOperation*`), it was bugging me a bit. I updated the diagram above accordingly, sorry for the trouble.", "Friendly reminder @sjamesr to please re-approve this PR,\r\n\r\nSince it is a XL pull request, I'm just afraid that I'll need to rebase it more than once if we wait too long between approvals. Thanks!"]}, {"number": 24857, "title": "Getting wrong predictions after saving and reloading an Estimator", "body": "I am trying to save an `Estimator` and then load it to predict as required. Part where I train the model:\r\n\r\n    classifier = tf.estimator.Estimator(model_fn=bag_of_words_model)\r\n    \r\n    # Train\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"words\": x_train},  # x_train is 2D numpy array of shape (26, 5)\r\n        y=y_train,                   # y_train is 1D panda series of length 26\r\n        batch_size=1000,\r\n        num_epochs=None,\r\n        shuffle=True)\r\n    \r\n    classifier.train(input_fn=train_input_fn, steps=300)\r\n\r\nI then save the model as follows:\r\n\r\n    def serving_input_receiver_fn():\r\n        serialized_tf_example = tf.placeholder(dtype=tf.int64, shape=(None, 5), name='words')\r\n        receiver_tensors = {\"predictor_inputs\": serialized_tf_example}\r\n        features = {\"words\": tf.tile(serialized_tf_example, multiples=[1, 1])}\r\n        return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n    \r\n    full_model_dir = classifier.export_savedmodel(export_dir_base=\"E:/models/\",\r\n                                                  serving_input_receiver_fn=serving_input_receiver_fn)\r\n\r\nI now load the model and give the test set to it for prediction:\r\n\r\n    from tensorflow.contrib import predictor\r\n    \r\n    classifier = predictor.from_saved_model(\"E:\\\\models\\\\1547122667\")\r\n    predictions = classifier({'predictor_inputs': x_test})\r\n    print(predictions)\r\n\r\nThis gives me predictions like:\r\n\r\n    {'class': array([ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0, 15,  0,\r\n            0,  5,  0, 20,  0,  5,  0,  0,  0], dtype=int64),\r\n    'prob': array([[9.9397606e-01, 6.5355714e-05, 2.2225287e-05, ..., 1.4510043e-07,\r\n                1.6920333e-07, 1.4865007e-07],\r\n               [9.9886864e-01, 1.4976941e-06, 7.0847680e-05, ..., 9.4182191e-08,\r\n                1.1828639e-07, 9.5683227e-08],\r\n               [9.9884748e-01, 2.1105163e-06, 1.1994909e-05, ..., 8.3957858e-08,\r\n                1.0476184e-07, 8.5592234e-08],\r\n               ...,\r\n               [9.6145850e-01, 6.9048328e-05, 1.1446012e-04, ..., 7.3761731e-07,\r\n                8.8173107e-07, 7.3824998e-07],\r\n               [9.7115618e-01, 2.9716679e-05, 5.9592247e-05, ..., 2.8933655e-07,\r\n                3.4183532e-07, 2.9737942e-07],\r\n               [9.7387028e-01, 6.9163914e-05, 1.5800977e-04, ..., 1.6116818e-06,\r\n                1.9025001e-06, 1.5990496e-06]], dtype=float32)}\r\n\r\n`class` and `prob` are two things that I am predicting. Now, if I predict the output with the same test set without saving and loading the model:\r\n\r\n    # Predict.\r\n    test_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"words\": x_test}, y=y_test, num_epochs=1, shuffle=False)\r\n    predictions = classifier.predict(input_fn=test_input_fn)\r\n    print(predictions)\r\n\r\nthen I get the output as follows:\r\n\r\n    {'class': 0, 'prob': array([9.9023646e-01, 2.6038184e-05, 3.9950578e-06, ..., 1.3950405e-08,\r\n           1.5713249e-08, 1.3064114e-08], dtype=float32)}\r\n    {'class': 1, 'prob': array([2.0078469e-05, 9.9907070e-01, 8.9245419e-05, ..., 6.6533559e-08,\r\n           7.1365662e-08, 6.8764685e-08], dtype=float32)}\r\n    {'class': 2, 'prob': array([3.0828053e-06, 9.6484597e-05, 9.9906868e-01, ..., 5.9190391e-08,\r\n           6.0995028e-08, 6.2322023e-08], dtype=float32)}\r\n    {'class': 3, 'prob': array([7.4923842e-06, 1.1112734e-06, 1.1697492e-06, ..., 4.4295877e-08,\r\n           4.4563325e-08, 4.0475427e-08], dtype=float32)}\r\n    {'class': 4, 'prob': array([4.6085161e-03, 2.8403942e-05, 2.0638861e-05, ..., 7.6083229e-09,\r\n           8.5255349e-09, 6.7836012e-09], dtype=float32)}\r\n    {'class': 5, 'prob': array([6.2119620e-06, 7.2357750e-07, 2.6231232e-06, ..., 7.4999367e-09,\r\n           9.0847436e-09, 7.5630142e-09], dtype=float32)}\r\n    {'class': 6, 'prob': array([4.4882968e-06, 2.2007227e-06, 8.3352124e-06, ..., 2.3130213e-09,\r\n           2.3657243e-09, 2.0045692e-09], dtype=float32)}\r\n    {'class': 7, 'prob': array([1.88617545e-04, 9.01482690e-06, 1.47353385e-05, ...,\r\n           3.38567552e-09, 3.97709154e-09, 3.37017392e-09], dtype=float32)}\r\n    {'class': 8, 'prob': array([1.9843496e-06, 4.5909755e-06, 4.8804057e-05, ..., 2.2636470e-08,\r\n           2.0094852e-08, 2.0215294e-08], dtype=float32)}\r\n    {'class': 9, 'prob': array([2.5907659e-04, 4.4661370e-05, 6.9490757e-06, ..., 1.6249915e-08,\r\n           1.7579131e-08, 1.5439820e-08], dtype=float32)}\r\n    {'class': 10, 'prob': array([3.6456138e-05, 7.5861579e-05, 3.0208937e-05, ..., 2.7859956e-08,\r\n           2.5423596e-08, 2.8662368e-08], dtype=float32)}\r\n    {'class': 11, 'prob': array([1.1723863e-05, 9.1407037e-06, 4.8835855e-04, ..., 2.3693143e-08,\r\n           2.0524153e-08, 2.3223269e-08], dtype=float32)}\r\n    {'class': 12, 'prob': array([1.2886175e-06, 2.6652628e-05, 2.7812246e-06, ..., 4.8295210e-08,\r\n           4.4282604e-08, 4.7342766e-08], dtype=float32)}\r\n    {'class': 13, 'prob': array([3.3486103e-05, 1.3361238e-05, 3.6493871e-05, ..., 2.2195401e-09,\r\n           2.4768412e-09, 2.0150714e-09], dtype=float32)}\r\n    {'class': 14, 'prob': array([4.6108948e-05, 3.0377207e-05, 2.0945006e-06, ..., 4.2276231e-08,\r\n           5.2376720e-08, 4.4969173e-08], dtype=float32)}\r\n    {'class': 15, 'prob': array([1.7165689e-04, 2.9350400e-05, 3.2283624e-05, ..., 7.1849078e-09,\r\n           7.6871531e-09, 6.6224697e-09], dtype=float32)}\r\n    {'class': 16, 'prob': array([5.9876328e-07, 3.0931276e-06, 1.5760432e-05, ..., 4.0450086e-08,\r\n           4.2720632e-08, 4.6017195e-08], dtype=float32)}\r\n    {'class': 17, 'prob': array([2.6658317e-04, 9.9656281e-05, 4.0355867e-06, ..., 1.2873563e-08,\r\n           1.4808875e-08, 1.2155732e-08], dtype=float32)}\r\n    {'class': 18, 'prob': array([1.4914459e-04, 2.1025437e-06, 1.2505146e-05, ..., 9.8899635e-09,\r\n           1.1115599e-08, 8.9312255e-09], dtype=float32)}\r\n    {'class': 19, 'prob': array([2.5615416e-04, 2.3750392e-05, 2.2886352e-04, ..., 3.9635733e-08,\r\n           4.5139984e-08, 3.8605780e-08], dtype=float32)}\r\n    {'class': 20, 'prob': array([6.3949975e-04, 2.3652929e-05, 7.8577641e-06, ..., 2.0959168e-09,\r\n           2.5495863e-09, 2.0428985e-09], dtype=float32)}\r\n    {'class': 21, 'prob': array([8.2179489e-05, 8.4409467e-06, 5.4756888e-06, ..., 2.2360982e-09,\r\n           2.4820561e-09, 2.1206517e-09], dtype=float32)}\r\n    {'class': 22, 'prob': array([3.9681905e-05, 2.4394642e-06, 8.9102805e-06, ..., 2.0282410e-08,\r\n           2.1132811e-08, 1.8368105e-08], dtype=float32)}\r\n    {'class': 23, 'prob': array([3.0794261e-05, 6.5104805e-06, 3.3528936e-06, ..., 2.0360846e-09,\r\n           1.9360573e-09, 1.7195430e-09], dtype=float32)}\r\n    {'class': 24, 'prob': array([3.4596618e-05, 2.2907707e-06, 2.5318438e-06, ..., 1.1038886e-08,\r\n           1.2148775e-08, 9.9556408e-09], dtype=float32)}\r\n    {'class': 25, 'prob': array([1.4846727e-03, 1.9189476e-06, 5.3232620e-06, ..., 3.1966723e-09,\r\n           3.5612517e-09, 3.0947123e-09], dtype=float32)}\r\n\r\nwhich is correct. Notice the difference between two outputs is that the `class` in the second one is increasing 1 by 1 while the `class` in the first case shows 0s at most places.\r\n\r\nWhy is there a difference in the prediction? Am I saving the model in a wrong way?\r\n\r\n\r\n@ispirmustafa @random-forests @DynamicWebPaige", "comments": ["I have also asked a question on stackover https://stackoverflow.com/questions/54129098/getting-wrong-prediction-after-loading-a-saved-model", "Just to remove one of the variables here, can you test with the [SavedModelEstimator](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/SavedModelEstimator) as well? That will remove the Predictor element and at least point to the SavedModel or Estimator predict as the faulty party here.", "I tried it but I got same results.\r\n\r\n```\r\nfrom tensorflow.contrib.estimator import SavedModelEstimator\r\nclassifier = SavedModelEstimator(\"E:\\\\models\\\\1547539334\")\r\ntest_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={WORDS_FEATURE: x_test}, y=y_test, num_epochs=1, shuffle=False)\r\npredictions = classifier.predict(input_fn=test_input_fn)\r\nfor p in predictions:\r\n    print(p)\r\n```\r\n\r\nThis gives:\r\n\r\n```\r\n{'class': 0, 'prob': array([9.9717832e-01, 8.5786020e-04, 6.4049636e-05, ..., 8.6544226e-08,\r\n       8.6821146e-08, 8.1394326e-08], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9960703e-01, 2.0104484e-05, 1.6036544e-04, ..., 3.1367048e-08,\r\n       3.2443833e-08, 3.0766806e-08], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9885404e-01, 7.5795266e-05, 6.7050540e-05, ..., 1.3488129e-07,\r\n       1.4307327e-07, 1.3248858e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9899799e-01, 6.4979497e-05, 4.9802653e-05, ..., 4.6777117e-08,\r\n       4.7913996e-08, 4.3893024e-08], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8838729e-01, 3.7744928e-03, 2.5661272e-04, ..., 7.3713346e-07,\r\n       7.7911784e-07, 7.5461628e-07], dtype=float32)}\r\n{'class': 5, 'prob': array([1.2085142e-05, 1.5967547e-05, 1.6229365e-05, ..., 1.8335292e-07,\r\n       2.0694857e-07, 2.1337026e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8932624e-01, 3.0042315e-04, 3.4193668e-04, ..., 1.1369635e-06,\r\n       1.2429477e-06, 1.1229140e-06], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8753148e-01, 1.1704578e-03, 4.1641024e-04, ..., 4.1676759e-07,\r\n       4.4668820e-07, 3.9701357e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8414856e-01, 4.8066137e-04, 4.8338709e-04, ..., 8.6252527e-07,\r\n       8.3684807e-07, 7.9297627e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8127502e-01, 4.0246081e-04, 9.1002218e-04, ..., 9.3697497e-07,\r\n       9.7139502e-07, 9.0600975e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9923956e-01, 4.5225835e-05, 3.3328299e-05, ..., 6.3075376e-08,\r\n       6.8231451e-08, 6.2429535e-08], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9892765e-01, 6.2995081e-05, 8.0713442e-05, ..., 8.5985938e-08,\r\n       9.5674274e-08, 8.6504272e-08], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9878377e-01, 1.2373565e-04, 5.3474701e-05, ..., 1.2326316e-07,\r\n       1.2942408e-07, 1.1582518e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8602110e-01, 5.0440140e-04, 3.7236928e-04, ..., 1.1675936e-06,\r\n       1.2900981e-06, 1.1921265e-06], dtype=float32)}\r\n{'class': 0, 'prob': array([9.99113023e-01, 1.20989556e-04, 4.73294276e-05, ...,\r\n       6.17745854e-08, 6.85775419e-08, 6.50879883e-08], dtype=float32)}\r\n{'class': 15, 'prob': array([7.3899527e-04, 7.2398130e-03, 1.9108003e-03, ..., 1.9992517e-07,\r\n       2.1110104e-07, 2.0947941e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9912304e-01, 4.3242489e-05, 3.3706045e-05, ..., 5.1705765e-08,\r\n       5.6073219e-08, 4.9725138e-08], dtype=float32)}\r\n{'class': 0, 'prob': array([9.7001451e-01, 5.3095614e-04, 2.6374252e-03, ..., 2.1944559e-06,\r\n       2.3350731e-06, 2.0890418e-06], dtype=float32)}\r\n{'class': 18, 'prob': array([7.0199365e-04, 6.0002081e-04, 7.1425311e-04, ..., 3.8498611e-06,\r\n       4.1461471e-06, 3.8720273e-06], dtype=float32)}\r\n{'class': 0, 'prob': array([9.9898738e-01, 4.7222173e-05, 7.6803917e-05, ..., 8.8532246e-08,\r\n       9.0705051e-08, 8.4998817e-08], dtype=float32)}\r\n{'class': 18, 'prob': array([6.7510635e-02, 4.0915180e-03, 6.6295490e-03, ..., 1.0756982e-05,\r\n       1.0628254e-05, 1.0272101e-05], dtype=float32)}\r\n{'class': 0, 'prob': array([9.7325921e-01, 9.5799292e-04, 9.8795001e-04, ..., 1.9478718e-07,\r\n       2.0265945e-07, 1.8819739e-07], dtype=float32)}\r\n{'class': 5, 'prob': array([5.7400834e-02, 7.8264829e-03, 9.8085916e-03, ..., 3.3320775e-05,\r\n       3.6073146e-05, 3.5094916e-05], dtype=float32)}\r\n{'class': 0, 'prob': array([9.7876966e-01, 1.3300353e-03, 1.1287389e-03, ..., 6.9847374e-07,\r\n       7.3241597e-07, 6.7505181e-07], dtype=float32)}\r\n{'class': 0, 'prob': array([9.7674805e-01, 6.3739752e-04, 1.0363439e-03, ..., 1.0288414e-06,\r\n       1.0741144e-06, 1.0069001e-06], dtype=float32)}\r\n{'class': 0, 'prob': array([9.8600847e-01, 6.5606064e-04, 6.1158347e-04, ..., 1.3908433e-06,\r\n       1.4873067e-06, 1.3897480e-06], dtype=float32)}\r\n```\r\n\r\nMost of the classes are again ```0```.", "Are either of these known correct? My suspicion here is that Estimator.predict is giving you the wrong values, and that both predictor and SavedModelEstimator are correct, as the class values from .predict seem to be just incrementing. Can you provide the Estimator code in question? @bananabowl , can you take a look?", "(To clarify, I meant provide the model_fn, bag_of_words_model)", "Sure. Here you go:\r\n\r\n```\r\ndef bag_of_words_model(features, labels, mode):\r\n    \"\"\"A bag-of-words model. Note it disregards the word order in the text.\"\"\"\r\n    bow_column = tf.feature_column.categorical_column_with_identity(\r\n        WORDS_FEATURE, num_buckets=n_words)\r\n    bow_embedding_column = tf.feature_column.embedding_column(\r\n        bow_column, dimension=EMBEDDING_SIZE)\r\n    bow = tf.feature_column.input_layer(\r\n        features, feature_columns=[bow_embedding_column])\r\n    logits = tf.layers.dense(bow, MAX_LABEL, activation=None)\r\n\r\n    return estimator_spec_for_softmax_classification(\r\n        logits=logits, labels=labels, mode=mode)\r\n\r\ndef estimator_spec_for_softmax_classification(logits, labels, mode):\r\n  \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\r\n  predicted_classes = tf.argmax(logits, 1)\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    export_outputs = {'predict_output': tf.estimator.export.PredictOutput({\"class\": predicted_classes, 'prob': tf.nn.softmax(logits)})}\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions={\r\n            'class': predicted_classes,\r\n            'prob': tf.nn.softmax(logits)\r\n        },\r\n        export_outputs=export_outputs)\r\n\r\n  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n  eval_metric_ops = {\r\n      'accuracy':\r\n          tf.metrics.accuracy(labels=labels, predictions=predicted_classes)\r\n  }\r\n  return tf.estimator.EstimatorSpec(\r\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\r\n```\r\n\r\n> \r\n> \r\n> Are either of these known correct? My suspicion here is that Estimator.predict is giving you the wrong values, and that both predictor and SavedModelEstimator are correct, as the class values from .predict seem to be just incrementing.\r\n\r\nActually, my test set is exactly like the train set. Each entry belongs to different class.\r\n\r\n", "Thanks for reporting this, I will take a look. Are you running TF 1.12.0?", "Two theories:\r\n\r\n1. The exported saved model is incorrect. You might be able to rule this out with the saved model CLI tool which you can use to manually determine the output of a given input to the model: https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel\r\nIf it produces the wrong output, we'll know there's some bug in the export step.\r\n\r\n2. The x_test is somehow different between the two calls.\r\n\r\nYou may want to remove the y kwarg passed to numpy_input_fn when building test_input_fn as it is not needed in prediction. This shouldn't affect the output though.", "> \r\n>     2. The x_test is somehow different between the two calls.\r\n\r\nYes, this was indeed the case. I checked the model using `saved_model_cli` but got the same outputs as like before. I wasn't sure about `x_test` being wrong between 2 different calls but at last it seemed to be the only option. I found out that `x_test` which I was passing after loading the model had values +1 than required. I subtracted 1 from each value and it worked! Thank you very much @bananabowl  @karmel .\r\n\r\n"]}, {"number": 24856, "title": "Eager Execution Guide link is broken", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: v2.0\r\n- Doc Link: https://github.com/ehennis/Blog/tree/master/TensorFlow\r\n\r\n\r\n**Describe the documentation issue**\r\nI was looking through the eager execution documentation and one of the links is missing the extension 'ipynb'. If you click on the \"eager execution guide\" the link is broken.\r\n\r\nMarkdown Page: https://github.com/ehennis/Blog/tree/master/TensorFlow.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["The link you are pointing in this issue is incorrect since you are the owner of the repo.\r\nI think you meant this link for the [eager execution guide](https://github.com/tensorflow/docs/tree/master/site/en/r2/tutorials/eager).\r\nCan you please confirm?", "Jeez, copy and paste issue.\r\n\r\nHere is the page: https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/eager/index.md. If you look at the link \"eager execution guide\" it goes to \"https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/eager\" which gets a 404.", "I can edit the original if that would make more sense.", "I will send PR to fix this. Thanks for the catch!\r\n\r\n", "Fixed in https://github.com/tensorflow/docs/pull/295\r\nThanks"]}, {"number": 24855, "title": "[Error, Quantization] tf.nn.quantized_conv2d with filter type tf.qint8", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0, b'v1.12.0-5132-g02b966e'\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nError with using `tf.nn.quantized_conv2d()`. If both input and filter have type `tf.quint8` then everything is fine. But if filter have type `tf.qint8` then error occurs. Tensorflow was built with `--config=mkl`. There is kernel for filter with type `tf.qint8`:\r\n```\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  device='CPU'; Tinput in [DT_QUINT8]; Tfilter in [DT_QINT8]; out_type in [DT_QINT32]\r\n  device='CPU'; Tinput in [DT_QUINT8]; Tfilter in [DT_QUINT8]; out_type in [DT_QINT32]\r\n\r\n         [[QuantizedConv2D]]\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nA = tf.random_normal([100, 30, 30, 1])\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.random_normal([3, 3, 1, 4])\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nqA = tf.quantize(A, min_A, max_A, tf.quint8, mode='MIN_FIRST')\r\nqW = tf.quantize(W, min_W, max_W, tf.qint8, mode='MIN_FIRST')\r\nqAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\nAW = tf.dequantize(*qAW, mode='MIN_FIRST')\r\n\r\ntf.Session().run(AW)\r\n```\r\n\r\n**Error log**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Missing 0-th output from {{node QuantizedConv2D}}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 16, in <module>\r\n    tf.Session().run(AW)\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/workspace/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Missing 0-th output from node QuantizedConv2D (defined at test.py:13)\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node QuantizedConv2D:\r\n Const_3 (defined at test.py:9)\r\n random_normal_1/stddev (defined at test.py:7)\r\n Const_1 (defined at test.py:5)\r\n random_normal/stddev (defined at test.py:3)\r\n Const (defined at test.py:4)\r\n Const_2 (defined at test.py:8)\r\n```\r\n", "comments": ["I ran the stock code (installed from conda) on mac with tensorflow 1.12 and got the following error after running the above code\r\n```import tensorflow as tf\r\n\r\nA = tf.random_normal([100, 30, 30, 1])\r\nmin_A = tf.reduce_min(A)\r\nmax_A = tf.reduce_max(A)\r\n\r\nW = tf.random_normal([3, 3, 1, 4])\r\nmin_W = tf.reduce_min(W)\r\nmax_W = tf.reduce_max(W)\r\n\r\nqA = tf.quantize(A, min_A, max_A, tf.quint8, mode='MIN_FIRST')\r\nqW = tf.quantize(W, min_W, max_W, tf.qint8, mode='MIN_FIRST')\r\nqAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\nAW = tf.dequantize(*qAW, mode='MIN_FIRST')\r\n\r\ntf.Session().run(AW)\r\n```\r\n\r\nThis is the **error log** I am getting, is this the expectation? Do I need to compile tensorflow with `mkl` to test the quantized ops?\r\n\r\n```InvalidArgumentError: No OpKernel was registered to support Op 'QuantizedConv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; Tinput in [DT_QUINT8]; Tfilter in [DT_QUINT8]; out_type in [DT_QINT32]\r\n\r\n\t [[node QuantizedConv2D (defined at <ipython-input-1-64455ed8b073>:13)  = QuantizedConv2D[Tfilter=DT_QINT8, Tinput=DT_QUINT8, dilations=[1, 1, 1, 1], out_type=DT_QINT32, padding=\"SAME\", strides=[1, 1, 1, 1]](QuantizeV2, QuantizeV2_1, QuantizeV2:1, QuantizeV2:2, QuantizeV2_1:1, QuantizeV2_1:2)]]\r\n\r\nCaused by op u'QuantizedConv2D', defined at:\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/bin/ipython\", line 11, in <module>\r\n    sys.exit(start_ipython())\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/__init__.py\", line 119, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/terminal/ipapp.py\", line 355, in start\r\n    self.shell.mainloop()\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 495, in mainloop\r\n    self.interact()\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/terminal/interactiveshell.py\", line 486, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-64455ed8b073>\", line 13, in <module>\r\n    qAW = tf.nn.quantized_conv2d(qA[0], qW[0], qA[1], qA[2], qW[1], qW[2], [1, 1, 1, 1], 'SAME')\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 6329, in quantized_conv2d\r\n    dilations=dilations, name=name)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shoubhik/anaconda/anaconda3/envs/tensorflow-1.12_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'QuantizedConv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; Tinput in [DT_QUINT8]; Tfilter in [DT_QUINT8]; out_type in [DT_QINT32]\r\n\r\n\t [[node QuantizedConv2D (defined at <ipython-input-1-64455ed8b073>:13)  = QuantizedConv2D[Tfilter=DT_QINT8, Tinput=DT_QUINT8, dilations=[1, 1, 1, 1], out_type=DT_QINT32, padding=\"SAME\", strides=[1, 1, 1, 1]](QuantizeV2, QuantizeV2_1, QuantizeV2:1, QuantizeV2:2, QuantizeV2_1:1, QuantizeV2_1:2)]] \r\n```", "Could it be because of QuantizedConv2D being registered like this\r\n\r\n```\r\n// Right now we only support taking two eight bit inputs, and returning the\r\n// results as signed 32-bit integers.\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"QuantizedConv2D\")\r\n        .Device(DEVICE_CPU)\r\n        .TypeConstraint<quint8>(\"Tinput\")\r\n        .TypeConstraint<quint8>(\"Tfilter\")\r\n        .TypeConstraint<qint32>(\"out_type\"),\r\n    QuantizedConv2DOp<quint8, quint8, qint32, Im2ColConvFunctor>);\r\n\r\n```\r\n[here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/quantized_conv_ops.cc#L587-L595)", "@shoubhik There is another kernel for `quantized_conv2d()` with `mkl` in `tensorflow/core/kernels/mkl_conv_ops.cc`:\r\n```\r\n// INT8 kernel registration\r\n// Register NoOp kernel for QunatizedConv2D for qint8 filter\r\nREGISTER_KERNEL_BUILDER(Name(\"QuantizedConv2D\")\r\n                            .Device(DEVICE_CPU)\r\n                            .TypeConstraint<quint8>(\"Tinput\")\r\n                            .TypeConstraint<qint8>(\"Tfilter\")\r\n                            .TypeConstraint<qint32>(\"out_type\"),\r\n                        NoOp);\r\n--//--\r\n// Register a templatized implementation of MklQuntizedConv2D.\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"_MklQuantizedConv2D\")\r\n        .Device(DEVICE_CPU)\r\n        .TypeConstraint<quint8>(\"Tinput\")\r\n        .TypeConstraint<qint8>(\"Tfilter\")\r\n        .TypeConstraint<qint32>(\"out_type\")\r\n        .Label(mkl_op_registry::kMklQuantizedOpLabel),\r\n    MklQuantizedConv2DOp<CPUDevice, float, qint32, qint32, false>);\r\n```", "@Vooblin Thanks for your error reporting. We appreciate your time trying out Intel CPU quantization works. However, the quantization work is still in progress. You could build with adding `--copt=\"-DINTEL_MKL_QUANTIZED\"` to bazel build option in order to get it work.\r\n\r\nYou are getting the error [`tensorflow.python.framework.errors_impl.InternalError: Missing 0-th output from node QuantizedConv2D (defined at test.py:13)`], because of `NoOp` kernel registration with the operator. The computation graph needs to go through some code in `tensorflow/core/graph/mkl_layout_pass.cc`, \r\n e.g., https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/graph/mkl_layout_pass.cc#L297-L319\r\nto get the `NoOp` kernel replaced by `MklQuantizedConv2DOp`", "@mdfaijul Thank you very much for your response! It helped me fix this error."]}, {"number": 24854, "title": "second max_pool probably is missing in \"create_conv_model\" function in tensorflow/examples/speech_commands/models.py", "body": "in function `create_conv_model()` which is defined in: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py\r\n\r\nIt seems that the second max pool is missing.\r\nDue to the documentation of the function this should be the layout of the graph:\r\n\r\n&nbsp;(fingerprint_input)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n [Conv2D]<-(weights)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n [BiasAdd]<-(bias)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Relu]\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[MaxPool]\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n[Conv2D]<-(weights)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n[BiasAdd]<-(bias)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Relu]\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[MaxPool] <================is this missing???\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n[MatMul]<-(weights)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n[BiasAdd]<-(bias)\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v\r\n\r\nAfter the first_dropout, max pooling is correctly performed:\r\n`line 272: max_pool = tf.nn.max_pool(first_dropout, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')`\r\n\r\nBut I could't find any max pooling after the second_dropout.", "comments": ["Thanks! I think that's just a documentation problem, since I discovered the second max pool was not necessary for that model."]}, {"number": 24853, "title": "Additional test on make_csv_dataset", "body": "Verify the delim and NAvalue are compatible with different inputs.", "comments": []}, {"number": 24852, "title": "TF Keras tf_utils_test missing test cases add", "body": "1-Certain basic unit test cases add in tf_utils_test(//tensorflow/python/keras:tf_utils_test)\r\n2-testGetReachableFromInputs test case move under right package", "comments": ["@jvdillon \r\n\r\nWelcome for review and thanks.....", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 24851, "title": "TF Keras tf_utils_test missing test cases add", "body": "\r\n1-Certain basic unit test cases add in tf_utils_test(//tensorflow/python/keras:tf_utils_test)\r\n2-testGetReachableFromInputs test case move under right package", "comments": []}, {"number": 24850, "title": "Too many LSTM implementations (6)", "body": "**System information**\r\n- TensorFlow version: v1.8\r\n- Doc Link: [tf.contrib.rnn.LSTMCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMCell), [tf.contrib.rnn.LSTMBlockCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMBlockCell), [tf.contrib.rnn.LSTMBlockFusedCell](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/contrib/rnn/LSTMBlockFusedCell), [tf.nn.rnn_cell.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell), [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [tf.contrib.cudnn_rnn.CudnnLSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM), [tf.contrib.cudnn_rnn.CudnnLSTMSaveable](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTMSaveable)\r\n\r\nIgnoring any fancy versions like Grid LSTM cells and LSTM cells somehow combined with convolutions there are **6!** different LSTM implementations in tensorflow (see documentation links above). Some are better documented than others, some claim to be faster than others (there seems to be the order  tf.contrib.rnn.LSTMCell < tf.contrib.rnn.LSTMBlockCell < tf.contrib.rnn.LSTMBlockFusedCell but then why keep the slower implementations instead of just replacing the slower ones with faster implementations?), all have slightly different APIs.\r\n\r\nWhat I am missing is a guide that tells me:\r\n* What is the fastest implementation on what hardware (I might be willing to deal with poor documentation if the result is at least fast, but having to deal with poor documentation just to find out that the performance is bad sucks).\r\n* What is the stable implementation which I can expect to be maintained for a longer time period.\r\n* What implementations are just included for historic reasons or to maintain backwards compatibility (and therefor should be avoided when starting a new project).\r\n* Maybe there are good reasons (that I am not aware of) to keep multiple implementations in parallel because they all have different tradeoffs. In this case I would like to know more what the differences are between the implementations. This can just be a table with pros and cons.\r\n\r\nThe state of affairs is probably similar for GRU cells and simple RNN cells. The situation with the LSTM cells is just exemplary ...", "comments": ["i found the code of tensorflow full of history, complex and vague", "Seems like the issue is considered by the roadmap for v2.0 (read it here in [short](https://www.tensorflow.org/community/roadmap) or [long](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A)).", "Thanks for sharing the roadmap links @erniejunior. We can expect better structured tf.contrib module after TF 2.0 official release."]}, {"number": 24849, "title": "GradientTape return none for variable not being present in expression", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, Gradient tape return none for derivative respect to variables not being present in expression. Semantically, the gradient should return zero for variables. The current behavior make programmer have to checkout whether the output is none and replace it with zero.\r\n\r\n**Will this change the current api? How?**\r\nno change to current api\r\n\r\n**Who will benefit with this feature?**\r\npeople who use gradient tape will get intuitive result. \r\n\r\n**Any Other info.**\r\n", "comments": ["See the unconnected_gradients argument to gradienttape.gradient. You can set it to get zeros instead of None.\r\n\r\n(None is useful for debugging as code might crash if it expects a gradient and doesn't find one, which can catch quite a few bugs)"]}, {"number": 24848, "title": "TF Keras conv_utils missing test cases add", "body": "Certain basic unit test cases add in conv_utils(//tensorflow/python/keras:conv_utils_test)", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@fchollet Can you please approve this PR? I fixed some minor nit. Thanks!"]}, {"number": 24847, "title": "Upgrade tensor flow from 1.9 to 1.12, Using tf.keras : ImportError: cannot import name 'Layer'", "body": "'from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional'\r\n\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-b2fcd5276851> in <module>()\r\n----> 1 from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional\r\n      2 from tensorflow.keras.models import Model\r\n      3 from tensorflow.keras.optimizers import Adam\r\n      4 \r\n      5 from tensorflow.keras.losses import binary_crossentropy\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/keras/__init__.py in <module>()\r\n     18 from . import estimator\r\n     19 from . import initializers\r\n---> 20 from . import layers\r\n     21 from . import losses\r\n     22 from . import metrics\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/keras/layers/__init__.py in <module>()\r\n      6 from __future__ import print_function\r\n      7 \r\n----> 8 from tensorflow.python.estimator.keras import Layer\r\n      9 from tensorflow.python.keras import Input\r\n     10 from tensorflow.python.keras.applications.densenet import Activation\r\n\r\nImportError: cannot import name 'Layer'", "comments": ["Can you try,\r\n> from tensorflow.keras.layers import Layer", "I have fixed it just typing \"from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional'\" again. Thanks!", "Closing this issue since its resolved. Thanks!", "I have the same issue. The solution from @liupeng89 does not help. I still get the same error.", "I had the same issue when upgrading to tensorflow 1.13 (windows, through conda). In the end I just used keras with tensorflow backend, i.e. for this example:\r\nfrom keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional", "I have the same problem.\r\n`pydoc.ErrorDuringImport: problem in tensorflow.keras - ImportError: cannot import name 'Layer'`\r\n\r\nAny proper solution for this instead of workaround?\r\n\r\n\r\n", "> I have fixed it just typing \"from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LSTM, Activation, Conv2D, Reshape, Average, Bidirectional'\" again. Thanks!\r\n\r\nWe have the same name. "]}, {"number": 24846, "title": "gpu installation automation", "body": "installation with gpu is very difficult. could you please publish a one-command script that does the whole process? see a list of the steps here (i wrote them):\r\n\r\nhttps://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions", "comments": ["Thanks for the document!", "Unfortunately, a lot of the GPU dependencies we have are through NVIDIA.\r\nSome of these components require you to read and review and their licenses.\r\nInstalling through their repositories will imply these.\r\nI am not sure if we can redistribute, or hide their installation.\r\nFor these, I have to redirect you to NVIDIA.\r\n\r\nMoreover, while those instructions will work for ubuntu, on redhat, or windows they are completely different. even different ubuntu versions can require completely different commands for different steps.\r\nSo unfortunately, not sure we can provide a one script to install everything that solves the problem for everyone, even if we can.\r\n", "I will close the issue, but thanks for the document!\r\nIt will still be discoverable through search, and I am sure it will be very helpful to users."]}, {"number": 24845, "title": "Different with keras.layers.Dropout and tensorflow.keras.layers.Dropout, rate argument", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.2\r\n- CUDA/cuDNN version: 9.0.176 \r\n- GPU model and memory: NVIDIA 1060\r\n\r\n\r\n* keras version\r\n```python\r\nfrom keras import layers\r\nfrom keras import models\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))\r\nmodel.add(layers.Dropout(rate=1.0))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(units=1, activation=None))\r\n```\r\nit's fine, but\r\n\r\n* tensorflow version\r\n```python\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import models\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid'))\r\nmodel.add(layers.Dropout(rate=1.0))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(units=1, activation=None))\r\n```\r\nit occurred ValueError like below:\r\n```sh\r\nValueError: keep_prob must be a scalar tensor or a float in the range (0, 1], got 0\r\n```\r\n\r\nWhen I see the [source code line](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/keras/layers/core.py#L142), why it works like `1 - self.rate`?\r\n\r\nRelated issue #24526 ", "comments": ["`rate` is what fraction of neurons you drop, i.e. everything would be zeros. In general, this is not something users should be able to do. This error is intended, except perhaps it should mention `rate` if the `rate` arg is used rather than `keep_prob`", "@omalleyt12 I can understand what you want to say, but my point is why they use different applied argument between `keras` and `tensorflow.keras`.", "#24526  merged"]}, {"number": 24844, "title": "BUS Error, likely with blas", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac Sierra\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n- Python version:\r\n3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nno Cuda\r\n- GPU model and memory:\r\nnot using GPU\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI'm trying Magenta from Google Brain. When I run `python onsets_frames_transcription_transcribe.py --acoustic_run_dir /Users/lorenzori/Downloads/maestro_checkpoint  ~/Downloads/test_audio.wav` the script starts, then it ends with: \r\n```\r\n/Users/lorenzori/virtualenvs/test-audio/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\n/Users/lorenzori/virtualenvs/test-audio/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\n/Users/lorenzori/virtualenvs/test-audio/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\n2019-01-10 17:49:57.458805: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:tensorflow:Restoring parameters from /Users/lorenzori/Downloads/maestro_checkpoint/train/model.ckpt-maestro\r\nINFO:tensorflow:Starting transcription for /Users/lorenzori/Downloads/test_audio.wav...\r\nINFO:tensorflow:Processing file...\r\nINFO:tensorflow:Running inference...\r\n[1]    20612 bus error  python onsets_frames_transcription_transcribe.py --acoustic_run_dir\r\n```\r\n\r\nUsing lldb I obtain the following info:\r\n```\r\nProcess 19280 stopped\r\n* thread #25, stop reason = EXC_BAD_ACCESS (code=2, address=0x700001382000)\r\n    frame #0: 0x0000000110a24805 libopenblasp-r0.3.0.dev.dylib`dgemm_thread_tn + 1541\r\nlibopenblasp-r0.3.0.dev.dylib`dgemm_thread_tn:\r\n->  0x110a24805 <+1541>: xchgq  %rdi, -0x40(%rsi)\r\n    0x110a24809 <+1545>: xorl   %edi, %edi\r\n    0x110a2480b <+1547>: xchgq  %rdi, (%rsi)\r\n    0x110a2480e <+1550>: addq   $0x200, %rsi              ; imm = 0x200\r\nTarget 0: (python) stopped.\r\n(lldb) bt\r\n* thread #25, stop reason = EXC_BAD_ACCESS (code=2, address=0x700001382000)\r\n  * frame #0: 0x0000000110a24805 libopenblasp-r0.3.0.dev.dylib`dgemm_thread_tn + 1541\r\n    frame #1: 0x00000001108f3e26 libopenblasp-r0.3.0.dev.dylib`cblas_dgemm + 854\r\n    frame #2: 0x0000000105564285 multiarray.cpython-36m-darwin.so`cblas_matrixproduct + 4917\r\n    frame #3: 0x0000000105529d37 multiarray.cpython-36m-darwin.so`PyArray_MatrixProduct2 + 215\r\n    frame #4: 0x000000010552ed1f multiarray.cpython-36m-darwin.so`array_matrixproduct + 191\r\n    frame #5: 0x00000001000d1cbe Python`_PyCFunction_FastCallDict + 463\r\n    frame #6: 0x00000001001362d6 Python`call_function + 489\r\n    frame #7: 0x000000010012f18b Python`_PyEval_EvalFrameDefault + 4811\r\n    frame #8: 0x0000000100136a38 Python`_PyEval_EvalCodeWithName + 1719\r\n    frame #9: 0x000000010013713b Python`fast_function + 218\r\n    frame #10: 0x00000001001362ad Python`call_function + 448\r\n    frame #11: 0x000000010012f224 Python`_PyEval_EvalFrameDefault + 4964\r\n    frame #12: 0x00000001001373db Python`_PyFunction_FastCall + 121\r\n    frame #13: 0x00000001001362ad Python`call_function + 448\r\n    frame #14: 0x000000010012f18b Python`_PyEval_EvalFrameDefault + 4811\r\n    frame #15: 0x0000000100136a38 Python`_PyEval_EvalCodeWithName + 1719\r\n    frame #16: 0x000000010013730b Python`_PyFunction_FastCallDict + 449\r\n    frame #17: 0x0000000100099f21 Python`_PyObject_FastCallDict + 196\r\n    frame #18: 0x0000000100182073 Python`partial_call + 258\r\n    frame #19: 0x0000000100099da2 Python`PyObject_Call + 101\r\n    frame #20: 0x000000010012f3f4 Python`_PyEval_EvalFrameDefault + 5428\r\n    frame #21: 0x0000000100136a38 Python`_PyEval_EvalCodeWithName + 1719\r\n    frame #22: 0x000000010013730b Python`_PyFunction_FastCallDict + 449\r\n    frame #23: 0x0000000100099f21 Python`_PyObject_FastCallDict + 196\r\n    frame #24: 0x000000010009a044 Python`_PyObject_Call_Prepend + 156\r\n    frame #25: 0x0000000100099da2 Python`PyObject_Call + 101\r\n    frame #26: 0x00000001000e4460 Python`slot_tp_call + 50\r\n    frame #27: 0x0000000100099da2 Python`PyObject_Call + 101\r\n    frame #28: 0x00000001212b078e _pywrap_tensorflow_internal.so`tensorflow::PyFuncOp::Compute(tensorflow::OpKernelContext*) + 974\r\n    frame #29: 0x000000012bd82422 libtensorflow_framework.so`tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 6690\r\n    frame #30: 0x000000012bd895ba libtensorflow_framework.so`std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 58\r\n    frame #31: 0x000000012bdde824 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 1876\r\n    frame #32: 0x000000012bdddfd4 libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 52\r\n    frame #33: 0x000000012be00070 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 96\r\n    frame #34: 0x00007fffb96b893b libsystem_pthread.dylib`_pthread_body + 180\r\n    frame #35: 0x00007fffb96b8887 libsystem_pthread.dylib`_pthread_start + 286\r\n    frame #36: 0x00007fffb96b808d libsystem_pthread.dylib`thread_start + 13\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe script should run\r\n\r\n**Code to reproduce the issue**\r\npython onsets_frames_transcription_transcribe.py --acoustic_run_dir <checkpoint_dir> <wav_file>\r\n", "comments": ["@lorenzoriano Please post this issue in [TensorFlow magenta repo](https://github.com/tensorflow/magenta/issues). This repository is mainly for solving issues TF Core and TF Lite. Thanks!", "I've posted in the Magenta repo, but I'm not sure the error is due to Magenta or TF Core. The latter is likely imho ", "@lorenzoriano Did you try the solution that was provided by @pangwang in Magenta Repo [here](https://github.com/tensorflow/magenta/issues/1410). Please let us know how it progresses. Thanks!", "I am closing the issue as it is duplicate. Please check Magenta Repo for a solution. Thanks!"]}, {"number": 24843, "title": "Add MetaOptimizer option to save the graph after each optimization pass", "body": "Adding an option to MetaOptimizer to dump the graph after each optimization pass will help in development of optimizers and understanding of how the graph is modified before being executed. Currently only final graph is saved if VLOG level is 1. Extending it to the per optimization level is much more useful than saving the final graph, since it is already possible to get final graph through RunMetadata.", "comments": ["tagging @azaks2  since we discussed this a few months back and current implementation is not covering this issue.", "Resolved in https://github.com/tensorflow/tensorflow/commit/16c0e72b92497587b6f7e1614869066ed122c9e4."]}]