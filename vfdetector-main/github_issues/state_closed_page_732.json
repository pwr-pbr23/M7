[{"number": 31615, "title": "I have installed tensorflow-gpu 2.0.0 alpha,how can I upgrade it to tensorflow-gpu==2.0.0-beta1,but didn't influence my tensorlayer.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@YonghuiXu, Try upgrading the tensorflow using  `pip install tensorflow-gpu==2.0.0.beta1`. Let us know if that helps. Thanks!", "Yes, I have done it like what you said. It works well. \r\n\r\n\r\n\r\n---Original---\r\nFrom: \"gadagashwini\"<notifications@github.com>\r\nDate: Fri, Aug 16, 2019 16:52 PM\r\nTo: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com>;\r\nCc: \"Mention\"<mention@noreply.github.com>;\"YonghuiXu\"<2259949930@qq.com>;\r\nSubject: Re: [tensorflow/tensorflow] I have installed tensorflow-gpu 2.0.0 alpha,how can I upgrade it to tensorflow-gpu==2.0.0-beta1,but didn't influence my tensorlayer. (#31615)\r\n\r\n\r\n\r\n@YonghuiXu, Try upgrading the tensorflow using  pip install tensorflow-gpu==2.0.0.beta1. Let us know if that helps. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or mute the thread.", "@YonghuiXu Glad to know that it worked.\r\nClosing the issue since its resolved. Thanks!"]}, {"number": 31614, "title": "Licencing of TensorFlow conda distribution in miniconda docker image", "body": "I am using conda distribution of Tensorflow inside miniconda3 docker image.\r\nminiconda3 docker image i have downloaded from below link:- \r\n\r\nhttps://hub.docker.com/r/continuumio/miniconda3\r\n\r\nInstallation of tensorflow & other dependencies in docker image:\r\n\r\n\r\n    FROM continuumio/miniconda3\r\n    RUN apt-get update && \\\r\n    apt-get install -y openjdk-8-jre-headless && \\\r\n    apt-get install -y ant && \\\r\n\tapt-get install -y python3-tk && \\\r\n    apt-get clean;\r\n    conda install --yes tensorflow==1.12\r\n\r\n\r\nI created another docker image on the top of miniconda3 docker image.\r\n\r\nScan this final image in whitesource tool for getting information of GPL and AGPL licence. \r\n\r\nI am getting lot's of GPL and AGPL inside tensorflow package folder.\r\n\r\nBelow is the example of one package:-\r\n\r\n\r\n    Slic3r-1.41.0-alpha3.1    GPL 2.0AGPL 3.0GPL    opt\\conda\\lib\\python3.6\\site-packages\\tensorflow\\include\\Eigen\\src\\Core\\Assign_MKL.h\r\n\r\nNow i want to use this in our Enterprise application. is there any other way to use this in application or this the issue of tool to give GPL inside package?\r\n\r\n \r\n", "comments": ["@rgupta2508 ,\r\nCan you please refer this [link](https://www.tensorflow.org/install/docker).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31614\">No</a>\n"]}, {"number": 31613, "title": "ImportError: DLL load failed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution :windows 10 pro 16299\r\n- TensorFlow installed from : conda install\r\n- TensorFlow version (use command below): tensorflow 2.0.0b1\r\n- Python version:3.6.9\r\n\r\nlog\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-6-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     38 import sys as _sys\r\n     39 \r\n---> 40 from tensorflow.python.tools import module_util as _module_util\r\n     41 \r\n     42 from tensorflow._api.v2 import audio\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\BLACK MANTIS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BLACK MANTIS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BLACK MANTIS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BLACK MANTIS\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BLACK MANTIS\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["> You have to use CUDA 10.0 for tensorflow-gpu-1.13.1 (don't forget to download cuDNN 7.5 for CUDA 10.0 too).\r\n\r\nThis pertains to tensorflow 2.0 as well as far as I can tell\r\nDuplicate of #26364", "\nThanks so much sir. I will definitely do that.\nSent from Yahoo Mail on Android \n \n  On Wed, Aug 14, 2019 at 4:34 PM, Sean McGuire<notifications@github.com> wrote:   \n\nYou have to use CUDA 10.0 for tensorflow-gpu-1.13.1 (don't forget to download cuDNN 7.5 for CUDA 10.0 too).\n\n\nDuplicate of #26364\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n  \n", "@youngmantis \r\nCan you please confirm if @mcguires5's workaround is working for you.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31613\">No</a>\n"]}, {"number": 31612, "title": "tf-lite android so build fail", "body": "**System information**\r\n- OS Platform and Distribution:  (e.g., Linux Ubuntu 18.04)\r\n- TensorFlow installed from (source or binary):  source \r\n- TensorFlow version: r1.13\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):  0.24.1\r\n- GCC/Compiler version (if compiling from source):  gcc version 4.8.5\r\nNDK version 14\r\nSDK version 24\r\n\r\n*Describe the problem**\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cxxopt=-std=c++11    --cpu=arm64-v8a`\r\n\r\nwhen i try the build command, it throws clang compile error. so why ndk14 use clang but not gcc g++?\r\n\r\nThe document might be confused, since some code have changed on git but doc haven't\r\nIn fact, I have try many tools version combination: ndk 14 / 12, bazel 0.20.0/ 0.21.0/ 0.24.1. Each combination throws different error\r\n\r\nCould you show me a working tool verison combination of bazel, ndk, sdk and tf version, which is tested?\r\n\r\nThank you!\r\n\r\n------------------------------------------------\r\n**error log:**\r\nERROR: /home/ger/opt/tensorflow/tensorflow/core/kernels/BUILD:6575:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/cwise_op_erf.cc:16:\r\nIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\nIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:29:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:415:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n                                       ^             ~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:44: note: in instantiation of member function 'Eigen::internal::erf_impl<float>::run' requested here\r\n  return EIGEN_MATHFUNC_IMPL(erf, Scalar)::run(x);\r\n                                           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:248:31: note: in instantiation of function template specialization 'Eigen::numext::erf<float>' requested here\r\n    using numext::erf; return erf(a);\r\n                              ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:434:12: note: in instantiation of member function 'Eigen::internal::scalar_erf_op<float>::operator()' requested here\r\n    return m_functor(m_argImpl.coeff(index));\r\n           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:156:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n                                         ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:218:17: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >, Eigen::ThreadPoolDevice>::evalScalar' requested here\r\n      evaluator.evalScalar(i);\r\n                ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:283:39: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n                           EvalRange::run(&evaluator, firstIdx, lastIdx);\r\n                                      ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:278:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n  out.device(d) = rhs;\r\n                ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:541:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long>, 16, MakePointer> > >' requested here\r\n    Assign(d, out, in.unaryExpr(typename Functor::func()));\r\n    ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:249:5: note: in instantiation of member function 'tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::erf<float> >::operator()' requested here\r\n    functor::UnaryFunctor<Device, Functor>()(\r\n    ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:234:12: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<float> >::Compute' requested here\r\n  explicit UnaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\r\n           ^\r\ntensorflow/core/kernels/cwise_op_erf.cc:19:11: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<float> >::UnaryOp' requested here\r\nREGISTER3(UnaryOp, CPU, \"Erf\", functor::erf, float, Eigen::half, double);\r\n          ^\r\nIn file included from tensorflow/core/kernels/cwise_op_erf.cc:16:\r\nIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\nIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:29:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:415:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n                                       ^             ~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:44: note: in instantiation of member function 'Eigen::internal::erf_impl<Eigen::half>::run' requested here\r\n  return EIGEN_MATHFUNC_IMPL(erf, Scalar)::run(x);\r\n                                           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:248:31: note: in instantiation of function template specialization 'Eigen::numext::erf<Eigen::half>' requested here\r\n    using numext::erf; return erf(a);\r\n                              ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:434:12: note: in instantiation of member function 'Eigen::internal::scalar_erf_op<Eigen::half>::operator()' requested here\r\n    return m_functor(m_argImpl.coeff(index));\r\n           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:156:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n                                         ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:218:17: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long>, 16, MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> > >, Eigen::ThreadPoolDevice>::evalScalar' requested here\r\n      evaluator.evalScalar(i);\r\n                ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:283:39: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n                           EvalRange::run(&evaluator, firstIdx, lastIdx);\r\n                                      ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:278:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> > >' requested here\r\n  out.device(d) = rhs;\r\n                ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:541:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<Eigen::half>, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer> > >' requested here\r\n    Assign(d, out, in.unaryExpr(typename Functor::func()));\r\n    ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:249:5: note: in instantiation of member function 'tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::erf<Eigen::half> >::operator()' requested here\r\n    functor::UnaryFunctor<Device, Functor>()(\r\n    ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:234:12: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<Eigen::half> >::Compute' requested here\r\n  explicit UnaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\r\n           ^\r\ntensorflow/core/kernels/cwise_op_erf.cc:19:11: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<Eigen::half> >::UnaryOp' requested here\r\nREGISTER3(UnaryOp, CPU, \"Erf\", functor::erf, float, Eigen::half, double);\r\n          ^\r\nIn file included from tensorflow/core/kernels/cwise_op_erf.cc:16:\r\nIn file included from ./tensorflow/core/kernels/cwise_ops_common.h:29:\r\nIn file included from ./tensorflow/core/kernels/cwise_ops.h:23:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:29:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../SpecialFunctions:50:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:415:5: error: static_assert failed \"THIS_TYPE_IS_NOT_SUPPORTED\"\r\n    EIGEN_STATIC_ASSERT((internal::is_same<Scalar, Scalar>::value == false),\r\n    ^                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/util/StaticAssert.h:33:40: note: expanded from macro 'EIGEN_STATIC_ASSERT'\r\n    #define EIGEN_STATIC_ASSERT(X,MSG) static_assert(X,#MSG);\r\n                                       ^             ~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:2114:44: note: in instantiation of member function 'Eigen::internal::erf_impl<double>::run' requested here\r\n  return EIGEN_MATHFUNC_IMPL(erf, Scalar)::run(x);\r\n                                           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsFunctors.h:248:31: note: in instantiation of function template specialization 'Eigen::numext::erf<double>' requested here\r\n    using numext::erf; return erf(a);\r\n                              ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorEvaluator.h:434:12: note: in instantiation of member function 'Eigen::internal::scalar_erf_op<double>::operator()' requested here\r\n    return m_functor(m_argImpl.coeff(index));\r\n           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:156:42: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> >, Eigen::ThreadPoolDevice>::coeff' requested here\r\n    m_leftImpl.coeffRef(i) = m_rightImpl.coeff(i);\r\n                                         ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:218:17: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >, Eigen::ThreadPoolDevice>::evalScalar' requested here\r\n      evaluator.evalScalar(i);\r\n                ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:283:39: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n                           EvalRange::run(&evaluator, firstIdx, lastIdx);\r\n                                      ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:278:17: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n  out.device(d) = rhs;\r\n                ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:541:5: note: in instantiation of function template specialization 'tensorflow::functor::Assign<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<double, 1, 1, long>, 16, MakePointer>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_erf_op<double>, const Eigen::TensorMap<Eigen::Tensor<const double, 1, 1, long>, 16, MakePointer> > >' requested here\r\n    Assign(d, out, in.unaryExpr(typename Functor::func()));\r\n    ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:249:5: note: in instantiation of member function 'tensorflow::functor::UnaryFunctor<Eigen::ThreadPoolDevice, tensorflow::functor::erf<double> >::operator()' requested here\r\n    functor::UnaryFunctor<Device, Functor>()(\r\n    ^\r\n./tensorflow/core/kernels/cwise_ops_common.h:234:12: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<double> >::Compute' requested here\r\n  explicit UnaryOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\r\n           ^\r\ntensorflow/core/kernels/cwise_op_erf.cc:19:11: note: in instantiation of member function 'tensorflow::UnaryOp<Eigen::ThreadPoolDevice, tensorflow::functor::erf<double> >::UnaryOp' requested here\r\nREGISTER3(UnaryOp, CPU, \"Erf\", functor::erf, float, Eigen::half, double);\r\n          ^\r\n3 errors generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\n", "comments": ["What you were trying to build `//tensorflow/contrib/android:libtensorflow_inference.so` is a part of deprecated TensorFlow Mobile. You may wanna try [TFLite](https://www.tensorflow.org/lite/guide) instead.", "> What you were trying to build `//tensorflow/contrib/android:libtensorflow_inference.so` is a part of deprecated TensorFlow Mobile. You may wanna try [TFLite](https://www.tensorflow.org/lite/guide) instead.\r\n\r\nThank you for your suggestion, actually i want to make a windows Java application running on the arm development board with ubuntu18.04. I need jar and so lib to use the python model.", "> What you were trying to build `//tensorflow/contrib/android:libtensorflow_inference.so` is a part of deprecated TensorFlow Mobile. You may wanna try [TFLite](https://www.tensorflow.org/lite/guide) instead.\r\n\r\nI try to use tensorflow-lite, get the .arr file. but it load the so file fail, seems arm64-v8a so can not run on aarch64 board?", "1. I don't know if Java binding works well on non-Android platforms\r\n2. when you said \"arm64-v8a so\", you meant Android one right? Android's system libraries (libc and others) are different from standard Linux. That's why it doesn't work. Usually, arm64-v8a == aarch64, but that doesn't mean software compatible.\r\n3. model is not in Python\r\n4. You may want to start from prebuilt aarch64 binaries for Python, such as ones at [here](https://github.com/PINTO0309/Tensorflow-bin) built by @PINTO0309", "> 1. I don't know if Java binding works well on non-Android platforms\r\n> 2. when you said \"arm64-v8a so\", you meant Android one right? Android's system libraries (libc and others) are different from standard Linux. That's why it doesn't work. Usually, arm64-v8a == aarch64, but that doesn't mean software compatible.\r\n> 3. model is not in Python\r\n> 4. You may want to start from prebuilt aarch64 binaries for Python, such as ones at [here](https://github.com/PINTO0309/Tensorflow-bin) built by @PINTO0309\r\n\r\n\u975e\u5e38\u611f\u8b1d\u63d0\u4f9b\u9019\u4e9b\u601d\u8def\uff01\r\ni try using python, then setup service for java calling", "@GerScau, Were you able to resolve the issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31612\">No</a>\n"]}, {"number": 31611, "title": "[issue]Pb convert to tflite :how to calculate output min max of concatenation layer", "body": "I notice that tensorflow source code in quantized_concat_op.cc use the function of **CalculateInputAndOutputRange()** to caculate the minmax value of input and output feature maps, which just let the output min value be the minimum of the  input_mins[i], and max value be the maximum of the  input_maxes[i].\r\nBut when i convert pb file to tflite, the minmax value of output feature map of  concat layer not be the value  what it should be.\r\n\r\n\r\nthis is my convet command:\r\ntoco  \\\r\n--graph_def_file=rpn_190516.pb \\\r\n--output_file=rpn_190516_tt.tflite \\\r\n--input_shapes=1,2048,2048,3 \\\r\n--input_arrays='input_rpn' \\\r\n--output_arrays='concat' \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_dev_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n\r\n\r\nand the reslut screenshot:\r\n![image](https://user-images.githubusercontent.com/13192737/63005315-d7770800-beae-11e9-9372-edd151d909dc.png)\r\n\r\ni would appreciate it , if someone can explain the result.\r\n", "comments": ["Has the problem been solved? I also encountered the same problem.", "@MasterGG you specified `--change_concat_input_ranges=false`", "@freedomtan  thank you for your reply! Do you mean that if i want to use toco to convet pb files , i should always set the flag  --change_concat_input_ranges=true?\r\nAlso i have found in tensorflow/lite/toco/graph_transformations/quantize.cc ,which has code as below :\r\n![image](https://user-images.githubusercontent.com/13192737/63087573-1b3c4100-bf85-11e9-891e-62fc79f88128.png)\r\nDoes it means that if  the flag is true ,concatenation's output minmax just equal to the minmax of the first input arrays?", "yes, but as far as I can remember the the parameters of input was changed before the code snippet you showed.", "@freedomtan  Hi, i have tried some trials, if i set --change_concat_input_ranges=true, i got some tflite whoese minmax of all inputs  and outputs of  concat layers change to the same value; but some other tflite  just make the minmax of outputs be the same as the first input array,and didn't change the minmax of other inputs arrays.\r\nSo i am confused about the rules how to set the toco convert flags to get valid minmax values of concatenation layers. Can you give me some guidance about the Complete  rules?", "@MasterGG Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? refer [link](https://stackoverflow.com/questions/54830869/understanding-tf-contrib-lite-tfliteconverter-quantization-parameters) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31611\">No</a>\n"]}, {"number": 31610, "title": "fatal error LNK1201:check for insufficient disk space, invalid path, or insufficient privilege", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- TensorFlow installed from (source or binary): build from source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): VS2015 update3\r\n- CUDA/cuDNN version:9/7\r\n- GPU model and memory:Titan V 11G\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I build debug version Tensorflow on windows 10\u3002Linker error 1201 ocoured\uff0c\r\nLINK : fatal error LNK1201: error writing to program database 'C:\\users\\dl\\_bazel_dl\\hd37h63o\\execroot\\org_tensorflow\\bazel-out\\x64_windows-dbg\\bin\\tensorflow\\libtensorflow_cc.pdb'; check for insufficient disk space, invalid path, or insufficient privilege\u3002\r\n\r\n\r\n**Any other info / logs**\r\nERROR: D:/clone/tensorflow-1.12.0/source/tensorflow/BUILD:449:1: Linking of rule '//tensorflow:libtensorflow_cc.so' failed (Exit 1201): link.exe failed: error executing command\r\n  cd C:/users/dl/_bazel_dl/hd37h63o/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=D:/clone/tensorflow-1.12.0/venv/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=D:/clone/tensorflow-1.12.0/venv/lib/site-packages\r\n    SET TEMP=C:\\Users\\DL\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\DL\\AppData\\Local\\Temp\r\n    SET USE_LINKER=1\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -def:tensorflow/tf_exported_symbols_msvc.lds -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib /MACHINE:X64 @bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so-2.params /DEBUG:FULL /INCREMENTAL:NO\r\n   Creating library bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so.if.lib and object bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so.if.exp\r\nlibtensorflow_cc.so.if.exp : warning LNK4070: /OUT:tensorflow_cc.dll directive in .EXP differs from output filename 'bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow_cc.so'; ignoring directive\r\npin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\ncollective_param_resolver_distributed.lib(collective_param_resolver_distributed.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nbatch_kernels.lo.lib(batch_kernels.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\ncaptured_function.lib(captured_function.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\narithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_NewStatus imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_DeleteStatus imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_GetCode imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\ntfprof_show.lib(tfprof_show.obj) : warning LNK4217: locally defined symbol TF_Message imported in function \"protected: bool __cdecl tensorflow::tfprof::TFShow::LookUpCheckPoint(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unique_ptr<class tensorflow::tfprof::TFProfTensor,struct std::default_delete<class tensorflow::tfprof::TFProfTensor> > *)\" (?LookUpCheckPoint@TFShow@tfprof@tensorflow@@IEAA_NAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAV?$unique_ptr@VTFProfTensor@tfprof@tensorflow@@U?$default_delete@VTFProfTensor@tfprof@tensorflow@@@std@@@5@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ??0ErrorCode@icu_62@@QEAA@XZ (public: __cdecl icu_62::ErrorCode::ErrorCode(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ??1ErrorCode@icu_62@@UEAA@XZ (public: virtual __cdecl icu_62::ErrorCode::~ErrorCode(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ??BErrorCode@icu_62@@QEAAPEAW4UErrorCode@@XZ (public: __cdecl icu_62::ErrorCode::operator enum UErrorCode *(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ?isSuccess@ErrorCode@icu_62@@QEBACXZ (public: signed char __cdecl icu_62::ErrorCode::isSuccess(void)const ) imported in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nunicode_script_op.lo.lib(unicode_script_op.obj) : warning LNK4217: locally defined symbol ?reset@ErrorCode@icu_62@@QEAA?AW4UErrorCode@@XZ (public: enum UErrorCode __cdecl icu_62::ErrorCode::reset(void)) imported in function \"public: virtual void __cdecl tensorflow::UnicodeScriptOp::Compute(class tensorflow::OpKernelContext *)\" (?Compute@UnicodeScriptOp@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nicuuc.lib(udata.obj) : warning LNK4049: locally defined symbol icudt62_dat imported\r\narithmetic_optimizer.lib(arithmetic_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nlayout_optimizer.lib(layout_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\npin_to_host_optimizer.lib(pin_to_host_optimizer.obj) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nLINK : fatal error LNK1201: error writing to program database 'C:\\users\\dl\\_bazel_dl\\hd37h63o\\execroot\\org_tensorflow\\bazel-out\\x64_windows-dbg\\bin\\tensorflow\\libtensorflow_cc.pdb'; check for insufficient disk space, invalid path, or insufficient privilege", "comments": ["Just to verify did you get chance to follow instructions from TensorFlow [website](https://www.tensorflow.org/install/source_windows) .Please, let us know. \r\nAlso, Provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31610\">No</a>\n", "Hi, I also ran into the same issue. I am trying to build Tensorflow sources in optimised mode, with  debug information, so that I can get some debug information in the form of pdb files. \r\nInformation:\r\nSystem : Windows 10\r\nTensorflow : 1.13.1\r\nBazel : 0.25.1\r\nPython : 3.6.8\r\nMicrosoft Visual Studio Build Tools 2019(Not using 2017 or 2015 Build tools)\r\nCUDA/GPU/ROCm : no\r\nOverride Eigen strong inline : Yes\r\nArch: AVX2\r\n\r\nCommands:\r\n1. python ./configure.py\r\n2. bazel build --define=tensorflow_mkldnn_contraction_kernel=0 --local_ram_resources=2048 -c opt --copt=/Z7 --copt=/FS  --linkopt=/DEBUG --strip=never --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nError:\r\nLINK : fatal error LNK1201: error writing to program database 'C:\\users\\tensorflow\\_bazel_tensorflow\\a7oebymx\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\python\\_pywrap_tensorflow_internal.pdb'; check for insufficient disk space, invalid path, or insufficient privilege\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3959.292s, Critical Path: 1017.43s\r\nINFO: 4150 processes: 4150 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nMore info:\r\nThe size of the pdb: C:\\users\\tensorflow\\_bazel_tensorflow\\a7oebymx\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\python\\_pywrap_tensorflow_internal.pdb was just 4GB and there was enough disk space available. \r\nCAn somebody help me give a better insight of the issue? \r\n", "how to fix it?", "This is happening because the .pdb file is hitting 4.0 GB, which seems to be some kind of hard max on pdb files.  The workaround is not to use pdb's at all.  Compile with `--copt /Z7` and `--linkopt /DEBUG:FASTLINK`.\r\n\r\n`/Z7` puts the debugging information into the obj files themselves, while `/DEBUG:FASTLINK` creates an \"index\" pdb into those objs.\r\n", "Just in case anyone else finds this article - LINK1201 can also occur if the output pdb file is in use.  I had a project open in Visual Studio 2019 that used zlibd.dll and zlibd.pdb.  At the same time I was trying to run vcpkg to recreate those files and got this error.  It was resolved by closing all instances of Visual Studio (and its zombie debugging consoles), then running vcpkg.  The zlibd.pdb file was only 1MB in size.  "]}, {"number": 31609, "title": "\"Init node weights/Assign doesn't exist in graph\" happens when use convert in tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: Tensorflow nightly\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 7/10\r\n\r\n**Describe the problem**\r\nWhen I tried to convert a TensorFlow GraphDef into a TensorFlow Lite FlatBuffer from a tf.Session object, a error happend such like this:\r\n`2019-08-14 16:01:23.946453: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-08-14 16:01:23.947157: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node weights/Assign doesn't exist in graph\r\n`\r\nand my code all showed below:\r\n\r\n\r\n    def main(_):\r\n\r\n      def loss_function(weight, logits, labels):\r\n        labels = tf.one_hot(labels,4)\r\n        labels = tf.cast(labels, tf.float32)\r\n        first = tf.reduce_sum(tf.multiply(-labels, logits),1)\r\n        second_0 = tf.add(tf.exp(logits[:,0]),tf.exp(logits[:,1]))\r\n        second_1 = tf.add(tf.exp(logits[:,2]),tf.exp(logits[:,3]))\r\n        log = tf.log(tf.add(second_1,second_0))\r\n        weight = tf.transpose(tf.reduce_sum(tf.multiply(labels, weight),1))\r\n        output = tf.multiply(weight,tf.add(first,log))\r\n\r\n        return output\r\n\r\n      def normalize(stft):\r\n        stft_1 = numpy.empty([stft.shape[0],128,128])\r\n        stft_2 = numpy.empty([stft_1.shape[0],stft_1.shape[1],stft_1.shape[2],1])\r\n        for i in range(stft_1.shape[0]):\r\n          image = Image.fromarray(stft[i,:,:])\r\n          image = image.resize([128,128])\r\n          stft_1[i,:,:] = numpy.array(image)\r\n\r\n          min = numpy.min(stft_1[i,:,:])\r\n          max = numpy.max(stft_1[i,:,:])\r\n          stft_1[i,:,:] = (stft_1[i,:,:]-min)/(max-min)\r\n          stft_2[i,:,:,:] = stft_1[i,:,:].reshape((stft_1.shape[1],stft_1.shape[2],1))\r\n        return stft_2  \r\n    # Get the data.\r\n    \r\n    stft_training, mfcc_training, labels_training = joblib.load(open(FLAGS.input, mode='rb'))\r\n    stft_test, mfcc_test, labels_test = joblib.load(open(FLAGS.test, mode='rb'))\r\n\r\n    stft_test = numpy.array(stft_test)\r\n    mfcc_test = numpy.array(mfcc_test)\r\n    labels_test = numpy.array(labels_test)\r\n    stft_test = normalize(stft_test)\r\n    mfcc_test = normalize(mfcc_test)\r\n\r\n    stft_training = numpy.array(stft_training)\r\n    mfcc_training = numpy.array(mfcc_training)\r\n    labels_training = numpy.array(labels_training)\r\n    stft_training = normalize(stft_training)\r\n    mfcc_training = normalize(mfcc_training)\r\n\r\n    stft_shape = stft_training.shape\r\n    stft_shape = (None, stft_shape[1], stft_shape[2], 1)\r\n\r\n    mfcc_shape = mfcc_training.shape\r\n    mfcc_shape = (None, mfcc_shape[1], mfcc_shape[2], 1)\r\n\r\n    labels_shape = labels_training.shape\r\n    labels_shape = (None)\r\n\r\n    stft_placeholder = tf.placeholder(stft_training.dtype, stft_shape)\r\n    labels_placeholder = tf.placeholder(labels_training.dtype, labels_shape)\r\n    mfcc_placeholder = tf.placeholder(mfcc_training.dtype, mfcc_shape)\r\n    \r\n    dataset_training = tf.data.Dataset.from_tensor_slices((stft_placeholder, mfcc_placeholder, labels_placeholder))\r\n    dataset_training  = dataset_training.apply(\r\n        tf.data.experimental.shuffle_and_repeat(len(stft_training), None))  \r\n    dataset_training  = dataset_training.batch(BATCH_SIZE)\r\n    dataset_training  = dataset_training.prefetch(1)\r\n    iterator_training = dataset_training.make_initializable_iterator()\r\n    next_element_training = iterator_training.get_next()\r\n    num_epochs = FLAGS.epochs\r\n\r\n      train_size = labels_training.shape[0]\r\n\r\n      with tf.name_scope('input'):\r\n        stft = tf.placeholder(\r\n            name=\"stft\",\r\n            dtype=data_type(),\r\n            shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n        mfcc = tf.placeholder(\r\n            name=\"mfcc\",\r\n            dtype=data_type(),\r\n            shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n        labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\r\n\r\n      with tf.name_scope('test_input'):\r\n        stft_t = tf.placeholder(\r\n            data_type(),\r\n            shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n        mfcc_t = tf.placeholder(\r\n            data_type(),\r\n            shape=(EVAL_BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WEITH, NUM_CHANNELS))\r\n\r\n      model = BRN()\r\n      logits = model.forward(stft, mfcc)\r\n      logits_ = tf.add(0.,logits,name=\"logits_\")\r\n      try:\r\n        scalar_summary = tf.scalar_summary\r\n        SummaryWrite = tf.train.SummaryWrite\r\n        merge_summary = tf.merge_summary\r\n      except:\r\n        scalar_summary = tf.summary.scalar\r\n        SummaryWrite = tf.summary.FileWriter\r\n        merge_summary = tf.summary.merge\r\n      with tf.name_scope('loss'):\r\n        weights = [1.0, 1.7, 4.1, 5.7]\r\n         mid = loss_function(weights, logits=logits, labels=labels)\r\n        loss = tf.reduce_sum(mid)\r\n    \r\n        loss_summary = scalar_summary('loss', loss)\r\n        regularizers = (tf.nn.l2_loss(model.conv1_weights) + tf.nn.l2_loss(model.conv2_weights) +\r\n                    tf.nn.l2_loss(model.fc_weights) + tf.nn.l2_loss(model.fc_biases))\r\n\r\n        batch = tf.Variable(0, dtype=data_type())\r\n \r\n      with tf.name_scope('train'):\r\n\r\n        optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\r\n      train_prediction = tf.nn.softmax(logits)\r\n      eval_prediction = tf.nn.softmax(model.forward(stft_t, mfcc_t))\r\n      start_time = time.time()\r\n\r\n      def eval_in_batches(stft_data, mfcc_data, sess, type):\r\n        size = stft_data.shape[0]\r\n        if size < EVAL_BATCH_SIZE:\r\n          raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\r\n        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\r\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\r\n          end = begin + EVAL_BATCH_SIZE\r\n          if end <= size:\r\n            if type == 'train':\r\n              predictions[begin:end, :] = sess.run(\r\n                  train_prediction,\r\n                  feed_dict={stft: stft_data[begin:end, ...], mfcc: mfcc_data[begin:end, ...]})\r\n            else: \r\n              predictions[begin:end, :] = sess.run(\r\n                  eval_prediction,\r\n                  feed_dict={stft_t: stft_data[begin:end, ...], mfcc_t: mfcc_data[begin:end, ...]})\r\n          else:\r\n            if type == 'train':\r\n              batch_predictions = sess.run(\r\n                  train_prediction,\r\n                  feed_dict={stft: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc: mfcc_data[-EVAL_BATCH_SIZE:, ...]})\r\n            else:\r\n               batch_predictions = sess.run(\r\n                  eval_prediction,\r\n                  feed_dict={stft_t: stft_data[-EVAL_BATCH_SIZE:, ...], mfcc_t: mfcc_data[-EVAL_BATCH_SIZE:, ...]})\r\n            predictions[begin:, :] = batch_predictions[begin - size:, :]\r\n        return predictions\r\n\r\n\r\n      config = tf.ConfigProto()\r\n      config.gpu_options.allow_growth = True  \r\n\r\n      with tf.Session(config=config) as sess:\r\n   \r\n        tf.global_variables_initializer().run()\r\n\r\n        merged = tf.summary.merge_all()\r\n        writer = SummaryWrite(FLAGS.logs + 'train', sess.graph)\r\n        sess.run(iterator_training.initializer, feed_dict={stft_placeholder:stft_training,\r\n                          mfcc_placeholder:mfcc_training,\r\n                         labels_placeholder:labels_training})\r\n\r\n        for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\r\n\r\n          batch_stft, batch_mfcc, batch_labels = sess.run(next_element_training)\r\n  \r\n          feed_dict = {stft: batch_stft,\r\n                       mfcc: batch_mfcc,\r\n                       labels: batch_labels}\r\n          sess.run(optimizer, feed_dict=feed_dict)\r\n          if step % EVAL_FREQUENCY == 0:\r\n\r\n        summary, l = sess.run([merged, loss],\r\n                                      feed_dict=feed_dict)\r\n            writer.add_summary(summary, step)\r\n            elapsed_time = time.time() - start_time\r\n            start_time = time.time()\r\n            rate, acc = error_rate(eval_in_batches(stft_training, mfcc_training, sess, 'train'), labels_training)\r\n            acc_summary = scalar_summary('accuracy', acc)\r\n            print('Step %d (epoch %.2f), Minibatch loss: %.3f, Minibatch error: %.1f%%, Accuracy:%.4f' %\r\n              (step, float(step) * BATCH_SIZE / train_size,\r\n              l,rate, acc))\r\n            sys.stdout.flush()\r\n            test_error, test_acc = error_rate(eval_in_batches(stft_test, mfcc_test, sess, 'test'), labels_test)\r\n            print('Testset error: %.1f%%, Accuracy:%.4f' % (test_error, test_acc))\r\n\r\n    converter = tf.lite.TFLiteConverter.from_session(sess, [stft,mfcc], [logits_])\r\n    tflite_model = converter.convert()\r\n    open(\"BRN.tflite\", \"wb\").write(tflite_model)\r\n        \r\n    writer.close()\r\n\r\nWhen I run the official demo of converting a TensorFlow GraphDef into a TensorFlow Lite FlatBuffer from a tf.Session object, the error also happens. Does that ok? I mean, can I use the weight trained in TensorFlow Lite? or the file doesn't save the weight?\r\n\r\n", "comments": ["@mmmmayi, Please provide the complete code to reproduce the reported issue. Thanks!", "> @mmmmayi, Please provide the complete code to reproduce the reported issue. Thanks!\r\n\r\ni have edited it and add up the whole useful code", "@mmmmayi I tried replicating the issue, looks like some entities are not defined `NameError: name 'joblib' is not defined`. Please help us reproduce the issue. Thanks!", "> @mmmmayi I tried replicating the issue, looks like some entities are not defined `NameError: name 'joblib' is not defined`. Please help us reproduce the issue. Thanks!\r\n\r\nI'm so sorry, I misunderstand what you need, then you can find the whole code and database(wavelet_stft.p and wavelet_stft_test.p) in my google cloud: \r\nhttps://drive.google.com/open?id=1DfV7WPJymj66jJ13ds6javg3GSLPhwyr\r\nbut for your convenience,  you can also use the official demo, it caused the same error in my condition:\r\n```\r\nimport tensorflow as tf\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nvar = tf.get_variable(name=\"weights\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + var\r\nout = tf.identity(val, name=\"out\")\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  converter = tf.lite.TFLiteConverter.from_session(sess, [img], [out])\r\n  tflite_model = converter.convert()\r\n  open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n", "@mmmmayi, Thanks for providing the code.\r\nI tried reproducing the issue on Colab with official demo code but i didn't receive any error. Please take a look at gist [here](https://colab.research.google.com/drive/1GYSh6qXCGzz4pOWU3Ht0a_raXYOfToKp). let us know. Thanks!", "> @mmmmayi, Thanks for providing the code.\r\n> I tried reproducing the issue on Colab with official demo code but i didn't receive any error. Please take a look at gist [here](https://colab.research.google.com/drive/1GYSh6qXCGzz4pOWU3Ht0a_raXYOfToKp). let us know. Thanks!\r\n\r\nThanks for your reply, but when I try the error still exist, I don't know why. Here are the whole log:\r\n``` \r\n(lite) mmmmayi@BICASL:~/Document/test$ python tflite_test.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0822 15:05:10.332202 140343267317568 module_wrapper.py:136] From /home/mmmmayi/.local/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\n2019-08-22 15:05:10.340097: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-08-22 15:05:10.345417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-08-22 15:05:10.465133: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d4cbf7c60 executing computations on platform CUDA. Devices:\r\n2019-08-22 15:05:10.465156: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-08-22 15:05:10.484734: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500000000 Hz\r\n2019-08-22 15:05:10.485570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d4ccb8b70 executing computations on platform Host. Devices:\r\n2019-08-22 15:05:10.485611: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-22 15:05:10.486958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:05:10.487298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:05:10.489793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:05:10.491786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:05:10.492278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:05:10.495154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:05:10.496298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:05:10.498792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:05:10.500421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:05:10.500452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:05:10.501338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 15:05:10.501348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-22 15:05:10.501354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-22 15:05:10.502606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6940 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n2019-08-22 15:05:10.824691: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-08-22 15:05:10.824793: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-08-22 15:05:10.826763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:05:10.826831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:05:10.826863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:05:10.826885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:05:10.826905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:05:10.826927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:05:10.826947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:05:10.826969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:05:10.828266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:05:10.828291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 15:05:10.828297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-22 15:05:10.828303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-22 15:05:10.829652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6940 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n2019-08-22 15:05:10.831310: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-08-22 15:05:10.831328: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-08-22 15:05:10.831334: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nW0822 15:05:10.833272 140343267317568 deprecation.py:323] From /home/mmmmayi/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0822 15:05:10.833422 140343267317568 deprecation.py:323] From /home/mmmmayi/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:275: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2019-08-22 15:05:10.840123: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-08-22 15:05:10.840224: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-08-22 15:05:10.841963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:05:10.842020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:05:10.842043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:05:10.842064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:05:10.842083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:05:10.842104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:05:10.842123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:05:10.842144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:05:10.844217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:05:10.844260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 15:05:10.844276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-22 15:05:10.844289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-22 15:05:10.846621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6940 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n2019-08-22 15:05:10.847889: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node weights/Assign doesn't exist in graph\r\n(lite) mmmmayi@BICASL:~/Document/test$ python tflite_test.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0822 15:07:51.153700 140406728853312 module_wrapper.py:136] From /home/mmmmayi/.local/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\n2019-08-22 15:07:51.162202: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-08-22 15:07:51.173639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:07:51.173781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:07:51.174777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:07:51.175635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:07:51.175847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:07:51.176969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:07:51.177846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:07:51.180346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:07:51.181777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:07:51.182033: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-08-22 15:07:51.295496: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557122a73a90 executing computations on platform CUDA. Devices:\r\n2019-08-22 15:07:51.295533: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-08-22 15:07:51.316734: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500000000 Hz\r\n2019-08-22 15:07:51.317891: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55712309cbf0 executing computations on platform Host. Devices:\r\n2019-08-22 15:07:51.317934: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-22 15:07:51.319601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:07:51.319669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:07:51.319697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:07:51.319722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:07:51.319746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:07:51.319770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:07:51.319793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:07:51.319818: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:07:51.322084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:07:51.322145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:07:51.324135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 15:07:51.324159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-22 15:07:51.324173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-22 15:07:51.326230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6940 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n2019-08-22 15:07:51.723521: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-08-22 15:07:51.723724: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-08-22 15:07:51.726067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:07:51.726138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:07:51.726168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:07:51.726195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:07:51.726221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:07:51.726247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:07:51.726274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:07:51.726300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:07:51.728298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:07:51.728319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 15:07:51.728325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-22 15:07:51.728331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-22 15:07:51.729443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6940 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n2019-08-22 15:07:51.730986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-08-22 15:07:51.731002: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-08-22 15:07:51.731007: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nW0822 15:07:51.733329 140406728853312 deprecation.py:323] From /home/mmmmayi/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0822 15:07:51.733462 140406728853312 deprecation.py:323] From /home/mmmmayi/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:275: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2019-08-22 15:07:51.738129: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-08-22 15:07:51.738175: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-08-22 15:07:51.739189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:b3:00.0\r\n2019-08-22 15:07:51.739222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-22 15:07:51.739231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-22 15:07:51.739239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-22 15:07:51.739248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-22 15:07:51.739255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-22 15:07:51.739263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-22 15:07:51.739271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-22 15:07:51.740303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-08-22 15:07:51.740323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-22 15:07:51.740328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-08-22 15:07:51.740333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-08-22 15:07:51.741429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6940 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n2019-08-22 15:07:51.742438: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node weights/Assign doesn't exist in graph\r\n```\r\n", "@mmmmayi I couldn't reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ed91d93d0fa43a8a210881da87443b62/tf_31609_tflite.ipynb). \r\n\r\n1. Based on the error trace, you are running the code with `tensorflow-gpu` right? Can you try with 'tensorflow-cpu` only to see whether the issue caused due to any `cuda` related file. \r\n\r\n2. Can you run the gist provided here and see what you get as output?\r\n\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31609\">No</a>\n", "I think the problem is about get_variable, because if I use tf.placeholder to get var in the demo code, there isn't error, otherwise, the tf.get_variable leads to it", "> @mmmmayi I couldn't reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ed91d93d0fa43a8a210881da87443b62/tf_31609_tflite.ipynb).\r\n> \r\n> 1. Based on the error trace, you are running the code with `tensorflow-gpu` right? Can you try with 'tensorflow-cpu`only to see whether the issue caused due to any`cuda` related file.\r\n> 2. Can you run the gist provided here and see what you get as output?\r\n> \r\n> Thanks!\r\n\r\nit still exists when I use CPU, and you can get the output of it from the link(named converted_model3.tflite):\r\nhttps://drive.google.com/drive/folders/1Ds8ihFsz9K5ZKPCS0FPQ2_qZBUOn3to3?usp=sharing", "@mmmmayi 1. Did you ran the gist I created? Do you see any error?\r\n2. Are you using `tf-nightly` right? Just a confirmation.   Thanks!", "@mmmmayi I am currently facing the same issue, have you found a solution ?\r\nThanks !", "> @mmmmayi I am currently facing the same issue, have you found a solution ?\r\n> Thanks !\r\n\r\nI just fixed this problem by using tf 1.13.1 but not tf 1.14...", "@DragonX081mk2 Can you try `TF1.15.0rc3` and let us know whether the issue persists with the latest version. As `TF1.14` was not built properly, there were lot of issues when using it. Hence, TF team released new version `TF1.15`. Thanks!", "> @DragonX081mk2 Can you try `TF1.15.0rc3` and let us know whether the issue persists with the latest version. As `TF1.14` was not built properly, there were lot of issues when using it. Hence, TF team released new version `TF1.15`. Thanks!\r\n\r\nBefore trying 1.13.1, I have tried the 1.15.0 rc3, this problem existed too.. ", "> @mmmmayi I am currently facing the same issue, have you found a solution ?\r\n> Thanks !\r\n\r\nhi, actually I didn't figure it out, but it seems have no effect to the result, because I can still get the tflite model even with this error", "@mmmmayi Can we close this issue? I don't see any error when I used `TF1.15.0`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/bc515f885097fc6fd01076a3859e7e5b/tf_31609_tflite.ipynb). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31609\">No</a>\n"]}, {"number": 31608, "title": "Auto-Configuration Error: Couldn't find undname.exe under C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\", "body": "**System information**\r\n- OS Platform : Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.13\r\n- Python version:  python3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- Compiler version (if compiling from source): VS2019,16.2,MSVC++=14.22\r\n\r\n**Describe the problem**\r\nAuto-Configuration Error: Couldn't find undname.exe under C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\, please check your VC installation and set BAZEL_VC environment variable correctly.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1)conda create -n tensorflow-1.13.1-without-mkl python=3.6 anaconda\r\n2)conda activate tensorflow-1.13.1-without-mkl\r\n3)Install MSYS2\r\nset environmental variable, under PATH -> C:\\msys64\\usr\\bin\r\n4)Install JDK12 and set environmental variable : JAVA_HOME = C:\\Program Files\\Java\\jdk-12\r\n5)pacman -Syu zip unzip\r\n6)pacman -Syuu --noconfirm patch\r\n7)set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\r\n8) set BAZEL_SH=C:\\msys64\\usr\\bin\\bash.exe\r\n9) git clone https://github.com/tensorflow/tensorflow.git -b r1.4\r\n10) python ./configure.py\r\n\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\python.exe]:\r\nFound possible Python library paths:\r\n  C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\lib\\site-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\n\r\n11)bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: ca6515c1-83e0-4ddf-a065-733711b4a9ff\r\nERROR: C:/users/zen2-/tensorflow/tensorflow/python/BUILD:4172:1: no such package '@local_config_def_file_filter//': Traceback (most recent call last):\r\n        File \"C:/users/zen2-/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl\", line 37\r\n                auto_configure_fail((\"Couldn't find undname.exe unde...))\r\n        File \"C:/users/zen2-/_bazel_zen2_microsoft/ldf7kfdv/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\", line 109, in auto_configure_fail\r\n                fail((\"\\n%sAuto-Configuration Error:%...)))\r\n\r\nAuto-Configuration Error: Couldn't find undname.exe under C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\, please check your VC installation and set BAZEL_VC environment variable correctly.\r\n and referenced by '//tensorflow/python:pywrap_tensorflow_filtered_def_file'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@local_config_def_file_filter//': Traceback (most recent call last):\r\n        File \"C:/users/zen2-/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl\", line 37\r\n                auto_configure_fail((\"Couldn't find undname.exe unde...))\r\n        File \"C:/users/zen2-/_bazel_zen2_microsoft/ldf7kfdv/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\", line 109, in auto_configure_fail\r\n                fail((\"\\n%sAuto-Configuration Error:%...)))\r\n\r\nAuto-Configuration Error: Couldn't find undname.exe under C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\, please check your VC installation and set BAZEL_VC environment variable correctly.\r\nINFO: Elapsed time: 0.713s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (5 packages loaded, 20 targets configured)\r\n    currently loading: tensorflow/lite/schema ... (3 packages)\r\n    Fetching @cython; fetching\r\n    Fetching @swig; fetching\r\n    Fetching @local_config_git; Restarting.\r\n    Fetching @grpc; fetching\r\n    Fetching @local_config_def_file_filter; fetching\r\n    Fetching @flatbuffers; fetching\r\n    Fetching @eigen_archive; fetching\r\n    Fetching @icu; fetching ... (9 fetches)\r\n\r\nPlease correct me in case wrong . ", "comments": ["I do not think bazel 0.21 has any support for visual studio 2019.\r\nAFAICR, you need at least bazel 0.26 to do anything with MSVC2019.\r\nAdding @laszlocsomor @meteorcloudy ", "Correct.", "Error After bazel 0.28.1 and tf-r1.14\r\n\r\nERROR: C:/users/zen2-/rev1/tensorflow/tensorflow/compiler/mlir/lite/python/BUILD:12:1: C++ compilation of rule '//tensorflow/compiler/mlir/lite/python:graphdef_to_tfl_flatbuffer' failed (Exit 2)\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(144): error C2059: syntax error: ':'\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(147): error C2143: syntax error: missing ';' before '}'\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(148): error C2065: 'pass_config': undeclared identifier\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(148): error C2065: 'pm': undeclared identifier\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(148): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(148): error C2365: 'tensorflow::AddTFToTFLConversionPasses': redefinition; previous definition was 'function'\r\n.\\tensorflow/compiler/mlir/lite/tf_tfl_passes.h(34): note: see declaration of 'tensorflow::AddTFToTFLConversionPasses'\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(148): error C2513: 'tensorflow::AddTFToTFLConversionPasses': no variable declared before '='\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(150): error C2059: syntax error: 'return'\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(156): error C2059: syntax error: '}'\r\ntensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc(156): error C2143: syntax error: missing ';' before '}'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3035.976s, Critical Path: 1511.08s\r\nINFO: 10156 processes: 10156 local.\r\nFAILED: Build did NOT complete successfully", "Tensorflow-1.14.0 needs Bazel 0.24.1 (https://www.tensorflow.org/install/source#linux).\r\nSorry, looks like you can't use VS 2019 until TensorFlow supports at least Bazel 0.26", "If you can install VS 2017 and set `BAZEL_VC`, and use Bazel 0.24.1, it should work.", "@laszlocsomor TF-r1.14 failed with VS 2017 and Bazel 0.24.1\r\n\r\nERROR: C:/users/zen2-/rev2/tensorflow/tensorflow/python/keras/api/BUILD:12:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/zen2-/_bazel_zen2_microsoft/z7ecj6tr/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl;C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\Library\\mingw-w64\\bin;C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\Library\\usr\\bin;C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\Library\\bin;C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\Scripts;C:\\Users\\zen2-\\Anaconda2\\envs\\tensorflow-1.13.1-without-mkl\\bin;C:\\Users\\zen2-\\Anaconda2\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\Strawberry\\c\\bin;C:\\Strawberry\\perl\\site\\bin;C:\\Strawberry\\perl\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\CMake\\bin;C:\\Program Files\\AMD\\AMDuProf\\bin;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.3.0\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Ruby26-x64\\bin;C:\\Users\\zen2-\\Anaconda2;C:\\Users\\zen2-\\Anaconda2\\Library\\mingw-w64\\bin;C:\\Users\\zen2-\\Anaconda2\\Library\\usr\\bin;C:\\Users\\zen2-\\Anaconda2\\Library\\bin;C:\\Users\\zen2-\\Anaconda2\\Scripts;C:\\Users\\zen2-\\Anaconda3;C:\\Users\\zen2-\\Anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\zen2-\\Anaconda3\\Library\\usr\\bin;C:\\Users\\zen2-\\Anaconda3\\Library\\bin;C:\\Users\\zen2-\\Anaconda3\\Scripts;C:\\Users\\zen2-\\AppData\\Roaming\\local\\bin;C:\\Users\\zen2-\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Program Files (x86)\\Notepad++\\;C:\\Strawberry\\perl\\bin\\;C:\\Program Files\\Git\\bin\\;C:\\Users\\zen2-\\AppData\\Local\\bin\\NASM\\;C:\\Program Files\\7-Zip\\;C:\\Users\\zen2-\\AppData\\Local\\Programs\\Python\\Python37-32\\;C:\\msvc\\src\\ExternalApis\\Windows\\10\\sdk\\10.0\\bin\\x64\\;C:\\Users\\zen2-\\.dotnet\\tools;C:\\msys64\\usr\\bin\\;C:\\Program Files (x86)\\IntelSWTools\\VTune Amplifier 2019\\bin64\\;C:\\bazel;\r\n    SET PYTHON_BIN_PATH=C:/Users/zen2-/Anaconda2/envs/tensorflow-1.13.1-without-mkl/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/zen2-/Anaconda2/envs/tensorflow-1.13.1-without-mkl/lib/site-packages\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.exe  --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api --apiname=keras --apiversion=1  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/layers/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/mixed_precision/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/mixed_precision/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/optimizers/schedules/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/keras/wrappers/scikit_learn/__init__.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\zen2-\\AppData\\Local\\Temp\\Bazel.runfiles_rjl2ejn7\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\zen2-\\AppData\\Local\\Temp\\Bazel.runfiles_rjl2ejn7\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"\\\\?\\C:\\Users\\zen2-\\AppData\\Local\\Temp\\Bazel.runfiles_rjl2ejn7\\runfiles\\org_tensorflow\\tensorflow\\python\\keras\\__init__.py\", line 32, in <module>\r\n    from tensorflow.python.keras import datasets\r\n  File \"\\\\?\\C:\\Users\\zen2-\\AppData\\Local\\Temp\\Bazel.runfiles_rjl2ejn7\\runfiles\\org_tensorflow\\tensorflow\\python\\keras\\datasets\\__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras.datasets import imdb\r\n  File \"\\\\?\\C:\\Users\\zen2-\\AppData\\Local\\Temp\\Bazel.runfiles_rjl2ejn7\\runfiles\\org_tensorflow\\tensorflow\\python\\keras\\datasets\\imdb.py\", line 25, in <module>\r\n    from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq\r\n  File \"\\\\?\\C:\\Users\\zen2-\\AppData\\Local\\Temp\\Bazel.runfiles_rjl2ejn7\\runfiles\\org_tensorflow\\tensorflow\\python\\keras\\preprocessing\\__init__.py\", line 21, in <module>\r\n    import keras_preprocessing\r\nModuleNotFoundError: No module named 'keras_preprocessing'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 57.284s, Critical Path: 54.35s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\nI have tried pip install keras --user but didn't work for me.\r\n", "So the C++ compilation succeeds, that's good news!\r\nSorry, but I don't know about the Python error. @gunan , does it ring a bell?", "Looks like missing dependencies.\r\nPlease make sure you follow the guide here to install all the prerequisites:\r\nhttps://www.tensorflow.org/install/source#setup_for_linux_and_macos", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31608\">No</a>\n", "@mayadav1 so how did you resolve the problem eventually? I'm currently on Bazel 3.2 but still got pretty much the same problem as yours. My BAZEL_VC was set to F:\\C\\vs\\Enterprise\\VC\\Tools\\MSVC\\14.24.28314\\bin\\Hostx86\\x86, where there literally is a undname.exe but bazel build keeps giving me auto-config error", "@AtlantaPepsi did you make progress here? Same thing happens to me now with tf2.4.0, bazel 3.1 and MSVC 2019"]}, {"number": 31607, "title": "Keras Models with tf.sparse.sparse_dense_matmul can't be saved - Not JSON Serializable ", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Version 1607\r\n- TensorFlow version: 2.0.0-beta1\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nError:\r\n```\r\nException has occurred: TypeError\r\n('Not JSON Serializable:', b'\\n\\ttranspose\\x12\\tTranspose\\x1a\\x10dense_1/Identity\\x1a\\x0etranspose/perm*\\x0b\\n\\x05Tperm\\x12\\x020\\x03*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n  File \"C:\\Visual_Studio_Codes\\Sparse_Save.py\", line 30, in <module>\r\n    model.save(\"model.h5\")\r\n```\r\n\r\n**Describe the expected behavior**\r\nSave without error\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import (Dense, Input, Lambda)\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom scipy import sparse\r\nimport numpy as np\r\n\r\n\r\ndef layer_lambda(input_x):\r\n    sparse = input_x[0]\r\n    dense = input_x[1]\r\n    dense = tf.transpose(dense)\r\n    y = tf.sparse.sparse_dense_matmul(sparse, dense)\r\n    return tf.transpose(y)\r\n\r\n\r\ndense_mat = np.eye(30, 30, dtype=np.float32)\r\nsparse_mat = sparse.coo_matrix(dense_mat)\r\nsparse_indices = np.mat([sparse_mat.row, sparse_mat.col]).transpose()\r\nsparse_tensor = tf.SparseTensor(sparse_indices, sparse_mat.data, sparse_mat.shape)\r\n\r\nmodel = Sequential()\r\nmodel_input = Input(shape=(20,))\r\nx = Dense(20)(model_input)\r\nx = Dense(30)(x)\r\nx = Lambda(layer_lambda, output_shape=(None, 30, 30))([sparse_tensor, x])\r\nmodel = Model(model_input, x)\r\n\r\nmodel.predict([[np.ones(20)]])\r\n\r\nmodel.save(\"model.h5\")\r\n```", "comments": ["@Asorie Can you please take a look at the similar [issue](https://github.com/tensorflow/tensorflow/issues/27112) and let me know if it helps. Thanks!", "This is fixed with latest version of TF-2.0 nightly build. Thanks!\r\n", "All in [this issue](https://github.com/tensorflow/tensorflow/issues/27112) suggested solutions dont work in this case. \r\nI ended up using `pip install tf-nightly-gpu` to install tf-version `1.15.0.dev20190814`. This solved this issue"]}, {"number": 31606, "title": "[Intel MKL] set build 2.0 as default in Dockerfile.devel-mkl", "body": "We use three file to build the TF docker images in the derectory tensorflow/tools/ci_build/linux/mkl/.\r\nWe set build 2.0 as default in both set-build-env.py and build-dev-container.sh.\r\nBut set disable 2.0 as default in Dockerfile.devel-mkl.\r\nIn this situation, we always disable 2.0 build.\r\n\r\nThis PR fix this bug that we can build the right TF 2.0 image.", "comments": ["@angersson Can you help to take time to review this? thanks you.", "Can one of the admins verify this patch?", "@penpornk thanks for for comments, I have update it, please help to check.\r\nthanks."]}, {"number": 31605, "title": "QueueDequeueV2 Operation's device is different between tensorboard and timeline", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source \r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.18\r\n- GCC/Compiler version (if compiling from source):5.0\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory:P100\r\n\r\n\r\n**Describe the current behavior**\r\nOn tensorboard QueueDequeueV2 Operation's device is GPU:1.\r\n![image](https://user-images.githubusercontent.com/8842010/62991048-b992ae80-be80-11e9-9dfe-e0661e29d9ae.png)\r\nBut on timeline, it is CPU:0\r\n![image](https://user-images.githubusercontent.com/8842010/62991076-c9aa8e00-be80-11e9-9876-0066bdc71261.png)\r\n\r\n**Describe the expected behavior**\r\nWhich one is correct \uff1f\uff1fSet log_device_placement=True, we can see QueueDequeueV2 is on cpu(clone_1/fifo_queue_Dequeue: (QueueDequeueV2)/job:worker/replica:0/task:0/device:CPU:0)\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Same to this: https://github.com/tensorflow/models/issues/1390#issuecomment-301695244 and https://github.com/tensorflow/models/issues/2038\r\nBut we did not see the \"Ignoring device specification /device:GPU:X for node 'clone_X/fifo_queue_Dequeue'\" message.", "Tensorboard is incorrect. Tensorboard is shown based on user's python code. If we set an OP on a non-existing deive, such as 'gpu:100', we also can see it on tensorboard."]}, {"number": 31604, "title": " custom implementations: DEPTH_TO_SPACE.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 14.04):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (1.14.0):\r\n\r\n  Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, TANH. Here is a list of operators for which you will need custom implementations: DEPTH_TO_SPACE.\r\n\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jichangsheng,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "i want to convert pb file to tflite file using TFliteConverter of TensorFlow 1.14.0, it is failed because SubpixelConv2d in tensorlayer contained a custom op named \u201cDEPTH_TO_SPACE\u201d appeared.When I set \u201cconverter.allow_custom_ops=True\u201d it is successed to\nconverted to tflite file\uff0cbut when i use python to reuse\nthe tflie file to produce model\uff0cit is still faied showing \u201cdidnot find custom op for name \u201cDEPTH_TO_SPACE\u201d\uff0ci wonder how to slove this issue\uff1fThinks\n\n\n\n\n\n\n--\n\u53d1\u81ea\u6211\u7684\u7f51\u6613\u90ae\u7bb1\u624b\u673a\u667a\u80fd\u7248\n\n\n\n\u5728 2019-08-19 16:35:00\uff0cgadagashwini <notifications@github.com> \u5199\u9053\uff1a\n\n\n@jichangsheng,\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.", "Hi,\r\n\r\nDepthToSpace is added as built-in op recently into TF-Lite. Could you try run the conversion with the most recent tf-nightly build? Let me know if you encounter new issues, thanks!"]}, {"number": 31603, "title": "Lookahead optimizer", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nhttps://arxiv.org/abs/1907.08610\r\n\r\nThis is not a conventional optimizer as in tensorflow API. It observes through k iterations of optimizers(SGD, Adam), and linear interpolates entire weight into direction where fast weight after k iteration has reached. \r\n\r\nI have seen an implementation of this defining two ops for slow weight update and fast weight update. However I see some potential performance benefit if it was defined on backend. \r\n\r\n**Will this change the current api? How?**\r\n\r\nI believe not.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nTasks with heavy training load will benefit from faster learning rate.\r\n\r\n**Any Other info.**\r\n", "comments": ["Yep sounds good. @jinmel do you want to contribute to it?", "@tanzhenyu Sure", "preflight:\r\nhttps://github.com/jinmel/lookahead-tensorflow", "@jinmel thanks for your implementation, but it seems to be missing the sparse gradients update. Is this a limitation of this specific optimizer?", "@nova77 I still need to implement this feature.", "This optimizer might be a better fit in [TensorFlow Addons](https://www.github.com/tensorflow/addons), then graduated to TensorFlow Core. @jinmel, do you have an update on progress for the sparse gradients update?", "Hi all, addons holds a lookahead optimizer implementation.\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lookahead.py", "Closing this issue, as there is an implementation available in TensorFlow Addons. Thank you for the feature request!"]}, {"number": 31602, "title": "How the embedding matrix is optimized using DNNLinearCombinedClassifier", "body": "From the document: https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNLinearCombinedClassifier\r\n\r\nWe know the parameter includes linear_optimizer and dnn_optimizer that can be used to optimize the linear part and dnn part of the weights. However, it seems that the optimization of the weight of embedding matrix is not involved, such as embedding matrix of the wide and deep model. So my question is that is the optimization of the embedding matrix not involved in reality or is there any default way of optimizing the embedding matrix that I do not know\r\n\r\nestimator = DNNLinearCombinedClassifier(\r\n    # common settings\r\n    n_classes=n_classes,\r\n    weight_column_name=weight_column_name,\r\n    # wide settings\r\n    linear_feature_columns=[sparse_feature_a_x_sparse_feature_b],\r\n    linear_optimizer=tf.compat.v1.train.FtrlOptimizer(...),\r\n    # deep settings\r\n    dnn_feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],\r\n    dnn_hidden_units=[1000, 500, 100],\r\n    dnn_optimizer=tf.compat.v1.train.AdagradOptimizer(...))\r\n\r\n", "comments": ["@sjtusmartboy \r\nSorry, I didn't quite understand your question.\r\nBut DNNLinearCombinedClassifier definitely can be used to build and train the Wide and Deep models.", "@sjtusmartboy ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nAlso can you check feedback from @Leslie-Fang regarding the issue.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31602\">No</a>\n"]}, {"number": 31601, "title": "Fix build with latest flatbuffers", "body": "See https://github.com/tensorflow/tensorflow/pull/27504\r\n\r\nMotivation of this pull-request is to be possible to make tensorflow-lite with newest flatbuffers and fp16.", "comments": ["Can one of the admins verify this patch?", "@mattn Could you please check reviewer comments and keep us posted. Thanks!", ">They generally pin to stable releases only,\r\n\r\nIt mean farmhash is too?", "Can you give some description of why the newest farmhash commit is better than the old one? What does it bring for the PR", "The description of this pull-request is to be possible to build tensorflow lite with latest flatbuffers. (hopefuly, want to build on Windows) Recently, flatbuffers seems that included many improvement for performance. So I want to use tensorflow lite with latest flatbuffers.\r\n\r\nOne another my motivation is that we will be able to build tensorflow lite using mingw compiler without any patches. But the version of farmhash that is used in tensorflow is bits older than I want. \r\n\r\nhttps://github.com/google/farmhash/commit/6193375f121f1e2288f0d9282c4d9be1547e2c0f\r\n\r\n![image](https://user-images.githubusercontent.com/10111/64277426-47941d80-cf85-11e9-87e5-e8e7cd6bd747.png)\r\n\r\nThis commit of farmhash make be possible to build tensorflow lite with mingw compiler.\r\n", "Thank you, now it's clearer.\r\n\r\nIt will take some time before we can upgrade the flatbuffers dependency, but I'll come back to this PR once that's done. Sorry for the delay", "@mihaimaruseac Any update on this PR, please.", "I think we need to wait for 2 other internal changes to land first but they keep getting delayed.\r\n\r\nIn the meanwhile, we need to fix the conflicts", "Any update on this PR?\r\nI'm running into exactly the same issue where flatbuffers fetch fails.\r\n\r\n`\r\nPlease specify the location of python. [Default is C:\\Python37\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nPS C:\\Users\\alisy\\OneDrive\\Documents\\Tensorflow\\tensorflow> bazel build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/c:tensorflowlite_c\r\nStarting local Bazel server and connecting to it...\r\nINFO: Writing tracer profile to 'C:/users/alisy/_bazel_alisy/wlnshcl6/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=135\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Python37/lib/site-packages --python_path=C:/Python37/python.exe --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm in file c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --incompatible_windows_native_test_wrapper --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\alisy\\onedrive\\documents\\tensorflow\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'flatbuffers' which is a third_party_http_archive (rule definition at C:/users/alisy/onedrive/documents/tensorflow/tensorflow/third_party/repo.bzl:206:28):\r\n - C:/users/alisy/onedrive/documents/tensorflow/tensorflow/third_party/flatbuffers/workspace.bzl:6:5\r\n - C:/users/alisy/onedrive/documents/tensorflow/tensorflow/tensorflow/workspace.bzl:49:5\r\n - C:/users/alisy/onedrive/documents/tensorflow/tensorflow/tensorflow/workspace.bzl:92:5\r\n - C:/users/alisy/onedrive/documents/tensorflow/tensorflow/WORKSPACE:19:1\r\nINFO: Repository 'flatbuffers' used the following cache hits instead of downloading the corresponding file.\r\n * Hash '3f4a286642094f45b1b77228656fbd7ea123964f19502f9ecfd29933fd23a50b' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\nIf the definition of 'flatbuffers' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'flatbuffers':\r\n   java.io.IOException: Could not create symlink from C:/users/alisy/onedrive/documents/tensorflow/tensorflow/third_party/flatbuffers/build_defs.bzl to C:/users/alisy/_bazel_alisy/wlnshcl6/external/flatbuffers/build_defs.bzl: C:/users/alisy/_bazel_alisy/wlnshcl6/external/flatbuffers/build_defs.bzl (File exists)\r\nERROR: Skipping '//tensorflow/lite/c:tensorflowlite_c': no such package '@flatbuffers//': java.io.IOException: Could not create symlink from C:/users/alisy/onedrive/documents/tensorflow/tensorflow/third_party/flatbuffers/build_defs.bzl to C:/users/alisy/_bazel_alisy/wlnshcl6/external/flatbuffers/build_defs.bzl: C:/users/alisy/_bazel_alisy/wlnshcl6/external/flatbuffers/build_defs.bzl (File exists)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@flatbuffers//': java.io.IOException: Could not create symlink from C:/users/alisy/onedrive/documents/tensorflow/tensorflow/third_party/flatbuffers/build_defs.bzl to C:/users/alisy/_bazel_alisy/wlnshcl6/external/flatbuffers/build_defs.bzl: C:/users/alisy/_bazel_alisy/wlnshcl6/external/flatbuffers/build_defs.bzl (File exists)\r\nINFO: Elapsed time: 9.720s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/lite/c `"]}, {"number": 31600, "title": "Issue with model.predict()", "body": "\r\n- **Have I written custom code - Yes I have written some custom code\r\n- **OS Platform and Distribution - Ubuntu Server 18.04\r\n- **TensorFlow installed from - Installed by pip\r\n- **TensorFlow version (use command below)**: v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- **Python version**: 3.6.7\r\n- **Exact command to reproduce**: \r\n```\r\n        trainarray = np.array([0,0,0,0,0,0,0])\r\n        model = tf.keras.models.Sequential([\r\n          tf.keras.layers.Dense(7, input_shape= trainarray.shape),\r\n          tf.keras.layers.Dense(12),\r\n          tf.keras.layers.Dense(1)\r\n        ])\r\n        model.predict(np.array([0,0,0,0,0,0,0]))\r\n```\r\n\r\n### Describe the problem\r\nWhen I try to predict with the model created above I get the following error: \r\n\r\n> ValueError: Error when checking input: expected dense_input to have shape (7,) but got array with shape (1,)\r\n\r\nThe input array (np.array([0,0,0,0,0,0,0])) does obviously have the shape 7, but not 1, so to me this looks like a bug. I've tried to go under the hood to see what might be causing this, but I haven't found the cause yet.\r\n", "comments": ["Hi @imbakassinn, since dense layer takes `N-D` tensor (N >= 2) as input with first dimension=batch_size and obviously the `trainarray` is a `1-D` array, this is why the issue caused. So the workable example should be\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# suppose batch_size=1\r\n# trainarray is a 2-D array of shape [1, 7] of batch_size=1\r\ntrainarray = np.array([[0,0,0,0,0,0,0]])\r\n# excluding batch_size\r\n# for more details, please refer to the first example in\r\n# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\r\ninput_shape = [trainarray.shape[1]]\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Dense(7, input_shape=input_shape),\r\n  tf.keras.layers.Dense(12),\r\n  tf.keras.layers.Dense(1)\r\n])\r\n\r\nmodel.predict(np.array([[0,0,0,0,0,0,0]]))\r\n```\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense#input_shape", "@imbakassinn, did @WindQAQ's solution worked for you. Thanks!", "Looks like issue is resolved. Please reopen if still issue persists. Thanks!"]}, {"number": 31599, "title": "[INTEL MKL] Fix to MKLDNN build failure", "body": "The following commit has caused mkldnn build failure. https://github.com/tensorflow/tensorflow/commit/ddde447e792231cdf83b435b0eeb59dd59bf4044\r\nThis PR re-enables exceptions flags for MKLDNN library.", "comments": ["@penpornk Our MKLDNN source code uses C++ exceptions. However, a recent commit https://github.com/tensorflow/tensorflow/commit/ddde447e792231cdf83b435b0eeb59dd59bf4044\r\nbroke all mkldnn builds. This is an emergency fix. We will do a better fix later so as to remove `nocopts` attribute from cc_* rules.", "@scentini @dslomov Just wanted to check with you.\r\nI want to make sure we do not break bazel head, or break TF build latest bazel versions.\r\n\r\nWhat do you recommend?", "The flag that removes `nocopts` is not yet flipped, I don't expect this change to cause breakages.\r\n\r\n@mdfaijul, can you please follow up on bazelbuild/bazel#8706? Seems like your build is not using Bazel's default autoconfigured toolchain, I presume we can fix this in the relevant toolchain.", "@gunan Thanks a lot! Could you please point me to the possible fix in the toolchain.", "@penpornk @gunan Is there any issue with this PR? I'm not not familiar with `import/copybara` failure.", "Seems like this is not caused by a toolchain flag, but rather the [tf_mkl_kernel_library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L1504) adds the \"-fno-exceptions\" flag to its `copts`, and then filters it out through `nocopts`, I didn't see that coming :)\r\nI'll take care of that and re-removing the `nocopts`.", "@mdfaijul There are some problems with the internal testing systems. They seem unrelated to this PR. I'll keep you updated.\r\n@scentini That would be awesome. Thank you very much! :)", "@scentini @penpornk Thanks! Let us know if anything from our side is needed.", "> Approving now with the condition that within 1 week we need to remove \"nocopts\" from the build.\r\n@gunan You mentioned we could fix in toolchain. Could you please point us what the possible changes could be?", "@mdfaijul I did not mention that. @scentini did.", "@scentini We would like to resolve removing `nocopt` attribute. It would be  great to have some inputs from you on how to address that with toolchain.", "(Unrelated)\r\nJust a note for future reference: https://github.com/tensorflow/tensorflow/commit/ddde447e792231cdf83b435b0eeb59dd59bf4044 isn't in r2.0 so I don't need to cherrypick this PR into r2.0.", "@mdfaijul This was fixed in e5f8043, no further action is needed.", "@scentini Thank you again! :)"]}, {"number": 31598, "title": "In 1.14  does using  TF-TRT still show TensorRT nodes in Tensorboard graph ?", "body": "I am not seeing any TensorRT related nodes in  Tensorflow 1.14  in Tensorboard graph.   Is that supposed to be that way ?  In that case how do I make sure TF-TRT is in fact used ? ", "comments": ["In 1.14 TF-TRT was disable by default accidentally due to some release problem, and we're fixing the it in 1.15. In fact [1.15rc1](https://pypi.org/project/tensorflow-gpu/1.15.0rc1/) and [2.0rc1](https://pypi.org/project/tensorflow-gpu/2.0.0rc1/) were out and TF-TRT was enabled in both, please try with that and let me know.\r\n\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31598\">No</a>\n"]}, {"number": 31597, "title": "Handle depthwise backward filter conv op using cudnn group convolution", "body": "This PR deals with mapping depthwise backward filter convolutions to feature grouped backward filter convolutions. Currently, tf2xla maps depthwise backward filter convs to fwd batch_grouped convolutions. Batch group convolutions is a concept which is absent in GPUs and CPUs. Hence, a pass `convolution_group_converter` is being used to map these batch group convolutions to fwd convolutions with feature group count = 1 so that they can be thunked appropriately for GPU and CPU. This PR introduces a new pass, `depthwise_convolution_converter` which is used in lieu of `convolution_group_converter`, that maps these batch_group convolutions to the correct feature_group convolutions. These feature_group convolutions are then matched by `cudnn_conv_rewriter` and thunked to cudnn backward filter APIs. \r\n\r\ntf2xla, in order to map feature_group backward filter convs to fwd grouped convs, needs to transform the input via reshapes and transposes. `cudnn_conv_rewriter` then transforms the input back to its original form so that the cudnn APIs can consume it. This leaves the graph with a bunch of reshapes and transposes that can be eliminated. In this change I also introduce another pipeline of `AlgebraicSimplifiction` that eliminates all these reshapes and transposes to improve performance. \r\n\r\nThe best part about this PR is that it improves tf-xla performance for mobilenet_v2 by 25%  & mobilenet_v1 by around 20%. \r\n\r\nFollowing are graph dumps to illustrate the transform flow:\r\nBefore `depthwise_convolution_converter`:\r\n<img width=\"834\" alt=\"Screen_Shot_2019-07-31_at_11 00 45_AM\" src=\"https://user-images.githubusercontent.com/42984676/62986004-9cad9b00-bdee-11e9-947e-bebe66ae8147.png\">\r\n\r\nAfter `depthwise_convolution_converter`: \r\n<img width=\"661\" alt=\"Screen_Shot_2019-07-31_at_11 02 29_AM\" src=\"https://user-images.githubusercontent.com/42984676/62986041-be0e8700-bdee-11e9-818c-aba5ca417237.png\">\r\n\r\nAfter `cudnn_conv_rewriter`: \r\n<img width=\"641\" alt=\"Screen_Shot_2019-07-31_at_11 02 55_AM\" src=\"https://user-images.githubusercontent.com/42984676/62986076-daaabf00-bdee-11e9-8e1d-226f923267ac.png\">\r\n\r\nAfter `AlgebraicSimplification` pipeline: \r\n<img width=\"1268\" alt=\"Screen_Shot_2019-07-31_at_11 03 20_AM\" src=\"https://user-images.githubusercontent.com/42984676/62986106-f4e49d00-bdee-11e9-8acc-fb753d40380e.png\">\r\n\r\nGradient check tests for depthwise filter backward. In addition, I verified changes via loss tracking for mobilenet_v2 \r\n![Loss_comparison_of_tf-native_with_tf-xla_with_my_changes_for_mobilenet-v2__3_](https://user-images.githubusercontent.com/42984676/62986130-09289a00-bdef-11e9-81b0-1679ace5b928.png)\r\n", "comments": ["@AyanmoI Could you please resolve the conflicts? Thanks!", "FYI: We have a public holiday this Thursday, so unfortunately I can only review any changes on Friday.\r\nSorry for that :(", "Sorry I was caught up with something else yesterday :\\. I still need to incorporate 2 more of your comments. ", "Can one of the admins verify this patch?", "Hey @akuegel ...I incorporated all the comments...plz take a look :). Also can you plz take a look at https://github.com/tensorflow/tensorflow/pull/30775 ?", "@AyanmoI Several files got incorrect mode changes (0644 -> 0755). Mind to fix this?\r\n\r\n```\r\n mode change 100644 => 100755 tensorflow/compiler/xla/service/BUILD\r\n mode change 100644 => 100755 tensorflow/compiler/xla/service/gpu/BUILD\r\n mode change 100644 => 100755 tensorflow/compiler/xla/service/gpu/gpu_compiler.cc\r\n mode change 100644 => 100755 tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc\r\n mode change 100644 => 100755 tensorflow/compiler/xla/service/hlo_instruction.cc\r\n mode change 100644 => 100755 tensorflow/compiler/xla/service/hlo_instructions.h\r\n```", "@byronyi Hi..it seems `tensorflow/compiler/xla/service/gpu/gpu_compiler.cc` and `tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc` are already in 644. The rest are 755. Please correct me if I'm wrong. I will change the modes of the rest and submit.\r\n", "@AyanmoI Would it be better to add support thunking backward input convolution to cuDNN as well? We are wondering because separable conv2d does not yield expected performance gain over regular conv2d when XLA is enabled...", "> @AyanmoI Would it be better to add support thunking backward input convolution to cuDNN as well? We are wondering because separable conv2d does not yield expected performance gain over regular conv2d when XLA is enabled...\r\n\r\nBackward input convolution should already be thunking to cuDNN. The issue might be because now all backward filter grouped conv (including depthwise separable convs) are being thunked to forward convolutions with feature_grp count =1. This is due to the changes in tf2xla handling grouped backward filter convs and change in semantics of batch_group_convolutions. It has been seen before that thunking to cudnn grouped backward filter kernels delivers better performance which is not happening in most cases right now. "]}, {"number": 31596, "title": "TFLiteConverter fails with tf.gather when the params argument is a layer attribute", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS \r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.0.0-dev20190807\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 8 x Tesla P100-PCIE-16GB \r\n\r\n**Describe the current behavior**\r\nI am not able to convert a SavedModel to a FlatBuffer using TFLiteConverter when the corresponding tf.keras.Model contains a layer with a tf.gather op for which the params argument comes from a variable that was initialized in the build method of that said layer.\r\n\r\nWhen the params argument is from a locally defined variable, or when using tf.nn.embedding_lookup instead of tf.gather, everything works perfectly fine.\r\n\r\nIt also applies to tf.gather_nd.\r\n\r\n**Describe the expected behavior**\r\nI expect tf.gather to work for the case in which the params argument is an attribute of the tf.keras.layers.Layer, just as it does for the other cases mentioned.\r\n\r\n**Code to reproduce the issue**\r\nI wrote a toy example to reproduce the issue, it might be clearer than the description above.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nclass Embedding(tf.keras.layers.Layer):\r\n    def __init__(self, vocab_size, hidden_size):\r\n        super(Embedding, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n    \r\n    def build(self, input_shape):\r\n        self.shared_weights = self.add_weight(\r\n            \"weights\",\r\n            shape=(self.vocab_size, self.hidden_size),\r\n            dtype=tf.float32,\r\n            initializer=tf.random_normal_initializer(\r\n                mean=0.0, \r\n                stddev=self.hidden_size ** (-0.5)\r\n            )\r\n        )\r\n    \r\n    def call(self, input_):\r\n        # return tf.nn.embedding_lookup(self.shared_weights, input_)\r\n        # return tf.gather(tf.zeros(shape=(self.vocab_size, self.hidden_size)), input_)\r\n        return tf.gather(self.shared_weights, input_)\r\n\r\n\r\nclass SimpleModel(tf.keras.Model):\r\n    def __init__(self, vocab_size, hidden_size):\r\n        super(SimpleModel, self).__init__()\r\n        self.embedding_layer = Embedding(vocab_size, hidden_size)\r\n    \r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, ), dtype=tf.int64, name='input')])\r\n    def call(self, input_):\r\n        return self.embedding_layer(input_)\r\n\r\nvocab_size = 20000\r\nhidden_size = 300\r\n\r\n# Building the model.\r\nmodel = SimpleModel(vocab_size, hidden_size)\r\ninput_ = tf.random.uniform(shape=(20, ), dtype=tf.int64, maxval=100)\r\nmodel(input_)\r\n\r\n# Exporting to SavedModel.\r\nsaved_model_dir = 'simple_model/'\r\ntf.saved_model.save(model, saved_model_dir)\r\n\r\n# TFLite conversion.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/michael/.conda/envs/tf20/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: Placeholder statefulpartitionedcall_args_1 should be specied by input_arrays.\r\n\r\n```\r\n\r\n\r\n", "comments": ["Issue replicating with TF nightly-2.0-preview.", "Follow-up: another (temporary) solution I found is to use tf.identity,  ``` return tf.gather(tf.identity(self.shared_weights), input_) ``` works fine.\r\nIt might not be the most efficient way of fixing the issue (especially for large tensors) but it works.\r\n", "This should be fixed by b42547e3bca7ab796f40f20726f4b09bf552e833. Please reopen if the original code doesn't work in the nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31596\">No</a>\n", "Updated tensorflow 2.0 preview to the 20190903 nightly and still facing an issue.\r\n\r\n#### Code to reproduce\r\n\r\nThe code is basically the same as before: it runs when using tf.nn.embedding_lookup or wrapping the shared weights with a tf.identity op, but fails in the regular case with a tf.gather.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nimport os\r\n\r\nclass Embedding(tf.keras.layers.Layer):\r\n    def __init__(self, vocab_size, hidden_size):\r\n        super(Embedding, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.hidden_size = hidden_size\r\n    \r\n    def build(self, input_shape):\r\n        self.shared_weights = self.add_weight(\r\n            \"weights\",\r\n            shape=(self.vocab_size, self.hidden_size),\r\n            dtype=tf.float32,\r\n            initializer=tf.random_normal_initializer(\r\n                mean=0.0, \r\n                stddev=self.hidden_size ** (-0.5)\r\n            )\r\n        )\r\n    \r\n    def call(self, input_, mode=\"embedding\"):\r\n        # return tf.gather(tf.identity(self.shared_weights), input_)\r\n        # return tf.nn.embedding_lookup(self.shared_weights, input_)\r\n        return tf.gather(self.shared_weights, input_)\r\n\r\n\r\nclass SimpleModel(tf.keras.Model):\r\n    def __init__(self, vocab_size, hidden_size):\r\n        super(SimpleModel, self).__init__()\r\n        self.embedding_layer = Embedding(vocab_size, hidden_size)\r\n    \r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 25), dtype=tf.int64, name='input')])\r\n    def call(self, input_):\r\n        return self.embedding_layer(input_)\r\n\r\nvocab_size = 20000\r\nhidden_size = 300\r\n\r\n# Building the model.\r\nmodel = SimpleModel(vocab_size, hidden_size)\r\ninput_ = tf.random.uniform(shape=(10, 25), dtype=tf.int64, maxval=100)\r\nmodel(input_)\r\n\r\n# Exporting to SavedModel.\r\nsaved_model_dir = 'simple_model/'\r\ntf.saved_model.save(model, saved_model_dir)\r\n\r\n# TFLite conversion.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\n```\r\n#### Other info / logs\r\n\r\nAlthough it is not the same error message as before (ResourceGather related), I think it might be related to the initial issue.\r\n\r\n```\r\n2019-09-04 10:49:32.751617: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 6 operators, 9 arrays (0 quantized)\r\n2019-09-04 10:49:32.751814: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 6 operators, 9 arrays (0 quantized)\r\n2019-09-04 10:49:32.751891: F tensorflow/lite/toco/graph_transformations/resolve_gather_attributes.cc:47] Check failed: axis_data.size() == 1 (2 vs. 1)Multidimensional gather not supported on {Gather operator with output StatefulPartitionedCall/embedding_4/Gather}\r\nFatal Python error: Aborted\r\n```\r\n", "> Updated tensorflow 2.0 preview to the 20190903 nightly and still facing an issue.\r\n\r\n@michaelbenayoun,\r\nI was able to run the code without any issues with the latest TF-nightly i.e. 2.3.0-dev20200514. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dfb9e315b72999f54e8e6f258d155195/31596-tf-nightly.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31596\">No</a>\n"]}, {"number": 31595, "title": "How to create a concrete function for getting coordinates of a masked image and convert it to tensorflow lite model", "body": "How to create concrete function in tensorflow to get semantic segmentation image and calculate co-ordinates.\r\n\r\nWe are having current code for the same but not able to concrete function as we wanted to convert in tflite file for mobile application.\r\n\r\nExisting normal code is given below:\r\ndef predict_new_image(img_path, model):\r\nimg = load_img(img_path, grayscale=True)\r\nx_img = img_to_array(img)\r\nx_img = resize(x_img, (128, 128, 1), mode='constant', preserve_range=True)\r\n\r\nX = np.zeros((1, 128, 128, 1), dtype=np.float32)\r\ny_img = np.zeros((1, 128, 128, 1), dtype=np.float32)\r\nX[0, ..., 0] = x_img.squeeze() / 255\r\n\r\npred = model.predict(X)\r\npreds_img = (pred > 0.5).astype(np.uint8)\r\n\r\nimg_arr = preds_img[:,:,0]\r\n\r\n# get the coordinates where the pixel isn't white (at a threshold)\r\nblack_thres = 1\r\nidx = [(i,j) for i,x in enumerate(img_arr) for j,y in enumerate(x) if img_arr[i,j]==black_thres]\r\nreturn idx", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "We are using a UNET architecture for medical image segmentation. Using the keras version present within tensorflow 2.0. trying to compile from the source . The model gets packaged well within the TFLite. But not able to package the additional code for calculating coordinates of the masked image.", "Please, go through below link and see if it helps you to create a concrete function and convert to tensorflowlite. Thanks!\r\nhttps://github.com/tensorflow/tensorflow/issues/26708\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31595\">No</a>\n"]}, {"number": 31594, "title": "[TF:XLA] Add cluster_scoping_pass.", "body": "@sanjoy, could you please take a look of this PR when you are free. Thanks.\r\n\r\nThis new cluster_scoping_pass uses some heuristics to add scopes to nodes to guide the clustering results. Currently, the only heuristic is to preserve the parallelism between Tensorflow pipeline stages.\r\n\r\nSome notes:\r\n1. This PR resolves a clustering issue in a NVIDIA GoogleNet implementation due to bad clustering adding artificial dependencies between input pipeline stages. The performance impact is significant. XLA was 20% slower than Classic TF (without the PR) and becomes 20% faster with this PR.\r\n2. This PR changes the effects of the JIT scopes such that the scopes are respected by auto-jit (These scopes were overridden/ignored by auto-jit). It feels to me a good behavior, but please let me know whether you have any concerns about it.\r\n", "comments": ["@trentlo can you please resolve conflicts ?\r\n", "> @trentlo can you please resolve conflicts ?\r\n\r\nSure. Done.", "@sanjoy: I should have addressed all of your existing comments. Please help to take a look again when you have a moment. Thanks.", "Can one of the admins verify this patch?"]}, {"number": 31593, "title": "parallel_for: No converter defined for Cross", "body": "Currently PFor is not registered for tf.cross operation, as evident in [pfor.py](https://github.com/tensorflow/tensorflow/blob/21f5e55ccc42daf14e6386d1d27d0103b29f2c92/tensorflow/python/ops/parallel_for/pfor.py).\r\n\r\nWith this operation being very fundamental to linear algebra, it seems to me it would be very useful, particularly for enabling vectorization for efficient jacobain computation of functions using the cross product.", "comments": ["A converter has been added and should make its way into the nightly."]}, {"number": 31592, "title": "Add some macros to simplify the parameterized dataset tests", "body": "This PR adds some macros (e.g. `ITERATOR_GET_NEXT_TEST_P` and `ITERATOR_SAVE_AND_RESTORE_TEST_P`) to simplify the parameterized dataset tests.\r\n\r\ncc: @jsimsa  ", "comments": ["@feihugis  can you please sign CLA ?", "> @feihugis can you please sign CLA ?\r\n\r\nI think CLA has been signed. CLAs check passed this time. Thanks for adding `cla:yes` tag, @rthadur!"]}, {"number": 31591, "title": "tf.signal.stft does not work in eager execution mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: GTX 1060\r\n\r\n**Describe the current behavior**\r\ntf.signal.stft does not work in eager execution mode, producing the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/signal/spectral_ops.py\", line 83, in stft\r\n    signals, frame_length, frame_step, pad_end=pad_end)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/signal/shape_ops.py\", line 120, in frame\r\n    num_outer_dimensions = array_ops.size(outer_dimensions)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 416, in size\r\n    return size_internal(input, name, optimize=True, out_type=out_type)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 437, in size_internal\r\n    num_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  # pylint: disable=protected-access\r\n  File \"/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 2772, in prod\r\n    initial=initial)\r\n  File \"/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\r\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n```\r\n**Describe the expected behavior**\r\ntf.signal.stft should work as it does without eager execution\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ninput = tf.keras.Input([None])\r\nspec = tf.signal.stft(input, 400, 160)\r\n```", "comments": ["Was able to reproduce the error in [TF 2.0 Beta](https://colab.sandbox.google.com/gist/gowthamkpr/74a254946776db6e740d9a0ad9ed40c8/untitled89.ipynb) and also in [TF1.14](https://colab.sandbox.google.com/gist/gowthamkpr/6e84c9853c21172a507ba5c7267ebb3d/untitled87.ipynb). ", "@pplantinga Hi, I think you just missed the lambda layer. Because the functional API is used to building graph, so the tensor operations have to be wrapped in `tf.keras.layers.Layer` instance. So, a workable example should be\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ninput = tf.keras.Input([None])\r\nspec = tf.keras.layers.Lambda(lambda x: tf.signal.stft(x, 400, 160))(input)\r\nmodel = tf.keras.Model(inputs=input, outputs=spec)\r\n\r\ny = model(tf.random.uniform(minval=-100, maxval=100, shape=(100, 10000), dtype=tf.float32))\r\nprint(y)\r\n```\r\nIf you don't really want to build graph via functional API, the following example can manipulate `stft` result eagerly:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nx = tf.random.uniform(minval=-100, maxval=100, shape=(100, 10000), dtype=tf.float32)\r\nprint(tf.signal.stft(x, 400, 160))\r\n```\r\n\r\nSee more details about [functional API](https://www.tensorflow.org/beta/guide/keras/functional). Thanks!", "Yes, you're right, wrapping this in a lambda layer does solve the issue. Thanks for your help!"]}, {"number": 31590, "title": "[ROCm] enable nextafter op on ROCm.", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 31589, "title": "[ROCm] enable Xlogy op on ROCm.", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 31588, "title": "Performance issue: per-device memory usage increases with number of devices within tf.distribute strategy ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): tf-nightly-gpu-2.0-preview==\r\n- TensorFlow version (use command below): v1.12.1-8566-g207bd43 2.0.0-dev20190812\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: (4) Titan Xp 12 gb \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensor memory allocation fails for the same batch size (determined by n_acton_samples)  as I scale the number of GPUs in my strategy.\r\n\r\n**Describe the expected behavior**\r\nWithout the distributed strategy, I can readily pass a batch size of 300 to a given GPU, within the distributed strategy, this drops to 200 on 2 GPUs, and 70 on 4 GPUs.\r\n\r\n**Code to reproduce the issue**\r\n<details>\r\n  <summary>Example code snippet</summary>\r\n\r\n```\r\ndef tree_search(\r\n    inputs, self_model, world_model, plan_sequence\r\n):\r\n    n_action_samples = 1000\r\n    tree_value = tf.zeros([n_action_samples, 1], dtype=tf.float32)\r\n\r\n    for idx, action in enumerate(plan_sequence):\r\n        # print(\"idx: \", idx)\r\n        if idx == 0 and action:\r\n\r\n            candidate_actions = tf.random.uniform(\r\n                [n_action_samples, 1, num_fingers, 7],\r\n                minval=-1.0,\r\n                maxval=1.0,\r\n                dtype=tf.float32,\r\n            )\r\n\r\n\r\n        elif idx > 0 and action:\r\n            pass\r\n\r\n        elif idx > 0 and not action:\r\n               candidate_actions = tf.zeros(\r\n                [n_action_samples, 1, num_fingers, 7],\r\n                dtype=tf.float32,\r\n            )\r\n\r\n\r\n        else:\r\n            pass\r\n\r\n        tree_value += policy_model(candidate_actions, inputs)\r\n        inputs = world_model(candidate_actions, inputs)\r\n\r\n    retval = {\"action\": candidate_actions, \"tree_value\": tree_value}\r\n\r\n    return retval\r\n\r\n@tf.function\r\ndef policy(\r\n    inputs,\r\n    policy_model,\r\n    world_model,\r\n    strategy\r\n):\r\n\r\n    take_action = [True, False, False]\r\n\r\n    distributed_output = strategy.experimental_run_v2(\r\n        tree_search,\r\n        args=(\r\n            inputs,\r\n            self_model,\r\n            world_model,\r\n            plan_sequence,\r\n        ),\r\n    )\r\n\r\n     distributed_output[\"tree_value\"] = tf.concat(\r\n            strategy.experimental_local_results(distributed_output[\"tree_value\"]), axis=0\r\n        )\r\n\r\n     distributed_output[\"action\"] = tf.concat(\r\n            strategy.experimental_local_results(distributed_output[\"action\"]), axis=0\r\n        )\r\n\r\n\r\n    action = choose_max_value_action(\r\n       distributed_output[\"action\"], distributed_output[\"tree_value\"]\r\n    )\r\n\r\n    return action\r\n```\r\n\r\n</details>\r\n\r\n\r\n**Other info / logs**\r\nOn a related note, it seems like there's a regression in the tf.summary.start_trace() based method of profiling models that is required by 2.0. The new tensorboard interface allows visualizing execution time of ops, but there doesn't seem to be a way to show memory usage.", "comments": ["As an update, it appears this was partly caused by the use of range instead of tf.range in both the unrolling of my dynamics model and composing keras layers (I converted pre 2.0 code that was built using Graph mode to autograph which iterated over a list containing each layer size). Swapping in tf.range and switching to tensor arrays to collect loop results decreased by autograph graph construction time from 15 minutes to 5 minutes, and decreased memory usage by a factor of ~5. This is still long enough that the multi-device function optimizer times out\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0819 20:45:18.744279 140121059866368 deprecation.py:506] From /home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:17\r\n81: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nW0819 20:45:28.425643 140121059866368 deprecation.py:323] From /home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (f\r\nrom tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n2019-08-19 20:51:27.661471: W tensorflow/core/common_runtime/process_function_library_runtime.cc:688] Ignoring multi-device function optimization failure: Deadline exceeded: meta_op\r\ntimizer exceeded deadline.\r\n```\r\n\r\nMore problematically, I'm now getting errors of the form:\r\n\r\n```\r\n2019-08-19 23:58:27.044069: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: List contains uninitialized tensor\r\n at index 0 but leading_dims has only 0 elements.\r\n         [[{{node replica_2/TensorListConcatV2}}]]\r\n         [[L2Loss/_890]]\r\n2019-08-19 23:58:27.045491: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: List contains uninitialized tensor\r\n at index 0 but leading_dims has only 0 elements.\r\n         [[{{node replica_2/TensorListConcatV2}}]]\r\n2019-08-19 23:58:28.630952: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: List contains uninitialized tensor\r\n at index 0 but leading_dims has only 0 elements.\r\n         [[{{node replica_2/TensorListConcatV2}}]]\r\n         [[Adam/Cast_9/ReadVariableOp/_52]]\r\nTraceback (most recent call last):\r\n  File \"../boxphysics/scripts/train.py\", line 340, in <module>\r\n    main()\r\n  File \"../boxphysics/scripts/train.py\", line 333, in main\r\n    train_worker.distributed_train()\r\n  File \"/home/mjlbach/Repositories/box-physics/boxphysics/scripts/worker.py\", line 499, in distributed_train\r\n    self.train_params,\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 461, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1822, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 4 root error(s) found.\r\n  (0) Invalid argument:  List contains uninitialized tensor at index 0 but leading_dims has only 0 elements.\r\n         [[node replica_2/TensorListConcatV2 (defined at home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n         [[L2Loss/_890]]\r\n  (1) Invalid argument:  List contains uninitialized tensor at index 0 but leading_dims has only 0 elements.\r\n         [[node replica_2/TensorListConcatV2 (defined at home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (2) Cancelled:\r\n  (3) Cancelled:\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_distributed_train_step_743779]\r\nFunction call stack:\r\ndistributed_train_step -> distributed_train_step -> distributed_train_step -> distributed_train_step\r\n\r\n\r\nIf you suspect this is an IPython bug, please report it at:\r\n    https://github.com/ipython/ipython/issues\r\nor send an email to the mailing list at ipython-dev@python.org\r\n\r\nYou can print a more detailed traceback right now with \"%tb\", or use \"%debug\"\r\nto interactively debug it.\r\n\r\nExtra-detailed tracebacks for bug-reporting purposes can be enabled via:\r\n    %config Application.verbose_crash=True\r\n\r\nPolicy time: 10.194841623306274\r\nException ignored in: <bound method Socket.__del__ of <zmq.sugar.socket.Socket object at 0x7f8048041e18>>\r\nTraceback (most recent call last):\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/zmq/sugar/socket.py\", line 67, in __del__\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/zmq/sugar/socket.py\", line 105, in close\r\n  File \"/home/mjlbach/.virtualenvs/physics3/lib/python3.6/site-packages/zmq/sugar/context.py\", line 153, in _rm_socket\r\nTypeError: 'NoneType' object is not callable\r\ntrain_3obj_2for.sh: line 50: 152409 Segmentation fault      (core dumped) python \r\n```\r\n\r\nEager to autograph is extremely hard to debug due to how opaque the error messages are, and the lack of comparable tools for debugging like the memory profiler and analyzer which work only in the tf1 style graph mode. I saw the recent RFC 20190815-tfdbg-v2-callbacks which may go some of the way to addressing this, but currently I don't think there is a good way to view per-tensor memory usage in a way similar to 1.0.\r\n\r\nHaving a way to pipe abseil errors to a file (not to stdout) when enabling autgraph's debug verbosity level would be nice.\r\n\r\nI'm assuming entries like these in the autograph debug output (set to 10) indicate the function is being called with different tensor input shapes and has to be retraced?\r\n\r\n```\r\nCache hit for entity <function sum_effects at 0x7f76802d7c80> key <code object sum_effects at 0x7f76802d19c0, file \"/home/mjlbach/Repositories/physics/physics/models/interaction_uti\r\nls.py\", line 11> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f74e862cef0>, frozenset()): _ConvertedEntityFactoryInfo(tf__sum_effects in tmp1lu\r\nsxjty)\r\nDefaults of <function create_converted_entity_factory.<locals>.create_converted_entity.<locals>.tf__sum_effects at 0x7f74e85fe2f0> : None\r\nKW defaults of <function create_converted_entity_factory.<locals>.create_converted_entity.<locals>.tf__sum_effects at 0x7f74e85fe2f0> : None\r\nCalling <function create_converted_entity_factory.<locals>.create_converted_entity.<locals>.tf__sum_effects at 0x7f74e85fe2f0> with\r\n    E: Tensor(\"phiR/phiR_fc6_2/Identity:0\", shape=(None, 100), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:3)\r\n    Rr: Tensor(\"GatherNd_106:0\", shape=(None, 2), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:3)\r\n    bs: 450\r\n    no: 350\r\n```\r\n\r\nA final issue, is that there currently doesn't seem to be a way to declare a for loop uses shape_invariants as the documentation suggests for autograph, you need to manually rewrite the code to use a tf.while loop. Shape invariants are also somewhat inconvenient for nested tensors.", "Leaving aside the debugability issues and the support for shape_invariants for a moment, using range means that the loop is unrolled in the graph. So with range, a new subgraph is created for each loop iteration. That should explain the slow graph construction as well as the memory usage increase, because there are more temporary tensors involved. My wild guess would be that the graph construction time and memory usage is in the order of n_acton_samples * <number of devices>.\r\n\r\nAssigning @guptapriya for the slowness question when tf.range is involved.", "@mjlbach can you share the code that throws the error you mentioned above  (List contains uninitialized tensor at index 0 but leading_dims has only 0 elements). Do you get this error only when using dist strat?", "I agree that the memory growth issue under dist-strat was compounded due to the for loops using range, which makes sense given the creation of additional subgraphs. However, when using tf.range, it appears to impose additional constraints on the functions called within the loop which cause the uninitialized tensor errors. The issue originated because a lot of my model involves graph operations on a flat graph of the following type:\r\n\r\n```\r\noutputs = list()\r\nfor i in range(iterations):\r\n    output.append(MLP(graph[:, i]))\r\ntf.concat(output, axis=-1)\r\n```\r\nWhile fine under tf 1.0, these constructs throw an error if wrapped in a tf.range.\r\n\r\nIf I convert to use TensorArrays and tf.range:\r\n```\r\noutputs = tf.TensorArray(dtype=tf.float32, size=iterations, infer_size=False\r\nfor i in tf.range(iterations):\r\n    outputs = outputs.write(i, MLP(input[i]))\r\noutputs = outputs.concat()\r\n```\r\n\r\nAre the performance reasons always in favor of using tf.range? Is there ever any reason to use range? It seems like tf.range is more memory efficient, but also might come with a bit of a performance penalty in terms of execution speed... \r\n\r\nAnother related issue seems to be that under distribution strategy, tf.range loops cannot call models which call assign_add on a member variable (like a running mean) that is synchronized across models.\r\n\r\nI'm still investigating and I'll try to post a minimal example reproducing the issue and as a template for others with similar issues.", "About range vs. tf.range: in general, when the number of iterations is fixed, the memory constraints (particularly graph size) dictate the preference for tf.range. You are correct, the tf.range version might pay some runtime overhead, but that should be extremely small. There are some situations when tf.range cannot be used, for example in a loop that builds the layers of a neural net: `for i in range(num_layers): build_layer(i)`. Otherwise, it's a good idea to use tf.range whenever possible.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31588\">No</a>\n"]}, {"number": 31587, "title": "[ROCm] enable nextafter op on ROCm.", "body": "", "comments": ["failing local tests on ROCm. reopen after revising."]}, {"number": 31586, "title": "[ROCm] enable InTopK op on ROCm.", "body": "", "comments": []}]