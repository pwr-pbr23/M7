[{"number": 40501, "title": "SavedModel does not work correctly with signatures that use structured inputs or outputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-34056-gde8db50778 2.3.0-dev20200613\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nAs described [here](https://www.tensorflow.org/guide/concrete_function#nested_arguments), support for structured/nested arguments is available in TF nightly builds and coming to 2.3. Getting concrete functions with structured inputs and outputs works as documented, but using these concrete functions as signatures in saved models leads to a various issues.\r\n\r\nSee the following code snippet for simple example cases.\r\n```python\r\nfrom collections import namedtuple\r\nimport tensorflow as tf\r\n\r\nPair = namedtuple('Pair', ('x', 'y'))\r\n\r\nclass TestModel(tf.keras.Model):\r\n  @tf.function\r\n  def test(self, inputs: Pair):\r\n    # Unlike inputs, outputs would need to be manually flattened if nested.\r\n    return Pair(inputs.x + inputs.y, inputs.x - inputs.y)\r\n\r\nmodel = TestModel()\r\nspecs = Pair(tf.TensorSpec([], tf.int32), tf.TensorSpec([], tf.int32))\r\nsignatures = {'test': model.test.get_concrete_function(inputs=specs)}\r\n\r\ntf.saved_model.save(model, 'test_model', signatures=signatures)\r\nloaded_model = tf.saved_model.load('test_model')\r\n\r\n# When using the original signature it works correctly.\r\n# This produces a Pair with tensor values (5, 1).\r\ninputs = Pair(tf.constant(3, tf.int32), tf.constant(2, tf.int32))\r\noutput_pair = signatures['test'](inputs)\r\n\r\n# The same does not work when using the signature from the loaded model.\r\n# It fails because inputs have been flattened. This is acceptable, but not documented.\r\nloaded_model.signatures['test'](inputs)\r\n\r\n# However, flattening the input data does not help.\r\n# This fails too because the signature now requires keyword arguments.\r\nloaded_model.signatures['test'](*tf.nest.flatten(inputs, expand_composites=True))\r\n\r\n# This fails too because the keyword argument names are generated by TF.\r\nloaded_model.signatures['test'](x=inputs.x, y=inputs.y)\r\n\r\n# This works, but it's utterly confusing. Note the args 'inputs' and 'inputs_1'.\r\noutputs = loaded_model.signatures['test'](inputs=inputs.x, inputs_1=inputs.y)\r\n\r\n# Outputs are also flattened. However, we get a dict. This means we cannot\r\n# do this to recover the structure as order is not necessarily preserved.\r\n# For more complex nested structures this would be pretty much needed.\r\nbad_output_pair = tf.nest.pack_sequence_as(specs, list(outputs.values()))\r\n\r\n# Not only that, but key names are not even consistent with inputs.\r\n# This fails because the 'output' key does not exist.\r\noutput_pair = Pair(outputs['output'], outputs['output_1'])\r\n\r\n# This works, but it's again confusing and not self-consistent.\r\noutput_pair = Pair(outputs['output_0'], outputs['output_1'])\r\n```\r\n\r\n**Describe the expected behavior**\r\nWhile it would be desirable to be able to use structured types directly, flattening them is a reasonable tradeoff for signatures in exported models.\r\n\r\nHowever:\r\n1. It should be properly documented.\r\n2. It should make recovering arbitrarily nested structures easy.\r\n3. It should be self-consistent.\r\n\r\nIf flattening inputs and outputs is required, then a much better approach would be to transform potentially nested structures into tuples of their flattened contents. These tuples could then be referred either by position or by their keyword argument.\r\n\r\nFor example:\r\n```python\r\n# This could work if it accepted flattened tuples by position.\r\nflat_outputs = loaded_model.signatures['test'](tf.nest.flatten(inputs, True))\r\n\r\n# This could work if it accepted flattened tuples by keyword argument,\r\n# which would help with functions taking multiple structured input arguments.\r\nflat_outputs = loaded_model.signatures['test'](inputs=tf.nest.flatten(inputs, True))\r\n\r\n# flat_outputs can still be a dict if it maps to tuples of flattened tensors.\r\n# This would work for arbitrarily complex nested structures,\r\n# and would also remove the need to generate new output names.\r\noutput_pair = tf.nest.pack_sequence_as(specs, flat_outputs['output'])\r\n\r\n# Returning the flattened tuple directly or a sequence of outputs instead of a dict\r\n# would perhaps make more sense, but this would need to work with other use cases.\r\noutput_pair = tf.nest.pack_sequence_as(specs, flat_outputs[0])\r\n```\r\n\r\nThese changes would make the behavior of loaded model signatures tf.nest-friendly.\r\n\r\nIt would also be best to make these changes before the feature goes public in the incoming TF 2.3 release, as otherwise developers might start writing code that depends on the current confusing and inconsistent keyword args behavior.\r\n\r\nEdit: corrected the code example to correctly account for composite tensors on inputs, and to mention the additional issue from the first comment.", "comments": ["Another issue: unlike inputs, nested outputs (not just a named tuple like in the example, but nesting them) are not automatically flattened. You need to manually flatten them to be able to export them.\r\n\r\nIf you don't, you get a `ValueError` that says:\r\n> Signatures have one Tensor per output, so to have predictable names Python functions used to generate these signatures should avoid outputting Tensors in nested structures.\r\n\r\nThis is another inconsistency and a problem that could also be avoided with the proposal above, since the entire nested structure would be flattened in a tuple with a single name.", "@leandro-gracia-gil \r\nI ran the code shared above, please let us know if [this gist](https://colab.research.google.com/gist/Saduf2019/10e7dfe5c9a68ee76391c9dcb266b11f/untitled233.ipynb) confirms your issue.", "@Saduf2019 Yes it does. However, note that my code contains multiple examples of failing cases for different reasons as indicated in the comments. Each of these will cause an error (with the exception of the `bad_output_pair` result where there is no error but the result is unsafe due to unreliable ordering). The gist is simply failing on the first of such cases.", "Looks like the input structure issues I reported here actually come from a misunderstanding of how exported signatures should be used. In the example above, instead of `loaded_model.signatures['test'](inputs)` what should be used is `loaded_model.test(inputs)`, which works as expected and correctly produces an output `Pair`.  (Thanks to @edloper for clarifying this in the RFC review).\r\n\r\nHowever, there's still an issue with nested structured outputs. If instead of a simple `Pair` we nest them, we get an error.\r\n\r\n```python\r\nclass TestModel(tf.keras.Model):\r\n  @tf.function\r\n  def test(self, inputs: Pair):\r\n    pair = Pair(inputs, Pair(inputs.x + inputs.y, inputs.x - inputs.y))\r\n    return pair\r\n```\r\n\r\nTrying to save the model exporting the signature now raises this:\r\n`ValueError: Got non-flat outputs 'Pair(x=Pair(x=<tf.Tensor 'PartitionedCall:0' shape=() dtype=int32>, y=<tf.Tensor 'PartitionedCall:1' shape=() dtype=int32>), y=Pair(x=<tf.Tensor 'PartitionedCall:2' shape=() dtype=int32>, y=<tf.Tensor 'PartitionedCall:3' shape=() dtype=int32>))' from 'b'__inference_test_26'' for SavedModel signature 'test'. Signatures have one Tensor per output, so to have predictable names Python functions used to generate these signatures should avoid outputting Tensors in nested structures.`\r\n\r\nThis issue can be manually avoided by manually flattening the outputs: `return tf.nest.flatten(pair)`, but the output structure is lost and it must be reconstructed. If instead of one output the function returns multiple of them, then all of them need to be flattened and concatenated together.\r\n\r\nIs this the expected behavior? Would it be possible to somehow preserve the nested structure of the outputs, given that it does seem possible to preserve it in the inputs?", "@leandro-gracia-gil As I commented on the extension types RFC, the `signatures` in a `SavedModel` are intended to interoperate with non-python interfaces (c++, tensorflow serving, etc.), where Python's types (such as namedtuple) are not available.  Currently, this means that the concrete functions in the `signatures` dictionary are expected to take tensor arguments and return a tensor or a string-keyed dictionary of tensors.\r\n\r\nBut if you're going to be using your saved model from Python, then you don't need to use `signatures` at all.  You can save a model with functions that take nested or composite tensor parameters, and that return nested or composite tensor values.  You do need to call `get_concrete_function` for each concrete signature you want the saved model to contain (so it can trace the graph for those signatures), but you don't need to store the result.  (You can call `get_concrete_function` multiple times if you want your saved model to support different input signatures.)  \r\n\r\nI.e., I think that the following will do what you want:\r\n\r\n```python\r\nclass TestModel(tf.keras.Model):\r\n  @tf.function\r\n  def test(self, inputs: Pair):\r\n    pair = Pair(inputs, Pair(inputs.x + inputs.y, inputs.x - inputs.y))\r\n    return pair\r\n\r\nmodel = TestModel()\r\nspecs = Pair(tf.TensorSpec([], tf.int32), tf.TensorSpec([], tf.int32))\r\nmodel.test.get_concrete_function(inputs=specs)  # trace graph for expected spec(s)\r\n\r\ntf.saved_model.save(model, 'test_model')  # note: no signatures argument here\r\nloaded_model = tf.saved_model.load('test_model')\r\nprint(loaded_model.test(Pair(10, 20)))\r\n```", "Thanks for the info. This approach does seem to work and keep the structure of the outputs too without having to flatten, but it rises a couple of questions.\r\n\r\n1. How can I decide which concrete functions to export? I have other concrete functions in my actual model for my train and test pipelines, but I only want to export the ones relevant for inference. With signatures I could control this, but this way it seems like every single concrete function of the model gets automatically exported.\r\n\r\n2. If I do not use explicit signatures, is it still possible to use the model from C++? How would the arguments work in that case?", "Hi @leandro-gracia-gil \r\n\r\nhave you been able to get answers to those questions? ", "@stefan-falk No, not yet.", "Ah I see. I am at this point my self right now. I guess you solved it using a wrapper?\r\n\r\n```python\r\nclass MyModelWrapper:\r\n  def __init__(self, model: MyModel):\r\n    self.my_fun = tf.function(model.my_fun, ...)\r\n```\r\n\r\nor something like that?\r\n  ", "@stefan-falk I'm not currently using `signatures`, so my model is exporting all concrete functions (though I only want to export a few particular ones). As for the C++ question, I haven't tried using the exported functions in C++ yet.", "@leandro-gracia-gil Alright, thank you for the insight :) ", "@leandro-gracia-gil Could you please let us know if this is still an issue in TF v2.6.0 ?Thanks!", "@sushreebarsa Following the instructions provided in [this comment](https://github.com/tensorflow/tensorflow/issues/40501#issuecomment-669187477) it works fine. There's still the problem that it forces you to export all concrete functions that are ever traced instead of a signature subset of your choice, but at least there's a workaround for getting things working. I'll be closing this for now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40501\">No</a>\n"]}, {"number": 40500, "title": "FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])", "body": "Hi, how to disable it?\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  not working\r\n\r\ni'm using TS 1.14\r\nkeras\r\nand imageai\r\nfull warning: \r\n`{ProjectPath}\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])`\r\n\r\n\r\n", "comments": ["@iBelow \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Can you please elaborate about the issue & the context.Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I get this conclusion at the start of the project.\r\nat the moment, everything that happens - only imports are loaded\r\n\r\n\r\nimport list: \r\n```\r\nimport telebot\r\nimport os\r\nfrom imageai.Detection import ObjectDetection\r\nfrom imageai.Detection import VideoObjectDetection\r\nimport cv2\r\nimport configurations\r\nimport datetime\r\nimport pytesseract as ts\r\n```\r\n\r\n\r\nfullstack warnings: \r\n```\r\nD:\\Developing\\[PY]CVF\\Scripts\\python.exe D:/Developing/[PY]CVF/tbot.py\r\nUsing TensorFlow backend.\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nD:\\Developing\\[PY]CVF\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nD:\\Developing\\[PY]CVF\\resnet50_coco_best_v2.0.1.h5\r\nconnected to bot api\r\nawaiting new content\r\n```\r\n\r\n\r\n\r\ncode: \r\n```\r\nimport telebot\r\nimport os\r\nfrom imageai.Detection import ObjectDetection\r\nfrom imageai.Detection import VideoObjectDetection\r\nimport cv2\r\nimport configurations\r\nimport datetime\r\nimport pytesseract as ts\r\n\r\n\r\n\r\nAPI_TOKEN = configurations.TOKEN\r\nbot = telebot.TeleBot(API_TOKEN)\r\nprint('connected to bot api')\r\nprint('awaiting new content')\r\n\r\n//some events\r\n\r\nbot.polling\r\n```\r\n\r\n\r\nTensorflow official build 1.14\r\n```\r\n\r\nabsl-py              0.9.0\r\nastor                0.8.1\r\ncertifi              2020.4.5.2\r\nchardet              3.0.4\r\ncycler               0.10.0\r\ngast                 0.3.3\r\ngoogle-pasta         0.2.0\r\ngrpcio               1.29.0\r\nh5py                 2.10.0\r\nhikvisionapi         0.2.1\r\nidna                 2.9\r\nimageai              2.1.5\r\nimportlib-metadata   1.6.1\r\nKeras                2.3.1\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.2\r\nkiwisolver           1.2.0\r\nMarkdown             3.2.2\r\nmatplotlib           3.2.1\r\nMouseInfo            0.1.3\r\nnumpy                1.18.5\r\nopencv-python        4.2.0.34\r\nPillow               7.1.2\r\npip                  20.1.1\r\nprotobuf             3.12.2\r\nPyAutoGUI            0.9.50\r\nPyGetWindow          0.0.8\r\nPyMsgBox             1.0.8\r\npyparsing            2.4.7\r\npyperclip            1.8.0\r\nPyRect               0.1.4\r\nPyScreeze            0.1.26\r\npyTelegramBotAPI     3.7.1\r\npytesseract          0.3.4\r\npython-dateutil      2.8.1\r\nPyTweening           1.0.3\r\nPyYAML               5.3.1\r\nrequests             2.23.0\r\nscipy                1.4.1\r\nsetuptools           47.1.1\r\nsix                  1.15.0\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntermcolor            1.1.0\r\nurllib3              1.25.9\r\nWerkzeug             1.0.1\r\nwheel                0.34.2\r\nwrapt                1.12.1\r\nxmltodict            0.12.0\r\nzipp                 3.1.0\r\n\r\n```\r\n\r\ncan i disable this warnings? \r\n\r\n\r\n", "@iBelow \r\n\r\nCan you please help with colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nhttps://colab.research.google.com/drive/1_7bAl9x7rx4XGWEchrz8hUiLKi2A_h5n?usp=sharing", "@iBelow I am not able reproduce the issue. Can you please share a standalone code to reproduce the issue? The current code runs forever. Also, I am not sure whether it is a bug with TF or some other modules imported in the code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Duplicate of #30427.\r\nThis should be fixed with https://github.com/tensorflow/tensorflow/commit/3402d7118460857cf484f57338d14d9113597d15 in #30559 as part of v1.15.0 and newer."]}, {"number": 40499, "title": "Tensorflow 2.0 converter.experimental_new_converter = True still giving 'str' object has no attribute 'call'", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): tensorflow 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_file)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"linear.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-b0b66b38a77c> in <module>\r\n----> 1 converter = tf.lite.TFLiteConverter.from_keras_model(keras_file)\r\n      2 converter.experimental_new_converter = True\r\n      3 tflite_model = converter.convert()\r\n      4 open(\"linear.tflite\", \"wb\").write(tflite_model)\r\n\r\nc:\\python\\python38\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in from_keras_model(cls, model)\r\n    426     # to None.\r\n    427     # Once we have better support for dynamic shapes, we can remove this.\r\n--> 428     if not isinstance(model.call, _def_function.Function):\r\n    429       # Pass `keep_original_batch_size=True` will ensure that we get an input\r\n    430       # signature including the batch dimension specified by the user.\r\n\r\nAttributeError: 'str' object has no attribute 'call'\r\n\r\n```\r\n\r\n(https://github.com/tensorflow/tensorflow/issues/32693)\r\nI referred to this previous forum for the same error and saw one of the solutions was to use the 'experimental_new_converter = True' function but I'm still getting the same error. I would reopen the mentioned forum if I could but I'm not sure how.", "comments": ["@JacobOfCorns\r\n\r\nHi, I think you need to use the following converter API since `from_keras_model_file` accepts a keras file path while `from_keras_model` accepts a keras model instance.\r\n\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"linear.tflite\", \"wb\").write(tflite_model)\r\n```", "It works!! you are amazing thank you so much!\r\n\r\n> @JacobOfCorns\r\n> \r\n> Hi, I think you need to use the following converter API since `from_keras_model_file` accepts a keras file path while `from_keras_model` accepts a keras model instance.\r\n> \r\n> ```\r\n> converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\n> converter.experimental_new_converter = True\r\n> tflite_model = converter.convert()\r\n> open(\"linear.tflite\", \"wb\").write(tflite_model)\r\n> ```\r\n\r\n"]}, {"number": 40497, "title": "Undefined references when using TensorFlow Lite in Arduino Nano 33 BLE Sense + Platform IO", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nUbunutu 20.04 on x86_64\r\nTensorFlow from source 2.2.0\r\nTarget: Arduino Nano 33 BLE Sense\r\n\r\nHi,\r\n\r\nI'm trying to get micro_speech running on a Arduino Nano 33 BLE Sense, using a model I've trained myself on Google Colab.  As Google Colab is currently using TensorFlow 2.2.0, this model is also 2.2.0.  Using the Arduino IDE 1.8.12 sample 'micro_speech', does work, and I can get inferences to occur, however this is a long way from TensorFlow 2.2.0.  In fact its many versions behind and as a result, I'm unable to get my model to run on this version.\r\n\r\nIn addition, this project is a stepping stone to another project which will swap out the model for other models, and introduce other libraries which introduce further development and debugging complexity.  For this reason, I've chosen to adopt the Platform IO + VSCode development environment.  With this combination I can debug the code, step by step, which is a feature not supported by the Microsoft VSCode Arduino extension or Arduino IDE itself.  I can also use unit testing technologies which aren't available elsewhere.\r\n\r\nUnfortunately, my code is running into Undefined reference exceptions on build, in what is a fairly trivial port of the micro_speech application to the Platform IO code.  I get these same errors building the code using the local TensorFlow Arduino library zip which I built myself from the TensorFlow 2.2.0 master branch a couple of days ago.\r\n\r\nMay I please get some advice?\r\n- Any ideas why Undefined references are appearing in local builds of [micro_speech](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech), or my port of the [micro_speech](https://github.com/victorromeo/pio_micro_speech) product?  It was also reported in this issue [27629](https://github.com/tensorflow/tensorflow/issues/27629)\r\n- Given the explosion of options, which development environment is the recommended approach for products which extend beyond 'hello-world' targeting micro-controllers? \r\n  - Arduino Online \r\n  - Arduino IDE with local TensorFlow Arduino library zip  \r\n  - TensorFlow Lite + Bazel from command  line\r\n  - Platform IO (IDE / VSCode / Atom)\r\n  - Microsoft Arduino over VSCode\r\n  - entirely manual arm-none-eabi-g++\r\n\r\n```\r\n> Executing task: platformio run --verbose <\r\n\r\nProcessing nano33ble (platform: nordicnrf52; board: nano33ble; framework: arduino; debug_tool: jlink; upload_protocol: jlink)\r\n----------------------------------------------------------------\r\nCONFIGURATION: https://docs.platformio.org/page/boards/nordicnrf52/nano33ble.html\r\nPLATFORM: Nordic nRF52 4.2.1 > Arduino Nano 33 BLE\r\nHARDWARE: NRF52840 64MHz, 256KB RAM, 960KB Flash\r\nDEBUG: Current (jlink) External (cmsis-dap, jlink)\r\nPACKAGES: \r\n - framework-arduino-nrf52-mbedos 1.1.3 \r\n - tool-sreccat 1.164.0 (1.64) \r\n - toolchain-gccarmnoneeabi 1.80201.181220 (8.2.1)\r\nLDF: Library Dependency Finder -> http://bit.ly/configure-pio-ldf\r\nLDF Modes: Finder ~ chain, Compatibility ~ soft\r\nFramework incompatible library /home/ian/.platformio/packages/framework-arduino-nrf52-mbedos/libraries/mbed-memory-status\r\nFound 7 compatible libraries\r\nMore details about \"Library Compatibility Mode\": https://docs.platformio.org/page/librarymanager/ldf.html#ldf-compat-mode\r\nScanning dependencies...\r\nDependency Graph\r\n|-- <PDM> 1.0 (/home/ian/.platformio/packages/framework-arduino-nrf52-mbedos/libraries/PDM)\r\n|-- <micro_features> (/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/micro_features)\r\n|   |-- <tensorflow> (/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow)\r\n|-- <tensorflow> (/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow)\r\nBuilding in release mode\r\narm-none-eabi-g++ -o .pio/build/nano33ble/firmware.elf -T linker_script.ld -DMBED_APP_SIZE=0xf0000 -DMBED_APP_START=0x10000 -DMBED_BOOT_STACK_SIZE=2048 -Wl,--gc-sections -Wl,--wrap,_calloc_r -Wl,--wrap,_free_r -Wl,--wrap,_malloc_r -Wl,--wrap,_memalign_r -Wl,--wrap,_realloc_r -Wl,--wrap,atexit -Wl,--wrap,exit -Wl,--wrap,main -Wl,-n -mcpu=cortex-m4 -mfloat-abi=softfp -mfpu=fpv4-sp-d16 -mthumb --specs=nano.specs --specs=nosys.specs -Wl,--as-needed .pio/build/nano33ble/src/audio_provider.cpp.o .pio/build/nano33ble/src/command_responder.cpp.o .pio/build/nano33ble/src/feature_provider.cpp.o .pio/build/nano33ble/src/main.cpp.o .pio/build/nano33ble/src/recognize_commands.cpp.o -L.pio/build/nano33ble -L/home/ian/.platformio/packages/framework-arduino-nrf52-mbedos/variants/ARDUINO_NANO33BLE -L/home/ian/.platformio/packages/framework-arduino-nrf52-mbedos/variants/ARDUINO_NANO33BLE/libs -Wl,--start-group -Wl,--whole-archive .pio/build/nano33ble/libe06/libPDM.a .pio/build/nano33ble/lib5e8/libtensorflow.a .pio/build/nano33ble/lib082/libmicro_features.a .pio/build/nano33ble/libFrameworkArduinoVariant.a .pio/build/nano33ble/libFrameworkArduino.a -lmbed -lcc_310_core -lcc_310_ext -lcc_310_trng -Wl,--no-whole-archive -lstdc++ -lsupc++ -lm -lc -lgcc -lnosys -Wl,--end-group\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/command_responder.cpp.o: in function `RespondToCommand(tflite::ErrorReporter*, long, char const*, unsigned char, bool)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/command_responder.cpp:48: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/feature_provider.cpp.o: in function `FeatureProvider::PopulateFeatureData(tflite::ErrorReporter*, long, long, int*)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/feature_provider.cpp:102: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/main.cpp.o: in function `tflite::MicroMutableOpResolver<4u>::GetOpDataParser(tflite::BuiltinOperator) const':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow/tensorflow/lite/micro/micro_mutable_op_resolver.h:62: undefined reference to `tflite::ParseOpData(tflite::Operator const*, tflite::BuiltinOperator, tflite::ErrorReporter*, tflite::BuiltinDataAllocator*, void**)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/main.cpp.o: in function `tflite::MicroMutableOpResolver<4u>::AddCustom(char const*, TfLiteRegistration*)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow/tensorflow/lite/micro/micro_mutable_op_resolver.h:98: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow/tensorflow/lite/micro/micro_mutable_op_resolver.h:108: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/main.cpp.o: in function `tflite::MicroMutableOpResolver<4u>::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration*)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow/tensorflow/lite/micro/micro_mutable_op_resolver.h:68: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/tensorflow/tensorflow/lite/micro/micro_mutable_op_resolver.h:78: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/main.cpp.o: in function `setup':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:75: undefined reference to `tflite::ops::micro::Register_DEPTHWISE_CONV_2D()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:80: undefined reference to `tflite::ops::micro::Register_FULLY_CONNECTED()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:85: undefined reference to `tflite::ops::micro::Register_SOFTMAX()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:90: undefined reference to `tflite::ops::micro::Register_RESHAPE()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:98: undefined reference to `tflite::MicroInterpreter::MicroInterpreter(tflite::Model const*, tflite::MicroOpResolver const&, unsigned char*, unsigned int, tflite::ErrorReporter*)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:102: undefined reference to `tflite::MicroInterpreter::AllocateTensors()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:109: undefined reference to `tflite::MicroInterpreter::input(unsigned int)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:131: undefined reference to `tflite::MicroInterpreter::~MicroInterpreter()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:59: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:114: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/main.cpp.o: in function `loop':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:172: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:157: undefined reference to `tflite::MicroInterpreter::Invoke()'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/main.cpp:164: undefined reference to `tflite::MicroInterpreter::output(unsigned int)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/main.cpp.o:(.data._ZZ5setupE20micro_error_reporter+0x0): undefined reference to `vtable for tflite::MicroErrorReporter'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/recognize_commands.cpp.o: in function `RecognizeCommands::ProcessLatestResults(TfLiteTensor const*, long, char const**, unsigned char*, bool*)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/recognize_commands.cpp:46: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/recognize_commands.cpp:60: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/src/recognize_commands.cpp:74: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/recognize_commands.cpp.o: in function `PreviousResultsQueue::push_back(PreviousResultsQueue::Result const&)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/include/recognize_commands.h:66: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/src/recognize_commands.cpp.o: in function `PreviousResultsQueue::pop_front()':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/include/recognize_commands.h:79: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/lib082/libmicro_features.a(micro_features_generator.cpp.o): in function `InitializeMicroFeatures(tflite::ErrorReporter*)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/micro_features/micro_features_generator.cpp:50: undefined reference to `FrontendPopulateState'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: /home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/micro_features/micro_features_generator.cpp:52: undefined reference to `tflite::ErrorReporter::Report(char const*, ...)'\r\n/home/ian/.platformio/packages/toolchain-gccarmnoneeabi/bin/../lib/gcc/arm-none-eabi/8.2.1/../../../../arm-none-eabi/bin/ld: .pio/build/nano33ble/lib082/libmicro_features.a(micro_features_generator.cpp.o): in function `GenerateMicroFeatures(tflite::ErrorReporter*, short const*, int, int, signed char*, unsigned int*)':\r\n/home/ian/Documents/PlatformIO/Projects/Nano33Take01/lib/micro_features/micro_features_generator.cpp:79: undefined reference to `FrontendProcessSamples'\r\ncollect2: error: ld returned 1 exit status\r\n*** [.pio/build/nano33ble/firmware.elf] Error 1\r\n================== [FAILED] Took 1.25 seconds ==================\r\nThe terminal process terminated with exit code: 1\r\n\r\nTerminal will be reused by tasks, press any key to close it.\r\n```", "comments": ["So, I've reverted the codebase back to the 'micro_speech' example code from the Arduino Web Editor, then transferred these into PlatformIO using VSCode.  The build is executing to completion now. Even if inferences are failing its progress.\r\n\r\nI've abandoned the 'micro_speech' implementation as it current appears in the TensorFlow GitHub repo (v 2.2.0), as a source for this project, as it simply doesn't seem to be easily ported.\r\n\r\nIf I can recommend anything from my experience its that the example projects and the instructions for building them need to demonstrate the current implementation of the TensorFlow technologies, because that's where the online documentation for all other training is guiding developers.\r\n\r\nThe Arduino implementation I'm using is now targeting the TensorFlow Lite 2.1.x, trained using Keras over TensorFlow 2.3.0+ (tf-nightly), whereas micro_speech and speech_command examples are straight TF 1.x.\r\n\r\nHappy for the issue to close at the discretion of the TF team.", "Closing due to inactivity", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40497\">No</a>\n"]}, {"number": 40496, "title": "Sign compare warning fixes batch 2", "body": "In resolution of warnings with ids: [\r\n47, 48, 49, 51, 52,\r\n53, 54, 55, 56, 57, \r\n59, 60, 61, 63, 65, 70, \r\n]\r\n\r\n@mihaimaruseac ", "comments": ["@joker-eph  There are no mlir related changes in this PR. "]}, {"number": 40495, "title": "[batch AUC vs streaming AUC]: I want an implement a batch AUC method", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.4 and 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow only support streaming auc. If we want to monitoring the batch auc, we need to reset auc local variables or change the source code of `tensorflow.python.ops.metrics_impl`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt only add an optional arg named `batch=False` to current metrics.auc op like:\r\n\r\n- tf.keras.metrics.AUC(_, batch=False)\r\n\r\n- tf.compat.v1.metrics.auc(_, batch=False)\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers want to monitoring the realtime batch_auc.  E.g:\r\n\r\n- online training\r\n\r\n- monitoring ordered training/eval data with diff distribution\r\n\r\n- compute and log each batch auc\r\n\r\n\r\n**Any Other info.**\r\n\r\nI have implement a batch AUC method for myself and works fine. It changes the code of `tensorflow.python.ops.metrics_impl`. \r\n\r\nI also changed accuracy metric to enable the batch metric function.\r\n", "comments": []}, {"number": 40494, "title": "tf-nightly build spits out thousands of lines about \"whitelisting\" during training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6.2\r\n- CUDA/cuDNN version: release 10.0, V10.0.130\r\n- GPU model and memory: TITAN X\r\n\r\n**Describe the current behavior**\r\nI am attempting to train a model using the tf-nightly build using the tf.data.Dataset api. It works (and continues to train) but prints out thousands (if not hundreds of thousands) of lines about \"whitelisting\" parameters.\r\n\r\n**Other info / logs**\r\nThis is an example of what it's printing out:\r\n\r\nI0615 19:58:57.821352 140192954160896 ag_logging.py:140] Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_t\r\no_update_var at 0x7f80b7957620>: from cache\r\nWhitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x7f80b7957620>: from cache\r\nI0615 19:58:57.822939 140192954160896 ag_logging.py:140] Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_gr\r\nad_to_update_var at 0x7f80b7957620>\r\n    args: (<tf.Variable 'conv4_block11_1_bn/gamma:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/resnet152v2/conv4_block\r\n11_1_bn/FusedBatchNormGradV3:1' shape=(256,) dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x7f80b7957620>\r\n    args: (<tf.Variable 'conv4_block11_1_bn/gamma:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/resnet152v2/conv4_block\r\n11_1_bn/FusedBatchNormGradV3:1' shape=(256,) dtype=float32>)\r\n    kwargs: {}\r\n\r\nI0615 19:58:57.823056 140192954160896 ag_logging.py:140] Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_t\r\no_update_var at 0x7f80b7957620>: from cache\r\nWhitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x7f80b7957620>: from cache\r\nI0615 19:58:57.824649 140192954160896 ag_logging.py:140] Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_gr\r\nad_to_update_var at 0x7f80b7957620>\r\n    args: (<tf.Variable 'conv4_block11_1_bn/beta:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/resnet152v2/conv4_block1\r\n1_1_bn/FusedBatchNormGradV3:2' shape=(256,) dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x7f80b7957620>\r\n    args: (<tf.Variable 'conv4_block11_1_bn/beta:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradient_tape/resnet152v2/conv4_block1\r\n1_1_bn/FusedBatchNormGradV3:2' shape=(256,) dtype=float32>)\r\n    kwargs: {}\r\n\r\nI0615 19:58:57.824765 140192954160896 ag_logging.py:140] Whitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_t\r\no_update_var at 0x7f80b7957620>: from cache\r\nWhitelisted <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x7f80b7957620>: from cache\r\nI0615 19:58:57.826352 140192954160896 ag_logging.py:140] Converted call: <function OptimizerV2._distributed_apply.<locals>.apply_gr\r\nad_to_update_var at 0x7f80b7957620>\r\n    args: (<tf.Variable 'conv4_block11_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>, <tf.Tensor 'gradient_tape/resnet152v\r\n2/conv4_block11_2_conv/Conv2DBackpropFilter:0' shape=(3, 3, 256, 256) dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function OptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var at 0x7f80b7957620>\r\n    args: (<tf.Variable 'conv4_block11_2_conv/kernel:0' shape=(3, 3, 256, 256) dtype=float32>, <tf.Tensor 'gradient_tape/resnet152v\r\n2/conv4_block11_2_conv/Conv2DBackpropFilter:0' shape=(3, 3, 256, 256) dtype=float32>)\r\n    kwargs: {}\r\n", "comments": ["@adrian-dalessandro \r\nCan you please share a simple stand alone code for us to replicate the issue faced or if possible share a colab gist for us to analyse the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40494\">No</a>\n"]}, {"number": 40493, "title": "Refactor AutoCastVariable tests to use strategy_combinations", "body": "This PR refactors the `AutoCastVariable` tests to user `strategy_combinations` in order to properly test behaviour using a mirrored strategy with two devices.\r\n\r\n/cc @reedwm", "comments": ["Thanks for the fast review \ud83d\udc4d "]}, {"number": 40492, "title": "Reduce size of published wheel files (_pywrap_tensorflow_internal.so)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): N/A\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe size of the wheel files published for Tensorflow over the 2.x.x releases has increased dramatically and is reaching somewhat absurd levels. Taking for example the cp37 manylinux2010 wheel:\r\n\r\n2.0.0: 84 MB\r\n2.1.0: 422 MB\r\n2.2.0: 516 MB\r\n\r\nPresently, anyone depending on these wheels may have seen their download and build times quintuple over the 2.x.x lifespan, with resulting images etc also now significantly larger than before.\r\n\r\nThe increase in size appears to be dominated by a single 1.2 GB shared library file `_pywrap_tensorflow_internal.so`. Presumably this increase might relate to the switch from swig to pybind, with this .so file containing nearly half a million symbols. Many of these symbols are stacks of variations of parameters to single functions.\r\n\r\nObviously just asking for a file to be smaller isn't much help, but I would hope this can prompt some discussion about ways this can be remedied? Assuming it is related to the pybind switch, is there anything that can be learned from the prior swig architecture and applied in the current process?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAll users who depend on the Python .whl files published to PyPI. Users with custom builds of Tensorflow will likely also have this large .so issue.\r\n\r\n**Any Other info.**\r\nThere may already be an issue or discussion related to this, but GitHub search didn't reveal much.", "comments": ["Hey @aaliddell So the migration to pybind11 is not the primary cause of the size increase. It's the [CUDA compute capabilities](https://github.com/tensorflow/tensorflow/blob/692bb1da53493a6cf37dc28a4c1e1a82df32d9fa/tensorflow/tools/ci_build/release/ubuntu_16/gpu_py37_full/pip.sh#L43) that we package with each version. I think `tensorflow` at 2.0.0 was the CPU only version, but if you look at [1.15](https://pypi.org/project/tensorflow/1.15.0/#files) *(where the default package switched to GPU support)* we were hovering around 412MB as well. At `tensorflow-gpu==2.0.0` we were also around [380MB](https://pypi.org/project/tensorflow-gpu/2.0.0/#files). \r\n\r\nThat being said we are looking into packaging compressed versions of the computes. @chsigg has [submitted this](https://github.com/tensorflow/tensorflow/commit/cf1b6b3dfe9ba82e805fddf7f4462b2d92fe550a#diff-6c70c9243b0c415aafd2f909a553ee22) and we have already seen a size decrease in the `tf-nightly` binaries to around [350MB](https://pypi.org/project/tf-nightly/2.3.0.dev20200614/#files).", "Ok, that's great to hear and I'm glad it's being look at; I see the .so file is now halved in size in the nightly wheel that you've linked. I guess other inputs to that library have been looked at to see if similar gains are available? I don't have much visibility into what now dominates the size of this file.\r\n\r\nIn terms of this issue: is it worth holding to track actions for reducing the size, or are there better more-specific issues available to track this? I'll trust your judgement on what you'd like to do with this one.\r\n", "So we have internal trackers and goals to monitor and reduce the size. Since the particular issue you referenced is not the root cause, and we have significantly decreased the size for now, I'll close this issue and link it internally. Hope that is okay. Thanks for filing the issue and looking into this!"]}, {"number": 40491, "title": "Minimum pooling operation. ", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): Latest as of 6/15/20\r\n- Are you willing to contribute it (Yes/No): Yes.\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorflow/Keras has max pooling, and average pooling, but there is no minimum pooling. A minimum pooling operation feature would be great to have. \r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nThere could be a tf.nn.min_pool\r\n\r\n**Who will benefit with this feature?**\r\n\r\nMinimum pooling of embeddings can be powerful features, especially if those embeddings that are created with architectures which use a tanh activation (instead of rulu, sigmoid, etc.), since these embeddings would contain useful information in both the positive and negative direction of the embedding. \r\n\r\n\r\n", "comments": ["nevermind, it looks like https://www.tensorflow.org/api_docs/python/tf/math/reduce_min does the job "]}, {"number": 40490, "title": "Sign compare warning fixes batch 1 1", "body": "@mihaimaruseac \r\n\r\nPardon the large batch size, I am batching based on warning id cardinality, rather than the number of files.\r\nI will batch based on number of files committed going forward. ", "comments": ["@joker-eph There are no mlir related changes in this PR. ", "This is pesky. There were 35 files, and now only 6 are showing as changed. Requires investigation, I will take a further look. "]}, {"number": 40489, "title": "[Intel MKL] Adding DNNL ops (part 2) supporting threadpool work", "body": "Adding threadpool support for mkl-dnn ops (this is part-2 of 2 PRs)", "comments": ["Thanks for the suggestion, I have changed it!"]}, {"number": 40488, "title": "[Intel MKL] Adding MKL-DNNL ops (part 1) supporting threadpool work", "body": "Adding threadpool support for mkl-dnn ops (this is part-1 of 2 PRs)", "comments": []}, {"number": 40485, "title": "d", "body": "d", "comments": ["Can you please provide a description of your error?", "> > Can you please provide a description of your error?\r\n> \r\n> What I don't understand is that it searches for tensorflow in sites-application while I deposit it in the application folder\r\n\r\nHave you used os.path.join in your codebase instead of a file path?", "> > Can you please provide a description of your error?\r\n> \r\n> What I don't understand is that it searches for tensorflow in sites-application while I deposit it in the application folder\r\n\r\nYes, if you make an installer of any application it searches in sites-application. You can try **Hidden-import** parameter to explicitly link tensorflow to your build.", "Can you share your error when you tried using Pyinstaller? I have a better understanding of making the installer in it.", "@Lucysmith8,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the TensorFlow version you are using.\r\n\r\nAlso, please take a look at [this similar](https://github.com/pyinstaller/pyinstaller/issues/4200) issue and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Lucysmith8,\r\nPlease take a look at [this comment](https://github.com/pyinstaller/pyinstaller/issues/3362#issuecomment-375843871) from a similar issue and [this issue](https://github.com/pyinstaller/pyinstaller/issues/3942) thread, which seem to work.\r\n\r\nAlso, looks like this issue is a duplicate of [#40458](https://github.com/tensorflow/tensorflow/issues/40458). If it is could you please close one of them. Thanks!\r\n\r\n\r\n", "> > Can you share your error when you tried using Pyinstaller? I have a better understanding of making the installer in it.\r\n> \r\n> @ratansingh98 I changed the question to add world code.\r\n\r\nThank you @Lucysmith8 , i will look into it.", "> > Can you share your error when you tried using Pyinstaller? I have a better understanding of making the installer in it.\r\n> \r\n> @ratansingh98 I changed the question to add world code.\r\n\r\nIn the case of Pyinstaller, use TensorFlow 1.14 and add 'pkg_resources.py2_warn' as a hidden import. "]}, {"number": 40484, "title": "Bug in tf.strings.split vs tf.compat.v1.string_split", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\ntf.strings.split doesn't work as expected when the seperator is multiple characters.\r\n**Describe the expected behavior**\r\ntf.strings.split().to_sparse() should have the exact same behavior as tf.compat.v1.string_split()\r\nyet when multiple characters are provided, tf.strings.split doesn't work\r\n\r\n**Standalone code to reproduce the issue**\r\n[github gist](https://gist.github.com/tommywei110/85328711e3cf1c0544fb4aca14bda37b)\r\n\r\n`import tensorflow as tf`\r\n`sentence = tf.constant([['cat goes meow'], ['dog?goes!woof']])`\r\n`print(sentence)`\r\n`v2 = tf.strings.split(sentence,' !?').to_sparse()`\r\n`print(v2)`\r\n`v1 = tf.compat.v1.string_split(tf.reshape(sentence, [-1]), ' !?')`\r\n`print(v1)`\r\n", "comments": ["@tommywei110 Since TF 2.0+, the behavior of `tf.strings.split` has been adjusted to match the behavior of python's native split, i.e., \r\n```\r\n>>> 'cat goes meow'.split(' !?')\r\n['cat goes meow']\r\n>>> 'cat goes meow'.split(' ')\r\n['cat', 'goes', 'meow']\r\n>>> \r\n```\r\n\r\nFor that the behavior of `tf.strings.split` is expected.", "@tommywei110\r\nPlease update as per above comment", "I see how this complies with python's string split.\r\nBut on the other hand, I wish there's a clearer indication in the documentation to talk about this upgrade from v1.\r\nAnd from the usability standpoint, the equivalent of splitting on multiple characters by v1.string_split would be harder for the v2 strings.split to accomplish. In the python equivalent, I would have to use re.split.\r\nIs there any work to support this functionality?", "tf.text has a \"regex_split\" operation.\r\n\r\n```\r\n!pip install tensorflow-text\r\nimport tensorflow as tf\r\nimport tensorflow_text as tf_text\r\ntf_text.regex_split(\"dog?goes!woof\", \"[!?]\")\r\n```\r\n", "@tommywei110 Can I go ahead and close this issue as the issue has been resolved? Thanks!", "Thanks for the swift response from the team.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40484\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40484\">No</a>\n"]}, {"number": 40483, "title": "[TFTRT] Add Dynamic Shape Testing for ConvertSquare", "body": "Changes the ConvertSquare test to use the new testing API", "comments": ["Thanks for the review, the changes have been consolidated into one commit"]}, {"number": 40481, "title": "How to use sliding_window_batch in TF 2.2.0", "body": "How to use tf.contrib.data.sliding_window_batch in tf 2.2.0", "comments": ["@santoshreddy254,\r\n`tf.contrib` has been deprecated from TensorFlow 2.x.\r\n\r\nInstead, please take a look at the [window method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window) from `tf.data.Dataset` and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Window method from tf.data.Dataset cannot be integrated as a tf transformation inside tfx. I think is relevant. "]}, {"number": 40480, "title": "Fix unknown output shape issue in autograph for tf.equal", "body": "This PR tries to address the issue raised in #40471 where\r\nthe output shape of an autograph consists of tf.equal\r\ncould not inference correctly. Specifically\r\n`x.shape == [None, 10, 1]` and `y.shape == [None, 1, 4]`\r\nonly yield `shape == None` (should be `shape == [None, 10, 4]`).\r\n\r\nThe reason was that the shape inbference function for equal\r\ndidn't capture the cases where both x and y's dim are None.\r\n\r\nThis PR fixes the issue.\r\n\r\nThis PR fixes #40471.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mdanatg Thanks for the review. I have updated the PR to move the test case  to math_ops_test.cc. Also added additional shape inference cases with a better coverage. Please take a look and let me know if there are any issues."]}, {"number": 40479, "title": "Enable resize_images_with_crop_or_pad to pad with normal tensorflow.pad calls (mode, name, and constant_values)", "body": "I was wanting to resize_with_crop_or_pad and pad with values not equal to 0. Now you can pass the same arguments that will feed into pad into resize_with_crop_or_pad", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40479) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40479) for more info**.\n\n<!-- ok -->", "Sure, I'll start updating it and will @ you when it is finished\r\nThanks!", "@hyeygit Sorry for the delay, I have updated the example ^^\r\nEdit: had to make some spelling fixes*", "@brianmanderson Can you please check @hyeygit's comments and keep us posted. Thanks!", "@gbaned Absolutely ^^, I've updated the fork to reflect @hyeygit 's changes and recommendations\r\nThank you!", "@brianmanderson Can you please address Ubuntu Sanity errors? Thanks!", "@brianmanderson Any update on this PR? Please. Thanks!", "Hello, sorry for the delay, I will address this today ^^", "@gbaned updated, thank you ^^", "@gbaned @hyeygit Hello, I reviewed the sanity errors again today, looks like I keep failing to fit everything within the allotted amount of line space.. I think that everything should pass the pylinting now\r\nThank you for your patience\r\nBrian", "@brianmanderson Still, build failures are appearing, Can you fix those?. Thank you!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@brianmanderson Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 40478, "title": "Predcit function result is different from Training-Val using the same data", "body": "Version:\r\nTensorflow 2.0.0 \r\nKeras 2.3.1\r\nUbuntu 18\r\n\r\nI use predict to check the model performance on val data but it is totally different from the training log file. **Predict and Val in the training use the same dataset.**\r\n\r\nThe top layers of my model:\r\n`        \r\n        model = Sequential()\r\n        model.add(Embedding(input_dim = num_words,\r\n                            output_dim=EMBEDDING_DIM,\r\n                                    weights=[emb],  # remove this if you want to train your weight\r\n                                    input_length=maxlen,\r\n                                    trainable=train_embedding,\r\n                                    mask_zero=True))\r\n        ........\r\n        .........\r\n         model.add(Dropout(0.2))\r\n        # Heuristic model: outputs 1-of-num_classes prediction\r\n        model.add(BatchNormalization())\r\n\r\n        model.add(Dense(dense_layer_size, activation=\"relu\"))\r\n\r\n        model.add(Dense(1, activation=\"sigmoid\"))\r\n\r\n        self.model = model\r\n\r\n        self.model.compile(\r\n            optimizer=\"adam\",\r\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n            metrics=[\r\n                  tf.keras.metrics.AUC(curve=\"PR\", name=\"PR\"),\r\n                      tf.keras.metrics.AUC(curve=\"ROC\", name=\"ROC\"),\r\n                    \"accuracy\",\r\n                    tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),\r\n                     tf.keras.metrics.binary_accuracy],\r\n        )\r\n`\r\n\r\nTrain code\r\n`\r\nself.model.fit_generator(train_generator, epochs=epochs, verbose=verbose,\r\n                                     validation_data = validation_generator,\r\n                                     shuffle=True, callbacks=[checkpoint], class_weight=class_weights)\r\n`\r\nPrediction and Compute AUC-PR code:\r\n\r\n`\r\n\r\npadded_x = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)\r\n\r\nclasslable, pro = model.predict(padded_x, batch_size)\r\npro = np.squeeze(pro)\r\ny_test = np.squeeze(y_test)\r\nfrom sklearn import metrics\r\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, pro)\r\npr = metrics.auc(recall, precision)\r\nprint(\"PR {}\".format(pr))\r\n\r\n`\r\nUse Prediction compute PR-AUC:  0.2803784709997255\r\nBut  Val_PR: 0.4\r\n\r\nI tried some solutions I found, use model.save.save or from keras.backend import manual_variable_initialization\r\nmanual_variable_initialization(True).\r\n1. When I save the whole model weights, does the save function also save the Embedding layer?\r\n2. I am doing binary-classification task, am I correctly using the metrics in model.compile  function?", "comments": ["Hi, I use tensorflow.keras instead of keras and upgrade tensorflow to 2.2.0. ", "@Marvinmw \r\nDoes that resolve the problem, if it does please move this issue to closed status.", "yes.solved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40478\">No</a>\n"]}, {"number": 40477, "title": "Post-training full integer quantization produces model with float inputs/outputs", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Running from Colab notebook\r\n- TensorFlow installed from (source or binary): Running from Colab notebook\r\n- TensorFlow version (or github SHA if from source): v2.2.0-0-g2b96f3662b\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nColab notebook:\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb)\r\n\r\n**Failure details**\r\nThe conversion is successful, however the quantized model with float inputs/outputs (`/tmp/mnist_tflite_models/mnist_model_quant.tflite`) and the one with supposed int8 inputs/outputs (`/tmp/mnist_tflite_models/mnist_model_quant_io.tflite`) are identical. I've verified this by running \r\n```\r\ndiff mnist_model_quant.tflite mnist_model_quant_io.tflite\r\n```\r\nwhich resulted in an empty output (that is, the files are identical).\r\n\r\n**Any other info / logs**\r\nHere's what `mnist_model_quant_io.tflite` looks like when I open it with netron:\r\n![netron](https://user-images.githubusercontent.com/30210403/84680403-f1500400-af32-11ea-980f-4a7b4b013dc9.png)\r\nRather than having a \"Quantize\" node right after the float input and a \"Dequantize\" node right before the float output, I would like to have int8 inputs/outputs directly. How to do that?\r\n\r\n", "comments": ["Hi, \r\nby default the post training tool keeps the input and output types float so that users can conveniently use the model interchangeably with their float model code. Users can change the input type and output type via the inferece_input_type and inference_output_type flags.\r\n\r\nThe colab here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb has an example of that usage and these flags.\r\n\r\nHope that helps!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40477\">No</a>\n", "> Hi,\r\n> by default the post training tool keeps the input and output types float so that users can conveniently use the model interchangeably with their float model code. Users can change the input type and output type via the inferece_input_type and inference_output_type flags.\r\n> \r\n> The colab here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb has an example of that usage and these flags.\r\n> \r\n> Hope that helps!\r\n\r\nActually, the notebook you've referenced is the one I've referenced too. Setting `inference_input_type` and `inference_output_type` to `tf.uint8` (or `tf.int8`) still results in a model with float inputs/outputs, as witnessed by the fact that `mnist_model_quant.tflite` and`mnist_model_quant_io.tflite` are identical.\r\n\r\nI managed to solve the problem by updating to TensorFlow nightly 2.3.0, so it appears to be a problem with TensorFlow 2.2.0. With TF 2.3.0, I can obtain a model with **int8** inputs/outputs.\r\n\r\nI guess this issue can be closed now, although I believe you should at least update that notebook, because it doesn't work properly with TF 2.2.0"]}, {"number": 42806, "title": "Little bug? ", "body": "I think it's a bug\r\n![image](https://user-images.githubusercontent.com/7847271/84676251-f498bf00-af35-11ea-8bd4-845697e447e0.png)\r\n", "comments": ["I want to debug it... please assign me this role. I am starting my journey in opensource world. give me an opportunity to work on it.", "Not sure, but I believe you're looking at this notebook: https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/load_data/images.ipynb\r\n\r\nIt looks fixed so closing this issue. But, FYI, the TF 1.x notebooks have been archived and are no longer maintained. Thanks"]}, {"number": 40476, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.   Failed to load the native TensorFlow runtime.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10 Pro updated\r\nMobile device: None\r\n- TensorFlow version:**TensorFlow-2.2.0**\r\n- Python version:** python .3.8.3**\r\n- Installed using **pip**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**from keras.layers import Convolution2D**\r\nI'm sure that TensorFlow is installed in my current environment. But still, the following error is shown up when I tried to run the above command.\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-027d7fb732fd> in <module>\r\n----> 1 from keras.layers import Convolution2D\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 from . import losses_utils\r\n      8 from . import metrics_utils\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n----> 1 from .load_backend import epsilon\r\n      2 from .load_backend import set_epsilon\r\n      3 from .load_backend import floatx\r\n      4 from .load_backend import set_floatx\r\n      5 from .load_backend import cast_to_floatx\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\load_backend.py in <module>\r\n     88 elif _BACKEND == 'tensorflow':\r\n     89     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 90     from .tensorflow_backend import *\r\n     91 else:\r\n     92     # Try and load external backend.\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.eager import context\r\n      7 from tensorflow.python.framework import device as tfdev\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\n~\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\BASHEER\\.anaconda\\anaconda1\\envs\\deeplearning\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@basheerghub,\r\nPlease take a look at [this](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) comment from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40476\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40476\">No</a>\n"]}, {"number": 40475, "title": "Try to make _global_policy thread local", "body": "As in the source code `TODO` list.\r\nAlso we don't want to use not exposed methods in this class from mixed precision tests in https://github.com/tensorflow/addons/pull/1924\r\n", "comments": ["/cc @reedwm ", "The main issue with making this thread local is that `MirroredStrategy` spawns multiple threads. If the policy is thread local and the policy is updated, the new threads won't see the new values of the policy. If you run the test `//tensorflow/python/keras/mixed_precision/experimental:keras_test`, it should fail due to this issue.\r\n\r\nTensorFlow has lots of thread local variables. Normally to get around this, [this class](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/distribute/mirrored_run.py;l=242;drc=0f3562ba77d62abef6c55c7c7649f836398dbc6e) copies over the values of the thread local variables from the old thread to the new thread. Unfortunately, we cannot have it copy the value of the Keras policy, since we cannot introduce a new dependency from TensorFlow to Keras.\r\n\r\n@guptapriya, @yuefengz, can we introduce an API to allow new thread-local variables to be copied by distribution strategy? Alternatively, perhaps the policy should be kept as a global variable instead of a thread local variable.", "As an alternative is exporting `policy_scope` safe?", "Until the policy is thread-local, `policy_scope` is not thread safe, as it will still be setting a global, non-thread local variable. So I don't think exposing `policy_scope` will solve this issue.\r\n\r\nFor #1924, since unit tests run sequentially, you don't need to make policy thread-local. The recommend way of setting policies in unit tests is to set the policy back to float32 in the tearDown function, like [here](https://cs.opensource.google/tensorflow/models/+/master:official/nlp/modeling/networks/encoder_scaffold_test.py;l=57;drc=8c408bbeb9142979600fa671e454d001628d1cb7). Then within unit tests, you can set the policy however you wish and it will be set back to float32 before running a new unit test.", "@reedwm So why [this is working](https://github.com/tensorflow/addons/pull/1924/files#diff-fa100414eab669c5277ea22483702affR132) and instead using exposed API `set_policy` and back was not working?", "P.s. In the CI we are using `pytest -n` so we are not executing tests on a single process.", "It shouldn't make a difference. You can see the source code of [policy_scope](https://github.com/tensorflow/tensorflow/blob/f55ffa1ab219d5b521f9aaea0378463d81ceb778/tensorflow/python/keras/mixed_precision/experimental/policy.py#L568) is simply to set the policy at the start of the scope and set it back at the end. The try-finally is only to make sure it sets it back if an exception is thrown.\r\n\r\nI think `pytest -n` runs in separate processes, so each process should have its own value of the global policy. So having this be a global varaible shouldn't matter.\r\n\r\nIf you give me instructions to reproduce the test I can try running your PR locally to see why `set_policy` does not work and `policy_scope` does work.", "Yes I also think it is a multiprocess but this flavour was not working https://github.com/tensorflow/addons/blob/fba902866131ea4333c976118cb9f2bc324763e9/tensorflow_addons/optimizers/tests/lookahead_test.py#L131", "This Is the line for running test https://github.com/tensorflow/addons/blob/master/tools/testing/build_and_run_tests.sh#L38", "To fix, replace the line\r\n\r\n```\r\ntf.keras.mixed_precision.experimental.global_policy()\r\n```\r\n\r\nwith\r\n\r\n```\r\ntf.keras.mixed_precision.experimental.set_policy(\"float32\")\r\n```\r\n\r\nThe issue is `global_policy` only returns the global policy without changing it. Setting the global policy back to its default of \"float32\" fixes the issue.\r\n\r\nYou should also use a try-finally to ensure the policy is set back to float32:\r\n\r\n```python\r\n    try:\r\n        tf.keras.mixed_precision.experimental.set_policy(\"mixed_float16\")\r\n        model = tf.keras.models.Sequential()\r\n        model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))\r\n        model.compile(Lookahead(\"sgd\"), loss=\"mse\")\r\n    finally:\r\n        tf.keras.mixed_precision.experimental.set_policy(\"float32\")\r\n```\r\n\r\nThis means even if a line in the try-block throws an exception, the policy will still be set back to float32. This logic emulates what the `policy_scope` does.", "@reedwm Ok that seem to work I remeber that I had `float32` locally and that failed in the CI but probably was something else. \r\n\r\nWaiting here on what we want to do for the multi-thread case.\r\n\r\nAs a side note do you think that `set_policy()` without argument is handled correctly?", "> As a side note do you think that set_policy() without argument is handled correctly?\r\n\r\nCan you clarify? `set_policy` requires an argument.", "Yes sorry  I meant  `None` is not going to set it to global_policy() right?\r\n\r\nEdit:\r\nOk I checked that when \r\n```\r\nset_policy (None) \r\npolicy_defaults_to_floatx()\r\n``` \r\nreturn `True`", "> The main issue with making this thread local is that `MirroredStrategy` spawns multiple threads. If the policy is thread local and the policy is updated, the new threads won't see the new values of the policy. If you run the test `//tensorflow/python/keras/mixed_precision/experimental:keras_test`, it should fail due to this issue.\r\n> \r\n> TensorFlow has lots of thread local variables. Normally to get around this, [this class](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/distribute/mirrored_run.py;l=242;drc=0f3562ba77d62abef6c55c7c7649f836398dbc6e) copies over the values of the thread local variables from the old thread to the new thread. Unfortunately, we cannot have it copy the value of the Keras policy, since we cannot introduce a new dependency from TensorFlow to Keras.\r\n> \r\n> @guptapriya, @yuefengz, can we introduce an API to allow new thread-local variables to be copied by distribution strategy? Alternatively, perhaps the policy should be kept as a global variable instead of a thread local variable.\r\n\r\nWhat API are you thinking of? If this would be user-visible change anyway, is it acceptable to set the thread_local object in a `replica_fn`?", "Setting the policy instead of `None` instead of `\"float32\"` means the policy will default to floatx. However, floatx is almost always float32 anyway so in tests, I just set back to \"float32\" for simplicity. `None` works equally as well.\r\n\r\n> What API are you thinking of? If this would be user-visible change anyway, is it acceptable to set the thread_local object in a replica_fn?\r\n\r\nI don't think the policy should ever be set in a replica_fn (we should probably raise an error if this is done). But it will definitely be set outsdie a replica_fn and retrieved in a replica_fn.\r\n\r\nI'm thinking we should have a `tf.distribute.register_thread_local(thread_local) that takes in a `threading.local` object. Then when MirroredStrategy spawns threads, it would copy over the attributes of the thread local object to the new thread.\r\n\r\n", "`tf.distribute.register_thread_local(thread_local)` would introduce some global states to `tf.distribute`, which might be fine given mixed precision is global as well.\r\n\r\nWhat is the problem with setting thread local in a `replica_fn`? Could adding an argument to `run` as an option?", "I think introducing global state is necessary if we want the policy to be thread-local.\r\n\r\n> What is the problem with setting thread local in a replica_fn? Could adding an argument to run as an option?\r\n\r\nIf a thread in a replica_fn sets a thread local variable, does it remain set outside the replica_fn? If so, what happens if different threads set it to different values? If not, it may cause confusion if the thread local variable is changed but then is unchanged outside the replica_fn.\r\n\r\nAs a solution, we could allow it to be set in a replica_fn, but require it to be set to it's original value by the time the replica_fn ends. This allows scopes like `policy_scope` to be used in a replica_fn (if we expose `policy_scope`)", "I am thinking about to try to not expose new user interfaces but I don't know deeply the complete flow of these internals. \r\nIs there a way to identify that we are inside a `MirroredStrategy` thread?", "> f the policy is thread local and the policy is updated, the new threads won't see the new values of the policy. If you run the test //tensorflow/python/keras/mixed_precision/experimental:keras_test, it should fail due to this issue.\r\n\r\n@reedwm I ran `bazel test //tensorflow/python/keras/mixed_precision/experimental:keras_test` on this branch but the test is passing. What I need to do to let it fail?", "I was mistaken, `keras_test` does not run into this issue. To reproduce, a layer needs to be created inside a function passed to `MirroredStrategy.run`, which `keras_test` does not do. A natural way to reproduce the issue is by creating another layer in `build()`, which will be called in  `MirroredStrategy.run`. For example:\r\n\r\n```python\r\nfrom absl import app\r\nimport tensorflow as tf\r\n\r\n# Represents two sequential dense layers.\r\nclass DoubleDense(tf.keras.layers.Layer):\r\n\r\n  def build(self, _):\r\n    self.dense1 = tf.keras.layers.Dense(10)\r\n    self.dense2 = tf.keras.layers.Dense(10)\r\n\r\n  def call(self, inputs):\r\n    y = self.dense1(inputs)\r\n    return self.dense2(y)\r\n\r\n\r\ndef main(_):\r\n\r\n  # Create two logical CPUs so that we can use a multi-device mirrored strategy\r\n  (cpu,) = tf.config.list_physical_devices('CPU')\r\n  tf.config.set_logical_device_configuration(\r\n      cpu, [tf.config.LogicalDeviceConfiguration(),\r\n            tf.config.LogicalDeviceConfiguration()])\r\n\r\n  # Run the model\r\n  tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\r\n  with tf.distribute.MirroredStrategy(['/cpu:0', '/cpu:1']).scope() as strategy:\r\n    layer = DoubleDense()\r\n    def run_fn():\r\n      x = tf.ones((10, 10))\r\n      y = layer(x)\r\n      assert y.dtype == tf.float16, 'y has dtype: %s' % (y.dtype.name,)\r\n      return y\r\n    strategy.run(run_fn)\r\n\r\n\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\nIn this example, if policy is thread-local, the two `Dense` layers will not see the fact the policy is updated to `'mixed_float16'`. \r\n\r\nWith this PR, the example gives the error `AttributeError: '_thread._local' object has no attribute '_global_policy'`, since `thread_local._global_policy` is only set on the main thread. This is typically solved by a hasattr check, as in done [here](https://github.com/tensorflow/tensorflow/blob/821d1b087be221aa3c58be8e077d4553d6851ef2/tensorflow/python/distribute/cross_device_utils.py#L280), but that will not solve the main issue in that the `Dense` layers do not see that the policy is updated to `'mixed_float16'`. ", "> I am thinking about to try to not expose new user interfaces but I don't know deeply the complete flow of these internals.\r\n> Is there a way to identify that we are inside a `MirroredStrategy` thread?\r\n\r\nIf `tf.distribute.get_replica_context()` is None, then we are inside a `MirroredStrategy` thread.", "> If `tf.distribute.get_replica_context()` is None, then we are inside a `MirroredStrategy` thread.\r\n\r\n@yuefengz Also the coordinator is in a new thread how can I check that I am inside a coordinator?\r\n", "I've done a small hack but I am not sure if it could cover \"distributed\" spawned threads.", "The hack helps, but there are still cases where it won't work properly, such as if a user spawns their own threads or if a user sets the policy inside a `MirroredStrategy` thread.\r\n\r\nI think for now, we should keep the policy as a global variable. I do not see a strong use case for making this thread local yet. The primary use case would be to set the policy in a `MirroredStrategy` thread, but I'm not sure why a user would do that (we should probably raise an error if they try). If we do find a use case for making it thread local, we can add a `tf.distribute.register_thread_local` function in order to make the policy thread-local.\r\n\r\nI'm closing this PR for now. @bhack, sorry for the misleading TODO. If you want, you can create a PR to raise an error if the `set_policy` is called in a `MirroredStrategy` thread. Otherwise I can implement this.", "@reedwm What about two python threads launching two models one with mixed_precision policy and the other one without it?\r\nWhat is the behavior in this case?"]}, {"number": 40474, "title": "Copy reverse_sequence docstring to _v2 to remove deprecation notices", "body": "The [documentation](https://www.tensorflow.org/api_docs/python/tf/reverse_sequence?version=nightly) is currently showing deprecation warnings for arguments that were never included in V2.", "comments": []}, {"number": 40473, "title": "Fix docstring format of tf.executing_eagerly", "body": "", "comments": []}, {"number": 40472, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "Good day, \r\n\r\nI am new to TensorFlow and I am trying to install the package (cpu_version) but I am failing to.\r\n**Python version:** _Python 3.7, 64 bit (AMD64)] :: Anaconda, Inc. on win32_. \r\nI have been following the install steps as outlined here - [https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html](url) but when I get to the `import tensorflow as tf` part I am getting this error: \r\n\r\n_import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\nFailed to load the native TensorFlow runtime._\r\n\r\nPlease help :(\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Noted, thank you. So which earlier versions of TensorFlow is compatible with my computer?", "@Horaln \r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it helps. Thanks!\r\n\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) from here.\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40472\">No</a>\n"]}, {"number": 40471, "title": "Equal function forgets static shape in graph-mode in tensorflow 2.1 and 2.2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Microsoft Windows 10 Enterprise\r\n- TensorFlow installed from (source or binary): official pipy wheel\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nIn graph-mode when `x.shape == [None, 10, 1]`, `y.shape == [None, 1, 4]` then one would expect `(x==y).shape` to be `[None, 10, 4]`. This really works in tensorflow 2.0. In tensorflow 2.1 and 2.2 it is a completely undefined shape.\r\n\r\n**Describe the expected behavior**\r\nThe same behaviour as in tensorflow 2.0.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function(input_signature=[tf.TensorSpec([None, 10, 1]), tf.TensorSpec([None, 1, 4])])\r\ndef fun(x, y):\r\n    z = x == y\r\n    tf.print(\"z.shape =\", z.shape)\r\n    return z\r\n\r\nfun(tf.ones([5, 10, 1]), tf.ones([5, 1, 4]));\r\n```\r\nIn tensorflow 2.0, this code prints `z.shape = TensorShape([None, 10, 4])`.\r\nIn tensorflow 2.2 and 2.1, this code prints `z.shape = TensorShape(None)`.", "comments": ["I have tried in colab with TF version 2.1, 2.2 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cf487e179e38859d2301e11969b41bbb/untitled25.ipynb).Thanks!", "Added a PR #40480 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40471\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40471\">No</a>\n"]}, {"number": 40470, "title": "Using own dataset in microspeech project doesn't work ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): currently working with Google Colab \r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): Tensorflow 1.x\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33 BLE Sense\r\n\r\n**Describe the problem**\r\n\r\nHello everyone, \r\n\r\nI just started to get into the world of machine learning and deep learning and I still need to learn a lot, but I hope that there might be someone who faced similar problems to the ones that I try to solve at the moment. \r\n\r\nA few weeks ago, I started working with the microspeech example for microcontrollers. My goal was (and still is) to get the project working with an own wake word. To save time, I decided to expand the speech commands dataset with a folder containing my own data. My wanted wake word is a german one (I'm not sure if this is an important information but I try to tell as much information as possible at the moment). \r\n\r\nAt first, I only used samples of my own voice. To get more samples in a short time, I combined the original recordings with ones that were slightly manipulated. \r\nAfter that, I uploaded my data to Google Drive so that I could import it to Colab. I changed the 'wanted word' section and the 'data_dir' section according to my wanted word. \r\n\r\nTraining seemed to work well and I changed the code in the microspeech project in the Arduino IDE and uploaded it to the Arduino. I expected the green LED to flash when I say the wake word and the blue one to do so if I say some other word of the dataset (as it is intended in the original project). But no matter what I said, only the blue one LED flashed. \r\n\r\nIn a second step, I changed my dataset because I thought that it might be a problem that my dataset only consists of recordings of my own voice. So I collected new data with recordings from different people and tried everything that I did before with my new data. \r\nThis time the arduino behaves a bit different as the green LED flashes randomly when nothing is said at all. And when I say my wake word, the blue LED is still flashing as it did before. \r\n\r\nAs I said at the beginning, I'm new to this topic and so I only can imagine some possible causes for my issues (please feel free to correct me if some things don't make sense):\r\n\r\n- I didn't collect enough samples of my wake word (at the moment there are 820 samples in the folder)\r\n- there might be a problem that I only want to detect one word instead of two or more\r\n- I forgot to change sth. in the Arduino project code or did sth. wrong\r\n- it could be a problem that my dataset is a mix of English and German words \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nAfter training, I copy the code and paste it into the _micro_features_tiny_conv_micro_features_model_data.cpp_ and I also correct the data length parameter at the end of the source file. \r\n\r\nThen, I change _micro_features_micro_model_settings.cpp_ and _micro_features_micro_model_settings.h_ according to my wake word and the number of labels that I've got. \r\n\r\nFinally I go to the _arduino_command_responder.cpp_ and change the first if condition to **if(found_command[0] == 'h')**, as the wake word begins with an h. I erase the second if condition because I don't have a second wake word. \r\n\r\nThen I upload the project to the arduino. \r\n\r\nIf there are any information missing please tell me and I will provide them.\r\n\r\nKind regards, \r\neeesi", "comments": ["Hi eeesi, the issue is the Tensorflow Lite library for Arduino is not compatible with the models produced by the newly updated Google Collab code. A \"fix\" is to download the Tensorflow repository and run test_arduino.sh to create an up to date zip library to import into the Arduino IDE. I put fix in parentheses because while it does detect audio the inference runs too slow with the example micro speech code which causes issues. As of right now this seems to be an ongoing issue the Tensorflow Lite team has not addressed so I am glad more people are bringing it to attention here. ", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40470\">No</a>\n"]}, {"number": 40469, "title": "Tensorflow Lite subtraction in TOCO quantized model takes longer than all the convolutions in a MobileNet v3 model", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu/ARM\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: \r\n\r\n**Describe the current behavior**\r\nCreated MobileNet v3 model that includes subtraction and multiplication for normalization in the graph. Quantized the model with TOCO using post training quantization. Model was running very slow on ARM and x86. Profiling shows suctraction is responsible for 21% of the time\r\n![image](https://user-images.githubusercontent.com/6794993/84648998-b2538b80-aefd-11ea-8407-36839ea175cf.png)\r\n\r\nUsing MLIR produces much more reasonable results\r\n![image](https://user-images.githubusercontent.com/6794993/84649672-d499d900-aefe-11ea-9324-9cde5fd5c0ce.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nLatency due to subtraction should be negligible\r\n\r\nCan supply model and other info if necessary\r\n", "comments": ["hi, quantized Sub is not optimized yet.\r\n", "Thanks for the update. Is there a timeline for this?", "can you share your model for testing?\r\n\r\nthanks", "Unfortunately it looks like not in a public forum like this. Happy to share privately", "what's your sub's config? shape? datatype?\r\n\r\nthanks", "@msalvaris It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40469\">No</a>\n"]}]