[{"number": 19731, "title": "How to release GPU memory after sess.close()?", "body": "hi, all:\r\n     I'm training models iteratively. After each model trained, I run sess.close() and recreate a new session to run a new training process. But it seems that the GPU memory was not relseased and it's increasing constantly. \r\nI tried tf.reset_default_graph() before run session also typed gc.collect() after sess.close(), but takes no effect. \r\n     How could I release GPU memory timely to avoid OOM error please? \r\n     Thanks!", "comments": ["I am not 100% sure of specifying GPU releasing, but these are some methods that I used. \r\n\r\n1. You would already know this method \r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\nsess = tf.Session(config=config)\r\n\r\n2. Maybe this will help? \r\nhttps://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\r\n\r\n3. tf.Session. reset() -> this method just resets the session but I don't believe it doesn't necessarily let go of the memory. ( **## I strongly recommend this one**, and here is the link to a blog that I used to train multiple models : http://bit.ly/2J8lhqz) \r\n\r\nI think issue #1578 had a similar problem, here is the link: https://github.com/tensorflow/tensorflow/issues/1578", "hi, @JaeDukSeo \r\nthanks for your kind advices and I tried them\r\n1. I initialized session with config as you said.\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    sess = tf.Session(config=config)\r\n2. I didn't set 'per_process_gpu_memory_fraction', because I have only one process and total gpu memory can be used\r\n3. I tried sess.reset(), but errors occured, which indicates that reset() takes at least 1 argument (0 given). https://www.tensorflow.org/api_docs/python/tf/Session#reset shows that reset() is implemented for distributed session.  I didn't use distributed session also did not set target arg here.\r\n\r\nWhy the gpu memory usage is still lingering after sess.close() and del graph?\r\nHow could I use tf.Session.reset() with a single machine and a single session?\r\nIs there any other advice for releasing resources?\r\nany advice would be appreciated :)", "hi,all\r\n    I tried method referred here:https://github.com/tensorflow/tensorflow/issues/17048\r\n    and the memory could be released now!\r\n    Thanks a lot~", "haha I am glad! and my bad the commend was actually \r\ntf.reset_default_graph()\r\n<img width=\"410\" alt=\"screen shot 2018-06-04 at 8 34 06 am\" src=\"https://user-images.githubusercontent.com/22832406/40917823-8f7fc1a6-67d2-11e8-92a9-19191a26319d.png\">\r\n", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have a similar problem. I have a couple of questions in this regard:\r\n\r\n@githubgsq when you mention about the method from [#17048](https://github.com/tensorflow/tensorflow/issues/17048), do you mean moving your TensorFlow session code into a subprocess? So when the subprocess exits, the GPU memory is released?\r\n\r\n@JaeDukSeo You mention setting `allow_growth=True`, but if my model is very large and a large amount of memory is allocated to TensorFlow even after `allow_growth=True`, then it will not be deallocated, right? (Since the [docs](https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth) say that \"Note that we do not release memory, since that can lead to even worse memory fragmentation\").\r\n\r\nThanks in advance.", "@saxenarohan97 yes, a subprocess run the session code. I also called tf.reset_default_graph() before the subprocess executed.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@JaeDukSeo do you happen to have an answer for @saxenarohan97 ? ", "@cy89  I agree with @saxenarohan97 if allow_growth is set to true I don't think it automatically deallocates GPU ", "@JaeDukSeo thanks for your reply!\r\nI'll close, as it looks like this thread has answers to all open questions. ", "I use [numba ](http://numba.pydata.org/numba-doc/0.13/CUDADevice.html)to release the gpu. With tensorflow I can not find a effect method.", "@TanLingxiao were you able to find any other method? numba is a great way with the drawback being that once you run cuda.close(), you can no longer run your process again in the same process/session. Was hoping that tensorflow has config option to free GPU Memory after the processing ends.", "Hi I'm new to GitHub and have been trying to get Tensorflow running in Python. \r\n\r\nIn principle I have the thing up and running, but the GPU memory is not released, causing an OOM error at some point. These few lines already clutter the memory.\r\n\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nsess.close()\r\n\r\nI've been googling the problem but so far I have only managed to solve it by either\r\n\r\n- Clossing the console (and thus losing all varibales which is what I am actually trying to avoid)\r\n- Using numba to kill the cuda device (however, this only works once) - see above. In that case my code looked something like this:\r\n\r\nfrom numba import cuda\r\ncuda.select_device(0)\r\n#do tf stuff\r\ncuda.close()\r\n#the memory was released here!\r\ncuda.select_device(0)\r\n#to tf stuff -> caused an OOM\r\n(compare to https://numba.pydata.org/numba-doc/dev/cuda/device-management.html)\r\n\r\nAs mentioned above I would like to avoid killing the session and thus losing my varibales in memory (used to train a NN).  \r\n\r\n- I am aware that I can alocate only a fraction of the memory (cfg.gpu_options.per_process_gpu_memory_fraction = 0.1) or let the memory grow (cfg.gpu_options.allow_growth=True) and this both works fine, but afterwards I simply am unable to release the memory. \r\n\r\n- I have tried to put it into threads and pools following ideas and workaounds people suggested. \r\n\r\n- Also the K.session_close() and/or gc.collect() combination does not work. \r\n\r\n- I also downgraded tf to 1.9 and Keras to 2.1.9 (second not really relevant for the above code). \r\n\r\n- I have also upgraded my graphics card driver to the newest release (see below and note the memory which is not released after the call from above).\r\n\r\n![grafik](https://user-images.githubusercontent.com/43452440/45842749-e6e0e600-bd1d-11e8-8023-387c219b1bbf.png)\r\n\r\n\r\nI'd be very thankful for any suggestions what to do with the code snippet from above to ensure that the GPU memory is free in the end\r\n ", "> Hi I'm new to GitHub and have been trying to get Tensorflow running in Python.\r\n> \r\n> In principle I have the thing up and running, but the GPU memory is not released, causing an OOM error at some point. These few lines already clutter the memory.\r\n> \r\n> import tensorflow as tf\r\n> sess = tf.Session()\r\n> sess.close()\r\n> \r\n> I've been googling the problem but so far I have only managed to solve it by either\r\n> \r\n> * Clossing the console (and thus losing all varibales which is what I am actually trying to avoid)\r\n> * Using numba to kill the cuda device (however, this only works once) - see above. In that case my code looked something like this:\r\n> \r\n> from numba import cuda\r\n> cuda.select_device(0)\r\n> #do tf stuff\r\n> cuda.close()\r\n> #the memory was released here!\r\n> cuda.select_device(0)\r\n> #to tf stuff -> caused an OOM\r\n> (compare to https://numba.pydata.org/numba-doc/dev/cuda/device-management.html)\r\n> \r\n> As mentioned above I would like to avoid killing the session and thus losing my varibales in memory (used to train a NN).\r\n> \r\n> * I am aware that I can alocate only a fraction of the memory (cfg.gpu_options.per_process_gpu_memory_fraction = 0.1) or let the memory grow (cfg.gpu_options.allow_growth=True) and this both works fine, but afterwards I simply am unable to release the memory.\r\n> * I have tried to put it into threads and pools following ideas and workaounds people suggested.\r\n> * Also the K.session_close() and/or gc.collect() combination does not work.\r\n> * I also downgraded tf to 1.9 and Keras to 2.1.9 (second not really relevant for the above code).\r\n> * I have also upgraded my graphics card driver to the newest release (see below and note the memory which is not released after the call from above).\r\n> \r\n> ![grafik](https://user-images.githubusercontent.com/43452440/45842749-e6e0e600-bd1d-11e8-8023-387c219b1bbf.png)\r\n> \r\n> I'd be very thankful for any suggestions what to do with the code snippet from above to ensure that the GPU memory is free in the end\r\n\r\nExactly same problem for me.\r\nVery hope for some suggestion about this.", "Have the same issue hear; I can only fit a model once using Keras with TensorFlow backend, and the second time (with the very same model), it just crashes (OOM error).\r\nAlso appreciate suggestions here.", "+1", "I have solved this issue with some kind of duct tape. I've used the bash script, which launched my module multiple times, after every execution the GPU memory has been released. It is also possible to use `subprocess.Popen` to call the module, that uses TF with GPU", "I have solved it by running the session in a separate Thread. When the session is completed, the memory used by such process is released, when the process is killed.  Remember to save your session results on disk in the same method. \r\n\r\n", "@marinone94 do you have code / proof?  I tried a thread too but the only thing that worked for me is to use a subprocess.   Others have seen similar: https://github.com/tensorflow/tensorflow/issues/20387 https://github.com/tensorflow/tensorflow/issues/15880", "@yurmchg @marinone94 \r\nYou guys are right. I have tried this way, and it does work.\r\nHowever, I think it' not very efficient method for program. Because it may cause many problem and inconvenient when we want to communicate with other process(ex: send or receive data).\r\nVery Hope tensorflow official can provide the release gpu memory function, or just make it release after end this thread (not process).\r\n", "@p890040 can you please post a code example?  I definitely tried python threads to no avail.", "> @p890040 can you please post a code example? I definitely tried python threads to no avail.\r\n\r\nThis may help\r\n[https://github.com/tensorflow/tensorflow/issues/17048#issuecomment-368082470](url)", "import tensorflow_gpu as tf\r\nimport tensorflow_hub as hub\r\nimport threading\r\nimport pickle\r\nimport os\r\nimport time\r\n\r\necondModuleUrl = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\r\n'''\r\nQUICKLY REWRITTEN, THERE MIGHT BE TYPOS AND/OR OTHER ERRORS, NO TIME TO CHECK IT\r\nEDIT: YOU MIGHT HAVE TO ADD CORRECT INDENTATION\r\nI might have missed something for running on GPU, if it doesn't work add:\r\n\r\ntfconfig = tf.ConfigProto():\r\ntfconfig.gpu_options.allow_growth = True\r\n\r\nand use it to instantiate the session, but I'm quite confident you don't need it\r\n'''\r\n\r\ndef ThreadCall(task, args):\r\n    \"\"\"\r\n    Starts a new thread where task with arguments args is running.\r\n    If args is a non-iterable pbject, eg audio = AudioFile(), use args = (audio,) instead of args = audio\r\n    If no args, use args = []\r\n    \"\"\"\r\n    t = threading.Thread(target = task, args = args)\r\n    t.start()\r\n\r\ndef SentenceEmbedding(sentences):\r\n    embed = hub.Module(econdModuleUrl)\r\n    if os.path.exist(r'.\\example'):\r\n        os.remove(r'.\\example')\r\n    with tf.Session() as session:\r\n        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\r\n        message_embedding = session.run(embed(sentences))\r\n        session.close() #useless\r\n    with open(r'.\\example', 'wb') as f:\r\n        pickle.dump(message_embedding, f)\r\n    \r\n\r\nsentences = ['This is the first sentence', 'and this is the second', 'just to show how to release ']\r\nThreadCall(task=SentenceEmbedding, args=sentences)\r\ntime.sleep(1)\r\n#you have to set your own way to check when the thread is done, this is just a stupid example\r\nwhile True:\r\n    if os.path.exist(r'.\\example'):\r\n        break\r\nwith open(r'.\\example', 'rb') as f:\r\n    message_embedding  = pickle.load(message_embedding, f)\r\n\r\n\r\n", "> I use [numba ](http://numba.pydata.org/numba-doc/0.13/CUDADevice.html)to release the gpu. With tensorflow I can not find a effect method.\r\n\r\nthanks for this!", "`numba` cause Segmentation fault.\r\n\r\nExample:\r\nhttps://stackoverflow.com/questions/58792739/tensorflow-model-wrapper-that-can-release-gpu-resources\r\n\r\nSo as I understand the only method so far is to run model load and predict in separate process or maybe pass same session to any used model clearing old graph before loading new model.", "I tried the following things, but, none **guaranteed** to free the memory up\r\n```\r\n# didn't work for me\r\ntf.reset_default_graph()\r\nK.clear_session()\r\ncuda.select_device(0); cuda.close()\r\nmodel = get_new_model() # overwrite\r\nmodel = None\r\ndel model\r\ngc.collect()\r\n``` \r\nCreating [separate processes](https://stackoverflow.com/a/58944126/3125070) always worked and **guaranteed** that the memory is freed up. Further that helped me manage and allocate resources the way I wanted.\r\n```\r\n# works for me\r\nprocess_train = multiprocessing.Process(train_model, args=...())\r\nprocess_train.start()\r\nprocess_train.join()\r\n```", "+1 @saravanabalagi subprocess works for me, but that is in many cases impractical.  ", "Seeing this issue (and its many variations) closed, we see it is a design flaw and should move on. For new projects that need no JavaScript runtime I will always recommend PyTorch, which has a dedicated function for releasing GPU memory, `torch.cuda.empty_cache()`\r\n\r\nSaves a lot of hacking around memory management and as a bonus the python API proved more stable over time...", "> tf.reset_default_graph()\r\n\r\nThis is being deprecated by TF 2.0 and there is no equivalence in TF 2.0.", "Please reopen this issue.", "Same issue here, please reopen the issue... TF 2.0 didn't solve anything, just made my code run slower...", "tf.reset_default_graph() not working in 2.1.\r\nHow to release GPU memory after model training in Eager execution mode.", "@cy89 Please re-open.  Closing this issue was not helpful to the community.", "Same issue, please reopen the issue.", "Same issue, please reopen.", "Same issue. Seems crazy that a framework like Tensorflow does not even have a one simple way to release the memory.", "@dd1923 Can you try this one. On TensorFlow 2.1 it seems work\r\n`import tensorflow.keras.backend as K`\r\n`K.clear_session()`", "@asis-shukla Tried it. Does not work. Still leaves all gpus full unfortunately.", "@asis-shukla doesn't work.", "Not work tf-2.2", "Someone please give a correct answer!", "> Someone please give a correct answer!\r\n\r\n@yinghuang  As a lot of other people, I'm using subprocesses to release the memory, which works fine.\r\n\r\nE.g.\r\n```python\r\nimport multiprocessing\r\n\r\ndef run_inference_or_training(param1, param2, ...):\r\n    ...\r\n\r\nif __name__ == '__main__':\r\n    p = multiprocessing.Process(\r\n        target=run_inference_or_training,\r\n        args=(param1, param2, )\r\n    )\r\n    p.start()\r\n    p.join()  # Add this if you want to wait for the process to finish.\r\n```\r\nWhen the function `run_inference_or_training` is done, the process `p` will be closed down and the memory allocated inside `run_inference_or_training` will be released.", "There are ways to reset default graph in tf2 \r\n1:\r\n\r\nwith tf.Graph().as_default():\r\n    main()\r\n2:\r\nfrom tensorflow.python.framework import ops\r\nops.reset_default_graph()\r\nsess = tf.InteractiveSession()"]}, {"number": 19730, "title": "[tflite] make benchmark_model tflite build", "body": "//tensorflow/contrib/lite/tools:benchmark_model doesn't build\r\nfor either x86 or android targets. With this, something like\r\n\r\n```\r\nbazel build --config opt \\\r\n//tensorflow/contrib/lite/tools:benchmark_model\r\n```\r\nor\r\n```\r\nbazel build --config android_arm64 --config monolithic \\\r\n--cxxopt=-std=c++11 --linkopt=-llog \\\r\n//tensorflow/contrib/lite/tools:benchmark_model\r\n```\r\nworks", "comments": ["It's fixed by e2d30082. I'll close this."]}, {"number": 19729, "title": "Getting empty tensorflow installation", "body": "### System information\r\n\r\n\r\n== cat /etc/issue ===============================================\r\nLinux analyst-PC 4.4.0-124-generic #148~14.04.1-Ubuntu SMP Thu May 3 07:26:53 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux analyst-PC 4.4.0-124-generic #148~14.04.1-Ubuntu SMP Thu May 3 07:26:53 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.3            \r\nnumpydoc                           0.7.0             \r\nprotobuf                           3.5.2.post1       \r\ntensorflow-gpu                     1.5.1             \r\ntensorflow-tensorboard             1.5.1             \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSun Jun  3 23:29:37 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660     Off  | 00000000:01:00.0 N/A |                  N/A |\r\n| 30%   35C    P8    N/A /  N/A |    266MiB /  1998MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n \r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n#running from console says about unsupported operation.\r\n#in jupyter notebook:\r\nAttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\n\r\n\r\n### Describe the problem\r\nThen tried to import tensorflow, I get no error However tf.set_random_seed(random_state) - returns AttributeError: module 'tensorflow' has no attribute 'set_random_seed'\r\n\r\nThen in the jupyter notebook with shit-tab get library help:\r\nType:        module\r\nString form: <module 'tensorflow' (namespace)>\r\nDocstring:   <no docstring>\r\n\r\n\r\nSeems to isnt righ. However :\r\n\r\n ~ $ pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.5.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /home/analyst/anaconda3/lib/python3.6/site-packages\r\nRequires: absl-py, wheel, six, tensorflow-tensorboard, numpy, protobuf\r\nRequired-by: \r\n\r\n$ pip show tensorflow  #Returns nothing\r\nWhat might be wrong?\r\n\r\n\r\nI have 2 active namaspaces: how to fix that:\r\n\r\n    > tf.__path__\r\n    \r\n    _NamespacePath(['/home/aa/tensorflow', '/home/aa/anaconda3/lib/python3.6/site-packages/tensorflow'])", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I used to get this problem as well. for me simply reinstalling did the trick. I know this is an obvious answer question but did you tried to reinstall it as well? ", "I have tried to reinstall couple of times, different versions, even by trying manually deleting folders.\r\n", "@Diyago one method that worked for me was using the precomplied version\r\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow\r\n\r\nMaybe give this a try and for precomplied GPU version: https://github.com/mind/wheels\r\n\r\nLet me know how it goes I wanna help", "Was this helpful?", "haha I want to know as well", "Installing precomputed version leads me to another error, problems with intel libraries. Trying to solve such problem I found the solution\r\n`\r\nconda install -c anaconda tensorflow-gpu \r\n`\r\n\r\nYou can specify here version you want to install as well", "Great so lets close issue??", "yep;)", "OK, closing, thanks very much @JaeDukSeo for solving the problem."]}, {"number": 19728, "title": "Branch 199005227", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Looks like passing all tests.\r\nCLA is ok to override.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Thanks @gunan! I'm going to start a pull FYI."]}, {"number": 19727, "title": "Re-Merge accidentally reverted change", "body": "", "comments": []}, {"number": 19726, "title": "[WIP] Sync package version of double-conversion between bazel and cmake", "body": "*NOTE: There are some issues with windows build, not ready to be reviewed yet.*\r\n\r\nThis fix tries to sync package version of double-conversion between\r\nbazel and cmake.\r\n\r\nThe double-conversion package was added in PR #12102 and was reverted\r\nin PR #15133. At that time the package version was `5664746` for both\r\nbazel and cmake.\r\n\r\nLater on, the double-conversion was re-introduced in PR #18746. The\r\npackage version of double-conversion in bazel has been advanced\r\nto `3992066` but the version in cmake remains the old `5664746`.\r\n\r\nThis fix updates the double-conversion version in cmake so that\r\nit is synced with the version (`3992066`) used in bazel.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Close this PR for now. Will re-open once the issue is resolved."]}, {"number": 19725, "title": "I cannot code to use  individual elements of a tensor outside a session ", "body": "I am writing a model in Keras, where the output of one of the layers gives me the coordinates of a mask that I want to apply on the image input. The problem is that I could not find any way to do this, methods such as tf.slice() do not work if you are not inside a session. \r\nI want to do equivalent to this:\r\nx=dense(2)(x)\r\nA=np.zeros((224,224, 3)) or K.zeros((224,224,3))\r\nx=int(x)\r\nA[x[0],x[1],:]=1\r\nwhere x comes from previous layers and is reassigned in the layer shown (dense layer from keras), the output is two numbers representing the position in the picture, then I create A which is a mask to be multiplied with the input (input not shown), and I need both, make the output of the layer an integer, and then use it to add ones to the mask A at position (x[0],x[1]) (and other ones too, but just having one is enough to know the method. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "You can't write to individual elements.\r\n\r\nTensorflow (in graph mode) is a declarative framework. You describe the computation first, in python, then use `session.run(..)` to execute the computation in another step.\r\n\r\nWith eager mode, you can run in python as you normally would. Please try it out.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19724, "title": "Add int16 support for `tf.as_string`", "body": "In `tf.as_string`, integers are mostly supported (`int8`, `int32`, `int64`) but not `int16`. This fix adds the `int16` support for `tf.as_string`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19723, "title": "ERROR while training ( tensorflow.python.framework.errors_impl.NotFoundError)", "body": "hello . first I'm sorry for my weak english lang.\r\nWould you please help me to fix this problem?\r\n![11](https://user-images.githubusercontent.com/24391257/40887860-748c26e4-673e-11e8-83f2-2b170205b4a5.png)\r\n\r\nI have object-detection.pbtxt and I gave different models path in command but I got same error again \r\n\r\nI follow this youtube  video https://www.youtube.com/watch?v=JR8CmWyh2E8\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19722, "title": "Faild clone libFuzzer when compiling tensorflow on win10", "body": "When compiling tensorflow on windows 10 with vs2017, follow error occor:\r\n\r\n  Cloning into 'third_party/libFuzzer'...\r\n  fatal: unable to access 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/': Failed to connect to chromium.googlesource.com port 443: Timed out\r\n  fatal: clone of 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer' into submodule path 'third_party/libFuzzer' failed\r\n  Failed to recurse into submodule path 'third_party/bloaty'\r\n  CMake Error at D:/program/tfMake/tensorflow-master/tensorflow/contrib/cmake/build/grpc/tmp/grpc-gitclone.cmake:93 (message):\r\n    Failed to update submodules in:\r\n    'D:/program/tfMake/tensorflow-master/tensorflow/contrib/cmake/build/grpc/src/grpc'\r\n\r\nHowever, open https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/  I see:\r\n024d10d [libFuzzer] Delete llvm/lib/Fuzzer by vitalybuka \u00b7 8 months ago master \r\n\r\nThe libFuzzer is already deleted. I think this is a bug need to fix.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: git clone\r\nTensorFlow version: latest\r\nBazel version: \r\nCUDA/cuDNN version: CUDA9.0, cuDNN 7.0 for win10\r\nGPU model and memory: GPU GTX970M, 3G\r\nExact command to reproduce:\r\n\r\ncode for CMake:\r\ncmake .. -DCMAKE_C_COMPILER=\"C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/cl.exe\" -DCMAKE_CXX_COMPILER=\"C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/cl.exe\" -T v140 -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:/Users/hasee/Anaconda3/python.exe -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\" -Dtensorflow_BUILD_SHARED_LIB=ON -Dtensorflow_ENABLE_GRPC_SUPPORT=ON -DCUDA_HOST_COMPILER=\"C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/cl.exe\" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -Dtensorflow_BUILD_CC_EXAMPLE=OFF -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF\r\n\r\ncode for MSBuild:\r\n& \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\MSBuild.exe\" ALL_BUILD.vcxproj /m:1 /p:CL_MPCount=1 /p:Configuration=Release /p:Platform=x64 /p:PreferredToolArchitecture=x64 /filelogger", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I meet the same situation, and compile tensorflow failed", "Could you please provide all the details asked for in the issue template? For example, which version of the codebase is being built, what commands are being executed etc.\r\n\r\nThanks!", "## Objective\r\nTo inference my trained model in Windows using executable program without python, so I try to build tf follow [cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) instructions.\r\n## Enviroment\r\nHave I written custom code\r\n**No**\r\nOS Platform and Distribution\r\n**Win10 Pro, VS2015**\r\nTensorFlow installed from\r\n**Source code**\r\nTensorFlow version\r\n**Git pull tf yesterday(08, Aug) and use master branch**\r\nBazel version\r\n**Using cmake 3.10**\r\nCUDA/cuDNN version\r\n**Do not use GPU, tensorflow_GPU_ENABLE=OFF, but my PC has CUDA 9.0, cuDNN 7**\r\nGPU model and memory\r\n**N/A**\r\nOther cmake options\r\ntensorflow_BUILD_PYTHON_BINDINGS = OFF\r\ntensorflow_BUILD_ALL_KERNELS = ON\r\ntensorflow_BUILD_CC_EXAMPLE = ON\r\ntensorflow_ENABLE_POSITION_INDEPENDENT_CODE = ON\r\ntensorflow_ENABLE_SNAPPY_SUPPORT = ON\r\n## Problem: \r\n### Build failed with gRPC supported\r\n**Git clone grpc/third_party/libFuzzer failed**\r\nCloning into 'third_party/libFuzzer'...\r\nfatal: unable to access 'https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/': Failed to connect to chromium.googlesource.com port 443: Timed out\r\n### Build failed without gRPC supported\r\n**tensorflow_ENABLE_GRPC_SUPPORT = OFF,  but compile failed bcz cannot find \"grpc++\u201c header file, why still need grpc related file?**\r\nError\tC1083\tCannot open include file: 'grpc++/grpc++.h': No such file or directory (compiling source file D:\\Workspace\\Github\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\eager\\context.cc)\ttf_core_cpu\tD:\\Workspace\\Github\\tensorflow\\tensorflow\\tensorflow\\core\\distributed_runtime\\rpc\\grpc_server_lib.h\t21\t\r\n", "@7oud The problem is caused by libFuzzer which is git submodule bloaty of gRPC. In my solution, you need download libFuzzer from https://chromium.googlesource.com/chromium/llvm-project/llvm/lib/Fuzzer/ (using version 273735e) and build your own git repo libFuzzer. Then, you need fork bloaty and gRPC to your own branch. Then, delete submodule libFuzzer in your bloaty repo and add your libFuzzer repo to your bloaty repo as submodule. And do this to your gRPC repo again. Finally, you need change GRPC_URL and GRPC_TAG to your own repo in tensorflow-master\\tensorflow\\contrib\\cmake\\external\\grpc.cmake\r\n\r\nIf you want a quick solution, you can change GRPC_URL and GRPC_TAG in tensorflow-master\\tensorflow\\contrib\\cmake\\external\\grpc.cmake to:\r\nGRPC_URL https://github.com/lzkmylz/grpc\r\nGRPC_TAG 4446434eab7bfa9c7af000fc043ce9ecc267efdd\r\n\r\nAfter this modification, you can compile it with some warning and generate tensorflow.dll and tensorflow.lib.", "@tensorflowbutler @asimshankar \r\nThis issue has been solved and can be closed.", "@lzkmylz GRPC_URL https://github.com/lzkmylz/grpc 404\uff0cnot a valid address?"]}, {"number": 19721, "title": "UnicodeDecodeError while loading trained model through import_meta_graph function.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nWindows 10 Home, but using online Jupyter Notebook environment for coding purposes\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nVersion : 1.8.0\r\n- **Python version**: \r\n\r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n\r\nN/A\r\n- **GPU model and memory**:\r\n\r\nTesla K80, 11.5 GB RAM\r\n- **Exact command to reproduce**:\r\n\r\nnew_saver = tf.train.import_meta_graph('model-21-epochs.meta')\r\n\r\n\r\n### Describe the problem\r\n\r\nI trained a text classification model on an online Jupyter Notebook, and saved models at each epoch to evaluate the best performing one. On the notebook itself, after the training session is over, running the tf.train.import_meta_graph() function seemed to work fine. \r\n\r\n**I then downloaded all 3 files : the .meta file, the .data-00000-of-00001 file and the .index file** and on running it locally on my machine, I get a UnicodeDecodeError like this : \r\n\r\n```\r\n2018-06-03 17:46:04.730649: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nTraceback (most recent call last):\r\n  File \"code.py\", line 14, in <module>\r\n    new_saver = tf.train.import_meta_graph('model-31.meta')\r\n  File \"C:\\Users\\sekha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1947, in import_meta_graph\r\n    meta_graph_def = meta_graph.read_meta_graph_file(meta_graph_or_file)\r\n  File \"C:\\Users\\sekha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 643, in read_meta_graph_file\r\n    text_format.Merge(file_content.decode(\"utf-8\"), meta_graph_def)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 1: invalid continuation byte\r\n```\r\nThe online notebook only stores files on it for a certain amount of time, and after that they are deleted. So I uploaded the meta file on the notebook again, to see if the problem persisted on a new session ( session on the notebook, not used in the tensorflow 'session' lingo ) in the notebook. And it gave me the same error then too. \r\n\r\nAn issue almost identical to this one has been raised fairly recently in #19573 but there hasn't been an update at all. And this was asked on StackOverflow too, to which the response was that it might be a bug  and github was the place for it, as can be seen in #19573. \r\n\r\nFor reference, this is the entire inference portion of the code:\r\n\r\n  ```\r\ngraph = tf.Graph()\r\n  with tf.Session(graph = graph) as sess: \r\n      \r\n        new_saver = tf.train.import_meta_graph('/content/runs/1527946417/checkpoints/model-31.meta') \r\n        new_saver.restore(sess, ('/content/runs/1527946417/checkpoints/model-31'))\r\n        sess.run(tf.tables_initializer())\r\n        e = (graph.get_operations())\r\n   \r\n        arr_placeholder = graph.get_operation_by_name('arr_placeholder/inp_array').outputs[0]\r\n        str_placeholder = graph.get_tensor_by_name('str_placeholder/inp_string:0')\r\n        dropout_keep_prob = graph.get_operation_by_name('dropout_keep_prob/keep_prob').outputs[0]\r\n        \r\n        logis = (graph.get_operation_by_name('logits/scores')).outputs[0]\r\n\r\n        a =  (sess.run(logis, feed_dict = {arr_placeholder : x_dev, dropout_keep_prob : 1.0, str_placeholder : ls[4731:]}))\r\n```\r\n\r\nSo is it the case that saved models and meta graphs only be restored from the directory it was saved to during the training phase? Is it impossible to run inference on a pre-trained model downloaded from an online source?", "comments": ["Turns out that the error was caused by the 'checkpoint' file not being downloaded. Apart from the 3 files of .meta, .index and .data-00000-of-00001 file, the other saved file named 'checkpoint' too needs to be downloaded. Keeping this here in case another person faces the same issue."]}, {"number": 19719, "title": "Mentioned Visual C++ 2015 dependency for Windows JNI library", "body": "Modified MD file to cover this - https://github.com/tensorflow/tensorflow/issues/14456#issuecomment-394026175", "comments": ["@asimshankar I have modified install_java.md as per your suggestions.\r\nmerged upstream/master changes in this PR.\r\nPlease check now.", "@asimshankar made changes to Dll name, Please check now", "Thanks!"]}, {"number": 19718, "title": "the gradient function returned by tfe.implicit_value_and_gradients() doesn't support keyword argument", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0\r\n- **Python version**: \r\nPython 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:\r\nNo\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nWhen using eager mode, I found that the gradient function returned by `tfe.implicit_value_and_gradients()` doesn't support keyword argument.\r\n\r\n\r\n### Source code / logs\r\nMy code like this\r\n```python\r\ngrad_fn = tfe.implicit_gradients(MLP)\r\ngrads_and_vars = grad_fn(data, label, is_training=True)\r\n```\r\n\r\nAnd I got the error\r\n>Traceback (most recent call last):\r\n  File \"xxx.py\", line 91, in <module>\r\n    grads_and_vars = grad_fn(data, label, is_training=True)\r\n  File \"xxx\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\", line 289, in grad_fn\r\n    return implicit_val_and_grad(f)(*args, **kwds)[1]\r\nTypeError: grad_fn() got an unexpected keyword argument 'is_training'\r\n\r\nWhen I remove the keyword \"is_training\", like this\r\n```\r\ngrads_and_vars = grad_fn(data, label, True)\r\n```\r\nThen the code has no problem. \r\n\r\nThe bug can be fixed by modifying \u2019xxx\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u2018 and adding `**kwds` in function `implicit_val_and_grad(f)`\r\nthe modified code like this\r\n```python\r\n  def grad_fn(*args, **kwds):\r\n    \"\"\"Computes the gradient of the wrapped function.\"\"\"\r\n    this_tape = tape.push_new_tape()\r\n    try:\r\n      end_node = f(*args, **kwds)\r\n  ...\r\n```\r\n\r\n\r\n", "comments": ["I agree with your suggested fix. Do you want to send a pull request? (I could do it, but I think it's nicer if you get the credit for your contribution!)", "OK~ You can check it at here: https://github.com/tensorflow/tensorflow/pull/19804\r\n", "Nagging Assignee @alextp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like this was fixed by #19804."]}, {"number": 19716, "title": "TF Lite wants help on Andorid Between ImageClassifier and ObjectDetection", "body": "Dear Sir/Madam:\r\n       (1)I run tf-lite to recognize things ok according to \r\n `https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0`\r\n     (2)But When I want to detect Objects on camera,Only `https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android`\r\n     the first one use  `private Interpreter tflite` to run .lite for result,while the second one use `TensorFlowInferenceInterface` to build graph and feed inputs,this need to using bazel compile the whole project,a lot time need to spend\r\n   So my Question is: Can Examples like the first one for Object Detection using lighweight interpreter?\r\n    Plus tensorflow-lite is valued very import in many areas,Hope for talents helping,Thank you.\r\nPlus \r\nOS Platform and Distribution:Ubuntu16.04LTS\r\nTensorFlow installed from:PIP install\r\nTensorFlow version:tensorflow-gpu:1.6\r\nBazel version:0.11.1\r\nCUDA/cuDNN version\r\nGPU model and memory:nvidia 1080TI 12G\r\nExact command to reproduce:N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Dear Sir:\r\n   I have learned the newest video on Tensorflow Lite by engineer Renming Gu,Shanghai ,China,R&D develop of Google.\r\n   May I contact with him to give me some suggestions on Tensorflow Lite Examples.\r\n  ", "@cableyang, \r\n\r\nI'm here. I notice you are in CN region, we have active forum here.\r\nhttps://www.tensorflowers.cn/\r\n\r\nAs for knowledge discussion, I would prefer you go to forum or mail list. Github issues are more for tracking feature requests, issues etc.\r\n"]}, {"number": 19715, "title": "[CMAKE] Improve cmake build for MKL and MKL-DNN on Windows", "body": "## Improve MKL and MKL-DNN compilation\r\nIn previous PR #16936, one still need to pre-install MKL on the machine, while we found that MKL dependency is released with mkl-dnn on its github [release page](https://github.com/intel/mkl-dnn/releases).\r\n- Add mkl.cmake script to download the latest mkl libraries and header files\r\n- Change mkldnn.cmake according to mkl.cmake (depends on mkl)\r\n- Change tf_python.cmake to add mkl path when executing api generator\r\n- mod .gitignore too ignore `build` folder\r\n\r\nThis behaviour of the cmake script is more close to Bazel after the improvement.\r\n\r\n## Fix compile errors on VS\r\n- Function `int NumHyperthreadsPerCore()` not implemented on windows, (now implemented in `port.cc`)\r\n- `i_malloc, i_free, i_realloc, i_calloc` not declared on windows.", "comments": ["@mrry I fixed typo and explain the `if-else` right before the sentence", "Can you please resolve the conflict in `tf_python.cmake`? After that, this will be ready to merge.", "Thanks, conflict resolved @mrry ", "Hi @mrry, could this PR be merged and closed?"]}, {"number": 19714, "title": "Add complex numbers to the supported data types for UnsortedSegmentProd", "body": "In the kernel implementation both UnsortedSegmentProd and UnsortedSegmentSum supports complex numbers. However, unlike UnsortedSegmentSum, the op of UnsortedSegmentProd does not register complex number types in math_ops.cc.\r\n\r\nThis fix adds the supported complex number types to math_ops.cc, and enables test cases for it.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19713, "title": "Replace direct download link with bazel mirror (mirror.bazel.build)", "body": "Since the download package for gemmlowp has been propagated to the bazel mirror (mirror.bazel.build), this fix replaced the direct link with the mirrored one, and removed the related `TODO`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19712, "title": "Add KinesisDataset support for tensorflow Dataset", "body": "This fix is an attempt to add Kinesis support for tensorflow's Dataset. Kinesis is provided by AWS as a managed data streaming service. It is similar to Apache Kafka, often used in places where maintaining an independent Kafka cluster on AWS is not desirable, or not possible.\r\n\r\nThis fix adds the Kinesis support for tensorflow Dataset. Similiar to the Kafka integration in tensorflow, KinesisDataset outputs tf.string for records.\r\n\r\nTest cases have also been added, which could be invoked manually.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["(good for API)", "Thanks @mrry for the detailed review and feedback. The PR has been updated. Please take a look and let me know if there are any issues.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you please resolve conflicts and rerun the tests? This should be good to merge once we get a clean set of presubmit checks.", "Thanks @mrry. The PR has been rebased and updated.", "@mrry Thanks for the help! \ud83d\udc4d \ud83c\udf89 "]}, {"number": 19711, "title": "Remove duplicate imports", "body": "Inside ffmpeg/__init__.py the last import line:\r\n```\r\nfrom tensorflow.contrib.ffmpeg.ffmpeg_ops import decode_video\r\n```\r\nis a duplicate of the previous import. This fix removes the duplicate.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 19710, "title": "TF dataset: support for aspect ratio group batching", "body": "Current TF dataset class only support fixed size padding for batching different sized input elements, however there are cases like in object detection, we may need to batch images according to their aspect ratios, i.e., push images with similar aspect ratio into one batch and pad them with minimal extra paddings, is there any existing approach that does not use feed_dict approach but directly utilize TF operators and read TF record files for accomplish this ? Thanks!", "comments": ["This should be possible using [`tf.contrib.data.group_by_window()`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/group_by_window) to group the images into batches based on their aspect ratios, and then apply the appropriate padding for the aspect ratio in the window function. (This is the same mechanism that\u2019s used to [bucket sequences by length](https://www.tensorflow.org/api_docs/python/tf/contrib/data/bucket_by_sequence_length), and looking at the [implementation](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/data/python/ops/grouping.py#L95) of that feature might be useful.)", "Thank you @mrry Could you provide a simple example of using this function, I mean how to set \r\n\r\n```\r\n(key_func,\r\n reduce_func,\r\n window_size=None,\r\n window_size_func=None)\r\n```\r\nfor my case ?", "```\r\nbatch_size_training = 4\r\nbucket_boundaries_ = [0.3, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 1.7]\r\nbucket_batch_sizes_ = (len(bucket_boundaries_)+1)*[batch_size_training]\r\n\r\ndef element_length_function(elem):\r\n    height, width = tf.cast(tf.shape(elem['image'])[0], tf.float32), \\\r\n                    tf.cast(tf.shape(elem['image'])[1], tf.float32)\r\n    ratio = width/height\r\n    return ratio\r\ndef bucket_by_sequence_length(element_length_func = element_length_function,\r\n                              bucket_boundaries= bucket_boundaries_,\r\n                              bucket_batch_sizes = bucket_batch_sizes_,\r\n                              padded_shapes=None,\r\n                              padding_values=None,\r\n                              pad_to_bucket_boundary=False):\r\n\r\n    with ops.name_scope(\"bucket_by_seq_length\"):\r\n        if len(bucket_batch_sizes) != (len(bucket_boundaries) + 1):\r\n            raise ValueError(\r\n                \"len(bucket_batch_sizes) must equal len(bucket_boundaries) + 1\")\r\n\r\n        batch_sizes = constant_op.constant(bucket_batch_sizes, dtype=dtypes.int64)\r\n\r\n        def element_to_bucket_id(*args):\r\n            \"\"\"Return int64 id of the length bucket for this element.\"\"\"\r\n            seq_length = element_length_func(*args)\r\n\r\n            boundaries = list(bucket_boundaries)\r\n            buckets_min = [np.iinfo(np.int32).min] + boundaries\r\n            buckets_max = boundaries + [np.iinfo(np.int32).max]\r\n            conditions_c = math_ops.logical_and(\r\n                math_ops.less_equal(buckets_min, seq_length),\r\n                math_ops.less(seq_length, buckets_max))\r\n            bucket_id = math_ops.reduce_min(array_ops.where(conditions_c))\r\n\r\n            return bucket_id\r\n\r\n\r\n\r\n        def window_size_fn(bucket_id):\r\n            # The window size is set to the batch size for this bucket\r\n            window_size = batch_sizes[bucket_id]\r\n            return window_size\r\n\r\n\r\n        def make_padded_shapes(shapes, none_filler=None):\r\n            padded = []\r\n            for shape in nest.flatten(shapes):\r\n                shape = tensor_shape.TensorShape(shape)\r\n                shape = [\r\n                    none_filler if d.value is None else d\r\n                    for d in shape\r\n                ]\r\n                padded.append(shape)\r\n            return nest.pack_sequence_as(shapes, padded)\r\n\r\n\r\n        def batching_fn(bucket_id, grouped_dataset):\r\n            \"\"\"Batch elements in dataset.\"\"\"\r\n            batch_size = batch_sizes[bucket_id]\r\n            none_filler = None\r\n            if pad_to_bucket_boundary:\r\n                err_msg = (\"When pad_to_bucket_boundary=True, elements must have \"\r\n                           \"length <= max(bucket_boundaries).\")\r\n                check = check_ops.assert_less(\r\n                    bucket_id,\r\n                    constant_op.constant(len(bucket_batch_sizes) - 1,\r\n                                         dtype=dtypes.int64),\r\n                    message=err_msg)\r\n                with ops.control_dependencies([check]):\r\n                    boundaries = constant_op.constant(bucket_boundaries,\r\n                                                      dtype=dtypes.int64)\r\n                    bucket_boundary = boundaries[bucket_id]\r\n                    none_filler = bucket_boundary\r\n            shapes = make_padded_shapes(\r\n                padded_shapes or grouped_dataset.output_shapes,\r\n                none_filler=none_filler)\r\n            return grouped_dataset.padded_batch(batch_size, shapes, padding_values)\r\n\r\n\r\n        def _apply_fn(dataset):\r\n            return dataset.apply(\r\n                tf.contrib.data.group_by_window(element_to_bucket_id, batching_fn,\r\n                                window_size_func=window_size_fn))\r\n\r\n        return _apply_fn\r\n```\r\n\r\n@mrry, I tried to modify the  bucket sequences by length, but it only generate one image a time, can you pls point where I'm wrong?", "OK, it works!", "Great! Glad to hear it's working. If you have other questions about how to use the API, I'd recommend posting on Stack Overflow, under the `[tensorflow-datasets]` tag.", "@mrry Thank you, one more thing, since reading the lower level code could be a bit hard, could you briefly explain how group_by_window is implemented, I read pytorch implementation of faster-rcnn with aspect ratio grouping, they scan the whole dataset and group images with similar aspect ratio into batches, every epoch, shuffle is done between batches,  is group_by_window doing the same thing?", "Jumping in with a minor comment.. @MrWanter just saw that you've been posting many dataset-related issues for the past week. This is exactly why I wrote this in tensorpack tutorial:\r\n> Unlike running a mathematical model, data processing is a complicated and poorly-structured task. You need to handle different formats, handle corner cases, noisy data, combination of data. Doing these requires condition operations, loops, data structures, sometimes even exception handling. These operations are naturally not the right task for a symbolic graph.\r\n\r\nIt depends on your exact task but for Faster-RCNN my experience is that a Python loader can run much faster than the speed of training.", "@ppwwyyxx Hi, Yuxin, I see, thanks for sharing."]}, {"number": 19709, "title": "build tensorflow-lite example label_image to .so ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.8.0 gpu\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: c++11  \r\n- **CUDA/cuDNN version**:7.5.18\r\n- **GPU model and memory**:TITAN,12GB\r\n- **Exact command to reproduce**:N/A\r\n\r\n\r\n### Describe the problem\r\nI want to use bazel to build label_image to .so file. I modified the BUILD in /tensorflow/contrib/lite/examples/label_image/BUILD like below:\r\n\r\n\r\n\r\n```\r\npackage(default_visibility = [\"//visibility:public\"])\r\n\r\nlicenses([\"notice\"])  # Apache 2.0\r\n\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\", \"tf_copts\", \"if_android\")\r\nload(\"//tensorflow/contrib/lite:build_def.bzl\", \"tflite_linkopts\")\r\n\r\nexports_files([\r\n    \"version_script.lds\",\r\n])\r\n\r\nfilegroup(\r\n    name = \"label_image_src\",\r\n    srcs = glob([\r\n        \"label_image.cc\",\r\n        \"bitmap_helpers.cc\",\r\n        \"label_image.h\",\r\n        \"bitmap_helpers.h\",\r\n    ]),\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\r\nLINKER_SCRIPT = \"//tensorflow/contrib/lite/examples/label_image:version_script.lds\"\r\n\r\ncc_binary(\r\n    name = \"libtflite_label_image.so\",\r\n    srcs = [],\r\n    copts = tf_copts() + [\r\n        \"-ffunction-sections\",\r\n        \"-fdata-sections\",\r\n    ],\r\n    linkopts = if_android([\r\n        \"-landroid\",\r\n        \"-latomic\",\r\n        \"-ldl\",\r\n        \"-llog\",\r\n        \"-lm\",\r\n        \"-z defs\",\r\n        \"-s\",\r\n        \"-Wl,--gc-sections\",\r\n        \"-Wl,--version-script\",  # This line must be directly followed by LINKER_SCRIPT.\r\n        LINKER_SCRIPT,\r\n    ]),\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    tags = [\r\n        \"manual\",\r\n        \"notap\",\r\n    ],\r\n    deps = [\r\n        \":label_image\",\r\n        \"//tensorflow/contrib/lite:framework\",\r\n        \"//tensorflow/contrib/lite:string_util\",\r\n        \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/core:android_tensorflow_lib\",\r\n        LINKER_SCRIPT,\r\n    ],\r\n)\r\ncc_library(\r\n    name = \"label_image\",\r\n    srcs = if_android([\":label_image_src\"]),\r\n    copts = tf_copts(),\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\r\n        \"//tensorflow/contrib/lite:framework\",\r\n        \"//tensorflow/contrib/lite:string_util\",\r\n        \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/core:android_tensorflow_lib_lite\",\r\n    ],\r\n    alwayslink = 1,\r\n)\r\n```\r\n\r\nI can generated libtflite_label_image.so successfully by using  command\r\n`\"bazel build --config monolithic --cxxopt=-std=c++11   --crosstool_top=//external:android/crosstool   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   --cpu=arm64-v8a   //tensorflow/contrib/lite/examples/label_image:libtflite_label_image.so --verbose_failures\"`\r\nbut when I test the libtflite_label_image.so by using Cmake file, I got the error ` undefined reference to `tflite::label_image::label_image`\r\nI think maybe the .so file is generated uncorrectly, some one can help me?\r\nThank you!\r\n\r\n", "comments": ["Try removing \"-Wl,--version-script\" and LINKER_SCRIPT (those options might be hiding the symbols you care about)"]}, {"number": 19708, "title": "[XLA] Explicitly use ::xla::Layout", "body": "MSVC uses delayed template parsing, so it confuses `Layout` as `::xla::match::Layout` below (which is a function) instead of `::xla::Layout`.\r\n\r\n#15213", "comments": ["The only failing test `//tensorflow/compiler/tests:conv2d_test_gpu` likely has nothing to do with this PR. This PR is for fixing a compile error on MSVC only.", "Yeah, that's most likely a bad GPU."]}, {"number": 19707, "title": "[Intel MKL] Bootstrapping MKL test infrastructure", "body": "@gunan This PR will allow us to kickoff our community build. I copied the run_mkl.sh script rather than moved it from the linux/cpu folder because I didn't want to disturb any existing tests that might expect it to still be there. Let me know what other tests you'd like to see.", "comments": []}, {"number": 19706, "title": "Add eager with estimator", "body": " By adding \r\n```\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\n```\r\nat the beginning of the [python code](https://raw.githubusercontent.com/aymericdamien/TensorFlow-Examples/90bb4de75322f8c01048dd98c7f194442051d257/examples/3_NeuralNetworks/neural_network.py), I got the following RuntimeError: Estimators are not supported when eager execution is enabled.. \r\n\r\n```\r\nFile \"neural_network.py\", line 88, in <module>\r\n    model = tf.estimator.Estimator(model_fn)\r\n  File \"/.../anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 171, in __init__\r\n    'Estimators are not supported when eager execution is enabled.')\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@xiaocai00 : Could you please fill in the details asked for and provide a [minimal, verifiable, complete](https://stackoverflow.com/help/mcve) example to reproduce the problem?\r\n\r\nThat said though, a recent change (https://github.com/tensorflow/tensorflow/commit/0385bfe0726ad9710bfcca145e19611e9e2391bb), which is not included in TensorFlow 1.8 but will be in TensorFlow 1.9, makes it possible to use `Estimator` objects even after eager execution is enabled.\r\n\r\nSo most likely the feature you desire should be in the next version of TensorFlow. (The release branch for which was just cut recently).\r\n\r\nHope that helps!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this since `Estimator` objects can now be used with eager execution enabled.", "This issue is fixed in a sense that eager is simply disabled throughout the estimator code. See for example: [https://github.com/tensorflow/estimator/blob/r2.0/tensorflow_estimator/python/estimator/estimator.py#L340](https://github.com/tensorflow/estimator/blob/r2.0/tensorflow_estimator/python/estimator/estimator.py#L340) for the details. \r\n\r\nAre there any plans to actually enable eager execution in estimator? This way developers could use eager e.g. to debug their model function or custom metrics."]}, {"number": 19705, "title": "Update loss calculation to use one hot labels", "body": "The detailed description of the code for the loss function uses one hot labels as input. Therefore, the target code itself should use one hot labels as well.", "comments": ["Please make PRs against master only."]}, {"number": 19704, "title": "Updating loss calculation to use one_hot labels", "body": "The in-depth description for the code uses one_hot labels as input to the softmax_cross_entropy() function, therefore the code should use one hot labels as well.", "comments": ["@MarkDaoust Sure thing, I'll keep the sparse_softmax_cross_entropy() version of the code, and update the tutorial to match. Changes to come shortly.", "@MarkDaoust Were my changes in the second commit d7c971c1563cb61f0df6dc6ce3c89641ca0a5d8a sufficient for your review, or was there anything further that you'd like me to address?"]}, {"number": 19703, "title": "Updating version for 1.9.0-rc0.", "body": "", "comments": []}, {"number": 19702, "title": "Updating release notes for r1.9.", "body": "", "comments": ["@karmel I recall you had a SavedModel change without relnotes, did that make it in here?", "Yes, looks like it. Thanks, @av8ramit ", "Yeah I handled that one."]}, {"number": 19701, "title": "Branch 198913026", "body": "", "comments": []}, {"number": 19700, "title": "ppc64le: //tensorflow/python:nn_test test fails", "body": "Please assign this issue to me and add the tag: stat:community support\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, Have written a Dockerfile.gpu.ppc64le file and modified ci_parameterized_build.sh to allow for build/test runs on ppc64le. (Will submit as a PR)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master from may 30th\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2.88, 7\r\n- **GPU model and memory**: 2 P100 GPUs with 16 GB of memory each\r\n\r\n- **Exact command to reproduce**:\r\nbazel test --config=cuda -c opt --local_test_jobs=2 --cache_test_results=no --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/python:nn_test\r\n\r\n### Describe the problem\r\n```\r\n======================================================================\r\nFAIL: testNaNs (__main__.ReluTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py\", line 934, in testNaNs\r\n    self.assertTrue(np.isnan(z).all())\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 79 tests in 12.600s\r\n\r\nFAILED (failures=1)\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\nL2Loss gradient err = 9.6958e-12\r\nL2Normalize gradient err = 4.2424e-08\r\nL2Normalize gradient err = 5.45829e-07\r\nL2Normalize gradient err = 7.61142e-05\r\n================================================================================\r\n```\r\n\r\n### Source code / logs\r\n[nn_test.log](https://github.com/tensorflow/tensorflow/files/2063777/nn_test.log)\r\n", "comments": ["/CC @gunan, can you comment? I do not know the state of PowerPC support.", "Triaged as requested by @wdirons ", "@smatzek shared some information on this defect with me. There is an issue in Eigen opened for this http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1494 dealing with minmax nan propagation. Sam provided a patch that fixed this problem: http://eigen.tuxfamily.org/bz_attachmentbase/attachment.cgi?id=831, but the patch that was integrated into eigen added `#ifdef __VSX__`, that doesn't appear defined in Power as it is not following that code path.\r\n\r\nI tried in my own environment patching https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L110 with the commit containing the patch for 1494 and it did not resolve the issue.", "With the latest code in the master branch, this test is now passing on ppc64le. My guess is this commit: https://github.com/tensorflow/tensorflow/commit/4bc01f8f63074337c846a1b60a4a2b88d420bd56 to update Eigen resolved the issue."]}]