[{"number": 27588, "title": "Updated docs formatting", "body": "tf.feature_column.categorical_column_with_vocabulary_file defined in python/feature_column/feature_column_v2.py.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27588) for more info**.\n\n<!-- need_sender_cla -->", "@neighlyd thanks for your contribution, please sign CLA", "@neighlyd gentle ping to sign CLA ", "I signed it this morning!\n\nOn Sat, Apr 6, 2019 at 12:54 PM googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27588>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27588#issuecomment-480532682>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIrDh-UKDvj2VqQ7N35Yyu9mOyN0Z2Izks5vePtXgaJpZM4cgYpl>\n> .\n>\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27588) for more info**.\n\n<!-- ok -->", "I apologize, it looks like I hadn't replied to the googlebot's CLA message! I went ahead and did that."]}, {"number": 27587, "title": "Clickable link edit in documentation.", "body": "Thank you for the Doc-Contributing tutorial (from GDG-Seattle)!", "comments": []}, {"number": 27586, "title": "Additional info from Keras.io", "body": "", "comments": ["@elainecrutchley can you please sign CLA ", "@elainecrutchley gentle ping to sign cla", "can I have a link to the document?\n\nOn Mon, May 6, 2019 at 2:16 AM gbaned <notifications@github.com> wrote:\n\n> @elainecrutchley <https://github.com/elainecrutchley> gentle ping to sign\n> cla\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27586#issuecomment-489555864>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKU6CRFESVH4DDPJYJGM7W3PT7ZNPANCNFSM4HEBQ2HQ>\n> .\n>\n", "@elainecrutchley Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27585, "title": "Added additional information from Keras.io", "body": "", "comments": ["@elainecrutchley please sign cla", "@elainecrutchley gentle ping to sign cla", "i believe it's signed now, thanks\n\nOn Sun, Apr 28, 2019 at 8:38 PM gbaned <notifications@github.com> wrote:\n\n> @elainecrutchley <https://github.com/elainecrutchley> gentle ping to sign\n> cla\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27585#issuecomment-487443515>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKU6CREXPEGUNG3GY742VSTPSZUUJANCNFSM4HEBQL5Q>\n> .\n>\n", "@elainecrutchley can you please check build failures.", "@elainecrutchley Did you get a chance to check build failures? Please let us know on the update. Thanks!", "@elainecrutchley Could you please check build failures and resolve the conflicts? Thanks!", "@elainecrutchley gentle ping to check build failures and resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27584, "title": "No simple function optimisation examples for tensorflow 2.0?", "body": "I don't see any simple function optimisation example for tensorflow 2.0.\r\n\r\nSomething like log(x) ** 2 could be good.", "comments": ["@cottrell Currently there are several [tutorials](https://www.tensorflow.org/alpha) covering major concepts of TF broadly. In future, more tutorials will be added. Please stay tuned. I am closing the issue. Thanks!"]}, {"number": 27583, "title": "Doc keras/Model for \"predict\" function the list of argument is not display properly (fullblock instead of list)", "body": "\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model\r\n\r\n**Describe the documentation issue**\r\n\r\nIn the documentation keras/Model for \"predict\" function the list of argument is not display properly (fullblock instead of list) on both Chrome and Firefox\r\n\r\n<img width=\"910\" alt=\"Screenshot 2019-04-06 at 20 59 00\" src=\"https://user-images.githubusercontent.com/12021701/55674016-0cdf6980-58af-11e9-995f-fc569131a741.png\">\r\n\r\n", "comments": ["I think the issue is that the first line `x: Input samples.` should be prefixed with 4 spaces (not 5). Created a PR #27596 for the fix.", "@dynamicwebpaige  @margaretmz  Since you are experts, any idea why this PR didn't pass test ? It was just to fix some display issue of the Keras doc. I didn't do the PR but I see it didn't pass the test and it is not clear why. Thanks", "I don't know if it normally take 2 days to deploy the new version of the doc, but I still see the same issue in the doc:\r\n![image](https://user-images.githubusercontent.com/12021701/57180374-4c867a80-6e88-11e9-97ee-1da60b9932bf.png)\r\nThanks", "The issue was not fixed with the PR, please reopen the ticket. Thanks", "@tarrade The page is pointing to 2.0.0alpha so it still use the docstring in 2.0.0 python. The PR #27596 fixes the issue in the master branch, it will show up in the next release of 2.0.0 (alpha or beta), then the web page will be re-rendered to capture the change.\r\n\r\nI will re-open the issue for now, though I think once the next release of 2.0.0 (alpha or beta) is out the page will be updated and corrected automatically.\r\n", "@yongtang ah ok got it. Since this was independend of a Tensorflow version (the format), I was think it will be probagated to the main branch and I could see the update on the main page. I am fine to have the ticket closed since the issue is really fixed.", "I am closing the issue as it was resolved. I have checked that the PR was merged already. It will reflect on the website soon. Thanks!"]}, {"number": 27582, "title": "Error during testing of Object Detection API", "body": "**System information**\r\n- OS Platform and Distribution :Windows 10\r\n- TensorFlow installed from (source or binary):cmd\r\n- TensorFlow version:1.13.1\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- CUDA/cuDNN version: don't know\r\n- GPU model and memory: 1.13.1\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\npython object_detection/builders/model_builder_test.py\r\n\r\n\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Abhitm\\mawa\\abc\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n", "comments": ["@Abhit05 i once faced this issue and mine was due to the wrong version of CUDNN. For a better help can you tell me the version of CUDA and CUDNN you're using??", "In order to expedite the trouble-shooting process, please provide a code snippet/steps to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27581, "title": "keras model.predict_on_batch/model.test_on_batch with tf.dataset: dataset.make_initializable_iterator is not supported when eager execution is enabled.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): yes \r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nbest_model.test_on_batch(testing_dataset)\r\nand \r\nbest_model.predict_on_batch(training_dataset)\r\n\r\ncrashing with the following error messages\r\n\"RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.\"\r\n\r\nbut best_model.evaluate(testing_dataset,) is wokring.\r\n\r\n**Describe the expected behavior**\r\nExact same code working with Tensorflow 1.12\r\n\r\n**Code to reproduce the issue**\r\nAll the code and llogs are here:\r\nhttps://github.com/tarrade/proj_DL_models_and_pipelines_with_GCP/blob/master/notebook/TF_2.0/05-Mnist_model_validation_and_interpretation.ipynb\r\n\r\n**Other info / logs**\r\nHere the full logs:\r\n\r\n---------------------------------------------------------------------------\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)\r\n   1852     # some datasets (e.g. for prefetching) override its behavior.\r\n-> 1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access\r\n   1854   except AttributeError:\r\n\r\nAttributeError: 'PrefetchDataset' object has no attribute '_make_initializable_iterator'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-21-3d53d52be0f8> in <module>\r\n      1 #!! Bug TF 2.0\r\n----> 2 score = best_model.test_on_batch(testing_dataset)\r\n      3 \r\n      4 # print test accuracy\r\n      5 print('Loss:')\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in test_on_batch(self, x, y, sample_weight, reset_metrics)\r\n   1310     # Validate and standardize user data.\r\n   1311     x, y, sample_weights = self._standardize_user_data(\r\n-> 1312         x, y, sample_weight=sample_weight, extract_tensors_from_dataset=True)\r\n   1313 \r\n   1314     if self.run_eagerly:\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2439       if extract_tensors_from_dataset:\r\n   2440         # We do this for `train_on_batch`/etc.\r\n-> 2441         x, y, sample_weight = training_utils.extract_tensors_from_dataset(x)\r\n   2442     elif isinstance(x, iterator_ops.Iterator):\r\n   2443       # Graph mode iterator. We extract the symbolic tensors.\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in extract_tensors_from_dataset(dataset)\r\n   1382     Tuple of tensors `x, y, weights`. `y` and `weights` entry may be None.\r\n   1383   \"\"\"\r\n-> 1384   iterator = get_iterator(dataset)\r\n   1385   inputs, targets, sample_weight = unpack_iterator_input(iterator)\r\n   1386   return inputs, targets, sample_weight\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in get_iterator(dataset)\r\n   1362 def get_iterator(dataset):\r\n   1363   \"\"\"Create and initialize an iterator from a dataset.\"\"\"\r\n-> 1364   iterator = dataset_ops.make_initializable_iterator(dataset)\r\n   1365   initialize_iterator(iterator)\r\n   1366   return iterator\r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in make_initializable_iterator(dataset, shared_name)\r\n   1853     return dataset._make_initializable_iterator(shared_name)  # pylint: disable=protected-access\r\n   1854   except AttributeError:\r\n-> 1855     return DatasetV1Adapter(dataset)._make_initializable_iterator(shared_name)  # pylint: disable=protected-access\r\n   1856 \r\n   1857 \r\n\r\n~/anaconda3/envs/env_gcp_dl_2_0_alpha/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in _make_initializable_iterator(self, shared_name)\r\n   1495     if context.executing_eagerly():\r\n   1496       raise RuntimeError(\r\n-> 1497           \"dataset.make_initializable_iterator is not supported when eager \"\r\n   1498           \"execution is enabled.\")\r\n   1499     _ensure_same_dataset_graph(self)\r\n\r\nRuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.", "comments": ["Did you upgrade your [TF 1.X code to TensorFlow 2.0](https://www.tensorflow.org/alpha/guide/upgrade) before executing it in TF 2.0?", "@ymodak yes, I migrated my python code using the script tf_upgrade_v2 and I use the tool tf2up.ml to help migrating the notebook. I will update the logs with the error because it was corrupted after I deleted some flags (the error messgae stay the same)\r\n\r\nI am not saying that I didn't do something wrong on my side but do you see why the following will work:\r\n\r\nbest_model.evaluate(testing_dataset,) \r\n\r\nand not\r\n\r\nbest_model.predict_on_batch(training_dataset)\r\n\r\nboth use the same best_model keras and the tf.dataset (migrated to TF 2.0).\r\n<img width=\"1080\" alt=\"Screenshot 2019-04-09 at 21 15 33\" src=\"https://user-images.githubusercontent.com/12021701/55828465-cb4bfa00-5b0c-11e9-92a7-7358216a8322.png\">\r\n\r\n", "@ymodak here some modifided code from Tensorflow 2.0 where you can easily reproduce the issue:\r\n\r\npip install tensorflow==2.0.0-alpha0\r\npip install tensorflow-datasets\r\n\r\nthe usage I am doing from \"predict_on_btach\" if conform to what is written in the doc:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom absl import logging\r\n\r\nlogging.set_verbosity(logging.INFO)\r\n# Define the estimator's input_fn\r\nSTEPS_PER_EPOCH = 5\r\n#BUFFER_SIZE = 10 # Use a much larger value for real code. \r\nBATCH_SIZE = 64\r\nNUM_EPOCHS = 5\r\n\r\n\r\ndef input_fn():\r\n    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    mnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n    BUFFER_SIZE = 10000\r\n    BATCH_SIZE = 64\r\n\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n    \r\n        return image, label[..., tf.newaxis]\r\n\r\n    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n    return train_data.repeat()\r\n\r\n\r\ndef make_model():\r\n    return tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu',\r\n                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dropout(0.1),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\nmodel = make_model()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nprint(model.summary())\r\n\r\ntraining_dataset=input_fn()\r\n\r\nprint(\"train\")\r\nmodel.fit(training_dataset,\r\n          steps_per_epoch=5,\r\n          epochs=10,\r\n          verbose = 1)\r\n\r\nprint(\"evaluate\")\r\nmodel.evaluate(training_dataset,\r\n              steps=1)\r\n\r\nprint(\"predict on batch\")\r\nmodel.predict_on_batch(training_dataset)\r\n```", "@tarrade Apologies for the delay in response. Thanks for the minimal code snippet. I was able to reproduce your behavior. It executes successfully in TF 1.13.1 and fails at ```model.predict_on_batch(training_dataset)``` with TF 2.0 alpha.", "@ymodak I tested today with \r\n\r\n`tf-nightly-2.0-preview==2.0.0.dev20190516`\r\n\r\nand the issues are fixed for both :\r\n`model.predict_on_batch(training_dataset)`\r\nand\r\n`model.test_on_batch(training_dataset)`\r\n\r\nclosing this ticket", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27581\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27581\">No</a>\n"]}, {"number": 27580, "title": "Updated doc with code formatting for SparseConditionalAccumulator", "body": "Small documentation improvement - Adding code formatting for ```SparseConditionalAccumulator ``` related documentation.", "comments": []}, {"number": 27579, "title": "Updated citation formatting for endpoints", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27579) for more info**.\n\n<!-- need_sender_cla -->", "I signed the CLA!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27579) for more info**.\n\n<!-- ok -->", "@yifeif can you please help merge this PR", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 27578, "title": "Updated with code formatting for endpoints.", "body": "", "comments": []}, {"number": 27577, "title": "TensorFlow 2.0 Alpha Tensorboard graph bug", "body": "**System information**\r\n- Have I written custom code: No, I'm using MNIST Tensorboard example provided by Google.\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- Binary installation (sudo pip3 install tf-nightly-2.0-preview)\r\n- TensorFlow version (use command below): 2.0.0-dev20190405\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA GTX 1080 Ti 11GB\r\n\r\n**Describe the current behavior**\r\nMy code is provided below.\r\n\r\nRunning this code gives a directory called logs. I launch Tensorboard using **tensorboard --logdir=./logs**\r\nI go to localhost:6006, click the Graphs tab in my browser, and I get the message saying:\r\n**Graph: Failed Normalizing names**\r\nTensorboard console doesn't show any errors.\r\n\r\nI'm using the MNIST Tensorboard example provided here: https://www.tensorflow.org/tensorboard/r2/graphs\r\nAnd it doesn't work in TensorFlow 2.0. Why?\r\n\r\n**Describe the expected behavior**\r\nI expect it to show the graph for the model.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom datetime import datetime\r\n#from packaging import version\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nprint(\"TensorFlow version: \", tf.__version__)\r\n\r\n# Define the model.\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(32, activation='relu'),\r\n    keras.layers.Dropout(0.2),\r\n    keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='sparse_categorical_crossentropy',\r\n    metrics=['accuracy'])\r\n(train_images, train_labels), _ = keras.datasets.fashion_mnist.load_data()\r\ntrain_images = train_images / 255.0\r\n\r\nlogdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\r\n\r\n# Train the model.\r\nmodel.fit(\r\n    train_images,\r\n    train_labels, \r\n    batch_size=64,\r\n    epochs=5, \r\n    callbacks=[tensorboard_callback])\r\n```\r\n\r\nAttached file is zipped logs directory to be viewed with Tensorboard.\r\n[logs.zip](https://github.com/tensorflow/tensorflow/files/3050778/logs.zip)\r\n", "comments": ["@plentypvp hey! i used the same code and ran tensorboard i didn't faced any issue and got the graph as expected!!\r\nDid you tried running it again and starting tensorboard again??", "Yes, I have tried doing it multiple times. I'm using TensorBoard 1.14.0a20190319. If you got the graph as expected, can you share your OS distro, Python version, TensorFlow version, TensorBoard version?\r\n\r\n**Update**: I reinstalled Python and Python3, removed /usr/local/lib/python2.7/site-packages/*, /usr/local/lib/python2.7/dist-packages/*, /usr/local/lib/python3.5/site-packages/*. Then I did a fresh **sudo pip3 install tensorflow-gpu==2.0.0-alpha0**\r\n\r\nAnd the problem got solved. The graph does show up now. But the conceptual graph does not. Selecting keras from tags gives me an error. See the screenshots attached.\r\n![screen_tensor](https://user-images.githubusercontent.com/49348066/55675987-c7d62a00-58e4-11e9-833f-4b76a272774a.png)\r\n![screen_tensor2](https://user-images.githubusercontent.com/49348066/55675988-c86ec080-58e4-11e9-9b1e-b2374b902c0f.png)\r\n\r\nMeanwhile TensorBoard console prints:\r\n\r\n```\r\nTensorBoard 1.14.0a20190301 at http://AI:6006 (Press CTRL+C to quit)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/werkzeug/serving.py\", line 302, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/usr/local/lib/python3.5/dist-packages/werkzeug/serving.py\", line 290, in execute\r\n    application_iter = app(environ, start_response)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorboard/backend/application.py\", line 310, in __call__\r\n    return self.data_applications[clean_path](environ, start_response)\r\n  File \"/usr/local/lib/python3.5/dist-packages/werkzeug/wrappers/base_request.py\", line 235, in application\r\n    resp = f(*args[:-2] + (request,))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorboard/plugins/graph/graphs_plugin.py\", line 219, in graph_route\r\n    result = self.graph_impl(run, tag, is_conceptual, limit_attr_size, large_attrs_key)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorboard/plugins/graph/graphs_plugin.py\", line 152, in graph_impl\r\n    keras_model_config = json.loads(tensor_events[0].tensor_proto.string_val[0])\r\n  File \"/usr/lib/python3.5/json/__init__.py\", line 312, in loads\r\n    s.__class__.__name__))\r\nTypeError: the JSON object must be str, not 'bytes'\r\n```\r\n\r\nChanging the line\r\n`keras_model_config = json.loads(tensor_events[0].tensor_proto.string_val[0])`\r\nto\r\n`keras_model_config = json.loads(tensor_events[0].tensor_proto.string_val[0].decode('ascii'))`\r\nresulted in keras (conceptual) graph showing.\r\n\r\nBut selecting \"batch_2\" from tags results in another problem:\r\nTensorboard hangs showing **Data: Parsing graph.pbtxt**", "@plentypvp I ran your given code and for me the keras tag worked without any changing like you mentioned but yes the tag batch_2 stuck at `Parsing graph.pbtxt`\r\n\r\nOS: Ubuntu 116.04\r\nTensorflow Version: Tensorflow 2.0 Alpha\r\nPython 3.6", "\"for me the keras tag worked without any changing like you mentioned\". It worked without any changing for you probably because you have Python 3.6.\r\n\r\n\"but yes the tag batch_2 stuck at Parsing graph.pbtxt\". And that's the bug that should be forwarded to Tensorflow team.\r\n\r\nIn conclusion, I get the following:\r\nUbuntu 16.04\r\nPython 3.5.2\r\n\r\n1. Tensorflow v2.0.0-dev20190405, TensorBoard 1.14.0a20190319:\r\nThe graph doesn't show up at all: Graph: Failed Normalizing names\r\n\r\n2. Tensorflow v2.0.0-alpha0, TensorBoard 1.14.0a20190301:\r\nThe graph shows up. Keras (conceptual) graph shows up after a change mentioned above, but this is probably becuase of Python version. Batch_2 graph doesn't show up, getting stuck at \"Parsing graph.pbtxt\".\r\n\r\nLet's wait until TensorFlow Team responds.", "@wchargin  @nfelt  what would you say for this??", "This may be a duplicate of tensorflow/tensorboard#2062, which should be\r\nfixed in the latest version of `tb-nightly`. Could you please try\r\nuninstalling TensorBoard and then running `pip install -U tb-nightly`\r\ninside your virtualenv to see whether the issue persists?\r\n", "I don't use virtualenv. Ubuntu 16.04 (fresh installation), Python 3.5.2.\r\nsudo pip3 install tensorflow-gpu==2.0.0-alpha0\r\nsudo pip3 install -U tb-nightly\r\n\r\nThese commands resulted in having Tensorflow 2.0.0-alpha0 and TensorBoard 1.14.0a20190412 installed. After installation, I ran the same code as in the beginning. As expected, it created logs directory. In Tensorboard \"Default\" graph showed up as expected, \"keras\" and \"batch_2\" gave the same error (see the screenshots below).\r\n![image](https://user-images.githubusercontent.com/49348066/56071046-21d36580-5da5-11e9-949a-229727e9a77d.png)\r\n![image](https://user-images.githubusercontent.com/49348066/56071058-3f083400-5da5-11e9-8e2b-914722fd171b.png)\r\n\r\nTensorboard console log:\r\n```\r\nTensorBoard 1.14.0a20190412 at http://AI:6006 (Press CTRL+C to quit)\r\nI0413 04:27:05.439245 140161711109888 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:05] \"GET / HTTP/1.1\" 200 -\r\nI0413 04:27:05.726644 140161702717184 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:05] \"GET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1\" 200 -\r\nI0413 04:27:06.709269 140161702717184 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/environment HTTP/1.1\" 200 -\r\nI0413 04:27:06.709728 140161585235712 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/runs HTTP/1.1\" 200 -\r\nI0413 04:27:06.711792 140161576843008 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/experiments HTTP/1.1\" 200 -\r\nI0413 04:27:06.712903 140161711109888 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/plugins_listing HTTP/1.1\" 200 -\r\nI0413 04:27:06.919516 140161711109888 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /font-roboto/RxZJdnzeo3R5zSexge8UUZBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\" 200 -\r\nI0413 04:27:06.920198 140161576843008 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/plugin/scalars/tags HTTP/1.1\" 200 -\r\nI0413 04:27:06.920916 140161702717184 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/environment HTTP/1.1\" 200 -\r\nI0413 04:27:06.924833 140161585235712 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/plugins_listing HTTP/1.1\" 200 -\r\nI0413 04:27:06.925154 140161358755584 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/runs HTTP/1.1\" 200 -\r\nI0413 04:27:06.925925 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /data/experiments HTTP/1.1\" 200 -\r\nI0413 04:27:06.951884 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:06] \"GET /font-roboto/d-6IYplOFocCacKzxwXSOJBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\" 200 -\r\nI0413 04:27:07.041337 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:07] \"GET /data/plugin/scalars/scalars?tag=epoch_accuracy&run=logs%2Ffit%2F20190413-042625%2Ftrain&experiment= HTTP/1.1\" 200 -\r\nI0413 04:27:07.041760 140161358755584 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:07] \"GET /data/plugin/scalars/scalars?tag=epoch_loss&run=logs%2Ffit%2F20190413-042625%2Ftrain&experiment= HTTP/1.1\" 200 -\r\nI0413 04:27:07.046586 140161358755584 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:07] \"GET /data/plugin/scalars/tags HTTP/1.1\" 200 -\r\nI0413 04:27:07.224785 140161358755584 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:07] \"GET /data/plugin/scalars/scalars?tag=epoch_loss&run=logs%2Ffit%2F20190413-042625%2Ftrain&experiment= HTTP/1.1\" 200 -\r\nI0413 04:27:07.225165 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:07] \"GET /data/plugin/scalars/scalars?tag=epoch_accuracy&run=logs%2Ffit%2F20190413-042625%2Ftrain&experiment= HTTP/1.1\" 200 -\r\nI0413 04:27:08.560219 140161358755584 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:08] \"GET /data/plugin/graphs/info HTTP/1.1\" 200 -\r\nI0413 04:27:08.561780 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:08] \"GET /data/plugins_listing HTTP/1.1\" 200 -\r\nI0413 04:27:08.705659 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:08] \"GET /data/plugin/graphs/graph?run=logs%2Ffit%2F20190413-042625%2Ftrain&conceptual=false HTTP/1.1\" 200 -\r\nI0413 04:27:09.080152 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:09] \"GET /data/plugins_listing HTTP/1.1\" 200 -\r\nI0413 04:27:12.141783 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:27:12] \"GET /data/plugin/graphs/graph?run=logs%2Ffit%2F20190413-042625%2Ftrain&conceptual=false&tag=batch_2 HTTP/1.1\" 200 -\r\nI0413 04:30:51.835412 140161333577472 _internal.py:122] ::ffff:127.0.0.1 - - [13/Apr/2019 04:30:51] \"GET /data/plugin/graphs/graph?run=logs%2Ffit%2F20190413-042625%2Ftrain&conceptual=true&tag=keras HTTP/1.1\" 500 -\r\nE0413 04:30:51.837328 140161333577472 _internal.py:122] Error on request:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/werkzeug/serving.py\", line 302, in run_wsgi\r\n    execute(self.server.app)\r\n  File \"/usr/local/lib/python3.5/dist-packages/werkzeug/serving.py\", line 290, in execute\r\n    application_iter = app(environ, start_response)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorboard/backend/application.py\", line 309, in __call__\r\n    return self.data_applications[clean_path](environ, start_response)\r\n  File \"/usr/local/lib/python3.5/dist-packages/werkzeug/wrappers/base_request.py\", line 235, in application\r\n    resp = f(*args[:-2] + (request,))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorboard/plugins/graph/graphs_plugin.py\", line 218, in graph_route\r\n    result = self.graph_impl(run, tag, is_conceptual, limit_attr_size, large_attrs_key)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorboard/plugins/graph/graphs_plugin.py\", line 151, in graph_impl\r\n    keras_model_config = json.loads(tensor_events[0].tensor_proto.string_val[0])\r\n  File \"/usr/lib/python3.5/json/__init__.py\", line 312, in loads\r\n    s.__class__.__name__))\r\nTypeError: the JSON object must be str, not 'bytes'\r\n\r\n```\r\nI also attached generated logs directory as zip.\r\n[logs.zip](https://github.com/tensorflow/tensorflow/files/3075465/logs.zip)", "@nfelt  @wchargin Can this be migrated to tensorboard repo?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "No", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27577\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27577\">No</a>\n"]}, {"number": 27576, "title": "nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809 17763.316\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2015 Build tools update 3\r\n- CUDA/cuDNN version: CUDA 10.0 / cuDNN 7.5\r\n- GPU model and memory: GTX980 4GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilding from tag 2.0.0-alpha0 results in `nvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)` being thrown on 2 files, repeatably:\r\n - tensorflow/core/kernels/bincount_op_gpu.cu.cc\r\n - tensorflow/core/kernels/multinomial_op_gpu.cu.cc\r\n \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nC:\\projects\\tensorflow [(v2.0.0-alpha0)]> ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.23.0 installed.\r\nPlease specify the location of python. [Default is C:\\Python37\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0\r\n\r\n\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.5\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.2\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nC:\\projects\\tensorflow [(v2.0.0-alpha0)]> bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: C:/projects/tensorflow/tensorflow/core/kernels/BUILD:3963:1: C++ compilation of rule '//tensorflow/core/kernels:bincount_op_gpu' failed (Exit 5): python.exe failed: error executing command\r\n  cd C:/users/damlo/_bazel_damlo/fttvoi5m/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Python37/lib/site-packages\r\n    SET TEMP=C:\\Users\\damlo\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.2\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\damlo\\AppData\\Local\\Temp\r\n  C:/Python37/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/cub_archive /Ibazel-out/x64_windows-opt/genfiles/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive /Ibazel-out/x64_windows-opt/bin/external/cub_archive/_virtual_includes/cub /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN /arch:AVX2 -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/bincount_op_gpu/bincount_op_gpu.cu.o /c tensorflow/core/kernels/bincount_op_gpu.cu.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/com_google_absl\\absl/strings/string_view.h(496): warning: expression has no effect\r\n\r\nc:\\users\\damlo\\_bazel_damlo\\fttvoi5m\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/arch/GPU/PacketMathHalf.h(149): warning: missing return statement at end of non-void function \"Eigen::internal::ptrue(const Packet &) [with Packet=half2]\"\r\n\r\nc:\\users\\damlo\\_bazel_damlo\\fttvoi5m\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/products/Parallelizer.h(24): warning: variable \"m_maxThreads\" was set but never used\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1025): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(496): warning: expression has no effect\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1025): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\nnvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 359.982s, Critical Path: 193.33s\r\nINFO: 253 processes: 253 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Sorry @ymodak, I am not the correct person to be assigned for this issue.", "Same problem here...\r\nHere is the previous error I had while unning blazer to generate TensorFlow\r\n.\\tensorflow/core/kernels/slice_op.h(44): note: voir la r\u00e9f\u00e9rence \u00e0 l'instanciation de la fonction mod\u00e8le 'Eigen::TensorDevice<Derived,Device> &Eigen::TensorDevice<Derived,Device>::operator =<Eigen::TensorSlicingOp<const Eigen::DSizes<int,3>,const Eigen::DSizes<int,3>,const Eigen::TensorMap<Eigen::Tensor<T,3,1,int>,16,Eigen::MakePointer>>>(const OtherDerived &)' en cours de compilation\r\n        with\r\n        [\r\n            Derived=Eigen::TensorMap<Eigen::Tensor<tensorflow::int64,3,1,int>,16,Eigen::MakePointer>,\r\n            Device=tensorflow::CpuDevice,\r\n            T=const __int64,\r\n            OtherDerived=Eigen::TensorSlicingOp<const Eigen::DSizes<int,3>,const Eigen::DSizes<int,3>,const Eigen::TensorMap<Eigen::Tensor<const __int64,3,1,int>,16,Eigen::MakePointer>>\r\n        ]\r\n.\\tensorflow/core/kernels/slice_op.h(32): note: lors de la compilation de la fonction membre '<Inconnu>' de la classe <Inconnu>\r\n.\\tensorflow/core/kernels/slice_op_cpu_impl.h(32): note: voir la r\u00e9f\u00e9rence \u00e0 l'instanciation de la classe mod\u00e8le 'tensorflow::functor::Slice<tensorflow::CpuDevice,tensorflow::int64,3>' en cours de compilation\r\nERROR: E:/sg/tensorflow/tensorflow/core/kernels/BUILD:1167:1: C++ compilation of rule '//tensorflow/core/kernels:snapshot_op_gpu' failed (Exit 5): python.exe failed: error executing command\r\n\r\nThen it continue to run...\r\nuntil it fail with the exact same error as this thread error title.\r\n\r\nHope that help...\r\n", "This is actually an nvcc bug. we are also running into this and reported to nvidia.\r\nNvidia has fixed this issue in cuda 10.1, but reported that they will not patch 10.0 with the fix.", "This is actually an nvcc bug. we are also running into this and reported to nvidia.\r\nNvidia has fixed this issue in cuda 10.1, but reported that they will not patch 10.0 with the fix.", "@gunan Can you guys push more on that? I only ask because it seems EVERYTHING supports 10.0 and nothing 10.1", "@tfboyd may be able to loop in relevant people from nvidia on the thread.", "I cannot assign to Nathan because he is not in the system yet but I assigned to Ben for now.\r\n\r\n@nluehr ", "@rlewkowicz, what packages do you have in mind that lack cuda 10.1 support?", "@nluehr\r\n\r\nI managed to get 10.1 compiled, but there's a bunch of bugs for tensorflow and the common response is just \"we don't support 10.1 at this time\". \r\n\r\nI had to do it because of the access violation error. But who knows? Maybe I didn't actually have to. Your head starts to spin trying to get this stuff working (took time to realize the jit and mkl stuff is a no go) I spent 40+ hours and landed on vs2017, cuda 10.1, tensorflow 2 beta and that didn't even resolve the windows performance issues I was having.", "You can \"patch\" nvcc yourself and still use cuda 10.0\r\n\r\nDownload and install CUDA 10.1 update 1, but check only to install nvcc.\r\nRename C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudafe++.exe cudafe++.exe.v10.0\r\nCopy C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\cudafe++.exe to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudafe++.exe\r\n\r\nThis fixed this error.\r\nI had a few other errors I had to fix as well before building successfully (building tensorflow branch r2.0 with GPU support, Win 10 19.03, Anaconda 3.7 in a conda virtual environment, Visual Studio 2017, CUDA 10, CUDNN 7.6, bazel 0.26.0, GTX-1080 Ti, RTX-2080 Ti, Compute 6.1,7.5):\r\n\r\n1) Modify the WORKSPACE file to avoid an error on building from bazel:\r\n\r\nedit WORKSPACE\r\n\r\nUnder the line near the top that reads \"load(\"@bazel_tools....\" ADD:\r\n\r\nhttp_archive(\r\n    name = \"io_bazel_rules_docker\",\r\n    sha256 = \"aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d\",\r\n    strip_prefix = \"rules_docker-0.7.0\",\r\n    urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz\"],\r\n)\r\n\r\nCheck this is still the correct lines to use at https://github.com/bazelbuild/rules_docker\r\n\r\n2) Avoid the cudart64_.dll not found error\r\nCreate a copy of:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudart64_100.dll as cudart64_.dll in the same folder\r\ni.e. I kept both the original file and the renamed file in the same folder.\r\n\r\n3) Avoid the cudnn64_.dll not found error\r\nCreate a copy of:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_100.dll as cudnn64_.dll in the same folder\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package\r\n", "I'm running into this problem on r1.14 + CUDA 10.0, with tensorflow/core/kernels/tile_functor_gpu_int64.cu.cc\r\n\r\nWondering how TensorFlow didn't run into that for their Windows builds...\r\n\r\nWill try the 10.1 nvcc workaround.", "@dbonner Thanks for the sharing these details!  I managed to build TF 1.14 for Windows using the CUDA 10.0 + cudafe++ from CUDA 10.1 trick!\r\n\r\nThe cudart_.dll, cudnn_.dll problems seems to be a problem in this build_info genrule (tensorflow.bzl):\r\n\r\n```\r\ndef tf_py_build_info_genrule():\r\n    native.genrule(\r\n        name = \"py_build_info_gen\",\r\n        outs = [\"platform/build_info.py\"],\r\n        cmd =\r\n            \"$(location //tensorflow/tools/build_info:gen_build_info) --raw_generate \\\"$@\\\" --build_config \" +\r\n            if_cuda(\"cuda\", \"cpu\") +\r\n            \" --key_value \" +\r\n            if_cuda(\" cuda_version_number=$${TF_CUDA_VERSION:-} cudnn_version_number=$${TF_CUDNN_VERSION:-} \", \"\") +\r\n            if_windows(\" msvcp_dll_name=msvcp140.dll \", \"\") +\r\n            if_windows_cuda(\" \".join([\r\n                \"nvcuda_dll_name=nvcuda.dll\",\r\n                \"cudart_dll_name=cudart64_$$(echo $${TF_CUDA_VERSION:-} | sed \\\"s/\\\\.//\\\").dll\",\r\n                \"cudnn_dll_name=cudnn64_$${TF_CUDNN_VERSION:-}.dll\",\r\n            ]), \"\"),\r\n        local = 1,\r\n        tools = [clean_dep(\"//tensorflow/tools/build_info:gen_build_info\")],\r\n    )\r\n```\r\n\r\nAFACT, the shell expansion works only when starting bazel from a MinGW Bash shell, not from the command prompt.  Or at least in my case, I was unable to build using cmd.exe but once I got the build configured with \"Git Bash\", the cudnn_.dll/cudart_.dll problem went away.", "> Wondering how TensorFlow didn't run into that for their Windows builds...\r\n\r\nI am also wondering. I am trying to build TF 1.14 and fail because of this very error. How did the maintainers ever succeed building TF 1.14 on Windows? Can we safely say that those instructions actually do not work on Windows (at least for TF 1.14) and then if the maintainers are not following them then why not amending these instructions into something that reflects what they are actually doing to make it work ?\r\n\r\n", "https://github.com/tensorflow/tensorflow/issues/27706\r\n\r\nSummary:\r\nThere is a bug in the cudafe++ binary that comes with CUDA 10.0.\r\nWe had to install both cuda 10 and cuda 10.1 and use the cudafe++ binary from 10.1 on windows.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27576\">No</a>\n", "can cuda11 cudafe++ used for this combination (Cuda 10 + tensorflow 1.14 + bazel 0.24.1) ?\r\nI notice the file version of cudafe++ of cuda11 is with same version of cuda 10\r\n\r\nyes, cuda11 this file can also work."]}, {"number": 27575, "title": "State build requires bazel <=0.23.0 for v2.0.0-alpha0", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: v2.0.0-alpha0\r\n- Doc Link: https://www.tensorflow.org/install/source_windows#install_bazel\r\n\r\n\r\n**Describe the documentation issue**\r\nI downloaded the latest (at the time of writing, 0.24.1) Bazel binary and when I tried to `./configure.py` I got this: \r\n```\r\nYou have bazel 0.24.1 installed.\r\nPlease downgrade your bazel installation to version 0.23.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n```\r\nI think it would be useful to state explicitly that 0.23.0 is the version required to build Tensorflow to save some time. I even just downloaded 0.23.2 because I didn't see that the message says 0.23.0 and sure enough, I got the same complaint about 0.23.2.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nYes. Will do a PR shortly.\r\n", "comments": ["PR https://github.com/tensorflow/docs/pull/465", "PR has been merged. Closing this issue. Thanks!"]}, {"number": 27574, "title": "[XLA on GPU] reduce noise for autotune in multi-stream environment", "body": "Before this change, activities on sub-streams are ignored when `WillAutotuneKernel` is true, which is not consistent with its design.", "comments": ["@sanjoy , if we work on a compilation pass for GEMM autotuning, this patch shouldn't be needed, right?", "I'll defer to @cheshire on this.", "@wyzero Sorry for the delay. Could you expand on what your end goal is? Is it affected by https://github.com/tensorflow/tensorflow/commit/1e1dd2e445995a173aa14aae6a6c36de1a5aaa85 ? Additionally, use of multiple streams is not enabled by default, are you enabling them in your application?", "@cheshire Thanks for your reply. I think `WillAutotuneKernel` in `GpuExecutable` is applied not only to GEMM thunk but also  to other custom thunks that may need autotune as well (e.g. codegen multiple versions, and select by autotuning). As for multi-stream setting, I'm sorry for missing that it has been changed from 'open by default' to 'close by default'  recently. I think it would be better to be consistent with the single stream case if we still allow users to open multi-stream when necessary.", "@wyzero Actually it's only overriden for GEMMs. Your pull request makes sense, but we're about to convert this autotuning during runtime to autotuning as an HloPass.", "@cheshire Thanks. Do you mean this interface will be removed in the future? I propose this PR because one of our custom thunk class use this to support autotune. Is there any equivalent function \r\nto support non-GEMM thunk autotuning after the interface is removed? To add a new hlo pass again?  Thanks in advance!", "@wyzero Yes, the interface is removed now. The equivalent would be blocking inside the autotuning code, which you can also do in your custom thunk.\r\n\r\nAnd yes, autotuning is in the process of being moved to HloPasses.", "Closing based on https://github.com/tensorflow/tensorflow/pull/27574#issuecomment-483361291"]}, {"number": 27573, "title": "Bug Fix for Relu1 graph transformation.", "body": "Fied the bug in the code and add a TC file.", "comments": ["@miaout17 , i have resolved the merge conflicts and re-based the code, can you please review the PR.\r\n\r\nRegards\r\nAmit", "@miaout17, can you please review the PR and provide your feedback.\r\n\r\nRegards \r\nAmit ", "@gbaned , i have resolved the merge conflicts @miaout17 , can you please check the PR.\r\n\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 27572, "title": "Update docs for tf.linalg.triangular_solve", "body": "Not sure if `api_def_MatrixTriangularSolve.pbtxt` is going to be rendered as I expect it to be rendered\r\n\r\nAs a part of TensorFlow docs sprints from Munich.\r\n\r\ncc @dynamicwebpaige", "comments": ["any updates on this one? @dynamicwebpaige @annarev "]}, {"number": 27571, "title": "Doc: add example for batch_flatten()", "body": "Adding example for batch_flatten()\r\n\r\ncc @dynamicwebpaige ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27571) for more info**.\n\n<!-- need_sender_cla -->", "@vjravi thank you for your contribution, please sign CLA ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27571) for more info**.\n\n<!-- ok -->", "@vjravi can you please address Ubuntu Sanity build failures."]}, {"number": 27570, "title": "Doc: batch_flatten example added", "body": "batch_dot() seems to have an example and batch_flatten() doesn't. \r\nAdding example to tensorflow.keras.backend.py --> batch_flatten()\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27570) for more info**.\n\n<!-- need_sender_cla -->", "Hi Vijay, this seems like a good change. Can you re-open and sign the CLA? Thanks", "@lamberta Thanks, will do!"]}, {"number": 27569, "title": "Update docs for compute_gradient", "body": "As a part of TensorFlow docs sprints from Munich.\r\n\r\n\r\ncc @dynamicwebpaige", "comments": ["@alextp I've updated the PR"]}, {"number": 27568, "title": "TF2 Keras rms_optimizer.get_updates error", "body": "Hello, trying to migrate from standalone Keras and TF 1.12 to TF2.Keras. \r\nI used the tf2 conversion script and, then changed import keras.xx to import **tensorflow**.keras.xx. Yet, still getting error: `TypeError: get_updates() takes 3 positional arguments but 4 were given. `\r\n\r\nin the line:\r\n`updates = self.rms_optimizer.get_updates(self.model.trainable_weights, [], loss)`\r\n\r\nIt works in the standalone Keras and tf 1.12. Any idea what else I need to change. or is it a bug in tf2.\r\nusing tf-nightly-2.0-preview           2.0.0.dev20190404 in windows 10/Python 3.6.6", "comments": ["Thank you for reaching out to us. Can you please provide minimum reproducible code snippet to help us proceed further and verify what is going wrong.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I got the same problem when trying to use A3C model from https://github.com/germain-hug/Deep-RL-Keras. Basically, what's going on is this:\r\n```python3\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nimport tensorflow.keras.backend as K\r\n\r\n# Create shared model.\r\nshared_input = Input((10,))\r\nlayer = Dense(64, activation='relu')(shared_input)\r\nlayer = Dense(128, activation='relu')(layer)\r\nshared_model = Model(shared_input, layer)\r\n\r\n# Create actor model on top of shared model.\r\nactor_layer = Dense(128, activation='relu')(shared_model.output)\r\nactor_output = Dense(5, activation='softmax')(actor_layer)\r\nactor_model = Model(shared_model.input, actor_output)\r\n\r\n# Create critic model on top of shared model.\r\ncritic_layer = Dense(128, activation='relu')(shared_model.output)\r\ncritic_output = Dense(1, activation='linear')(critic_layer)\r\ncritic_model = Model(shared_model.input, critic_output)\r\n\r\n# Create actor optimizer\r\naction_pl = K.placeholder(shape=(None, 5))\r\nadvantages_pl = K.placeholder(shape=(None,))\r\nweighted_actions = K.sum(action_pl * actor_model.output, axis=1)\r\neligibility = K.log(weighted_actions + 1e-10) * K.stop_gradient(advantages_pl)\r\nentropy = K.sum(actor_model.output * K.log(actor_model.output + 1e-10), axis=1)\r\nloss = 0.001 * entropy - K.sum(eligibility)\r\n# TypeError: get_updates() takes 3 positional arguments but 4 were given.\r\nupdates = RMSprop(lr=0.0001, epsilon=0.1, rho=0.99).get_updates(actor_model.trainable_weights, [], loss)\r\nactor_opt = K.function([actor_model.input, action_pl, advantages_pl], [], updates=updates)\r\n```", "I believe this is fixed by changing\r\n```python3\r\nget_updates(actor_model.trainable_weights, [], loss)\r\n```\r\nto\r\n```python3\r\nget_updates(params=actor_model.trainable_weights, loss=loss)\r\n```", "no"]}, {"number": 27567, "title": "Update docs for create_local_cluster", "body": "\r\nWRONG\r\n- currently docs links to `tf.train.Server` that does not exist anymore\r\n\r\nTODO:\r\n- [x] add image in a proper way\r\n- [ ] fix a link to architecture page\r\n\r\n\r\ncc @dynamicwebpaige ", "comments": ["I've added image as a markdown image, not sure if it's the best way. Since I've seen in some other part of the docs also `img` with some styles", "@dynamicwebpaige what do we do with non-existing `tf.train.Server`?"]}, {"number": 27566, "title": "Improve description for tf.test.is_gpu_available", "body": "As a part of TensorFlow docs sprints from Munich.\r\n\r\nA small explanation for added warning - https://colab.research.google.com/drive/10pT_SQBAbZm9yVcoaACgrAlQ3YbpM_RI\r\n\r\n\r\ncc @dynamicwebpaige ", "comments": ["any updates on this one? @annarev @ymodak @dynamicwebpaige ", "oops, didn't realize it is already merged."]}, {"number": 27565, "title": "[TF==2.0.0a0] @tf.function raises ValueError when computing gradients", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow version (use command below): pip install tensorflow(-gpu)==2.0.0a0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nThe code executes normally, but raise ValueError when computing gradients (`tape.gradient)` if I decorate the training function with `@tf.function`. The traceback is as follows:\r\n<pre>\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/Workspaces/fgenl/run.py in <module>()\r\n     80     for batch_id in range(num_batches_each_epoch):\r\n     81         batch_data = data_generator.get_data() # v2\r\n---> 82         loss, outputs = train_one_step(batch_data) # v2\r\n     83         # _, loss, outputs, inputs = sess.run([opt_op, loss_, outputs_, batch_data])\r\n     84         if loss_metrics is None:\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    424     # This is the first call of __call__, so we have to initialize.\r\n    425     initializer_map = {}\r\n--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    427     if self._created_variables:\r\n    428       try:\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    368     self._concrete_stateful_fn = (\r\n    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 370             *args, **kwds))\r\n    371\r\n    372     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1311     if self._input_signature:\r\n   1312       args, kwargs = None, None\r\n-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1314     return graph_function\r\n   1315\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1578           or call_context_key not in self._function_cache.missed):\r\n   1579         self._function_cache.missed.add(call_context_key)\r\n-> 1580         graph_function = self._create_graph_function(args, kwargs)\r\n   1581         self._function_cache.primary[cache_key] = graph_function\r\n   1582         return graph_function, args, kwargs\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1510             arg_names=arg_names,\r\n   1511             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1512             capture_by_value=self._capture_by_value),\r\n   1513         self._function_attributes)\r\n   1514\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    692                                           converted_func)\r\n    693\r\n--> 694       func_outputs = python_func(*func_args, **func_kwargs)\r\n    695\r\n    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    316         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    318     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    319\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    684                   optional_features=autograph_options,\r\n    685                   force_conversion=True,\r\n--> 686               ), args, kwargs)\r\n    687\r\n    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    390     return _call_unconverted(f, args, kwargs)\r\n    391\r\n--> 392   result = converted_f(*effective_args, **kwargs)\r\n    393\r\n    394   # The converted function's closure is simply inserted into the function's\r\n\r\n/tmp/tmpx0xgcbu3.py in tf__train_one_step(batch_data)\r\n      6     outputs = ag__.converted_call(model, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (batch_data,), {})\r\n      7     loss, info = ag__.converted_call('calculate_loss', loss_object, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (outputs, batch_data), {})\r\n----> 8   gradients = ag__.converted_call('gradient', tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, model.trainable_variables), {})\r\n      9   update_list = [(grad, var) for grad, var in ag__.converted_call(zip, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (gradients, model.trainable_variables), {}) if grad is not None]\r\n     10   ag__.converted_call('apply_gradients', optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (update_list,), {})\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\r\n    265\r\n    266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):\r\n--> 267     return _call_unconverted(f, args, kwargs)\r\n    268\r\n    269   # internal_convert_user_code is for example turned off when issuing a dynamic\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)\r\n    186     return f.__self__.call(args, kwargs)\r\n    187\r\n--> 188   return f(*args, **kwargs)\r\n    189\r\n    190\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n    954         flat_sources,\r\n    955         output_gradients=output_gradients,\r\n--> 956         unconnected_gradients=unconnected_gradients)\r\n    957\r\n    958     if not self._persistent:\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)\r\n     70       sources,\r\n     71       output_gradients,\r\n---> 72       compat.as_str(unconnected_gradients.value))\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _aggregate_grads(gradients)\r\n    565         indexed_slices = ops.IndexedSlices(\r\n    566             grad,\r\n--> 567             math_ops.range(grad.shape[0]),\r\n    568             constant_op.constant(grad.shape.as_list()))\r\n    569         indexed_slices_list.append(indexed_slices)\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in range(start, limit, delta, dtype, name)\r\n   1258   with ops.name_scope(name, \"Range\", [start, limit, delta]) as name:\r\n   1259     start = ops.convert_to_tensor(start, dtype=dtype, name=\"start\")\r\n-> 1260     limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\r\n   1261     delta = ops.convert_to_tensor(delta, dtype=dtype, name=\"delta\")\r\n   1262\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\r\n   1048   preferred_dtype = deprecation.deprecated_argument_lookup(\r\n   1049       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\r\n-> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n   1051\r\n   1052\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1106       name=name,\r\n   1107       preferred_dtype=dtype_hint,\r\n-> 1108       as_ref=False)\r\n   1109\r\n   1110\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\r\n   1184\r\n   1185     if ret is None:\r\n-> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1187\r\n   1188     if ret is NotImplemented:\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    302                                          as_ref=False):\r\n    303   _ = as_ref\r\n--> 304   return constant(v, dtype=dtype, name=name)\r\n    305\r\n    306\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    243   \"\"\"\r\n    244   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 245                         allow_broadcast=True)\r\n    246\r\n    247\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    281       tensor_util.make_tensor_proto(\r\n    282           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 283           allow_broadcast=allow_broadcast))\r\n    284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    285   const_tensor = g.create_op(\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    453   else:\r\n    454     if values is None:\r\n--> 455       raise ValueError(\"None values not supported.\")\r\n    456     # if dtype is provided, forces numpy array to be the type\r\n    457     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n</pre>\r\n\r\n**Describe the expected behavior**\r\nThe code should also execute normally when using `@tf.function`.\r\n\r\n**Code to reproduce the issue**\r\n~~Sorry, I do not have a simple snippet to reproduce this issue. But could you find something in the traceback?~~ See below please.\r\n", "comments": ["I found a simple script to reproduce this issue, and it seems that the op `tf.sparse.sparse_dense_matmul` causes this issue.\r\n<pre>\r\n# -*- coding: utf-8 -*-\r\n# @Author  : Lin Lan (ryan.linlan@gmail.com)\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport scipy as sp\r\nimport tensorflow as tf\r\n\r\n\r\ndef sparse_to_tuple(sparse_mx):\r\n    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\r\n    def to_tuple(mx):\r\n        if not sp.sparse.isspmatrix_coo(mx):\r\n            mx = mx.tocoo()\r\n        coords = np.vstack((mx.row, mx.col)).transpose()\r\n        values = mx.data\r\n        shape = mx.shape\r\n        return coords, values, shape\r\n\r\n    if isinstance(sparse_mx, list):\r\n        for i in range(len(sparse_mx)):\r\n            sparse_mx[i] = to_tuple(sparse_mx[i])\r\n    else:\r\n        sparse_mx = to_tuple(sparse_mx)\r\n\r\n    return sparse_mx\r\n\r\n\r\ndef construct_tf_sparse_tensor(sp_sparse_matrix):\r\n    if not sp.sparse.issparse(sp_sparse_matrix):\r\n        raise TypeError\r\n\r\n    tuple_format = sparse_to_tuple(sp_sparse_matrix)\r\n    tf_sparse_tensor = tf.sparse.SparseTensor(\r\n        indices=tuple_format[0],\r\n        values=tuple_format[1],\r\n        dense_shape=tuple_format[2])\r\n    tf_sparse_tensor = tf.sparse.reorder(tf_sparse_tensor)\r\n    return tf_sparse_tensor\r\n\r\n\r\nweights = tf.Variable(\r\n    tf.random.uniform([512, 128]),\r\n    dtype=tf.float32,\r\n    trainable=True)\r\noptimizer = tf.optimizers.Adam()\r\n\r\n\r\n@tf.function\r\ndef train(x):\r\n    with tf.GradientTape() as tape:\r\n        embeddings = tf.sparse.sparse_dense_matmul(\r\n            x,\r\n            weights)\r\n        batch_embeddings = tf.nn.embedding_lookup(\r\n            embeddings, [1, 2, 3, 4, 5, 7, 8, 9, 10])\r\n        # embeddings = tf.nn.embedding_lookup(\r\n        #     embeddings, list(range(512)))\r\n        logits = tf.matmul(batch_embeddings, embeddings, transpose_b=True)\r\n        loss = tf.reduce_mean(logits)\r\n    gradients = tape.gradient(loss, [weights])\r\n    optimizer.apply_gradients(zip(gradients, [weights]))\r\n\r\n\r\nrandom_array = np.random.rand(512, 512)\r\nsparse_array = sp.sparse.csr_matrix(\r\n    np.asarray(random_array > 0.5, dtype=np.float32))\r\nsparse_tensor = construct_tf_sparse_tensor(sparse_array)\r\ntrain(sparse_tensor)\r\n</pre>\r\n\r\nTo let the above code compute gradients normally, one way is to uncomment `embeddings = tf.nn.embedding_lookup(embeddings, list(range(512)))`.", "The Tape is unable to see the variables.\r\nSo, use `tape.watch(embeddings)` after `sparse_dense_matmul()`. (Sparse Tensors cannot be watched, so watching x is not an option). \r\nThat solves the problem.", "@captain-pool  The above code works well in eager mode. It only fails when we use `@tf.function` decoration. So, the tape only cannot see the variable with AutoGraph?", "Well, that is exactly what's happening. For Autograph it works only with watch(). I'm still looking through the codebase to find the reason.", "There seems to be a slightly more helpful error in tf-nightly, but it looks like it's unrelated to tape.watch or autograph. The shape of `embeddings` seems to be partially unknown after the `sparse_dense_matmul`, and this line fixed in my tests:\r\n\r\n```\r\n        embeddings = tf.sparse.sparse_dense_matmul(\r\n            x,\r\n            weights)\r\n        embeddings.set_shape((512, 128))  # This removes the error.\r\n        batch_embeddings = tf.nn.embedding_lookup(\r\n            embeddings, tf.constant([1, 2, 3, 4, 5, 7, 8, 9, 10]))\r\n```\r\n\r\nReassigning to triage the tape error.", "There is a bug in the backprop code, where it does `math_ops.range(grad.shape[0])` which uses the static shape of the grad tensor, which might be known (a number) or None. To use the dynamic shape we need something like `math_ops.range(array_ops.shape(grad)[0])`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27565\">No</a>\n"]}, {"number": 27564, "title": "Correct doc of Ftrl optimizer by remplacing $ to $$", "body": "Doc of Ftrl (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Ftrl) only use one $ instead of two for mathematical equations, so these equations are not displaying correctly", "comments": []}, {"number": 27563, "title": "Correct doc of Ftrl optimizer by remplacing $ to $$", "body": "Doc of Ftrl (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/Ftrl) only use one $ instead of two for mathematical equations, so these equations are not displaying correctly", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27563) for more info**.\n\n<!-- need_author_cla -->", "Hi S\u00e9bastien, this seems like a good change. Can you re-open and sign the CLA? THanks", "Hi ! I had an issue with the email address, so I changed it and did a new pull request with the same changes. Please see https://github.com/tensorflow/tensorflow/pull/27564 :)"]}, {"number": 27562, "title": "tf.estimator.train_and_evaluate does not show anything when running multi-worker example in TensorFlow 2.0 Alpha", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): TF 2.0 Alpha CPU only \r\n- Python version: 2.7\r\n\r\nI have run this example in TF 2.0, however the estimator train and evaluate does not show anything as provided in this link: https://www.tensorflow.org/alpha/tutorials/distribute/multi_worker\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\nimport os, json\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\n\r\ndef input_fn(mode, input_context=None):\r\n  datasets, ds_info = tfds.load(name='mnist',\r\n                                with_info=True,\r\n                                as_supervised=True)\r\n  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\r\n                   datasets['test'])\r\n\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  if input_context:\r\n    mnist_dataset = mnist_dataset.apply(tf.data.experimental.filter_for_shard(\r\n        input_context.num_input_pipelines, input_context.input_pipeline_id))\r\n  return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n\r\nNUM_WORKERS = 1\r\nIP_ADDRS = ['localhost']\r\nPORTS = [12345]\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': ['%s:%d' % (IP_ADDRS[w], PORTS[w]) for w in range(NUM_WORKERS)]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\nLEARNING_RATE = 1e-4\r\ndef model_fn(features, labels, mode):\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  logits = model(features, training=False)\r\n  \r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    predictions = {'logits': logits}\r\n    return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\r\n  \r\n  optimizer = tf.compat.v1.train.GradientDescentOptimizer(\r\n      learning_rate=LEARNING_RATE)\r\n  loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n      from_logits=True)(labels, logits)\r\n  if mode == tf.estimator.ModeKeys.EVAL:\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n  \r\n  return tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      loss=loss,\r\n      train_op=optimizer.minimize(\r\n          loss, tf.compat.v1.train.get_or_create_global_step()))\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy, eval_distribute=strategy)\r\n\r\nclassifier = tf.estimator.Estimator(\r\n    model_fn=model_fn, model_dir='/tmp/multiworker', config=config)\r\ntf.estimator.train_and_evaluate(\r\n    classifier,\r\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\r\n)\r\n```\r\nThe result in the terminal is like this:\r\n\r\n```\r\n2019-04-06 19:26:08.807447: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-04-06 19:26:08.825158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2693670000 Hz\r\n2019-04-06 19:26:08.825963: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x5944140 executing computations on platform Host. Devices:\r\n2019-04-06 19:26:08.825997: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0406 19:26:08.827748 47224889664000 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2019-04-06 19:26:08.830148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345}\r\n2019-04-06 19:26:08.831611: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:359] Started server with target: grpc://localhost:12345\r\nW0406 19:26:08.841363 47224889664000 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nW0406 19:26:08.843209 47224889664000 cross_device_ops.py:1106] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nW0406 19:26:09.397347 47224889664000 monitored_session.py:344] Collective ops may deadlock with `save_checkpoints_secs` please use `save_checkpoint_steps` instead. Clearing `save_checkpoint_secs` and setting `save_checkpoint_steps` to 1000 now.\r\n2019-04-06 19:26:09.526794: I tensorflow/core/distributed_runtime/master_session.cc:1194] Start master session fdddc32e0a2bc48c with config: device_filters: \"/job:worker/task:0\" device_filters: \"/job:worker/task:0\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE scoped_allocator_optimization: ON scoped_allocator_opts { enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" enable_op: \"CollectiveReduce\" } } } experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\n2019-04-06 19:26:10.319007: W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.\r\n2019-04-06 19:26:23.553348: W tensorflow/core/common_runtime/eager/context.cc:195] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```\r\n\r\nIs this bug? or how to solve this problem?\r\n\r\n\r\n", "comments": ["It seems that when I enable: tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO). This error happens\r\n\r\n```\r\nI0407 00:24:10.743343 47080957506048 monitored_session.py:241] Graph was finalized.\r\nI0407 00:24:10.745394 47871495983616 monitored_session.py:241] Graph was finalized.\r\n```\r\n\r\n```\r\nINFO:tensorflow:An error was raised while a session was being created. This may be due to a preemption of a connected worker or parameter server. A new session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the job. Error: OS Error\r\nINFO:tensorflow:Graph was finalized.\r\n```\r\n", "I solve this problem. It seems that we must use 'localhost', cannot use 'host' only", "That's great. For now we are closing the issue. If you are stuck anywhere, we will open the same again. Thanks!", "I changed my \"localhost\" to \"ip\" and I try other methods, however, it doesn't work.\r\n @ymsaputra can you help me? thx!"]}, {"number": 27561, "title": "Fix typo for local_client.h", "body": "fix typo", "comments": ["@wyzero thanks you for your contribution , latest changes have deleted that line, we don't need this change now, thank you"]}, {"number": 27560, "title": "Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model", "body": "Hi,\r\nI am training a model for image classification in TensorFlow using pre-trained model of MobileNet and embedding the .tflite file in Android app. \r\n\r\nIt so happens that if I do `model.add(layers.Flatten())` after adding the MobileNet model, I get the following error, when executing the Android app:\r\n`Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model`\r\n\r\nHowever, if I use `model.add(layers.GlobalAveragePooling2D())` after the MobileNet model, the app works normally. \r\n\r\nCould someone please help me as to why the `layers.Flatten()` is causing trouble?\r\n\r\nComplete code is available [here](https://github.com/PikkaPikkachu/train-em-all/). \r\nThanks!\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan thanks! Have opened the question [here](https://stackoverflow.com/questions/55948770/why-does-using-layers-flatten-in-cnn-model-cause-the-error-in-android-when-i-a) for anyone else having the same query."]}, {"number": 27558, "title": "Added Oxford comma in README.md", "body": "Added an Oxford comma to line 28, to be consistent with the rest of the file.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27558) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27558) for more info**.\n\n<!-- ok -->", "@Yoshimitsu thank you for your contribution , i don't think we need this change.", "Hi Alex, thanks for the PR and improving the docs!\r\nI think this is a good change and is in line with the Google Developer Docs style guide: https://developers.google.com/style/commas\r\n"]}]