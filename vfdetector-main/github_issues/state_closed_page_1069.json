[{"number": 21207, "title": "removed inline function", "body": "if one includes the header and does not link with coding.cc gets unresolved error. not the case when you just include extern functions and do not use them, more over this function does not seem to save so much being inlined.", "comments": ["Sorry I completely missed this one. Are you still interested in proceeding?", "well, if it makes sense now\n\nOn Fri, 30 Nov 2018 at 01:25, andrehentz <notifications@github.com> wrote:\n\n> Sorry I completely missed this one. Are you still interested in proceeding?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21207#issuecomment-443031232>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cT-gA_WO_MZFbQPG7IJwywGkcMTlks5u0G0EgaJpZM4VlCpm>\n> .\n>\n", "i think the code base is the same, so the error is there", "Thanks for you contribution and your understanding!", "Changes are already in the master branch , closing this "]}, {"number": 21206, "title": "Nysnc and highwayhash cmake fixes", "body": "", "comments": ["Cmake build probably fails, because protobuf is not updated to 3.6.1"]}, {"number": 21205, "title": "tensorflow on Python 3.7", "body": "I upgraded to python 3.7. \r\n\r\nNow when I try to run it in jupyter notebook or in terminal, I receive following errors: \r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n\r\n  File \"<ipython-input-1-39d68f4585c2>\", line 4, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid syntax", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "duplicate of #20517"]}, {"number": 21204, "title": "Retrieve 'feature_importances` from canned BoostedTreesRegressor and BoostedTreesClassifier", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Feature request\r\n\r\nWhen using the Premade BoostedTreesRegressor or Classifier in TensorFlow to train a model, it will be nice to have a way to retrieve the `feature_imporances` or feature ranking to know which variables have the most effect in the resulting model as seen in Scikit-learn's implementation.\r\n\r\nPerhaps it can be something like, `trees.get_variable_value('feature_imporances')` to retrieve the feature rank in the order of the inputs.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This is not supported as of now in a canned version. You can use the contrib version tensorflow/contrib/boosted trees, which can export feature importances (see boston.py example)", "Should this be kept open as a feature request? Would you accept contributions? ", "Sure, contributions are welcome. Technically all the data is already kept around - it is gain in NodeMetadata. One would need to go through all the nodes of all the trees and sum the gains groupped by the feature ids.", "I'm interested in the issue, and I'd like to take a try :-)", "Go for it. Mention @nataliaponomareva for review when you have something. Thanks!", "Nice. I just forget to remind that PR #21509 is created to fix the issue.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 21203, "title": "Added dim parameter to tf.losses.softmax_cross_entropy", "body": "Added dim parameter to losses.softmax_cross_entropy. Fixes #20866", "comments": ["(for API owners) Yes, should be axis. Fine otherwise.", "I have made the changes as requested. @martinwicke  Can you review and accept the changes.", "@Siddhant085 Sorry for the delay, but please take a look at the test failures, e.g.,\r\nhttps://source.cloud.google.com/results/invocations/f9de5bb5-3450-4f3c-b30a-e978b5e2203b/targets/%2F%2Ftensorflow%2Fpython%2Fkernel_tests:losses_test/log", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 21202, "title": "Quick Fix for Python 3.7", "body": "Fix for 3.7 I made before I found https://github.com/tensorflow/tensorflow/pull/20766\r\n\r\nJust built CUDA 9.2 CuDNN 7 compute 6.1 python 3.7 on VS2017 using CMake.\r\n\r\nThat seems to be the place for working on what will be the final version, but I'm just putting this out there if anyone wants a quick fix in the meantime (currently that PR conflicts). This version casts away consts, so it isn't exactly safe but should be fine if you just want things up and running. Tensorflow overuses const_cast but that is an issue for another day.\r\n\r\nJust merging this branch into your own work is an easy option if you need to build for 3.7. I'll close this PR when the other branch gets finished.\r\n\r\nRelated reading materials:\r\n- https://github.com/tensorflow/tensorflow/issues/20517\r\n- https://github.com/tensorflow/tensorflow/issues/17022\r\n- https://github.com/tensorflow/tensorflow/issues/20690\r\n- https://github.com/tensorflow/tensorflow/pull/20766\r\n\r\nCheers", "comments": ["@alextp @asimshankar Should we merge this PR instead of #20766 since there has been no updates on the other PR?", "Whichever PR makes progress is fine with me :)\r\nI'll close that other one since it seems the author of the other hasn't gotten around to it.\r\n\r\nThat said, the comments on that PR still apply here:\r\n\r\n1. Let's avoid the `const_cast`.\r\n2. Let's isolate the changes to the SWIG files instead of changing the C API.\r\n\r\n@bstriner : Would you be able to update this PR to do 2.?\r\n(I had a pending change for 1., so can submit that unless you want to update the PR)", "Commited abb903df7a5998b33547c02e95f9fa47c00f31f4 for avoiding the `const_cast`.\r\n\r\n@bstriner : Are you interested in addressing 2. in this PR?", "@asimshankar I'm trying to change SWIG interface to fix the problem as you suggested. However I got the following error:\r\n```\r\nERROR: \r\n(cache directory)/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc: In function \u2018bool google::protobuf::python::descriptor::_GetItemByKey(google::protobuf::python::PyContainer*, PyObject*, const void**)\u2019:\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from \u2018const char*\u2019 to \u2018char*\u2019 [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                                             ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: note: in expansion of macro \u2018PyString_AsStringAndSize\u2019\r\n         if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {\r\n             ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from \u2018const char*\u2019 to \u2018char*\u2019 [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                                             ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: note: in expansion of macro \u2018PyString_AsStringAndSize\u2019\r\n         if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {\r\n             ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \u2018-Wno-writable-strings\u2019\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\nIt seems that `const_cast` problem still exists, and I just can't find file named `descriptor_containers.cc`. It seems this is caused by protobuf, can we solve this only modifying source of tensorflow?\r\n\r\n[Fix of protobuf](https://github.com/google/protobuf/pull/4862)", "protobuf 3.6.1 has fixed the problem already.\r\nEDIT: I read it wrong. See below.", "@ppwwyyxx : I might be reading incorrectly, but it seems that protobuf 3.6.1 does not include the change to make it compatible with Python 3.7 (commit https://github.com/google/protobuf/commit/0a59054c30e4f0ba10f10acfc1d7f3814c63e1a7 isn't included in the 3.6.x branch)\r\n\r\n@Hong-Xiang : You're right, we'll need to update the version of protobuf that TensorFlow depends on, perhaps after there is a release version of protobuf that is compatible with Python 3.7?", "I think this would also fix #20444.", "I'll take a look tonight. One issue was getting swig to generate consts for\ntemporary variables. Afk right now but will post details later.\n\nOn Mon, Aug 13, 2018, 4:45 PM Martin Wicke <notifications@github.com> wrote:\n\n> I think this would also fix #20444\n> <https://github.com/tensorflow/tensorflow/issues/20444>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21202#issuecomment-412656556>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL4rbDTfnMCKxHf0pdzhrTTkR1zGvWhPks5uQeV9gaJpZM4Vk9NK>\n> .\n>\n", "@asimshankar the renames seem cleaner if we do them in the cpps, but I'm willing to go either way. Bulky to have custom wrappers for a few functions just because of the `async` name conflict.\r\n- Unfortunately, no purely swig renaming like there is for function names\r\n- A macro would work, but just blanket replacing the word `async` feels like overkill\r\n- Could create new functions that wrap the old functions and have better parameter names\r\n- Could rename the parameters in the cpps\r\n\r\nAny way we do it, any thoughts on the rename? `async_` is closest typographically but maybe something like `is_async` is better semantically.\r\n\r\nThanks!", "@asimshankar BTW, do the `const_cast`s in TF bother anyone else? This idiom tastes like spaghetti: `reinterpret_cast<T*>(const_cast<char*>((out->tensor_data().data())));`.\r\n\r\nThe void pointer gets casted up to `const` then `const_cast`ed back, and gets casted from `void` to `char` to `T`. Getting `out->buf_` would be great if there was an accessor.", "Thanks  @bstriner : It seems that at this point this PR can be reduced to 2 lines, let's just do that: Change `async` to `enable` in the two functions in `c_api.h` (or if you want in the `.cc` file too). If you want to update the PR to do that, that would be awesome.\r\n\r\nRegarding `const_cast`, I haven't audited all the uses, but in general, yeah we prefer to avoid them, though it may not always be possible.", "This includes the protobuf bump as well just so I could get it to build, but let me know what you want to do about that. Moved the protobuf data into defines so it is just one place instead of 3 places.\r\n\r\nI don't have a good way to get rid of the `const_cast`ing in `pywrap_tfe.i`. `TFE_GetPythonString` is now returning a `const char*`, which makes sense.\r\n\r\nSWIG is currently generating\r\n```c\r\nchar *arg2 = (char *) 0 ;\r\narg2 = const_cast<char*>(TFE_GetPythonString(obj1));\r\nTFE_ContextAddFunctionDef(arg1,(char const *)arg2,arg3,arg4);\r\n```\r\n\r\nI want it to generate this but I can't figure out how to make `arg2` a `const`\r\n```c\r\nconst char *arg2 = (const char *) 0 ;\r\narg2 = TFE_GetPythonString(obj1);\r\nTFE_ContextAddFunctionDef(arg1,arg2,arg3,arg4);\r\n```\r\n\r\nDoes anyone know a way to get that `const` the way I want it? Something like `typemap(arginit)` but for declarations? I tried things like `%typemap(in) const char* device_name (const char*)` but can't figure out any syntax that will work.", "@asimshankar the thing responsible for like 50% of the `const_cast` (just eyeballing it) is `tensor_data`. There is no way around using a bunch of `const_cast`. Tensors have a `void*` to the data, but the only getter (`tensor_data`) gets you a `const char*` even if you want to mutate the data. A getter like `T* tensor::data_ptr<T>()` would eliminate that mess and the casting.", "Just built python3.7 successfully on both linux and windows. New protobuf has issues on windows so I had to make a patch file.\r\n\r\nAdd these 2 patch files to build on windows:\r\n- protobuf https://gist.github.com/bstriner/1d4ee0db5e5efcc04c6a82b20ee0bebb\r\n- eigen https://gist.github.com/bstriner/e11afeddf002640ee6a773ab0e6bb9b3\r\n", "Based on commit 0bfe6381f25a686999cfb77a685a698ceb86f7b4 compiled for macOS [tensorflow-1.10.0-cp37-cp37m-macosx_10_13_x86_64.whl](https://yadi.sk/d/h73TAasm3aLTCE)\r\nThank you, @bstriner.", "hi @bstriner, apparently we just wait you to do two minor tweaks +  what about the 2 patch files to build on windows ?", "@asimshankar I reverted the protobuf version but left in the variables so it will be easier to change version as necessary.\r\n\r\n@stonebig the patch files are above. Hopefully the libs will be fixed soon so the patches won't be necessary.\r\n\r\nFor the untested version of protobuf that I used, but it seems to work on windows, change to the following, and add `patch_file=PROTOBUF_patch_file` to the 3 libraries in workspace.bzl.\r\n\r\n```\r\n    PROTOBUF_urls =[\r\n \"https://mirror.bazel.build/github.com/google/protobuf/archive/ab09b2a2e203b204e11ac64750e62b3f1da7dc6e.tar.gz\",\r\n    \"https://github.com/google/protobuf/archive/ab09b2a2e203b204e11ac64750e62b3f1da7dc6e.tar.gz\",\r\n    ]\r\n    PROTOBUF_sha256 = \"4a1d4dc12fe2e3621233cfeaf6baf9e47f6e184e1171f7f0c9c126942a22e31b\"\r\n    PROTOBUF_strip_prefix = \"protobuf-ab09b2a2e203b204e11ac64750e62b3f1da7dc6e\"\r\n    PROTOBUF_patch_file = \"//tensorflow:protobuf-patch.txt\"\r\n```\r\n\r\nFor eigen patch, same thing. Download the file and add it as a `patch_file=` to the eigen library in workspace.bzl.", "@bstriner nice work. hope the pr merged soon. ", "Thanks! Is there a separate issue already for tracking protobuf and eigen fixes? Also might want to start an issue in swig for that const issue.", "@bstriner protobuf issue - https://github.com/tensorflow/tensorflow/issues/20950", "Come on guys please upgrade to python 3.7 and make official release..... I need typing and data classes from python 3.7", "@SpikingNeurons I feel you, but can you post any issues you have building from source with the above?", "@bstriner Unfortunately, I depend on windows binaries and don't compile from source. Its shame that I cannot add value. ", "I'm getting this error as soon as I change the protobuf URLs/checksum/prefix for the ones above:\r\n\r\n`ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package 'tensorflow': Extension file not found. Unable to load package for '@bazel_skylib//:lib.bzl': The repository could not be resolved\r\n`\r\n\r\nWorks fine when I change it for 3.6.1, but the fix for Python 3.7 is not included. Any idea what I'm doing wrong?", "@mattpepin copy bazel_skylib from protobuf workspace to tensorflow workspace. Let me know if you have any issues after that.", "@bstriner Thanks! It worked perfectly!", "@bstriner I don't understand if this issue has been resolved for Ubuntu with a PR+merge or not. I tried a git checkout r1.10, a bazel clean, reconfigure and build, but still get the Protobuf error. Am I supposed to do something manual for this to build on Python 3.7, Ubuntu 18.04? r1.12 also does not compile.", "I just tried to build the r1.12 branch:\r\n```\r\nERROR: /home/[...]/.cache/bazel/_bazel_[...]/20b747b5896a556e0b493e9e90c43fcf/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\n\r\nexternal/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc: In function 'PyObject* google::protobuf::python::extension_dict::_FindExtensionByName(google::protobuf::python::ExtensionDict*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:56:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/extension_dict.cc:184:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n```\r\nI'm using Arch linux with up-to-date packages, i.e. Python 3.7 and so on.", "@RyanRosario correct. There is no stable release of protobuf with the required changes. See discussion above and in the linked post about how to switch protobuf versions. Feel free to check github for current master and try that version.\r\n\r\n@Hoeze That is also a protobuf issue. Same story.", "```\r\nINFO: Build options have changed, discarding analysis cache.\r\nERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:223:1: error loading package 'tensorflow': Extension file not found. Unable to load package for '@bazel_skylib//:lib.bzl': The repository could not be resolved and referenced by '//tensorflow/tools/pip_package:build_pip_package'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package 'tensorflow': Extension file not found. Unable to load package for '@bazel_skylib//:lib.bzl': The repository could not be resolved\r\nINFO: Elapsed time: 0.380s\r\n```\r\n> @mattpepin copy bazel_skylib from protobuf workspace to tensorflow workspace. Let me know if you have any issues after that.\r\n\r\nwhat does this mean?\r\nthere's nothing in `\\tensorflow\\third_party\\protobuf` nor any string match 'skylib' in `\\tensorflow\\workspace.bzl`"]}, {"number": 21201, "title": "[CMAKE] Add MKL to PATH", "body": "Currently, instead of adding `mkl_BIN_DIRS` to the path, the path is replaced by `mkl_BIN_DIRS`.  I was getting weird errors from the generated command because my `PATH` was now gone. This PR adds the MKL binary path to the front of `PATH`  instead of replacing `PATH` .\r\n\r\nBrief discussion of why this wasn't done before is in the CMake file comments. Refer to those comments for details. It appears there were some issues with having more than one path in `PATH`. I've tested this on Windows to generate the correct syntax, but haven't tested on a Linux build.\r\n\r\n@LoSealL @mrry It looks like both of you worked on that section.  What kinds of issues were happening with multiple items in `PATH`? With only MKL on the path, generated commands fail for me at importing numpy.\r\n\r\nIf the command fails it shows up in the build log as `error MSB6006: \"cmd.exe\" exited with code 1.`\r\n\r\nBTW, the other minor changes are from tab to space.\r\n\r\nCheers", "comments": ["Yes, you're right. The previous PR is just a work around.\r\nBut I noticed that the build failure is caused by the project `estimator_python_api`, which was introduced later after the PR, and this project needs the path of MKL to locate the libraries.\r\n\r\nHence in my new [PR](https://github.com/tensorflow/tensorflow/pull/20951), I changed this part of logic, just by copying MKL libraries into python package, so the generating script could locate the MKL libraries correctly without assigning a path explicitly. I think this could be a better way to handle the path to MKL libraries, and the built distributed python package (.whl) could installed MKL to system, while in the previous CMAKE script one needs to copy MKL libraries into system path explicitly after installing .whl package.\r\n", "And when I executed command like `cmake -E env PYTHONPATH=$a $b $c ...` or `cmake -E env PYTHONPATH=$a;$b;$c ...`, neither of them is correctly executed in cmake building process. The correct pattern I think is `cmake -E env PYTHONPATH=\"$a;$b;$c\" ...` but I found the cmake interprets it as `PYTHONPATH=$a $b $c` or something like `PYTHONPATH=\\\"$a $b $c\\\"` which definitely incurs errors", "@LoSealL The standard quoting would be `PATH=\"a;b;c\"` or `\"PATH=a;b;c\"`. The command this generates looks odd but actually parses correctly (at least in my testing): `cmake ... \"PATH=a\";\"b\";\"c\" ...`.\r\n\r\nMakes a lot of sense to get everything in the same directory and not rely on the PATH. That way it can coexist with other things on my system. I'm using MKL and MKLDNN for other things, too, so it would be nice to be sure I'm loading the right MKLDNN in tensorflow without relying on the PATH."]}, {"number": 21200, "title": "the weights of tf.contrib.rnn.BasicLSTMCell can't be updated", "body": "Environment\r\nOS: Ubuntu 16.04\r\nTensorflow-gpu: 1.8\r\n\r\n```\r\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\r\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\r\n    if self.mode == \"train\":\r\n      lstm_cell = tf.contrib.rnn.DropoutWrapper(\r\n          lstm_cell,\r\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\r\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\r\n\r\n    with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\r\n      # Feed the image embeddings to set the initial LSTM state.\r\n      zero_state = lstm_cell.zero_state(\r\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\r\n\r\n      # Allow the LSTM variables to be reused.\r\n      #lstm_scope.reuse_variables()\r\n\r\n      ........\r\n\r\n      scores = tf.Variable(tf.random_normal(shape=[K, self.config.batch_size, C]), name=\"scores\")\r\n\r\n      M = tf.Variable(tf.random_normal(shape=[K+1, self.config.batch_size, 2, 3]), name=\"M\")\r\n      tf.assign(M[0], tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.]]))\r\n\r\n      lstm_input_size = 14\r\n      zk_size = 4096\r\n\r\n      hidden = zero_state\r\n\r\n      for k in range(0, K+1):\r\n          .......\r\n\r\n          lstm_outputs, hidden = lstm_cell(f_k, hidden) \r\n```\r\n\r\nM and Scores can be updated. Why BasicLSTMCell is not in the optimizeLoss module\uff1f\r\n\r\n\r\n![opimizeloss](https://user-images.githubusercontent.com/7899459/43352992-ce472a2a-925f-11e8-9f48-08c9d64fa503.png)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "After I add \"reuse=True\" into tf.contrib.rnn.BasicLSTMCell, it encounters error when it comes to `lstm_outputs, hidden = lstm_cell(f_k, hidden) `\r\nThe error is shown as:\r\n`ValueError: Variable lstm/basic_lstm_cell/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?`\r\n\r\n@reedwm", "Would anybody else answer this question ?\r\n@tensorflowbutler "]}, {"number": 21199, "title": "Documentation issue ", "body": "![screenshot 25 _li](https://user-images.githubusercontent.com/25861787/43352882-816db4d2-9244-11e8-8f0a-3af1eef9b29f.jpg)\r\nIt's a Fashion MNIST dataset Tesorflow tutorial.What are digits doing here?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I think we're talking about the classification of images of numeric arabic digits.", "Hi @abdurrehman1998 \r\n\r\nYes, this tutorial is using fashion-mnist, this comment is left-over from an earlier version.\r\n\r\nThe source file for this tutorial is here: https://github.com/tensorflow/models/blob/master/samples/core/tutorials/keras/basic_classification.ipynb\r\n\r\nCan you send me a PR with a fix?\r\nThe easiest way to do it is to edit in Colab and use the \"save a copy to github\" option. \r\n\r\nThanks!", "Hey guys, I sent a PR to fix this [#5032](https://github.com/tensorflow/models/pull/5032).", "Merged. Thanks!"]}, {"number": 21198, "title": "'toco_from_protos: not found' error found using prebuilt tensorflow binary for Raspbian Stretch 9 on Raspberry Pi 3 Model B", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian Stretch 9\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Raspberry Pi 3 Model B\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:\r\ntf.GIT_VERSION returns v1.9.0-0-g25c197e\r\n- **Python version**:3.5.3\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:Please refer to Raspberry Pi 3 Model B specs, but I use the cpu only version of tensorflow.\r\n- **Exact command to reproduce**:\r\nuse example code in tf-lite wiki... One sec...\r\n# Converting a GraphDef from file. converter = lite.TocoConverter.from_frozen_graph( graph_def_file, input_arrays, output_arrays) tflite_model = converter.convert() open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\nThis was taken from tf.contrib.lite.TocoConverter's API webpage.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\ntl;dw2r run the script on the above specs and you get 'toco_from_protos: not found'\r\nlong story:\r\nI am trying to convert a frozen pb file into a tflite format.  I have a Windows 10 system, but tflite isn't available yet on the Windows 10 binaries.  The only other system that I have available is a Raspberry Pi so I figured that its version of tensorflow should have access to the tflite segment of the api.  Sure enough, it was there.  But when I ran the script, the following error occurred.  I can try to use Ubuntu, but it's been a bit of a learning curve trying to manage all the systems I have built on my Windows computer with Linux alternatives.  If at all possible, I would like to have a solution on either of these devices.  \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Whilst waiting for a response, I found this stack overflow answer helpful for my situation. https://stackoverflow.com/questions/50902067/how-to-import-the-tensorflow-lite-interpreter-in-python", "Ok, folks. Try using the python2.7 nightly build.  I think this is the only way we will get the interpreter to work atm.  For some reason, python3 doesn't play nice with the interpreter.  \r\n::Edit::\r\nNope, python2.7 nightly build doesn't work \ud83d\ude26 \r\n::Edit End::", "I'll take a look at why Python 3 interpreter is not working and try to fix it. Thanks for trying both and identifying that as the problem. Did you try the interpreter on the x86 linux. i.e., Is this a problem with just the Raspberry PI?", "To aselle:\r\nJust a Raspberry Pi problem.  Interpreter works like a charm on x86_64 ubuntu.  Raspberry Pi throws out an error.  My python2 is broken on my Raspberry Pi at the moment, but trying the 2.7 build worked for someone else.  \r\n::Edit::\r\nI must have misunderstood.  The 2.7 build doesn't work either.\r\n::Edit End::\r\n::Edit2::\r\nOk, it's the ARMv7 implementations that are interfering with the interpreter.  Everything works peachy on the ci.tensorflow.org pi-zero binary.  Follow the comments below to kind of follow along.\r\n::End Edit2::\r\nTo those looking for a python3 workaround:\r\nIf you really want python3 to work, thortex's fix seems to be working, but I've been running his script for 12 hrs now.  Furthermore, you need a usb drive as additional swap space.  https://github.com/thortex/rpi3-tensorflow", "TL;DR: Script mention earlier doesn't fix interpreter issue.  Interpreter and more specifically the interpreter wrapper seems to be the problem.  \r\n\r\nLong story:\r\nthortex's script \"worked.\"  If I open a terminal, go:\r\n`python3`\r\n`import tensorflow as tf`\r\n`dir(tf.contrib.lite)`\r\nI get:\r\n`['DecodeError', 'Interpreter', 'OpHint', 'PY3', 'TocoConverter', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_freeze_graph', '_graph_pb2', '_is_frozen_graph', '_session', '_text_format', 'absolute_import', 'constants', 'convert_op_hints_to_stubs', 'division', 'freeze_saved_model', 'get_tensors_from_tensor_names', 'global_variables_initializer', 'import_graph_def', 'print_function', 'set_tensor_shapes', 'signature_constants', 'tag_constants', 'tensor_name', 'tf_graph_util', 'toco_convert', 'toco_convert_protos']`\r\nBUT as soon as I try freedomtan.py's [label_image.py](https://github.com/tensorflow/tensorflow/pull/19736/commits) script, I get something like:\r\n`2018-08-03 04:58:17.421863: F tensorflow/core/framework/variant_op_registry.cc:104] Check failed: existing == nullptr (0x212ae74 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::Tensor already registered\r\nAborted`\r\nI enabled `tf.enable_eager_execution()` and ran the script on thonny:\r\n::Start Thonny Copy Pasta::\r\n%Run freedomtan.py\r\nBackend terminated (returncode: -6)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x76fdb010 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 222 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 914 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 577 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 666 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 693 in _load\r\n  File \"/usr/lib/python3.5/imp.py\", line 342 in load_dynamic\r\n  File \"/usr/lib/python3.5/imp.py\", line 242 in load_module\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24 in swig_import_helper\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 222 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 673 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 673 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 958 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 969 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986 in _gcd_import\r\n  File \"/usr/lib/python3.5/importlib/__init__.py\", line 126 in import_module\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 42 in _load\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 53 in __getattr__\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tensorflow/contrib/lite/python/interpreter.py\", line 50 in __init__\r\n  File \"/home/pi/wontae-cv/freedomtan.py\", line 65 in <module>\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/thonny/backend.py\", line 588 in execute_source\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/thonny/backend.py\", line 427 in _execute_source_ex\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/thonny/backend.py\", line 374 in _execute_file\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/thonny/backend.py\", line 155 in _cmd_Run\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/thonny/backend.py\", line 119 in handle_command\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/thonny/backend.py\", line 97 in mainloop\r\n  File \"/usr/lib/python3/dist-packages/thonny/shared/backend_launcher.py\", line 41 in <module>\r\nResetting ...\r\n>>> \r\n::End Thonny Copy Pasta::\r\nAnyways, the interpreter is most definitely the issue, specifically the wrapper.  \r\nI'm going to rerun spec scripts again because I'm going to wipe my OS to see if I can fix my broken python/pip issue.\r\npi@pi:~/wontae-cv $ python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nunknown 1.9.0\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2256092/tf_env.txt)\r\n\r\n", "I wiped my raspberry pi clean.  Installed Raspbian Stretch 4.14.52-v7+ Lite.  Installed the nightly python2.7 build and received the same interpreter error.  I'd love to give you all the error log, but I have no idea how to do that from a command-line only os.  ", "It's broken on 1.8 ci.tensorflow.org package as well. \ud83d\ude22 ", "Ok, I've more or less figured it out.  The ARMv7 NEON implementations are interfering with the the interpreter wrapper.  Use the ci.tensorflow.org pi-zero implementations.  They work for both python3 and python 2.  It's all built on docker apparently, so just change the file names and install like any pip install.  To be explicit: \r\ncp34->cp35\r\narmv6l->arvm7l that's an lowercase 'L' at the end there.", "If you need to compile from source for whatever reason.  Cross compile off of docker and DO NOT USE the armv7 settings.  Instead use something like PI_ONE.  \r\n[It's all here](https://www.tensorflow.org/install/install_raspbian)\r\n`$ tensorflow/tools/ci_build/ci_build.sh PI tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE`", "@gragundier FYI, I was able to build pip package on RPI 3 w/ my one-line patch (https://github.com/tensorflow/tensorflow/pull/21109) and ran my label_image.py without problem.\r\n\r\n```\r\nbazel build --config opt \\\r\n--local_resources 1024.0,0.5,0.5 \\\r\n--cxxopt=-mfpu=neon-vfpv4 \\\r\n--cxxopt=-ftree-vectorize \\\r\n--cxxopt=-funsafe-math-optimizations \\\r\n--cxxopt=-ftree-loop-vectorize \\\r\n--cxxopt=-fomit-frame-pointer \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```", "@gragundier Did @freedomtan's patch work? If so, I'd like to close the issue. Thanks!", "Nagging Assignees @petewarden, @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tofulawrence Sorry, I've been busy as of late.  I just got my tinkerboard s and raspberry pi 3 b+ today as well, so I'd like to see if the patches work on them too.  Give me another 2 days please.", "Any updates, @gragundier? Thanks!", "I haven't been able to test on the tinkerboard but it works for rapsberry Pi's closing!"]}, {"number": 21197, "title": "tensorflow conflicts with PCL", "body": "I want to build a .so file using tensorflow and pcl in ubuntu16.04LTS, but can't succeed. In order to find what arise the errors, I deleted all codes but INCLUDE sentence, and found the header of tensorflow conflicts with pcl'. How should I do?\r\n---------------------------------------------------------------------------------------------------------------------------------\r\ncpp file:\r\n#include <pcl/point_types.h>\r\n#include <pcl/registration/icp.h>\r\n#include <pcl/registration/lum.h>\r\n#include \"tensorflow/core/framework/common_shape_fns.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/util/work_sharder.h\"\r\nCMakeLists.txt:\r\ncmake_minimum_required(VERSION 2.8)\r\nproject(icp_op)\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\r\nadd_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\r\nset(PCL_DIR /home/XXX/software/pcl181/share/pcl-1.8)\r\nfind_package(PCL REQUIRED)\r\ninclude_directories(${PCL_INCLUDE_DIRS})\r\ninclude_directories(\r\n\t/home/XXX/software/eigen/include/eigen3\r\n\t/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/eigen_archive\r\n\t/usr/local/lib/python2.7/dist-packages/tensorflow/include\r\n\t/usr/local/lib/python2.7/dist-packages/tensorflow/include/external/nsync/public\r\n)\r\nlink_directories(\r\n\t/usr/local/lib/python2.7/dist-packages/tensorflow\r\n)\r\nadd_library(${PROJECT_NAME} SHARED\r\n\ticp_op_kernel.cpp\r\n)\r\ntarget_link_libraries(${PROJECT_NAME}\r\n\t${PCL_LIBRARIES}\r\n\ttensorflow_framework\r\n)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 17 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21196, "title": "MobileNet v2 slower than v1 when loading from Frozen GraphDef", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.3 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nDoesn't apply\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**:\r\nPython3\r\n- **Bazel version (if compiling from source)**:\r\nDoesn't apply\r\n- **GCC/Compiler version (if compiling from source)**:\r\nDoesn't apply\r\n- **CUDA/cuDNN version**:\r\nCUDA: 9.0 ; cuDNN: 7.1.4\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n### Summary\r\nMobileNet v2 is faster than v1 only when loading from the checkpoint format i.e, Variable Ops (meta, index, data) whereas it is slower than v1 when running in the frozen graph format (Const Ops) (.pb)\r\n\r\n### Description\r\nI have two TensorFlow trained models that are in the checkpoint format (meta, index, data) namely mobilenetv1_0.75.ckpt and mobilenetv2_0.75_6.ckpt. \r\n\r\nThe model definitions are as described in the [MobileNetv1](https://arxiv.org/abs/1704.04861) and [MobileNetv2](https://arxiv.org/pdf/1801.04381.pdf) papers. Both models are trained with a width_multiplier of 0.75. MobileNetv2 has an expansion factor of 6. This means that MAC wise, v2 is better than v1 (v1: 26.5 Mil, v2: 20.6Mil) and is expected to be slightly faster than v1.\r\n\r\nTo compare how the models actually perform, I evaluated them in two ways\r\nMethod 1: After Training - Inference by loading the models from checkpoints\r\nMethod 2: During Deployment - Inference by loading the models from frozen GraphDefs ([freeze_graph](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) for converting Variables to Consts)\r\n\r\n**Tools Used for Testing:**\r\n- TF's Timeline Trace Tool\r\n- benchmark_model Tool\r\n- Naive Python time module\r\n\r\n### Timeline Tool\r\nI used the TensorFlow's [timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) tool to view the execution times in the Chrome Trace format. \r\n\r\n**Method 1:** \r\nMobileNetv1 Trace\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350531-b0ad3f8e-9225-11e8-95da-d21ac7cdc1e6.png\">\r\n\r\nMobileNetv2 Trace\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350534-bc7a644a-9225-11e8-8032-ec10bddf52a9.png\">\r\n\r\nEyeballing it, V2 is only slightly faster than V1; measuring from Conv1 to SoftMax,\r\n\r\nv1: 9.73 ms\r\nv2: 8.153 ms\r\n\r\n**Method 2:**\r\n\r\nMobileNetv1 Trace\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350556-ea35af48-9225-11e8-8f2d-56ccd7e600d3.png\">\r\n\r\nMobileNet v2 Trace\r\n\r\n<img width=\"1437\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7610546/43350562-f60ebda0-9225-11e8-992d-5c7ae66a5eb4.png\">\r\n\r\nAs you can clearly see, when the models are loaded from a frozen format, v2 is slower than v1. This affects the performance timings for v2 especially while deployment since models are usually exported in the frozen format.\r\n\r\n\r\n### Time Measurements with Python's time module\r\n\r\nApart from the time traces above, I calculated the FPS by measuring the time between session.run. Even though the frozen models are faster than the checkpoint format, v2 is slower than v1 in the frozen format. Why is that so?\r\n\r\nModel | Checkpoint - FPS | Frozen (FPS)\r\n-- | -- | --\r\nMobileNetv1 | 97.6 | 254.86\r\nMobileNetv2 | 159.22 | 185.04\r\n\r\n\r\n### Benchmark Model Tool Results\r\n\r\n\r\n**MobileNet_v1_75**\r\n\r\nCommand Used:\r\n`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V1_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true`\r\n\r\n[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]\r\n-- | -- | -- | -- | -- | -- | --\r\nConv2D | 15 | 1.044 | 21.20% | 21.20% | 8658.432 | 15\r\ngpu:Conv2D | 15 | 0.571 | 11.60% | 32.80% | 0 | 35\r\nBiasAdd | 28 | 0.557 | 11.31% | 44.11% | 0 | 28\r\nAdd | 27 | 0.493 | 10.01% | 54.12% | 0 | 27\r\nMul | 27 | 0.48 | 9.75% | 63.87% | 0 | 27\r\nRelu6 | 27 | 0.414 | 8.41% | 72.28% | 0 | 27\r\nDepthwiseConv2dNative | 13 | 0.392 | 7.96% | 80.24% | 540.672 | 13\r\nConst | 113 | 0.37 | 7.51% | 87.75% | 0 | 113\r\ngpu:Mul | 27 | 0.082 | 1.67% | 89.42% | 0 | 27\r\ngpu:Add | 27 | 0.081 | 1.65% | 91.06% | 0 | 27\r\ngpu:DepthwiseConv2dNative | 13 | 0.061 | 1.24% | 92.30% | 0 | 13\r\nNoOp | 1 | 0.061 | 1.24% | 93.54% | 0 | 2\r\ngpu:BiasAdd | 28 | 0.059 | 1.20% | 94.74% | 0 | 28\r\nTranspose | 2 | 0.055 | 1.12% | 95.86% | 49.152 | 2\r\ngpu:Relu6 | 27 | 0.054 | 1.10% | 96.95% | 0 | 27\r\ngpu:MEMCPYHtoD | 1 | 0.043 | 0.87% | 97.83% | 0 | 1\r\nSoftmax | 1 | 0.04 | 0.81% | 98.64% | 0.512 | 1\r\nAvgPool | 1 | 0.033 | 0.67% | 99.31% | 3.072 | 1\r\ngpu:Softmax | 1 | 0.009 | 0.18% | 99.49% | 0 | 3\r\ngpu:AvgPool | 1 | 0.006 | 0.12% | 99.61% | 0 | 1\r\n_Arg | 1 | 0.006 | 0.12% | 99.74% | 0 | 1\r\ngpu:Transpose | 1 | 0.005 | 0.10% | 99.84% | 0 | 1\r\n_Retval | 1 | 0.004 | 0.08% | 99.92% | 0 | 1\r\nReshape | 1 | 0.003 | 0.06% | 99.98% | 0 | 1\r\ngpu:MEMCPYDtoH | 1 | 0.001 | 0.02% | 100.00% | 0 | 1\r\nTotal | \u00a0 | 4.924 | \u00a0 | \u00a0 | \u00a0\r\n\r\n**MobileNet_v2_75**\r\n\r\nCommand Used:\r\n`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=MobileNet_V2_75.pb --input_layer=\"input/Placeholder\" --input_layer_shape=\"1,64,64,3\" --input_layer_type=\"float\" --output_layer=\"output/Softmax_1\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true`\r\n\r\n[Node type] | [count] | [avg ms] | [avg %] | [cdf %] | [mem KB] | [times called]\r\n-- | -- | -- | -- | -- | -- | --\r\nConv2D | 39 | 2.967 | 37.97% | 37.97% | 7955.2 | 39\r\nAdd | 53 | 1.217 | 15.58% | 53.55% | 0 | 53\r\ngpu:Conv2D | 39 | 1.036 | 13.26% | 66.80% | 0 | 93\r\nRelu6 | 36 | 0.554 | 7.09% | 73.89% | 0 | 36\r\nDepthwiseConv2dNative | 17 | 0.408 | 5.22% | 79.11% | 959.232 | 17\r\nConst | 129 | 0.403 | 5.16% | 84.27% | 0 | 129\r\nMul | 17 | 0.304 | 3.89% | 88.16% | 0 | 17\r\nAddN | 12 | 0.264 | 3.38% | 91.54% | 0 | 12\r\ngpu:Add | 53 | 0.161 | 2.06% | 93.60% | 0 | 53\r\nNoOp | 1 | 0.076 | 0.97% | 94.57% | 0 | 2\r\ngpu:Relu6 | 36 | 0.073 | 0.93% | 95.51% | 0 | 36\r\ngpu:DepthwiseConv2dNative | 17 | 0.07 | 0.90% | 96.40% | 0 | 17\r\nTranspose | 2 | 0.053 | 0.68% | 97.08% | 49.152 | 2\r\ngpu:Mul | 17 | 0.052 | 0.67% | 97.75% | 0 | 17\r\ngpu:MEMCPYHtoD | 1 | 0.044 | 0.56% | 98.31% | 0 | 1\r\nSoftmax | 1 | 0.041 | 0.53% | 98.84% | 0.512 | 1\r\nAvgPool | 1 | 0.031 | 0.40% | 99.23% | 3.84 | 1\r\ngpu:AddN | 12 | 0.025 | 0.32% | 99.55% | 0 | 12\r\ngpu:Softmax | 1 | 0.009 | 0.12% | 99.67% | 0 | 3\r\ngpu:AvgPool | 1 | 0.007 | 0.09% | 99.76% | 0 | 1\r\n_Arg | 1 | 0.006 | 0.08% | 99.83% | 0 | 1\r\ngpu:Transpose | 1 | 0.005 | 0.06% | 99.90% | 0 | 1\r\n_Retval | 1 | 0.004 | 0.05% | 99.95% | 0 | 1\r\nReshape | 1 | 0.003 | 0.04% | 99.99% | 0 | 1\r\ngpu:MEMCPYDtoH | 1 | 0.001 | 0.01% | 100.00% | 0 | 1\r\nTotal | \u00a0 | 7.814 | \u00a0 | \u00a0 | \u00a0\r\n\r\n\r\n**Avg Time\r\nv1: 4.924 ms\r\nv2: 7.814 ms**\r\n\r\n### Source code / logs\r\n\r\nHow I load the model with method 1:\r\n\r\n```\r\n    def __load_model(self):\r\n        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)\r\n        if latest_checkpoint:\r\n            print(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\r\n            self.saver.restore(self.sess, latest_checkpoint)\r\n            print(\"Checkpoint loaded\\n\\n\")\r\n        else:\r\n            print(\"No checkpoints available!\\n\\n\")\r\n```\r\n\r\nHow I load the model with method 2:\r\n\r\n```\r\ndef create_graph(checkpoint_path):\r\n   with tf.gfile.FastGFile(checkpoint_path, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        _ = tf.import_graph_def(graph_def, name='')\r\n```\r\n\r\nI'm interested to know the details on why v2's performance is only half as good as v1 when in the paper it is discussed that v2 is supposed to be 35% faster.\r\n\r\nNote:\r\nI've cross checked this with other hardware such as P40, an i5 7th Gen CPU and an alternative TF version (1.5.0) and this pattern is the same.", "comments": ["@derekjchow Can you take a look at this?", "This may help, from https://arxiv.org/abs/1903.08469v1 :\r\n\r\n\"However, MobileNet V2 uses **depthwise separable convolutions which are not directly supported in GPU firmware (the cuDNN library)**. Therefore, MobileNet V2 tends to be slower than ResNet18 in most experimental setups. Note that the same issue disqualifies usage of the DenseNet architecture [12], since it requires efficient convolution over a non-contiguous tensor, which is still not supported in cuDNN.\"", "@suryaprakaz,\r\nSorry for the delayed response. You can consider using [MobileNet_V3](https://arxiv.org/pdf/1905.02244.pdf) which is the most recent and efficient. Please refer the [documentation of MobileNet](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md) for more information. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 21195, "title": "updating tf.Variable call in sample to execute with eager", "body": "the current colab code errors out when it calls tf.Variable:\r\n\r\nRuntimeError: tf.Variable not supported when eager execution is enabled. Please use tf.contrib.eager.Variable instead\r\n\r\nNot sure why this is needed -- I thought eager would allow this syntax to work as is, but the error was pretty clear.", "comments": ["The colab in the `master` branch is in sync with the code in the `master` branch (where `tf.Variable` is supported). However, the last release (1.9, 1.10-rc0) do not yet support it.\r\n\r\nThis seems like a bug on on website, where the www.tensorflow.org/tutorials should link to the colabs in the appropriate release branch, not the ones in master.\r\n\r\n@MarkDaoust @wolffg @lamberta - Could you help with that?\r\n\r\n@yufengg : Apologies for the confusion. For now, could you replace `master` with say `r1.9` to get to the right colab. i.e.,\r\n\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/eager/python/examples/notebooks/eager_basics.ipynb\r\n\r\ninstead of\r\n\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/notebooks/eager_basics.ipynb", "(Oh, and we shouldn't change the notebook in the master branch)"]}, {"number": 21194, "title": "errors after installing pydot-ng  ImportError: DLL load failed: The specified module could not be found. and SyntaxError: invalid syntax", "body": "I followed the instructions given in the link. \r\nhttp://www.codesofinterest.com/2017/02/visualizing-model-structures-in-keras.html\r\n\r\nthen I did:\r\nconda install pydot-ng \r\n\r\nThen i installed graphviz from here http://www.graphviz.org/download/ and added C:\\Program Files (x86)\\Graphviz2.38\\bin to PATH\r\n\r\n  after that it gives me errors if i made any \" conda\" instruction in command window or anaconda prompt\r\nand also give the same error when writing \"  activate tensorflow  \" in command window\r\n\r\n\r\n\r\nError:\r\n\r\n(base) C:\\Users\\BioHelwan>activate tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\cli\\main.py\", line 98, in main\r\n    return activator_main()\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\activate.py\", line 632, in main\r\n    print(activator.execute(), end='')\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\activate.py\", line 166, in execute\r\n    return getattr(self, self.command)()\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\activate.py\", line 152, in activate\r\n    return self._finalize(self._yield_commands(self.build_activate(self.env_name_or_prefix)),\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\activate.py\", line 231, in build_activate\r\n    prefix = locate_prefix_by_name(env_name_or_prefix)\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\base\\context.py\", line 951, in locate_prefix_by_name\r\n    envs_dirs = context.envs_dirs\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\base\\context.py\", line 381, in envs_dirs\r\n    join(self._user_data_dir, 'envs'),\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\base\\context.py\", line 416, in _user_data_dir\r\n    return user_data_dir(APP_NAME, APP_NAME)\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\_vendor\\appdirs.py\", line 67, in user_data_dir\r\n    path = os.path.join(_get_win_folder(const), appauthor, appname)\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\_vendor\\appdirs.py\", line 284, in _get_win_folder_with_pywin32\r\n    from win32com.shell import shellcon, shell\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\Scripts\\conda-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\cli\\main.py\", line 108, in main\r\n    init_loggers()\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\cli\\main.py\", line 55, in init_loggers\r\n    from ..gateways.logging import initialize_logging, set_verbosity\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\gateways\\logging.py\", line 12, in <module>\r\n    from ..common.io import attach_stderr_handler\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\lib\\site-packages\\conda\\common\\io.py\", line 5, in <module>\r\n    from concurrent.futures import ThreadPoolExecutor, _base, as_completed\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\concurrent\\futures\\__init__.py\", line 8, in <module>\r\n    from concurrent.futures._base import (FIRST_COMPLETED,\r\n  File \"C:\\Users\\BioHelwan\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\concurrent\\futures\\_base.py\", line 414\r\n    raise exception_type, self._exception, self._traceback\r\n                        ^\r\nSyntaxError: invalid syntax\r\n\r\n\r\n\r\nNote: any instruction related to anaconda gives me an  error also i can't remove anaconda to start setup from beginning to get rid of from this problem\r\n\r\ni'm using \r\nwindows10 (64)  - Anaconda 4.2 and python 3.5.2\r\ni installed tensor flow gpu version from instructions in this link https://www.tensorflow.org/install/install_windows \r\ntensorflow version=1.9.0\r\ncuda versions installed 8.0 /9.0\r\nGPU : NVIDIA GeForce GTX 960M   specs link: https://www.geforce.com/hardware/notebook-gpus/geforce-gtx-960m/specifications\r\n\r\nExact command to reproduce : activate tensorflow (also any code related to anaconda i can't use \"conda\" or \"pip\" instructions in my command prompt because they give errors too)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "i solved the problem by uninstall anaconda then reinstall the anaconda and tensor flow and keras\r\n--------------------\r\nfor visualization keras models\r\nthis link solved my problem \r\nhttps://www.experts-exchange.com/questions/29106033/Pydot-having-problems-with-GraphViz.html", "if you use Anaconda, you can use  `conda install -c hesi_m tensorflow ` to install **tensorflow 1.9.0** for CPU \r\n\r\nif you specifically use Keras also, you can use  `conda install -c hesi_m keras` which installs **Keras 2.2.2** in accompany with everything including tensorflow 1.9.0\r\n\r\nIt is better to make a fresh anaconda environment for your experiments"]}, {"number": 21193, "title": "please how i can convert string input to numbers", "body": "hi ..\r\nplease give me a wey to convert a string input like user information ( contry, name ...) to use it in the functions\r\nthank you :)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21192, "title": "Unimplemented cast int64 to string is not supported", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04.4\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5.2\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: described below\r\n- Mobile device: N/A\r\n\r\n### Describe the problem\r\nAfter following the steps in Using SavedModel with Estimators: When sending a classification request from client, the server is not able to cast it into serialized tf example that model is expecting.\r\n\r\nThe same error comes up even when I used ```\r\ntf.contrib.predictor.from_saved_model```\r\n\r\nSo far, I have checked StackOverflow, Tf documentation, issues on Tf and Tf/serving. No one has ever encountered this error. Hence reporting here. This [issue {#1017}](https://github.com/tensorflow/serving/issues/1017) was rejected by @chrisolston in Tf/serving because the error trace indicates it is an issue in one of the core modules on main Tf repo and it had looked like there was no serialization of tf example proto in the client side code: But that task is already accomplished by [prediction_service_pb2.py](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service_pb2.py) file. \r\n\r\nThere are no other pointers online to what is going wrong here, please help!\r\n\r\n### Source code / error logs\r\nTensorFlow model server **error** trace:\r\n```\r\n2018-07-27 14:54:49.685755: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at cast_op.cc:77 : Unimplemented: Cast int64 to string is not supported\r\n2018-07-27 14:54:49.685822: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\r\n```\r\n\r\n**Error** on running client side code:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 193, in _blocking_unary_unary\r\n    credentials=_credentials(protocol_options))\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 500, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 434, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Cast int64 to string is notsupported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]])>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 63, in <module>\r\n    run(args.host, args.port, args.input, args.model, args.signature_name)\r\n  File \"/home/mldev/intelligent_sp/train_scripts/client.py\", line 42, in run\r\n    result = stub.Classify(request, 10.0)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 309, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 195, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.LocalError: LocalError(code=StatusCode.UNIMPLEMENTED, details=\"Cast int64 to string isnot supported\r\n         [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[2]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\")\r\n```\r\n\r\n**Relevant Client Code**:\r\n```\r\n    channel = implementations.insecure_channel(host, port)\r\n    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n    # Pre-processing\r\n    prediction_input = [json.dumps(eval(input_str))]\r\n    \r\n    ink, classname = creat.parse_line(prediction_input[0])\r\n\r\n    classnames = ['doodle', 'expression', 'symbols']\r\n    features = {}\r\n    features[\"class_index\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[classnames.index(\"doodle\")]))\r\n    features[\"ink\"] = tf.train.Feature(float_list=tf.train.FloatList(value=ink.flatten()))\r\n    features[\"shape\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=ink.shape))\r\n    f = tf.train.Features(feature=features)\r\n    example = tf.train.Example(features=f)\r\n    final_req = [example]\r\n    start = time.time()\r\n\r\n    # Call classification model to make prediction\r\n    request = classification_pb2.ClassificationRequest()\r\n    request.model_spec.name = model\r\n    request.model_spec.signature_name = signature_name\r\n    request.input.example_list.examples.extend(final_req)\r\n    \r\n    result = stub.Classify(request, 10.0)\r\n```\r\n\r\nImports used in Client Code:\r\n```\r\nfrom tensorflow_serving.apis import classification_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\nfrom tensorflow_serving.apis import input_pb2 as final_inp\r\n```\r\n\r\nCode used to **export saved model** (runs successfully):\r\n```\r\n      feature_spec = {\r\n      \"ink\": tf.VarLenFeature(dtype=tf.float32),\r\n      \"shape\": tf.FixedLenFeature([2], dtype=tf.int64),\r\n      \"class_index\": tf.FixedLenFeature([1], dtype=tf.int64)\r\n      }\r\n      \r\n      # defining serving input receiver function\r\n      def serving_input_receiver_fn():\r\n        \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\r\n        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_example_tensor')\r\n        receiver_tensors = {'examples': serialized_tf_example}\r\n        parsed_features = tf.parse_example(serialized_tf_example, feature_spec)\r\n        parsed_features[\"ink\"] = tf.sparse_tensor_to_dense(parsed_features[\"ink\"])\r\n        return tf.estimator.export.ServingInputReceiver(parsed_features, receiver_tensors)\r\n\r\n      # export saved model\r\n      estimator.export_savedmodel(FLAGS.model_dir+\"/serve/\", serving_input_receiver_fn, strip_default_attrs=True)\r\n      print(\"done exporting\")\r\n```\r\n\r\nThe output from **saved_model_cli** (used to inspect saved model) looks like this:\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['output']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['inputs'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1)\r\n        name: input_example_tensor:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['classes'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (2)\r\n        name: Cast:0\r\n    outputs['scores'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (2, 348)\r\n        name: dense/BiasAdd:0\r\n  Method name is: tensorflow/serving/classify\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['inputs'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1)\r\n        name: input_example_tensor:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['classes'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (2)\r\n        name: Cast:0\r\n    outputs['scores'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (2, 348)\r\n        name: dense/BiasAdd:0\r\n  Method name is: tensorflow/serving/classify\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "cc/ @sukritiramesh @nfiedel ", "Hi @aroraakshit -- I imagine it's been frustrating to have to go through so many places to find an answer, but hopefully we can figure out what's happening here. \r\n\r\nIt would help to make a simpler example, to extract the possibility that it's something inside the model that's raising an error. Can you create a minimal example that starts with a tf.example holding an int, and a SavedModel from a model_fn that just accepts that int and doesn't actually do any learning? That will isolate whether this is an issue inside the model, or in the communication between the client and the server.", "Hi @karmel, thank you for looking into this. \r\n\r\nSince the way I am exporting my SavedModel is [using Estimator API](https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators), to isolate the error I am facing, I would simply extend the TensorFlow's [RNN for Drawing Classification](https://www.tensorflow.org/tutorials/sequences/recurrent_quickdraw) tutorial by adding SavedModel functionality to it. I have put the code and documented the steps to reproduce the error in a small repo [here](https://github.com/aroraakshit/cast_issue_poc_). \r\n\r\nThanks!", "Great, thanks, that helps to extract the model. Does that produce precisely the same error? My guess is the cited node is different? Can you provide the trace?", "To my knowledge, it is precisely the same error. I visualized the graph on TensorBoard, it is the same node that's giving the error, you can find the graph [here](https://github.com/aroraakshit/cast_issue_poc_/blob/master/TensorBoard_vis.png). (You may want to download it for checking it properly)\r\n\r\nHere is the server side log upon a single request:\r\n```\r\n2018-07-31 17:50:15.878264: I tensorflow_serving/model_servers/main.cc:323] Running ModelServer at 0.0.0.0:9000 ...\r\n2018-07-31 17:55:12.725076: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at cast_op.cc:77 : Unimplemented: Cast int64 to string is not supported\r\n2018-07-31 17:55:12.725149: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Unimplemented: Cast int64 to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[8]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\r\n```\r\n\r\nHere is the client side error trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 193, in _blocking_unary_unary\r\n    credentials=_credentials(protocol_options))\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 500, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/_channel.py\", line 434, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNIMPLEMENTED, Cast int64 to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[8]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]])>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/storage/test_env/cast_issue_poc_/client.py\", line 59, in <module>\r\n    run(args.host, args.port, args.input, args.model, args.signature_name)\r\n  File \"/storage/test_env/cast_issue_poc_/client.py\", line 41, in run\r\n    result = stub.Classify(request, 10.0)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 309, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/home/mldev/venv/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 195, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.LocalError: LocalError(code=StatusCode.UNIMPLEMENTED, details=\"Cast int64 to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_INT64, _output_shapes=[[8]], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ArgMax)]]\")\r\n```", "Ah, sorry, perhaps I was unclear above-- I meant can you try with a _different_ model. If you were originally trying with the RNN tutorial, can you try with a matmul or MNIST or something as simple as possible? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 21191, "title": "Use correct hash_bucket_size parameter", "body": "`s/hash_buckets_size/hash_bucket_size/` since that is the correct argument spelling for the Python method.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "thanks for the clean up!"]}, {"number": 21190, "title": "[Bug] tf.map_fn cannot work with tf.histogram_fixed_width when using float", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: Quardo M4000\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI have tested the tf.map_fn to produce a list of histograms for a tensor. My example codes are as below.\r\nMy findings: \r\n\r\n1. When running tf.histogram_fixed_width for single floating tensor, there is no problem. (see test0() below)\r\n2. When running tf.map_fn together with tf.histogram_fixed_width for integer tensor, there is no problem (see test2() below)\r\n3. When running tf.map_fn together with tf.histogram_fixed_width for floating tensor, the program crashed (see test1() below)\r\n\r\n### Source code / logs\r\n\timport tensorflow as tf\r\n\timport numpy as np\r\n\tdef test0():\r\n\t  V = tf.placeholder(tf.float32, [None, 3])\r\n\t  hists = tf.histogram_fixed_width(V[0], [0.0,10.0], 10)\r\n\t  A = [[1.0,2.0,3.0], [4.0,5.0,6.0]]\r\n\t  with tf.Session() as sess:\r\n\t\tprint(sess.run(hists, {V:A}))\r\n\t\t\r\n\tdef test1():\r\n\t  V = tf.placeholder(tf.float32, [None, 3])\r\n\t  hists = tf.map_fn(\r\n\t\tlambda x: tf.histogram_fixed_width(x, [0.0,10.0], 10), V)\r\n\t  A = [[1.0,2.0,3.0], [4.0,5.0,6.0]]\r\n\t  with tf.Session() as sess:\r\n\t\tprint(sess.run(hists, {V:A}))\r\n\r\n\tdef test2():\r\n\t  V = tf.placeholder(tf.int32, [None, 3])\r\n\t  hists = hists = tf.map_fn(\r\n\t\tlambda x: tf.histogram_fixed_width(x, [0,10], 10), V)\r\n\t  A = [[1,2,3], [4,5,6]]\r\n\t  with tf.Session() as sess:\r\n\t\tprint(sess.run(hists, {V:A}))\r\n\r\n\r\ntest0() and test2() work fine. test1() produce the following error. Therefore, I think this may be a bug in tensorflow.\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n\t\treturn fn(*args)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1307, in _run_fn\r\n\t\toptions, feed_dict, fetch_list, target_list, run_metadata)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1409, in _call_tf_sessionrun\r\n\t\trun_metadata)\r\n\ttensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray dtype is float but Op is trying to write dtype int32.\r\n\t\t\t [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](map/while/TensorArrayWrite/TensorArrayWriteV3/Enter, map/while/Switch_1/_31, map/while/histogram_fixed_width, map/while/Switch_2/_33)]]\r\n\t\t\t [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3/_35 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_66_map/while/TensorArrayWrite/TensorArrayWriteV3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_cloopmap/while/NextIteration_2/_6)]]\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"train_model.py\", line 196, in <module>\r\n\t\tmain()\r\n\t  File \"train_model.py\", line 193, in main\r\n\t\ttest1()\r\n\t  File \"train_model.py\", line 166, in test1\r\n\t\tprint(sess.run(hists, {V:A}))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n\t\trun_metadata_ptr)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1135, in _run\r\n\t\tfeed_dict_tensor, options, run_metadata)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1316, in _do_run\r\n\t\trun_metadata)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1335, in _do_call\r\n\t\traise type(e)(node_def, op, message)\r\n\ttensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray dtype is float but Op is trying to write dtype int32.\r\n\t\t\t [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](map/while/TensorArrayWrite/TensorArrayWriteV3/Enter, map/while/Switch_1/_31, map/while/histogram_fixed_width, map/while/Switch_2/_33)]]\r\n\t\t\t [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3/_35 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_66_map/while/TensorArrayWrite/TensorArrayWriteV3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_cloopmap/while/NextIteration_2/_6)]]\r\n\r\n\tCaused by op 'map/while/TensorArrayWrite/TensorArrayWriteV3', defined at:\r\n\t  File \"train_model.py\", line 196, in <module>\r\n\t\tmain()\r\n\t  File \"train_model.py\", line 193, in main\r\n\t\ttest1()\r\n\t  File \"train_model.py\", line 163, in test1\r\n\t\tlambda x: tf.histogram_fixed_width(x, [0.0,10.0], 10), V)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 459, in map_fn\r\n\t\tmaximum_iterations=n)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3209, in while_loop\r\n\t\tresult = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2941, in BuildLoop\r\n\t\tpred, body, original_loop_vars, loop_vars, shape_invariants)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2878, in _BuildLoop\r\n\t\tbody_result = body(*packed_vars_for_body)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3179, in <lambda>\r\n\t\tbody = lambda i, lv: (i + 1, orig_body(*lv))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 451, in compute\r\n\t\ttas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 451, in <listcomp>\r\n\t\ttas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 118, in wrapped\r\n\t\treturn _add_should_use_warning(fn(*args, **kwargs))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 842, in write\r\n\t\treturn self._implementation.write(index, value, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 118, in wrapped\r\n\t\treturn _add_should_use_warning(fn(*args, **kwargs))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 276, in write\r\n\t\tname=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_data_flow_ops.py\", line 7870, in tensor_array_write_v3\r\n\t\tflow_in=flow_in, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n\t\top_def=op_def)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\r\n\t\top_def=op_def)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\r\n\t\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\tInvalidArgumentError (see above for traceback): TensorArray dtype is float but Op is trying to write dtype int32.\r\n\t\t\t [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](map/while/TensorArrayWrite/TensorArrayWriteV3/Enter, map/while/Switch_1/_31, map/while/histogram_fixed_width, map/while/Switch_2/_33)]]\r\n\t\t\t [[Node: map/while/TensorArrayWrite/TensorArrayWriteV3/_35 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_66_map/while/TensorArrayWrite/TensorArrayWriteV3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_cloopmap/while/NextIteration_2/_6)]]\r\n\r\n", "comments": ["@ybsave In the docstring `tf.map_fn`:\r\n```\r\ndtype: (optional) The output type(s) of fn. If fn returns a structure of Tensors differing\r\nfrom the structure of elems, then dtype is not optional and must have the same\r\nstructure as the output of fn.\r\n```\r\n\r\nAs the return type of `tf.histogram_fixed_width` is `tf.int32` (differs from `V`), I think the issue could be resolved with:\r\n```\r\n  V = tf.placeholder(tf.float32, [None, 3])\r\n  hists = tf.map_fn(\r\n\tlambda x: tf.histogram_fixed_width(x, [0.0,10.0], 10), V, tf.int32)\r\n  A = [[1.0,2.0,3.0], [4.0,5.0,6.0]]\r\n  with tf.Session() as sess:\r\n\tprint(sess.run(hists, {V:A}))\r\n```", "@yongtang Thank you for your help. It works now. "]}, {"number": 21189, "title": "How to know the right values of X and Y for strip_unused_nodes(type=X, shape=\"y0,y1,y3,3\") to use in the transofrm_graph tool?", "body": "", "comments": ["Posted here https://stackoverflow.com/questions/51562086/right-parameters-for-strip-unused-nodes"]}, {"number": 21188, "title": "TypeError: Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor.", "body": "Error:\r\n/usr/lib64/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-07-27 08:35:51.967246: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\r\n    subfeed, allow_tensor=True, allow_operation=False)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3590, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3679, in _as_graph_element_locked\r\n    types_str))\r\nTypeError: Can not convert a NoneType into a Tensor.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"--------------.py\", line 397, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"---------------------.py\", line 88, in main\r\n    print(prediction(e))\r\n  File \"---------------------.py\", line 361, in prediction\r\n    model_prediction = sess.run(pred_label, feed_dict={x: subject_npy, y: y_const_npy, keep_prob: 1.0})\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1078, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Can not convert a NoneType into a Tensor.\r\n\r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nI've tried what some stackoverflow posts recommended, e.g. keeping the feed_dict placeholder names as x and y, when either saving & restoring the graph or just making new placeholders.\r\n\r\nMy objective: I'm trying to restore and run an lstm.\r\n\r\nMy code:\r\n\"\"\"setup for training-------(no issue with training)\"\"\"\r\n  out_weights = tf.Variable(tf.random_normal([num_units,n_classes]), name = \"out_weights\")\r\n  out_bias = tf.Variable(tf.random_normal([n_classes]), name = \"out_bias\")\r\n  x = tf.placeholder(\"float\",[None,time_steps,n_input])\r\n  y = tf.placeholder(\"float\",[None,n_classes])\r\n  keep_prob = tf.placeholder(\"float\", shape=())\r\ninput = tf.unstack(x ,time_steps,1, name = \"inputs\")\r\nlstm_layer = rnn.BasicLSTMCell(num_units,forget_bias=1)\r\noutputs, _ = rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\r\n  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\r\n  tf.summary.scalar('cross_entrophy', loss)\r\n  opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n  pred_label = tf.argmax(prediction,1)\r\n  actual_label = tf.argmax(y,1)\r\n  correct_prediction = tf.equal(pred_label, actual_label)\r\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver = tf.train.Saver()\r\n    for i in range(N):\r\n        sess.run(, feed_dict = {x: batch[0], y: batch[1], keep_prob: 0.5})\r\n        if i%100==0:\r\n                    saver.save(sess, \"path/model.ckpt\")\r\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\"\"\"LOADING  lstm\"\"\"\r\n  with tf.Session() as sess:\r\n    from_saver = tf.train.import_meta_graph('path/model.ckpt.meta')\r\n    from_saver.restore(sess, tf.train.latest_checkpoint('path_to_summaries_dir'))\r\n   lstm_graph = tf.get_default_graph()\r\n    out_weights = lstm_graph.get_tensor_by_name(\"out_weights:0\")\r\n    out_bias = lstm_graph.get_tensor_by_name(\"out_bias:0\")\r\n    x = tf.placeholder(\"float\",[time_steps,n_input])\r\n    y = tf.placeholder(\"float\",[n_classes])\r\n    keep_prob = keep_prob = tf.placeholder(\"float\", shape=())\r\n    inputs = lstm_graph.get_tensor_by_name(\"inputs:0\")\r\n    prediction = lstm_graph.get_tensor_by_name(\"prediction:0\") + out_bias\r\n    pred_label = tf.argmax(prediction,1)\r\n\r\n\"\"\"RUNNING MODEL\"\"\"\r\n  with tf.Session(graph=lstm_graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    model_prediction = sess.run(pred_label, feed_dict={x: subject_npy, y: y_const_npy, keep_prob: 1.0})\r\n\r\n------------------------\r\n\r\n\r\nHave I written custom code:   yes\r\nOS Platform and Distribution:    centos 6\r\nTensorFlow installed from:    pip3\r\nTensorFlow version:    v1.9\r\nBazel version:    N/A\r\nCUDA/cuDNN version:    N/A\r\nGPU model and memory:    N/A\r\nExact command to reproduce:    N/A\r\nMobile device:    N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device"]}, {"number": 21187, "title": "Added a normalization term to ctc_beam_search_decoder", "body": "Added a normalization term to ctc_beam_search_decoder in order to get correct log probabilities.\r\nIt solves https://github.com/tensorflow/tensorflow/issues/6034\r\n\r\nThe decoder is supposed to get logits as inputs instead of log probabilities, so it should compute log(softmax(logits)) inside.\r\nThe original code was relied on an equation: log(exp(x)) = x. But it didn't include a softmax normalization term: sum(exp(x_j)).", "comments": ["This PR looks right; and may have fixed a bug in how we handle uncentered logits.  We're discussing it and will have more to say in a little bit.", "@ebrevdo Sure, I switched to Eigen and updated ground truth values in Keras CTC beam search test.", "This is perfect, thanks!"]}, {"number": 21186, "title": "Keras TBCallback: Make batch-level logging configurable", "body": "4921064dd535d84aa031f8116e583b151dd46e97 introduced batch-level metrics logging.\r\n\r\nThis can lead to performance problems in certain situations, especially with fast training models. This will result in the following warning to be triggered:\r\n```python-traceback\r\nWARNING:tensorflow:Method on_batch_end() is slow compared to the batch update. Check your callbacks.\r\n```\r\n\r\nThis PR makes this behavior configurable by introducting the `batch_level_metrics` config option to allow users to restore the old behavior.\r\n\r\n@fchollet Would you prefer adding a more fine grained option that makes the frequency of logging batch-level metrics during training configurable, similar to `histogram_freq`?", "comments": ["@lgeiger Does it make sense to add a unit test for this?", "At the current state I think almost everything is covered by the existing unit tests. Though if we decide to add a more elaborate config option (i.e. configure the logging frequency) I'm happy to add more unit tests.", "@fchollet Thanks for the review. I added a frequency argument so the logging frequency can be configured as well as unit tests.\r\n ", "@fchollet, @caisq Friendly ping \ud83d\ude07. Any change this can be reviewed?", "Nagging Assignee @caisq: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> Nagging Assignee @caisq: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\r\n\r\nIn the meantime this behavior was introduced in c98ffffcb4e0cc668c0ff7b73d51677a7eb7dcf4.\r\n\r\nThis is now my second PR (after #20753) that went obsolete during the time it took to get a review. Are there any discussions (GH issues) going on around improving the management of contributions from outside Google? I'd be happy to join the conversation."]}, {"number": 21184, "title": "ValueError: No op named QuantizedAdd in defined operations.", "body": "python version:2.7.5 tensorflow version:1.2.0 bazel version:\r\n\r\ni want to use tensorflow bazel to quantize my trained model,the quantize code is follow:\r\n\r\n`bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=/home/oup/zzl/zzl/AdversarialJPEG/JPEGFinalXu/Discriminative145000.pb \\\r\n  --out_graph=/home/oup/zzl/zzl/AdversarialJPEG/JPEGFinalXu/quantized8_Discriminative145000.pb \\\r\n  --outputs=Group_22/y_ \\\r\n   --transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,253,253,1\")\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_nodes\r\n  quantize_weights(minimum_size=0)'`\r\n\r\nbut when i try to get my trained model to test it accuracy, i get the error:\r\n\r\n`tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0)\r\n2018-07-27 14:01:15.643854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:05:00.0)\r\n2018-07-27 14:01:15.643863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla P100-PCIE-16GB, pci bus id: 0000:8a:00.0)\r\nTraceback (most recent call last):\r\n  File \"testpb2.py\", line 82, in <module>\r\n    _ = tf.import_graph_def(graph_def,name=\"\")\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 283, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named QuantizedAdd in defined operations.`\r\n\r\nand the code snippet to lead the error:\r\n\r\n`with tf.Session(config=config) as sess:\r\n    #load the quantity model\r\n     with open(path+'quantized64_noDCT_retrained5000_Pruned_Discriminative145000.pb', 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        **_ = tf.import_graph_def(graph_def,name=\"\")**\r\n     y_ = sess.graph.get_tensor_by_name('Group_22/y_:0')\r\n     correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\r\n     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n     init = tf.global_variables_initializer()\r\n     sess.run(init)`\r\n\r\n\r\ni don't konw why i delete the params 'quantize_nodes' in the quantize code it work normal,but add 'quantize_nodes' will raise the error above,is't my tensorflow version is too low lead this error?i have google that error but i can't found any solution,please give me some suggestion for that error,thank you very much!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler thank you for your reply,but i know the reason is that my tensorflow version is too low,i use version 1.7,it's work."]}, {"number": 21183, "title": "make sparsemax nan and infinity safe", "body": "logits that are -inf will be given 0 probability and logits that are\r\ninf will result in a nan output. Likewise if all logits are -inf the\r\noutput will also be nan.\r\n\r\nThis is done by using where operators, mostly because `0 * inf = nan`\r\nand `x/0 = sign(x) inf` following the IEEE 754 standard. However these\r\nresults are not mathematically correct in the context of the sparsemax\r\nalgorithm.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/15564", "comments": ["@rmlarsen Thanks. Please take another look :)", "Nagging Assignee @caisq: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Adding API review label to discuss moving sparsemax into core.", "After consulting the fantastic https://tf-contrib-analyzer.herokuapp.com/, I see no uses of this on GitHub. I don't think we'd want it in core for now. It is definitely a candidate for inclusion in a repo maintained by the (to be formed) SIG-contrib. ", "Can you look at the test failures for this?", "@martinwicke I will go on vacation for a week. I will fix this when I get back.", "@martinwicke I think I fixed the issues. Can you run the CI again?", "@AndreasMadsen Sorry about the delay. merging now."]}, {"number": 21182, "title": "if you install tensorflow v1.9 through wheel\uff0cyou should install tensorboard v1.9.0 but not v1.10.0", "body": "as title i typed, is this a bug of tf1.9?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Could you elaborate on what exactly you're trying and what happened? Please provide a [minimal, verifiable, complete example](https://stackoverflow.com/help/mcve).\r\n\r\nFor example, on a clean install this is what I see:\r\n\r\n```bash\r\n# Validate that neither TensorFlow nor tensorboard are installed yet - the list below should be empty\r\npip list | grep tensor\r\n\r\n# Install tensorflow 1.9.0\r\npip install tensorflow==1.9.0\r\n\r\n# Now grep again and see that the two versions are in sync\r\npip list | grep tensor\r\n```", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21181, "title": "Lambda Layer issue when receiving a list of DeferredTensor with a TensorShape shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**:\r\nPython 3.6.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nSee below.\r\n\r\n### Describe the problem\r\nFollowing [this issue](https://github.com/tensorflow/tensorflow/issues/20338), I used the work-around and wrapped the result of `compute_output_shape` with tf.TensorShape.\r\nHowever when using a Lambda layer with multiple inputs, the shape isn't handled properly and throws the exception:\r\n\r\n`TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'`\r\n\r\n### Source code / logs\r\nHere is a basic set-up to reproduce it:\r\n\r\n    import tensorflow as tf\r\n\r\n    class MyLayer(tf.layers.Layer):\r\n\r\n        def __init__(self, **kwargs):\r\n            super(MyLayer, self).__init__(**kwargs)\r\n\r\n        def call(self, x):\r\n            return x\r\n\r\n        def compute_output_shape(self, input_shape):\r\n            return tf.TensorShape(input_shape[0])\r\n\r\n    tf.enable_eager_execution()\r\n\r\n    a = tf.keras.layers.Input(shape=[1])\r\n    b = tf.keras.layers.Input(shape=[1])\r\n    layer_1 = MyLayer()(a)\r\n    layer_2 = MyLayer()(b)\r\n\r\n    layer_3 = tf.keras.layers.Lambda(lambda x: x[0])([layer_1, layer_2])\r\n\r\n\r\nAnd the stack-trace:\r\n\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Users/jnd/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-C/ch-0/181.5087.37/PyCharm CE.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/jnd/dev/mask_rcnn/bug_replication.py\", line 23, in <module>\r\n    layer_3 = tf.keras.layers.Lambda(lambda x: x[0])([layer_1, layer_2])\r\n  File \"/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 714, in __call__\r\n    output_shapes = self.compute_output_shape(input_shapes)\r\n  File \"/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 677, in compute_output_shape\r\n    input_shape = tuple(tensor_shape.TensorShape(input_shape).as_list())\r\n  File \"/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 541, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 541, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 482, in as_dimension\r\n    return Dimension(value)\r\n  File \"/usr/local/anaconda3/envs/mask_rcnn_cpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 37, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'\r\n\r\n```", "comments": ["Hi @jnd77, this should now work in the nightly build: ```pip install tf-nightly``` (or ```pip install tf-nightly-gpu``` if you have a GPU)", "Hi !\r\nI did run the code above with the nightly version '1.11.0-dev20180808', and I have the same exception.\r\n\r\n", "Hi @jnd77 that version is going to be just a little too old to have the fix, can you please upgrade to ```1.11.0.dev20180813``` and let me know if you still see the error?", "Unfortunately I have a Mac, and the latest version is still 1.11.0-dev20180808. Let me get back to you when a newer Mac version is available ...", "@omalleyt12 Just ran the test. It works. Thanks for your help !", "Great, no prob!"]}, {"number": 21180, "title": "(MirroredStrategy) AttributeError: 'NoneType' object has no attribute 'merge_call'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.1\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Describe the problem\r\nI have several GPUs that I'd like to train a model with. I'm using `tf.estimator.train_and_evaluate` because I'd like to do early stopping (checking loss on a validation set every now and then during training, and stopping the training loop when the model stops improving on the validation data). I'm currently doing this with `tf.contrib.estimator.stop_if_no_decrease_hook`. This crashes when the evaluation starts (because `tf.metrics` cannot run distributed yet?).\r\n\r\nThis is my current workaround for local training:\r\n```python\r\nbest = float('inf')\r\nfor epoch in range(100):\r\n    estimator.train(training_input_fn)\r\n    metrics = estimator.evaluate(validation_input_fn)\r\n    if metrics['loss'] < best:\r\n        best = metrics['loss']\r\n    else:\r\n        tf.logging.info(f'Early stopping after {epoch} epochs.')\r\n        break\r\n```\r\n\r\n### Source code / logs\r\nThis seems to reproduce the error consistently:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef input_fn():\r\n    dataset = (\r\n        tf.data.Dataset\r\n        .range(100)\r\n        .map(lambda x: tf.random_normal([100]))\r\n        .batch(32)\r\n        .map(lambda x: ({'input': x}, {'output': x}))\r\n        .repeat()\r\n    )\r\n    return dataset\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    units = features['input'].shape[-1]\r\n    predictions = tf.layers.dense(features['input'], units)\r\n    labels = labels['output']\r\n\r\n    loss = tf.losses.mean_squared_error(labels, predictions)\r\n    tf.losses.add_loss(loss)\r\n    loss = tf.losses.get_total_loss()\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode,\r\n            loss=loss,\r\n            eval_metric_ops=metrics,\r\n        )\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        step = tf.train.get_or_create_global_step()\r\n        optimizer = tf.train.AdamOptimizer()\r\n        train_op = optimizer.minimize(loss, step)\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode,\r\n            loss=loss,\r\n            train_op=train_op,\r\n        )\r\n\r\n\r\nestimator = tf.estimator.Estimator(\r\n    model_fn,\r\n    config=tf.estimator.RunConfig(\r\n        save_checkpoints_steps=100,\r\n        train_distribute=tf.contrib.distribute.MirroredStrategy()),\r\n)\r\n\r\ntf.estimator.train_and_evaluate(\r\n    estimator,\r\n    train_spec=tf.estimator.TrainSpec(input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn),\r\n)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b27091873f02> in <module>()\r\n     56     estimator,\r\n     57     train_spec=tf.estimator.TrainSpec(input_fn),\r\n---> 58     eval_spec=tf.estimator.EvalSpec(input_fn),\r\n     59 )\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    449         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    450 \r\n--> 451   return executor.run()\r\n    452 \r\n    453 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run(self)\r\n    588         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    589       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 590       return self.run_local()\r\n    591 \r\n    592     # Distributed case.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run_local(self)\r\n    689         max_steps=self._train_spec.max_steps,\r\n    690         hooks=train_hooks,\r\n--> 691         saving_listeners=saving_listeners)\r\n    692 \r\n    693     eval_result = listener_for_eval.eval_result or _EvalResult(\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    374 \r\n    375       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 376       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    377       logging.info('Loss for final step: %s.', loss)\r\n    378       return self\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1141   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1142     if self._distribution:\r\n-> 1143       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1144     else:\r\n   1145       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1366         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1367                                                hooks, global_step_tensor,\r\n-> 1368                                                saving_listeners)\r\n   1369 \r\n   1370   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1449       loss = None\r\n   1450       while not mon_sess.should_stop():\r\n-> 1451         _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n   1452     return loss\r\n   1453 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    581                           feed_dict=feed_dict,\r\n    582                           options=options,\r\n--> 583                           run_metadata=run_metadata)\r\n    584 \r\n    585   def run_step_fn(self, step_fn):\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1057                               feed_dict=feed_dict,\r\n   1058                               options=options,\r\n-> 1059                               run_metadata=run_metadata)\r\n   1060       except _PREEMPTION_ERRORS as e:\r\n   1061         logging.info('An error was raised. This may be due to a preemption in '\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1148         raise six.reraise(*original_exc_info)\r\n   1149       else:\r\n-> 1150         raise six.reraise(*original_exc_info)\r\n   1151 \r\n   1152 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1133   def run(self, *args, **kwargs):\r\n   1134     try:\r\n-> 1135       return self._sess.run(*args, **kwargs)\r\n   1136     except _PREEMPTION_ERRORS:\r\n   1137       raise\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1213               results=outputs[hook] if hook in outputs else None,\r\n   1214               options=options,\r\n-> 1215               run_metadata=run_metadata))\r\n   1216     self._should_stop = self._should_stop or run_context.stop_requested\r\n   1217 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in after_run(self, run_context, run_values)\r\n    462       if self._timer.should_trigger_for_step(global_step):\r\n    463         self._timer.update_last_triggered_step(global_step)\r\n--> 464         if self._save(run_context.session, global_step):\r\n    465           run_context.request_stop()\r\n    466 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in _save(self, session, step)\r\n    487     should_stop = False\r\n    488     for l in self._listeners:\r\n--> 489       if l.after_save(session, step):\r\n    490         logging.info(\r\n    491             \"A CheckpointSaverListener requested that training be stopped. \"\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in after_save(***failed resolving arguments***)\r\n    495       return True\r\n    496     if self._timer.should_trigger_for_step(global_step_value):\r\n--> 497       self._evaluate(global_step_value)  # updates self.eval_result\r\n    498       if not self._continuous_eval_listener.after_eval(self.eval_result):\r\n    499         logging.info('Exiting evaluation, as requested by '\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in _evaluate(self, global_step_value)\r\n    515     self._timer.update_last_triggered_step(global_step_value)\r\n    516     self.eval_result, self.export_results = (\r\n--> 517         self._evaluator.evaluate_and_export())\r\n    518     if self.eval_result.status != _EvalStatus.EVALUATED:\r\n    519       #  This is unexpected; should never happen.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in evaluate_and_export(self)\r\n    882           name=self._eval_spec.name,\r\n    883           checkpoint_path=latest_ckpt_path,\r\n--> 884           hooks=self._eval_spec.hooks)\r\n    885 \r\n    886       # _EvalResult validates the metrics.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in evaluate(self, input_fn, steps, hooks, checkpoint_path, name)\r\n    461         (scaffold, update_op,\r\n    462          eval_dict, all_hooks) = self._evaluate_build_graph(\r\n--> 463              input_fn, hooks, checkpoint_path)\r\n    464         return self._evaluate_run(\r\n    465             checkpoint_path=checkpoint_path,\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _evaluate_build_graph(self, input_fn, hooks, checkpoint_path)\r\n   1461                                                     model_fn_lib.ModeKeys.EVAL))\r\n   1462     estimator_spec = self._call_model_fn(\r\n-> 1463         features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\r\n   1464 \r\n   1465     # Call to warm_start has to be after model_fn is called.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1131 \r\n   1132     logging.info('Calling model_fn.')\r\n-> 1133     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1134     logging.info('Done calling model_fn.')\r\n   1135 \r\n\r\n<ipython-input-2-b27091873f02> in model_fn(features, labels, mode, params)\r\n     26 \r\n     27     if mode == tf.estimator.ModeKeys.EVAL:\r\n---> 28         metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}\r\n     29 \r\n     30         return tf.estimator.EstimatorSpec(\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in mean_squared_error(labels, predictions, weights, metrics_collections, updates_collections, name)\r\n   1297   squared_error = math_ops.square(labels - predictions)\r\n   1298   return mean(squared_error, weights, metrics_collections, updates_collections,\r\n-> 1299               name or 'mean_squared_error')\r\n   1300 \r\n   1301 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in mean(values, weights, metrics_collections, updates_collections, name)\r\n    374       return mean_t\r\n    375 \r\n--> 376     mean_t = distribute_lib.get_tower_context().merge_call(\r\n    377         aggregate_across_towers, total, count)\r\n    378     update_op = _safe_div(update_total_op, update_count_op, 'update_op')\r\n\r\nAttributeError: 'NoneType' object has no attribute 'merge_call'\r\n```\r\n", "comments": ["This should be fixed in master now"]}, {"number": 21179, "title": "Reverting the numpy version for the Docker tests and builds.", "body": "", "comments": []}, {"number": 21178, "title": "Support placeholders without shape in build_raw_serving_input_receiver_fn", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, a small demo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian-based Linux distribution.\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\n```tf.estimator.export.build_raw_serving_input_receiver_fn``` crashes on ```tf.placeholder```s that do not have a shape specified.\r\n\r\n### Source code / logs\r\nBelow is a minimal demo that causes the crash:\r\n```\r\n# Attempts to learn x+y.\r\n\r\nimport tensorflow as tf\r\n\r\ndef input_fn():\r\n  x = tf.random_uniform(shape=[1])\r\n  y = tf.random_uniform(shape=[1])\r\n  return {'x': x, 'y': y}, tf.add(x, y)\r\n\r\n\r\ndef main(_):\r\n  estimator = tf.estimator.LinearRegressor([\r\n      tf.contrib.layers.real_valued_column('x'),\r\n      tf.contrib.layers.real_valued_column('y'),\r\n  ])\r\n\r\n  train_spec = tf.estimator.TrainSpec(input_fn)\r\n\r\n  phs = { 'x': tf.placeholder(tf.float32), 'y': tf.placeholder(tf.float32) }\r\n  #receiver_fn = lambda: tf.estimator.export.ServingInputReceiver(phs, phs)\r\n  receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(phs)\r\n\r\n  eval_spec = tf.estimator.EvalSpec(\r\n      input_fn,\r\n      exporters=[tf.estimator.LatestExporter('latest', receiver_fn)]\r\n  )\r\n\r\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n\r\nAttempting to execute this code produces the following error and stacktrace (after 10mins, when model export is attempted):\r\n```\r\n  File \"bug_demo.py\", line 26, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\", line 447, in train_and_evaluate\r\n    return executor.run()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\", line 531, in run\r\n    return self.run_local()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\", line 681, in run_local\r\n    eval_result, export_results = evaluator.evaluate_and_export()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\", line 898, in evaluate_and_export\r\n    is_the_final_export)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\", line 931, in _export_eval_result\r\n    is_the_final_export=is_the_final_export))\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/exporter.py\", line 472, in export\r\n    is_the_final_export)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/exporter.py\", line 126, in export\r\n    strip_default_attrs=self._strip_default_attrs)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 650, in export_savedmodel\r\n    mode=model_fn_lib.ModeKeys.PREDICT)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 703, in _export_saved_model_for_mode\r\n    strip_default_attrs=strip_default_attrs)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 811, in _export_all_saved_models\r\n    mode=model_fn_lib.ModeKeys.PREDICT)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 872, in _add_meta_graph_for_mode\r\n    input_receiver = input_receiver_fn()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py\", line 335, in serving_input_receiver_fn\r\n    features, default_batch_size)\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py\", line 312, in _placeholders_from_receiver_tensors_dict\r\n    for name, t in input_vals.items()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py\", line 312, in <dictcomp>\r\n    for name, t in input_vals.items()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/estimator/export/export.py\", line 298, in _placeholder_from_tensor\r\n    shape_list = t.get_shape().as_list()\r\n  File \"tf_public/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 903, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n```\r\n\r\nReplacing ```receiver_fn``` as per the commented-out line or adding shape parameters to ```tf.placeholder``` prevents this crash. ", "comments": ["Just ran into this too. Is it just a matter of replacing `t.get_shape()` with `tf.shape(t)`?", "Hi, I create the PR #21536 to fix the issue."]}, {"number": 21177, "title": "[Intel MKL] Adding support for MPI and Horovod to MKL containers", "body": "", "comments": ["@congxu-ml Here is the Horovod container", "Thanks, Clayne! I can answer any feedback related to this commit.", "@caisq Hi Shanqing, can you please review this PR? It is very important for us, lots of our customers need to use it. Thanks a lot!", "Adding angerson@ to take a look who is looking at refactoring our Docker files.", "angerson@ Thanks for approving my commit! Sure, I will modify my files once the proposed Dockerfile assembler is merged."]}]