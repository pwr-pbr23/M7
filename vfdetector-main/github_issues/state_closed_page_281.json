[{"number": 45908, "title": "Update SQLite to the lastest sqlite-amalgamation-3340000", "body": "This PR updates SQLite to the latest sqlite-amalgamation-3340000\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 45907, "title": "Value of num_replicas parameter now propagates.", "body": "The value of `num_replicas` parameter of the `_report` function in `resnet50_test.py` was not passed to the `report` function in `resnet50_test_util.py`, as a relevant parameter (with the same name) was assigned a value in the call. \r\n\r\n", "comments": []}, {"number": 45906, "title": "TF 2.4.0 Python dependency issues", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : OSX, Centos\r\n- TensorFlow installed from (source or binary): package installed with pipenv from pypi\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: pipenv\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```pipenv install``` from the following Pipfile, which now picks up the latest TF 2.4.0:\r\n```\r\n[[source]]\r\nname = \"pypi\"\r\nurl = \"https://pypi.org/simple\"\r\nverify_ssl = true\r\n\r\n[dev-packages]\r\npylint = \"*\"\r\npytest-cov = \"*\"\r\ngrpcio-tools = \"*\"\r\n\r\n[packages]\r\nscikit-learn = \"*\"\r\npytest = \"*\"\r\npytest-docker = \"*\"\r\nnumpy = \"*\"\r\ntensorflow-serving-api = \"*\"\r\ntensorflow.python.framework = \"*\"\r\ngrpcio = \"*\"\r\ngrpcio-reflection = \"*\"\r\nspacy = \"*\"\r\npy-grpc-prometheus = \"*\"\r\nprotobuf = \"*\"\r\ndynaconf = \"*\"\r\ngoogle-cloud-storage = \"*\"\r\n\r\n[requires]\r\npython_version = \"3.8\"\r\n```\r\n\r\n**Any other info / logs**\r\nHere is the log from ```pipenv check``` after installation:\r\n```\r\nChecking PEP 508 requirements...\r\nPassed!\r\nChecking installed package safety...\r\n38932: cryptography <=3.2 resolved (3.1.1 installed)!\r\nCryptography 3.2 was released with the warning that its maintainers became aware of a Bleichenbacher vulnerability that they were only partly able to mitigate. See: CVE-2020-25659.\r\n```\r\nHowever, this was from an existing ```Pipfile.lock```. Deleting ```Pipfile.lock``` and re-installing in a new venv resulted in the following dependency mismatches:\r\n```\r\n[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.\r\n  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.\r\n Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.\r\n  Hint: try $ pipenv lock --pre if it is a pre-release dependency.\r\nERROR: Could not find a version that matches grpcio>=1.0<2,>=1.10.0,>=1.34.0,~=1.32.0 (from -r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 5))\r\nTried: 0.13.0, 0.13.1, 0.14.0, 0.15.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.1.3, 1.2.0, 1.2.1, 1.3.0, 1.3.5, 1.4.0, 1.6.0, 1.6.3, 1.7.0, 1.7.3, 1.8.1, 1.8.2, 1.8.3, 1.8.4, 1.8.6, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.11.0, 1.11.1, 1.12.0, 1.12.1, 1.13.0, 1.14.0, 1.14.1, 1.14.2, 1.15.0, 1.16.0, 1.16.1, 1.17.0, 1.17.1, 1.18.0, 1.19.0, 1.20.0, 1.20.1, 1.21.1, 1.22.0, 1.22.1, 1.23.0, 1.23.1, 1.24.0, 1.24.1, 1.24.3, 1.25.0, 1.26.0, 1.26.0, 1.27.1, 1.27.1, 1.27.2, 1.27.2, 1.28.1, 1.28.1, 1.29.0, 1.29.0, 1.30.0, 1.30.0, 1.31.0, 1.31.0, 1.32.0, 1.32.0, 1.33.1, 1.33.1, 1.33.2, 1.33.2, 1.34.0, 1.34.0\r\nSkipped pre-versions: 0.4.0a0, 0.4.0a1, 0.4.0a2, 0.4.0a3, 0.4.0a4, 0.4.0a5, 0.4.0a6, 0.4.0a7, 0.4.0a8, 0.4.0a13, 0.4.0a14, 0.5.0a0, 0.5.0a1, 0.5.0a2, 0.9.0a0, 0.9.0a1, 0.10.0a0, 0.11.0b0, 0.11.0b1, 0.12.0b0, 0.13.1rc1, 0.14.0rc1, 1.0.0rc1, 1.0.0rc2, 1.0.1rc1, 1.0.2rc0, 1.1.0rc1, 1.2.0rc1, 1.2.0rc2, 1.4.0rc1, 1.6.0rc1, 1.7.0rc1, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.10.0rc1, 1.10.0rc2, 1.10.1rc1, 1.10.1rc2, 1.11.0rc1, 1.11.0rc2, 1.11.1rc1, 1.12.0rc1, 1.13.0rc1, 1.13.0rc2, 1.13.0rc3, 1.14.0rc1, 1.14.0rc2, 1.14.2rc1, 1.15.0rc1, 1.16.0rc1, 1.16.1rc1, 1.17.0rc1, 1.17.1rc1, 1.18.0rc1, 1.19.0rc1, 1.20.0rc1, 1.20.0rc2, 1.20.0rc3, 1.21.0rc1, 1.21.1rc1, 1.22.0rc1, 1.23.0rc1, 1.24.0rc1, 1.25.0rc1, 1.26.0rc1, 1.26.0rc1, 1.27.0rc1, 1.27.0rc1, 1.27.0rc2, 1.27.0rc2, 1.28.0.dev0, 1.28.0.dev0, 1.28.0rc1, 1.28.0rc1, 1.28.0rc2, 1.28.0rc2, 1.28.0rc3, 1.28.0rc3, 1.30.0rc1, 1.30.0rc1, 1.31.0rc1, 1.31.0rc1, 1.31.0rc2, 1.31.0rc2, 1.32.0rc1, 1.32.0rc1, 1.33.0rc1, 1.33.0rc1, 1.33.0rc2, 1.33.0rc2, 1.34.0rc1, 1.34.0rc1\r\nThere are incompatible versions in the resolved dependencies:\r\n  grpcio (from -r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 5))\r\n  grpcio>=1.0<2 (from tensorflow-serving-api==2.4.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 13))\r\n  grpcio>=1.10.0 (from py-grpc-prometheus==0.2.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 12))\r\n  grpcio>=1.34.0 (from grpcio-reflection==1.34.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 10))\r\n  grpcio~=1.32.0 (from tensorflow==2.4.0->-r /var/folders/ky/kytzhfhs1cdfkwgs_4hyfpbmbtbvby/T/pipenvjgjmye8frequirements/pipenv-fz4mci2f-constraints.txt (line 3))\r\n```\r\nThe problem here is TF 2.4.0 depends on ```grpcio``` 1.32.0, which does not exist.\r\n\r\nFixing version to TF 2.3.1 and tensorflow-serving-api to 2.3.0 resolves the pipenv install and the check failure. Pipfile as follows:\r\n```\r\n[[source]]\r\nname = \"pypi\"\r\nurl = \"https://pypi.org/simple\"\r\nverify_ssl = true\r\n\r\n[dev-packages]\r\npylint = \"*\"\r\npytest-cov = \"*\"\r\ngrpcio-tools = \"*\"\r\n\r\n[packages]\r\nscikit-learn = \"*\"\r\npytest = \"*\"\r\npytest-docker = \"*\"\r\nnumpy = \"*\"\r\ntensorflow = \"==2.3.1\"\r\ntensorflow-serving-api = \"==2.3.0\"\r\ngrpcio = \"*\"\r\ngrpcio-reflection = \"*\"\r\nspacy = \"*\"\r\npy-grpc-prometheus = \"*\"\r\nprotobuf = \"*\"\r\ndynaconf = \"*\"\r\ngoogle-cloud-storage = \"*\"\r\n\r\n[requires]\r\npython_version = \"3.8\"\r\n```", "comments": ["@adriangay,\r\nCould you please update the pip package to the latest version and check if you are facing the same `grpcio` error.\r\n\r\nAlso, take a look at [#45785](https://github.com/tensorflow/tensorflow/issues/45785) and let us know if it is the same issue. Thanks!", "@amahendrakar Hi, thx for the quick response. I did not notice issue #45785. Yes, it's the same issue. So I will close this one. On your first point, the first Pipfile above uses asterisks, so already pulls latest of everything, and that's what causes the 2.4.0 issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45906\">No</a>\n"]}, {"number": 45905, "title": "Entmax-alpha in addons", "body": "I would like to implement the entmax-alpha feature as is described in https://arxiv.org/abs/1905.05702 in the addons. For alpha=1, it is softmax, and for alpha=2, it is sparsemax. \r\n\r\nThis feature is a key to realize sparse seq-to-seq model. It will not change the current API. \r\n\r\nI have almost finished implementing the feature. Therefore, I would like to know if this has any chance of being merged before continuing. \r\n", "comments": ["@rwbfd,\r\nTopics related to TensorFlow Addons are tracked in the tensorflow/addons repo. Could you please submit a new feature request from [this link](https://github.com/tensorflow/addons/issues/new/choose) so that it can be tracked there. Thanks!", "@amahendrakar Thank you. I will close the issue now."]}, {"number": 45904, "title": "regression: dropped Support of CUDA\u00ae architecture 5.0?", "body": "I'm using trying to use tensorflow 2.4 with nvidia 960m (compute compatibility 5.0)\r\n\r\nAccording to the following change: https://github.com/tensorflow/docs/commit/cb886cfdd16d66ff7f8d1430676ff395b02910e6\r\n\r\nSupport of my card and many others have been dropped (My card at least definitely worked fine with tensorflow 2.2)\r\n\r\nThis is a major regression for many who are trying to upgrade, and currently, it is not even clear to me if it's intended and what is the rational behind it. If it is intended, please document it prominently in the 2.4 release. (and please consider not dropping the support of this architecture)", "comments": ["Hi, @amahendrakarI are you sure this is not a regression? AFAIKT, a year ago tensorflow supported all cuda architectures greater than 3.5", "I think this was intentional change. See https://github.com/tensorflow/tensorflow/issues/41892#issuecomment-667452483", "It's bad for tensorflow maintainers among others, I'm sure many issues could just be resolved when considering they are running an unsupported compute version, but even not all maintainers seem to be familier with this new limitation.\r\n\r\nAlso, got do admit I don't follow why/how `3.5`, `3.7`, `5.2`, `6.0`, and `6.1` are supported but specifically `5.0` is skipped (https://www.tensorflow.org/install/gpu) when in the 2.3 release notes we have: \"PTX is not compiled before version `7.0`\".\r\n\r\n", "Side note for docker users landing here - because they are not able to use their gpus anymore:\r\n\r\nThe problems you may have when trying to run with gpus from the docker image `tensorflow/tensorflow:latest-gpu` appear not to be related to this issue: Using `tensorflow/tensorflow:nightly-gpu` works. ", "TF 2.4 should support CC 5.0, please see announcement [here](https://groups.google.com/a/tensorflow.org/g/developers/c/uCKQeiTA3u8/m/xDBWKcrUAgAJ).\r\n\r\nThe list of compute capabilities on https://www.tensorflow.org/install/gpu is stale, we will fix it soon.", "The gpu docs have been updated with commit [b5c78b0](https://github.com/tensorflow/docs/commit/430fe3be375a1f3f6267b86300e1c0f1d79b2b26#diff-db2f8c542d222b663b56e2614d6e42b9531606ded976427355d2c7442d6eee43).\r\nSee https://www.tensorflow.org/install/gpu#hardware_requirements\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45904\">No</a>\n"]}, {"number": 45903, "title": "Model with custom metrics broken if saved and reloaded", "body": "There is a new problem in r2.4 (not present in 2.3.1). After saving and reloading a model with custom metric the model is broken. The next training will not work. Here is my minimum code to easily reproduce: \r\n```\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model, Sequential\r\nfrom tensorflow.keras.layers import Flatten, Dense\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ndef cmetrics(y_true, y_pred):\r\n\treturn(0)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(10,activation=\"relu\", input_shape=(331, 331, 3)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(10, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy',\r\n\toptimizer=Adam(),\r\n\tmetrics=[cmetrics])\r\nmodel.summary()\r\nxdata = np.random.rand(100,331,331,3)\r\nydata = np.random.rand(100,10)\r\nhistory = model.fit(x=xdata, y=ydata)\r\nmodel.save('test.h5', save_format='h5')\r\nmodel = load_model('test.h5', custom_objects={'cmetrics': cmetrics,})\r\nhistory = model.fit(x=xdata,y=ydata)\r\n```\r\nWhen running with tensorflow 2.3.1, I get the expected result:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 331, 331, 10)      40        \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1095610)           0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                10956110  \r\n=================================================================\r\nTotal params: 10,956,150\r\nTrainable params: 10,956,150\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n4/4 [==============================] - 1s 149ms/step - loss: 52.2964 - cmetrics: 0.0000e+00\r\n4/4 [==============================] - 1s 142ms/step - loss: 46.6724 - cmetrics: 0.0000e+00\r\n```\r\nWhen running with tensorflow 2.4.0, I get this:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 331, 331, 10)      40        \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1095610)           0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                10956110  \r\n=================================================================\r\nTotal params: 10,956,150\r\nTrainable params: 10,956,150\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2020-12-21 13:30:33.688730: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-12-21 13:30:33.708412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2593735000 Hz\r\n4/4 [==============================] - 1s 190ms/step - loss: 18.9856 - cmetrics: 0.0000e+00\r\nTraceback (most recent call last):\r\n  File \"modeltest.py\", line 22, in <module>\r\n    history = model.fit(x=xdata,y=ydata)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:790 run_step  **\r\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2793 _minimum_control_deps\r\n        outputs = nest.flatten(outputs, expand_composites=True)\r\n    /home/ludger/safe/sources/python/test/env/lib/python3.7/site-packages/tensorflow/python/util/nest.py:341 flatten\r\n        return _pywrap_utils.Flatten(structure, expand_composites)\r\n\r\n    TypeError: '<' not supported between instances of 'function' and 'str'\r\n\r\n```\r\n**Additional Information:**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Debian 10**\r\n- TensorFlow installed from (source or binary):\r\n**pip install tensorflow**\r\n- TensorFlow version (use command below):\r\n**2.4.0**\r\n- Python version:\r\n**3.7**\r\n", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/f40af1a87f69955c299da17231af81ae/45903-2-4.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/adc3a14752623be88fae2720f897d7c7/45903-tf-nightly.ipynb). \r\n\r\nHowever, did not face any errors with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e603460b7628b2375aea39eae0b4a26a/45903-2-3.ipynb). Please check the linked gist for reference. Thanks!", "> However, did not face any errors with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e603460b7628b2375aea39eae0b4a26a/45903-2-3.ipynb). Please check the linked gist for reference. Thanks!\r\n\r\nThat is exactly my observation. 2.3.1 works 100% OK, 2.4.0 does not. Thanks for the quick answer...", "Try to save pre-trained model weights and model configuration itself separately. \r\nIt might temporarily solve the problem.", "> Try to save pre-trained model weights and model configuration itself separately.\r\n> It might temporarily solve the problem.\r\n\r\nThank you for the hint, but I will keep the system with the problem on TF2.3.1 for now. Hopefully there will be a fix for a new version at some point...", "Try compile your model with exact same metric again after loading it. See https://github.com/keras-team/keras/issues/14231", "> Try compile your model with exact same metric again after loading it. See [keras-team/keras#14231](https://github.com/keras-team/keras/issues/14231)\r\n\r\nYep, this works for me. Thank you very much!", "Will recompiling the model reset the optimizer state?", "@alecgunny I think it does. See [this colab](https://colab.research.google.com/drive/1z7I5J-GgDGV71kkTV3Wgog4iPSOnF0oF?usp=sharing) for example.\r\n@byronyi is there a solution for someone who wants to reuse the optimizer's state?", "My hunch is to use `weights_only=True`, i.e. save only model/optimizer state instead of model structure.", "I see, but in order to restart from a SavedModel format in the current state?", "Just a reminder for anyone wanting to save the model's optimizer state, there is [an SO answer](https://stackoverflow.com/a/49504376/4332585) giving the idea on how to do it.", "FYI, what I did to escape this issue is recompiling a model with saved loss and optimizer.\r\n```\r\nmodel = load_model(\"mymodel_best.h5\", custom_objects={\"mymetric\":mymetric}, compile=True)\r\nmodel.compile(loss=model.loss, optimizer=model.optimizer, metrics=[mymetric])\r\n```", "> \r\n> \r\n> FYI, what I did to escape this issue is recompiling a model with saved loss and optimizer.\r\n> \r\n> ```\r\n> model = load_model(\"mymodel_best.h5\", custom_objects={\"mymetric\":mymetric}, compile=True)\r\n> model.compile(loss=model.loss, optimizer=model.optimizer, metrics=[mymetric])\r\n> ```\r\n\r\nI'm thoroughly thankful for your input! The most practical solution to this bug, which doesn't look like it's being addressed in 2.5.0. I wish you never ending happiness!\r\n\r\n", "Hi all--\r\n\r\nSorry forgot to update this!\r\n\r\nThis issue is now fixed with https://github.com/tensorflow/tensorflow/commit/6bd24c2096fd0f89301b4a6e1f4e8375324e0469, which unfortunately did not make it into 2.5.0 but is now in tf-nightly.\r\n\r\nThe gist that @amahendrakar provided now works with tf-nightly.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45903\">No</a>\n"]}, {"number": 45901, "title": "How to catch internal TensorFlow error: 'GPU sync failed' in python?", "body": "TF version 1.x and 2.x.\r\n\r\nIs there a way to catch this error with try-expect in python?\r\n\r\n`tensorflow.python.framework.errors_impl.InternalError: GPU sync failed`", "comments": ["@kiflowb777,\r\n\r\nPlease take a look at the below code snippet and check if it works. \r\n\r\n```\r\ntry:\r\n   # do something\r\n   pass\r\n\r\nexcept InternalError:\r\n   # handle InternalError exception\r\n   pass\r\n\r\n```\r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is a larger community that reads questions there. Thanks!", "Thank you, can we assume that every InternalError is the 'GPU sync failed' error?", "https://stackoverflow.com/questions/65395940/how-to-catch-internal-tensorflow-error-gpu-sync-failed-in-python", "This InternalError can also be thrown in other cases:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal: Blas GEMM launch failed : a.shape=(63, 494), b.shape=(494, 2048), m=63, n=2048, k=494\r\n\t [[{{node tower_0/MatMul}}]]\r\n\t [[concat/concat/_99]]\r\n  (1) Internal: Blas GEMM launch failed : a.shape=(63, 494), b.shape=(494, 2048), m=63, n=2048, k=494\r\n\t [[{{node tower_0/MatMul}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "There is some way to check if an **InternalError** is the '**GPU sync failed**' error without comparing string?", "```\r\ntry:\r\n   #do_something\r\n\r\nexcept InternalError as e:\r\n   if is_e_a_gpu_sync_failed(e):\r\n      #do something\r\n\r\n```", "@kiflowb777,\r\nCould you please provide a minimal code snippet of the issue you are facing, so that we can reproduce the issue on our end and look into this. Thanks!", "This error is random. \r\nIt depends mainly on the network size,  number of input images and dimensions. \r\nOften when there is no memory in the graphics card.\r\n\r\nI will try to provide the code that often throws this error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45901\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45901\">No</a>\n"]}, {"number": 45900, "title": "How to catch/expect the error: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1", "body": "TF version 1.x and 2.x.\r\n\r\nIs there a way to catch this error with try-expect in python?\r\n\r\n`F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1`", "comments": ["@kiflowb777,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "https://stackoverflow.com/questions/65405886/how-to-catch-expect-the-error-f-tensorflow-core-common-runtime-gpu-gpu-event-mg", "hey ! I am a begginer in open source although I know tensorflow etc but I want to contribute can someone tell from where can I find issues for begginers like me ", "https://stackoverflow.com/questions/32391561/how-to-catch-exception-caused-in-c-in-python", "> https://stackoverflow.com/questions/65405886/how-to-catch-expect-the-error-f-tensorflow-core-common-runtime-gpu-gpu-event-mg\r\n\r\n@kiflowb777,\r\nCan we please close this issue, as it is being tracked on StackOverflow? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45900\">No</a>\n"]}, {"number": 45899, "title": "Training a keras model on TPU pods?", "body": "Hi!\r\n\r\nAre there any examples on how to run a Keras model on TPU pods? I have a model which runs fine on a V3-8. However, when trying to run the same code on V3-32, it fails with the following error:\r\n\r\n`Failed copying input tensor from /job:worker/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:1/device:CPU:0 in order to run DatasetFromGraph: FetchOutputs node : not found [Op:DatasetFromGraph]`\r\n\r\nThe model is created and compiled within the TPU scope as recommended:\r\n\r\n```\r\nwith strategy.scope():\r\n    keras_model = create_model()\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\r\n    keras_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n\r\n```\r\nI am running Tensorflow 2.3.1 whith the same version on the TPU.\r\n\r\nI have already asked on StackOverflow without getting any good pointers / answers, see link here:\r\n\r\nhttps://stackoverflow.com/questions/65331321/training-a-keras-model-using-tpu-pods?noredirect=1#comment115577267_65331321\r\n\r\nThanks!", "comments": ["@steindor,\r\nPlease take a look at this [TPU usage guide](https://www.tensorflow.org/guide/tpu) and let us know if it helps. \r\n\r\nAlso, could you please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Yes, I went through the TPU usage guide before I opened this issue. I will try to create a code snippet which reproduces this.", "> I will try to create a code snippet which reproduces this.\r\n\r\n@steindor,\r\nAny updates regarding this? Did you get a chance to work on it? Thanks!", "Yes, I ran the code example you gave me with v3-32 but did not get the same error so there is something else going on here. I'll close this since this is probably not related to tensorflow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45899\">No</a>\n"]}, {"number": 45897, "title": "Missing Documentation for compiling binaries in c++", "body": "Not sure if this is the right place to post this, but anyways, does anyone have a link for documentation on building TensorFlow for C++ to be used with VS 2019? I've looked everywhere and can't seem to find any up to date information on how to do so. The build from source page on TensorFlow's website seemed to look like it was just for building a pip installer for python. \r\n\r\nI followed along with \r\n\r\nhttps://medium.com/vitrox-publication/deep-learning-frameworks-tensorflow-build-from-source-on-windows-python-c-cpu-gpu-d3aa4d0772d8\r\n\r\nbut after linking the header files and .lib & .dll I got spewed a bunch of missing dependencies/syntax errors, so I'm guessing that that the bugs mentioned in the article didnt follow through to v2.4.0 (which is what I compiled it for)\r\n\r\nIf anyone could point to up to date documentation, or a pre-compiled library that works with c++ VS19 I'd appreciate it.", "comments": ["@Joel-De,\r\nCould you please take a look at this [TensorFlow C API](https://www.tensorflow.org/install/lang_c) installation guide and let us know if it helps. Thanks!", "@amahendrakar \r\n\r\nI've looked through what you've provided, if I'm not mistaken that appears to be for the c api, I've tried using it for cpp but there are compile errors,\r\n\r\nError\tLNK2019\tunresolved external symbol __imp__TF_Version referenced in function _main\r\nError\tLNK1120\t1 unresolved externals\t\r\n\r\nI believe this has to do with improperly converting the c based api to one that c++ can work with (from my searches online it reffers to using the 'extern \"C\" ' keyword). Is there a way to use the c api files with c++? Or do I need to build those myself, and if so how would I do that?\r\n\r\nAs a note, my goal is to use c++ to perform inferences for a pretrained model built using the python api.\r\n\r\nThanks!\r\n\r\n\r\n", "Yep, same problem here. I was trying to use cppflow but the road ended here instead", "@Joel-De \r\nCould you please refer to the updated [link](https://www.tensorflow.org/install/lang_c) and let us know if this is still an issue", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45897\">No</a>\n"]}, {"number": 45896, "title": "Merge pull request #43269 from rhdong:Mutablehashtable lookup support\u2026", "body": "This PR has been merged to master: https://github.com/tensorflow/tensorflow/pull/43269\r\nThis PR is one part of [RFC#313](https://github.com/tensorflow/community/pull/313), [RFC#237](https://github.com/tensorflow/community/pull/237)", "comments": ["@yuefengz Please help review, thank you~", "In general I don't think the TF team accept feature cherry-picks into release branches, but it might worth to check again.", "> In general I don't think the TF team accept feature cherry-picks into release branches, but it might worth to check again.\r\n\r\nThank you for your prompt, maybe I miss the best time, if I had to wait for the r2.5 release?  I think it will take too long time...", "Unfortunately 2.4 has been released already."]}, {"number": 45895, "title": "keras Callbacks without _supports_tf_logs update separate `logs` ", "body": "## System information\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 18.04:\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219\r\n- Python version: 3.7.7\r\n\r\n## Current Behaviour\r\nCallbacks in `CallbackList` update different `logs` dicts based on private `_supports_tf_logs` attribute (see e.g. [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L431)). This attribute is undocumented and it is unclear exactly what it is intended to signify. If a callback adds an entry to `logs`, the effect this has depends on this value.\r\n\r\n## Expected behavior\r\nCallbacks should be affected to previous callbacks' mutations of logs regardless of `_supports_tf_logs` values.\r\n\r\n## Standalone code\r\n[This colab](https://colab.research.google.com/drive/1Gd6JaZZk9fKmZSFdX7yd9Q_Oui-YzqDv?usp=sharing) shows the result of manually changing the property value of `LearningRateScheduler` and it's affect on `ProgbarLogger`'s behaviour.\r\n\r\nCode reproduced below for convenience\r\n```python\r\nimport tensorflow as tf\r\ninp = tf.keras.Input((1,))\r\nout = tf.keras.layers.Dense(1)(inp)\r\nmodel = tf.keras.Model(inp, out)\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\nx = tf.random.uniform((10, 1))\r\ny = 2 * x + 3\r\ndataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)\r\n\r\nsched = tf.keras.callbacks.LearningRateScheduler(lambda i: 1. / (i+1))\r\nsched._supports_tf_logs = True  # makes ProgbarLogger display lr\r\n# same issue with ReduceLROnPlateau\r\ncallbacks = [sched]\r\n\r\n# add logger at end, otherwise it's inserted at front and won't print lr\r\ncallbacks.append(tf.keras.callbacks.ProgbarLogger())\r\n\r\nmodel.fit(dataset, epochs=10, callbacks=callbacks)\r\n```\r\nOutput\r\n```txt\r\nEpoch 1/10\r\n5/5 [==============================] - 1s 109ms/sample - loss: 58.7811 - lr: 1.0000\r\nEpoch 2/10\r\n5/5 [==============================] - 0s 2ms/sample - loss: 94.1334 - lr: 0.5000\r\nEpoch 3/10\r\n5/5 [==============================] - 0s 2ms/sample - loss: 22.3592 - lr: 0.3333\r\nEpoch 4/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 3.3667 - lr: 0.2500\r\nEpoch 5/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 1.4383 - lr: 0.2000\r\nEpoch 6/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 0.8452 - lr: 0.1667\r\nEpoch 7/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 0.5714 - lr: 0.1429\r\nEpoch 8/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 0.4179 - lr: 0.1250\r\nEpoch 9/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 0.3216 - lr: 0.1111\r\nEpoch 10/10\r\n5/5 [==============================] - 0s 1ms/sample - loss: 0.2565 - lr: 0.1000\r\n```", "comments": ["@jackd,\r\nSetting **`sched._supports_tf_logs = True`** prints the Value of `Learning Rate` in the `Logs` and setting it to `False` doesn't prints its value. That is the difference I observe and the name `supports_tf_logs` seems apt to me. \r\n\r\nPlease find the [Colab Gist](https://colab.research.google.com/gist/rmothukuru/58719147c8113148e00b15acec8d40b6/callback-logs.ipynb).\r\n\r\nIf this is not what you mean, can you please demonstrate the behavior in your code? Thanks!", "@rmothukuru there is a very significant difference between the names `supports_tf_logs` and `_supports_tf_logs`.\r\n\r\nThe current behaviour is:\r\n\r\n- the first callback with `_supports_tf_logs = False` observes mutations to `logs` only from prior callbacks\r\n- subsequent callbacks with `_supports_tf_logs = False` observe mutations to `logs` only from callbacks up to the first callback with `_supports_tf_logs=False` and those with `_supports_tf_logs=False`\r\n- callbacks with `_supports_tf_logs=True` observe mutations only from previous callbacks with `_supports_tf_logs=True`\r\n\r\nPublic/protected attribute aside, the name `supports_tf_logs` does not imply *any* of those to me. In the example provided, why does `ProgbarLogger`'s behaviour change based on whether `LearningRateScheduler` supports tf logs?\r\n", "@jackd,\r\nWhat I meant was **`_supports_tf_logs`**, not **`supports_tf_logs`**. Sorry for the confusion.\r\n\r\nI didn't observe change in `ProgbarLogger`'s behavior in [this Gist](https://colab.research.google.com/gist/rmothukuru/58719147c8113148e00b15acec8d40b6/callback-logs.ipynb#scrollTo=7k7wwszKEAB1) based on the value of **`_supports_tf_logs`**. Can you please elaborate about it.\r\n\r\nThanks!", "Cell 2's output has `lr`, Cell 3's doesn't.", "@jackd,\r\nIs that not expected because it is changing as per the value of \r\n**`tf.keras.callbacks.LearningRateScheduler(lambda i: 1. / (i+1))._supports_tf_logs`**?", "... I suppose that depends what you mean by \"expect\". I expect it because I'm very familiar with the implementation, because I've gone digging when `lr` disappeared from my logs after a tf update, in the same way that I expect any bug to remain there until it gets fixed. I certainly didn't expect it before I went digging. It's not part of the public API, nor is it documented anyway, nor do I believe this partitioning \"feature\" of callbacks was intended by whoever changed things.", "This issue needs to be solved.\r\nI have a custom callback that evaluates recall metric on epoch end.\r\n```python\r\nclass RecallCallback(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs=None):\r\n      logs['recall@1'] = evaluate_recall(...)\r\n```\r\n```python\r\ncheckpoint = ModelCheckpoint(monitor='recall@1', ...)\r\n#checkpoint._supports_tf_logs = False\r\nnet.fit(callbacks=[RecallCallback(), checkpoint], ...)\r\n```\r\nIt does not sync the updates of logs.\r\nBut it works by setting the _supports_tf_logs to False.\r\n\r\nThe logs and the numpy_logs should be synced at calling each callback in [CallbackList class](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L431).\r\n```python\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    logs = logs or {}\r\n    numpy_logs = None\r\n    for callback in self.callbacks:\r\n      if getattr(callback, '_supports_tf_logs', False):\r\n        callback.on_epoch_end(epoch, logs)\r\n      else:\r\n        if numpy_logs is None:  # Only convert once.\r\n          numpy_logs = tf_utils.to_numpy_or_python_type(logs)\r\n        callback.on_epoch_end(epoch, numpy_logs)\r\n```", "@omalleyt12 @fchollet Could you take a look at this issue? This seems to break many existing callbacks, so it would be great if this could be fixed, or documented clearly that `logs`  shouldn't be mutated.", "I made a PR to fix this: https://github.com/tensorflow/tensorflow/pull/47922", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45895\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45895\">No</a>\n"]}, {"number": 45894, "title": "tf.data.experimental.assert_cardinality incompatible with INFINITE_CARDINALITY", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**: `tf.data.experimental.assert_cardinality`  can be used to fix `Dataset.cardinality` in instances where it cannot be inferred. In situations where the cardinality is infinite, this raises an error where it shouldn't.\r\n\r\n**Describe the expected behavior** if `ds = ds_base.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY)`, then an error should be raised if `ds_base` stops producing elements, as opposed to when the first element is produced.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Notebook](https://colab.research.google.com/drive/13s9geZzR4xUtYJ6ImINl0kuisL-n8H_6?usp=sharing)\r\n\r\nCode (same as notebook):\r\n```python\r\nimport tensorflow as tf\r\n\r\nds = tf.data.Dataset.range(5).repeat().flat_map(lambda i: tf.data.Dataset.range(i))\r\nprint(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # False\r\nds = ds.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY))\r\nprint(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # True\r\n\r\nfor example in ds.take(1):\r\n    pass\r\n# tensorflow.python.framework.errors_impl.FailedPreconditionError:\r\n# Input dataset was expected to contain -1 elements but contained at least 1 element.\r\n```", "comments": ["@jackd,\r\nAs per [the documentation](https://www.tensorflow.org/api_docs/python/tf/data#other-members), this behavior seems to be expected. What do you think?", "I think `tf.data.INFINITE_CARDINALITY` should consistently be used to indicate infinite cardinality, regardless of how it is encoded. I can understand why this error occurs, but I would consider it a bugged edge-case.\r\n\r\nAt the very least I would expect an error when creating  `tf.data.experimental.assert_cardinality` with a negative `cardinality` value, though that would just make me change this from a bug report to a feature request.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45894\">No</a>\n", "Thanks for reporting this @jackd! I agree with your assessment that it's a bugged edge-case, and submitted a fix so that asserting infinite cardinality will work as you described."]}, {"number": 45893, "title": "Disable failing test", "body": "", "comments": []}, {"number": 45892, "title": "Can't use contrib.slim in tf 1.14", "body": "I am running on google colab. \r\n`import tensorflow\r\nprint(tensorflow.__version__)`\r\nafter running this cell the output is 1.14.0 so I assume the above command `!pip install tensorflow==1.14` worked\r\nbut i have the following error: \r\n`import tensorflow.contrib.slim as slim`\r\n---> ImportError: No module named contrib.slim\r\nI've tried changing the import to `import tf_slim as slim` but i get a similar error: ImportError: No module named tf_slim", "comments": ["@andreixxi,\r\n`tensorflow.contrib` exists only in TensorFlow 1.x, which is not actively supported anymore. We'd recommend you to update TensorFlow to the latest stable version v2.4. Thanks!", "@amahendrakar `tensorflow.contrib` does not work in tensorflow 2.4 either, it is deprecated. should i just try another method and give up the whole slim import ?", "@andreixxi,\r\nPlease take a look at [this comment](https://github.com/tensorflow/models/issues/8020#issuecomment-572743379) from a similar issue and let us know if it helps. Thanks!", "@amahendrakar i've tried installing it by using `pip install --upgrade tf_slim` and then `import tf_slim as slim`. i use google.colab for this project. the import works well in the cell, but when i try to `import tf_slim as slim` in a .py script i get the following error: <br>import tf_slim as slim\r\nImportError: No module named tf_slim<br>\r\nEDIT: I commented the import in the script and kept it in the above cell. it seems to work now, thank you !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45892\">No</a>\n"]}, {"number": 45891, "title": "Disable failing test", "body": "", "comments": []}, {"number": 45890, "title": "Need to install tensorflow 1.12.0 for gpt-2, not working", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20 Ulyana\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 12\r\n- Python version: 3.8.3\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying `pip3 install tensorflow==1.12.0`\r\n\r\nWhich returns:\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4)\r\nERROR: No matching distribution found for tensorflow==1.12.0`\r\n\r\n**Describe the expected behavior**\r\n\r\n1.12.0 installing\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@artiblaster,\r\nAs per the [tested build configurations](https://www.tensorflow.org/install/source#cpu), TensorFlow 1.12 requires Python 3.3 - 3.6.\r\n\r\n\r\nVersion | Python version | Compiler | Build tools\r\n-- | -- | -- | --\r\ntensorflow-1.15.0 | 2.7, 3.3-3.7 | GCC 7.3.1 | Bazel 0.26.1\r\ntensorflow-1.14.0 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.24.1\r\ntensorflow-1.13.1 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.19.2\r\ntensorflow-1.12.0 | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.15.0\r\n\r\n\r\nAlso, I'd suggest you to update TensorFlow to the latest stable version v2.4 since TensorFlow 1.x is not actively supported. Thanks! ", "> @artiblaster,\r\n> As per the [tested build configurations](https://www.tensorflow.org/install/source#cpu), TensorFlow 1.12 requires Python 3.3 - 3.6.\r\n> Version \tPython version \tCompiler \tBuild tools\r\n> tensorflow-1.15.0 \t2.7, 3.3-3.7 \tGCC 7.3.1 \tBazel 0.26.1\r\n> tensorflow-1.14.0 \t2.7, 3.3-3.7 \tGCC 4.8 \tBazel 0.24.1\r\n> tensorflow-1.13.1 \t2.7, 3.3-3.7 \tGCC 4.8 \tBazel 0.19.2\r\n> tensorflow-1.12.0 \t2.7, 3.3-3.6 \tGCC 4.8 \tBazel 0.15.0\r\n> \r\n> Also, I'd suggest you to update TensorFlow to the latest stable version v2.4 since TensorFlow 1.x is not actively supported. Thanks!\r\n\r\nGpt-2 requires tensorflow 1.12. I will try a different python, thanks you!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45890\">No</a>\n"]}, {"number": 45889, "title": "Fixed build issue on windows: realpath command not found", "body": "When I build on windows with CUDA enabled I receive the following error immediatly when starting building:\r\n```\r\n3>CUSTOMBUILD : error : no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 1369\r\n3>\t\t_create_local_cuda_repository(<1 more arguments>)\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 1213, in _create_local_cuda_repository\r\n3>\t\tto_list_of_strings(<1 more arguments>)\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 1214, in to_list_of_strings\r\n3>\t\t_cuda_include_path(<2 more arguments>)\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 363, in _cuda_include_path\r\n3>\t\tinc_entries.append(<1 more arguments>)\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/gpus/cuda_configure.bzl\", line 363, in inc_entries.append\r\n3>\t\trealpath(repository_ctx, <1 more arguments>)\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/remote_config/common.bzl\", line 268, in realpath\r\n3>\t\texecute(repository_ctx, <1 more arguments>)\r\n3>\tFile \"C:/workspace/fast/build_release/external/tensorflow/src/tensorflow_download/third_party/remote_config/common.bzl\", line 208, in execute\r\n3>\t\tfail(<1 more arguments>)\r\n3>Repository command failed\r\n3>/usr/bin/bash: realpath: command not found\r\n```\r\n\r\nI found a post on msys2's gitter chat commenting the same error, but fixed it with just adding the -l option to bash when running the command. I testet this with msys from the terminal:\r\n```\r\nPS C:\\dev_tools\\msys64\\usr\\bin> .\\bash.exe -c realpath\r\n/usr/bin/bash: realpath: command not found\r\nPS C:\\dev_tools\\msys64\\usr\\bin> .\\bash.exe -cl realpath\r\nrealpath: missing operand\r\nTry 'realpath --help' for more information.\r\n```\r\nThus adding the -l option helps, why it helps I don't know. From the manual (https://www.gnu.org/software/bash/manual/html_node/Invoking-Bash.html) it says:\r\n\r\n> -l Make this shell act as if it had been directly invoked by login. When the shell is interactive, this is equivalent to starting a login shell with \u2018exec -l bash\u2019. When the shell is not interactive, the login shell startup files will be executed. \u2018exec bash -l\u2019 or \u2018exec bash --login\u2019 will replace the current shell with a Bash login shell. See Bash Startup Files, for a description of the special behavior of a login shell.\r\n\r\n\r\nThis pull request just adds this -l option to the realpath command in third_party/remote_config/common.bzl\r\n\r\nSome system info:\r\n* Windows 10\r\n* Python 3.8 64 bit\r\n* MSVC 16.8.4\r\n* Bazel 3.1.0\r\n* CUDA 10.1\r\n* msys2 64 bit 20201109", "comments": []}, {"number": 45888, "title": "TF 2.4.0 with disable_eager_execution twice slowlier than 2.3.1 with disable_eager_execution", "body": "I just upgraded from TF 2.3.1 to TF 2.4.0 and my tf.keras model is at least twice slowlier than it was. Is anyone encountering the same problem ?\r\n\r\nI am on Windows 10 x64 2004\r\nPython 3.8.6\r\nGPU RTX 2070 8 Gb RAM\r\nCPU i9 9900 K\r\n64 Gb RAM, 1Tb SSD\r\nCUDA 11.0\r\nCuDNN 8.0.2\r\nNumpy 1.19.3\r\nand I disabled Keras eager execution (disabled since I use TF 2.0) with the statement: tensorflow.compat.v1.disable_eager_execution()\r\n\r\nThe execution console gives:\r\n--------------------------------------------------------\r\n2020-12-20 17:42:12.138275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-20 17:49:15.949788: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-20 17:49:15.951856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-20 17:49:15.991681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-20 17:49:15.991885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-20 17:49:16.099671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-20 17:49:16.099749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-20 17:49:16.146047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-20 17:49:16.181370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-20 17:49:16.369302: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-20 17:49:16.412993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-20 17:49:16.426034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-20 17:49:16.426310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nModel: \"model\"\r\nLayer (type)                    Output Shape         Param #     Connected to        \r\n__________________________________________________________________________________________________\r\ninput_1 (InputLayer)            [(None, 8)]          0         \r\n__________________________________________________________________________________________________                      \r\ninput_2 (InputLayer)            [(None, 22)]         0                               \r\n__________________________________________________________________________________________________\r\nembtri10 (Embedding)            (None, 8, 16)        235584      input_1[0][0]       \r\n__________________________________________________________________________________________________\r\nfcpos (Dense)                   (None, 19)           437         input_2[0][0]       \r\n__________________________________________________________________________________________________\r\nfctri (FC_SYMETRIQUE)           (None, 46)           5888        embtri10[0][0]      \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 65)           0           fcpos[0][0]              fctri[0][0]         \r\n__________________________________________________________________________________________________\r\nline (Dense)                    (None, 1)            65          concatenate[0][0]   \r\n__________________________________________________________________________________________________\r\nTotal params: 241,974\r\nTrainable params: 241,974\r\nNon-trainable params: 0\r\n\r\n2020-12-20 17:49:16.988484: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-20 17:49:16.989498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-12-20 17:49:16.989582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-20 17:49:16.991351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-20 17:49:16.991616: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-20 17:49:16.991879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-20 17:49:16.992133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-20 17:49:16.992402: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-20 17:49:16.992655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-20 17:49:16.992919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-20 17:49:16.993210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-20 17:49:17.759695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-20 17:49:17.759774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2020-12-20 17:49:17.760494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2020-12-20 17:49:17.763105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6637 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-20 17:49:17.763980: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-20 17:49:17.776001: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2020-12-20 17:49:17.940776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-20 17:49:18.573803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n__________________________________________________________________________________________________\r\nEpoch 1/24\r\n10165/10165 - 95s - loss: 0.7623 - mae: 0.0554 - mse: 0.0053\r\nEpoch 2/24\r\n10165/10165 - 73s - loss: 0.7172 - mae: 0.0518 - mse: 0.0045\r\netc.\r\n__________________________________________________________________________________________________\r\nwhereas with TF 2.31 I use to have, for exactly the same model and data:\r\nEpoch 1/24\r\n10165/10165 - 34s - loss: 0.7643 - mae: 0.0555 - mse: 0.0053\r\nEpoch 2/24\r\n10165/10165 - 34s - loss: 0.7183 - mae: 0.0519 - mse: 0.0046\r\netc.\r\n\r\nMy model is compiled with:\r\nOPTIM = Adam(lr=LR_ADAM)\r\nMODELE.compile(optimizer=OPTIM, loss=MSE_BCE_LOSS, metrics=['mae', 'mse'])\r\n\r\nAnd the Fit statement:\r\nMODELE.fit(x=BATCH(T_JEUX, T_PARAMS, T_SCORES, BATCH_SIZE),\r\n               epochs=NB_EPOCHS_ADAM, shuffle=False, verbose=2)\r\n\r\nWith BATCH being a Sequence\r\n\r\nOlivier", "comments": ["And I add that my GPU utilization has dropped to only 5% !", "@Marmotte06,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet or a Colab Gist to reproduce the issue reported here. Thanks! ", "@rmothukuru \r\nI gave you access to the Python/Keras code in my GitHub repository, but the data files you would need to run it are far too big to be uploaded on GitHub. Don't know how to share them.", "@Marmotte06,\r\nWill it be possible for you to replicate the issue with a simple stand alone code. Thanks! ", "You can speed up the process enabling XLA and jit, and disabling CUDA LAUNCH BLOCKING, but may present some memory allocations problems and random freezes. Use the following before creating variables and models.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf \r\n\r\nos.environ['TF_DEVICE_MIN_SYS_MEMORY_IN_MB'] = '1050'\r\nos.environ['TF_XLA_FLAGS']='--tf_xla_enable_xla_devices --tf_xla_auto_jit=fusible --tf_xla_cpu_global_jit --tf_xla_always_defer_compilation=false --tf_xla_enable_lazy_compilation=true'\r\n\r\nos.environ['TF_ENABLE_GPU_GARBAGE_COLLECTION']='false'\r\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'\r\n\r\nos.environ['CUDA_LAUNCH_BLOCKING'] = '0'\r\n\r\nprint('jit:',tf.config.optimizer.get_jit())\r\ntf.config.optimizer.set_jit(True)\r\nprint('jit:',tf.config.optimizer.get_jit())\r\n```", "@rmothukuru,\r\n\r\nWhat I could do is a special version of my program that bypasses all the data prep phase and load a pre-prepared data set so it immediately runs the Model training. And I could make the data set smaller by a factor 10 or twenty to make it smaller while still having measurable epoch times. \r\nWould that help you understand what's going on ?\r\n\r\nApart from that, I looked at the change log between TF 2.3.1 and TF 2.4 and didn't find anything that would break my code. The only specificities of my code is that I have defined a custom Keras layer and a custom loss function, which bot use TF functions such as **tensor slicing (inputs[:, 0]), tf.concat, tf.matmul, tf.square, tf.cast, tf.sign, tf.nn.sigmoid_cross_entropy_with_logits, tf.reduce_mean**.\r\n**Do you think that could be a problem when compiling the model with TF 2.4 ?**\r\nI used to use the backend K. function is a previous version, but they seem to have disappeared now that Keras only supports the TF backend.", "@cr0m3r0\r\n\r\nThanks for the suggestion, will try. But does this explain why the same model takes 2+ more time to train with TF 2.4 than it used to with TF 2.3 ?\r\nWhen reading the TF 2.4 change log I noticed some issues with the XLA jit compiler on Ampere cards (I am still on a Turing card), but this seems to affect the startup time rather than the epoch time, isn't it ?", "I just made another test with a simpler Keras model, just one embedding layer and two dense layers, no custom layer or custom loss.\r\n\r\nThe result is even worse, each training epoch takes 3 times (x 3) longer with TF 2.4.0 than it was taking with TF 2.31. Same model, same data, same machine.\r\n\r\nSo it's not a problem with the model, it's a problem somewhere between the SW stack and the RTX 2070.\r\nTomorrow I'll check I have the most recent NVidia device driver.", "Ok, I might have a clue on what's going on:\r\n\r\nWhen upgrading from 1.5 to 2.0 I experienced a severe training performance impact due to Eager execution, which I resolved with tensorflow.compat.v1.disable_eager_execution()\r\nThat workaround worked fine until now (TF 2.3.1)\r\n\r\nThis morning I commented out the disable_eager_execution() instruction and... got the exact same bad performance with TF 2.4 than when that instruction is present.\r\n\r\nSo my guess is that I am suffering again the penalty of Eager execution, even though I am trying to disable it (I do not need Eager execution).\r\n\r\n**Hence that performance issue might actually be a bug, i.e. tensorflow.compat.v1.disable_eager_execution() doesn't work anymore.**\r\n\r\nOr, is there a new API to disable Eager execution and avoid the penalty of it ?", "Hi @Marmotte06,\r\n\r\ncan you share a colab gist of the simple (3 layer) model you mentioned above? We know about some gpu embedding-placement-related performance issues on TF 2.x vs TF 1.x, but these shouldn't have arisen between TF 2.3.1 & TF 2.4. (The embeddings get placed on CPU even in cases where they should be placed on gpu for performance reasons).\r\n\r\nOr are you saying in TF 2.3.1 you used to disable eager execution to get your performance and now you aren't disabling it?\r\n\r\nEither way, can you give the OneDeviceStrategy a shot? (Make your keras model inside a https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy that specifies the gpu). That should force the embeddings to be placed on gpu (tf.device won't unfortunately), and you can see if your performance improves then.\r\n\r\n", "@tomerk \r\n\r\nOn TF 2.3.1 I was disabling eager execution to keep the performance I had with TF 1.5 (otherwise I got a x2 training time increase)\r\nWhat I noticed with TF 24 is that I do have the x2 training time increase anyway, eager execution on or off. Which let me think the eager execution trick doesn't work anymore.\r\n\r\nI just tried the OneDeviceStrategy, including the -functional- model layer definitions and model compilation in the strategy scope, but not the fit instruction (I also tried to put the fit() in).\r\n\r\nI used the following statement:\r\nstrategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\r\n\r\nsince when starting TF tells on the console:\r\n2021-01-08 10:48:49.623571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6637 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n\r\nUnfortunately I get the following assertion error:\r\n\r\n  File \"C:\\Users\\Olivier\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 789, in fit\r\n    return func.fit(\r\n  File \"C:\\Users\\Olivier\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed_v1.py\", line 615, in fit\r\n    dataset = model._distribution_standardize_user_data(\r\n  File \"C:\\Users\\Olivier\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 2195, in _distribution_standardize_user_data\r\n    assert isinstance(x, dataset_ops.DatasetV2)\r\nAssertionError\r\n\r\nAny clue ?", "@tomerk\r\n\r\nI made some progress. The \"training_v1\" mentions in the error above let me think my compat.v1 import disable_eager_execution could be the cause of the error. I disabled that statement and the training started, even though I now get the following warning:\r\n\r\n2021-01-08 13:18:37.888518: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\r\nDon't know what that means and how to fix it, but I can see some performance improvement:\r\n\r\nWith 2.3.1 and Eager execution disabled I got 37 seconds / epoch\r\nWith 2.4 off-the-shelf I got 77 seconds / epoch\r\nAnd with 2.4 and OneDeviceStrategy I get 62 seconds / epoch.\r\n\r\nSo, I am not back to the 2.3.1 performance, but it's improving.\r\n\r\nAnd...  it seems I get a slightly better training accuracy:\r\nloss: 0.6421 - mae: 0.0452 - mse: 0.0034   instead of:    loss: 0.6434 - mae: 0.0453 - mse: 0.0035  after the same number of epochs.\r\nMy model is quite reproducible, so that's not an artifact. Any clue for this improvement ? I imagine doing the embedding training on the GPU might give some precision improvement over a mixed CPU/GPU training ?\r\n\r\n", "Yeah the distribution strategy probably won't work when you disable eager execution (effectively entering TF1 graphs/execution).\r\n\r\nThe remaining performance gap does sound like something we need to look into. As requested above can you share a colab gist of your simpler three-layer model repro?\r\n\r\nIt would also be really helpful if you could share TF profiler traces from these different runs in tf 2.3.1 & tf 2.4:\r\nhttps://www.tensorflow.org/guide/profiler", "Ok, I'll have a look at the profiler. However I don't have the 2.3.1 anymore. But maybe I'll roll back to it to get back my original performances until the 2.4 improves.\r\n\r\nI really like your idea of putting the embeddings on the GPU (and the improved accuracy), but I get tons of those \"sharding\" warnings that are very disturbing. I did some research and found I would have to change all my numpy arrays to Datasets to get rid of those warning, but that would probably change all my Sequence logic, so I don't want to go that path.\r\nI liked Keras for it's simplicity, but from version to version it is getting to a level of complexity that makes me wonder whether that's still the right choice. Maybe I should have a look to PyTorch.", "I also have this warning from TF 2.4\r\n2021-01-08 13:18:37.898614: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n\r\nDoes this have any performance impact ?", "In an effort to turnaround the problem, I rewrote my Sequence generator logic into a plain tf.data.Dataset logic.\r\nAfter struggling a bit with the the necessary steps, parameters, options, etc. it eventually works fine.\r\nThe good news is that don't have lengthy warning messages anymore and that the new code is much cleaner with the Dataset directly created from my Numpy arrays, all the code for the Sequence generator disappears.\r\nThe bad news is that I still have 64 seconds / epoch, i.e. twice the time I got with 2.3.1.\r\n\r\nThe next step is therefore to take the time to use the Profiler as suggested by @tomerk \r\n\r\nApart from the inconvenience of a doubled compute time, I definitively confirm that I get a better accuracy by moving the embeddings to the GPU, hence I am happy anyway with the move and will convert all my Sequences to Datasets.", "After struggling to install TensorBoard and the Profiler on Windows 10 (the cubti DLL has changed name with CUDA 11), I could eventually run a profiling session.\r\nThere is a lot of Profiling information, I don't really know where to look.\r\nBut the following figure strikes me, even though I suspected it given my 4% GPU utilization:\r\nTF Op Placement:\r\n    Host: 95.7%\r\n    Device: 4.3%\r\nIn the bottleneck analysis I could find this:\r\nSummary of All Input Pipelines\r\nHost              Input Pipeline       Min (us)       Avg (us)       Max (us)      # call    # slow calls\r\nPC-OLIVIER      Device:0                2                  2                 12              21             0\r\nPC-OLIVIER      Host:0                6,193          7,842           10,094           21           21\r\nfollowed by this recommendation:\r\n 1. Check the locality of a host and input data. Ideally, they should be in the same cell (or very close, like the same region).\r\n2. Parallelize reading from this dataset source\r\n\r\nWhat does that mean, knowing that all my data is loaded in RAM ?    (I have 64 GB of RAM and an i9 9900K)\r\n\r\nIn the statistics I can see:\r\n                                                         #Occurrences     Total self-time (us)    Total self-time on Host (%)\r\nDataset                                                  156,589             163,071                          97.7%\r\nFlushSummaryWriter                                   1                    3,624                            2.2%\r\nMultiDeviceIteratorFromStringHandle        21                     154                              0.1%\r\nMultiDeviceIteratorGetNextFromShard       21                      78                               0%\r\nLogicalAnd                                                  1                       20                               0%\r\nWriteSummary                                            1                        7                                0%\r\nIdentity                                                       1                        0                                0%\r\n\r\nAny clue ?\r\n", "I did additional testing:\r\nThe above figure (64 sec / epoch) was obtained with a dataset created with tf.data.Dataset.from_tensor_slices(), knowing that all my training data is available in RAM in Numpy arrays.\r\nHowever, my Numpy arrays a quite big, up to tens of Gigabytes (the biggest datasets saturate my 64 Gb RAM), hence I tried the recommended approach with a Python Generator that yields rows of my Numpy arrays, one at a time, and tf.data.Dataset.from_generator() of that Generator, plus repeat(), batch(4096) and prefetch(autotune).\r\n\r\nThe result is just terrible, each epoch now takes forever and my GPU usage is ZERO % !\r\n\r\nWhat's going on ???", "After upgrading to 2.4 while still using a Turing card, I initially found that my average inferencing algorithms had the same time required, maybe 0.1% longer. However it increased in time required by 150% of the time req. in 2.3, increasing over time. This average was over the entire period so after running for at least 12 hours, the time required may have increased to 200% per iteration. I am using a turing card with cuda 11. \r\n\r\nI noticed that my GPU usage kept dropping really badly. Instead of when I restart and having full usage with the occasional spike it was probably only utilizing 100% down to 80% text on average and was dropping at very low utilization. This leads me to believe that XLA wasn't going very well.\r\n\r\n> You can speed up the process enabling XLA and jit, and disabling CUDA LAUNCH BLOCKING, but may present some memory allocations problems and random freezes. Use the following before creating variables and models.\r\n> \r\n> ```\r\n> import os\r\n> import tensorflow as tf \r\n> \r\n> os.environ['TF_DEVICE_MIN_SYS_MEMORY_IN_MB'] = '1050'\r\n> os.environ['TF_XLA_FLAGS']='--tf_xla_enable_xla_devices --tf_xla_auto_jit=fusible --tf_xla_cpu_global_jit --tf_xla_always_defer_compilation=false --tf_xla_enable_lazy_compilation=true'\r\n> \r\n> os.environ['TF_ENABLE_GPU_GARBAGE_COLLECTION']='false'\r\n> os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'\r\n> \r\n> os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\r\n> \r\n> print('jit:',tf.config.optimizer.get_jit())\r\n> tf.config.optimizer.set_jit(True)\r\n> print('jit:',tf.config.optimizer.get_jit())\r\n> ```\r\n\r\nI already had XLA enabled for CUDA but I will try with your recommended system variables and report back. ", "![memerror](https://user-images.githubusercontent.com/47997656/105098296-b67cf400-5a5e-11eb-9f2e-0b5f48a1c368.png)\r\nI'm unsure if this is significant but here's what happened at one run when I tried to run a model, I got a memory access error.\r\n\r\nTF_XLA_FLAGS = --tf_xla_enable_xla_devices --tf_xla_auto_jit=fusible --tf_xla_cpu_global_jit --tf_xla_always_defer_compilation=false --tf_xla_enable_lazy_compilation=true\r\n\r\n'TF_DEVICE_MIN_SYS_MEMORY_IN_MB': '1050',\r\n'TF_ENABLE_GPU_GARBAGE_COLLECTION': 'false',\r\n'TF_FORCE_GPU_ALLOW_GROWTH': 'true',\r\n'TF_XLA_FLAGS': '--tf_xla_enable_xla_devices --tf_xla_auto_jit=fusible --tf_xla_cpu_global_jit --tf_xla_always_defer_compilation=false --tf_xla_enable_lazy_compilation=true',\r\n\r\nBy the way my terminal keeps getting \r\ntensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0", "I've tried 3-5+ times trying to get it to run with \r\n--tf_xla_auto_jit=fusible\r\n\r\nhowever I can't use that flag. It either gives that error or xla fails to compile my functions and it just hangs, eventually closing the kernel.\r\n\r\nIt yells at me a lot with the subprocess error but I was able to get it to run without any\r\n--tf_xla_auto_jit\r\n\r\nflag\r\n\r\n```\r\n'CUDA_LAUNCH_BLOCKING': '0',  \r\nTF_DEVICE_MIN_SYS_MEMORY_IN_MB': '1050',  \r\n'TF_ENABLE_GPU_GARBAGE_COLLECTION': 'false',  \r\n'TF_FORCE_GPU_ALLOW_GROWTH': 'true',  \r\n\r\n'TF_XLA_FLAGS': '--tf_xla_enable_xla_devices --tf_xla_cpu_global_jit --tf_xla_always_defer_compilation=false --tf_xla_enable_lazy_compilation=true'\r\n```\r\n\r\nI am also using \r\ntf.config.optimizer.set_jit(True)\r\n\r\nI wonder if this has to do with hardware-accelerated gpu scheduling?", "@Marmotte06 sorry this got lost in my inbox!\r\n\r\nThe most performant way to turn large numpy array to datasets is to use `from_numpy` from TensorflowIO: https://www.tensorflow.org/io/api_docs/python/tfio/experimental/IODataset#from_numpy\r\n\r\n(It avoids the need for memory copies which from_tensor_slices introduces).\r\n\r\nFrom_generator generally isn't very performant because you get bottlenecked by the limitations of python, and don't get the full performance advantages of tf.data.\r\n\r\n@taesookim0412 if you haven't done so already, could you open a separate bug and include context around your example, your operating system & drivers, a small code repo if possible, and a profile? As-is what you've reported so far isn't super actionable for us. The comment you quoted itself noted that some of those systems environments can introduce memory allocation issues and crashes, so it's not surprising to me you're running into memory crashing issues when testing them out.", "Thanks @tomerk , I just had a try with tfio.from_numpy. The good news is that it works fine and is an easy replacement to tf.from_tensor_slices.\r\nHowever, the time to process an epoch is exactly the same, I see absolutely no difference.\r\nBut I need to have another try with one of my huge datasets (60 Gb) which makes tf.from_tensor_slices crash brutally without any error message.\r\nMaybe tfio.from_numpy will luckily pass it...", "I imagine tfio.from_numpy will be much better at handling your large datasets. without sharing the actual profiles from your traces though, it's hard to say more about the cause of your performance issues. (If you share traces from the profiler we'd be happy to take a look)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing because this is non-actionable for us currently. Feel free to open a new issue (Under a more relevant title) if/when you have profile trace files to share that we can look at in detail.", "@tomerk , I now confirm that tfio.experimental.IODataset.from_numpy allows to process much larger numpy arrays than tf.data.Dataset.from_tensor_slices or tf.data.Dataset.from_tensors, thanks a lot for the advise.\r\n\r\nA couple of questions:\r\n1. In the tfio doc I see there is also a tfio.v0.IODataset, would you know what's the difference with the experimental version ?\r\n2. In the options for the OneDeviceStrategy I am setting experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA . Is that correct, or would AUTO or OFF be better ?\r\n\r\nThanks again"]}, {"number": 45887, "title": "Cannot quantize only part of a QAT model for running on both Android CPU/DSP", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0-dev20201208\r\n- Python version: 3.7\r\n\r\n**Standalone code to reproduce the issue**\r\nWhat I'm trying to do is to quantize part of a model, running quantize-aware training, and then deploy it to run on both Android CPU/DSP for inference. The code is below:\r\n\r\n```\r\ndef apply_quantization(layer):\r\n    if isinstance(layer, keras.layers.Conv2D):\r\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n    return layer\r\n\r\nannotated_model = keras.models.clone_model(\r\n    base_model, # base_model is a mobilenetv2 or resnet50\r\n    clone_function=apply_quantization\r\n)\r\nq_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\n```\r\nThen I try to convert it to a quantized tflite model (with only weights)\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.convert()\r\n```\r\nThe tflite model works, but can only run on CPU but no op is delegated to DSP because the activations are not quantized (am I right?)\r\n\r\nSo I tried to quantize both weights and activations:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.allow_custom_ops = True\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\n```\r\nThe tflite model is generated, but I cannot initialize an interpreter with this model with following error:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-13-3d7cc3476638> in <module>\r\n      1 interpreter = tf.lite.Interpreter(model_content=m7)\r\n----> 2 interpreter.allocate_tensors()\r\n      3 input_index = interpreter.get_input_details()[0][\"index\"]\r\n      4 output_index = interpreter.get_output_details()[0][\"index\"]\r\n      5 print (input_index, output_index)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n    333   def allocate_tensors(self):\r\n    334     self._ensure_safe()\r\n--> 335     return self._interpreter.AllocateTensors()\r\n    336 \r\n    337   def _safe_to_run(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 5 (DEQUANTIZE) failed to prepare.\r\n```\r\nI've checked the generated tflite model, it seems there're two consecutive DEQUANTIZE op inserted. Any one knows how to fix it?", "comments": ["Hi xumengwei@,\r\n\r\nThe current setting is a little strange since you have chosen to fully quantize the model with post-training quantization but only quantized-aware training for convolution layers. The current tooling is a little confused and adds inappropriate dequantize/quantize ops expecting the input model to not be quantized. We are working on a fix for this.\r\n\r\nFor the meantime, I verified that our new quantizer tooling handles the input qat model appropriately.\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.allow_custom_ops = True\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\n# add this line before convert()\r\nconverter._experimental_new_quantizer = True\r\n\r\ntflite_model = converter.convert()\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "@daverim Thanks for your reply. I understand what I'm doing might sound minority. I want to quantize only certain layers (e.g., the 1st, 3rd conv layers) and keep others (e.g., the 2nd, 4th conv) as float32.\r\n\r\nI've tried the way you suggested using `_experimental_new_quantizer`. However, what I got is a fully quantized model instead of only some of them. As a result, the whole model is delegated to DSP. Do you have any idea how can I achieve my goal?", "@xumengwei, ah that makes more sense, in that case, you don't want to apply post-training quantization, just quantize-aware-training conversion. Try the following:\r\n\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\n```", "@daverim I've tried the way you suggested. Here is the complete code:\r\n\r\n```\r\nMODEL_NAME = 'mobilenetv2'\r\nbase_model = keras.applications.MobileNetV2(\r\n    input_shape=(224,224,3), alpha=1.0, include_top=True, weights='imagenet',\r\n    input_tensor=None, pooling=None, classes=1000,\r\n    classifier_activation='softmax'\r\n)\r\ndef apply_quantization(layer):\r\n    if type(layer) == keras.layers.Conv2D:\r\n        return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n    return layer\r\n\r\nannotated_model = keras.models.clone_model(\r\n    base_model,\r\n    clone_function=apply_quantization\r\n)\r\n\r\nq_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\n# converter._experimental_new_quantizer = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\npathlib.Path('/tmp/tmp.tflite').write_bytes(tflite_model)\r\n```\r\n\r\nBasically I tried to quantize only Conv2D layers of a mobilenetv2 model. However, the generated model is quite weird: the `quantize` and `dequantize` layers always come after Conv2D layer together... Isn't the `quantize` layer before the Conv2D layer?\r\n\r\n![image](https://user-images.githubusercontent.com/23546158/102842444-f5a92c80-4441-11eb-80f6-ad3c3f868116.png)\r\n", "@daverim I've also tried a simpler, manually constructed model with direct quantization annotation `tfmot.quantization.keras.quantize_annotate_layer`. The code is following.\r\n\r\n```\r\ninputs = keras.layers.Input(shape=(224, 224, 3))\r\nx1 = keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)\r\nx2 = keras.layers.Lambda(lambda x: tf.identity(x))(inputs)\r\nx2 = tfmot.quantization.keras.quantize_annotate_layer(\r\n    keras.layers.Conv2D(32, (3, 3), activation='relu')\r\n)(x2)\r\nx = keras.layers.concatenate(inputs = [x1,x2])\r\nx = keras.layers.MaxPool2D([5,5], strides=5)(x)\r\nx = keras.layers.Flatten()(x)\r\nx = keras.layers.Dense(2)(x)\r\nbase_model = keras.models.Model(inputs=inputs, outputs=x)\r\nq_aware_model = tfmot.quantization.keras.quantize_apply(base_model)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter._experimental_new_quantizer = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\npathlib.Path('/tmp/tmp.tflite').write_bytes(tflite_model)\r\n```\r\nIn this model, two conv layers take the same input and then their outputs are combined. I only quantize one of them. However, in the generated tflite model, both two conv layers are quantized...\r\n<img width=\"370\" alt=\"Screen Shot 2020-12-23 at 10 47 00 AM\" src=\"https://user-images.githubusercontent.com/23546158/102953409-32415a80-450c-11eb-980e-162f7a8ba921.png\">\r\n", "@daverim can you help or involve other people who may be familiar with this issue?", "Hi sorry for the delay, realized there were two bugs with this issue: workaround is posted  in\r\n\r\nhttps://github.com/tensorflow/model-optimization/issues/523", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45887\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45887\">No</a>\n"]}, {"number": 45886, "title": "Problem getting python include path", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Cygwin\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.4.0-rc2\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n\r\n**Describe the problem**\r\n\r\n```\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl\", line 212, column 41, in _create_local_python_repository\r\n                python_include = _get_python_include(repository_ctx, python_bin)\r\n        File \"C:/cygwin64/home/username/tensorflow/third_party/py/python_configure.bzl\", line 152, column 21, in _get_python_include\r\n                result = execute(\r\n        File \"C:/cygwin64/home/username/tensorflow/third_party/remote_config/common.bzl\", line 217, column 13, in execute\r\n                fail(\r\nError in fail: Problem getting python include path.\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"C:\\cygwin64\\home\\username\\_bazel_username\\5ud3wb25\\external\\local_config_python\\usr\\bin\\python3\" -c \"from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())\"): The system cannot find the file specified.\r\n (error: 2)\r\nIs the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?\r\nINFO: Repository llvm-project instantiated at:\r\n  C:/cygwin64/home/username/tensorflow/WORKSPACE:19:16: in <toplevel>\r\n  C:/cygwin64/home/username/tensorflow/tensorflow/workspace.bzl:690:20: in tf_repositories\r\nRepository rule tf_http_archive defined at:\r\n  C:/cygwin64/home/username/tensorflow/third_party/repo.bzl:131:34: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Problem getting python include path.\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"C:\\cygwin64\\home\\username\\_bazel_username\\5ud3wb25\\external\\local_config_python\\usr\\bin\\python3\" -c \"from __future__ import print_function;from distutils import sysconfig;print(sysconfig.get_python_inc())\"): The system cannot find the file specified.\r\n (error: 2)\r\nIs the Python binary path set up right? (See ./configure or PYTHON_BIN_PATH.) Is distutils installed?\r\nINFO: Elapsed time: 2.198s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)\r\n    Fetching @local_execution_config_python; fetching\r\n```\r\n\r\n`C:\\cygwin64\\home\\username\\_bazel_username\\5ud3wb25\\external\\local_config_python\\` is an empty directory...\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nany of the following\r\n```\r\n$ bazel build '//tensorflow/tools/pip_package:build_pip_package'\r\n$ env PYTHON_BIN_PATH='/usr/bin' bazel build '//tensorflow/tools/pip_package:build_pip_package'\r\n$ env PYTHON_BIN_PATH='C:/cygwin64/usr/bin' bazel build '//tensorflow/tools/pip_package:build_pip_package'\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI would of assumed it has something to do with bazel getting confused by cygwin paths (e.g. it can not handle any paths that have a space character in their own build script), but that would not explain why it is using such a bizarre path in the first place.\r\n\r\nbazel can not be compiled from source, so that is not an option.\r\ntensorflow can not be installed via pip, so that is not an option.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45886\">No</a>\n", "As a desperate measure I tried the windows version of python3.8 and, WOW, it worked, I could install tensorflow... unfortunately the python program I am using depends on version 1 of tensorflow and assumes unix/linux system behavior, and tensorflow 1.8-1.15 can not be installed under any version of python(cygwin or windows)... so compiling tensorflow from source is still a general requirement.  \r\n\r\nThis problem may extend back to tensorflow versions before version 2, which would render the software I am trying to use effectively dead without a fix, which is how I am now forced to regard it lest more time be wasted.", "@9607835,\r\nCould you please explain in detail the issue you are facing and also provide the exact sequence of commands that you executed before running into the error, so that we can look into this. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45886\">No</a>\n"]}, {"number": 45885, "title": "Colab session getting restarted during conversion of the Boundless model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.4.0\r\n\r\n\r\n## Colab Notebook for reproducing the issue\r\nhttps://colab.research.google.com/gist/sayakpaul/baf963933cbc3e627e66f6330d66c519/boundless_tflite.ipynb\r\n\r\n## Issue\r\n\r\nWhen trying to convert the [Boundless model](https://tfhub.dev/google/boundless/quarter/1) Colab Notebook session is getting restarted and the converter is unable to generate the TensorFlow Lite model. \r\n\r\n## Code\r\n\r\n```python\r\nmodel_handle = https://tfhub.dev/google/boundless/quarter/1\r\nmodel = hub.load(model_handle)\r\nconcrete_function = model.signatures['default']\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**Note** that I did try to run the conversion without `converter.optimizations = [tf.lite.Optimize.DEFAULT]` and it does not help. \r\n\r\n## Logs\r\n\r\n```\r\nWARNING:tensorflow:Issue encountered when serializing model_variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\nWARNING:tensorflow:Issue encountered when serializing model_variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\nWARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\nWARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\nWARNING:tensorflow:Issue encountered when serializing trainable_variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\nWARNING:tensorflow:Issue encountered when serializing trainable_variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nto_proto not supported in EAGER mode.\r\n```\r\n\r\n## Useful references\r\n\r\nThe pre-trained Boundless model on TensorFlow Hub comes with [this tutorial](https://www.tensorflow.org/hub/tutorials/boundless). \r\n\r\nAnything I am missing out on during the conversion process? ", "comments": ["Cc: @MeghnaNatraj ", "I can confirm that there is an issue with TF 2.4.0 version when the boundless TF hub model conversion is being conducted.\r\n\r\nBTW, you can convert the above model with Flex through TF nightly version.", "Works with the suggestions given in https://github.com/tensorflow/tensorflow/issues/45885#issuecomment-748736435. For anyone interested in checking out the updated Colab Notebook, refer [here](https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Boundless_TFLite.ipynb). "]}, {"number": 45884, "title": "Disable failing test", "body": "", "comments": []}, {"number": 45883, "title": "Model.test_on_batch reset_metrics incorrect", "body": "URL with issue: [Model.test_on_batch](https://www.tensorflow.org/api_docs/python/tf/keras/Model#test_on_batch)\r\n\r\n## Description of issue (what needs changing):\r\n`reset_metrics` states: \r\n\r\n> If\u00a0True, the metrics returned will be only for this batch. If\u00a0False, the metrics will be statefully accumulated across batches.\r\n\r\nThis is incorrect, as metric states are only reset AFTER values are computed. See example below.\r\n\r\nThis could be considered a bug, but it might be considered a breaking change to fix it, and addressing it as a documentation issue may be preferable.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Counter(tf.keras.metrics.Metric):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.count = self.add_weight('count', dtype=tf.int64, initializer='zeros')\r\n\r\n    def update_state(self, *args, **kwargs):\r\n        self.count.assign_add(1)\r\n\r\n    def result(self):\r\n        return self.count\r\n\r\n\r\ntf.random.set_seed(0)\r\ninp = tf.keras.Input((1,))\r\nout = tf.keras.layers.Dense(2, activation='softmax')(inp)\r\nmodel = tf.keras.Model(inp, out)\r\n\r\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[Counter()])\r\nbatch_size = 1\r\nx = tf.zeros((batch_size, 1,), dtype=tf.float32)\r\ny = tf.zeros((batch_size,), dtype=tf.int64)\r\nmodel.train_on_batch(x, y, reset_metrics=False)\r\nlogs = model.test_on_batch(x, y, reset_metrics=True, return_dict=True)\r\n#  logs contains metrics for both both steps\r\nprint(f\"counter = {logs['counter']}\")  # counter = 2\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/61d5f07d5daa4ed4355b9383b8e8f03f/45883.ipynb#scrollTo=fLJhsUW1Swlj). Thanks!", "@jackd,\r\nCan you please elaborate on what the Expected Behavior should be? Thanks! ", "Based on the documentation, I would expect counter to always be `1` when `return_dict=True` (i.e. `reset_metrics` should be called before metrics are updated and results are computed, rather than afterwards).", "@jackd,\r\nWould you be interested to submit a pull request to fix the issue? \r\n\r\nIf so, please see the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rmothukuru sorry, work's been busy. Are you looking for a bug fix or just making the documentation consistent with the current behaviour?", "I'm able to to see the expected output, `counter=1`, [here](https://colab.research.google.com/gist/sachinprasadhs/a18cb67463c00482aa447a546fff331f/45883.ipynb) is the gist for the same.\r\nPlease move this issue to close if your issue is resolved. Thanks!", "Looks to work as of 2.7, thanks.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 45882, "title": "Ubuntu18.04 running on WSL2: \"libcuda.so.1\" does not exist. How to create that?", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n\r\n```\r\ndef test_sum():\r\n    assert sum([1, 2, 3]) == 6, \"Should be 6\"\r\n\r\nif __name__ == \"__main__\":\r\n    test_sum()\r\n    print(\"Everything passed\")\r\n\r\n    # import tensorflow as tf\r\n    import tensorflow.compat.v1 as tf       # To get TF 1.x like behaviour in TF 2.0 one can run\r\n    tf.disable_v2_behavior()\r\n\r\n    print(tf.__version__)\r\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 running on WSL2\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:: installed using conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA11.0/ cuDNN8\r\n- GPU model and memory: \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 running on WSL2\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.4.0\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:: installed using conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA11.0/ cuDNN8\r\n- GPU model and memory: Quadro RTX 4000, 8GB\r\n\r\n\r\n\r\n\r\n-   **Exact command to reproduce**:\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\n\r\n### Describe the problem\r\nThe number of GPU available is being shown to be zero. \r\n\r\nI am getting following output:\r\n \r\n```\r\n(base) dushyant@DESKTOP-U96RKFC:/mnt/c/Windows/System32$ python3 /home/$USER/test.py\r\nEverything passed\r\n2020-12-19 12:47:55.757762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nWARNING:tensorflow:From /home/dushyant/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2.4.0\r\n2020-12-19 12:47:56.642478: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-19 12:47:56.645707: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\r\n2020-12-19 12:47:56.645745: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-12-19 12:47:56.645767: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-U96RKFC): /proc/driver/nvidia/version does not exist\r\nNum GPUs Available:  0\r\n```\r\nCould not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\r\n\r\nI searched for libcuda.so.1 in my directory: \"LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:\" and elsewhere. It does not seem to exist.\r\n\r\nI also search for 'libcuda.so*' and found the following:\r\n```\r\n(base) dushyant@DESKTOP-U96RKFC:/mnt/c/Windows/System32$ find /usr/ -name 'libcuda.so*'\r\n/usr/local/cuda-11.0/doc/man/man7/libcuda.so.7\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/stubs/libcuda.so\r\n```\r\n\r\nI am under impression that I can create \"libcuda.so.1\". However, I have no idea how to create that. Any help would be appreciated.\r\n", "comments": ["@dushyantkumr1,\r\nCould you please let us know if you have installed CUDA/cuDNN on Windows or on WSL?\r\n\r\nAlso, please go through [this guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html) from NVIDIA and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45882\">No</a>\n", "Sounds like Nvidia does not provide the runtime support for WSL2.\r\n\r\nI use the `CMakeList.txt` file to make my test program compilable\r\n```cmake\r\ncmake_minimum_required(VERSION 3.11)\r\nproject(test_llvmgpu)\r\n\r\nset(CMAKE_CXX_FLAGS \"-I/usr/local/cuda/include \")\r\nset(CMAKE_CXX_STANDARD 20)\r\n\r\nadd_executable(test_llvmgpu main.cpp)\r\ntarget_link_libraries(test_llvmgpu /usr/local/cuda-11.0/targets/x86_64-linux/lib/stubs/libcuda.so)\r\n```", "Hi Abhilash,\n\nYes, I was able to install that.\n\nWould it be okay if I respond in a couple of days as I am busy with some\nimportant submission? It will also take me some time to remember what\nexactly I did.\n\nYou can email me at ***@***.***\n\nRegards,\nDushyant\n\n==============================\n\nDushyant Kumar\n\nDept. of Radiology,\n\nUniversity of Pennsylvania,\n\nPhiladelphia, PA, 19104\n==============================\n\n\n\nOn Mon, Dec 21, 2020 at 7:32 AM Abhilash Mahendrakar <\n***@***.***> wrote:\n\n> @dushyantkumr1 <https://github.com/dushyantkumr1>,\n> Could you please let us know if you have installed CUDA/cuDNN on Windows\n> or on WSL?\n>\n> Also, please go through this guide\n> <https://docs.nvidia.com/cuda/wsl-user-guide/index.html> from NVIDIA and\n> let us know if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45882#issuecomment-748950736>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APHJE7QA6Z2EJ3KIFEZUSADSV455PANCNFSM4VCPJ6RA>\n> .\n>\n"]}, {"number": 45881, "title": "When tried to Make Project or Build the app got an error \"Task :lib_task_api:downloadModelFile FAILED\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Emulator Pixel 3 API 29\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.3 \r\n\r\n**Describe the current behavior**\r\nWhen try to Debug or Run, Android Studio gives the following error (Sometimes it happen when I do Make Project too):\r\n```\r\n> Task :lib_interpreter:downloadModelFile\r\nDownload https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite\r\n\r\n> Task :lib_interpreter:downloadModelFile FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':lib_interpreter:downloadModelFile'.\r\n> javax.net.ssl.SSLException: Connection reset\r\n\r\n```\r\nThere was no issue with the internet connection at that time. However, this work without errors previously, suddenly started getting this error.\r\n\r\n**Describe the expected behavior**\r\nIt should build without errors.\r\n\r\n**Standalone code to reproduce the issue**\r\nDownload text_classification example from https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android\r\nAfter downloading, try to Make Project or Run/Debug. \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nWARNING: The specified Android SDK Build Tools version (29.0.0) is ignored, as it is below the minimum supported version (29.0.2) for Android Gradle Plugin 4.0.0.\r\nAndroid SDK Build Tools 29.0.2 will be used.\r\nTo suppress this warning, remove \"buildToolsVersion '29.0.0'\" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.\r\nWARNING: The specified Android SDK Build Tools version (29.0.0) is ignored, as it is below the minimum supported version (29.0.2) for Android Gradle Plugin 4.0.0.\r\nAndroid SDK Build Tools 29.0.2 will be used.\r\nTo suppress this warning, remove \"buildToolsVersion '29.0.0'\" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.\r\n> Task :app:preBuild UP-TO-DATE\r\n> Task :app:preInterpreterDebugBuild UP-TO-DATE\r\n\r\n> Task :lib_interpreter:downloadModelFile\r\nDownload https://storage.googleapis.com/download.tensorflow.org/models/tflite/text_classification/text_classification_v2.tflite\r\n\r\n> Task :lib_interpreter:downloadModelFile FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':lib_interpreter:downloadModelFile'.\r\n> javax.net.ssl.SSLException: Connection reset\r\n\r\n* Try:\r\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\r\n\r\n* Get more help at https://help.gradle.org\r\n\r\nBUILD FAILED in 22s\r\n1 actionable task: 1 executed\r\n```\r\n", "comments": ["This issue was solved after I moved to another network (changed internet connection). Previously this issue also poped up when I tried to update Android Studio.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45881\">No</a>\n"]}, {"number": 45880, "title": "When tried to Make Project or Build the app got an error \"", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45880\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45880\">No</a>\n"]}, {"number": 45878, "title": "Fix audio bugs in the Arduino micro_speech example", "body": "In the Arduino audio_provider, the LatestAudioTimestamp increments twice as fast as real-time because the audio code confuses bytes and samples (one sample is two bytes).\r\n\r\nRelated to this, the feature_provider sometimes attempts to read too far ahead from the audio capture buffer.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "I spent quite a bit of time verifying that the feature provider gives the exact same spectrograms as the TF training script. With my changes to the feature provider, the spectrograms computed by the Arduino code are exactly the same as the ones computed by the TF op, when given the same WAV input. (Except for occasional rounding errors, most likely because of the quantization that happens on the Arduino.)\r\n\r\nIf the test case fails now, it's probably because there is a bug in the test case. ;-)", "I'm not entirely sure what I need to do to change this PR to only include the audio capture changes. Undo the feature provider changes in my local copy and commit?", "OK I guess that worked. :-)\r\n\r\nAnother thing I should mention that is related to the audio capture is the following definition:\r\n\r\n```\r\n// The size of the input time series data we pass to the FFT to produce the\r\n// frequency information. This has to be a power of two, and since we're dealing\r\n// with 30ms of 16KHz inputs, which means 480 samples, this is the next value.\r\nconstexpr int kMaxAudioSampleSize = 512;\r\n```\r\n\r\nThis comment is misleading. It's OK to define `kMaxAudioSampleSize = 480` because the FFT will copy this data into its own buffer and pad it out to the nearest power of two.\r\n\r\nIn `GetAudioSamples` in arduino/audio_provider.cc, I would personally change the line,\r\n\r\n```\r\n*audio_samples_size = kMaxAudioSampleSize;\r\n```\r\n\r\nto:\r\n\r\n```\r\n*audio_samples_size = duration_sample_count;\r\n```\r\n\r\nand possibly include an assertion that makes sure `duration_sample_count <= kMaxAudioSampleSize`. The code will work either way, but now the `audio_samples_size` is correct whenever the requested duration is less than 30ms.\r\n", "I opened a new issue for the feature_provider changes: https://github.com/tensorflow/tensorflow/issues/46365", "I encountered the same problem when preparing/testing a simulation in [Renode](https://renode.io).\r\n\r\nTesting on the looped [mock data](https://github.com/tensorflow/tensorflow/blob/4c63c27f015d09c35e11506651f0642245d522ef/tensorflow/lite/micro/examples/micro_speech/yes_1000ms_sample_data.cc#L21), original code gives me the following results:\r\n```\r\n4 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.\r\nHeard yes (210) @2016ms\r\nHeard yes (201) @4512ms\r\nHeard yes (204) @6240ms\r\nHeard yes (203) @8512ms\r\nHeard yes (211) @10272ms\r\nHeard yes (201) @12480ms\r\nHeard yes (203) @14240ms\r\nHeard yes (207) @16512ms\r\nHeard yes (225) @18272ms\r\nHeard yes (206) @20544ms\r\nHeard yes (205) @22304ms\r\nHeard yes (209) @26272ms\r\nHeard yes (206) @28544ms\r\n```\r\n\r\nwheres with the proposed changes I get:\r\n```\r\n4 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.\r\nHeard yes (205) @1488ms\r\nHeard yes (251) @2992ms\r\nHeard yes (246) @4496ms\r\nHeard yes (250) @6000ms\r\nHeard yes (246) @7520ms\r\nHeard yes (246) @9024ms\r\nHeard yes (246) @10528ms\r\nHeard yes (250) @12032ms\r\nHeard yes (245) @13552ms\r\nHeard yes (245) @15056ms\r\nHeard yes (245) @16560ms\r\nHeard yes (251) @18080ms\r\nHeard yes (245) @19584ms\r\nHeard yes (244) @21088ms\r\nHeard yes (245) @22592ms\r\nHeard yes (252) @24112ms\r\nHeard yes (245) @25616ms\r\n```\r\n\r\nI can also confirm that I noticed problems with reading \"outside the buffer\" as reported in #46365 - in my case I changed the frequency of PDM interrupts (it's easy to simulate in Renode, probably not that easy to test on the actual HW) and noticed that TF tried to access samples \"from the future\" - not yet read from the device.", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 45877, "title": "SavedModel FROM OFFICAL DOCS always returns the same class( use OFFICAL DOCS' CODE TO PREDICT)", "body": "Hello every1!\r\nI'm learning tensorflow to predict multi classes images. And I first use **OFFCIAL DOCS** to learn.\r\nurl's here( a transfer learning ex): \r\ntensorflow.google.cn/tutorials/images/transfer_learning\r\n\r\nand I add only two lines of code to export my model, like this:\r\n```\r\n# keras_model_path = './keras_save'\r\n# model.save(keras_model_path)\r\n```\r\nAfter that, i tried to use this model to make a simple prediction of dogs and cats, like this:\r\n```\r\nfrom keras.models import load_model\r\nfrom keras.preprocessing import image\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = load_model('keras_save')\r\n\r\nimg = image.load_img('imgs_to_predict/1.jpg', target_size=(160,160))\r\nimg_array = tf.expand_dims(image.img_to_array(img), 0)\r\npredictions = model.predict(img_array)\r\nscore = tf.nn.softmax(predictions[0])\r\n\r\nclass_names = ['cats','dogs'] # right names, use a code to print when generated the model\r\n\r\nprint(\r\n    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\r\n    .format(class_names[np.argmax(score)], 100 * np.max(score)))\r\n```\r\nand, it's basically from this **offical doc**, here:\r\ntensorflow.google.cn/tutorials/images/classification\r\n\r\nthe last part of it, the only thing is the image which the doc used is from Internet.\r\n\r\nWhen trying to make a classification, the output always :\r\n`This image most likely belongs to cats with a 100 percent confidence.`\r\neven it's a **cat,** a **dog** ,or a **sunflower** from the offical doc.\r\n\r\nI'm wondering what's wrong with it, and thanks for all guys who answering my question. Appreciate! :)", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/92ef3c316fb0d67bec1f985c83d299d1/45877.ipynb). Thanks!", "@ghost,\r\nSorry for the delayed response. Can you please let us know if this issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45877\">No</a>\n"]}, {"number": 45876, "title": "tf.compat.v1.get_variable does not reuse variables within a scope and same variable name", "body": "1-  Tensorflow version = **2.4.0**\r\n\r\n2- **Google Colab Notebook** \r\n\r\nThe following code from the documentation of [tf.compat.v1.variable_scope](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) throws **assertionError**\r\n\r\n```\r\ndef foo():\r\n  with tf.compat.v1.variable_scope(\"foo\", reuse=tf.compat.v1.AUTO_REUSE):\r\n    v = tf.compat.v1.get_variable(\"v\", [1])\r\n  return v\r\n\r\nv1 = foo()  # Creates v.\r\nv2 = foo()  # Gets the same, existing v.\r\nassert v1 == v2\r\n```\r\n\r\nHere is a screenshot of the error:\r\n![Screenshot from 2020-12-19 19-52-16](https://user-images.githubusercontent.com/35839837/102691633-e6549400-4233-11eb-86a7-58a860acb263.png)\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/58f9b10bb2454d4f2832d251d59a4de5/45876.ipynb). Thanks!", "@tyagi619,\r\nThe error can be resolved by **`Disabling Eager Execution`**, as the **`Variable Scope`** is applicable in **`Graph Mode`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/7b19a9203df283dfde12e5cd12ba7d29/45876.ipynb) of working code. \r\n\r\nI will submit a Fix to add **`tf.compat.v1.disable_eager_execution()`** in the documentation of [tf.compat.v1.variable_scope](https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@tyagi619,\r\nCan you please respond to the [above comment](https://github.com/tensorflow/tensorflow/issues/45876#issuecomment-750749305)? Thanks!", "Yes that solves the issue. But I think the documentation must mention that eager execution must be disabled before running the particular piece of code", "@tyagi619,\r\nClosing this issue as it has resolved your error. Adding the command to `disable eager execution` in the documentation will be tracked in #46133. Thanks!"]}]